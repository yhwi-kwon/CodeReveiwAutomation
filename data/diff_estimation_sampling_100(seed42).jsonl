{"patch": "@@ -156,6 +156,9 @@ type Node struct {\n \t// SectorBuilder is used by the miner to fill and seal sectors.\n \tsectorBuilder sectorbuilder.SectorBuilder\n \n+\t// PeerTracker maintains a list of peers good for fetching.\n+\tPeerTracker *net.PeerTracker\n+\n \t// Fetcher is the interface for fetching data from nodes.\n \tFetcher net.Fetcher\n ", "y": 1, "oldf": "package node\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sync\"\n\t\"time\"\n\n\tps \"github.com/cskr/pubsub\"\n\t\"github.com/ipfs/go-bitswap\"\n\tbsnet \"github.com/ipfs/go-bitswap/network\"\n\tbserv \"github.com/ipfs/go-blockservice\"\n\t\"github.com/ipfs/go-cid\"\n\t\"github.com/ipfs/go-datastore\"\n\t\"github.com/ipfs/go-hamt-ipld\"\n\tbstore \"github.com/ipfs/go-ipfs-blockstore\"\n\t\"github.com/ipfs/go-ipfs-exchange-interface\"\n\t\"github.com/ipfs/go-ipfs-exchange-offline\"\n\toffroute \"github.com/ipfs/go-ipfs-routing/offline\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/ipfs/go-merkledag\"\n\t\"github.com/libp2p/go-libp2p\"\n\tautonatsvc \"github.com/libp2p/go-libp2p-autonat-svc\"\n\tcircuit \"github.com/libp2p/go-libp2p-circuit\"\n\t\"github.com/libp2p/go-libp2p-core/host\"\n\tp2pmetrics \"github.com/libp2p/go-libp2p-core/metrics\"\n\t\"github.com/libp2p/go-libp2p-core/peer\"\n\t\"github.com/libp2p/go-libp2p-core/routing\"\n\t\"github.com/libp2p/go-libp2p-kad-dht\"\n\t\"github.com/libp2p/go-libp2p-kad-dht/opts\"\n\tlibp2pps \"github.com/libp2p/go-libp2p-pubsub\"\n\trhost \"github.com/libp2p/go-libp2p/p2p/host/routed\"\n\t\"github.com/libp2p/go-libp2p/p2p/protocol/ping\"\n\tma \"github.com/multiformats/go-multiaddr\"\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/filecoin-project/go-filecoin/address\"\n\t\"github.com/filecoin-project/go-filecoin/chain\"\n\t\"github.com/filecoin-project/go-filecoin/clock\"\n\t\"github.com/filecoin-project/go-filecoin/config\"\n\t\"github.com/filecoin-project/go-filecoin/consensus\"\n\t\"github.com/filecoin-project/go-filecoin/core\"\n\t\"github.com/filecoin-project/go-filecoin/flags\"\n\t\"github.com/filecoin-project/go-filecoin/metrics\"\n\t\"github.com/filecoin-project/go-filecoin/mining\"\n\t\"github.com/filecoin-project/go-filecoin/net\"\n\t\"github.com/filecoin-project/go-filecoin/net/pubsub\"\n\t\"github.com/filecoin-project/go-filecoin/paths\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing/cfg\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing/cst\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing/dag\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing/msg\"\n\t\"github.com/filecoin-project/go-filecoin/plumbing/strgdls\"\n\t\"github.com/filecoin-project/go-filecoin/porcelain\"\n\t\"github.com/filecoin-project/go-filecoin/proofs/sectorbuilder\"\n\t\"github.com/filecoin-project/go-filecoin/proofs/verification\"\n\t\"github.com/filecoin-project/go-filecoin/protocol/block\"\n\t\"github.com/filecoin-project/go-filecoin/protocol/hello\"\n\t\"github.com/filecoin-project/go-filecoin/protocol/retrieval\"\n\t\"github.com/filecoin-project/go-filecoin/protocol/storage\"\n\t\"github.com/filecoin-project/go-filecoin/repo\"\n\t\"github.com/filecoin-project/go-filecoin/sampling\"\n\t\"github.com/filecoin-project/go-filecoin/state\"\n\t\"github.com/filecoin-project/go-filecoin/types\"\n\t\"github.com/filecoin-project/go-filecoin/util/moresync\"\n\tvmerr \"github.com/filecoin-project/go-filecoin/vm/errors\"\n\t\"github.com/filecoin-project/go-filecoin/wallet\"\n)\n\nvar log = logging.Logger(\"node\") // nolint: deadcode\n\nvar (\n\t// ErrNoMinerAddress is returned when the node is not configured to have any miner addresses.\n\tErrNoMinerAddress = errors.New(\"no miner addresses configured\")\n)\n\ntype pubSubHandler func(ctx context.Context, msg pubsub.Message) error\n\ntype nodeChainReader interface {\n\tGenesisCid() cid.Cid\n\tGetHead() types.TipSetKey\n\tGetTipSet(types.TipSetKey) (types.TipSet, error)\n\tGetTipSetState(ctx context.Context, tsKey types.TipSetKey) (state.Tree, error)\n\tHeadEvents() *ps.PubSub\n\tLoad(context.Context) error\n\tStop()\n}\n\ntype nodeChainSyncer interface {\n\tHandleNewTipset(ctx context.Context, tipsetCids types.TipSetKey, from peer.ID) error\n}\n\n// Node represents a full Filecoin node.\ntype Node struct {\n\thost     host.Host\n\tPeerHost host.Host\n\n\tConsensus   consensus.Protocol\n\tChainReader nodeChainReader\n\tSyncer      nodeChainSyncer\n\tPowerTable  consensus.PowerTableView\n\n\tBlockMiningAPI *block.MiningAPI\n\tPorcelainAPI   *porcelain.API\n\tRetrievalAPI   *retrieval.API\n\tStorageAPI     *storage.API\n\n\t// HeavyTipSetCh is a subscription to the heaviest tipset topic on the chain.\n\t// https://github.com/filecoin-project/go-filecoin/issues/2309\n\tHeaviestTipSetCh chan interface{}\n\t// cancelChainSync cancels the context for chain sync subscriptions and handlers.\n\tcancelChainSync context.CancelFunc\n\n\t// Incoming messages for block mining.\n\tInbox *core.Inbox\n\t// Messages sent and not yet mined.\n\tOutbox *core.Outbox\n\n\tWallet *wallet.Wallet\n\n\t// Mining stuff.\n\tAddNewlyMinedBlock newBlockFunc\n\t// cancelMining cancels the context for block production and sector commitments.\n\tcancelMining    context.CancelFunc\n\tMiningWorker    mining.Worker\n\tMiningScheduler mining.Scheduler\n\tmining          struct {\n\t\tsync.Mutex\n\t\tisMining bool\n\t}\n\tminingDoneWg *sync.WaitGroup\n\n\t// Storage Market Interfaces\n\tStorageMiner *storage.Miner\n\n\t// Retrieval Interfaces\n\tRetrievalMiner *retrieval.Miner\n\n\t// Network Fields\n\tBlockSub     pubsub.Subscription\n\tMessageSub   pubsub.Subscription\n\tHelloSvc     *hello.Handler\n\tBootstrapper *net.Bootstrapper\n\n\t// Data Storage Fields\n\n\t// Repo is the repo this node was created with\n\t// it contains all persistent artifacts of the filecoin node\n\tRepo repo.Repo\n\n\t// SectorBuilder is used by the miner to fill and seal sectors.\n\tsectorBuilder sectorbuilder.SectorBuilder\n\n\t// Fetcher is the interface for fetching data from nodes.\n\tFetcher net.Fetcher\n\n\t// Exchange is the interface for fetching data from other nodes.\n\tExchange exchange.Interface\n\n\t// Blockstore is the un-networked blocks interface\n\tBlockstore bstore.Blockstore\n\n\t// Blockservice is a higher level interface for fetching data\n\tblockservice bserv.BlockService\n\n\t// CborStore is a temporary interface for interacting with IPLD objects.\n\tcborStore *hamt.CborIpldStore\n\n\t// OfflineMode, when true, disables libp2p\n\tOfflineMode bool\n\n\t// Router is a router from IPFS\n\tRouter routing.Routing\n}\n\n// Config is a helper to aid in the construction of a filecoin node.\ntype Config struct {\n\tBlockTime   time.Duration\n\tLibp2pOpts  []libp2p.Option\n\tOfflineMode bool\n\tVerifier    verification.Verifier\n\tRewarder    consensus.BlockRewarder\n\tRepo        repo.Repo\n\tIsRelay     bool\n}\n\n// ConfigOpt is a configuration option for a filecoin node.\ntype ConfigOpt func(*Config) error\n\n// OfflineMode enables or disables offline mode.\nfunc OfflineMode(offlineMode bool) ConfigOpt {\n\treturn func(c *Config) error {\n\t\tc.OfflineMode = offlineMode\n\t\treturn nil\n\t}\n}\n\n// IsRelay configures node to act as a libp2p relay.\nfunc IsRelay() ConfigOpt {\n\treturn func(c *Config) error {\n\t\tc.IsRelay = true\n\t\treturn nil\n\t}\n}\n\n// BlockTime sets the blockTime.\nfunc BlockTime(blockTime time.Duration) ConfigOpt {\n\treturn func(c *Config) error {\n\t\tc.BlockTime = blockTime\n\t\treturn nil\n\t}\n}\n\n// Libp2pOptions returns a node config option that sets up the libp2p node\nfunc Libp2pOptions(opts ...libp2p.Option) ConfigOpt {\n\treturn func(nc *Config) error {\n\t\t// Quietly having your options overridden leads to hair loss\n\t\tif len(nc.Libp2pOpts) > 0 {\n\t\t\tpanic(\"Libp2pOptions can only be called once\")\n\t\t}\n\t\tnc.Libp2pOpts = opts\n\t\treturn nil\n\t}\n}\n\n// VerifierConfigOption returns a function that sets the verifier to use in the node consensus\nfunc VerifierConfigOption(verifier verification.Verifier) ConfigOpt {\n\treturn func(c *Config) error {\n\t\tc.Verifier = verifier\n\t\treturn nil\n\t}\n}\n\n// RewarderConfigOption returns a function that sets the rewarder to use in the node consensus\nfunc RewarderConfigOption(rewarder consensus.BlockRewarder) ConfigOpt {\n\treturn func(c *Config) error {\n\t\tc.Rewarder = rewarder\n\t\treturn nil\n\t}\n}\n\n// New creates a new node.\nfunc New(ctx context.Context, opts ...ConfigOpt) (*Node, error) {\n\tn := &Config{}\n\tfor _, o := range opts {\n\t\tif err := o(n); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn n.Build(ctx)\n}\n\ntype blankValidator struct{}\n\nfunc (blankValidator) Validate(_ string, _ []byte) error        { return nil }\nfunc (blankValidator) Select(_ string, _ [][]byte) (int, error) { return 0, nil }\n\n// readGenesisCid is a helper function that queries the provided datastore for\n// an entry with the genesisKey cid, returning if found.\nfunc readGenesisCid(ds datastore.Datastore) (cid.Cid, error) {\n\tbb, err := ds.Get(chain.GenesisKey)\n\tif err != nil {\n\t\treturn cid.Undef, errors.Wrap(err, \"failed to read genesisKey\")\n\t}\n\n\tvar c cid.Cid\n\terr = json.Unmarshal(bb, &c)\n\tif err != nil {\n\t\treturn cid.Undef, errors.Wrap(err, \"failed to cast genesisCid\")\n\t}\n\treturn c, nil\n}\n\n// buildHost determines if we are publically dialable.  If so use public\n// Address, if not configure node to announce relay address.\nfunc (nc *Config) buildHost(ctx context.Context, makeDHT func(host host.Host) (routing.Routing, error)) (host.Host, error) {\n\t// Node must build a host acting as a libp2p relay.  Additionally it\n\t// runs the autoNAT service which allows other nodes to check for their\n\t// own dialability by having this node attempt to dial them.\n\tmakeDHTRightType := func(h host.Host) (routing.PeerRouting, error) {\n\t\treturn makeDHT(h)\n\t}\n\n\tif nc.IsRelay {\n\t\tcfg := nc.Repo.Config()\n\t\tpublicAddr, err := ma.NewMultiaddr(cfg.Swarm.PublicRelayAddress)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tpublicAddrFactory := func(lc *libp2p.Config) error {\n\t\t\tlc.AddrsFactory = func(addrs []ma.Multiaddr) []ma.Multiaddr {\n\t\t\t\tif cfg.Swarm.PublicRelayAddress == \"\" {\n\t\t\t\t\treturn addrs\n\t\t\t\t}\n\t\t\t\treturn append(addrs, publicAddr)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\trelayHost, err := libp2p.New(\n\t\t\tctx,\n\t\t\tlibp2p.EnableRelay(circuit.OptHop),\n\t\t\tlibp2p.EnableAutoRelay(),\n\t\t\tlibp2p.Routing(makeDHTRightType),\n\t\t\tpublicAddrFactory,\n\t\t\tlibp2p.ChainOptions(nc.Libp2pOpts...),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Set up autoNATService as a streamhandler on the host.\n\t\t_, err = autonatsvc.NewAutoNATService(ctx, relayHost)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn relayHost, nil\n\t}\n\treturn libp2p.New(\n\t\tctx,\n\t\tlibp2p.EnableAutoRelay(),\n\t\tlibp2p.Routing(makeDHTRightType),\n\t\tlibp2p.ChainOptions(nc.Libp2pOpts...),\n\t)\n}\n\n// Build instantiates a filecoin Node from the settings specified in the config.\nfunc (nc *Config) Build(ctx context.Context) (*Node, error) {\n\tif nc.Repo == nil {\n\t\tnc.Repo = repo.NewInMemoryRepo()\n\t}\n\n\tbs := bstore.NewBlockstore(nc.Repo.Datastore())\n\n\tvalidator := blankValidator{}\n\n\tvar peerHost host.Host\n\tvar router routing.Routing\n\n\tbandwidthTracker := p2pmetrics.NewBandwidthCounter()\n\tnc.Libp2pOpts = append(nc.Libp2pOpts, libp2p.BandwidthReporter(bandwidthTracker))\n\n\tif !nc.OfflineMode {\n\t\tmakeDHT := func(h host.Host) (routing.Routing, error) {\n\t\t\tr, err := dht.New(\n\t\t\t\tctx,\n\t\t\t\th,\n\t\t\t\tdhtopts.Datastore(nc.Repo.Datastore()),\n\t\t\t\tdhtopts.NamespacedValidator(\"v\", validator),\n\t\t\t\tdhtopts.Protocols(net.FilecoinDHT),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"failed to setup routing\")\n\t\t\t}\n\t\t\trouter = r\n\t\t\treturn r, err\n\t\t}\n\n\t\tvar err error\n\t\tpeerHost, err = nc.buildHost(ctx, makeDHT)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\trouter = offroute.NewOfflineRouter(nc.Repo.Datastore(), validator)\n\t\tpeerHost = rhost.Wrap(noopLibP2PHost{}, router)\n\t}\n\n\t// set up pinger\n\tpingService := ping.NewPingService(peerHost)\n\n\t// setup block validation\n\t// TODO when #2961 is resolved do the needful here.\n\tblkValid := consensus.NewDefaultBlockValidator(nc.BlockTime, clock.NewSystemClock())\n\n\t// set up bitswap\n\tnwork := bsnet.NewFromIpfsHost(peerHost, router)\n\t//nwork := bsnet.NewFromIpfsHost(innerHost, router)\n\tbswap := bitswap.New(ctx, nwork, bs)\n\tbservice := bserv.New(bs, bswap)\n\tfetcher := net.NewBitswapFetcher(ctx, bservice, blkValid)\n\n\tipldCborStore := hamt.CborIpldStore{Blocks: bserv.New(bs, offline.Exchange(bs))}\n\tgenCid, err := readGenesisCid(nc.Repo.Datastore())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// set up chainstore\n\tchainStore := chain.NewStore(nc.Repo.ChainDatastore(), &ipldCborStore, &state.TreeStateLoader{}, genCid)\n\tchainState := cst.NewChainStateProvider(chainStore, &ipldCborStore)\n\tpowerTable := &consensus.MarketView{}\n\n\t// set up processor\n\tvar processor consensus.Processor\n\tif nc.Rewarder == nil {\n\t\tprocessor = consensus.NewDefaultProcessor()\n\t} else {\n\t\tprocessor = consensus.NewConfiguredProcessor(consensus.NewDefaultMessageValidator(), nc.Rewarder)\n\t}\n\n\t// set up consensus\n\tvar nodeConsensus consensus.Protocol\n\tif nc.Verifier == nil {\n\t\tnodeConsensus = consensus.NewExpected(&ipldCborStore, bs, processor, blkValid, powerTable, genCid, &verification.RustVerifier{}, nc.BlockTime)\n\t} else {\n\t\tnodeConsensus = consensus.NewExpected(&ipldCborStore, bs, processor, blkValid, powerTable, genCid, nc.Verifier, nc.BlockTime)\n\t}\n\n\t// Set up libp2p network\n\t// TODO PubSub requires strict message signing, disabled for now\n\t// reference issue: #3124\n\tfsub, err := libp2pps.NewFloodSub(ctx, peerHost, libp2pps.WithMessageSigning(false))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to set up network\")\n\t}\n\n\tbackend, err := wallet.NewDSBackend(nc.Repo.WalletDatastore())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to set up wallet backend\")\n\t}\n\tfcWallet := wallet.New(backend)\n\n\t// only the syncer gets the storage which is online connected\n\tchainSyncer := chain.NewSyncer(nodeConsensus, chainStore, fetcher, chain.Syncing)\n\tmsgPool := core.NewMessagePool(nc.Repo.Config().Mpool, consensus.NewIngestionValidator(chainState, nc.Repo.Config().Mpool))\n\tinbox := core.NewInbox(msgPool, core.InboxMaxAgeTipsets, chainStore)\n\n\tmsgQueue := core.NewMessageQueue()\n\toutboxPolicy := core.NewMessageQueuePolicy(chainStore, core.OutboxMaxAgeRounds)\n\tmsgPublisher := newDefaultMessagePublisher(pubsub.NewPublisher(fsub), net.MessageTopic, msgPool)\n\toutbox := core.NewOutbox(fcWallet, consensus.NewOutboundMessageValidator(), msgQueue, msgPublisher, outboxPolicy, chainStore, chainState)\n\n\tnd := &Node{\n\t\tblockservice: bservice,\n\t\tBlockstore:   bs,\n\t\tcborStore:    &ipldCborStore,\n\t\tConsensus:    nodeConsensus,\n\t\tChainReader:  chainStore,\n\t\tSyncer:       chainSyncer,\n\t\tPowerTable:   powerTable,\n\t\tFetcher:      fetcher,\n\t\tExchange:     bswap,\n\t\thost:         peerHost,\n\t\tInbox:        inbox,\n\t\tOfflineMode:  nc.OfflineMode,\n\t\tOutbox:       outbox,\n\t\tPeerHost:     peerHost,\n\t\tRepo:         nc.Repo,\n\t\tWallet:       fcWallet,\n\t\tRouter:       router,\n\t}\n\n\tnd.PorcelainAPI = porcelain.New(plumbing.New(&plumbing.APIDeps{\n\t\tBitswap:       bswap,\n\t\tChain:         chainState,\n\t\tConfig:        cfg.NewConfig(nc.Repo),\n\t\tDAG:           dag.NewDAG(merkledag.NewDAGService(bservice)),\n\t\tDeals:         strgdls.New(nc.Repo.DealsDatastore()),\n\t\tExpected:      nodeConsensus,\n\t\tMsgPool:       msgPool,\n\t\tMsgPreviewer:  msg.NewPreviewer(chainStore, &ipldCborStore, bs),\n\t\tMsgQueryer:    msg.NewQueryer(chainStore, &ipldCborStore, bs),\n\t\tMsgWaiter:     msg.NewWaiter(chainStore, bs, &ipldCborStore),\n\t\tNetwork:       net.New(peerHost, pubsub.NewPublisher(fsub), pubsub.NewSubscriber(fsub), net.NewRouter(router), bandwidthTracker, net.NewPinger(peerHost, pingService)),\n\t\tOutbox:        outbox,\n\t\tSectorBuilder: nd.SectorBuilder,\n\t\tWallet:        fcWallet,\n\t}))\n\n\t// Bootstrapping network peers.\n\tperiodStr := nd.Repo.Config().Bootstrap.Period\n\tperiod, err := time.ParseDuration(periodStr)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"couldn't parse bootstrap period %s\", periodStr)\n\t}\n\n\t// Bootstrapper maintains connections to some subset of addresses\n\tba := nd.Repo.Config().Bootstrap.Addresses\n\tbpi, err := net.PeerAddrsToAddrInfo(ba)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"couldn't parse bootstrap addresses [%s]\", ba)\n\t}\n\tminPeerThreshold := nd.Repo.Config().Bootstrap.MinPeerThreshold\n\tnd.Bootstrapper = net.NewBootstrapper(bpi, nd.Host(), nd.Host().Network(), nd.Router, minPeerThreshold, period)\n\n\treturn nd, nil\n}\n\n// Start boots up the node.\nfunc (node *Node) Start(ctx context.Context) error {\n\tif err := metrics.RegisterPrometheusEndpoint(node.Repo.Config().Observability.Metrics); err != nil {\n\t\treturn errors.Wrap(err, \"failed to setup metrics\")\n\t}\n\n\tif err := metrics.RegisterJaeger(node.host.ID().Pretty(), node.Repo.Config().Observability.Tracing); err != nil {\n\t\treturn errors.Wrap(err, \"failed to setup tracing\")\n\t}\n\n\tvar err error\n\tif err = node.ChainReader.Load(ctx); err != nil {\n\t\treturn err\n\t}\n\n\t// Only set these up if there is a miner configured.\n\tif _, err := node.miningAddress(); err == nil {\n\t\tif err := node.setupMining(ctx); err != nil {\n\t\t\tlog.Errorf(\"setup mining failed: %v\", err)\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// TODO: defer establishing these API endpoints until the chain is synced when the commands\n\t//   can handle their absence: https://github.com/filecoin-project/go-filecoin/issues/3137\n\terr = node.setupProtocols()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to set up protocols:\")\n\t}\n\tnode.RetrievalMiner = retrieval.NewMiner(node)\n\n\tvar syncCtx context.Context\n\tsyncCtx, node.cancelChainSync = context.WithCancel(context.Background())\n\n\t// Wire up propagation of new chain heads from the chain store to other components.\n\thead, err := node.PorcelainAPI.ChainHead()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to get chain head\")\n\t}\n\tgo node.handleNewChainHeads(syncCtx, head)\n\n\tif !node.OfflineMode {\n\t\t// Start bootstrapper.\n\t\tnode.Bootstrapper.Start(context.Background())\n\n\t\t// Establish a barrier to be released when the initial chain sync has completed.\n\t\t// Services which depend on a more-or-less synced chain can wait for this before starting up.\n\t\tchainSynced := moresync.NewLatch(1)\n\n\t\t// Start up 'hello' handshake service\n\t\tsyncCallBack := func(pid peer.ID, cids []cid.Cid, height uint64) {\n\t\t\tcidSet := types.NewTipSetKey(cids...)\n\t\t\terr := node.Syncer.HandleNewTipset(context.Background(), cidSet, pid)\n\t\t\tif err != nil {\n\t\t\t\tlog.Infof(\"error handling blocks: %s\", cidSet.String())\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// For now, consider the initial bootstrap done after the syncer has (synchronously)\n\t\t\t// processed the chain up to the head reported by the first peer to respond to hello.\n\t\t\t// This is an interim sequence until a secure network bootstrap is implemented:\n\t\t\t// https://github.com/filecoin-project/go-filecoin/issues/2674.\n\t\t\t// For now, we trust that the first node to respond will be a configured bootstrap node\n\t\t\t// and that we trust that node to inform us of the chain head.\n\t\t\t// TODO: when the syncer rejects too-far-ahead blocks received over pubsub, don't consider\n\t\t\t// sync done until it's caught up enough that it will accept blocks from pubsub.\n\t\t\t// This might require additional rounds of hello.\n\t\t\t// See https://github.com/filecoin-project/go-filecoin/issues/1105\n\t\t\tchainSynced.Done()\n\t\t}\n\t\tnode.HelloSvc = hello.New(node.Host(), node.ChainReader.GenesisCid(), syncCallBack, node.PorcelainAPI.ChainHead, node.Repo.Config().Net, flags.Commit)\n\n\t\t// Subscribe to block pubsub after the initial sync completes.\n\t\tgo func() {\n\t\t\tchainSynced.Wait()\n\t\t\tif syncCtx.Err() == nil {\n\t\t\t\t// Subscribe to block pubsub topic to learn about new chain heads.\n\t\t\t\tnode.BlockSub, err = node.pubsubscribe(syncCtx, net.BlockTopic, node.processBlock)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(err)\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\t// Subscribe to the message pubsub topic to learn about messages to mine into blocks.\n\t\t// TODO: defer this subscription until after mining (block production) is started:\n\t\t// https://github.com/filecoin-project/go-filecoin/issues/2145.\n\t\t// This is blocked by https://github.com/filecoin-project/go-filecoin/issues/2959, which\n\t\t// is necessary for message_propagate_test to start mining before testing this behaviour.\n\t\tnode.MessageSub, err = node.pubsubscribe(syncCtx, net.MessageTopic, node.processMessage)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Start heartbeats.\n\t\tif err := node.setupHeartbeatServices(ctx); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to start heartbeat services\")\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Subscribes a handler function to a pubsub topic.\nfunc (node *Node) pubsubscribe(ctx context.Context, topic string, handler pubSubHandler) (pubsub.Subscription, error) {\n\tsub, err := node.PorcelainAPI.PubSubSubscribe(topic)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to subscribe to %s\", topic)\n\t}\n\tgo node.handleSubscription(ctx, sub, handler)\n\treturn sub, nil\n}\n\nfunc (node *Node) setupHeartbeatServices(ctx context.Context) error {\n\tmag := func() address.Address {\n\t\taddr, err := node.miningAddress()\n\t\t// the only error miningAddress() returns is ErrNoMinerAddress.\n\t\t// if there is no configured miner address, simply send a zero\n\t\t// address across the wire.\n\t\tif err != nil {\n\t\t\treturn address.Undef\n\t\t}\n\t\treturn addr\n\t}\n\n\t// start the primary heartbeat service\n\tif len(node.Repo.Config().Heartbeat.BeatTarget) > 0 {\n\t\thbs := metrics.NewHeartbeatService(node.Host(), node.Repo.Config().Heartbeat, node.PorcelainAPI.ChainHead, metrics.WithMinerAddressGetter(mag))\n\t\tgo hbs.Start(ctx)\n\t}\n\n\t// check if we want to connect to an alert service. An alerting service is a heartbeat\n\t// service that can trigger alerts based on the contents of heatbeats.\n\tif alertTarget := os.Getenv(\"FIL_HEARTBEAT_ALERTS\"); len(alertTarget) > 0 {\n\t\tahbs := metrics.NewHeartbeatService(node.Host(), &config.HeartbeatConfig{\n\t\t\tBeatTarget:      alertTarget,\n\t\t\tBeatPeriod:      \"10s\",\n\t\t\tReconnectPeriod: \"10s\",\n\t\t\tNickname:        node.Repo.Config().Heartbeat.Nickname,\n\t\t}, node.PorcelainAPI.ChainHead, metrics.WithMinerAddressGetter(mag))\n\t\tgo ahbs.Start(ctx)\n\t}\n\treturn nil\n}\n\nfunc (node *Node) setupMining(ctx context.Context) error {\n\t// initialize a sector builder\n\tsectorBuilder, err := initSectorBuilderForNode(ctx, node)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to initialize sector builder\")\n\t}\n\tnode.sectorBuilder = sectorBuilder\n\n\treturn nil\n}\n\nfunc (node *Node) setIsMining(isMining bool) {\n\tnode.mining.Lock()\n\tdefer node.mining.Unlock()\n\tnode.mining.isMining = isMining\n}\n\nfunc (node *Node) handleNewMiningOutput(ctx context.Context, miningOutCh <-chan mining.Output) {\n\tdefer func() {\n\t\tnode.miningDoneWg.Done()\n\t}()\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase output, ok := <-miningOutCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif output.Err != nil {\n\t\t\t\tlog.Errorf(\"stopping mining. error: %s\", output.Err.Error())\n\t\t\t\tnode.StopMining(context.Background())\n\t\t\t} else {\n\t\t\t\tnode.miningDoneWg.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\tif node.IsMining() {\n\t\t\t\t\t\tnode.AddNewlyMinedBlock(ctx, output.NewBlock)\n\t\t\t\t\t}\n\t\t\t\t\tnode.miningDoneWg.Done()\n\t\t\t\t}()\n\t\t\t}\n\t\t}\n\t}\n\n}\n\nfunc (node *Node) handleNewChainHeads(ctx context.Context, prevHead types.TipSet) {\n\tnode.HeaviestTipSetCh = node.ChainReader.HeadEvents().Sub(chain.NewHeadTopic)\n\n\tfor {\n\t\tselect {\n\t\tcase ts, ok := <-node.HeaviestTipSetCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tnewHead, ok := ts.(types.TipSet)\n\t\t\tif !ok {\n\t\t\t\tlog.Error(\"non-tipset published on heaviest tipset channel\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !newHead.Defined() {\n\t\t\t\tlog.Error(\"tipset of size 0 published on heaviest tipset channel. ignoring and waiting for a new heaviest tipset.\")\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif err := node.Outbox.HandleNewHead(ctx, prevHead, newHead); err != nil {\n\t\t\t\tlog.Error(\"updating outbound message queue for new tipset\", err)\n\t\t\t}\n\t\t\tif err := node.Inbox.HandleNewHead(ctx, prevHead, newHead); err != nil {\n\t\t\t\tlog.Error(\"updating message pool for new tipset\", err)\n\t\t\t}\n\t\t\tprevHead = newHead\n\n\t\t\tif node.StorageMiner != nil {\n\t\t\t\terr := node.StorageMiner.OnNewHeaviestTipSet(newHead)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(err)\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (node *Node) cancelSubscriptions() {\n\tif node.cancelChainSync != nil {\n\t\tnode.cancelChainSync()\n\t}\n\n\tif node.BlockSub != nil {\n\t\tnode.BlockSub.Cancel()\n\t\tnode.BlockSub = nil\n\t}\n\n\tif node.MessageSub != nil {\n\t\tnode.MessageSub.Cancel()\n\t\tnode.MessageSub = nil\n\t}\n}\n\n// Stop initiates the shutdown of the node.\nfunc (node *Node) Stop(ctx context.Context) {\n\tnode.ChainReader.HeadEvents().Unsub(node.HeaviestTipSetCh)\n\tnode.StopMining(ctx)\n\n\tnode.cancelSubscriptions()\n\tnode.ChainReader.Stop()\n\n\tif node.SectorBuilder() != nil {\n\t\tif err := node.SectorBuilder().Close(); err != nil {\n\t\t\tfmt.Printf(\"error closing sector builder: %s\\n\", err)\n\t\t}\n\t\tnode.sectorBuilder = nil\n\t}\n\n\tif err := node.Host().Close(); err != nil {\n\t\tfmt.Printf(\"error closing host: %s\\n\", err)\n\t}\n\n\tif err := node.Repo.Close(); err != nil {\n\t\tfmt.Printf(\"error closing repo: %s\\n\", err)\n\t}\n\n\tnode.Bootstrapper.Stop()\n\n\tfmt.Println(\"stopping filecoin :(\")\n}\n\ntype newBlockFunc func(context.Context, *types.Block)\n\nfunc (node *Node) addNewlyMinedBlock(ctx context.Context, b *types.Block) {\n\tlog.Debugf(\"Got a newly mined block from the mining worker: %s\", b)\n\tif err := node.AddNewBlock(ctx, b); err != nil {\n\t\tlog.Warningf(\"error adding new mined block: %s. err: %s\", b.Cid().String(), err.Error())\n\t}\n}\n\n// miningAddress returns the address of the mining actor mining on behalf of\n// the node.\nfunc (node *Node) miningAddress() (address.Address, error) {\n\taddr := node.Repo.Config().Mining.MinerAddress\n\tif addr.Empty() {\n\t\treturn address.Undef, ErrNoMinerAddress\n\t}\n\n\treturn addr, nil\n}\n\n// MiningTimes returns the configured time it takes to mine a block, and also\n// the mining delay duration, which is currently a fixed fraction of block time.\n// Note this is mocked behavior, in production this time is determined by how\n// long it takes to generate PoSTs.\nfunc (node *Node) MiningTimes() (time.Duration, time.Duration) {\n\tblockTime := node.PorcelainAPI.BlockTime()\n\tmineDelay := blockTime / mining.MineDelayConversionFactor\n\treturn blockTime, mineDelay\n}\n\n// StartMining causes the node to start feeding blocks to the mining worker and initializes\n// the SectorBuilder for the mining address.\nfunc (node *Node) StartMining(ctx context.Context) error {\n\tif node.IsMining() {\n\t\treturn errors.New(\"Node is already mining\")\n\t}\n\tminerAddr, err := node.miningAddress()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to get mining address\")\n\t}\n\t_, err = node.PorcelainAPI.ActorGet(ctx, minerAddr)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to get miner actor\")\n\t}\n\n\t// ensure we have a sector builder\n\tif node.SectorBuilder() == nil {\n\t\tif err := node.setupMining(ctx); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tminerOwnerAddr, err := node.PorcelainAPI.MinerGetOwnerAddress(ctx, minerAddr)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"failed to get mining owner address for miner %s\", minerAddr)\n\t}\n\n\t_, mineDelay := node.MiningTimes()\n\n\tif node.MiningWorker == nil {\n\t\tif node.MiningWorker, err = node.CreateMiningWorker(ctx); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif node.MiningScheduler == nil {\n\t\tnode.MiningScheduler = mining.NewScheduler(node.MiningWorker, mineDelay, node.PorcelainAPI.ChainHead)\n\t} else if node.MiningScheduler.IsStarted() {\n\t\treturn fmt.Errorf(\"miner scheduler already started\")\n\t}\n\n\tvar miningCtx context.Context\n\tminingCtx, node.cancelMining = context.WithCancel(context.Background())\n\n\toutCh, doneWg := node.MiningScheduler.Start(miningCtx)\n\n\tnode.miningDoneWg = doneWg\n\tnode.AddNewlyMinedBlock = node.addNewlyMinedBlock\n\tnode.miningDoneWg.Add(1)\n\tgo node.handleNewMiningOutput(miningCtx, outCh)\n\n\t// initialize a storage miner\n\tstorageMiner, err := initStorageMinerForNode(ctx, node)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to initialize storage miner\")\n\t}\n\tnode.StorageMiner = storageMiner\n\n\t// loop, turning sealing-results into commitSector messages to be included\n\t// in the chain\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase result := <-node.SectorBuilder().SectorSealResults():\n\t\t\t\tif result.SealingErr != nil {\n\t\t\t\t\tlog.Errorf(\"failed to seal sector with id %d: %s\", result.SectorID, result.SealingErr.Error())\n\t\t\t\t} else if result.SealingResult != nil {\n\n\t\t\t\t\t// TODO: determine these algorithmically by simulating call and querying historical prices\n\t\t\t\t\tgasPrice := types.NewGasPrice(1)\n\t\t\t\t\tgasUnits := types.NewGasUnits(300)\n\n\t\t\t\t\tval := result.SealingResult\n\t\t\t\t\t// This call can fail due to, e.g. nonce collisions. Our miners existence depends on this.\n\t\t\t\t\t// We should deal with this, but MessageSendWithRetry is problematic.\n\t\t\t\t\tmsgCid, err := node.PorcelainAPI.MessageSend(\n\t\t\t\t\t\tminingCtx,\n\t\t\t\t\t\tminerOwnerAddr,\n\t\t\t\t\t\tminerAddr,\n\t\t\t\t\t\ttypes.ZeroAttoFIL,\n\t\t\t\t\t\tgasPrice,\n\t\t\t\t\t\tgasUnits,\n\t\t\t\t\t\t\"commitSector\",\n\t\t\t\t\t\tval.SectorID,\n\t\t\t\t\t\tval.CommD[:],\n\t\t\t\t\t\tval.CommR[:],\n\t\t\t\t\t\tval.CommRStar[:],\n\t\t\t\t\t\tval.Proof[:],\n\t\t\t\t\t)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tlog.Errorf(\"failed to send commitSector message from %s to %s for sector with id %d: %s\", minerOwnerAddr, minerAddr, val.SectorID, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\tnode.StorageMiner.OnCommitmentSent(val, msgCid, nil)\n\t\t\t\t}\n\t\t\tcase <-miningCtx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// schedules sealing of staged piece-data\n\tif node.Repo.Config().Mining.AutoSealIntervalSeconds > 0 {\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-miningCtx.Done():\n\t\t\t\t\treturn\n\t\t\t\tcase <-time.After(time.Duration(node.Repo.Config().Mining.AutoSealIntervalSeconds) * time.Second):\n\t\t\t\t\tlog.Info(\"auto-seal has been triggered\")\n\t\t\t\t\tif err := node.SectorBuilder().SealAllStagedSectors(miningCtx); err != nil {\n\t\t\t\t\t\tlog.Errorf(\"scheduler received error from node.SectorBuilder.SealAllStagedSectors (%s) - exiting\", err.Error())\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t} else {\n\t\tlog.Debug(\"auto-seal is disabled\")\n\t}\n\tnode.setIsMining(true)\n\n\treturn nil\n}\n\nfunc initSectorBuilderForNode(ctx context.Context, node *Node) (sectorbuilder.SectorBuilder, error) {\n\tminerAddr, err := node.miningAddress()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get node's mining address\")\n\t}\n\n\tsectorSize, err := node.PorcelainAPI.MinerGetSectorSize(ctx, minerAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to get sector size for miner w/address %s\", minerAddr.String())\n\t}\n\n\tlastUsedSectorID, err := node.PorcelainAPI.MinerGetLastCommittedSectorID(ctx, minerAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to get last used sector id for miner w/address %s\", minerAddr.String())\n\t}\n\n\t// TODO: Currently, weconfigure the RustSectorBuilder to store its\n\t// metadata in the staging directory, it should be in its own directory.\n\t//\n\t// Tracked here: https://github.com/filecoin-project/rust-fil-proofs/issues/402\n\trepoPath, err := node.Repo.Path()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsectorDir, err := paths.GetSectorPath(node.Repo.Config().SectorBase.RootDir, repoPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstagingDir, err := paths.StagingDir(sectorDir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsealedDir, err := paths.SealedDir(sectorDir)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcfg := sectorbuilder.RustSectorBuilderConfig{\n\t\tBlockService:     node.blockservice,\n\t\tLastUsedSectorID: lastUsedSectorID,\n\t\tMetadataDir:      stagingDir,\n\t\tMinerAddr:        minerAddr,\n\t\tSealedSectorDir:  sealedDir,\n\t\tStagedSectorDir:  stagingDir,\n\t\tSectorClass:      types.NewSectorClass(sectorSize),\n\t}\n\n\tsb, err := sectorbuilder.NewRustSectorBuilder(cfg)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, fmt.Sprintf(\"failed to initialize sector builder for miner %s\", minerAddr.String()))\n\t}\n\n\treturn sb, nil\n}\n\nfunc initStorageMinerForNode(ctx context.Context, node *Node) (*storage.Miner, error) {\n\tminerAddr, err := node.miningAddress()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get node's mining address\")\n\t}\n\n\townerAddress, err := node.PorcelainAPI.MinerGetOwnerAddress(ctx, minerAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"no mining owner available, skipping storage miner setup\")\n\t}\n\tworkerAddress := ownerAddress\n\n\tsectorSize, err := node.PorcelainAPI.MinerGetSectorSize(ctx, minerAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to fetch miner's sector size\")\n\t}\n\n\tprover := storage.NewProver(minerAddr, workerAddress, sectorSize, node.PorcelainAPI, node.PorcelainAPI)\n\tminer, err := storage.NewMiner(minerAddr, ownerAddress, workerAddress, prover, sectorSize, node, node.Repo.DealsDatastore(), node.PorcelainAPI)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to instantiate storage miner\")\n\t}\n\n\treturn miner, nil\n}\n\n// StopMining stops mining on new blocks.\nfunc (node *Node) StopMining(ctx context.Context) {\n\tnode.setIsMining(false)\n\n\tif node.cancelMining != nil {\n\t\tnode.cancelMining()\n\t}\n\n\tif node.miningDoneWg != nil {\n\t\tnode.miningDoneWg.Wait()\n\t}\n\n\t// TODO: stop node.StorageMiner\n}\n\nfunc (node *Node) handleSubscription(ctx context.Context, sub pubsub.Subscription, handler pubSubHandler) {\n\tfor {\n\t\treceived, err := sub.Next(ctx)\n\t\tif err != nil {\n\t\t\tif ctx.Err() != context.Canceled {\n\t\t\t\tlog.Errorf(\"error reading message from topic %s: %s\", sub.Topic(), err)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tif err := handler(ctx, received); err != nil {\n\t\t\thandlerName := runtime.FuncForPC(reflect.ValueOf(handler).Pointer()).Name()\n\t\t\tif vmerr.ShouldRevert(err) {\n\t\t\t\tlog.Infof(\"error in handler %s for topic %s: %s\", handlerName, sub.Topic(), err)\n\t\t\t} else if err != context.Canceled {\n\t\t\t\tlog.Errorf(\"error in handler %s for topic %s: %s\", handlerName, sub.Topic(), err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// setupProtocols creates protocol clients and miners, then sets the node's APIs\n// for each\nfunc (node *Node) setupProtocols() error {\n\t_, mineDelay := node.MiningTimes()\n\tblockMiningAPI := block.New(\n\t\tnode.AddNewBlock,\n\t\tnode.ChainReader,\n\t\tnode.IsMining,\n\t\tmineDelay,\n\t\tnode.StartMining,\n\t\tnode.StopMining,\n\t\tnode.CreateMiningWorker)\n\n\tnode.BlockMiningAPI = &blockMiningAPI\n\n\t// set up retrieval client and api\n\tretapi := retrieval.NewAPI(retrieval.NewClient(node.host, node.PorcelainAPI))\n\tnode.RetrievalAPI = &retapi\n\n\t// set up storage client and api\n\tsmc := storage.NewClient(node.host, node.PorcelainAPI)\n\tsmcAPI := storage.NewAPI(smc)\n\tnode.StorageAPI = &smcAPI\n\treturn nil\n}\n\n// CreateMiningWorker creates a mining.Worker for the node using the configured\n// getStateTree, getWeight, and getAncestors functions for the node\nfunc (node *Node) CreateMiningWorker(ctx context.Context) (mining.Worker, error) {\n\tprocessor := consensus.NewDefaultProcessor()\n\n\tminerAddr, err := node.miningAddress()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get mining address\")\n\t}\n\n\tminerWorker, err := node.PorcelainAPI.MinerGetWorker(ctx, minerAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not get key from miner actor\")\n\t}\n\n\tminerOwnerAddr, err := node.PorcelainAPI.MinerGetOwnerAddress(ctx, minerAddr)\n\tif err != nil {\n\t\tlog.Errorf(\"could not get owner address of miner actor\")\n\t\treturn nil, err\n\t}\n\treturn mining.NewDefaultWorker(mining.WorkerParameters{\n\t\tAPI: node.PorcelainAPI,\n\n\t\tMinerAddr:      minerAddr,\n\t\tMinerOwnerAddr: minerOwnerAddr,\n\t\tMinerWorker:    minerWorker,\n\t\tWorkerSigner:   node.Wallet,\n\n\t\tGetStateTree: node.getStateTree,\n\t\tGetWeight:    node.getWeight,\n\t\tGetAncestors: node.getAncestors,\n\n\t\tMessageSource: node.Inbox.Pool(),\n\t\tProcessor:     processor,\n\t\tPowerTable:    node.PowerTable,\n\t\tBlockstore:    node.Blockstore}), nil\n}\n\n// getStateTree is the default GetStateTree function for the mining worker.\nfunc (node *Node) getStateTree(ctx context.Context, ts types.TipSet) (state.Tree, error) {\n\treturn node.ChainReader.GetTipSetState(ctx, ts.Key())\n}\n\n// getWeight is the default GetWeight function for the mining worker.\nfunc (node *Node) getWeight(ctx context.Context, ts types.TipSet) (uint64, error) {\n\tparent, err := ts.Parents()\n\tif err != nil {\n\t\treturn uint64(0), err\n\t}\n\t// TODO handle genesis cid more gracefully\n\tif parent.Len() == 0 {\n\t\treturn node.Consensus.Weight(ctx, ts, nil)\n\t}\n\tpSt, err := node.ChainReader.GetTipSetState(ctx, parent)\n\tif err != nil {\n\t\treturn uint64(0), err\n\t}\n\treturn node.Consensus.Weight(ctx, ts, pSt)\n}\n\n// getAncestors is the default GetAncestors function for the mining worker.\nfunc (node *Node) getAncestors(ctx context.Context, ts types.TipSet, newBlockHeight *types.BlockHeight) ([]types.TipSet, error) {\n\tancestorHeight := types.NewBlockHeight(consensus.AncestorRoundsNeeded)\n\treturn chain.GetRecentAncestors(ctx, ts, node.ChainReader, newBlockHeight, ancestorHeight, sampling.LookbackParameter)\n}\n\n// -- Accessors\n\n// Host returns the nodes host.\nfunc (node *Node) Host() host.Host {\n\treturn node.host\n}\n\n// SectorBuilder returns the nodes sectorBuilder.\nfunc (node *Node) SectorBuilder() sectorbuilder.SectorBuilder {\n\treturn node.sectorBuilder\n}\n\n// BlockService returns the nodes blockservice.\nfunc (node *Node) BlockService() bserv.BlockService {\n\treturn node.blockservice\n}\n\n// CborStore returns the nodes cborStore.\nfunc (node *Node) CborStore() *hamt.CborIpldStore {\n\treturn node.cborStore\n}\n\n// IsMining returns a boolean indicating whether the node is mining blocks.\nfunc (node *Node) IsMining() bool {\n\tnode.mining.Lock()\n\tdefer node.mining.Unlock()\n\treturn node.mining.isMining\n}\n", "idx": 1, "id": 20537, "msg": "Can we make this an interface instead? I think it should make changes easier down the road.", "proj": "filecoin-project-venus", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -32,14 +32,6 @@ import (\n )\n \n var (\n-\tpluginInfo = spi.GetPluginInfoResponse{\n-\t\tDescription: \"\",\n-\t\tDateCreated: \"\",\n-\t\tVersion:     \"\",\n-\t\tAuthor:      \"\",\n-\t\tCompany:     \"\",\n-\t}\n-\n \tsqlError = errs.Class(\"datastore-sql\")\n )\n ", "y": 0, "oldf": "package sql\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/gofrs/uuid\"\n\thclog \"github.com/hashicorp/go-hclog\"\n\t\"github.com/hashicorp/hcl\"\n\t\"github.com/jinzhu/gorm\"\n\n\t_ \"github.com/jinzhu/gorm/dialects/sqlite\" // gorm sqlite dialect init registration\n\t\"github.com/spiffe/spire/pkg/common/bundleutil\"\n\t\"github.com/spiffe/spire/pkg/common/catalog\"\n\t\"github.com/spiffe/spire/pkg/common/idutil\"\n\t\"github.com/spiffe/spire/pkg/common/protoutil\"\n\t\"github.com/spiffe/spire/pkg/common/telemetry\"\n\t\"github.com/spiffe/spire/pkg/server/plugin/datastore\"\n\t\"github.com/spiffe/spire/proto/spire/common\"\n\tspi \"github.com/spiffe/spire/proto/spire/common/plugin\"\n\t\"github.com/zeebo/errs\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n)\n\nvar (\n\tpluginInfo = spi.GetPluginInfoResponse{\n\t\tDescription: \"\",\n\t\tDateCreated: \"\",\n\t\tVersion:     \"\",\n\t\tAuthor:      \"\",\n\t\tCompany:     \"\",\n\t}\n\n\tsqlError = errs.Class(\"datastore-sql\")\n)\n\nconst (\n\tPluginName = \"sql\"\n\n\t// MySQL database type\n\tMySQL = \"mysql\"\n\t// PostgreSQL database type\n\tPostgreSQL = \"postgres\"\n\t// SQLite database type\n\tSQLite = \"sqlite3\"\n)\n\nfunc BuiltIn() catalog.Plugin {\n\treturn builtin(New())\n}\n\nfunc builtin(p *Plugin) catalog.Plugin {\n\treturn catalog.MakePlugin(PluginName,\n\t\tdatastore.PluginServer(p),\n\t)\n}\n\n// Configuration for the datastore.\n// Pointer values are used to distinguish between \"unset\" and \"zero\" values.\ntype configuration struct {\n\tDatabaseType       string  `hcl:\"database_type\" json:\"database_type\"`\n\tConnectionString   string  `hcl:\"connection_string\" json:\"connection_string\"`\n\tRoConnectionString string  `hcl:\"ro_connection_string\" json:\"ro_connection_string\"`\n\tRootCAPath         string  `hcl:\"root_ca_path\" json:\"root_ca_path\"`\n\tClientCertPath     string  `hcl:\"client_cert_path\" json:\"client_cert_path\"`\n\tClientKeyPath      string  `hcl:\"client_key_path\" json:\"client_key_path\"`\n\tConnMaxLifetime    *string `hcl:\"conn_max_lifetime\" json:\"conn_max_lifetime\"`\n\tMaxOpenConns       *int    `hcl:\"max_open_conns\" json:\"max_open_conns\"`\n\tMaxIdleConns       *int    `hcl:\"max_idle_conns\" json:\"max_idle_conns\"`\n\tDisableMigration   bool    `hcl:\"disable_migration\" json:\"disable_migration\"`\n\n\t// Undocumented flags\n\tLogSQL bool `hcl:\"log_sql\" json:\"log_sql\"`\n}\n\ntype sqlDB struct {\n\tdatabaseType     string\n\tconnectionString string\n\traw              *sql.DB\n\t*gorm.DB\n\n\tdialect     dialect\n\tstmtCache   *stmtCache\n\tsupportsCTE bool\n\n\t// this lock is only required for synchronized writes with \"sqlite3\". see\n\t// the withTx() implementation for details.\n\topMu sync.Mutex\n}\n\nfunc (db *sqlDB) QueryContext(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error) {\n\tstmt, err := db.stmtCache.get(ctx, query)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn stmt.QueryContext(ctx, args...)\n}\n\n// Plugin is a DataStore plugin implemented via a SQL database\ntype Plugin struct {\n\tdatastore.UnsafeDataStoreServer\n\n\tmu   sync.Mutex\n\tdb   *sqlDB\n\troDb *sqlDB\n\tlog  hclog.Logger\n}\n\n// New creates a new sql plugin struct. Configure must be called\n// in order to start the db.\nfunc New() *Plugin {\n\treturn &Plugin{}\n}\n\nfunc (ds *Plugin) SetLogger(logger hclog.Logger) {\n\tds.log = logger\n}\n\n// CreateBundle stores the given bundle\nfunc (ds *Plugin) CreateBundle(ctx context.Context, req *datastore.CreateBundleRequest) (resp *datastore.CreateBundleResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = createBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// UpdateBundle updates an existing bundle with the given CAs. Overwrites any\n// existing certificates.\nfunc (ds *Plugin) UpdateBundle(ctx context.Context, req *datastore.UpdateBundleRequest) (resp *datastore.UpdateBundleResponse, err error) {\n\tif err = ds.withReadModifyWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = updateBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// SetBundle sets bundle contents. If no bundle exists for the trust domain, it is created.\nfunc (ds *Plugin) SetBundle(ctx context.Context, req *datastore.SetBundleRequest) (resp *datastore.SetBundleResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = setBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// AppendBundle append bundle contents to the existing bundle (by trust domain). If no existing one is present, create it.\nfunc (ds *Plugin) AppendBundle(ctx context.Context, req *datastore.AppendBundleRequest) (resp *datastore.AppendBundleResponse, err error) {\n\tif err = ds.withReadModifyWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = appendBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// DeleteBundle deletes the bundle with the matching TrustDomain. Any CACert data passed is ignored.\nfunc (ds *Plugin) DeleteBundle(ctx context.Context, req *datastore.DeleteBundleRequest) (resp *datastore.DeleteBundleResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = deleteBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// FetchBundle returns the bundle matching the specified Trust Domain.\nfunc (ds *Plugin) FetchBundle(ctx context.Context, req *datastore.FetchBundleRequest) (resp *datastore.FetchBundleResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = fetchBundle(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// CountBundles can be used to count all existing bundles.\nfunc (ds *Plugin) CountBundles(ctx context.Context, req *datastore.CountBundlesRequest) (resp *datastore.CountBundlesResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = countBundles(tx)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// ListBundles can be used to fetch all existing bundles.\nfunc (ds *Plugin) ListBundles(ctx context.Context, req *datastore.ListBundlesRequest) (resp *datastore.ListBundlesResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = listBundles(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// PruneBundle removes expired certs and keys from a bundle\nfunc (ds *Plugin) PruneBundle(ctx context.Context, req *datastore.PruneBundleRequest) (resp *datastore.PruneBundleResponse, err error) {\n\tif err = ds.withReadModifyWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = pruneBundle(tx, req, ds.log)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn resp, nil\n}\n\n// CreateAttestedNode stores the given attested node\nfunc (ds *Plugin) CreateAttestedNode(ctx context.Context,\n\treq *datastore.CreateAttestedNodeRequest) (resp *datastore.CreateAttestedNodeResponse, err error) {\n\tif req.Node == nil {\n\t\treturn nil, sqlError.New(\"invalid request: missing attested node\")\n\t}\n\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = createAttestedNode(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// FetchAttestedNode fetches an existing attested node by SPIFFE ID\nfunc (ds *Plugin) FetchAttestedNode(ctx context.Context,\n\treq *datastore.FetchAttestedNodeRequest) (resp *datastore.FetchAttestedNodeResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = fetchAttestedNode(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// CountAttestedNodes counts all attested nodes\nfunc (ds *Plugin) CountAttestedNodes(ctx context.Context,\n\treq *datastore.CountAttestedNodesRequest) (resp *datastore.CountAttestedNodesResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = countAttestedNodes(tx)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn resp, nil\n}\n\n// ListAttestedNodes lists all attested nodes (pagination available)\nfunc (ds *Plugin) ListAttestedNodes(ctx context.Context,\n\treq *datastore.ListAttestedNodesRequest) (resp *datastore.ListAttestedNodesResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = listAttestedNodes(ctx, ds.db, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// UpdateAttestedNode updates the given node's cert serial and expiration.\nfunc (ds *Plugin) UpdateAttestedNode(ctx context.Context,\n\treq *datastore.UpdateAttestedNodeRequest) (resp *datastore.UpdateAttestedNodeResponse, err error) {\n\tif err = ds.withReadModifyWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = updateAttestedNode(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// DeleteAttestedNode deletes the given attested node\nfunc (ds *Plugin) DeleteAttestedNode(ctx context.Context,\n\treq *datastore.DeleteAttestedNodeRequest) (resp *datastore.DeleteAttestedNodeResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = deleteAttestedNode(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// SetNodeSelectors sets node (agent) selectors by SPIFFE ID, deleting old selectors first\nfunc (ds *Plugin) SetNodeSelectors(ctx context.Context, req *datastore.SetNodeSelectorsRequest) (resp *datastore.SetNodeSelectorsResponse, err error) {\n\tif req.Selectors == nil {\n\t\treturn nil, errors.New(\"invalid request: missing selectors\")\n\t}\n\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = setNodeSelectors(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// GetNodeSelectors gets node (agent) selectors by SPIFFE ID\nfunc (ds *Plugin) GetNodeSelectors(ctx context.Context,\n\treq *datastore.GetNodeSelectorsRequest) (resp *datastore.GetNodeSelectorsResponse, err error) {\n\tif req.TolerateStale && ds.roDb != nil {\n\t\treturn getNodeSelectors(ctx, ds.roDb, req)\n\t}\n\treturn getNodeSelectors(ctx, ds.db, req)\n}\n\n// ListNodeSelectors gets node (agent) selectors by SPIFFE ID\nfunc (ds *Plugin) ListNodeSelectors(ctx context.Context,\n\treq *datastore.ListNodeSelectorsRequest) (resp *datastore.ListNodeSelectorsResponse, err error) {\n\tif req.TolerateStale && ds.roDb != nil {\n\t\treturn listNodeSelectors(ctx, ds.roDb, req)\n\t}\n\treturn listNodeSelectors(ctx, ds.db, req)\n}\n\n// CreateRegistrationEntry stores the given registration entry\nfunc (ds *Plugin) CreateRegistrationEntry(ctx context.Context,\n\treq *datastore.CreateRegistrationEntryRequest) (resp *datastore.CreateRegistrationEntryResponse, err error) {\n\t// TODO: Validations should be done in the ProtoBuf level [https://github.com/spiffe/spire/issues/44]\n\tif err = validateRegistrationEntry(req.Entry); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = createRegistrationEntry(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// FetchRegistrationEntry fetches an existing registration by entry ID\nfunc (ds *Plugin) FetchRegistrationEntry(ctx context.Context,\n\treq *datastore.FetchRegistrationEntryRequest) (resp *datastore.FetchRegistrationEntryResponse, err error) {\n\treturn fetchRegistrationEntry(ctx, ds.db, req)\n}\n\n// CounCountRegistrationEntries counts all registrations (pagination available)\nfunc (ds *Plugin) CountRegistrationEntries(ctx context.Context,\n\treq *datastore.CountRegistrationEntriesRequest) (resp *datastore.CountRegistrationEntriesResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = countRegistrationEntries(tx)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn resp, nil\n}\n\n// ListRegistrationEntries lists all registrations (pagination available)\nfunc (ds *Plugin) ListRegistrationEntries(ctx context.Context,\n\treq *datastore.ListRegistrationEntriesRequest) (resp *datastore.ListRegistrationEntriesResponse, err error) {\n\tif req.TolerateStale && ds.roDb != nil {\n\t\treturn listRegistrationEntries(ctx, ds.roDb, req)\n\t}\n\treturn listRegistrationEntries(ctx, ds.db, req)\n}\n\n// UpdateRegistrationEntry updates an existing registration entry\nfunc (ds *Plugin) UpdateRegistrationEntry(ctx context.Context,\n\treq *datastore.UpdateRegistrationEntryRequest) (resp *datastore.UpdateRegistrationEntryResponse, err error) {\n\tif err = ds.withReadModifyWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = updateRegistrationEntry(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// DeleteRegistrationEntry deletes the given registration\nfunc (ds *Plugin) DeleteRegistrationEntry(ctx context.Context,\n\treq *datastore.DeleteRegistrationEntryRequest) (resp *datastore.DeleteRegistrationEntryResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = deleteRegistrationEntry(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// PruneRegistrationEntries takes a registration entry message, and deletes all entries which have expired\n// before the date in the message\nfunc (ds *Plugin) PruneRegistrationEntries(ctx context.Context, req *datastore.PruneRegistrationEntriesRequest) (resp *datastore.PruneRegistrationEntriesResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = pruneRegistrationEntries(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// CreateJoinToken takes a Token message and stores it\nfunc (ds *Plugin) CreateJoinToken(ctx context.Context, req *datastore.CreateJoinTokenRequest) (resp *datastore.CreateJoinTokenResponse, err error) {\n\tif req.JoinToken == nil || req.JoinToken.Token == \"\" || req.JoinToken.Expiry == 0 {\n\t\treturn nil, errors.New(\"token and expiry are required\")\n\t}\n\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = createJoinToken(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// FetchJoinToken takes a Token message and returns one, populating the fields\n// we have knowledge of\nfunc (ds *Plugin) FetchJoinToken(ctx context.Context, req *datastore.FetchJoinTokenRequest) (resp *datastore.FetchJoinTokenResponse, err error) {\n\tif err = ds.withReadTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = fetchJoinToken(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn resp, nil\n}\n\n// DeleteJoinToken deletes the given join token\nfunc (ds *Plugin) DeleteJoinToken(ctx context.Context, req *datastore.DeleteJoinTokenRequest) (resp *datastore.DeleteJoinTokenResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = deleteJoinToken(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// PruneJoinTokens takes a Token message, and deletes all tokens which have expired\n// before the date in the message\nfunc (ds *Plugin) PruneJoinTokens(ctx context.Context, req *datastore.PruneJoinTokensRequest) (resp *datastore.PruneJoinTokensResponse, err error) {\n\tif err = ds.withWriteTx(ctx, func(tx *gorm.DB) (err error) {\n\t\tresp, err = pruneJoinTokens(tx, req)\n\t\treturn err\n\t}); err != nil {\n\t\treturn nil, err\n\t}\n\treturn resp, nil\n}\n\n// Configure parses HCL config payload into config struct, and opens new DB based on the result\nfunc (ds *Plugin) Configure(ctx context.Context, req *spi.ConfigureRequest) (*spi.ConfigureResponse, error) {\n\tconfig := &configuration{}\n\tif err := hcl.Decode(config, req.Configuration); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := config.Validate(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tds.mu.Lock()\n\tdefer ds.mu.Unlock()\n\n\tif err := ds.openConnection(config, false); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif config.RoConnectionString == \"\" {\n\t\treturn &spi.ConfigureResponse{}, nil\n\t}\n\n\tif err := ds.openConnection(config, true); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &spi.ConfigureResponse{}, nil\n}\n\nfunc (ds *Plugin) openConnection(config *configuration, isReadOnly bool) error {\n\tconnectionString := getConnectionString(config, isReadOnly)\n\tsqlDb := ds.db\n\tif isReadOnly {\n\t\tsqlDb = ds.roDb\n\t}\n\n\tif sqlDb == nil || connectionString != sqlDb.connectionString || config.DatabaseType != ds.db.databaseType {\n\t\tdb, version, supportsCTE, dialect, err := ds.openDB(config, isReadOnly)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\traw := db.DB()\n\t\tif raw == nil {\n\t\t\treturn sqlError.New(\"unable to get raw database object\")\n\t\t}\n\n\t\tif sqlDb != nil {\n\t\t\tsqlDb.Close()\n\t\t}\n\n\t\tds.log.Info(\"Connected to SQL database\",\n\t\t\t\"type\", config.DatabaseType,\n\t\t\t\"version\", version,\n\t\t\t\"read_only\", isReadOnly,\n\t\t)\n\n\t\tsqlDb = &sqlDB{\n\t\t\tDB:               db,\n\t\t\traw:              raw,\n\t\t\tdatabaseType:     config.DatabaseType,\n\t\t\tdialect:          dialect,\n\t\t\tconnectionString: connectionString,\n\t\t\tstmtCache:        newStmtCache(raw),\n\t\t\tsupportsCTE:      supportsCTE,\n\t\t}\n\t}\n\n\tif isReadOnly {\n\t\tds.roDb = sqlDb\n\t} else {\n\t\tds.db = sqlDb\n\t}\n\n\tsqlDb.LogMode(config.LogSQL)\n\treturn nil\n}\n\nfunc (ds *Plugin) closeDB() {\n\tif ds.db != nil {\n\t\tds.db.Close()\n\t}\n\n\tif ds.roDb != nil {\n\t\tds.roDb.Close()\n\t}\n}\n\n// GetPluginInfo returns the sql plugin\nfunc (*Plugin) GetPluginInfo(context.Context, *spi.GetPluginInfoRequest) (*spi.GetPluginInfoResponse, error) {\n\treturn &pluginInfo, nil\n}\n\n// withReadModifyWriteTx wraps the operation in a transaction appropriate for\n// operations that will read one or more rows, change one or more columns in\n// those rows, and then set them back. This requires a stronger level of\n// consistency that prevents two transactions from doing read-modify-write\n// concurrently.\nfunc (ds *Plugin) withReadModifyWriteTx(ctx context.Context, op func(tx *gorm.DB) error) error {\n\tisolationLevel := sql.LevelRepeatableRead\n\tif ds.db.databaseType == MySQL {\n\t\t// MySQL REPEATABLE READ is weaker than that of PostgreSQL. Namely,\n\t\t// PostgreSQL, beyond providing the minimum consistency guarantees\n\t\t// mandated for REPEATABLE READ in the standard, automatically fails\n\t\t// concurrent transactions that try to update the same target row.\n\t\t//\n\t\t// MySQL SERIALIZABLE is the same as REPEATABLE READ except that it\n\t\t// automatically converts `SELECT` to `SELECT ... LOCK FOR SHARE MODE`\n\t\t// which \"sets a shared lock that permits other transactions to read\n\t\t// the examined rows but not to update or delete them\", which is what\n\t\t// we want.\n\t\tisolationLevel = sql.LevelSerializable\n\t}\n\treturn ds.withTx(ctx, op, false, &sql.TxOptions{Isolation: isolationLevel})\n}\n\n// withWriteTx wraps the operation in a transaction appropriate for operations\n// that unconditionally create/update rows, without reading them first. If two\n// transactions try and update at the same time, last writer wins.\nfunc (ds *Plugin) withWriteTx(ctx context.Context, op func(tx *gorm.DB) error) error {\n\treturn ds.withTx(ctx, op, false, nil)\n}\n\n// withWriteTx wraps the operation in a transaction appropriate for operations\n// that only read rows.\nfunc (ds *Plugin) withReadTx(ctx context.Context, op func(tx *gorm.DB) error) error {\n\treturn ds.withTx(ctx, op, true, nil)\n}\n\nfunc (ds *Plugin) withTx(ctx context.Context, op func(tx *gorm.DB) error, readOnly bool, opts *sql.TxOptions) error {\n\tds.mu.Lock()\n\tdb := ds.db\n\tds.mu.Unlock()\n\n\tif db.databaseType == SQLite && !readOnly {\n\t\t// sqlite3 can only have one writer at a time. since we're in WAL mode,\n\t\t// there can be concurrent reads and writes, so no lock is necessary\n\t\t// over the read operations.\n\t\tdb.opMu.Lock()\n\t\tdefer db.opMu.Unlock()\n\t}\n\n\ttx := db.BeginTx(ctx, opts)\n\tif err := tx.Error; err != nil {\n\t\treturn sqlError.Wrap(err)\n\t}\n\n\tif err := op(tx); err != nil {\n\t\ttx.Rollback()\n\t\treturn ds.gormToGRPCStatus(err)\n\t}\n\n\tif readOnly {\n\t\t// rolling back makes sure that functions that are invoked with\n\t\t// withReadTx, and then do writes, will not pass unit tests, since the\n\t\t// writes won't be committed.\n\t\treturn sqlError.Wrap(tx.Rollback().Error)\n\t}\n\treturn sqlError.Wrap(tx.Commit().Error)\n}\n\n// gormToGRPCStatus takes an error, and converts it to a GRPC error.  If the\n// error is already a gRPC status , it will be returned unmodified. Otherwise\n// if the error is a gorm error type with a known mapping to a GRPC status,\n// that code will be set, otherwise the code will be set to Unknown.\nfunc (ds *Plugin) gormToGRPCStatus(err error) error {\n\tunwrapped := errs.Unwrap(err)\n\tif _, ok := status.FromError(unwrapped); ok {\n\t\treturn unwrapped\n\t}\n\n\tcode := codes.Unknown\n\tswitch {\n\tcase gorm.IsRecordNotFoundError(unwrapped):\n\t\tcode = codes.NotFound\n\tcase ds.db.dialect.isConstraintViolation(unwrapped):\n\t\tcode = codes.AlreadyExists\n\tdefault:\n\t}\n\n\treturn status.Error(code, err.Error())\n}\n\nfunc (ds *Plugin) openDB(cfg *configuration, isReadOnly bool) (*gorm.DB, string, bool, dialect, error) {\n\tvar dialect dialect\n\n\tds.log.Info(\"Opening SQL database\", telemetry.DatabaseType, cfg.DatabaseType)\n\tswitch cfg.DatabaseType {\n\tcase SQLite:\n\t\tdialect = sqliteDB{log: ds.log}\n\tcase PostgreSQL:\n\t\tdialect = postgresDB{}\n\tcase MySQL:\n\t\tdialect = mysqlDB{}\n\tdefault:\n\t\treturn nil, \"\", false, nil, sqlError.New(\"unsupported database_type: %v\", cfg.DatabaseType)\n\t}\n\n\tdb, version, supportsCTE, err := dialect.connect(cfg, isReadOnly)\n\tif err != nil {\n\t\treturn nil, \"\", false, nil, err\n\t}\n\n\tgormLogger := ds.log.Named(\"gorm\")\n\tgormLogger.SetLevel(hclog.Debug)\n\tdb.SetLogger(gormLogger.StandardLogger(&hclog.StandardLoggerOptions{\n\t\tInferLevels: true,\n\t}))\n\tif cfg.MaxOpenConns != nil {\n\t\tdb.DB().SetMaxOpenConns(*cfg.MaxOpenConns)\n\t}\n\tif cfg.MaxIdleConns != nil {\n\t\tdb.DB().SetMaxIdleConns(*cfg.MaxIdleConns)\n\t}\n\tif cfg.ConnMaxLifetime != nil {\n\t\tconnMaxLifetime, err := time.ParseDuration(*cfg.ConnMaxLifetime)\n\t\tif err != nil {\n\t\t\treturn nil, \"\", false, nil, fmt.Errorf(\"failed to parse conn_max_lifetime %q: %v\", *cfg.ConnMaxLifetime, err)\n\t\t}\n\t\tdb.DB().SetConnMaxLifetime(connMaxLifetime)\n\t}\n\n\tif !isReadOnly {\n\t\tif err := migrateDB(db, cfg.DatabaseType, cfg.DisableMigration, ds.log); err != nil {\n\t\t\tdb.Close()\n\t\t\treturn nil, \"\", false, nil, err\n\t\t}\n\t}\n\n\treturn db, version, supportsCTE, dialect, nil\n}\n\nfunc createBundle(tx *gorm.DB, req *datastore.CreateBundleRequest) (*datastore.CreateBundleResponse, error) {\n\tmodel, err := bundleToModel(req.Bundle)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := tx.Create(model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.CreateBundleResponse{\n\t\tBundle: req.Bundle,\n\t}, nil\n}\n\nfunc updateBundle(tx *gorm.DB, req *datastore.UpdateBundleRequest) (*datastore.UpdateBundleResponse, error) {\n\tnewBundle := req.Bundle\n\tnewModel, err := bundleToModel(newBundle)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tmodel := &Bundle{}\n\tif err := tx.Find(model, \"trust_domain = ?\", newModel.TrustDomain).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tmodel.Data, newBundle, err = applyBundleMask(model, newBundle, req.InputMask)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif err := tx.Save(model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.UpdateBundleResponse{\n\t\tBundle: newBundle,\n\t}, nil\n}\n\nfunc applyBundleMask(model *Bundle, newBundle *common.Bundle, inputMask *common.BundleMask) ([]byte, *common.Bundle, error) {\n\tbundle, err := modelToBundle(model)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tif inputMask == nil {\n\t\tinputMask = protoutil.AllTrueCommonBundleMask\n\t}\n\n\tif inputMask.RefreshHint {\n\t\tbundle.RefreshHint = newBundle.RefreshHint\n\t}\n\n\tif inputMask.RootCas {\n\t\tbundle.RootCas = newBundle.RootCas\n\t}\n\n\tif inputMask.JwtSigningKeys {\n\t\tbundle.JwtSigningKeys = newBundle.JwtSigningKeys\n\t}\n\n\tnewModel, err := bundleToModel(bundle)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn newModel.Data, bundle, nil\n}\n\nfunc setBundle(tx *gorm.DB, req *datastore.SetBundleRequest) (*datastore.SetBundleResponse, error) {\n\tnewModel, err := bundleToModel(req.Bundle)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// fetch existing or create new\n\tmodel := &Bundle{}\n\tresult := tx.Find(model, \"trust_domain = ?\", newModel.TrustDomain)\n\tif result.RecordNotFound() {\n\t\tresp, err := createBundle(tx, &datastore.CreateBundleRequest{Bundle: req.Bundle})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &datastore.SetBundleResponse{\n\t\t\tBundle: resp.Bundle,\n\t\t}, nil\n\t} else if result.Error != nil {\n\t\treturn nil, sqlError.Wrap(result.Error)\n\t}\n\n\tresp, err := updateBundle(tx, &datastore.UpdateBundleRequest{Bundle: req.Bundle})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &datastore.SetBundleResponse{\n\t\tBundle: resp.Bundle,\n\t}, nil\n}\n\nfunc appendBundle(tx *gorm.DB, req *datastore.AppendBundleRequest) (*datastore.AppendBundleResponse, error) {\n\tnewModel, err := bundleToModel(req.Bundle)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// fetch existing or create new\n\tmodel := &Bundle{}\n\tresult := tx.Find(model, \"trust_domain = ?\", newModel.TrustDomain)\n\tif result.RecordNotFound() {\n\t\tresp, err := createBundle(tx, &datastore.CreateBundleRequest{Bundle: req.Bundle})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn &datastore.AppendBundleResponse{\n\t\t\tBundle: resp.Bundle,\n\t\t}, nil\n\t} else if result.Error != nil {\n\t\treturn nil, sqlError.Wrap(result.Error)\n\t}\n\n\t// parse the bundle data and add missing elements\n\tbundle, err := modelToBundle(model)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbundle, changed := bundleutil.MergeBundles(bundle, req.Bundle)\n\tif changed {\n\t\tnewModel, err := bundleToModel(bundle)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmodel.Data = newModel.Data\n\t\tif err := tx.Save(model).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t}\n\n\treturn &datastore.AppendBundleResponse{\n\t\tBundle: bundle,\n\t}, nil\n}\n\nfunc deleteBundle(tx *gorm.DB, req *datastore.DeleteBundleRequest) (*datastore.DeleteBundleResponse, error) {\n\ttrustDomainID, err := idutil.NormalizeSpiffeID(req.TrustDomainId, idutil.AllowAnyTrustDomain())\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tmodel := new(Bundle)\n\tif err := tx.Find(model, \"trust_domain = ?\", trustDomainID).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\t// Get a count of associated registration entries\n\tentriesAssociation := tx.Model(model).Association(\"FederatedEntries\")\n\tentriesCount := entriesAssociation.Count()\n\tif err := entriesAssociation.Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif entriesCount > 0 {\n\t\tswitch req.Mode {\n\t\tcase datastore.DeleteBundleRequest_DELETE:\n\t\t\t// TODO: figure out how to do this gracefully with GORM.\n\t\t\tif err := tx.Exec(bindVars(tx, `DELETE FROM registered_entries WHERE id in (\n\t\t\t\tSELECT\n\t\t\t\t\tregistered_entry_id\n\t\t\t\tFROM\n\t\t\t\t\tfederated_registration_entries\n\t\t\t\tWHERE\n\t\t\t\t\tbundle_id = ?)`), model.ID).Error; err != nil {\n\t\t\t\treturn nil, sqlError.Wrap(err)\n\t\t\t}\n\t\tcase datastore.DeleteBundleRequest_DISSOCIATE:\n\t\t\tif err := entriesAssociation.Clear().Error; err != nil {\n\t\t\t\treturn nil, sqlError.Wrap(err)\n\t\t\t}\n\t\tdefault:\n\t\t\treturn nil, status.Newf(codes.FailedPrecondition, \"datastore-sql: cannot delete bundle; federated with %d registration entries\", entriesCount).Err()\n\t\t}\n\t}\n\n\tif err := tx.Delete(model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tbundle, err := modelToBundle(model)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &datastore.DeleteBundleResponse{\n\t\tBundle: bundle,\n\t}, nil\n}\n\n// FetchBundle returns the bundle matching the specified Trust Domain.\nfunc fetchBundle(tx *gorm.DB, req *datastore.FetchBundleRequest) (*datastore.FetchBundleResponse, error) {\n\ttrustDomainID, err := idutil.NormalizeSpiffeID(req.TrustDomainId, idutil.AllowAnyTrustDomain())\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tmodel := new(Bundle)\n\terr = tx.Find(model, \"trust_domain = ?\", trustDomainID).Error\n\tswitch {\n\tcase err == gorm.ErrRecordNotFound:\n\t\treturn &datastore.FetchBundleResponse{}, nil\n\tcase err != nil:\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tbundle, err := modelToBundle(model)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &datastore.FetchBundleResponse{\n\t\tBundle: bundle,\n\t}, nil\n}\n\n// countBundles can be used to count existing bundles\nfunc countBundles(tx *gorm.DB) (*datastore.CountBundlesResponse, error) {\n\ttx = tx.Model(&Bundle{})\n\n\tvar count int\n\tif err := tx.Count(&count).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tresp := &datastore.CountBundlesResponse{\n\t\tBundles: int32(count),\n\t}\n\treturn resp, nil\n}\n\n// listBundles can be used to fetch all existing bundles.\nfunc listBundles(tx *gorm.DB, req *datastore.ListBundlesRequest) (*datastore.ListBundlesResponse, error) {\n\tif req.Pagination != nil && req.Pagination.PageSize == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot paginate with pagesize = 0\")\n\t}\n\n\tp := req.Pagination\n\tvar err error\n\tif p != nil {\n\t\ttx, err = applyPagination(p, tx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar bundles []Bundle\n\tif err := tx.Find(&bundles).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif p != nil {\n\t\tp.Token = \"\"\n\t\t// Set token only if page size is the same than bundles len\n\t\tif len(bundles) > 0 {\n\t\t\tlastEntry := bundles[len(bundles)-1]\n\t\t\tp.Token = fmt.Sprint(lastEntry.ID)\n\t\t}\n\t}\n\n\tresp := &datastore.ListBundlesResponse{\n\t\tPagination: p,\n\t}\n\tfor _, model := range bundles {\n\t\tmodel := model // alias the loop variable since we pass it by reference below\n\t\tbundle, err := modelToBundle(&model)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tresp.Bundles = append(resp.Bundles, bundle)\n\t}\n\n\treturn resp, nil\n}\n\nfunc pruneBundle(tx *gorm.DB, req *datastore.PruneBundleRequest, log hclog.Logger) (*datastore.PruneBundleResponse, error) {\n\t// Get current bundle\n\tcurrent, err := fetchBundle(tx, &datastore.FetchBundleRequest{TrustDomainId: req.TrustDomainId})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to fetch current bundle: %v\", err)\n\t}\n\n\tif current.Bundle == nil {\n\t\t// No bundle to prune\n\t\treturn &datastore.PruneBundleResponse{}, nil\n\t}\n\n\t// Prune\n\tnewBundle, changed, err := bundleutil.PruneBundle(current.Bundle, time.Unix(req.ExpiresBefore, 0), log)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"prune failed: %v\", err)\n\t}\n\n\t// Update only if bundle was modified\n\tif changed {\n\t\t_, err := updateBundle(tx, &datastore.UpdateBundleRequest{\n\t\t\tBundle: newBundle,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"unable to write new bundle: %v\", err)\n\t\t}\n\t}\n\n\treturn &datastore.PruneBundleResponse{BundleChanged: changed}, nil\n}\n\nfunc createAttestedNode(tx *gorm.DB, req *datastore.CreateAttestedNodeRequest) (*datastore.CreateAttestedNodeResponse, error) {\n\tmodel := AttestedNode{\n\t\tSpiffeID:        req.Node.SpiffeId,\n\t\tDataType:        req.Node.AttestationDataType,\n\t\tSerialNumber:    req.Node.CertSerialNumber,\n\t\tExpiresAt:       time.Unix(req.Node.CertNotAfter, 0),\n\t\tNewSerialNumber: req.Node.NewCertSerialNumber,\n\t\tNewExpiresAt:    nullableUnixTimeToDBTime(req.Node.NewCertNotAfter),\n\t}\n\n\tif err := tx.Create(&model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.CreateAttestedNodeResponse{\n\t\tNode: modelToAttestedNode(model),\n\t}, nil\n}\n\nfunc fetchAttestedNode(tx *gorm.DB, req *datastore.FetchAttestedNodeRequest) (*datastore.FetchAttestedNodeResponse, error) {\n\tvar model AttestedNode\n\terr := tx.Find(&model, \"spiffe_id = ?\", req.SpiffeId).Error\n\tswitch {\n\tcase err == gorm.ErrRecordNotFound:\n\t\treturn &datastore.FetchAttestedNodeResponse{}, nil\n\tcase err != nil:\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\treturn &datastore.FetchAttestedNodeResponse{\n\t\tNode: modelToAttestedNode(model),\n\t}, nil\n}\n\nfunc countAttestedNodes(tx *gorm.DB) (*datastore.CountAttestedNodesResponse, error) {\n\tvar count int\n\tif err := tx.Model(&AttestedNode{}).Count(&count).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tresp := &datastore.CountAttestedNodesResponse{\n\t\tNodes: int32(count),\n\t}\n\treturn resp, nil\n}\n\nfunc listAttestedNodes(ctx context.Context, db *sqlDB, req *datastore.ListAttestedNodesRequest) (*datastore.ListAttestedNodesResponse, error) {\n\tif req.Pagination != nil && req.Pagination.PageSize == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot paginate with pagesize = 0\")\n\t}\n\tif req.BySelectorMatch != nil && len(req.BySelectorMatch.Selectors) == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot list by empty selectors set\")\n\t}\n\n\tfor {\n\t\tresp, err := listAttestedNodesOnce(ctx, db, req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif req.BySelectorMatch == nil || len(resp.Nodes) == 0 {\n\t\t\treturn resp, nil\n\t\t}\n\n\t\tresp.Nodes = filterNodesBySelectorSet(resp.Nodes, req.BySelectorMatch.Selectors)\n\n\t\tif len(resp.Nodes) > 0 || resp.Pagination == nil || len(resp.Pagination.Token) == 0 {\n\t\t\treturn resp, nil\n\t\t}\n\n\t\treq.Pagination = resp.Pagination\n\t}\n}\n\n// filterNodesBySelectorSet filters nodes based on provided selectors\nfunc filterNodesBySelectorSet(nodes []*common.AttestedNode, selectors []*common.Selector) []*common.AttestedNode {\n\ttype selectorKey struct {\n\t\tType  string\n\t\tValue string\n\t}\n\tset := make(map[selectorKey]struct{}, len(selectors))\n\tfor _, s := range selectors {\n\t\tset[selectorKey{Type: s.Type, Value: s.Value}] = struct{}{}\n\t}\n\n\tisSubset := func(ss []*common.Selector) bool {\n\t\tfor _, s := range ss {\n\t\t\tif _, ok := set[selectorKey{Type: s.Type, Value: s.Value}]; !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tfiltered := make([]*common.AttestedNode, 0, len(nodes))\n\tfor _, node := range nodes {\n\t\tif isSubset(node.Selectors) {\n\t\t\tfiltered = append(filtered, node)\n\t\t}\n\t}\n\n\treturn filtered\n}\n\nfunc listAttestedNodesOnce(ctx context.Context, db *sqlDB, req *datastore.ListAttestedNodesRequest) (*datastore.ListAttestedNodesResponse, error) {\n\tquery, args, err := buildListAttestedNodesQuery(db.databaseType, db.supportsCTE, req)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\trows, err := db.QueryContext(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tdefer rows.Close()\n\n\tvar nodes []*common.AttestedNode\n\tif req.Pagination != nil {\n\t\tnodes = make([]*common.AttestedNode, 0, req.Pagination.PageSize)\n\t} else {\n\t\tnodes = make([]*common.AttestedNode, 0, 64)\n\t}\n\n\tpushNode := func(node *common.AttestedNode) {\n\t\tif node != nil && node.SpiffeId != \"\" {\n\t\t\tnodes = append(nodes, node)\n\t\t}\n\t}\n\n\tvar lastEID uint64\n\tvar node *common.AttestedNode\n\tfor rows.Next() {\n\t\tvar r nodeRow\n\t\tif err := scanNodeRow(rows, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif node == nil || lastEID != r.EId {\n\t\t\tlastEID = r.EId\n\t\t\tpushNode(node)\n\t\t\tnode = new(common.AttestedNode)\n\t\t}\n\n\t\tif err := fillNodeFromRow(node, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tpushNode(node)\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tresp := &datastore.ListAttestedNodesResponse{\n\t\tNodes: nodes,\n\t}\n\n\tif req.Pagination != nil {\n\t\tresp.Pagination = &datastore.Pagination{\n\t\t\tPageSize: req.Pagination.PageSize,\n\t\t}\n\t\tif len(resp.Nodes) > 0 {\n\t\t\tresp.Pagination.Token = strconv.FormatUint(lastEID, 10)\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\nfunc buildListAttestedNodesQuery(dbType string, supportsCTE bool, req *datastore.ListAttestedNodesRequest) (string, []interface{}, error) {\n\tswitch dbType {\n\tcase SQLite:\n\t\treturn buildListAttestedNodesQueryCTE(req, dbType)\n\tcase PostgreSQL:\n\t\t// The PostgreSQL queries unconditionally leverage CTE since all versions\n\t\t// of PostgreSQL supported by the plugin support CTE.\n\t\tquery, args, err := buildListAttestedNodesQueryCTE(req, dbType)\n\t\tif err != nil {\n\t\t\treturn query, args, err\n\t\t}\n\t\treturn postgreSQLRebind(query), args, nil\n\tcase MySQL:\n\t\tif supportsCTE {\n\t\t\treturn buildListAttestedNodesQueryCTE(req, dbType)\n\t\t}\n\t\treturn buildListAttestedNodesQueryMySQL(req)\n\tdefault:\n\t\treturn \"\", nil, sqlError.New(\"unsupported db type: %q\", dbType)\n\t}\n}\n\nfunc buildListAttestedNodesQueryCTE(req *datastore.ListAttestedNodesRequest, dbType string) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\tvar args []interface{}\n\n\t// Selectors will be fetched only when `FetchSelectors` or BySelectorMatch are in request\n\tfetchSelectors := req.FetchSelectors || req.BySelectorMatch != nil\n\n\t// Creates filtered nodes, `true` is added to simplify code, all filters will start with `AND`\n\tbuilder.WriteString(\"\\nWITH filtered_nodes AS (\\n\")\n\tbuilder.WriteString(\"\\tSELECT * FROM attested_node_entries WHERE true\\n\")\n\n\t// Filter by pagination token\n\tif req.Pagination != nil && req.Pagination.Token != \"\" {\n\t\ttoken, err := strconv.ParseUint(req.Pagination.Token, 10, 32)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, status.Errorf(codes.InvalidArgument, \"could not parse token '%v'\", req.Pagination.Token)\n\t\t}\n\t\tbuilder.WriteString(\"\\t\\tAND id > ?\")\n\t\targs = append(args, token)\n\t}\n\n\t// Filter by expiration\n\tif req.ByExpiresBefore != nil {\n\t\tbuilder.WriteString(\"\\t\\tAND expires_at < ?\\n\")\n\t\targs = append(args, time.Unix(req.ByExpiresBefore.Value, 0))\n\t}\n\n\t// Filter by Attestation type\n\tif req.ByAttestationType != \"\" {\n\t\tbuilder.WriteString(\"\\t\\tAND data_type = ?\\n\")\n\t\targs = append(args, req.ByAttestationType)\n\t}\n\n\t// Filter by banned, an Attestation Node is banned when serial number is empty.\n\t// This filter allows 3 outputs:\n\t// - nil:  returns all\n\t// - true: returns banned entries\n\t// - false: returns no banned entries\n\tif req.ByBanned != nil {\n\t\tif req.ByBanned.Value {\n\t\t\tbuilder.WriteString(\"\\t\\tAND serial_number = ''\\n\")\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\t\\tAND serial_number <> ''\\n\")\n\t\t}\n\t}\n\n\tbuilder.WriteString(\")\")\n\t// Fetch all selectors from filtered entries\n\tif fetchSelectors {\n\t\tbuilder.WriteString(`, filtered_nodes_and_selectors AS (\n\t    SELECT\n\t        filtered_nodes.*, nr.type AS selector_type, nr.value AS selector_value\n\t    FROM\n\t\t\tfiltered_nodes\n\t    LEFT JOIN\n\t \t    node_resolver_map_entries nr       \n\t    ON\n\t        nr.spiffe_id=filtered_nodes.spiffe_id\n\t)\n`)\n\t}\n\n\t// Add expected fields\n\tbuilder.WriteString(`\nSELECT \n\tid as e_id,\n\tspiffe_id,\n\tdata_type,\n\tserial_number,\n\texpires_at,\n\tnew_serial_number,\n\tnew_expires_at,`)\n\n\t// Add \"optional\" fields for selectors\n\tif fetchSelectors {\n\t\tbuilder.WriteString(`\n\tselector_type,\n\tselector_value \n\t  `)\n\t} else {\n\t\tbuilder.WriteString(`\n\tNULL AS selector_type,\n\tNULL AS selector_value`)\n\t}\n\n\t// Choose what table will be used\n\tfromQuery := \"FROM filtered_nodes\"\n\tif fetchSelectors {\n\t\tfromQuery = \"FROM filtered_nodes_and_selectors\"\n\t}\n\n\tbuilder.WriteString(\"\\n\")\n\tbuilder.WriteString(fromQuery)\n\tbuilder.WriteString(\"\\nWHERE id IN (\\n\")\n\n\t// MySQL requires a subquery in order to apply pagination\n\tif req.Pagination != nil && dbType == MySQL {\n\t\tbuilder.WriteString(\"\\tSELECT id FROM (\\n\")\n\t}\n\n\t// Add filter by selectors\n\tif req.BySelectorMatch != nil && len(req.BySelectorMatch.Selectors) > 0 {\n\t\t// Select IDs, that will be used to fetch \"paged\" entrieSelect IDs, that will be used to fetch \"paged\" entries\n\t\tbuilder.WriteString(\"\\tSELECT DISTINCT id FROM (\\n\")\n\n\t\tquery := \"SELECT id FROM filtered_nodes_and_selectors WHERE selector_type = ? AND selector_value = ?\"\n\n\t\tswitch req.BySelectorMatch.Match {\n\t\tcase datastore.BySelectors_MATCH_SUBSET:\n\t\t\t// Subset needs a union, so we need to group them and add the group\n\t\t\t// as a child to the root\n\t\t\tfor i := range req.BySelectorMatch.Selectors {\n\t\t\t\tbuilder.WriteString(\"\\t\\t\")\n\t\t\t\tbuilder.WriteString(query)\n\t\t\t\tif i < (len(req.BySelectorMatch.Selectors) - 1) {\n\t\t\t\t\tbuilder.WriteString(\"\\n\\t\\tUNION\\n\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase datastore.BySelectors_MATCH_EXACT:\n\t\t\tfor i := range req.BySelectorMatch.Selectors {\n\t\t\t\tswitch dbType {\n\t\t\t\t// MySQL does not support INTERSECT, so use INNER JOIN instead\n\t\t\t\tcase MySQL:\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t(\")\n\t\t\t\t\tbuilder.WriteString(query)\n\t\t\t\t\tbuilder.WriteString(fmt.Sprintf(\") c_%d\\n\", i))\n\t\t\t\t\t// First subquery does not need USING(ID)\n\t\t\t\t\tif i > 0 {\n\t\t\t\t\t\tbuilder.WriteString(\"\\t\\tUSING(id)\\n\")\n\t\t\t\t\t}\n\t\t\t\t\t// Last query does not need INNER JOIN\n\t\t\t\t\tif i < (len(req.BySelectorMatch.Selectors) - 1) {\n\t\t\t\t\t\tbuilder.WriteString(\"\\t\\tINNER JOIN\\n\")\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t\")\n\t\t\t\t\tbuilder.WriteString(query)\n\t\t\t\t\tif i < (len(req.BySelectorMatch.Selectors) - 1) {\n\t\t\t\t\t\tbuilder.WriteString(\"\\n\\t\\tINTERSECT\\n\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\treturn \"\", nil, errs.New(\"unhandled match behavior %q\", req.BySelectorMatch.Match)\n\t\t}\n\n\t\t// Add all selectors as arguments\n\t\tfor _, selector := range req.BySelectorMatch.Selectors {\n\t\t\targs = append(args, selector.Type, selector.Value)\n\t\t}\n\n\t\tbuilder.WriteString(\"\\n\\t) \")\n\t} else {\n\t\t// Prevent duplicate IDs when fetching selectors\n\t\tif fetchSelectors {\n\t\t\tbuilder.WriteString(\"\\t\\tSELECT DISTINCT id \")\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\t\\tSELECT id \")\n\t\t}\n\t\tbuilder.WriteString(\"\\n\\t\\t\")\n\t\tbuilder.WriteString(fromQuery)\n\t}\n\n\tif dbType == PostgreSQL ||\n\t\t(req.BySelectorMatch != nil && req.BySelectorMatch.Match == datastore.BySelectors_MATCH_SUBSET) {\n\t\tbuilder.WriteString(\" AS result_nodes\")\n\t}\n\tif req.Pagination != nil {\n\t\tbuilder.WriteString(\" ORDER BY id ASC LIMIT \")\n\t\tbuilder.WriteString(strconv.FormatInt(int64(req.Pagination.PageSize), 10))\n\n\t\t// Add workaround for limit\n\t\tif dbType == MySQL {\n\t\t\tbuilder.WriteString(\"\\n\\t) workaround_for_mysql_subquery_limit\")\n\t\t}\n\t}\n\n\tbuilder.WriteString(\"\\n)\\n\")\n\n\treturn builder.String(), args, nil\n}\n\nfunc buildListAttestedNodesQueryMySQL(req *datastore.ListAttestedNodesRequest) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\tvar args []interface{}\n\n\t// Selectors will be fetched only when `FetchSelectors` or `BySelectorMatch` are in request\n\tfetchSelectors := req.FetchSelectors || req.BySelectorMatch != nil\n\n\t// Add expected fields\n\tbuilder.WriteString(`\nSELECT \n\tN.id as e_id,\n\tN.spiffe_id,\n\tN.data_type,\n\tN.serial_number,\n\tN.expires_at,\n\tN.new_serial_number,\n\tN.new_expires_at,`)\n\n\t// Add \"optional\" fields for selectors\n\tif fetchSelectors {\n\t\tbuilder.WriteString(`\n\tS.type AS selector_type,\n\tS.value AS selector_value \nFROM attested_node_entries N\nLEFT JOIN \n\tnode_resolver_map_entries S\nON\n\tN.spiffe_id = S.spiffe_id\n`)\n\t} else {\n\t\tbuilder.WriteString(`\n\tNULL AS selector_type,\n\tNULL AS selector_value\nFROM attested_node_entries N\n`)\n\t}\n\n\twriteFilter := func() error {\n\t\tbuilder.WriteString(\"WHERE true\")\n\n\t\t// Filter by paginatioin token\n\t\tif req.Pagination != nil && req.Pagination.Token != \"\" {\n\t\t\ttoken, err := strconv.ParseUint(req.Pagination.Token, 10, 32)\n\t\t\tif err != nil {\n\t\t\t\treturn status.Errorf(codes.InvalidArgument, \"could not parse token '%v'\", req.Pagination.Token)\n\t\t\t}\n\t\t\tbuilder.WriteString(\" AND N.id > ?\")\n\t\t\targs = append(args, token)\n\t\t}\n\n\t\t// Filter by expiration\n\t\tif req.ByExpiresBefore != nil {\n\t\t\tbuilder.WriteString(\" AND N.expires_at < ?\")\n\t\t\targs = append(args, time.Unix(req.ByExpiresBefore.Value, 0))\n\t\t}\n\n\t\t// Filter by Attestation type\n\t\tif req.ByAttestationType != \"\" {\n\t\t\tbuilder.WriteString(\" AND N.data_type = ?\")\n\t\t\targs = append(args, req.ByAttestationType)\n\t\t}\n\n\t\t// Filter by banned, an Attestation Node is banned when serial number is empty.\n\t\t// This filter allows 3 outputs:\n\t\t// - nil:  returns all\n\t\t// - true: returns banned entries\n\t\t// - false: returns no banned entries\n\t\tif req.ByBanned != nil {\n\t\t\tif req.ByBanned.Value {\n\t\t\t\tbuilder.WriteString(\" AND N.serial_number = ''\")\n\t\t\t} else {\n\t\t\t\tbuilder.WriteString(\" AND N.serial_number <> ''\")\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\t// Add filter by selectors\n\tif fetchSelectors {\n\t\tbuilder.WriteString(\"WHERE N.id IN (\\n\")\n\t\tif req.Pagination != nil {\n\t\t\tbuilder.WriteString(\"\\tSELECT id FROM (\\n\")\n\t\t}\n\t\tbuilder.WriteString(\"\\t\\tSELECT DISTINCT id FROM (\\n\")\n\n\t\tbuilder.WriteString(\"\\t\\t\\t(SELECT N.id, N.spiffe_id FROM attested_node_entries N \")\n\t\tif err := writeFilter(); err != nil {\n\t\t\treturn \"\", nil, err\n\t\t}\n\t\tbuilder.WriteString(\") c_0\\n\")\n\n\t\tif req.BySelectorMatch != nil && len(req.BySelectorMatch.Selectors) > 0 {\n\t\t\tquery := \"SELECT spiffe_id FROM node_resolver_map_entries WHERE type = ? AND value = ?\"\n\n\t\t\tswitch req.BySelectorMatch.Match {\n\t\t\tcase datastore.BySelectors_MATCH_SUBSET:\n\t\t\t\tbuilder.WriteString(\"\\t\\t\\tINNER JOIN\\n\")\n\t\t\t\tbuilder.WriteString(\"\\t\\t\\t(SELECT spiffe_id FROM (\\n\")\n\n\t\t\t\t// subset needs a union, so we need to group them and add the group\n\t\t\t\t// as a child to the root.\n\t\t\t\tfor i := range req.BySelectorMatch.Selectors {\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t\\t\\t\")\n\t\t\t\t\tbuilder.WriteString(query)\n\t\t\t\t\tif i < (len(req.BySelectorMatch.Selectors) - 1) {\n\t\t\t\t\t\tbuilder.WriteString(\"\\n\\t\\t\\t\\tUNION\\n\")\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tbuilder.WriteString(\"\\t\\t\\t) s_1) c_2\\n\")\n\t\t\t\tbuilder.WriteString(\"\\t\\t\\tUSING(spiffe_id)\\n\")\n\t\t\tcase datastore.BySelectors_MATCH_EXACT:\n\t\t\t\tfor i := range req.BySelectorMatch.Selectors {\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t\\tINNER JOIN\\n\")\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t\\t(\")\n\t\t\t\t\tbuilder.WriteString(query)\n\t\t\t\t\tbuilder.WriteString(fmt.Sprintf(\") c_%d\\n\", i+1))\n\t\t\t\t\tbuilder.WriteString(\"\\t\\t\\tUSING(spiffe_id)\\n\")\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn \"\", nil, errs.New(\"unhandled match behavior %q\", req.BySelectorMatch.Match)\n\t\t\t}\n\n\t\t\tfor _, selector := range req.BySelectorMatch.Selectors {\n\t\t\t\targs = append(args, selector.Type, selector.Value)\n\t\t\t}\n\t\t}\n\t\tif req.Pagination != nil {\n\t\t\tbuilder.WriteString(\"\\t\\t) ORDER BY id ASC LIMIT \")\n\t\t\tbuilder.WriteString(strconv.FormatInt(int64(req.Pagination.PageSize), 10))\n\t\t\tbuilder.WriteString(\"\\n\")\n\n\t\t\tbuilder.WriteString(\"\\t) workaround_for_mysql_subquery_limit\\n\")\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\t)\\n\")\n\t\t}\n\t\tbuilder.WriteString(\") ORDER BY e_id, S.id\\n\")\n\t} else {\n\t\tif err := writeFilter(); err != nil {\n\t\t\treturn \"\", nil, err\n\t\t}\n\t\tif req.Pagination != nil {\n\t\t\tbuilder.WriteString(\" ORDER BY N.id ASC LIMIT \")\n\t\t\tbuilder.WriteString(strconv.FormatInt(int64(req.Pagination.PageSize), 10))\n\t\t}\n\t\tbuilder.WriteString(\"\\n\")\n\t}\n\n\treturn builder.String(), args, nil\n}\n\nfunc updateAttestedNode(tx *gorm.DB, req *datastore.UpdateAttestedNodeRequest) (*datastore.UpdateAttestedNodeResponse, error) {\n\tvar model AttestedNode\n\tif err := tx.Find(&model, \"spiffe_id = ?\", req.SpiffeId).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif req.InputMask == nil {\n\t\treq.InputMask = protoutil.AllTrueCommonAgentMask\n\t}\n\n\tupdates := make(map[string]interface{})\n\tif req.InputMask.CertNotAfter {\n\t\tupdates[\"expires_at\"] = time.Unix(req.CertNotAfter, 0)\n\t}\n\tif req.InputMask.CertSerialNumber {\n\t\tupdates[\"serial_number\"] = req.CertSerialNumber\n\t}\n\tif req.InputMask.NewCertNotAfter {\n\t\tupdates[\"new_expires_at\"] = nullableUnixTimeToDBTime(req.NewCertNotAfter)\n\t}\n\tif req.InputMask.NewCertSerialNumber {\n\t\tupdates[\"new_serial_number\"] = req.NewCertSerialNumber\n\t}\n\n\tif err := tx.Model(&model).Updates(updates).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.UpdateAttestedNodeResponse{\n\t\tNode: modelToAttestedNode(model),\n\t}, nil\n}\n\nfunc deleteAttestedNode(tx *gorm.DB, req *datastore.DeleteAttestedNodeRequest) (*datastore.DeleteAttestedNodeResponse, error) {\n\tvar model AttestedNode\n\tif err := tx.Find(&model, \"spiffe_id = ?\", req.SpiffeId).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif err := tx.Delete(&model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.DeleteAttestedNodeResponse{\n\t\tNode: modelToAttestedNode(model),\n\t}, nil\n}\n\nfunc setNodeSelectors(tx *gorm.DB, req *datastore.SetNodeSelectorsRequest) (*datastore.SetNodeSelectorsResponse, error) {\n\t// Previously the deletion of the previous set of node selectors was\n\t// implemented via query like DELETE FROM node_resolver_map_entries WHERE\n\t// spiffe_id = ?, but unfortunately this triggered some pessimistic gap\n\t// locks on the index even when there were no rows matching the WHERE\n\t// clause (i.e. rows for that spiffe_id). The gap locks caused MySQL\n\t// deadlocks when SetNodeSelectors was being called concurrently. Changing\n\t// the transaction isolation level fixed the deadlocks but only when there\n\t// were no existing rows; the deadlocks still occurred when existing rows\n\t// existed (i.e. reattestation). Instead, gather all of the IDs to be\n\t// deleted and delete them from separate queries, which does not trigger\n\t// gap locks on the index.\n\tvar ids []int64\n\tif err := tx.Model(&NodeSelector{}).Where(\"spiffe_id = ?\", req.Selectors.SpiffeId).Pluck(\"id\", &ids).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tif len(ids) > 0 {\n\t\tif err := tx.Where(\"id IN (?)\", ids).Delete(&NodeSelector{}).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t}\n\n\tfor _, selector := range req.Selectors.Selectors {\n\t\tmodel := &NodeSelector{\n\t\t\tSpiffeID: req.Selectors.SpiffeId,\n\t\t\tType:     selector.Type,\n\t\t\tValue:    selector.Value,\n\t\t}\n\t\tif err := tx.Create(model).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t}\n\n\treturn &datastore.SetNodeSelectorsResponse{}, nil\n}\n\nfunc getNodeSelectors(ctx context.Context, db *sqlDB, req *datastore.GetNodeSelectorsRequest) (*datastore.GetNodeSelectorsResponse, error) {\n\tquery := maybeRebind(db.databaseType, \"SELECT type, value FROM node_resolver_map_entries WHERE spiffe_id=? ORDER BY id\")\n\trows, err := db.QueryContext(ctx, query, req.SpiffeId)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tdefer rows.Close()\n\n\tvar selectors []*common.Selector\n\tfor rows.Next() {\n\t\tselector := new(common.Selector)\n\t\tif err := rows.Scan(&selector.Type, &selector.Value); err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t\tselectors = append(selectors, selector)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.GetNodeSelectorsResponse{\n\t\tSelectors: &datastore.NodeSelectors{\n\t\t\tSpiffeId:  req.SpiffeId,\n\t\t\tSelectors: selectors,\n\t\t},\n\t}, nil\n}\n\nfunc listNodeSelectors(ctx context.Context, db *sqlDB, req *datastore.ListNodeSelectorsRequest) (*datastore.ListNodeSelectorsResponse, error) {\n\trawQuery, args := buildListNodeSelectorsQuery(req)\n\tquery := maybeRebind(db.databaseType, rawQuery)\n\trows, err := db.QueryContext(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tdefer rows.Close()\n\n\tresp := new(datastore.ListNodeSelectorsResponse)\n\n\tvar currentID string\n\tselectors := make([]*common.Selector, 0, 64)\n\n\tpush := func(spiffeID string, selector *common.Selector) {\n\t\tswitch {\n\t\tcase currentID == \"\":\n\t\t\tcurrentID = spiffeID\n\t\tcase spiffeID != currentID:\n\t\t\tresp.Selectors = append(resp.Selectors, &datastore.NodeSelectors{\n\t\t\t\tSpiffeId:  currentID,\n\t\t\t\tSelectors: selectors,\n\t\t\t})\n\t\t\tcurrentID = spiffeID\n\t\t\tselectors = nil\n\t\t}\n\t\tselectors = append(selectors, selector)\n\t}\n\n\tfor rows.Next() {\n\t\tvar nsRow nodeSelectorRow\n\t\tif err := scanNodeSelectorRow(rows, &nsRow); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar spiffeID string\n\t\tif nsRow.SpiffeID.Valid {\n\t\t\tspiffeID = nsRow.SpiffeID.String\n\t\t}\n\n\t\tselector := new(common.Selector)\n\t\tfillNodeSelectorFromRow(selector, &nsRow)\n\t\tpush(spiffeID, selector)\n\t}\n\n\tpush(\"\", nil)\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn resp, nil\n}\n\nfunc buildListNodeSelectorsQuery(req *datastore.ListNodeSelectorsRequest) (query string, args []interface{}) {\n\tvar sb strings.Builder\n\tsb.WriteString(\"SELECT nre.id, nre.spiffe_id, nre.type, nre.value FROM node_resolver_map_entries nre\")\n\tif req.ValidAt != nil {\n\t\tsb.WriteString(\" INNER JOIN attested_node_entries ane ON nre.spiffe_id=ane.spiffe_id WHERE ane.expires_at > ?\")\n\t\targs = append(args, time.Unix(req.ValidAt.Seconds, 0))\n\t}\n\n\tsb.WriteString(\" ORDER BY nre.id ASC\")\n\n\treturn sb.String(), args\n}\n\nfunc createRegistrationEntry(tx *gorm.DB, req *datastore.CreateRegistrationEntryRequest) (*datastore.CreateRegistrationEntryResponse, error) {\n\tentryID, err := newRegistrationEntryID()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnewRegisteredEntry := RegisteredEntry{\n\t\tEntryID:    entryID,\n\t\tSpiffeID:   req.Entry.SpiffeId,\n\t\tParentID:   req.Entry.ParentId,\n\t\tTTL:        req.Entry.Ttl,\n\t\tAdmin:      req.Entry.Admin,\n\t\tDownstream: req.Entry.Downstream,\n\t\tExpiry:     req.Entry.EntryExpiry,\n\t}\n\n\tif err := tx.Create(&newRegisteredEntry).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tfederatesWith, err := makeFederatesWith(tx, req.Entry.FederatesWith)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := tx.Model(&newRegisteredEntry).Association(\"FederatesWith\").Append(federatesWith).Error; err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, registeredSelector := range req.Entry.Selectors {\n\t\tnewSelector := Selector{\n\t\t\tRegisteredEntryID: newRegisteredEntry.ID,\n\t\t\tType:              registeredSelector.Type,\n\t\t\tValue:             registeredSelector.Value}\n\n\t\tif err := tx.Create(&newSelector).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t}\n\n\tfor _, registeredDNS := range req.Entry.DnsNames {\n\t\tnewDNS := DNSName{\n\t\t\tRegisteredEntryID: newRegisteredEntry.ID,\n\t\t\tValue:             registeredDNS,\n\t\t}\n\n\t\tif err := tx.Create(&newDNS).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\t}\n\n\tentry, err := modelToEntry(tx, newRegisteredEntry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &datastore.CreateRegistrationEntryResponse{\n\t\tEntry: entry,\n\t}, nil\n}\n\nfunc fetchRegistrationEntry(ctx context.Context, db *sqlDB, req *datastore.FetchRegistrationEntryRequest) (*datastore.FetchRegistrationEntryResponse, error) {\n\tquery, args, err := buildFetchRegistrationEntryQuery(db.databaseType, db.supportsCTE, req)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\trows, err := db.QueryContext(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tdefer rows.Close()\n\n\tvar entry *common.RegistrationEntry\n\tfor rows.Next() {\n\t\tvar r entryRow\n\t\tif err := scanEntryRow(rows, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif entry == nil {\n\t\t\tentry = new(common.RegistrationEntry)\n\t\t}\n\t\tif err := fillEntryFromRow(entry, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.FetchRegistrationEntryResponse{\n\t\tEntry: entry,\n\t}, nil\n}\n\nfunc buildFetchRegistrationEntryQuery(dbType string, supportsCTE bool, req *datastore.FetchRegistrationEntryRequest) (string, []interface{}, error) {\n\tswitch dbType {\n\tcase SQLite:\n\t\t// The SQLite3 queries unconditionally leverage CTE since the\n\t\t// embedded version of SQLite3 supports CTE.\n\t\treturn buildFetchRegistrationEntryQuerySQLite3(req)\n\tcase PostgreSQL:\n\t\t// The PostgreSQL queries unconditionally leverage CTE since all versions\n\t\t// of PostgreSQL supported by the plugin support CTE.\n\t\treturn buildFetchRegistrationEntryQueryPostgreSQL(req)\n\tcase MySQL:\n\t\tif supportsCTE {\n\t\t\treturn buildFetchRegistrationEntryQueryMySQLCTE(req)\n\t\t}\n\t\treturn buildFetchRegistrationEntryQueryMySQL(req)\n\tdefault:\n\t\treturn \"\", nil, sqlError.New(\"unsupported db type: %q\", dbType)\n\t}\n}\n\nfunc buildFetchRegistrationEntryQuerySQLite3(req *datastore.FetchRegistrationEntryRequest) (string, []interface{}, error) {\n\tconst query = `\nWITH listing AS (\n\tSELECT id FROM registered_entries WHERE entry_id = ?\n)\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\nWHERE id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\nWHERE\n\tF.registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nORDER BY selector_id, dns_name_id\n;`\n\treturn query, []interface{}{req.EntryId}, nil\n}\n\nfunc buildFetchRegistrationEntryQueryPostgreSQL(req *datastore.FetchRegistrationEntryRequest) (string, []interface{}, error) {\n\tconst query = `\nWITH listing AS (\n\tSELECT id FROM registered_entries WHERE entry_id = $1\n)\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL ::integer AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL ::integer AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\nWHERE id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\nWHERE\n\tF.registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nORDER BY selector_id, dns_name_id\n;`\n\treturn query, []interface{}{req.EntryId}, nil\n}\n\nfunc buildFetchRegistrationEntryQueryMySQL(req *datastore.FetchRegistrationEntryRequest) (string, []interface{}, error) {\n\tconst query = `\nSELECT\n\tE.id AS e_id,\n\tE.entry_id AS entry_id,\n\tE.spiffe_id,\n\tE.parent_id,\n\tE.ttl AS reg_ttl,\n\tE.admin,\n\tE.downstream,\n\tE.expiry,\n\tS.id AS selector_id,\n\tS.type AS selector_type,\n\tS.value AS selector_value,\n\tB.trust_domain,\n\tD.id AS dns_name_id,\n\tD.value AS dns_name,\n\tE.revision_number\nFROM\n\tregistered_entries E\nLEFT JOIN\n\t(SELECT 1 AS joinItem UNION SELECT 2 UNION SELECT 3) AS joinItems ON TRUE\nLEFT JOIN\n\tselectors S ON joinItem=1 AND E.id=S.registered_entry_id\nLEFT JOIN\n\tdns_names D ON joinItem=2 AND E.id=D.registered_entry_id\nLEFT JOIN\n\t(federated_registration_entries F INNER JOIN bundles B ON F.bundle_id=B.id) ON joinItem=3 AND E.id=F.registered_entry_id\nWHERE E.entry_id = ?\nORDER BY selector_id, dns_name_id\n;`\n\treturn query, []interface{}{req.EntryId}, nil\n}\n\nfunc buildFetchRegistrationEntryQueryMySQLCTE(req *datastore.FetchRegistrationEntryRequest) (string, []interface{}, error) {\n\tconst query = `\nWITH listing AS (\n\tSELECT id FROM registered_entries WHERE entry_id = ?\n)\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\nWHERE id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\nWHERE\n\tF.registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\nWHERE registered_entry_id IN (SELECT id FROM listing)\n\nORDER BY selector_id, dns_name_id\n;`\n\treturn query, []interface{}{req.EntryId}, nil\n}\n\nfunc countRegistrationEntries(tx *gorm.DB) (*datastore.CountRegistrationEntriesResponse, error) {\n\tvar count int\n\tif err := tx.Model(&RegisteredEntry{}).Count(&count).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tresp := &datastore.CountRegistrationEntriesResponse{\n\t\tEntries: int32(count),\n\t}\n\treturn resp, nil\n}\n\nfunc listRegistrationEntries(ctx context.Context, db *sqlDB, req *datastore.ListRegistrationEntriesRequest) (*datastore.ListRegistrationEntriesResponse, error) {\n\tif req.Pagination != nil && req.Pagination.PageSize == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot paginate with pagesize = 0\")\n\t}\n\tif req.BySelectors != nil && len(req.BySelectors.Selectors) == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot list by empty selector set\")\n\t}\n\n\t// Exact/subset selector matching requires filtering out all registration\n\t// entries returned by the query whose selectors are not fully represented\n\t// in the request selectors. For this reason, it's possible that a paged\n\t// query returns rows that are completely filtered out. If that happens,\n\t// keep querying until a page gets at least one result.\n\tfor {\n\t\tresp, err := listRegistrationEntriesOnce(ctx, db, req)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif req.BySelectors == nil || len(resp.Entries) == 0 {\n\t\t\treturn resp, nil\n\t\t}\n\n\t\tresp.Entries = filterEntriesBySelectorSet(resp.Entries, req.BySelectors.Selectors)\n\t\tif len(resp.Entries) > 0 || resp.Pagination == nil || len(resp.Pagination.Token) == 0 {\n\t\t\treturn resp, nil\n\t\t}\n\n\t\treq.Pagination = resp.Pagination\n\t}\n}\n\nfunc filterEntriesBySelectorSet(entries []*common.RegistrationEntry, selectors []*common.Selector) []*common.RegistrationEntry {\n\ttype selectorKey struct {\n\t\tType  string\n\t\tValue string\n\t}\n\tset := make(map[selectorKey]struct{}, len(selectors))\n\tfor _, s := range selectors {\n\t\tset[selectorKey{Type: s.Type, Value: s.Value}] = struct{}{}\n\t}\n\n\tisSubset := func(ss []*common.Selector) bool {\n\t\tfor _, s := range ss {\n\t\t\tif _, ok := set[selectorKey{Type: s.Type, Value: s.Value}]; !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tfiltered := make([]*common.RegistrationEntry, 0, len(entries))\n\tfor _, entry := range entries {\n\t\tif isSubset(entry.Selectors) {\n\t\t\tfiltered = append(filtered, entry)\n\t\t}\n\t}\n\treturn filtered\n}\n\nfunc listRegistrationEntriesOnce(ctx context.Context, db *sqlDB, req *datastore.ListRegistrationEntriesRequest) (*datastore.ListRegistrationEntriesResponse, error) {\n\tquery, args, err := buildListRegistrationEntriesQuery(db.databaseType, db.supportsCTE, req)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\trows, err := db.QueryContext(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tdefer rows.Close()\n\n\tvar entries []*common.RegistrationEntry\n\tif req.Pagination != nil {\n\t\tentries = make([]*common.RegistrationEntry, 0, req.Pagination.PageSize)\n\t} else {\n\t\t// start the slice off with a little capacity to avoid the first few\n\t\t// reallocations\n\t\tentries = make([]*common.RegistrationEntry, 0, 64)\n\t}\n\n\tpushEntry := func(entry *common.RegistrationEntry) {\n\t\t// Due to previous bugs (i.e. #1191), there can be cruft rows related\n\t\t// to a deleted registration entries that are fetched with the list\n\t\t// query. To avoid hydrating partial entries, append only entries that\n\t\t// have data from the registered_entries table (i.e. those with an\n\t\t// entry id).\n\t\tif entry != nil && entry.EntryId != \"\" {\n\t\t\tentries = append(entries, entry)\n\t\t}\n\t}\n\n\tvar lastEID uint64\n\tvar entry *common.RegistrationEntry\n\tfor rows.Next() {\n\t\tvar r entryRow\n\t\tif err := scanEntryRow(rows, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif entry == nil || lastEID != r.EId {\n\t\t\tlastEID = r.EId\n\t\t\tpushEntry(entry)\n\t\t\tentry = new(common.RegistrationEntry)\n\t\t}\n\n\t\tif err := fillEntryFromRow(entry, &r); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tpushEntry(entry)\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tresp := &datastore.ListRegistrationEntriesResponse{\n\t\tEntries: entries,\n\t}\n\n\tif req.Pagination != nil {\n\t\tresp.Pagination = &datastore.Pagination{\n\t\t\tPageSize: req.Pagination.PageSize,\n\t\t}\n\t\tif len(resp.Entries) > 0 {\n\t\t\tresp.Pagination.Token = strconv.FormatUint(lastEID, 10)\n\t\t}\n\t}\n\n\treturn resp, nil\n}\n\nfunc buildListRegistrationEntriesQuery(dbType string, supportsCTE bool, req *datastore.ListRegistrationEntriesRequest) (string, []interface{}, error) {\n\tswitch dbType {\n\tcase SQLite:\n\t\t// The SQLite3 queries unconditionally leverage CTE since the\n\t\t// embedded version of SQLite3 supports CTE.\n\t\treturn buildListRegistrationEntriesQuerySQLite3(req)\n\tcase PostgreSQL:\n\t\t// The PostgreSQL queries unconditionally leverage CTE since all versions\n\t\t// of PostgreSQL supported by the plugin support CTE.\n\t\treturn buildListRegistrationEntriesQueryPostgreSQL(req)\n\tcase MySQL:\n\t\tif supportsCTE {\n\t\t\treturn buildListRegistrationEntriesQueryMySQLCTE(req)\n\t\t}\n\t\treturn buildListRegistrationEntriesQueryMySQL(req)\n\tdefault:\n\t\treturn \"\", nil, sqlError.New(\"unsupported db type: %q\", dbType)\n\t}\n}\n\nfunc buildListRegistrationEntriesQuerySQLite3(req *datastore.ListRegistrationEntriesRequest) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\n\tfiltered, args, err := appendListRegistrationEntriesFilterQuery(\"\\nWITH listing AS (\\n\", builder, SQLite, req)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tif filtered {\n\t\tbuilder.WriteString(\")\")\n\t}\n\n\tbuilder.WriteString(`\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE\\n\\tF.registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nORDER BY e_id, selector_id, dns_name_id\n;`)\n\n\treturn builder.String(), args, nil\n}\n\nfunc buildListRegistrationEntriesQueryPostgreSQL(req *datastore.ListRegistrationEntriesRequest) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\n\tfiltered, args, err := appendListRegistrationEntriesFilterQuery(\"\\nWITH listing AS (\\n\", builder, PostgreSQL, req)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tif filtered {\n\t\tbuilder.WriteString(\")\")\n\t}\n\n\tbuilder.WriteString(`\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL ::integer AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL ::integer AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE\\n\\tF.registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nORDER BY e_id, selector_id, dns_name_id\n;`)\n\n\treturn postgreSQLRebind(builder.String()), args, nil\n}\n\nfunc maybeRebind(dbType, query string) string {\n\tif dbType == PostgreSQL {\n\t\treturn postgreSQLRebind(query)\n\t}\n\treturn query\n}\n\nfunc postgreSQLRebind(s string) string {\n\treturn bindVarsFn(func(n int) string {\n\t\treturn \"$\" + strconv.Itoa(n)\n\t}, s)\n}\n\nfunc buildListRegistrationEntriesQueryMySQL(req *datastore.ListRegistrationEntriesRequest) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\tbuilder.WriteString(`\nSELECT\n\tE.id AS e_id,\n\tE.entry_id AS entry_id,\n\tE.spiffe_id,\n\tE.parent_id,\n\tE.ttl AS reg_ttl,\n\tE.admin,\n\tE.downstream,\n\tE.expiry,\n\tS.id AS selector_id,\n\tS.type AS selector_type,\n\tS.value AS selector_value,\n\tB.trust_domain,\n\tD.id AS dns_name_id,\n\tD.value AS dns_name,\n\tE.revision_number\nFROM\n\tregistered_entries E\nLEFT JOIN\n\t(SELECT 1 AS joinItem UNION SELECT 2 UNION SELECT 3) AS joinItems ON TRUE\nLEFT JOIN\n\tselectors S ON joinItem=1 AND E.id=S.registered_entry_id\nLEFT JOIN\n\tdns_names D ON joinItem=2 AND E.id=D.registered_entry_id\nLEFT JOIN\n\t(federated_registration_entries F INNER JOIN bundles B ON F.bundle_id=B.id) ON joinItem=3 AND E.id=F.registered_entry_id\n`)\n\n\tfiltered, args, err := appendListRegistrationEntriesFilterQuery(\"WHERE E.id IN (\\n\", builder, MySQL, req)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\n\tif filtered {\n\t\tbuilder.WriteString(\")\")\n\t}\n\n\tbuilder.WriteString(\"\\nORDER BY e_id, selector_id, dns_name_id\\n;\")\n\n\treturn builder.String(), args, nil\n}\n\nfunc buildListRegistrationEntriesQueryMySQLCTE(req *datastore.ListRegistrationEntriesRequest) (string, []interface{}, error) {\n\tbuilder := new(strings.Builder)\n\n\tfiltered, args, err := appendListRegistrationEntriesFilterQuery(\"\\nWITH listing AS (\\n\", builder, MySQL, req)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\tif filtered {\n\t\tbuilder.WriteString(\")\")\n\t}\n\n\tbuilder.WriteString(`\nSELECT\n\tid as e_id,\n\tentry_id,\n\tspiffe_id,\n\tparent_id,\n\tttl AS reg_ttl,\n\tadmin,\n\tdownstream,\n\texpiry,\n\tNULL AS selector_id,\n\tNULL AS selector_type,\n\tNULL AS selector_value,\n\tNULL AS trust_domain,\n\tNULL AS dns_name_id,\n\tNULL AS dns_name,\n\trevision_number\nFROM\n\tregistered_entries\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tF.registered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, B.trust_domain, NULL, NULL, NULL\nFROM\n\tbundles B\nINNER JOIN\n\tfederated_registration_entries F\nON\n\tB.id = F.bundle_id\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE\\n\\tF.registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, value, NULL\nFROM\n\tdns_names\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nUNION\n\nSELECT\n\tregistered_entry_id, NULL, NULL, NULL, NULL, NULL, NULL, NULL, id, type, value, NULL, NULL, NULL, NULL\nFROM\n\tselectors\n`)\n\tif filtered {\n\t\tbuilder.WriteString(\"WHERE registered_entry_id IN (SELECT id FROM listing)\\n\")\n\t}\n\tbuilder.WriteString(`\nORDER BY e_id, selector_id, dns_name_id\n;`)\n\n\treturn builder.String(), args, nil\n}\n\ntype idFilterNode struct {\n\t// mutually exclusive with children\n\t// supports multiline query\n\tquery []string\n\n\t// mutually exclusive with query\n\tchildren []idFilterNode\n\tunion    bool\n\tname     string\n\n\tfixed bool\n}\n\nfunc (n idFilterNode) Render(builder *strings.Builder, dbType string, indentation int, eol bool) {\n\tn.render(builder, dbType, 0, indentation, true, eol)\n}\n\nfunc (n idFilterNode) render(builder *strings.Builder, dbType string, sibling int, indentation int, bol, eol bool) {\n\tif len(n.query) > 0 {\n\t\tif bol {\n\t\t\tindent(builder, indentation)\n\t\t}\n\t\tfor idx, str := range n.query {\n\t\t\tif idx > 0 {\n\t\t\t\tindent(builder, indentation)\n\t\t\t}\n\t\t\tbuilder.WriteString(str)\n\t\t\tif idx+1 < len(n.query) {\n\t\t\t\tbuilder.WriteString(\"\\n\")\n\t\t\t}\n\t\t}\n\t\tif eol {\n\t\t\tbuilder.WriteString(\"\\n\")\n\t\t}\n\t\treturn\n\t}\n\n\tif !n.fixed && len(n.children) == 1 {\n\t\tn.children[0].render(builder, dbType, sibling, indentation, bol, eol)\n\t\treturn\n\t}\n\n\tif bol {\n\t\tindent(builder, indentation)\n\t}\n\tneedsName := true\n\tswitch {\n\tcase n.union:\n\t\tbuilder.WriteString(\"SELECT id FROM (\\n\")\n\t\tfor i, child := range n.children {\n\t\t\tif i > 0 {\n\t\t\t\tindent(builder, indentation+1)\n\t\t\t\tbuilder.WriteString(\"UNION\\n\")\n\t\t\t}\n\t\t\tchild.render(builder, dbType, i, indentation+1, true, true)\n\t\t}\n\tcase dbType != MySQL:\n\t\tbuilder.WriteString(\"SELECT id FROM (\\n\")\n\t\tfor i, child := range n.children {\n\t\t\tif i > 0 {\n\t\t\t\tindent(builder, indentation+1)\n\t\t\t\tbuilder.WriteString(\"INTERSECT\\n\")\n\t\t\t}\n\t\t\tchild.render(builder, dbType, i, indentation+1, true, true)\n\t\t}\n\tdefault:\n\t\tneedsName = false\n\t\tbuilder.WriteString(\"SELECT DISTINCT id FROM (\\n\")\n\t\tfor i, child := range n.children {\n\t\t\tif i > 0 {\n\t\t\t\tindent(builder, indentation+1)\n\t\t\t\tbuilder.WriteString(\"INNER JOIN\\n\")\n\t\t\t}\n\t\t\tindent(builder, indentation+1)\n\t\t\tbuilder.WriteString(\"(\")\n\n\t\t\tchild.render(builder, dbType, i, indentation+1, false, false)\n\t\t\tbuilder.WriteString(\") c_\")\n\t\t\tbuilder.WriteString(strconv.Itoa(i))\n\t\t\tbuilder.WriteString(\"\\n\")\n\t\t\tif i > 0 {\n\t\t\t\tindent(builder, indentation+1)\n\t\t\t\tbuilder.WriteString(\"USING(id)\\n\")\n\t\t\t}\n\t\t}\n\t}\n\tindent(builder, indentation)\n\tbuilder.WriteString(\")\")\n\tif n.name != \"\" {\n\t\tbuilder.WriteString(\" \")\n\t\tbuilder.WriteString(n.name)\n\t} else if needsName {\n\t\tbuilder.WriteString(\" s_\")\n\t\tbuilder.WriteString(strconv.Itoa(sibling))\n\t}\n\tif eol {\n\t\tbuilder.WriteString(\"\\n\")\n\t}\n}\n\nfunc indent(builder *strings.Builder, indentation int) {\n\tswitch indentation {\n\tcase 0:\n\tcase 1:\n\t\tbuilder.WriteString(\"\\t\")\n\tcase 2:\n\t\tbuilder.WriteString(\"\\t\\t\")\n\tcase 3:\n\t\tbuilder.WriteString(\"\\t\\t\\t\")\n\tcase 4:\n\t\tbuilder.WriteString(\"\\t\\t\\t\\t\")\n\tcase 5:\n\t\tbuilder.WriteString(\"\\t\\t\\t\\t\\t\")\n\tdefault:\n\t\tfor i := 0; i < indentation; i++ {\n\t\t\tbuilder.WriteString(\"\\t\")\n\t\t}\n\t}\n}\n\nfunc appendListRegistrationEntriesFilterQuery(filterExp string, builder *strings.Builder, dbType string, req *datastore.ListRegistrationEntriesRequest) (bool, []interface{}, error) {\n\tvar args []interface{}\n\n\tvar root idFilterNode\n\n\tif req.ByParentId != nil || req.BySpiffeId != nil {\n\t\tsubquery := new(strings.Builder)\n\t\tsubquery.WriteString(\"SELECT id FROM registered_entries WHERE \")\n\t\tif req.ByParentId != nil {\n\t\t\tsubquery.WriteString(\"parent_id = ?\")\n\t\t\targs = append(args, req.ByParentId.Value)\n\t\t}\n\t\tif req.BySpiffeId != nil {\n\t\t\tif req.ByParentId != nil {\n\t\t\t\tsubquery.WriteString(\" AND \")\n\t\t\t}\n\t\t\tsubquery.WriteString(\"spiffe_id = ?\")\n\t\t\targs = append(args, req.BySpiffeId.Value)\n\t\t}\n\t\troot.children = append(root.children, idFilterNode{\n\t\t\tquery: []string{subquery.String()},\n\t\t})\n\t}\n\n\tif req.BySelectors != nil && len(req.BySelectors.Selectors) > 0 {\n\t\tswitch req.BySelectors.Match {\n\t\tcase datastore.BySelectors_MATCH_SUBSET:\n\t\t\t// subset needs a union, so we need to group them and add the group\n\t\t\t// as a child to the root.\n\t\t\tgroup := idFilterNode{\n\t\t\t\tunion: true,\n\t\t\t}\n\t\t\tfor range req.BySelectors.Selectors {\n\t\t\t\tgroup.children = append(group.children, idFilterNode{\n\t\t\t\t\tquery: []string{\"SELECT registered_entry_id AS id FROM selectors WHERE type = ? AND value = ?\"},\n\t\t\t\t})\n\t\t\t}\n\t\t\troot.children = append(root.children, group)\n\t\tcase datastore.BySelectors_MATCH_EXACT:\n\t\t\t// exact match does uses an intersection, so we can just add these\n\t\t\t// directly to the root idFilterNode, since it is already an intersection\n\t\t\tfor range req.BySelectors.Selectors {\n\t\t\t\troot.children = append(root.children, idFilterNode{\n\t\t\t\t\tquery: []string{\"SELECT registered_entry_id AS id FROM selectors WHERE type = ? AND value = ?\"},\n\t\t\t\t})\n\t\t\t}\n\t\tdefault:\n\t\t\treturn false, nil, errs.New(\"unhandled selectors match behavior %q\", req.BySelectors.Match)\n\t\t}\n\t\tfor _, selector := range req.BySelectors.Selectors {\n\t\t\targs = append(args, selector.Type, selector.Value)\n\t\t}\n\t}\n\n\tif req.ByFederatesWith != nil && len(req.ByFederatesWith.TrustDomains) > 0 {\n\t\t// Take the trust domains from the request without duplicates\n\t\ttdSet := make(map[string]struct{})\n\t\tfor _, td := range req.ByFederatesWith.TrustDomains {\n\t\t\ttdSet[td] = struct{}{}\n\t\t}\n\t\ttrustDomains := make([]string, 0, len(tdSet))\n\t\tfor td := range tdSet {\n\t\t\ttrustDomains = append(trustDomains, td)\n\t\t}\n\n\t\t// Exact/subset federates-with matching requires filtering out all registration\n\t\t// entries whose federated trust domains are not fully represented in the request\n\t\tfilterNode := idFilterNode{}\n\t\tfilterNode.query = append(filterNode.query, \"SELECT E.id\")\n\t\tfilterNode.query = append(filterNode.query, \"FROM registered_entries E\")\n\t\tfilterNode.query = append(filterNode.query, \"INNER JOIN federated_registration_entries FE ON FE.registered_entry_id = E.id\")\n\t\tfilterNode.query = append(filterNode.query, \"INNER JOIN bundles B ON B.id = FE.bundle_id\")\n\t\tfilterNode.query = append(filterNode.query, \"GROUP BY E.id\")\n\t\tfilterNode.query = append(filterNode.query, \"HAVING\")\n\t\tsliceArg := buildSliceArg(len(trustDomains))\n\t\tfilterNode.query = append(filterNode.query, \"\\tCOUNT(CASE WHEN B.trust_domain NOT IN \"+sliceArg+\" THEN B.trust_domain ELSE NULL END) = 0 AND\")\n\t\tfor _, td := range trustDomains {\n\t\t\targs = append(args, td)\n\t\t}\n\n\t\tswitch req.ByFederatesWith.Match {\n\t\tcase datastore.ByFederatesWith_MATCH_SUBSET:\n\t\t\t// Subset federates-with matching requires filtering out all registration\n\t\t\t// entries that don't federate with even one trust domain in the request\n\t\t\tsliceArg := buildSliceArg(len(trustDomains))\n\t\t\tfilterNode.query = append(filterNode.query, \"\\tCOUNT(CASE WHEN B.trust_domain IN \"+sliceArg+\" THEN B.trust_domain ELSE NULL END) > 0\")\n\t\t\tfor _, td := range trustDomains {\n\t\t\t\targs = append(args, td)\n\t\t\t}\n\t\tcase datastore.ByFederatesWith_MATCH_EXACT:\n\t\t\t// Exact federates-with matching requires filtering out all registration\n\t\t\t// entries that don't federate with all the trust domains in the request\n\t\t\tsliceArg := buildSliceArg(len(trustDomains))\n\t\t\tfilterNode.query = append(filterNode.query, \"\\tCOUNT(DISTINCT CASE WHEN B.trust_domain IN \"+sliceArg+\" THEN B.trust_domain ELSE NULL END) = ?\")\n\t\t\tfor _, td := range trustDomains {\n\t\t\t\targs = append(args, td)\n\t\t\t}\n\t\t\targs = append(args, len(trustDomains))\n\t\tdefault:\n\t\t\treturn false, nil, errs.New(\"unhandled federates with match behavior %q\", req.ByFederatesWith.Match)\n\t\t}\n\t\troot.children = append(root.children, filterNode)\n\t}\n\n\tfiltered := false\n\tfilter := func() {\n\t\tif !filtered {\n\t\t\tbuilder.WriteString(filterExp)\n\t\t}\n\t\tfiltered = true\n\t}\n\n\tindentation := 1\n\tif req.Pagination != nil && dbType == MySQL {\n\t\tfilter()\n\t\tbuilder.WriteString(\"\\tSELECT id FROM (\\n\")\n\t\tindentation = 2\n\t}\n\n\tif len(root.children) > 0 {\n\t\tfilter()\n\t\troot.Render(builder, dbType, indentation, req.Pagination == nil)\n\t}\n\n\tif req.Pagination != nil {\n\t\tfilter()\n\t\tif len(root.children) == 0 {\n\t\t\tindent(builder, indentation)\n\t\t\tbuilder.WriteString(\"SELECT id FROM registered_entries\")\n\t\t}\n\t\tif len(req.Pagination.Token) > 0 {\n\t\t\ttoken, err := strconv.ParseUint(req.Pagination.Token, 10, 32)\n\t\t\tif err != nil {\n\t\t\t\treturn false, nil, status.Errorf(codes.InvalidArgument, \"could not parse token '%v'\", req.Pagination.Token)\n\t\t\t}\n\t\t\tif len(root.children) == 1 {\n\t\t\t\tbuilder.WriteString(\" AND id > ?\")\n\t\t\t} else {\n\t\t\t\tbuilder.WriteString(\" WHERE id > ?\")\n\t\t\t}\n\t\t\targs = append(args, token)\n\t\t}\n\t\tbuilder.WriteString(\" ORDER BY id ASC LIMIT \")\n\t\tbuilder.WriteString(strconv.FormatInt(int64(req.Pagination.PageSize), 10))\n\t\tbuilder.WriteString(\"\\n\")\n\n\t\tif dbType == MySQL {\n\t\t\tbuilder.WriteString(\"\\t) workaround_for_mysql_subquery_limit\\n\")\n\t\t}\n\t}\n\n\treturn filtered, args, nil\n}\n\nfunc buildSliceArg(length int) string {\n\tstrBuilder := new(strings.Builder)\n\tstrBuilder.WriteString(\"(?\")\n\tfor i := 1; i < length; i++ {\n\t\tstrBuilder.WriteString(\", ?\")\n\t}\n\tstrBuilder.WriteString(\")\")\n\treturn strBuilder.String()\n}\n\ntype nodeRow struct {\n\tEId             uint64\n\tSpiffeID        string\n\tDataType        sql.NullString\n\tSerialNumber    sql.NullString\n\tExpiresAt       sql.NullTime\n\tNewSerialNumber sql.NullString\n\tNewExpiresAt    sql.NullTime\n\tSelectorType    sql.NullString\n\tSelectorValue   sql.NullString\n}\n\nfunc scanNodeRow(rs *sql.Rows, r *nodeRow) error {\n\treturn sqlError.Wrap(rs.Scan(\n\t\t&r.EId,\n\t\t&r.SpiffeID,\n\t\t&r.DataType,\n\t\t&r.SerialNumber,\n\t\t&r.ExpiresAt,\n\t\t&r.NewSerialNumber,\n\t\t&r.NewExpiresAt,\n\t\t&r.SelectorType,\n\t\t&r.SelectorValue,\n\t))\n}\n\nfunc fillNodeFromRow(node *common.AttestedNode, r *nodeRow) error {\n\tif r.SpiffeID != \"\" {\n\t\tnode.SpiffeId = r.SpiffeID\n\t}\n\n\tif r.DataType.Valid {\n\t\tnode.AttestationDataType = r.DataType.String\n\t}\n\n\tif r.SerialNumber.Valid {\n\t\tnode.CertSerialNumber = r.SerialNumber.String\n\t}\n\n\tif r.ExpiresAt.Valid {\n\t\tnode.CertNotAfter = r.ExpiresAt.Time.Unix()\n\t}\n\n\tif r.NewExpiresAt.Valid {\n\t\tnode.NewCertNotAfter = r.NewExpiresAt.Time.Unix()\n\t}\n\n\tif r.NewSerialNumber.Valid {\n\t\tnode.NewCertSerialNumber = r.NewSerialNumber.String\n\t}\n\n\tif r.SelectorType.Valid {\n\t\tif !r.SelectorValue.Valid {\n\t\t\treturn sqlError.New(\"expected non-nil selector.value value for attested node %s\", node.SpiffeId)\n\t\t}\n\t\tnode.Selectors = append(node.Selectors, &common.Selector{\n\t\t\tType:  r.SelectorType.String,\n\t\t\tValue: r.SelectorValue.String,\n\t\t})\n\t}\n\n\treturn nil\n}\n\ntype nodeSelectorRow struct {\n\tEId      uint64\n\tSpiffeID sql.NullString\n\tType     sql.NullString\n\tValue    sql.NullString\n}\n\nfunc scanNodeSelectorRow(rs *sql.Rows, r *nodeSelectorRow) error {\n\treturn sqlError.Wrap(rs.Scan(\n\t\t&r.EId,\n\t\t&r.SpiffeID,\n\t\t&r.Type,\n\t\t&r.Value,\n\t))\n}\n\nfunc fillNodeSelectorFromRow(nodeSelector *common.Selector, r *nodeSelectorRow) {\n\tif r.Type.Valid {\n\t\tnodeSelector.Type = r.Type.String\n\t}\n\n\tif r.Value.Valid {\n\t\tnodeSelector.Value = r.Value.String\n\t}\n}\n\ntype entryRow struct {\n\tEId            uint64\n\tEntryID        sql.NullString\n\tSpiffeID       sql.NullString\n\tParentID       sql.NullString\n\tRegTTL         sql.NullInt64\n\tAdmin          sql.NullBool\n\tDownstream     sql.NullBool\n\tExpiry         sql.NullInt64\n\tSelectorID     sql.NullInt64\n\tSelectorType   sql.NullString\n\tSelectorValue  sql.NullString\n\tTrustDomain    sql.NullString\n\tDNSNameID      sql.NullInt64\n\tDNSName        sql.NullString\n\tRevisionNumber sql.NullInt64\n}\n\nfunc scanEntryRow(rs *sql.Rows, r *entryRow) error {\n\treturn sqlError.Wrap(rs.Scan(\n\t\t&r.EId,\n\t\t&r.EntryID,\n\t\t&r.SpiffeID,\n\t\t&r.ParentID,\n\t\t&r.RegTTL,\n\t\t&r.Admin,\n\t\t&r.Downstream,\n\t\t&r.Expiry,\n\t\t&r.SelectorID,\n\t\t&r.SelectorType,\n\t\t&r.SelectorValue,\n\t\t&r.TrustDomain,\n\t\t&r.DNSNameID,\n\t\t&r.DNSName,\n\t\t&r.RevisionNumber,\n\t))\n}\n\nfunc fillEntryFromRow(entry *common.RegistrationEntry, r *entryRow) error {\n\tif r.EntryID.Valid {\n\t\tentry.EntryId = r.EntryID.String\n\t}\n\tif r.SpiffeID.Valid {\n\t\tentry.SpiffeId = r.SpiffeID.String\n\t}\n\tif r.ParentID.Valid {\n\t\tentry.ParentId = r.ParentID.String\n\t}\n\tif r.RegTTL.Valid {\n\t\tentry.Ttl = int32(r.RegTTL.Int64)\n\t}\n\tif r.Admin.Valid {\n\t\tentry.Admin = r.Admin.Bool\n\t}\n\tif r.Downstream.Valid {\n\t\tentry.Downstream = r.Downstream.Bool\n\t}\n\tif r.Expiry.Valid {\n\t\tentry.EntryExpiry = r.Expiry.Int64\n\t}\n\tif r.RevisionNumber.Valid {\n\t\tentry.RevisionNumber = r.RevisionNumber.Int64\n\t}\n\n\tif r.SelectorType.Valid {\n\t\tif !r.SelectorValue.Valid {\n\t\t\treturn sqlError.New(\"expected non-nil selector.value value for entry id %s\", entry.EntryId)\n\t\t}\n\t\tentry.Selectors = append(entry.Selectors, &common.Selector{\n\t\t\tType:  r.SelectorType.String,\n\t\t\tValue: r.SelectorValue.String,\n\t\t})\n\t}\n\n\tif r.DNSName.Valid {\n\t\tentry.DnsNames = append(entry.DnsNames, r.DNSName.String)\n\t}\n\n\tif r.TrustDomain.Valid {\n\t\tentry.FederatesWith = append(entry.FederatesWith, r.TrustDomain.String)\n\t}\n\treturn nil\n}\n\n// applyPagination  add order limit and token to current query\nfunc applyPagination(p *datastore.Pagination, entryTx *gorm.DB) (*gorm.DB, error) {\n\tif p.PageSize == 0 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot paginate with pagesize = 0\")\n\t}\n\tentryTx = entryTx.Order(\"id asc\").Limit(p.PageSize)\n\n\tif len(p.Token) > 0 {\n\t\tid, err := strconv.ParseUint(p.Token, 10, 32)\n\t\tif err != nil {\n\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"could not parse token '%v'\", p.Token)\n\t\t}\n\t\tentryTx = entryTx.Where(\"id > ?\", id)\n\t}\n\treturn entryTx, nil\n}\n\nfunc updateRegistrationEntry(tx *gorm.DB,\n\treq *datastore.UpdateRegistrationEntryRequest) (*datastore.UpdateRegistrationEntryResponse, error) {\n\tif err := validateRegistrationEntryForUpdate(req.Entry, req.Mask); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the existing entry\n\tentry := RegisteredEntry{}\n\tif err := tx.Find(&entry, \"entry_id = ?\", req.Entry.EntryId).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\tif req.Mask == nil || req.Mask.Selectors {\n\t\t// Delete existing selectors - we will write new ones\n\t\tif err := tx.Exec(\"DELETE FROM selectors WHERE registered_entry_id = ?\", entry.ID).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\n\t\tselectors := []Selector{}\n\t\tfor _, s := range req.Entry.Selectors {\n\t\t\tselector := Selector{\n\t\t\t\tType:  s.Type,\n\t\t\t\tValue: s.Value,\n\t\t\t}\n\n\t\t\tselectors = append(selectors, selector)\n\t\t}\n\t\tentry.Selectors = selectors\n\t}\n\n\tif req.Mask == nil || req.Mask.DnsNames {\n\t\t// Delete existing DNSs - we will write new ones\n\t\tif err := tx.Exec(\"DELETE FROM dns_names WHERE registered_entry_id = ?\", entry.ID).Error; err != nil {\n\t\t\treturn nil, sqlError.Wrap(err)\n\t\t}\n\n\t\tdnsList := []DNSName{}\n\t\tfor _, d := range req.Entry.DnsNames {\n\t\t\tdns := DNSName{\n\t\t\t\tValue: d,\n\t\t\t}\n\n\t\t\tdnsList = append(dnsList, dns)\n\t\t}\n\t\tentry.DNSList = dnsList\n\t}\n\n\tif req.Mask == nil || req.Mask.SpiffeId {\n\t\tentry.SpiffeID = req.Entry.SpiffeId\n\t}\n\tif req.Mask == nil || req.Mask.ParentId {\n\t\tentry.ParentID = req.Entry.ParentId\n\t}\n\tif req.Mask == nil || req.Mask.Ttl {\n\t\tentry.TTL = req.Entry.Ttl\n\t}\n\tif req.Mask == nil || req.Mask.Admin {\n\t\tentry.Admin = req.Entry.Admin\n\t}\n\tif req.Mask == nil || req.Mask.Downstream {\n\t\tentry.Downstream = req.Entry.Downstream\n\t}\n\tif req.Mask == nil || req.Mask.EntryExpiry {\n\t\tentry.Expiry = req.Entry.EntryExpiry\n\t}\n\n\t// Revision number is increased by 1 on every update call\n\tentry.RevisionNumber++\n\n\tif err := tx.Save(&entry).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif req.Mask == nil || req.Mask.FederatesWith {\n\t\tfederatesWith, err := makeFederatesWith(tx, req.Entry.FederatesWith)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif err := tx.Model(&entry).Association(\"FederatesWith\").Replace(federatesWith).Error; err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// The FederatesWith field in entry is filled in by the call to modelToEntry below\n\t}\n\n\treturnEntry, err := modelToEntry(tx, entry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &datastore.UpdateRegistrationEntryResponse{\n\t\tEntry: returnEntry,\n\t}, nil\n}\n\nfunc deleteRegistrationEntry(tx *gorm.DB, req *datastore.DeleteRegistrationEntryRequest) (*datastore.DeleteRegistrationEntryResponse, error) {\n\tentry := RegisteredEntry{}\n\tif err := tx.Find(&entry, \"entry_id = ?\", req.EntryId).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\trespEntry, err := modelToEntry(tx, entry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = deleteRegistrationEntrySupport(tx, entry)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &datastore.DeleteRegistrationEntryResponse{\n\t\tEntry: respEntry,\n\t}, nil\n}\n\nfunc deleteRegistrationEntrySupport(tx *gorm.DB, entry RegisteredEntry) error {\n\tif err := tx.Model(&entry).Association(\"FederatesWith\").Clear().Error; err != nil {\n\t\treturn err\n\t}\n\n\tif err := tx.Delete(&entry).Error; err != nil {\n\t\treturn sqlError.Wrap(err)\n\t}\n\n\t// Delete existing selectors\n\tif err := tx.Exec(\"DELETE FROM selectors WHERE registered_entry_id = ?\", entry.ID).Error; err != nil {\n\t\treturn sqlError.Wrap(err)\n\t}\n\n\treturn nil\n}\n\nfunc pruneRegistrationEntries(tx *gorm.DB, req *datastore.PruneRegistrationEntriesRequest) (*datastore.PruneRegistrationEntriesResponse, error) {\n\tvar registrationEntries []RegisteredEntry\n\tif err := tx.Where(\"expiry != 0\").Where(\"expiry < ?\", req.ExpiresBefore).Find(&registrationEntries).Error; err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor _, entry := range registrationEntries {\n\t\tif err := deleteRegistrationEntrySupport(tx, entry); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn &datastore.PruneRegistrationEntriesResponse{}, nil\n}\n\nfunc createJoinToken(tx *gorm.DB, req *datastore.CreateJoinTokenRequest) (*datastore.CreateJoinTokenResponse, error) {\n\tt := JoinToken{\n\t\tToken:  req.JoinToken.Token,\n\t\tExpiry: req.JoinToken.Expiry,\n\t}\n\n\tif err := tx.Create(&t).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.CreateJoinTokenResponse{\n\t\tJoinToken: req.JoinToken,\n\t}, nil\n}\n\nfunc fetchJoinToken(tx *gorm.DB, req *datastore.FetchJoinTokenRequest) (*datastore.FetchJoinTokenResponse, error) {\n\tvar model JoinToken\n\terr := tx.Find(&model, \"token = ?\", req.Token).Error\n\tif err == gorm.ErrRecordNotFound {\n\t\treturn &datastore.FetchJoinTokenResponse{}, nil\n\t} else if err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.FetchJoinTokenResponse{\n\t\tJoinToken: modelToJoinToken(model),\n\t}, nil\n}\n\nfunc deleteJoinToken(tx *gorm.DB, req *datastore.DeleteJoinTokenRequest) (*datastore.DeleteJoinTokenResponse, error) {\n\tvar model JoinToken\n\tif err := tx.Find(&model, \"token = ?\", req.Token).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tif err := tx.Delete(&model).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.DeleteJoinTokenResponse{\n\t\tJoinToken: modelToJoinToken(model),\n\t}, nil\n}\n\nfunc pruneJoinTokens(tx *gorm.DB, req *datastore.PruneJoinTokensRequest) (*datastore.PruneJoinTokensResponse, error) {\n\tif err := tx.Where(\"expiry < ?\", req.ExpiresBefore).Delete(&JoinToken{}).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &datastore.PruneJoinTokensResponse{}, nil\n}\n\n// modelToBundle converts the given bundle model to a Protobuf bundle message. It will also\n// include any embedded CACert models.\nfunc modelToBundle(model *Bundle) (*common.Bundle, error) {\n\tbundle := new(common.Bundle)\n\tif err := proto.Unmarshal(model.Data, bundle); err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn bundle, nil\n}\n\nfunc validateRegistrationEntry(entry *common.RegistrationEntry) error {\n\tif entry == nil {\n\t\treturn sqlError.New(\"invalid request: missing registered entry\")\n\t}\n\n\tif entry.Selectors == nil || len(entry.Selectors) == 0 {\n\t\treturn sqlError.New(\"invalid registration entry: missing selector list\")\n\t}\n\n\tif len(entry.SpiffeId) == 0 {\n\t\treturn sqlError.New(\"invalid registration entry: missing SPIFFE ID\")\n\t}\n\n\tif entry.Ttl < 0 {\n\t\treturn sqlError.New(\"invalid registration entry: TTL is not set\")\n\t}\n\n\treturn nil\n}\n\nfunc validateRegistrationEntryForUpdate(entry *common.RegistrationEntry, mask *common.RegistrationEntryMask) error {\n\tif entry == nil {\n\t\treturn sqlError.New(\"invalid request: missing registered entry\")\n\t}\n\n\tif (mask == nil || mask.Selectors) &&\n\t\t(entry.Selectors == nil || len(entry.Selectors) == 0) {\n\t\treturn sqlError.New(\"invalid registration entry: missing selector list\")\n\t}\n\n\tif (mask == nil || mask.SpiffeId) &&\n\t\tentry.SpiffeId == \"\" {\n\t\treturn sqlError.New(\"invalid registration entry: missing SPIFFE ID\")\n\t}\n\n\tif (mask == nil || mask.Ttl) &&\n\t\t(entry.Ttl < 0) {\n\t\treturn sqlError.New(\"invalid registration entry: TTL is not set\")\n\t}\n\n\treturn nil\n}\n\n// bundleToModel converts the given Protobuf bundle message to a database model. It\n// performs validation, and fully parses certificates to form CACert embedded models.\nfunc bundleToModel(pb *common.Bundle) (*Bundle, error) {\n\tif pb == nil {\n\t\treturn nil, sqlError.New(\"missing bundle in request\")\n\t}\n\tid, err := idutil.NormalizeSpiffeID(pb.TrustDomainId, idutil.AllowAnyTrustDomain())\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tdata, err := proto.Marshal(pb)\n\tif err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\treturn &Bundle{\n\t\tTrustDomain: id,\n\t\tData:        data,\n\t}, nil\n}\n\nfunc modelToEntry(tx *gorm.DB, model RegisteredEntry) (*common.RegistrationEntry, error) {\n\tvar fetchedSelectors []*Selector\n\tif err := tx.Model(&model).Related(&fetchedSelectors).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tselectors := make([]*common.Selector, 0, len(fetchedSelectors))\n\tfor _, selector := range fetchedSelectors {\n\t\tselectors = append(selectors, &common.Selector{\n\t\t\tType:  selector.Type,\n\t\t\tValue: selector.Value,\n\t\t})\n\t}\n\n\tvar fetchedDNSs []*DNSName\n\tif err := tx.Model(&model).Related(&fetchedDNSs).Order(\"registered_entry_id ASC\").Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tvar dnsList []string\n\tif len(fetchedDNSs) > 0 {\n\t\tdnsList = make([]string, 0, len(fetchedDNSs))\n\t\tfor _, fetchedDNS := range fetchedDNSs {\n\t\t\tdnsList = append(dnsList, fetchedDNS.Value)\n\t\t}\n\t}\n\n\tvar fetchedBundles []*Bundle\n\tif err := tx.Model(&model).Association(\"FederatesWith\").Find(&fetchedBundles).Error; err != nil {\n\t\treturn nil, sqlError.Wrap(err)\n\t}\n\n\tvar federatesWith []string\n\tfor _, bundle := range fetchedBundles {\n\t\tfederatesWith = append(federatesWith, bundle.TrustDomain)\n\t}\n\n\treturn &common.RegistrationEntry{\n\t\tEntryId:        model.EntryID,\n\t\tSelectors:      selectors,\n\t\tSpiffeId:       model.SpiffeID,\n\t\tParentId:       model.ParentID,\n\t\tTtl:            model.TTL,\n\t\tFederatesWith:  federatesWith,\n\t\tAdmin:          model.Admin,\n\t\tDownstream:     model.Downstream,\n\t\tEntryExpiry:    model.Expiry,\n\t\tDnsNames:       dnsList,\n\t\tRevisionNumber: model.RevisionNumber,\n\t}, nil\n}\n\nfunc newRegistrationEntryID() (string, error) {\n\tu, err := uuid.NewV4()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn u.String(), nil\n}\n\nfunc modelToAttestedNode(model AttestedNode) *common.AttestedNode {\n\treturn &common.AttestedNode{\n\t\tSpiffeId:            model.SpiffeID,\n\t\tAttestationDataType: model.DataType,\n\t\tCertSerialNumber:    model.SerialNumber,\n\t\tCertNotAfter:        model.ExpiresAt.Unix(),\n\t\tNewCertSerialNumber: model.NewSerialNumber,\n\t\tNewCertNotAfter:     nullableDBTimeToUnixTime(model.NewExpiresAt),\n\t}\n}\n\nfunc modelToJoinToken(model JoinToken) *datastore.JoinToken {\n\treturn &datastore.JoinToken{\n\t\tToken:  model.Token,\n\t\tExpiry: model.Expiry,\n\t}\n}\n\nfunc makeFederatesWith(tx *gorm.DB, ids []string) ([]*Bundle, error) {\n\tvar bundles []*Bundle\n\tif err := tx.Where(\"trust_domain in (?)\", ids).Find(&bundles).Error; err != nil {\n\t\treturn nil, err\n\t}\n\n\t// make sure all of the ids were found\n\tidset := make(map[string]bool)\n\tfor _, bundle := range bundles {\n\t\tidset[bundle.TrustDomain] = true\n\t}\n\n\tfor _, id := range ids {\n\t\tif !idset[id] {\n\t\t\treturn nil, fmt.Errorf(\"unable to find federated bundle %q\", id)\n\t\t}\n\t}\n\n\treturn bundles, nil\n}\n\nfunc bindVars(db *gorm.DB, query string) string {\n\tdialect := db.Dialect()\n\tif dialect.BindVar(1) == \"?\" {\n\t\treturn query\n\t}\n\n\treturn bindVarsFn(dialect.BindVar, query)\n}\n\nfunc bindVarsFn(fn func(int) string, query string) string {\n\tvar buf bytes.Buffer\n\tvar n int\n\tfor i := strings.Index(query, \"?\"); i != -1; i = strings.Index(query, \"?\") {\n\t\tn++\n\t\tbuf.WriteString(query[:i])\n\t\tbuf.WriteString(fn(n))\n\t\tquery = query[i+1:]\n\t}\n\tbuf.WriteString(query)\n\treturn buf.String()\n}\n\nfunc (cfg *configuration) Validate() error {\n\tif cfg.DatabaseType == \"\" {\n\t\treturn errors.New(\"database_type must be set\")\n\t}\n\n\tif cfg.ConnectionString == \"\" {\n\t\treturn errors.New(\"connection_string must be set\")\n\t}\n\n\tif cfg.DatabaseType == MySQL {\n\t\tif err := validateMySQLConfig(cfg, false); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif cfg.RoConnectionString != \"\" {\n\t\t\tif err := validateMySQLConfig(cfg, true); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// getConnectionString returns the connection string corresponding to the database connection.\nfunc getConnectionString(cfg *configuration, isReadOnly bool) string {\n\tconnectionString := cfg.ConnectionString\n\tif isReadOnly {\n\t\tconnectionString = cfg.RoConnectionString\n\t}\n\treturn connectionString\n}\n\nfunc queryVersion(gormDB *gorm.DB, query string) (string, error) {\n\tdb := gormDB.DB()\n\tif db == nil {\n\t\treturn \"\", sqlError.New(\"unable to get raw database object\")\n\t}\n\n\tvar version string\n\tif err := db.QueryRow(query).Scan(&version); err != nil {\n\t\treturn \"\", sqlError.Wrap(err)\n\t}\n\treturn version, nil\n}\n\nfunc nullableDBTimeToUnixTime(dbTime *time.Time) int64 {\n\tif dbTime == nil {\n\t\treturn 0\n\t}\n\treturn dbTime.Unix()\n}\n\nfunc nullableUnixTimeToDBTime(unixTime int64) *time.Time {\n\tif unixTime == 0 {\n\t\treturn nil\n\t}\n\tdbTime := time.Unix(unixTime, 0)\n\treturn &dbTime\n}\n", "idx": 2, "id": 16301, "msg": "", "proj": "spiffe-spire", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -13,9 +13,6 @@ const (\n \tGPIO_OUTPUT\n )\n \n-// LED on the Arduino\n-const LED = 13\n-\n func (p GPIO) Configure(config GPIOConfig) {\n \tif config.Mode == GPIO_OUTPUT { // set output bit\n \t\tif p.Pin < 8 {", "y": 0, "oldf": "// +build avr\n\npackage machine\n\nimport (\n\t\"device/avr\"\n)\n\ntype GPIOMode uint8\n\nconst (\n\tGPIO_INPUT = iota\n\tGPIO_OUTPUT\n)\n\n// LED on the Arduino\nconst LED = 13\n\nfunc (p GPIO) Configure(config GPIOConfig) {\n\tif config.Mode == GPIO_OUTPUT { // set output bit\n\t\tif p.Pin < 8 {\n\t\t\t*avr.DDRD |= 1 << p.Pin\n\t\t} else {\n\t\t\t*avr.DDRB |= 1 << (p.Pin - 8)\n\t\t}\n\t} else { // configure input: clear output bit\n\t\tif p.Pin < 8 {\n\t\t\t*avr.DDRD &^= 1 << p.Pin\n\t\t} else {\n\t\t\t*avr.DDRB &^= 1 << (p.Pin - 8)\n\t\t}\n\t}\n}\n\nfunc (p GPIO) Set(value bool) {\n\tif value { // set bits\n\t\tif p.Pin < 8 {\n\t\t\t*avr.PORTD |= 1 << p.Pin\n\t\t} else {\n\t\t\t*avr.PORTB |= 1 << (p.Pin - 8)\n\t\t}\n\t} else { // clear bits\n\t\tif p.Pin < 8 {\n\t\t\t*avr.PORTD &^= 1 << p.Pin\n\t\t} else {\n\t\t\t*avr.PORTB &^= 1 << (p.Pin - 8)\n\t\t}\n\t}\n}\n\n// Get returns the current value of a GPIO pin.\nfunc (p GPIO) Get() bool {\n\tif p.Pin < 8 {\n\t\tval := *avr.PIND & (1 << p.Pin)\n\t\treturn (val > 0)\n\t} else {\n\t\tval := *avr.PINB & (1 << (p.Pin - 8))\n\t\treturn (val > 0)\n\t}\n}\n\n// InitPWM initializes the registers needed for PWM.\nfunc InitPWM() {\n\t// use waveform generation\n\t*avr.TCCR0A |= avr.TCCR0A_WGM00\n\n\t// set timer 0 prescale factor to 64\n\t*avr.TCCR0B |= avr.TCCR0B_CS01 | avr.TCCR0B_CS00\n\n\t// set timer 1 prescale factor to 64\n\t*avr.TCCR1B |= avr.TCCR1B_CS11\n\n\t// put timer 1 in 8-bit phase correct pwm mode\n\t*avr.TCCR1A |= avr.TCCR1A_WGM10\n\n\t// set timer 2 prescale factor to 64\n\t*avr.TCCR2B |= avr.TCCR2B_CS22\n\n\t// configure timer 2 for phase correct pwm (8-bit)\n\t*avr.TCCR2A |= avr.TCCR2A_WGM20\n}\n\n// Configure configures a PWM pin for output.\nfunc (pwm PWM) Configure() {\n\tif pwm.Pin < 8 {\n\t\t*avr.DDRD |= 1 << pwm.Pin\n\t} else {\n\t\t*avr.DDRB |= 1 << (pwm.Pin - 8)\n\t}\n}\n\n// Set turns on the duty cycle for a PWM pin using the provided value. On the AVR this is normally a\n// 8-bit value ranging from 0 to 255.\nfunc (pwm PWM) Set(value uint16) {\n\tvalue8 := value >> 8\n\tswitch pwm.Pin {\n\tcase 3:\n\t\t// connect pwm to pin on timer 2, channel B\n\t\t*avr.TCCR2A |= avr.TCCR2A_COM2B1\n\t\t*avr.OCR2B = avr.RegValue(value8) // set pwm duty\n\tcase 5:\n\t\t// connect pwm to pin on timer 0, channel B\n\t\t*avr.TCCR0A |= avr.TCCR0A_COM0B1\n\t\t*avr.OCR0B = avr.RegValue(value8) // set pwm duty\n\tcase 6:\n\t\t// connect pwm to pin on timer 0, channel A\n\t\t*avr.TCCR0A |= avr.TCCR0A_COM0A1\n\t\t*avr.OCR0A = avr.RegValue(value8) // set pwm duty\n\tcase 9:\n\t\t// connect pwm to pin on timer 1, channel A\n\t\t*avr.TCCR1A |= avr.TCCR1A_COM1A1\n\t\t// this is a 16-bit value, but we only currently allow the low order bits to be set\n\t\t*avr.OCR1AL = avr.RegValue(value8) // set pwm duty\n\tcase 10:\n\t\t// connect pwm to pin on timer 1, channel B\n\t\t*avr.TCCR1A |= avr.TCCR1A_COM1B1\n\t\t// this is a 16-bit value, but we only currently allow the low order bits to be set\n\t\t*avr.OCR1BL = avr.RegValue(value8) // set pwm duty\n\tcase 11:\n\t\t// connect pwm to pin on timer 2, channel A\n\t\t*avr.TCCR2A |= avr.TCCR2A_COM2A1\n\t\t*avr.OCR2A = avr.RegValue(value8) // set pwm duty\n\tdefault:\n\t\tpanic(\"Invalid PWM pin\")\n\t}\n}\n\n// ADC on the Arduino\nconst (\n\tADC0 = 0\n\tADC1 = 1\n\tADC2 = 2\n\tADC3 = 3\n\tADC4 = 4\n\tADC5 = 5\n)\n\n// InitADC initializes the registers needed for ADC.\nfunc InitADC() {\n\t// set a2d prescaler so we are inside the desired 50-200 KHz range at 16MHz.\n\t*avr.ADCSRA |= (avr.ADCSRA_ADPS2 | avr.ADCSRA_ADPS1 | avr.ADCSRA_ADPS0)\n\n\t// enable a2d conversions\n\t*avr.ADCSRA |= avr.ADCSRA_ADEN\n}\n\n// Configure configures a ADCPin to be able to be used to read data.\nfunc (a ADC) Configure() {\n\treturn // no pin specific setup on AVR machine.\n}\n\n// Get returns the current value of a ADC pin, in the range 0..0xffff. The AVR\n// has an ADC of 10 bits precision so the lower 6 bits will be zero.\nfunc (a ADC) Get() uint16 {\n\t// set the analog reference (high two bits of ADMUX) and select the\n\t// channel (low 4 bits), masked to only turn on one ADC at a time.\n\t// set the ADLAR bit (left-adjusted result) to get a value scaled to 16\n\t// bits. This has the same effect as shifting the return value left by 6\n\t// bits.\n\t*avr.ADMUX = avr.RegValue(avr.ADMUX_REFS0 | avr.ADMUX_ADLAR | (a.Pin & 0x07))\n\n\t// start the conversion\n\t*avr.ADCSRA |= avr.ADCSRA_ADSC\n\n\t// ADSC is cleared when the conversion finishes\n\tfor ok := true; ok; ok = (*avr.ADCSRA & avr.ADCSRA_ADSC) > 0 {\n\t}\n\n\tlow := uint16(*avr.ADCL)\n\thigh := uint16(*avr.ADCH)\n\treturn uint16(low) | uint16(high<<8)\n}\n", "idx": 1, "id": 5787, "msg": "", "proj": "tinygo-org-tinygo", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -2775,6 +2775,7 @@ public class DBService implements RolesProvider {\n \n                 StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                 auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n+                        .append(\"\\\", \\\"version\\\": \\\"\").append(versionForAuditLog)\n                         .append(\"\\\", \\\"assertionId\\\": \\\"\").append(assertionId)\n                         .append(\"\\\", \\\"deleted-assertions\\\": [\");\n                 auditLogAssertion(auditDetails, assertion, true);", "y": 0, "oldf": "/*\n * Copyright 2016 Yahoo Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.yahoo.athenz.zms;\n\nimport com.google.common.cache.Cache;\nimport com.google.common.cache.CacheBuilder;\nimport com.yahoo.athenz.auth.AuthorityConsts;\nimport com.yahoo.athenz.auth.Principal;\nimport com.yahoo.athenz.auth.impl.SimplePrincipal;\nimport com.yahoo.athenz.auth.util.AthenzUtils;\nimport com.yahoo.athenz.auth.util.StringUtils;\nimport com.yahoo.athenz.common.server.audit.AuditReferenceValidator;\nimport com.yahoo.athenz.common.server.db.RolesProvider;\nimport com.yahoo.athenz.common.server.log.AuditLogMsgBuilder;\nimport com.yahoo.athenz.common.server.log.AuditLogger;\nimport com.yahoo.athenz.common.server.util.AuthzHelper;\nimport com.yahoo.athenz.common.server.util.ResourceUtils;\nimport com.yahoo.athenz.zms.config.MemberDueDays;\nimport com.yahoo.athenz.zms.store.AthenzDomain;\nimport com.yahoo.athenz.zms.store.ObjectStore;\nimport com.yahoo.athenz.zms.store.ObjectStoreConnection;\nimport com.yahoo.athenz.zms.utils.ZMSUtils;\nimport com.yahoo.rdl.JSON;\nimport com.yahoo.rdl.Timestamp;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.*;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.BiConsumer;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\n\npublic class DBService implements RolesProvider {\n\n    ObjectStore store;\n    BitSet auditRefSet;\n    AuditLogger auditLogger;\n    Cache<String, DataCache> cacheStore;\n    QuotaChecker quotaCheck;\n    int retrySleepTime;\n    int defaultRetryCount;\n    int defaultOpTimeout;\n    ZMSConfig zmsConfig;\n\n    private static final Logger LOG = LoggerFactory.getLogger(DBService.class);\n\n    public static int AUDIT_TYPE_ROLE     = 0;\n    public static int AUDIT_TYPE_POLICY   = 1;\n    public static int AUDIT_TYPE_SERVICE  = 2;\n    public static int AUDIT_TYPE_DOMAIN   = 3;\n    public static int AUDIT_TYPE_ENTITY   = 4;\n    public static int AUDIT_TYPE_TENANCY  = 5;\n    public static int AUDIT_TYPE_TEMPLATE = 6;\n    public static int AUDIT_TYPE_GROUP    = 7;\n\n    private static final String ROLE_PREFIX = \"role.\";\n    private static final String POLICY_PREFIX = \"policy.\";\n    private static final String TEMPLATE_DOMAIN_NAME = \"_domain_\";\n    private static final String AUDIT_REF = \"Athenz User Authority Enforcer\";\n\n    AuditReferenceValidator auditReferenceValidator;\n    private ScheduledExecutorService userAuthorityFilterExecutor;\n\n    public DBService(ObjectStore store, AuditLogger auditLogger, ZMSConfig zmsConfig, AuditReferenceValidator auditReferenceValidator) {\n\n        this.store = store;\n        this.zmsConfig = zmsConfig;\n        this.auditLogger = auditLogger;\n        cacheStore = CacheBuilder.newBuilder().concurrencyLevel(25).build();\n\n        // default timeout in seconds for object store commands\n\n        defaultOpTimeout = Integer.parseInt(System.getProperty(ZMSConsts.ZMS_PROP_STORE_OP_TIMEOUT, \"60\"));\n        if (defaultOpTimeout < 0) {\n            defaultOpTimeout = 60;\n        }\n        int roleTagsLimit = Integer.getInteger(ZMSConsts.ZMS_PROP_QUOTA_ROLE_TAG, ZMSConsts.ZMS_DEFAULT_TAG_LIMIT);\n        int domainTagsLimit = Integer.getInteger(ZMSConsts.ZMS_PROP_QUOTA_DOMAIN_TAG, ZMSConsts.ZMS_DEFAULT_TAG_LIMIT);\n        if (this.store != null) {\n            this.store.setOperationTimeout(defaultOpTimeout);\n            this.store.setTagLimit(domainTagsLimit, roleTagsLimit);\n        }\n\n        // retrieve the concurrent update retry count. If we're given an invalid negative\n        // value for count, we'll default back to our default configured value of 120 retries\n        // which would result up to 30 seconds sleeping 250ms each time\n\n        defaultRetryCount = Integer.parseInt(System.getProperty(ZMSConsts.ZMS_PROP_CONFLICT_RETRY_COUNT, \"120\"));\n        if (defaultRetryCount < 0) {\n            defaultRetryCount = 120;\n        }\n\n        retrySleepTime = Integer.parseInt(System.getProperty(ZMSConsts.ZMS_PROP_CONFLICT_RETRY_SLEEP_TIME, \"250\"));\n        if (retrySleepTime < 0) {\n            retrySleepTime = 250;\n        }\n\n        // check what objects we're going to enforce audit reference flag\n\n        setAuditRefObjectBits();\n        this.auditReferenceValidator = auditReferenceValidator;\n\n        // create our quota checker class\n\n        quotaCheck = new QuotaChecker();\n\n        // start our thread to process user authority changes daily\n\n        userAuthorityFilterExecutor = Executors.newScheduledThreadPool(1);\n        userAuthorityFilterExecutor.scheduleAtFixedRate(\n                new UserAuthorityFilterEnforcer(), 0, 1, TimeUnit.DAYS);\n    }\n\n    void setAuditRefObjectBits() {\n\n        auditRefSet = new BitSet();\n\n        // by default we're only going to handle audit enabled roles and groups\n        // the value is a comma separated list of supported objects:\n        // role, group, policy, service, domain, entity, tenancy, and template\n\n        final String auditCheck = System.getProperty(ZMSConsts.ZMS_PROP_AUDIT_REF_CHECK_OBJECTS, \"role,group\");\n\n        String[] objects = auditCheck.split(\",\");\n        for (String object : objects) {\n            switch (object) {\n                case ZMSConsts.ZMS_AUDIT_TYPE_ROLE:\n                    auditRefSet.set(AUDIT_TYPE_ROLE);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_GROUP:\n                    auditRefSet.set(AUDIT_TYPE_GROUP);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_POLICY:\n                    auditRefSet.set(AUDIT_TYPE_POLICY);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_SERVICE:\n                    auditRefSet.set(AUDIT_TYPE_SERVICE);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_DOMAIN:\n                    auditRefSet.set(AUDIT_TYPE_DOMAIN);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_ENTITY:\n                    auditRefSet.set(AUDIT_TYPE_ENTITY);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_TENANCY:\n                    auditRefSet.set(AUDIT_TYPE_TENANCY);\n                    break;\n                case ZMSConsts.ZMS_AUDIT_TYPE_TEMPLATE:\n                    auditRefSet.set(AUDIT_TYPE_TEMPLATE);\n                    break;\n            }\n        }\n    }\n\n    public DomainRoleMembers listOverdueReviewRoleMembers(String domainName) {\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listOverdueReviewRoleMembers(domainName);\n        }\n    }\n\n    @Override\n    public List<Role> getRolesByDomain(String domain) {\n        AthenzDomain athenzDomain = getAthenzDomain(domain, false);\n        return athenzDomain.getRoles();\n    }\n\n    public DomainList lookupDomainByTag(String tagKey, String tagValue) {\n        DomainList domList = new DomainList();\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            domList.setNames(con.lookupDomainByTags(tagKey, tagValue));\n        }\n        return domList;\n    }\n\n    static class DataCache {\n        AthenzDomain athenzDomain;\n        long modTime;\n\n        DataCache(AthenzDomain athenzDomain, long modTime) {\n            this.athenzDomain = athenzDomain;\n            this.modTime = modTime;\n        }\n\n        AthenzDomain getAthenzDomain() {\n            return athenzDomain;\n        }\n\n        long getModTime() {\n            return modTime;\n        }\n    }\n\n    AthenzDomain getAthenzDomainFromCache(ObjectStoreConnection con, String domainName) {\n\n        DataCache data = cacheStore.getIfPresent(domainName);\n        if (data == null) {\n            return null;\n        }\n\n        // if we have a match for a given domain name then we're going\n        // to check if the last modified domain timestamp matches to what's\n        // in the db: So if there is no match, then we'll take the hit\n        // of extra db read, however, in most cases the domain data is not\n        // changed that often so we'll satisfy the request with just\n        // verifying the last modification time as oppose to reading the\n        // full domain data from db\n\n        long modTime = 0;\n\n        try {\n            modTime = con.getDomainModTimestamp(domainName);\n        } catch (ResourceException ignored) {\n            // if the exception is due to timeout or we were not able\n            // to get a connection to the object store then we're\n            // going to use our cache as is instead of rejecting\n            // the operation\n        }\n\n        // if our cache data is same or newer than db then return\n        // data from the cache (it could be newer if we just updated\n        // the cache based on write db but during read, the server\n        // hasn't replicated the data yet)\n\n        if (data.getModTime() >= modTime) {\n            return data.getAthenzDomain();\n        }\n\n        cacheStore.invalidate(domainName);\n        return null;\n    }\n\n    String getPrincipalName(ResourceContext ctx) {\n        if (ctx == null) {\n            return null;\n        }\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        if (principal == null) {\n            return null;\n        }\n        return principal.getFullName();\n    }\n\n    void saveChanges(ObjectStoreConnection con, String domainName) {\n\n        // we're first going to commit our changes which will\n        // also set the connection in auto-commit mode. we are\n        // going to change the domain timestamp in auto-commit\n        // mode so that we don't have a contention\n\n        con.commitChanges();\n        con.updateDomainModTimestamp(domainName);\n        cacheStore.invalidate(domainName);\n    }\n\n    void auditLogRequest(ResourceContext ctx, String domainName, String auditRef,\n            String caller, String operation, String entityName, String auditDetails) {\n        auditLogger.log(getAuditLogMsgBuilder(ctx, domainName, auditRef, caller, operation, entityName, auditDetails));\n    }\n\n    void auditLogRequest(String principal, String domainName, String auditRef,\n            String caller, String operation, String entityName, String auditDetails) {\n        AuditLogMsgBuilder msgBldr = getAuditLogMsgBuilder(null, domainName, auditRef, caller, operation, entityName, auditDetails);\n        msgBldr.who(principal);\n        auditLogger.log(msgBldr);\n    }\n\n    private AuditLogMsgBuilder getAuditLogMsgBuilder(ResourceContext ctx, String domainName,\n            String auditRef, String caller, String operation, String entityName, String auditDetails) {\n        AuditLogMsgBuilder msgBldr = ZMSUtils.getAuditLogMsgBuilder(ctx, auditLogger,\n                domainName, auditRef, caller, operation);\n        msgBldr.when(Timestamp.fromCurrentTime().toString()).whatEntity(entityName);\n        if (auditDetails != null) {\n            msgBldr.whatDetails(auditDetails);\n        }\n        return msgBldr;\n    }\n\n    Domain makeDomain(ResourceContext ctx, Domain domain, List<String> adminUsers,\n            List<String> solutionTemplates, String auditRef) {\n\n        final String caller = \"makedomain\";\n        final String domainName = domain.getName();\n        String principalName = getPrincipalName(ctx);\n        if (principalName == null) {\n            principalName = \"system-account\";\n        }\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            // get our connection object\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // before adding this domain we need to verify our\n                // quota check for sub-domains\n\n                quotaCheck.checkSubdomainQuota(con, domainName, caller);\n\n                boolean objectsInserted = con.insertDomain(domain);\n                objectsInserted &= processDomainTags(con, domain.getTags(), null, domainName, false);\n\n                if (!objectsInserted) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(\"makeDomain: Cannot create domain: \" +\n                            domainName + \" - already exists\", caller);\n                }\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"domain\\\": \");\n                auditLogDomain(auditDetails, domain);\n\n                // first create and process the admin role\n\n                Role adminRole = ZMSUtils.makeAdminRole(domainName, adminUsers);\n                auditDetails.append(\", \\\"role\\\": \");\n                if (!processRole(con, null, domainName, ZMSConsts.ADMIN_ROLE_NAME, adminRole,\n                        principalName, auditRef, false, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"makeDomain: Cannot process role: '\" +\n                            adminRole.getName(), caller);\n                }\n\n                // now create and process the admin policy\n\n                Policy adminPolicy = ZMSUtils.makeAdminPolicy(domainName, adminRole);\n                auditDetails.append(\", \\\"policy\\\": \");\n                if (!processPolicy(con, null, domainName, ZMSConsts.ADMIN_POLICY_NAME, adminPolicy,\n                        false, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"makeDomain: Cannot process policy: '\" +\n                            adminPolicy.getName(), caller);\n                }\n\n                // go through our list of templates and add the specified\n                // roles and polices to our domain\n\n                if (solutionTemplates != null) {\n                    for (String templateName : solutionTemplates) {\n                        auditDetails.append(\", \\\"template\\\": \");\n                        if (!addSolutionTemplate(con, domainName, templateName, principalName,\n                                null, auditRef, auditDetails)) {\n                            con.rollbackChanges();\n                            throw ZMSUtils.internalServerError(\"makeDomain: Cannot apply templates: '\" +\n                                    domain, caller);\n                        }\n                    }\n                }\n                auditDetails.append(\"}\");\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log entry\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_POST,\n                        domainName, auditDetails.toString());\n\n                return domain;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    boolean processPolicy(ObjectStoreConnection con, Policy originalPolicy, String domainName,\n            String policyName, Policy policy, boolean ignoreDeletes, StringBuilder auditDetails) {\n\n        // check to see if we need to insert the policy or update it\n\n        boolean requestSuccess;\n        if (originalPolicy == null) {\n            requestSuccess = con.insertPolicy(domainName, policy);\n        } else {\n            requestSuccess = con.updatePolicy(domainName, policy);\n        }\n\n        // if we didn't update any policies then we need to return failure\n\n        if (!requestSuccess) {\n            return false;\n        }\n\n        // open our audit record\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(policyName).append('\\\"');\n\n        // now we need process our policy assertions depending this is\n        // a new insert operation or an update\n\n        List<Assertion> newAssertions = policy.getAssertions();\n        if (originalPolicy == null) {\n\n            // we're just going to process our new assertions\n\n            if (newAssertions != null) {\n                for (Assertion assertion : newAssertions) {\n                    if (!con.insertAssertion(domainName, policyName, assertion)) {\n                        return false;\n                    }\n                }\n                auditLogAssertions(auditDetails, \"added-assertions\", newAssertions);\n            }\n\n        } else {\n\n            // first we need to retrieve the current set of assertions\n\n            List<Assertion> curAssertions = originalPolicy.getAssertions();\n            if (curAssertions == null) {\n                curAssertions = new ArrayList<>();\n            }\n            List<Assertion> addAssertions = new ArrayList<>();\n            List<Assertion> delAssertions = new ArrayList<>();\n            policyAssertionChanges(newAssertions, curAssertions, addAssertions, delAssertions);\n\n            if (!ignoreDeletes) {\n                for (Assertion assertion : delAssertions) {\n                    if (!con.deleteAssertion(domainName, policyName, assertion.getId())) {\n                        return false;\n                    }\n                }\n                auditLogAssertions(auditDetails, \"deleted-assertions\", delAssertions);\n            }\n\n            for (Assertion assertion : addAssertions) {\n                if (!con.insertAssertion(domainName, policyName, assertion)) {\n                    return false;\n                }\n            }\n            auditLogAssertions(auditDetails, \"added-assertions\", addAssertions);\n        }\n\n        auditDetails.append('}');\n        return true;\n    }\n\n    boolean removeMatchedAssertion(Assertion assertion, List<Assertion> assertions, List<Assertion> matchedAssertions) {\n\n        AssertionEffect effect = AssertionEffect.ALLOW;\n        if (assertion.getEffect() != null) {\n            effect = assertion.getEffect();\n        }\n\n        Iterator<Assertion> itr = assertions.iterator();\n        while (itr.hasNext()) {\n\n            Assertion checkAssertion = itr.next();\n\n            if (!assertion.getAction().equals(checkAssertion.getAction())) {\n                continue;\n            }\n            if (!assertion.getResource().equals(checkAssertion.getResource())) {\n                continue;\n            }\n            if (!assertion.getRole().equals(checkAssertion.getRole())) {\n                continue;\n            }\n\n            AssertionEffect checkEffect = AssertionEffect.ALLOW;\n            if (checkAssertion.getEffect() != null) {\n                checkEffect = checkAssertion.getEffect();\n            }\n\n            if (effect != checkEffect) {\n                continue;\n            }\n\n            itr.remove();\n            matchedAssertions.add(checkAssertion);\n            return true;\n        }\n\n        return false;\n    }\n\n    void policyAssertionChanges(List<Assertion> newAssertions, List<Assertion> curAssertions,\n            List<Assertion> addAssertions, List<Assertion> delAssertions) {\n\n        // let's iterate through the new list and the ones that are\n        // not in the current list should be added to the add list\n\n        List<Assertion> matchedAssertions = new ArrayList<>();\n        if (newAssertions != null) {\n            for (Assertion assertion : newAssertions) {\n                if (!removeMatchedAssertion(assertion, curAssertions, matchedAssertions)) {\n                    addAssertions.add(assertion);\n                }\n            }\n        }\n\n        // now our current list has been updated as well and\n        // all the assertions that were present moved to the\n        // matched assertion list so whatever left in the\n        // current list must be deleted\n\n        delAssertions.addAll(curAssertions);\n\n        // now let's go back and re-add the matched assertions\n        // back to our list so we can get the right audit data\n\n        curAssertions.addAll(matchedAssertions);\n    }\n\n    boolean processRole(ObjectStoreConnection con, Role originalRole, String domainName,\n            String roleName, Role role, String admin, String auditRef, boolean ignoreDeletes,\n            StringBuilder auditDetails) {\n\n        // check to see if we need to insert the role or update it\n\n        boolean requestSuccess;\n        if (originalRole == null) {\n            // auditEnabled can only be set with system admin privileges\n            role.setAuditEnabled(false);\n            requestSuccess = con.insertRole(domainName, role);\n        } else {\n            // carrying over auditEnabled from original role\n            role.setAuditEnabled(originalRole.getAuditEnabled());\n            requestSuccess = con.updateRole(domainName, role);\n        }\n\n        // if we didn't update any roles then we need to return failure\n\n        if (!requestSuccess) {\n            return false;\n        }\n\n        // open our audit record and log our trust field if one is available\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(roleName)\n            .append(\"\\\", \\\"trust\\\": \\\"\").append(role.getTrust()).append('\\\"');\n\n        // now we need process our role members depending this is\n        // a new insert operation or an update\n\n        List<RoleMember> roleMembers = role.getRoleMembers();\n        if (originalRole == null) {\n\n            // we are just going to process all members as new inserts\n\n            if (roleMembers != null) {\n\n                for (RoleMember member : roleMembers) {\n                    if (!con.insertRoleMember(domainName, roleName, member, admin, auditRef)) {\n                        return false;\n                    }\n                }\n                auditLogRoleMembers(auditDetails, \"added-members\", roleMembers);\n            }\n        } else {\n            if (!processUpdateRoleMembers(con, originalRole, roleMembers, ignoreDeletes,\n                    domainName, roleName, admin, auditRef, auditDetails)) {\n                return false;\n            }\n        }\n\n        if (!processRoleTags(role, roleName, domainName, originalRole, con)) {\n            return false;\n        }\n\n        auditDetails.append('}');\n        return true;\n    }\n\n    private boolean processRoleTags(Role role, String roleName, String domainName,\n                                    Role originalRole, ObjectStoreConnection con) {\n        if (role.getTags() != null && !role.getTags().isEmpty()) {\n            if (originalRole == null) {\n                return con.insertRoleTags(roleName, domainName, role.getTags());\n            } else {\n                return processUpdateRoleTags(role, originalRole, con, roleName, domainName);\n            }\n        }\n        return true;\n    }\n\n    private boolean processUpdateRoleTags(Role role, Role originalRole, ObjectStoreConnection con, String roleName, String domainName) {\n        if (originalRole.getTags() == null || originalRole.getTags().isEmpty()) {\n            if (role.getTags() == null || role.getTags().isEmpty()) {\n                // no tags to process..\n                return true;\n            }\n            return con.insertRoleTags(roleName, domainName, role.getTags());\n        }\n        Map<String, TagValueList> originalRoleTags = originalRole.getTags();\n        Map<String, TagValueList> currentTags = role.getTags();\n\n        Set<String> tagsToRemove = originalRoleTags.entrySet().stream()\n                .filter(curTag -> currentTags.get(curTag.getKey()) == null\n                        || !currentTags.get(curTag.getKey()).equals(curTag.getValue()))\n                .map(Map.Entry::getKey)\n                .collect(Collectors.toSet());\n\n        Map<String, TagValueList> tagsToAdd = currentTags.entrySet().stream()\n                .filter(curTag -> originalRoleTags.get(curTag.getKey()) == null\n                        || !originalRoleTags.get(curTag.getKey()).equals(curTag.getValue()))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        boolean res = con.deleteRoleTags(roleName, domainName, tagsToRemove);\n\n        return res && con.insertRoleTags(roleName, domainName, tagsToAdd);\n    }\n\n    boolean processGroup(ObjectStoreConnection con, Group originalGroup, final String domainName,\n                        final String groupName, Group group, final String admin, final String auditRef,\n                        StringBuilder auditDetails) {\n\n        // check to see if we need to insert the group or update it\n\n        boolean requestSuccess;\n        if (originalGroup == null) {\n            // auditEnabled can only be set with system admin privileges\n            group.setAuditEnabled(false);\n            requestSuccess = con.insertGroup(domainName, group);\n        } else {\n            // carrying over auditEnabled from original group\n            group.setAuditEnabled(originalGroup.getAuditEnabled());\n            requestSuccess = con.updateGroup(domainName, group);\n        }\n\n        // if we didn't update any groups then we need to return failure\n\n        if (!requestSuccess) {\n            return false;\n        }\n\n        // open our audit record and log our trust field if one is available\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(groupName).append('\\\"');\n\n        // now we need process our groups members depending this is\n        // a new insert operation or an update\n\n        List<GroupMember> groupMembers = group.getGroupMembers();\n\n        if (originalGroup == null) {\n\n            // we are just going to process all members as new inserts\n\n            for (GroupMember member : groupMembers) {\n                if (!con.insertGroupMember(domainName, groupName, member, admin, auditRef)) {\n                    return false;\n                }\n            }\n            auditLogGroupMembers(auditDetails, \"added-members\", groupMembers);\n\n        } else {\n\n            if (!processUpdateGroupMembers(con, originalGroup, groupMembers, domainName, groupName,\n                    admin, auditRef, auditDetails)) {\n                return false;\n            }\n        }\n\n        auditDetails.append('}');\n        return true;\n    }\n\n    void mergeOriginalRoleAndMetaRoleAttributes(Role originalRole, Role templateRole) {\n        //Only if the template rolemeta value is null, update with original role value\n        //else use the rolemeta value from template\n        if (templateRole.getSelfServe() == null) {\n            templateRole.setSelfServe(originalRole.getSelfServe());\n        }\n        if (templateRole.getMemberExpiryDays() == null) {\n            templateRole.setMemberExpiryDays(originalRole.getMemberExpiryDays());\n        }\n        if (templateRole.getServiceExpiryDays() == null) {\n            templateRole.setServiceExpiryDays(originalRole.getServiceExpiryDays());\n        }\n        if (templateRole.getGroupExpiryDays() == null) {\n            templateRole.setGroupExpiryDays(originalRole.getGroupExpiryDays());\n        }\n        if (templateRole.getGroupReviewDays() == null) {\n            templateRole.setGroupReviewDays(originalRole.getGroupReviewDays());\n        }\n        if (templateRole.getTokenExpiryMins() == null) {\n            templateRole.setTokenExpiryMins(originalRole.getTokenExpiryMins());\n        }\n        if (templateRole.getCertExpiryMins() == null) {\n            templateRole.setCertExpiryMins(originalRole.getCertExpiryMins());\n        }\n        if (templateRole.getSignAlgorithm() == null) {\n            templateRole.setSignAlgorithm(originalRole.getSignAlgorithm());\n        }\n        if (templateRole.getReviewEnabled() == null) {\n            templateRole.setReviewEnabled(originalRole.getReviewEnabled());\n        }\n        if (templateRole.getNotifyRoles() == null) {\n            templateRole.setNotifyRoles(originalRole.getNotifyRoles());\n        }\n        if (templateRole.getMemberReviewDays() == null) {\n            templateRole.setMemberReviewDays(originalRole.getMemberReviewDays());\n        }\n        if (templateRole.getServiceReviewDays() == null) {\n            templateRole.setServiceReviewDays(originalRole.getServiceReviewDays());\n        }\n        if (templateRole.getUserAuthorityFilter() == null) {\n            templateRole.setUserAuthorityFilter(originalRole.getUserAuthorityFilter());\n        }\n        if (templateRole.getUserAuthorityExpiration() == null) {\n            templateRole.setUserAuthorityExpiration(originalRole.getUserAuthorityExpiration());\n        }\n    }\n\n    private boolean processUpdateRoleMembers(ObjectStoreConnection con, Role originalRole,\n            List<RoleMember> roleMembers, boolean ignoreDeletes, String domainName,\n            String roleName, String admin, String auditRef, StringBuilder auditDetails) {\n\n        // first we need to retrieve the current set of members\n\n        List<RoleMember> originalMembers = originalRole.getRoleMembers();\n        List<RoleMember> curMembers = (null == originalMembers) ? new ArrayList<>() : new ArrayList<>(originalMembers);\n        List<RoleMember> delMembers = new ArrayList<>(curMembers);\n        ArrayList<RoleMember> newMembers = (null == roleMembers) ? new ArrayList<>() : new ArrayList<>(roleMembers);\n\n        // remove current members from new members\n\n        AuthzHelper.removeRoleMembers(newMembers, curMembers);\n\n        // remove new members from current members\n        // which leaves the deleted members.\n\n        AuthzHelper.removeRoleMembers(delMembers, roleMembers);\n\n        if (!ignoreDeletes) {\n            for (RoleMember member : delMembers) {\n                if (!con.deleteRoleMember(domainName, roleName, member.getMemberName(), admin, auditRef)) {\n                    return false;\n                }\n            }\n            auditLogRoleMembers(auditDetails, \"deleted-members\", delMembers);\n        }\n\n        for (RoleMember member : newMembers) {\n            if (!con.insertRoleMember(domainName, roleName, member, admin, auditRef)) {\n                return false;\n            }\n        }\n        auditLogRoleMembers(auditDetails, \"added-members\", newMembers);\n        return true;\n    }\n\n    private boolean processUpdateGroupMembers(ObjectStoreConnection con, Group originalGroup,\n                                              List<GroupMember> groupMembers, final String domainName,\n                                              final String groupName, final String admin, final String auditRef,\n                                              StringBuilder auditDetails) {\n\n        // first we need to retrieve the current set of members\n\n        List<GroupMember> originalMembers = originalGroup.getGroupMembers();\n        List<GroupMember> curMembers = new ArrayList<>(originalMembers);\n        List<GroupMember> delMembers = new ArrayList<>(curMembers);\n        ArrayList<GroupMember> newMembers = new ArrayList<>(groupMembers);\n\n        // remove current members from new members\n\n        AuthzHelper.removeGroupMembers(newMembers, curMembers);\n\n        // remove new members from current members\n        // which leaves the deleted members.\n\n        AuthzHelper.removeGroupMembers(delMembers, groupMembers);\n\n        for (GroupMember member : delMembers) {\n            if (!con.deleteGroupMember(domainName, groupName, member.getMemberName(), admin, auditRef)) {\n                return false;\n            }\n        }\n        auditLogGroupMembers(auditDetails, \"deleted-members\", delMembers);\n\n        for (GroupMember member : newMembers) {\n            if (!con.insertGroupMember(domainName, groupName, member, admin, auditRef)) {\n                return false;\n            }\n        }\n        auditLogGroupMembers(auditDetails, \"added-members\", newMembers);\n        return true;\n    }\n\n    boolean processServiceIdentity(ObjectStoreConnection con, ServiceIdentity originalService,\n            String domainName, String serviceName, ServiceIdentity service,\n            boolean ignoreDeletes, StringBuilder auditDetails) {\n\n        boolean requestSuccess;\n        if (originalService == null) {\n            // provider endpoint can only be set with system admin privileges\n            service.setProviderEndpoint(null);\n            requestSuccess = con.insertServiceIdentity(domainName, service);\n        } else {\n            // carrying over provider endpoint from original service\n            service.setProviderEndpoint(originalService.getProviderEndpoint());\n            requestSuccess = con.updateServiceIdentity(domainName, service);\n        }\n\n        // if we didn't update any services then we need to return failure\n\n        if (!requestSuccess) {\n            return false;\n        }\n\n        // open our audit record and log our service details\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(serviceName).append('\\\"')\n            .append(\", \\\"executable\\\": \\\"\").append(service.getExecutable()).append('\\\"')\n            .append(\", \\\"user\\\": \\\"\").append(service.getUser()).append('\\\"')\n            .append(\", \\\"group\\\": \\\"\").append(service.getGroup()).append('\\\"')\n            .append(\", \\\"description\\\": \\\"\").append(service.getDescription()).append('\\\"');\n\n        // now we need process our public keys depending this is\n        // a new insert operation or an update\n\n        List<PublicKeyEntry> publicKeys = service.getPublicKeys();\n        if (originalService == null) {\n\n            // we are just going to process all public keys as new inserts\n\n            if (publicKeys != null) {\n\n                for (PublicKeyEntry publicKey : publicKeys) {\n                    if (!con.insertPublicKeyEntry(domainName, serviceName, publicKey)) {\n                        return false;\n                    }\n                }\n                auditLogPublicKeyEntries(auditDetails, \"added-publickeys\", publicKeys);\n            }\n\n        } else {\n\n            // first we need to retrieve the current set of public keys\n\n            List<PublicKeyEntry> curPublicKeys = originalService.getPublicKeys();\n            Map<String, PublicKeyEntry> curPublicKeysMap = new HashMap<>();\n            if (curPublicKeys != null) {\n                for (PublicKeyEntry publicKey : curPublicKeys) {\n                    curPublicKeysMap.put(publicKey.getId(), publicKey);\n                }\n            }\n            Map<String, PublicKeyEntry> publicKeysMap = new HashMap<>();\n            if (publicKeys != null) {\n                for (PublicKeyEntry publicKey : publicKeys) {\n                    publicKeysMap.put(publicKey.getId(), publicKey);\n                }\n            }\n            Set<String> curPublicKeysSet = new HashSet<>(curPublicKeysMap.keySet());\n            Set<String> delPublicKeysSet = new HashSet<>(curPublicKeysSet);\n            Set<String> newPublicKeysSet = new HashSet<>(publicKeysMap.keySet());\n            newPublicKeysSet.removeAll(curPublicKeysSet);\n            delPublicKeysSet.removeAll(new HashSet<>(publicKeysMap.keySet()));\n\n            if (!ignoreDeletes) {\n                for (String publicKey : delPublicKeysSet) {\n                    if (!con.deletePublicKeyEntry(domainName, serviceName, publicKey)) {\n                        return false;\n                    }\n                }\n                auditLogPublicKeyEntries(auditDetails, \"deleted-publickeys\", delPublicKeysSet);\n            }\n\n            for (String publicKey : newPublicKeysSet) {\n                if (!con.insertPublicKeyEntry(domainName, serviceName, publicKeysMap.get(publicKey))) {\n                    return false;\n                }\n            }\n            auditLogPublicKeyEntries(auditDetails, \"added-publickeys\", newPublicKeysSet, publicKeysMap);\n        }\n\n        // now we need to process the hosts defined for this service\n\n        Set<String> curHosts;\n        if (originalService != null && originalService.getHosts() != null) {\n            curHosts = new HashSet<>(originalService.getHosts());\n        } else {\n            curHosts = new HashSet<>();\n        }\n\n        Set<String> newHosts;\n        if (service.getHosts() != null) {\n            newHosts = new HashSet<>(service.getHosts());\n        } else {\n            newHosts = new HashSet<>();\n        }\n\n        Set<String> delHosts = new HashSet<>(curHosts);\n        delHosts.removeAll(newHosts);\n        newHosts.removeAll(curHosts);\n\n        for (String host : delHosts) {\n            if (!con.deleteServiceHost(domainName, serviceName, host)) {\n                return false;\n            }\n        }\n        auditLogStrings(auditDetails, \"deleted-hosts\", delHosts);\n\n        for (String host : newHosts) {\n            if (!con.insertServiceHost(domainName, serviceName, host)) {\n                return false;\n            }\n        }\n        auditLogStrings(auditDetails, \"added-hosts\", newHosts);\n\n        auditDetails.append('}');\n        return true;\n    }\n\n    boolean shouldRetryOperation(ResourceException ex, int retryCount) {\n\n        // before doing anything else let's check to see if\n        // we still have the option to retry the operation\n\n        if (retryCount <= 1) {\n            return false;\n        }\n\n        // if we got a conflict result it means we either had\n        // no connection or deadlock was detected and as such\n        // the changes were aborted\n\n        boolean retry = false;\n        switch (ex.getCode()) {\n\n            case ResourceException.CONFLICT:\n\n                retry = true;\n                break;\n\n            case ResourceException.GONE:\n\n                // this error indicates that the server is reporting it is in\n                // read-only mode which indicates a fail-over has taken place\n                // and we need to clear all connections and start new ones\n                // this could only happen with write operations against the\n                // read-write object store\n\n                store.clearConnections();\n                retry = true;\n                break;\n        }\n\n        // if we're asked to retry then we're going to\n        // wait for a short period of time to allow the other\n        // connection to finish its work\n\n        if (retry) {\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\": possible deadlock, retries available: {}\", retryCount);\n            }\n\n            ZMSUtils.threadSleep(retrySleepTime);\n        }\n\n        // return our response\n\n        return retry;\n    }\n\n    void executePutPolicy(ResourceContext ctx, String domainName, String policyName, Policy policy,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // check that quota is not exceeded\n\n                quotaCheck.checkPolicyQuota(con, domainName, policy, caller);\n\n                // retrieve our original policy\n\n                Policy originalPolicy = getPolicy(con, domainName, policyName);\n\n                // now process the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                if (!processPolicy(con, originalPolicy, domainName, policyName, policy, false, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put policy: \" + policy.getName(), caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutRole(ResourceContext ctx, String domainName, String roleName, Role role,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, principal, AUDIT_TYPE_ROLE);\n\n                // check that quota is not exceeded\n\n                quotaCheck.checkRoleQuota(con, domainName, role, caller);\n\n                // retrieve our original role\n\n                Role originalRole = getRole(con, domainName, roleName, false, false, false);\n\n                if (originalRole != null &&\n                        (originalRole.getAuditEnabled() == Boolean.TRUE || originalRole.getReviewEnabled() == Boolean.TRUE)) {\n                    throw ZMSUtils.requestError(\"Can not update auditEnabled and/or reviewEnabled roles\", caller);\n                }\n\n                // now process the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                if (!processRole(con, originalRole, domainName, roleName, role,\n                        principal, auditRef, false, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put role: \" + role.getName(), caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        roleName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutGroup(ResourceContext ctx, final String domainName, final String groupName, Group group, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), principal, AUDIT_TYPE_GROUP);\n\n                // check that quota is not exceeded\n\n                quotaCheck.checkGroupQuota(con, domainName, group, ctx.getApiName());\n\n                // retrieve our original group\n\n                Group originalGroup = getGroup(con, domainName, groupName, false, false);\n\n                if (originalGroup != null &&\n                        (originalGroup.getAuditEnabled() == Boolean.TRUE || originalGroup.getReviewEnabled() == Boolean.TRUE)) {\n                    throw ZMSUtils.requestError(\"Can not update auditEnabled and/or reviewEnabled groups\", ctx.getApiName());\n                }\n\n                // now process the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                if (!processGroup(con, originalGroup, domainName, groupName, group,\n                        principal, auditRef, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put group: \" + group.getName(), ctx.getApiName());\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_PUT,\n                        groupName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutServiceIdentity(ResourceContext ctx, String domainName, String serviceName,\n            ServiceIdentity service, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_SERVICE);\n\n                // check that quota is not exceeded\n\n                quotaCheck.checkServiceIdentityQuota(con, domainName, service, caller);\n\n                // retrieve our original service identity object\n\n                ServiceIdentity originalService = getServiceIdentity(con, domainName, serviceName, false);\n\n                // now process the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                if (!processServiceIdentity(con, originalService, domainName, serviceName,\n                        service, false, auditDetails)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put service: \" + service.getName(), caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        serviceName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutPublicKeyEntry(ResourceContext ctx, String domainName, String serviceName,\n            PublicKeyEntry keyEntry, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_SERVICE);\n\n                // check to see if this key already exists or not\n\n                PublicKeyEntry originalKeyEntry = con.getPublicKeyEntry(domainName, serviceName,\n                        keyEntry.getId(), false);\n\n                // now we need verify our quota check if we know that\n                // that we'll be adding another public key\n\n                if (originalKeyEntry == null) {\n                    quotaCheck.checkServiceIdentityPublicKeyQuota(con, domainName, serviceName, caller);\n                }\n\n                // now process the request\n\n                boolean requestSuccess;\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n\n                if (originalKeyEntry == null) {\n                    requestSuccess = con.insertPublicKeyEntry(domainName, serviceName, keyEntry);\n                    auditDetails.append(\"{\\\"added-publicKeys\\\": [\");\n                } else {\n                    requestSuccess = con.updatePublicKeyEntry(domainName, serviceName, keyEntry);\n                    auditDetails.append(\"{\\\"updated-publicKeys\\\": [\");\n                }\n\n                if (!requestSuccess) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put public key: \" + keyEntry.getId() +\n                            \" in service \" + ResourceUtils.serviceResourceName(domainName, serviceName), caller);\n                }\n\n                // update our service and domain time-stamp and save changes\n\n                con.updateServiceIdentityModTimestamp(domainName, serviceName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogPublicKeyEntry(auditDetails, keyEntry, true);\n                auditDetails.append(\"]}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        serviceName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeletePublicKeyEntry(ResourceContext ctx, String domainName, String serviceName,\n            String keyId, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_SERVICE);\n\n                // now process the request\n\n                if (!con.deletePublicKeyEntry(domainName, serviceName, keyId)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"unable to delete public key: \" + keyId +\n                            \" in service \" + ResourceUtils.serviceResourceName(domainName, serviceName), caller);\n                }\n\n                // update our service and domain time-stamp and save changes\n\n                con.updateServiceIdentityModTimestamp(domainName, serviceName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        serviceName, \"{\\\"deleted-publicKeys\\\": [{\\\"id\\\": \\\"\" + keyId + \"\\\"}]}\");\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    boolean isTrustRole(Role role) {\n\n        if (role == null) {\n            return false;\n        }\n\n        return role.getTrust() != null && !role.getTrust().isEmpty();\n    }\n\n    void executePutMembership(ResourceContext ctx, String domainName, String roleName,\n            RoleMember roleMember, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, principal, AUDIT_TYPE_ROLE);\n\n                // make sure the role auditing requirements are met\n\n                Role originalRole = con.getRole(domainName, roleName);\n                if (originalRole == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": Unknown role: \" + roleName, caller);\n                }\n\n                checkObjectAuditEnabled(con, originalRole.getAuditEnabled(), originalRole.getName(),\n                        auditRef, caller, principal);\n\n                // before inserting a member we need to verify that\n                // this is a group role and not a delegated one.\n\n                if (isTrustRole(originalRole)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(caller + \": \" + roleName +\n                            \" is a delegated role\", caller);\n                }\n\n                // now we need verify our quota check\n\n                quotaCheck.checkRoleMembershipQuota(con, domainName, roleName, caller);\n\n                // process our insert role member support. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.insertRoleMember(domainName, roleName, roleMember,\n                        principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(caller + \": unable to insert role member: \" +\n                            roleMember.getMemberName() + \" to role: \" + roleName, caller);\n                }\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogRoleMember(auditDetails, roleMember, true);\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT, roleName,\n                        auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutGroupMembership(ResourceContext ctx, final String domainName, Group group,\n                                   GroupMember groupMember, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), principal, AUDIT_TYPE_GROUP);\n\n                // make sure the group auditing requirements are met\n\n                checkObjectAuditEnabled(con, group.getAuditEnabled(), group.getName(),\n                        auditRef, ctx.getApiName(), principal);\n\n                // now we need verify our quota check\n\n                final String groupName = ZMSUtils.extractGroupName(domainName, group.getName());\n                quotaCheck.checkGroupMembershipQuota(con, domainName, groupName, ctx.getApiName());\n\n                // process our insert group member support. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.insertGroupMember(domainName, groupName, groupMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(\"unable to insert group member: \" +\n                            groupMember.getMemberName() + \" to group: \" + groupName, ctx.getApiName());\n                }\n\n                // update our group and domain time-stamps, and invalidate local cache entry\n\n                con.updateGroupModTimestamp(domainName, groupName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogGroupMember(auditDetails, groupMember, true);\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_PUT, groupName,\n                        auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutEntity(ResourceContext ctx, String domainName, String entityName,\n            Entity entity, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_ENTITY);\n\n                // check that quota is not exceeded\n\n                quotaCheck.checkEntityQuota(con, domainName, entity, caller);\n\n                // check to see if this key already exists or not\n\n                Entity originalEntity = con.getEntity(domainName, entityName);\n\n                // now process the request\n\n                boolean requestSuccess;\n                if (originalEntity == null) {\n                    requestSuccess = con.insertEntity(domainName, entity);\n                } else {\n                    requestSuccess = con.updateEntity(domainName, entity);\n                }\n\n                if (!requestSuccess) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(\"unable to put entity: \"\n                            + entity.getName(), caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        entityName, JSON.string(entity.getValue()));\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteMembership(ResourceContext ctx, String domainName, String roleName,\n            String normalizedMember, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                final String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, principal, AUDIT_TYPE_ROLE);\n\n                // process our delete role member operation\n\n                if (!con.deleteRoleMember(domainName, roleName, normalizedMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete role member: \" +\n                            normalizedMember + \" from role: \" + roleName, caller);\n                }\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        roleName, \"{\\\"member\\\": \\\"\" + normalizedMember + \"\\\"}\");\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeletePendingMembership(ResourceContext ctx, String domainName, String roleName,\n                String normalizedMember, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, principal, AUDIT_TYPE_ROLE);\n\n                // process our delete role member operation\n\n                if (!con.deletePendingRoleMember(domainName, roleName, normalizedMember,\n                        principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete pending role member: \" +\n                            normalizedMember + \" from role: \" + roleName, caller);\n                }\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        roleName, \"{\\\"pending-member\\\": \\\"\" + normalizedMember + \"\\\"}\");\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteGroupMembership(ResourceContext ctx, final String domainName, final String groupName,\n                                      final String normalizedMember, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), principal, AUDIT_TYPE_GROUP);\n\n                // process our delete group member operation\n\n                if (!con.deleteGroupMember(domainName, groupName, normalizedMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"unable to delete group member: \" +\n                            normalizedMember + \" from group: \" + groupName, ctx.getApiName());\n                }\n\n                // update our group and domain time-stamps, and invalidate local cache entry\n\n                con.updateGroupModTimestamp(domainName, groupName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_DELETE,\n                        groupName, \"{\\\"member\\\": \\\"\" + normalizedMember + \"\\\"}\");\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeletePendingGroupMembership(ResourceContext ctx, final String domainName, final String groupName,\n                                             final String normalizedMember, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), principal, AUDIT_TYPE_GROUP);\n\n                // process our delete pending group member operation\n\n                if (!con.deletePendingGroupMember(domainName, groupName, normalizedMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"unable to delete pending group member: \" +\n                            normalizedMember + \" from group: \" + groupName, ctx.getApiName());\n                }\n\n                // update our group and domain time-stamps, and invalidate local cache entry\n\n                con.updateGroupModTimestamp(domainName, groupName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_DELETE,\n                        groupName, \"{\\\"pending-member\\\": \\\"\" + normalizedMember + \"\\\"}\");\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteServiceIdentity(ResourceContext ctx, String domainName, String serviceName,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_SERVICE);\n\n                // process our delete service request\n\n                if (!con.deleteServiceIdentity(domainName, serviceName)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete service: \" + serviceName, caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        serviceName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteEntity(ResourceContext ctx, String domainName, String entityName,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_ENTITY);\n\n                // process our delete role request\n\n                if (!con.deleteEntity(domainName, entityName)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete entity: \" + entityName, caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        entityName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteRole(ResourceContext ctx, String domainName, String roleName,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_ROLE);\n\n                // process our delete role request\n\n                if (!con.deleteRole(domainName, roleName)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete role: \" + roleName, caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        roleName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteGroup(ResourceContext ctx, final String domainName, final String groupName, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), getPrincipalName(ctx), AUDIT_TYPE_GROUP);\n\n                // process our delete group request\n\n                if (!con.deleteGroup(domainName, groupName)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"unable to delete group: \" + groupName, ctx.getApiName());\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_DELETE,\n                        groupName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeletePolicy(ResourceContext ctx, String domainName, String policyName,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // extract the current policy for audit log purposes\n\n                Policy policy = getPolicy(con, domainName, policyName);\n                if (policy == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to read policy: \" + policyName, caller);\n                }\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogPolicy(auditDetails, policy, \"deleted-assertions\");\n\n                // process our delete policy request\n\n                if (!con.deletePolicy(domainName, policyName)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete policy: \" + policyName, caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    /**\n     * If the domain has audit enabled, and user did not provide the auditRef,\n     * an exception will be thrown\n     **/\n    void checkDomainAuditEnabled(ObjectStoreConnection con, final String domainName,\n            final String auditRef, final String caller, final String principal, int objectType) {\n\n        // before retrieving the domain details make sure we are\n        // configured to enforce audit reference field on the given\n        // object type\n\n        if (!auditRefSet.get(objectType)) {\n            return;\n        }\n\n        Domain domain = con.getDomain(domainName);\n        if (domain == null) {\n            con.rollbackChanges();\n            throw ZMSUtils.notFoundError(caller + \": Unknown domain: \" + domainName, caller);\n        }\n\n        auditReferenceCheck(con, domain, auditRef, caller, principal);\n    }\n\n    void checkDomainAuditEnabled(ObjectStoreConnection con, Domain domain,\n            final String auditRef, final String caller, final String principal, int objectType) {\n\n        if (!auditRefSet.get(objectType)) {\n            return;\n        }\n\n        auditReferenceCheck(con, domain, auditRef, caller, principal);\n    }\n\n    void auditReferenceCheck(ObjectStoreConnection con, Domain domain, final String auditRef,\n            final String caller, final String principal) {\n\n        if (domain.getAuditEnabled() == Boolean.TRUE) {\n            if (auditRef == null || auditRef.length() == 0) {\n                con.rollbackChanges();\n                throw ZMSUtils.requestError(caller + \": Audit reference required for domain: \" + domain.getName(), caller);\n            }\n\n            if (auditReferenceValidator != null && !auditReferenceValidator.validateReference(auditRef, principal, caller)) {\n                con.rollbackChanges();\n                throw ZMSUtils.requestError(caller + \": Audit reference validation failed for domain: \" + domain.getName() + \", auditRef: \" + auditRef, caller);\n            }\n        }\n    }\n\n    void executeDeleteDomain(ResourceContext ctx, String domainName, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_DOMAIN);\n\n                // now process the request\n\n                con.deleteDomain(domainName);\n                con.commitChanges();\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        domainName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    List<String> listPrincipals(String domainName, boolean domainOnly) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n\n            List<String> principals = con.listPrincipals(domainName);\n\n            // if no further filtering is necessary, return the data\n            // right away\n\n            if (!domainOnly) {\n                return principals;\n            }\n\n            // generate our return list\n\n            List<String> users = new ArrayList<>();\n\n            // if we're asked for domain only then we need to match\n            // the domain name, if specified, and make sure the response\n            // only includes a single period/domain separator\n            // we need to skip an extra byte to accommodate for the\n            // domain separator (e.g. <domainName>.<userName>)\n\n            int prefixLength = 0;\n            if (domainName != null) {\n                prefixLength = domainName.length() + 1;\n            }\n\n            for (String principal : principals) {\n\n                // make sure the principal name doesn't have multiple\n                // components - e.g. user.joe.test since it represents\n                // a service or a sub-domain and we're only interested\n                // in actual users\n\n                if (prefixLength > 0) {\n                    if (principal.indexOf('.', prefixLength) == -1) {\n                        users.add(principal);\n                    }\n                } else {\n\n                    // we have a single separator when the first index\n                    // and the last index are the same\n\n                    if (principal.indexOf('.') == principal.lastIndexOf('.')) {\n                        users.add(principal);\n                    }\n                }\n            }\n\n            return users;\n        }\n    }\n\n    void removePrincipalFromDomainRoles(ObjectStoreConnection con, String domainName, String principalName,\n            String adminUser, String auditRef) {\n\n        // extract all the roles that this principal is member of\n        // we have to this here so that there are records of\n        // entries in the role member audit logs and the domain\n        // entries are properly invalidated\n\n        List<PrincipalRole> roles = con.listPrincipalRoles(domainName, principalName);\n\n        // we want to check if we had any roles otherwise\n        // we don't want to update the domain mod timestamp\n\n        if (roles.isEmpty()) {\n            return;\n        }\n\n        for (PrincipalRole role : roles) {\n\n            final String roleName = role.getRoleName();\n\n            // process our delete role member operation\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"removePrincipalFromDomainRoles: removing member {} from {}:role.{}\",\n                        principalName, domainName, roleName);\n            }\n\n            // we are going to ignore all errors here rather than\n            // rejecting the full operation\n\n            try {\n                con.deleteRoleMember(domainName, roleName, principalName, adminUser, auditRef);\n            } catch (ResourceException ex) {\n                LOG.error(\"removePrincipalFromDomainRoles: unable to remove {} from {}:role.{} - error {}\",\n                        principalName, domainName, roleName, ex.getMessage());\n            }\n\n            // update our role and domain time-stamps, and invalidate local cache entry\n\n            con.updateRoleModTimestamp(domainName, roleName);\n        }\n\n        con.updateDomainModTimestamp(domainName);\n    }\n\n    void removePrincipalFromAllRoles(ObjectStoreConnection con, String principalName,\n            String adminUser, String auditRef) {\n\n        // extract all the roles that this principal is member of\n        // we have to this here so that there are records of\n        // entries in the role member audit logs and the domain\n        // entries are properly invalidated\n\n        List<PrincipalRole> roles;\n        try {\n            roles = con.listPrincipalRoles(null, principalName);\n        } catch (ResourceException ex) {\n\n            // if there is no such principal then we have nothing to do\n\n            if (ex.getCode() == ResourceException.NOT_FOUND) {\n                return;\n            } else {\n                throw ex;\n            }\n        }\n\n        for (PrincipalRole role : roles) {\n\n            final String domainName = role.getDomainName();\n            final String roleName = role.getRoleName();\n\n            // process our delete role member operation\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"removePrincipalFromAllRoles: removing member {} from {}:role.{}\",\n                        principalName, domainName, roleName);\n            }\n\n            // we are going to ignore all errors here rather than\n            // rejecting the full operation. our delete user will\n            // eventually remove all these principals\n\n            try {\n                con.deleteRoleMember(domainName, roleName, principalName, adminUser, auditRef);\n            } catch (ResourceException ex) {\n                LOG.error(\"removePrincipalFromAllRoles: unable to remove {} from {}:role.{} - error {}\",\n                        principalName, domainName, roleName, ex.getMessage());\n            }\n\n            // update our role and domain time-stamps, and invalidate local cache entry\n\n            con.updateRoleModTimestamp(domainName, roleName);\n            con.updateDomainModTimestamp(domainName);\n        }\n    }\n\n    void removePrincipalDomains(ObjectStoreConnection con, String principalName) {\n\n        // first we're going to retrieve the list domains for\n        // the given user\n\n        final String domainPrefix = principalName + \".\";\n        List<String> subDomains = con.listDomains(domainPrefix, 0);\n\n        // first we're going to delete the user domain if\n        // one exists and then all the sub-domains. We're not\n        // going to fail the operation for these steps - only\n        // if the actual user is not deleted\n\n        con.deleteDomain(principalName);\n        cacheStore.invalidate(principalName);\n\n        for (String subDomain : subDomains) {\n            con.deleteDomain(subDomain);\n            cacheStore.invalidate(subDomain);\n        }\n    }\n\n    void executeDeleteDomainRoleMember(ResourceContext ctx, String domainName,\n             String memberName, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // remove this user from all roles manually so that we\n                // can have an audit log record for each role\n\n                removePrincipalFromDomainRoles(con, domainName, memberName,\n                        getPrincipalName(ctx), auditRef);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        memberName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteUser(ResourceContext ctx, String userName, String domainName,\n             String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // remove all principal domains\n\n                removePrincipalDomains(con, domainName);\n\n                // extract all principals that this user has - this would\n                // include the user self plus all services this user\n                // has created in the personal domain + sub-domains\n\n                List<String> userSvcPrincipals = con.listPrincipals(domainName);\n\n                // remove this user from all roles manually so that we\n                // can have an audit log record for each role\n\n                final String adminPrincipal = getPrincipalName(ctx);\n                removePrincipalFromAllRoles(con, userName, adminPrincipal, auditRef);\n                for (String userSvcPrincipal : userSvcPrincipals) {\n                    removePrincipalFromAllRoles(con, userSvcPrincipal, adminPrincipal, auditRef);\n                }\n\n                // finally delete the principal object. any roles that were\n                // left behind will be cleaned up from this operation\n\n                if (!con.deletePrincipal(userName, true)) {\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete user: \"\n                            + userName, caller);\n                }\n\n                // audit log the request\n\n                auditLogRequest(ctx, userName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        userName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    ServiceIdentity getServiceIdentity(String domainName, String serviceName, boolean attrsOnly) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return getServiceIdentity(con, domainName, serviceName, attrsOnly);\n        }\n    }\n\n    DomainTemplateList listDomainTemplates(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            DomainTemplateList domainTemplateList = new DomainTemplateList();\n            domainTemplateList.setTemplateNames(con.listDomainTemplates(domainName));\n            return domainTemplateList;\n        }\n    }\n\n    ServiceIdentity getServiceIdentity(ObjectStoreConnection con, String domainName,\n            String serviceName, boolean attrsOnly) {\n\n        ServiceIdentity service = con.getServiceIdentity(domainName, serviceName);\n        if (service != null && !attrsOnly) {\n            service.setPublicKeys(con.listPublicKeys(domainName, serviceName));\n            List<String> hosts = con.listServiceHosts(domainName, serviceName);\n            if (hosts != null && !hosts.isEmpty()) {\n                service.setHosts(hosts);\n            }\n        }\n        return service;\n    }\n\n    PublicKeyEntry getPublicKeyFromCache(String domainName, String serviceName, String keyId) {\n\n        DataCache data = cacheStore.getIfPresent(domainName);\n        if (data == null) {\n            return null;\n        }\n\n        AthenzDomain athenzDomain = data.getAthenzDomain();\n        if (athenzDomain == null) {\n            return null;\n        }\n\n        List<ServiceIdentity> services = athenzDomain.getServices();\n        if (services == null) {\n            return null;\n        }\n\n        final String fullServiceName = ResourceUtils.serviceResourceName(domainName, serviceName);\n        for (ServiceIdentity service : services) {\n            if (fullServiceName.equals(service.getName())) {\n                List<PublicKeyEntry> publicKeys = service.getPublicKeys();\n                if (publicKeys != null) {\n                    for (PublicKeyEntry publicKey : publicKeys) {\n                        if (keyId.equals(publicKey.getId())) {\n                            return publicKey;\n                        }\n                    }\n                }\n                break;\n            }\n        }\n\n        return null;\n    }\n\n    PublicKeyEntry getServicePublicKeyEntry(String domainName, String serviceName,\n            String keyId, boolean domainStateCheck) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getPublicKeyEntry(domainName, serviceName, keyId, domainStateCheck);\n        } catch (ResourceException ex) {\n            if (ex.getCode() != ResourceException.SERVICE_UNAVAILABLE) {\n                throw ex;\n            }\n        }\n\n        // if we got this far it means we couldn't get our public key\n        // from our DB store either due to timeout or communication\n        // error so we're going to see if we have the public key in\n        // our cache and use that for our requests\n\n        PublicKeyEntry keyEntry = getPublicKeyFromCache(domainName, serviceName, keyId);\n        if (keyEntry == null) {\n            throw new ResourceException(ResourceException.SERVICE_UNAVAILABLE,\n                    \"Unable to retrieve public key from DB store\");\n        }\n        return keyEntry;\n    }\n\n    public ResourceAccessList getResourceAccessList(String principal, String action) {\n\n        // this commands takes a quite a bit of time due to joining tables\n        // and needs to be optimized. For now we'll configure it with\n        // default timeout of 30 minutes to avoid any issues\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            con.setOperationTimeout(1800);\n            return con.listResourceAccess(principal, action, zmsConfig.getUserDomain());\n        }\n    }\n\n    Domain getDomain(String domainName, boolean masterCopy) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, masterCopy)) {\n            return con.getDomain(domainName);\n        }\n    }\n\n    List<String> listDomains(String prefix, long modifiedSince, boolean masterCopy) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, masterCopy)) {\n            return con.listDomains(prefix, modifiedSince);\n        }\n    }\n\n    DomainList lookupDomainById(final String account, final String subscription, int productId) {\n\n        DomainList domList = new DomainList();\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            String domain = con.lookupDomainById(account, subscription, productId);\n            if (domain != null) {\n                domList.setNames(Collections.singletonList(domain));\n            }\n        }\n        return domList;\n    }\n\n    DomainList lookupDomainByAWSAccount(final String account) {\n        return lookupDomainById(account, null, 0);\n    }\n\n    DomainList lookupDomainByAzureSubscription(final String subscription) {\n        return lookupDomainById(null, subscription, 0);\n    }\n\n    DomainList lookupDomainByProductId(Integer productId) {\n        return lookupDomainById(null, null, productId);\n    }\n\n    DomainList lookupDomainByBusinessService(final String businessService) {\n        DomainList domList = new DomainList();\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            domList.setNames(con.lookupDomainByBusinessService(businessService));\n        }\n        return domList;\n    }\n\n    DomainList lookupDomainByRole(String roleMember, String roleName) {\n\n        DomainList domList = new DomainList();\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            domList.setNames(con.lookupDomainByRole(roleMember, roleName));\n        }\n        return domList;\n    }\n\n    List<String> listRoles(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listRoles(domainName);\n        }\n    }\n\n    Membership getMembership(String domainName, String roleName, String principal,\n            long expiryTimestamp, boolean pending) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            Membership membership = con.getRoleMember(domainName, roleName, principal, expiryTimestamp, pending);\n            Timestamp expiration = membership.getExpiration();\n\n            //need to check expiration and set isMember if expired\n\n            if (expiration != null && expiration.millis() < System.currentTimeMillis()) {\n                membership.setIsMember(false);\n            }\n\n            return membership;\n        }\n    }\n\n    GroupMembership getGroupMembership(final String domainName, final String groupName, final String principal,\n                                       long expiryTimestamp, boolean pending) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            GroupMembership membership = con.getGroupMember(domainName, groupName, principal, expiryTimestamp, pending);\n            Timestamp expiration = membership.getExpiration();\n\n            //need to check expiration and set isMember if expired\n\n            if (expiration != null && expiration.millis() < System.currentTimeMillis()) {\n                membership.setIsMember(false);\n            }\n\n            return membership;\n        }\n    }\n\n    DomainRoleMembers listDomainRoleMembers(String domainName) {\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listDomainRoleMembers(domainName);\n        }\n    }\n\n    DomainRoleMember getPrincipalRoles(String principal, String domainName) {\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getPrincipalRoles(principal, domainName);\n        }\n    }\n\n    DomainGroupMember getPrincipalGroups(String principal, String domainName) {\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getPrincipalGroups(principal, domainName);\n        }\n    }\n\n    Group getGroup(final String domainName, final String groupName, Boolean auditLog, Boolean pending) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return getGroup(con, domainName, groupName, auditLog, pending);\n        }\n    }\n\n    Group getGroup(ObjectStoreConnection con, final String domainName, final String groupName,\n                 Boolean auditLog, Boolean pending) {\n\n        Group group = con.getGroup(domainName, groupName);\n        if (group == null) {\n            return null;\n        }\n\n        // let's retrieve our standard group members\n\n        group.setGroupMembers(con.listGroupMembers(domainName, groupName, pending));\n\n        if (auditLog == Boolean.TRUE) {\n            group.setAuditLog(con.listGroupAuditLogs(domainName, groupName));\n        }\n\n        return group;\n    }\n\n    public Role getRole(String domainName, String roleName, Boolean auditLog, Boolean expand, Boolean pending) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return getRole(con, domainName, roleName, auditLog, expand, pending);\n        }\n    }\n\n    Timestamp memberStrictExpiration(Timestamp groupExpiration, Timestamp memberExpiration) {\n        if (groupExpiration == null) {\n            return memberExpiration;\n        } else if (memberExpiration == null) {\n            return groupExpiration;\n        } else if (groupExpiration.millis() < memberExpiration.millis()) {\n            return groupExpiration;\n        } else {\n            return memberExpiration;\n        }\n    }\n\n    RoleMember convertGroupToRoleMember(GroupMember groupMember, Timestamp groupExpiration) {\n        return new RoleMember()\n                .setMemberName(groupMember.getMemberName())\n                .setActive(groupMember.getActive())\n                .setApproved(groupMember.getApproved())\n                .setAuditRef(groupMember.getAuditRef())\n                .setSystemDisabled(groupMember.getSystemDisabled())\n                .setRequestTime(groupMember.getRequestTime())\n                .setExpiration(memberStrictExpiration(groupExpiration, groupMember.getExpiration()));\n    }\n\n    void expandRoleGroupMembers(ObjectStoreConnection con, Role role, List<RoleMember> roleMembers, Boolean pending) {\n\n        List<RoleMember> expandedMembers = new ArrayList<>();\n        for (RoleMember roleMember : roleMembers) {\n            final String memberName = roleMember.getMemberName();\n\n            int idx = memberName.indexOf(AuthorityConsts.GROUP_SEP);\n            if (idx == -1) {\n                expandedMembers.add(roleMember);\n            } else {\n                final String domainName = memberName.substring(0, idx);\n                final String groupName = memberName.substring(idx + AuthorityConsts.GROUP_SEP.length());\n                List<GroupMember> groupMembers = con.listGroupMembers(domainName, groupName, pending);\n                for (GroupMember groupMember : groupMembers) {\n                    expandedMembers.add(convertGroupToRoleMember(groupMember, roleMember.getExpiration()));\n                }\n            }\n        }\n        role.setRoleMembers(expandedMembers);\n    }\n\n    Role getRole(ObjectStoreConnection con, String domainName, String roleName,\n            Boolean auditLog, Boolean expand, Boolean pending) {\n\n        Role role = con.getRole(domainName, roleName);\n        if (role != null) {\n\n            if (role.getTrust() == null) {\n\n                // if we have no trust field specified then we need to\n                // retrieve our standard group role members. However,\n                // since we can have groups as members in roles check\n                // to see if we're asked to expand them\n\n                if (expand == Boolean.TRUE) {\n                    expandRoleGroupMembers(con, role, con.listRoleMembers(domainName, roleName, pending), pending);\n                } else {\n                    role.setRoleMembers(con.listRoleMembers(domainName, roleName, pending));\n                }\n\n                if (auditLog == Boolean.TRUE) {\n                    role.setAuditLog(con.listRoleAuditLogs(domainName, roleName));\n                }\n\n            } else if (expand == Boolean.TRUE) {\n\n                // otherwise, if asked, let's expand the delegated\n                // membership and return the list of members\n\n                role.setRoleMembers(getDelegatedRoleMembers(con, domainName, role.getTrust(), roleName));\n            }\n\n            Map<String, TagValueList> roleTags = con.getRoleTags(domainName, roleName);\n            if (roleTags != null) {\n                role.setTags(roleTags);\n            }\n        }\n        return role;\n    }\n\n    List<RoleMember> getDelegatedRoleMembers(ObjectStoreConnection con, final String domainName,\n                                             final String trustDomain, final String roleName) {\n\n        // verify that the domain and trust domain are not the same\n\n        if (domainName.equals(trustDomain)) {\n            return null;\n        }\n\n        // retrieve our trust domain\n\n        AthenzDomain domain = null;\n        try {\n            domain = getAthenzDomain(con, trustDomain);\n        } catch (ResourceException ex) {\n            LOG.error(\"unable to fetch domain {}: {}\", trustDomain, ex.getMessage());\n        }\n\n        if (domain == null) {\n            return null;\n        }\n\n        // we need to use a set since we might be matching\n        // multiple assertions and we want to automatically\n        // skip any duplicate members\n\n        Map<String, RoleMember> roleMembers = new HashMap<>();\n\n        // generate our full role name\n\n        String fullRoleName = ResourceUtils.roleResourceName(domainName, roleName);\n\n        // iterate through all policies to see which one has the\n        // assume_role assertion for the given role\n\n        for (Policy policy : domain.getPolicies()) {\n\n            // ignore any inactive/multi-version policies\n            if (policy.getActive() == Boolean.FALSE) {\n                continue;\n            }\n\n            List<Assertion> assertions = policy.getAssertions();\n            if (assertions == null) {\n                continue;\n            }\n\n            for (Assertion assertion : assertions) {\n\n                if (!AuthzHelper.assumeRoleResourceMatch(fullRoleName, assertion)) {\n                    continue;\n                }\n\n                String rolePattern = StringUtils.patternFromGlob(assertion.getRole());\n                for (Role role : domain.getRoles()) {\n\n                    // make sure we have members before trying to match the name\n\n                    List<RoleMember> members = role.getRoleMembers();\n                    if (members == null || members.isEmpty()) {\n                        continue;\n                    }\n\n                    if (!role.getName().matches(rolePattern)) {\n                        continue;\n                    }\n\n                    for (RoleMember member : members) {\n                        String memberName = member.getMemberName();\n                        if (!roleMembers.containsKey(memberName)) {\n                            roleMembers.put(memberName, member);\n                        }\n                    }\n                }\n            }\n        }\n\n        return new ArrayList<>(roleMembers.values());\n    }\n\n    Policy getPolicy(String domainName, String policyName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return getPolicy(con, domainName, policyName);\n        }\n    }\n\n    Assertion getAssertion(String domainName, String policyName, Long assertionId) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getAssertion(domainName, policyName, assertionId);\n        }\n    }\n\n    void executePutAssertion(ResourceContext ctx, String domainName, String policyName,\n            Assertion assertion, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // now we need verify our quota check\n\n                quotaCheck.checkPolicyAssertionQuota(con, domainName, policyName, caller);\n\n                // process our insert assertion. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.insertAssertion(domainName, policyName, assertion)) {\n                    throw ZMSUtils.requestError(caller + \": unable to insert assertion: \" +\n                            \" to policy: \" + policyName, caller);\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogAssertion(auditDetails, assertion, true);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteAssertion(ResourceContext ctx, String domainName, String policyName,\n            Long assertionId, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // fetch the assertion for our audit log\n\n                Assertion assertion = con.getAssertion(domainName, policyName, assertionId);\n                if (assertion == null) {\n                    throw ZMSUtils.notFoundError(caller + \": unable to read assertion: \" +\n                            assertionId + \" from policy: \" + policyName, caller);\n                }\n\n                // process our delete assertion. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.deleteAssertion(domainName, policyName, assertionId)) {\n                    throw ZMSUtils.requestError(caller + \": unable to delete assertion: \" +\n                            assertionId + \" from policy: \" + policyName, caller);\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n                        .append(\"\\\", \\\"assertionId\\\": \\\"\").append(assertionId)\n                        .append(\"\\\", \\\"deleted-assertions\\\": [\");\n                auditLogAssertion(auditDetails, assertion, true);\n                auditDetails.append(\"]}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    List<String> listEntities(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listEntities(domainName);\n        }\n    }\n\n    Entity getEntity(String domainName, String entityName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getEntity(domainName, entityName);\n        }\n    }\n\n    Policy getPolicy(ObjectStoreConnection con, String domainName, String policyName) {\n\n        Policy policy = con.getPolicy(domainName, policyName);\n        if (policy != null) {\n            policy.setAssertions(con.listAssertions(domainName, policyName));\n        }\n        return policy;\n    }\n\n    List<String> listPolicies(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listPolicies(domainName, null);\n        }\n    }\n\n    List<String> listServiceIdentities(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.listServiceIdentities(domainName);\n        }\n    }\n\n    void executePutDomainMeta(ResourceContext ctx, Domain domain, DomainMeta meta,\n            final String systemAttribute, boolean deleteAllowed, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                final String domainName = domain.getName();\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_DOMAIN);\n\n                // now process the request. first we're going to make a\n                // copy of our domain\n\n                Domain updatedDomain = new Domain()\n                        .setName(domain.getName())\n                        .setEnabled(domain.getEnabled())\n                        .setId(domain.getId())\n                        .setAuditEnabled(domain.getAuditEnabled())\n                        .setDescription(domain.getDescription())\n                        .setOrg(domain.getOrg())\n                        .setApplicationId(domain.getApplicationId())\n                        .setAccount(domain.getAccount())\n                        .setAzureSubscription(domain.getAzureSubscription())\n                        .setYpmId(domain.getYpmId())\n                        .setCertDnsDomain(domain.getCertDnsDomain())\n                        .setMemberExpiryDays(domain.getMemberExpiryDays())\n                        .setServiceExpiryDays(domain.getServiceExpiryDays())\n                        .setGroupExpiryDays(domain.getGroupExpiryDays())\n                        .setTokenExpiryMins(domain.getTokenExpiryMins())\n                        .setRoleCertExpiryMins(domain.getRoleCertExpiryMins())\n                        .setServiceCertExpiryMins(domain.getServiceCertExpiryMins())\n                        .setSignAlgorithm(domain.getSignAlgorithm())\n                        .setUserAuthorityFilter(domain.getUserAuthorityFilter())\n                        .setBusinessService(domain.getBusinessService())\n                        .setTags(domain.getTags())\n                        .setBusinessService(domain.getBusinessService());\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                if (systemAttribute != null) {\n                    updateSystemMetaFields(updatedDomain, systemAttribute, deleteAllowed, meta);\n                } else {\n                    updateDomainMetaFields(updatedDomain, meta);\n                }\n\n                con.updateDomain(updatedDomain);\n\n                // if we're only updating our tags then we need to explicitly\n                // update our domain last mod timestamp since it won't be\n                // updated during the updateDomain call if there are no other\n                // changes present in the request\n\n                if (!processDomainTags(con, meta.getTags(), domain, domainName, true)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.internalServerError(caller + \"Unable to update tags\", caller);\n                }\n\n                con.commitChanges();\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogDomain(auditDetails, updatedDomain);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                // if the domain member expiry date has changed then we're going\n                // process all the members in the domain and update the expiration\n                // date accordingly\n\n                updateDomainMembersExpiration(ctx, con, domain, updatedDomain, auditRef, caller);\n\n                // if the domain user attribute expiry has changed we need to\n                // process all the members in the domain accordingly\n\n                updateDomainMembersUserAuthorityFilter(ctx, con, domain, updatedDomain, auditRef, caller);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    private boolean processDomainTags(ObjectStoreConnection con, Map<String, TagValueList> domainTags,\n            Domain originalDomain, final String domainName, boolean updateDomainLastModTimestamp) {\n\n        if (originalDomain == null || originalDomain.getTags() == null || originalDomain.getTags().isEmpty()) {\n            if (domainTags == null || domainTags.isEmpty()) {\n                // no tags to process..\n                return true;\n            }\n            if (!con.insertDomainTags(domainName, domainTags)) {\n                return false;\n            }\n            if (updateDomainLastModTimestamp) {\n                con.updateDomainModTimestamp(domainName);\n            }\n            return true;\n        }\n\n        if (domainTags == null) {\n            return true;\n        }\n\n        Map<String, TagValueList> originalDomainTags = originalDomain.getTags();\n\n        Set<String> tagsToRemove = originalDomainTags.entrySet().stream()\n            .filter(curTag -> domainTags.get(curTag.getKey()) == null\n                || !domainTags.get(curTag.getKey()).equals(curTag.getValue()))\n            .map(Map.Entry::getKey)\n            .collect(Collectors.toSet());\n\n        boolean tagsChanged = false;\n        if (tagsToRemove != null && !tagsToRemove.isEmpty()) {\n            if (!con.deleteDomainTags(originalDomain.getName(), tagsToRemove)) {\n                return false;\n            }\n            tagsChanged = true;\n        }\n\n        Map<String, TagValueList> tagsToAdd = domainTags.entrySet().stream()\n            .filter(curTag -> originalDomainTags.get(curTag.getKey()) == null\n                || !originalDomainTags.get(curTag.getKey()).equals(curTag.getValue()))\n            .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n\n        if (tagsToAdd != null && !tagsToAdd.isEmpty()) {\n            if (!con.insertDomainTags(originalDomain.getName(), tagsToAdd)) {\n                return false;\n            }\n            tagsChanged = true;\n        }\n\n        if (tagsChanged && updateDomainLastModTimestamp) {\n            con.updateDomainModTimestamp(domainName);\n        }\n        return true;\n    }\n\n    void updateDomainMembersUserAuthorityFilter(ResourceContext ctx, ObjectStoreConnection con, Domain domain,\n                                               Domain updatedDomain, String auditRef, String caller) {\n\n        // check if the authority filter has changed otherwise we have\n        // nothing to do\n\n        if (!isUserAuthorityFilterChanged(domain.getUserAuthorityFilter(), updatedDomain.getUserAuthorityFilter())) {\n            return;\n        }\n\n        final String domainName = domain.getName();\n        AthenzDomain athenzDomain;\n        try {\n            athenzDomain = getAthenzDomain(con, domainName);\n        } catch (ResourceException ex) {\n            LOG.error(\"unable to fetch domain {}: {}\", domainName, ex.getMessage());\n            return;\n        }\n\n        final String principal = getPrincipalName(ctx);\n        for (Role role : athenzDomain.getRoles()) {\n\n            // if it's a delegated role then we have nothing to do\n\n            if (role.getTrust() != null && !role.getTrust().isEmpty()) {\n                continue;\n            }\n\n            // if no role members, then there is nothing to do\n\n            final List<RoleMember> roleMembers = role.getRoleMembers();\n            if (roleMembers == null || roleMembers.isEmpty()) {\n                continue;\n            }\n\n            // process our role members and if there were any changes processed then update\n            // our role and domain time-stamps, and invalidate local cache entry\n\n            final String roleName = AthenzUtils.extractRoleName(role.getName());\n            List<RoleMember> roleMembersWithUpdatedDisabledState = getRoleMembersWithUpdatedDisabledState(roleMembers,\n                    role.getUserAuthorityFilter(), updatedDomain.getUserAuthorityFilter());\n            if (updateRoleMemberDisabledState(ctx, con, roleMembersWithUpdatedDisabledState, domainName,\n                    roleName, principal, auditRef, caller)) {\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n            }\n        }\n    }\n\n    void updateDomainMembersExpiration(ResourceContext ctx, ObjectStoreConnection con, Domain domain,\n            Domain updatedDomain, String auditRef, String caller) {\n\n        // we only need to process the domain role members if the new expiration\n        // is more restrictive than what we had before\n\n        boolean userMemberExpiryDayReduced = isNumOfDaysReduced(domain.getMemberExpiryDays(),\n                updatedDomain.getMemberExpiryDays());\n        boolean serviceMemberExpiryDayReduced = isNumOfDaysReduced(domain.getServiceExpiryDays(),\n                updatedDomain.getServiceExpiryDays());\n        boolean groupMemberExpiryDayReduced = isNumOfDaysReduced(domain.getGroupExpiryDays(),\n                updatedDomain.getGroupExpiryDays());\n\n        if (!userMemberExpiryDayReduced && !serviceMemberExpiryDayReduced && !groupMemberExpiryDayReduced) {\n            return;\n        }\n\n        AthenzDomain athenzDomain;\n        try {\n            athenzDomain = getAthenzDomain(con, domain.getName());\n        } catch (ResourceException ex) {\n            LOG.error(\"unable to fetch domain {}: {}\", domain.getName(), ex.getMessage());\n            return;\n        }\n\n        long userExpiryMillis = userMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedDomain.getMemberExpiryDays(), TimeUnit.DAYS) : 0;\n        long serviceExpiryMillis = serviceMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedDomain.getServiceExpiryDays(), TimeUnit.DAYS) : 0;\n        long groupExpiryMillis = groupMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedDomain.getGroupExpiryDays(), TimeUnit.DAYS) : 0;\n        Timestamp userExpiration = Timestamp.fromMillis(userExpiryMillis);\n        Timestamp serviceExpiration = Timestamp.fromMillis(serviceExpiryMillis);\n        Timestamp groupExpiration = Timestamp.fromMillis(groupExpiryMillis);\n\n        final String principal = getPrincipalName(ctx);\n        boolean domainModified = false;\n        for (Role role : athenzDomain.getRoles()) {\n            // if the role already has a specific expiry date set then we\n            // will automatically skip this role\n\n            if (role.getMemberExpiryDays() != null || role.getServiceExpiryDays() != null || role.getGroupExpiryDays() != null) {\n                continue;\n            }\n\n            // if it's a delegated role then we have nothing to do\n\n            if (role.getTrust() != null && !role.getTrust().isEmpty()) {\n                continue;\n            }\n\n            // if no role members, then there is nothing to do\n\n            final List<RoleMember> roleMembers = role.getRoleMembers();\n            if (roleMembers == null || roleMembers.isEmpty()) {\n                continue;\n            }\n\n            // process our role members and if there were any changes processed then update\n            // our role and domain time-stamps, and invalidate local cache entry\n\n            final String roleName = AthenzUtils.extractRoleName(role.getName());\n            List<RoleMember> roleMembersWithUpdatedDueDates = getRoleMembersWithUpdatedDueDates(roleMembers,\n                    userExpiration, userExpiryMillis, serviceExpiration, serviceExpiryMillis,\n                    groupExpiration, groupExpiryMillis, null, 0,\n                    null, 0, null, null, 0);\n            if (insertRoleMembers(ctx, con, roleMembersWithUpdatedDueDates, domain.getName(),\n                    roleName, principal, auditRef, caller)) {\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domain.getName(), roleName);\n                domainModified = true;\n            }\n        }\n        for (Group group : athenzDomain.getGroups()) {\n\n            // if the group already has a specific expiry date set then we\n            // will automatically skip this group\n\n            if (group.getMemberExpiryDays() != null || group.getServiceExpiryDays() != null) {\n                continue;\n            }\n\n            // if no group members, then there is nothing to do\n\n            final List<GroupMember> groupMembers = group.getGroupMembers();\n            if (groupMembers == null || groupMembers.isEmpty()) {\n                continue;\n            }\n\n            // process our group members and if there were any changes processed then update\n            // our group and domain time-stamps, and invalidate local cache entry\n\n            final String groupName = AthenzUtils.extractGroupName(group.getName());\n            List<GroupMember> groupMembersWithUpdatedDueDates = getGroupMembersWithUpdatedDueDates(groupMembers,\n                    userExpiration, userExpiryMillis, serviceExpiration, serviceExpiryMillis, null);\n            if (insertGroupMembers(ctx, con, groupMembersWithUpdatedDueDates, domain.getName(),\n                    groupName, principal, auditRef, caller)) {\n\n                // update our group and domain time-stamps, and invalidate local cache entry\n\n                con.updateGroupModTimestamp(domain.getName(), groupName);\n                domainModified = true;\n            }\n        }\n        if (domainModified) {\n            con.updateDomainModTimestamp(domain.getName());\n            cacheStore.invalidate(domain.getName());\n        }\n    }\n\n    void updateDomainMetaFields(Domain domain, DomainMeta meta) {\n\n        domain.setApplicationId(meta.getApplicationId());\n        domain.setDescription(meta.getDescription());\n        if (meta.getMemberExpiryDays() != null) {\n            domain.setMemberExpiryDays(meta.getMemberExpiryDays());\n        }\n        if (meta.getServiceExpiryDays() != null) {\n            domain.setServiceExpiryDays(meta.getServiceExpiryDays());\n        }\n        if (meta.getGroupExpiryDays() != null) {\n            domain.setGroupExpiryDays(meta.getGroupExpiryDays());\n        }\n        if (meta.getRoleCertExpiryMins() != null) {\n            domain.setRoleCertExpiryMins(meta.getRoleCertExpiryMins());\n        }\n        if (meta.getServiceCertExpiryMins() != null) {\n            domain.setServiceCertExpiryMins(meta.getServiceCertExpiryMins());\n        }\n        if (meta.getTokenExpiryMins() != null) {\n            domain.setTokenExpiryMins(meta.getTokenExpiryMins());\n        }\n        if (meta.getSignAlgorithm() != null) {\n            domain.setSignAlgorithm(meta.getSignAlgorithm());\n        }\n        if (meta.getBusinessService() != null) {\n            domain.setBusinessService(meta.getBusinessService());\n        }\n        if (meta.getTags() != null) {\n            domain.setTags(meta.getTags());\n        }\n    }\n\n    boolean isDeleteSystemMetaAllowed(boolean deleteAllowed, final String oldValue, final String newValue) {\n\n        // if authorized or old value is not set, then there is\n        // no need to check any value\n\n        if (deleteAllowed || oldValue == null || oldValue.isEmpty()) {\n            return true;\n        }\n\n        // since our old value is not null then we will only\n        // allow if the new value is identical\n\n        return oldValue.equals(newValue);\n    }\n\n    boolean isDeleteSystemMetaAllowed(boolean deleteAllowed, Integer oldValue, Integer newValue) {\n\n        // if authorized or old value is not set, then there is\n        // no need to check any value\n\n        if (deleteAllowed || oldValue == null || oldValue == 0) {\n            return true;\n        }\n\n        // since our old value is not null then we will only\n        // allow if the new value is identical\n\n        return newValue != null && newValue.intValue() == oldValue.intValue();\n    }\n\n    void updateSystemMetaFields(Domain domain, final String attribute, boolean deleteAllowed,\n            DomainMeta meta) {\n\n        final String caller = \"putdomainsystemmeta\";\n\n        // system attributes we'll only set if they're available\n        // in the given object\n\n        switch (attribute) {\n            case ZMSConsts.SYSTEM_META_ACCOUNT:\n                if (!isDeleteSystemMetaAllowed(deleteAllowed, domain.getAccount(), meta.getAccount())) {\n                    throw ZMSUtils.forbiddenError(\"unauthorized to reset system meta attribute: \" + attribute, caller);\n                }\n                domain.setAccount(meta.getAccount());\n                break;\n            case ZMSConsts.SYSTEM_META_AZURE_SUBSCRIPTION:\n                if (!isDeleteSystemMetaAllowed(deleteAllowed, domain.getAzureSubscription(), meta.getAzureSubscription())) {\n                    throw ZMSUtils.forbiddenError(\"unauthorized to reset system meta attribute: \" + attribute, caller);\n                }\n                domain.setAzureSubscription(meta.getAzureSubscription());\n                break;\n            case ZMSConsts.SYSTEM_META_PRODUCT_ID:\n                if (!isDeleteSystemMetaAllowed(deleteAllowed, domain.getYpmId(), meta.getYpmId())) {\n                    throw ZMSUtils.forbiddenError(\"unauthorized to reset system meta attribute: \" + attribute, caller);\n                }\n                domain.setYpmId(meta.getYpmId());\n                break;\n            case ZMSConsts.SYSTEM_META_CERT_DNS_DOMAIN:\n                if (!isDeleteSystemMetaAllowed(deleteAllowed, domain.getCertDnsDomain(), meta.getCertDnsDomain())) {\n                    throw ZMSUtils.forbiddenError(\"unauthorized to reset system meta attribute: \" + attribute, caller);\n                }\n                domain.setCertDnsDomain(meta.getCertDnsDomain());\n                break;\n            case ZMSConsts.SYSTEM_META_ORG:\n                if (!isDeleteSystemMetaAllowed(deleteAllowed, domain.getOrg(), meta.getOrg())) {\n                    throw ZMSUtils.forbiddenError(\"unauthorized to reset system meta attribute: \" + attribute, caller);\n                }\n                domain.setOrg(meta.getOrg());\n                break;\n            case ZMSConsts.SYSTEM_META_AUDIT_ENABLED:\n                domain.setAuditEnabled(meta.getAuditEnabled());\n                break;\n            case ZMSConsts.SYSTEM_META_USER_AUTH_FILTER:\n                domain.setUserAuthorityFilter(meta.getUserAuthorityFilter());\n                break;\n            case ZMSConsts.SYSTEM_META_ENABLED:\n                domain.setEnabled(meta.getEnabled());\n                break;\n            case ZMSConsts.SYSTEM_META_BUSINESS_SERVICE:\n                domain.setBusinessService(meta.getBusinessService());\n                break;\n            default:\n                throw ZMSUtils.requestError(\"unknown system meta attribute: \" + attribute, caller);\n        }\n    }\n\n    void updateRoleSystemMetaFields(ObjectStoreConnection con, Role updatedRole, Role originalRole,\n                                    final String attribute, RoleSystemMeta meta, final String caller) {\n\n        // system attributes we'll only set if they're available\n        // in the given object\n\n        if (ZMSConsts.SYSTEM_META_AUDIT_ENABLED.equals(attribute)) {\n            updatedRole.setAuditEnabled(meta.getAuditEnabled());\n\n            // we also need to verify that if we have any group members\n            // then those groups have the audit enabled flag as well\n\n            if (updatedRole.getAuditEnabled() == Boolean.TRUE && originalRole.getRoleMembers() != null) {\n                for (RoleMember roleMember : originalRole.getRoleMembers()) {\n                    final String memberName = roleMember.getMemberName();\n                    if (ZMSUtils.principalType(memberName, zmsConfig.getUserDomainPrefix(),\n                            zmsConfig.getAddlUserCheckDomainPrefixList()) != Principal.Type.GROUP) {\n                        continue;\n                    }\n\n                    int idx = memberName.indexOf(AuthorityConsts.GROUP_SEP);\n                    final String domainName = memberName.substring(0, idx);\n                    final String groupName = memberName.substring(idx + AuthorityConsts.GROUP_SEP.length());\n                    Group group = con.getGroup(domainName, groupName);\n                    if (group == null) {\n                        throw ZMSUtils.requestError(\"role has invalid group member: \" + memberName, caller);\n                    }\n                    if (group.getAuditEnabled() != Boolean.TRUE) {\n                        throw ZMSUtils.requestError(\"role member: \" + memberName + \" must have audit flag enabled\", caller);\n                    }\n                }\n            }\n        } else {\n            throw ZMSUtils.requestError(\"unknown role system meta attribute: \" + attribute, caller);\n        }\n    }\n\n    void updateGroupSystemMetaFields(Group group, final String attribute, GroupSystemMeta meta, final String caller) {\n\n        // system attributes we'll only set if they're available\n        // in the given object\n\n        if (ZMSConsts.SYSTEM_META_AUDIT_ENABLED.equals(attribute)) {\n            group.setAuditEnabled(meta.getAuditEnabled());\n        } else {\n            throw ZMSUtils.requestError(\"unknown group system meta attribute: \" + attribute, caller);\n        }\n    }\n\n    void updateServiceIdentitySystemMetaFields(ServiceIdentity service, final String attribute,\n            ServiceIdentitySystemMeta meta, final String caller) {\n\n        // system attributes we'll only set if they're available\n        // in the given object\n\n        if (ZMSConsts.SYSTEM_META_PROVIDER_ENDPOINT.equals(attribute)) {\n            service.setProviderEndpoint(meta.getProviderEndpoint());\n        } else {\n            throw ZMSUtils.requestError(\"unknown service system meta attribute: \" + attribute, caller);\n        }\n    }\n\n    void executePutDomainTemplate(ResourceContext ctx, String domainName, DomainTemplate domainTemplate,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TEMPLATE);\n\n                // go through our list of templates and add the specified\n                // roles and polices to our domain\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"add-templates\\\": \");\n                boolean firstEntry = true;\n\n                for (String templateName : domainTemplate.getTemplateNames()) {\n                    firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                    if (!addSolutionTemplate(con, domainName, templateName, getPrincipalName(ctx),\n                            domainTemplate.getParams(), auditRef, auditDetails)) {\n                        con.rollbackChanges();\n                        throw ZMSUtils.internalServerError(\"unable to put domain templates: \" + domainName, caller);\n                    }\n                }\n                auditDetails.append(\"}\");\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteDomainTemplate(ResourceContext ctx, String domainName, String templateName,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TEMPLATE);\n\n                // go through our list of templates and add the specified\n                // roles and polices to our domain\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"templates\\\": \");\n\n                Template template = zmsConfig.getServerSolutionTemplates().get(templateName);\n                deleteSolutionTemplate(con, domainName, templateName, template, auditDetails);\n\n                auditDetails.append(\"}\");\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        domainName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    boolean addSolutionTemplate(ObjectStoreConnection con, String domainName, String templateName,\n            String admin, List<TemplateParam> templateParams, String auditRef, StringBuilder auditDetails) {\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(templateName).append('\\\"');\n\n        // we have already verified that our template is valid but\n        // we'll just double check to make sure it's not null\n\n        Template template = zmsConfig.getServerSolutionTemplates().get(templateName);\n        if (template == null) {\n            auditDetails.append(\"}\");\n            return true;\n        }\n\n        boolean firstEntry = true;\n\n        // iterate through roles in the list.\n        // When adding a template, if the role does not exist in our domain\n        // then insert it otherwise only apply the changes to the member list.\n\n        List<Role> templateRoles = template.getRoles();\n        if (templateRoles != null) {\n            for (Role role : templateRoles) {\n\n                Role templateRole = updateTemplateRole(role, domainName, templateParams);\n\n                String roleName = ZMSUtils.removeDomainPrefix(templateRole.getName(),\n                    domainName, ROLE_PREFIX);\n\n                // retrieve our original role\n\n                Role originalRole = getRole(con, domainName, roleName, false, false, false);\n\n                // Merge original role with template role to handle role meta data\n                // if original role is null then it is an insert operation and no need of merging\n\n                if (originalRole != null) {\n                    mergeOriginalRoleAndMetaRoleAttributes(originalRole, templateRole);\n                }\n\n                // now process the request\n\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"add-role\\\": \");\n                if (!processRole(con, originalRole, domainName, roleName, templateRole,\n                        admin, auditRef, true, auditDetails)) {\n                    return false;\n                }\n            }\n        }\n\n        // iterate through policies in the list.\n        // When adding a template, if the policy does not exist in our domain\n        // then insert it otherwise only apply the changes to the assertions\n\n        List<Policy> templatePolicies = template.getPolicies();\n        if (templatePolicies != null) {\n            for (Policy policy : templatePolicies) {\n\n                Policy templatePolicy = updateTemplatePolicy(policy, domainName, templateParams);\n\n                String policyName = ZMSUtils.removeDomainPrefix(templatePolicy.getName(),\n                    domainName, POLICY_PREFIX);\n\n                // retrieve our original policy\n\n                Policy originalPolicy = getPolicy(con, domainName, policyName);\n\n                // now process the request\n\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"add-policy\\\": \");\n                if (!processPolicy(con, originalPolicy, domainName, policyName, templatePolicy,\n                        true, auditDetails)) {\n                    return false;\n                }\n            }\n        }\n\n        // iterate through service identities in the list.\n        // When adding a template, if the service identity does not exist in our domain\n        // then insert it otherwise only apply the changes\n\n        List<ServiceIdentity> templateServiceIdentities = template.getServices();\n        if (templateServiceIdentities != null) {\n            for (ServiceIdentity serviceIdentity : templateServiceIdentities) {\n\n                ServiceIdentity templateServiceIdentity = updateTemplateServiceIdentity(\n                        serviceIdentity, domainName, templateParams);\n\n                String serviceIdentityName = ZMSUtils.removeDomainPrefixForService(\n                        templateServiceIdentity.getName(), domainName);\n\n                // retrieve our original service\n\n                ServiceIdentity originalServiceIdentity = getServiceIdentity(con, domainName,\n                        serviceIdentityName, false);\n\n                // now process the request\n\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"add-service\\\": \");\n                if (!processServiceIdentity(con, originalServiceIdentity, domainName,\n                        serviceIdentityName, templateServiceIdentity, true, auditDetails)) {\n                    return false;\n                }\n            }\n        }\n\n        // if adding a template, only add if it is not in our current list\n        // check to see if the template is already listed for the domain\n\n        List<String> currentTemplateList = con.listDomainTemplates(domainName);\n        if (!currentTemplateList.contains(templateName)) {\n            con.insertDomainTemplate(domainName, templateName, null);\n        }\n\n        //on both insert and update templates, bump up the version of the template to latest version.\n        if (template.getMetadata().getLatestVersion() != null) {\n            con.updateDomainTemplate(domainName, templateName, template.getMetadata());\n        }\n\n        auditDetails.append(\"}\");\n        return true;\n    }\n\n    void deleteSolutionTemplate(ObjectStoreConnection con, String domainName, String templateName,\n            Template template, StringBuilder auditDetails) {\n\n        // currently there is no support for dynamic templates since the\n        // DELETE request has no payload and we can't pass our parameters\n\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(templateName).append('\\\"');\n\n        // we have already verified that our template is valid but\n        // we'll just double check to make sure it's not null\n\n        if (template == null) {\n            auditDetails.append(\"}\");\n            return;\n        }\n\n        boolean firstEntry = true;\n\n        // iterate through roles in the list and delete the role\n\n        List<Role> templateRoles = template.getRoles();\n        if (templateRoles != null) {\n            for (Role role : templateRoles) {\n                String roleName = ZMSUtils.removeDomainPrefix(role.getName(),\n                    TEMPLATE_DOMAIN_NAME, ROLE_PREFIX);\n\n                con.deleteRole(domainName, roleName);\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"delete-role\\\": \\\"\").append(roleName).append('\\\"');\n            }\n        }\n\n        // iterate through policies in the list and delete the policy\n\n        List<Policy> templatePolicies = template.getPolicies();\n        if (templatePolicies != null) {\n            for (Policy policy : templatePolicies) {\n                String policyName = ZMSUtils.removeDomainPrefix(policy.getName(),\n                    TEMPLATE_DOMAIN_NAME, POLICY_PREFIX);\n\n                con.deletePolicy(domainName, policyName);\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"delete-policy\\\": \\\"\").append(policyName).append('\\\"');\n            }\n        }\n\n        // iterate through services in the list and delete the service\n\n        List<ServiceIdentity> templateServices = template.getServices();\n        if (templateServices != null) {\n            for (ServiceIdentity serviceIdentity : templateServices) {\n                String serviceName = ZMSUtils.removeDomainPrefixForService(serviceIdentity.getName(),\n                    TEMPLATE_DOMAIN_NAME);\n                con.deleteServiceIdentity(domainName, serviceName);\n                firstEntry = auditLogSeparator(auditDetails, firstEntry);\n                auditDetails.append(\" \\\"delete-service\\\": \\\"\").append(serviceName).append('\\\"');\n            }\n        }\n\n        // delete the template from the current list\n\n        con.deleteDomainTemplate(domainName, templateName, null);\n        auditDetails.append(\"}\");\n    }\n\n    Role updateTemplateRole(Role role, String domainName, List<TemplateParam> params) {\n\n        // first process our given role name and carry out any\n        // requested substitutions\n\n        String templateRoleName = role.getName().replace(TEMPLATE_DOMAIN_NAME, domainName);\n        if (params != null) {\n            for (TemplateParam param : params) {\n                final String paramKey = \"_\" + param.getName() + \"_\";\n                templateRoleName = templateRoleName.replace(paramKey, param.getValue());\n            }\n        }\n        Role templateRole = new Role()\n                .setName(templateRoleName)\n                .setTrust(role.getTrust())\n                //adding additional role meta attributes if present in template->roles\n                .setCertExpiryMins(role.getCertExpiryMins())\n                .setSelfServe(role.getSelfServe())\n                .setMemberExpiryDays(role.getMemberExpiryDays())\n                .setTokenExpiryMins(role.getTokenExpiryMins())\n                .setSignAlgorithm(role.getSignAlgorithm())\n                .setServiceExpiryDays(role.getServiceExpiryDays())\n                .setGroupExpiryDays(role.getGroupExpiryDays())\n                .setGroupReviewDays(role.getGroupReviewDays())\n                .setMemberReviewDays(role.getMemberReviewDays())\n                .setServiceReviewDays(role.getServiceReviewDays())\n                .setReviewEnabled(role.getReviewEnabled())\n                .setNotifyRoles(role.getNotifyRoles())\n                .setUserAuthorityFilter(role.getUserAuthorityFilter())\n                .setUserAuthorityExpiration(role.getUserAuthorityExpiration());\n\n        List<RoleMember> roleMembers = role.getRoleMembers();\n        List<RoleMember> newMembers = new ArrayList<>();\n        if (roleMembers != null && !roleMembers.isEmpty()) {\n            for (RoleMember roleMember : roleMembers) {\n                RoleMember newRoleMember = new RoleMember();\n\n                // process our role members for any requested substitutions\n\n                String memberName = roleMember.getMemberName().replace(TEMPLATE_DOMAIN_NAME, domainName);\n                if (params != null) {\n                    for (TemplateParam param : params) {\n                        final String paramKey = \"_\" + param.getName() + \"_\";\n                        memberName = memberName.replace(paramKey, param.getValue());\n                    }\n                }\n                newRoleMember.setMemberName(memberName);\n                newRoleMember.setExpiration(roleMember.getExpiration());\n                newRoleMember.setReviewReminder(roleMember.getReviewReminder());\n                newMembers.add(newRoleMember);\n            }\n        }\n        templateRole.setRoleMembers(newMembers);\n        return templateRole;\n    }\n\n    Policy updateTemplatePolicy(Policy policy, String domainName, List<TemplateParam> params) {\n\n        // first process our given role name and carry out any\n        // requested substitutions\n\n        String templatePolicyName = policy.getName().replace(TEMPLATE_DOMAIN_NAME, domainName);\n        if (params != null) {\n            for (TemplateParam param : params) {\n                final String paramKey = \"_\" + param.getName() + \"_\";\n                templatePolicyName = templatePolicyName.replace(paramKey, param.getValue());\n            }\n        }\n\n        Policy templatePolicy = new Policy().setName(templatePolicyName);\n        List<Assertion> assertions = policy.getAssertions();\n        List<Assertion> newAssertions = new ArrayList<>();\n        if (assertions != null && !assertions.isEmpty()) {\n            for (Assertion assertion : assertions) {\n                Assertion newAssertion = new Assertion();\n                newAssertion.setAction(assertion.getAction());\n                newAssertion.setEffect(assertion.getEffect());\n\n                // process our assertion resource and role for any requested substitutions\n\n                String resource = assertion.getResource().replace(TEMPLATE_DOMAIN_NAME, domainName);\n                String role = assertion.getRole().replace(TEMPLATE_DOMAIN_NAME, domainName);\n                if (params != null) {\n                    for (TemplateParam param : params) {\n                        final String paramKey = \"_\" + param.getName() + \"_\";\n                        resource = resource.replace(paramKey, param.getValue());\n                        role = role.replace(paramKey, param.getValue());\n                    }\n                }\n                newAssertion.setResource(resource);\n                newAssertion.setRole(role);\n                newAssertions.add(newAssertion);\n            }\n        }\n        templatePolicy.setAssertions(newAssertions);\n        return templatePolicy;\n    }\n\n    ServiceIdentity updateTemplateServiceIdentity(ServiceIdentity serviceIdentity,\n            String domainName, List<TemplateParam> params) {\n\n        String templateServiceName = serviceIdentity.getName().replace(TEMPLATE_DOMAIN_NAME, domainName);\n        if (params != null) {\n            for (TemplateParam param : params) {\n                final String paramKey = \"_\" + param.getName() + \"_\";\n                templateServiceName = templateServiceName.replace(paramKey, param.getValue());\n            }\n        }\n\n        ServiceIdentity templateServiceIdentity = new ServiceIdentity().setName(templateServiceName);\n\n        templateServiceIdentity.setDescription(serviceIdentity.getDescription());\n        templateServiceIdentity.setExecutable(serviceIdentity.getExecutable());\n        templateServiceIdentity.setGroup(serviceIdentity.getGroup());\n        templateServiceIdentity.setUser(serviceIdentity.getUser());\n        templateServiceIdentity.setProviderEndpoint(serviceIdentity.getProviderEndpoint());\n\n        List<PublicKeyEntry> publicKeyEntries = serviceIdentity.getPublicKeys();\n        List<PublicKeyEntry> newPublicKeyEntries = new ArrayList<>();\n        if (publicKeyEntries != null && !publicKeyEntries.isEmpty()) {\n            for (PublicKeyEntry publicKeyEntry : publicKeyEntries) {\n                PublicKeyEntry newPublicKeyEntry = new PublicKeyEntry();\n                newPublicKeyEntry.setId(publicKeyEntry.getId());\n                newPublicKeyEntry.setKey(publicKeyEntry.getKey());\n                newPublicKeyEntries.add(newPublicKeyEntry);\n            }\n        }\n        templateServiceIdentity.setPublicKeys(newPublicKeyEntries);\n\n        List<String> hosts = serviceIdentity.getHosts();\n\n        if (hosts != null) {\n            templateServiceIdentity.setHosts(new ArrayList<>(hosts));\n        }\n\n        return templateServiceIdentity;\n    }\n\n    void setupTenantAdminPolicy(String tenantDomain, String provSvcDomain,\n            String provSvcName, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, tenantDomain, auditRef, caller, provSvcDomain + \".\" + provSvcName, AUDIT_TYPE_TENANCY);\n\n                String domainAdminRole = ResourceUtils.roleResourceName(tenantDomain, ZMSConsts.ADMIN_ROLE_NAME);\n                String serviceRoleResourceName = ZMSUtils.getTrustedResourceGroupRolePrefix(provSvcDomain,\n                        provSvcName, tenantDomain, null) + ZMSConsts.ADMIN_ROLE_NAME;\n\n                // our tenant admin role/policy name\n\n                final String tenancyResource = \"tenancy.\" + provSvcDomain + '.' + provSvcName;\n\n                String adminName = tenancyResource + \".admin\";\n                String tenantAdminRole = ResourceUtils.roleResourceName(tenantDomain, adminName);\n\n                // tenant admin role - if it already exists then we skip it\n                // by default it has no members.\n\n                if (con.getRole(tenantDomain, adminName) == null) {\n                    con.insertRole(tenantDomain, new Role().setName(tenantAdminRole));\n                }\n\n                // tenant admin policy - check to see if this already exists. If it does\n                // then we don't have anything to do\n\n                if (con.getPolicy(tenantDomain, adminName) == null) {\n\n                    Policy adminPolicy = new Policy().setName(ResourceUtils.policyResourceName(tenantDomain, adminName));\n                    con.insertPolicy(tenantDomain, adminPolicy);\n\n                    // we are going to create 2 assertions - one for the domain admin role\n                    // and another for the tenant admin role\n\n                    Assertion assertion = new Assertion().setRole(domainAdminRole)\n                            .setResource(serviceRoleResourceName).setAction(ZMSConsts.ACTION_ASSUME_ROLE)\n                            .setEffect(AssertionEffect.ALLOW);\n                    con.insertAssertion(tenantDomain, adminName, assertion);\n\n                    assertion = new Assertion().setRole(tenantAdminRole)\n                            .setResource(serviceRoleResourceName).setAction(ZMSConsts.ACTION_ASSUME_ROLE)\n                            .setEffect(AssertionEffect.ALLOW);\n                    con.insertAssertion(tenantDomain, adminName, assertion);\n\n                    // the tenant admin role must have the capability to provision\n                    // new resource groups in the domain which requires update\n                    // action capability on resource tenancy.<prov_domain>.<prov_svc>\n\n                    String tenantResourceName = tenantDomain + \":\" + tenancyResource;\n                    assertion = new Assertion().setRole(tenantAdminRole)\n                            .setResource(tenantResourceName).setAction(ZMSConsts.ACTION_UPDATE)\n                            .setEffect(AssertionEffect.ALLOW);\n                    con.insertAssertion(tenantDomain, adminName, assertion);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, tenantDomain);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutTenantRoles(ResourceContext ctx, String provSvcDomain, String provSvcName, String tenantDomain,\n            String resourceGroup, List<TenantRoleAction> roles, boolean ignoreDeletes, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, provSvcDomain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TENANCY);\n\n                String trustedRolePrefix = ZMSUtils.getTrustedResourceGroupRolePrefix(provSvcDomain,\n                        provSvcName, tenantDomain, resourceGroup);\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"put-tenant-roles\\\": [\");\n                boolean firstEntry = true;\n\n                for (TenantRoleAction ra : roles) {\n\n                    String tenantRole = ra.getRole();\n                    String tenantAction = ra.getAction();\n                    String trustedRole = trustedRolePrefix + tenantRole;\n                    String trustedName = trustedRole.substring((provSvcDomain + AuthorityConsts.ROLE_SEP).length());\n\n                    Role role = new Role().setName(trustedRole).setTrust(tenantDomain);\n\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"{}: add trusted Role to domain {}: {} -> {}\",\n                                caller, provSvcDomain, trustedRole, role);\n                    }\n\n                    // retrieve our original role in case one exists\n\n                    Role originalRole = getRole(con, provSvcDomain, trustedName, false, false, false);\n\n                    // now process the request\n\n                    firstEntry = auditLogSeparator(auditDetails, firstEntry);\n\n                    auditDetails.append(\"{\\\"role\\\": \");\n                    if (!processRole(con, originalRole, provSvcDomain, trustedName, role,\n                            getPrincipalName(ctx), auditRef, ignoreDeletes, auditDetails)) {\n                        con.rollbackChanges();\n                        throw ZMSUtils.internalServerError(\"unable to put role: \" + trustedRole, caller);\n                    }\n\n                    String policyResourceName = ResourceUtils.policyResourceName(provSvcDomain, trustedName);\n                    final String resourceName = provSvcDomain + \":service.\" +\n                            ZMSUtils.getTenantResourceGroupRolePrefix(provSvcName, tenantDomain, resourceGroup) + '*';\n                    List<Assertion> assertions = Collections.singletonList(\n                            new Assertion().setRole(trustedRole)\n                                    .setResource(resourceName)\n                                    .setAction(tenantAction));\n\n                    Policy policy = new Policy().setName(policyResourceName).setAssertions(assertions);\n\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"{}: add trust policy to domain {}: {} -> {}\",\n                                caller, provSvcDomain, trustedRole, policy);\n                    }\n\n                    // retrieve our original policy\n\n                    Policy originalPolicy = getPolicy(con, provSvcDomain, trustedName);\n\n                    // now process the request\n\n                    auditDetails.append(\", \\\"policy\\\": \");\n                    if (!processPolicy(con, originalPolicy, provSvcDomain, trustedName, policy, ignoreDeletes, auditDetails)) {\n                        con.rollbackChanges();\n                        throw ZMSUtils.internalServerError(\"unable to put policy: \" + policy.getName(), caller);\n                    }\n                    auditDetails.append('}');\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, provSvcDomain);\n\n                // audit log the request\n\n                auditLogRequest(ctx, provSvcDomain, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        tenantDomain, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void addAssumeRolePolicy(ObjectStoreConnection con, String rolePrefix,\n            String trustedRolePrefix, String role, List<RoleMember> roleMembers,\n            String tenantDomain, String admin, String auditRef,\n            StringBuilder auditDetails, String caller) {\n\n        // first create the role in the domain. We're going to create it\n        // only if the role does not already exist\n\n        String roleName = rolePrefix + role;\n        String roleResourceName = ResourceUtils.roleResourceName(tenantDomain, roleName);\n\n        // retrieve our original role in case one exists\n\n        Role originalRole = getRole(con, tenantDomain, roleName, false, false, false);\n\n        // we need to add the original role members to the new one\n\n        if (originalRole != null && originalRole.getRoleMembers() != null) {\n            roleMembers.addAll(originalRole.getRoleMembers());\n        }\n\n        // now process the request\n\n        Role roleObj = new Role().setName(roleResourceName).setRoleMembers(roleMembers);\n        auditDetails.append(\"{\\\"role\\\": \");\n        if (!processRole(con, originalRole, tenantDomain, roleName, roleObj,\n                admin, auditRef, false, auditDetails)) {\n            con.rollbackChanges();\n            throw ZMSUtils.internalServerError(\"unable to put role: \" + roleName, caller);\n        }\n\n        // now create the corresponding policy. We're going to create it\n        // only if the policy does not exist otherwise we'll just\n        // add a new assertion\n\n        String policyName = \"tenancy.\" + roleName;\n        String policyResourceName = ResourceUtils.policyResourceName(tenantDomain, policyName);\n        String serviceRoleResourceName = trustedRolePrefix + role;\n        Assertion assertion = new Assertion().setRole(roleResourceName)\n                .setResource(serviceRoleResourceName).setAction(ZMSConsts.ACTION_ASSUME_ROLE)\n                .setEffect(AssertionEffect.ALLOW);\n\n        if (LOG.isInfoEnabled()) {\n            LOG.info(\"executePutProviderRoles: ---- ASSUME_ROLE policyName is {}\", policyName);\n        }\n\n        // retrieve our original policy\n\n        Policy originalPolicy = getPolicy(con, tenantDomain, policyName);\n\n        // we need to add the original policy assertions to the new one\n\n        List<Assertion> newAssertions = new ArrayList<>();\n        if (originalPolicy != null && originalPolicy.getAssertions() != null) {\n            newAssertions.addAll(originalPolicy.getAssertions());\n        }\n\n        // if our new assertion is not already in the list then that will be added to\n\n        if (!newAssertions.contains(assertion)) {\n            newAssertions.add(assertion);\n        }\n\n        // now process the request\n\n        Policy assumeRolePolicy = new Policy().setName(policyResourceName).setAssertions(newAssertions);\n\n        auditDetails.append(\", \\\"policy\\\": \");\n        if (!processPolicy(con, originalPolicy, tenantDomain, policyName, assumeRolePolicy,\n                false, auditDetails)) {\n            con.rollbackChanges();\n            throw ZMSUtils.internalServerError(\"unable to put policy: \" +\n                    assumeRolePolicy.getName(), caller);\n        }\n        auditDetails.append('}');\n    }\n\n    void executePutProviderRoles(ResourceContext ctx, String tenantDomain, String provSvcDomain,\n            String provSvcName, String resourceGroup, List<String> roles, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, tenantDomain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TENANCY);\n\n                // we're going to create a separate role for each one of tenant roles returned\n                // based on its action and set the caller as a member in each role\n\n                final String principalName = getPrincipalName(ctx);\n\n                // now set up the roles and policies for all the provider roles returned.\n\n                final String rolePrefix = ZMSUtils.getProviderResourceGroupRolePrefix(provSvcDomain,\n                        provSvcName, resourceGroup);\n                final String trustedRolePrefix = ZMSUtils.getTrustedResourceGroupRolePrefix(provSvcDomain,\n                        provSvcName, tenantDomain, resourceGroup);\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"put-provider-roles\\\": [\");\n                boolean firstEntry = true;\n\n                for (String role : roles) {\n\n                    // we need to create a new object for each role since the list is updated\n                    // in case the role already has existing members, but we don't want to\n                    // add those members to other roles in our list\n\n                    List<RoleMember> roleMembers = new ArrayList<>();\n                    if (principalName != null) {\n                        RoleMember roleMember = new RoleMember();\n                        roleMember.setMemberName(principalName);\n                        roleMembers.add(roleMember);\n                    }\n\n                    role = role.toLowerCase();\n\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"executePutProviderRoles: provision ASSUME_ROLE policy for access remote role in \"\n                                + provSvcDomain + \".\" + provSvcName + \": \" + resourceGroup + \".\" + role);\n                    }\n\n                    firstEntry = auditLogSeparator(auditDetails, firstEntry);\n\n                    addAssumeRolePolicy(con, rolePrefix, trustedRolePrefix, role, roleMembers,\n                            tenantDomain, principalName, auditRef, auditDetails, caller);\n                }\n\n                auditDetails.append(\"]}\");\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, tenantDomain);\n\n                // audit log the request\n\n                auditLogRequest(ctx, tenantDomain, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        provSvcDomain, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteTenancy(ResourceContext ctx, String tenantDomain, String provSvcDomain,\n            String provSvcName, String resourceGroup, String auditRef, String caller) {\n\n        // create list of policies and delete them from the tenant domain\n        // have to get all policies that match \"tenant.<provider>.*\"\n        // ex: tenancy.weather.storage.admin\n\n        String rnamePrefix = ZMSUtils.getProviderResourceGroupRolePrefix(provSvcDomain, provSvcName,\n                resourceGroup);\n\n        final String pnamePrefix = \"tenancy.\" + rnamePrefix;\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, tenantDomain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TENANCY);\n\n                // first let's process and remove any policies that start with our\n                // provider prefix\n\n                List<String> pnames = con.listPolicies(tenantDomain, null);\n\n                for (String pname : pnames) {\n\n                    if (!validResourceGroupObjectToDelete(pname, pnamePrefix)) {\n                        if (LOG.isDebugEnabled()) {\n                            LOG.debug(\"{}: --ignore policy {}\", caller, pname);\n                        }\n                        continue;\n                    }\n\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"{}: --delete policy {}\", caller, pname);\n                    }\n\n                    con.deletePolicy(tenantDomain, pname);\n                }\n\n                // now we're going to find any roles that have the provider prefix as\n                // well but we're going to be careful about removing them. We'll check\n                // and if we have no more policies referencing them then we'll remove\n\n                List<String> rnames = con.listRoles(tenantDomain);\n                for (String rname : rnames) {\n\n                    if (!validResourceGroupObjectToDelete(rname, rnamePrefix)) {\n                        if (LOG.isDebugEnabled()) {\n                            LOG.debug(\"{}: --ignore role {}\", caller, rname);\n                        }\n                        continue;\n                    }\n\n                    if (!con.listPolicies(tenantDomain, rname).isEmpty()) {\n                        if (LOG.isDebugEnabled()) {\n                            LOG.debug(\"{}: --ignore role {} due to active references\", caller, rname);\n                        }\n                        continue;\n                    }\n\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"{}: --delete role {}\", caller, rname);\n                    }\n\n                    con.deleteRole(tenantDomain, rname);\n                }\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, tenantDomain);\n\n                // audit log the request\n\n                auditLogRequest(ctx, tenantDomain, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        ResourceUtils.entityResourceName(provSvcDomain, provSvcName), null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    boolean validResourceGroupObjectToDelete(String name, String prefix) {\n\n        if (!name.startsWith(prefix)) {\n            return false;\n        }\n\n        // the suffix must be the action which should only be\n        // simple-name thus it cannot contain any more .'s\n        // otherwise we don't want to make a mistake\n        // and match substring resource groups - e.g:\n        // system.engine and system.engine.test\n\n        return (name.indexOf('.', prefix.length()) == -1);\n    }\n\n    void executeDeleteTenantRoles(ResourceContext ctx, String provSvcDomain, String provSvcName,\n            String tenantDomain, String resourceGroup, String auditRef, String caller) {\n\n        // look for this tenants roles, ex: storage.tenant.sports.reader\n\n        String rolePrefix = ZMSUtils.getTenantResourceGroupRolePrefix(provSvcName, tenantDomain, resourceGroup);\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, provSvcDomain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_TENANCY);\n\n                // find roles and policies matching the prefix\n\n                List<String> rnames = con.listRoles(provSvcDomain);\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"tenant-roles\\\": [\");\n                boolean firstEntry = true;\n                for (String rname : rnames) {\n                    if (isTrustRoleForTenant(con, provSvcDomain, rname, rolePrefix,\n                            resourceGroup, tenantDomain)) {\n\n                        // good, its exactly what we are looking for\n\n                        con.deleteRole(provSvcDomain, rname);\n                        con.deletePolicy(provSvcDomain, rname);\n                        firstEntry = auditLogString(auditDetails, rname, firstEntry);\n                    }\n                }\n                auditDetails.append(\"]}\");\n\n                // update our domain time-stamp and save changes\n\n                saveChanges(con, provSvcDomain);\n\n                // audit log the request\n\n                auditLogRequest(ctx, tenantDomain, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        provSvcDomain, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    boolean isTrustRoleForTenant(ObjectStoreConnection con, String provSvcDomain, String roleName,\n            String rolePrefix, String resourceGroup, String tenantDomain) {\n\n        // first make sure the role name starts with the given prefix\n\n        if (!isTenantRolePrefixMatch(con, roleName, rolePrefix, resourceGroup, tenantDomain)) {\n            return false;\n        }\n\n        Role role = con.getRole(provSvcDomain, roleName);\n        if (role == null) {\n            return false;\n        }\n\n        // ensure it is a trust role for the tenant\n\n        String trustDom = role.getTrust();\n        return trustDom != null && trustDom.equals(tenantDomain);\n\n    }\n\n    boolean isTrustRoleForTenant(String provSvcDomain, String roleName, String rolePrefix,\n            String resourceGroup, String tenantDomain) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return isTrustRoleForTenant(con, provSvcDomain, roleName, rolePrefix, resourceGroup, tenantDomain);\n        }\n    }\n\n    boolean isTenantRolePrefixMatch(String roleName, String rolePrefix, String resourceGroup,\n            String tenantDomain) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return isTenantRolePrefixMatch(con, roleName, rolePrefix, resourceGroup, tenantDomain);\n        }\n    }\n\n    boolean isTenantRolePrefixMatch(ObjectStoreConnection con, String roleName, String rolePrefix,\n            String resourceGroup, String tenantDomain) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"isTenantRolePrefixMatch: role-name={}, role-prefix={}, resource-group={}, tenant-domain={}\",\n                    roleName, rolePrefix, resourceGroup, tenantDomain);\n        }\n\n        // first make sure the role name starts with the given prefix\n\n        if (!roleName.startsWith(rolePrefix)) {\n            return false;\n        }\n\n        // if we're dealing with a resource group then we need\n        // to make sure we're not going to match a substring\n        // resource group. Since we expect to see a SimpleName\n        // action after the name, if we get another '.' then\n        // we're dealing with a substring so the role does\n        // match the expected format\n\n        if (resourceGroup != null) {\n            return (roleName.indexOf('.', rolePrefix.length()) == -1);\n        }\n\n        // otherwise we're going to split the remaining value\n        // into components. If we have 2 components then we'll\n        // check if we have a domain for the first component\n        // if we don't then it's a resource group and as such\n        // it can be removed otherwise, we'll leave it alone\n\n        String[] comps = roleName.substring(rolePrefix.length()).split(\"\\\\.\");\n        if (comps.length == 2) {\n\n            // check to see if we have a subdomain - if we do then\n            // we're not going to include this role as we don't know\n            // for sure if this for a resource group or not\n\n            String subDomain = tenantDomain + \".\" + comps[0];\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"isTenantRolePrefixMatch: verifying tenant subdomain: {}\", subDomain);\n            }\n\n            return con.getDomain(subDomain) == null;\n\n        } else {\n\n            // if we have more than 2 subcomponents then we're\n            // definitely not dealing with resource groups\n\n            return comps.length <= 2;\n        }\n\n    }\n\n    public AthenzDomain getAthenzDomain(final String domainName, boolean masterCopy) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, masterCopy)) {\n            return getAthenzDomain(con, domainName);\n        }\n    }\n\n    AthenzDomain getAthenzDomain(ObjectStoreConnection con, final String domainName) {\n\n        // first check to see if we our data is in the cache\n\n        AthenzDomain athenzDomain = getAthenzDomainFromCache(con, domainName);\n        if (athenzDomain != null) {\n            return athenzDomain;\n        }\n\n        athenzDomain = con.getAthenzDomain(domainName);\n        athenzDomain.setRoleMemberPrincipalTypes(zmsConfig.getUserDomainPrefix(), zmsConfig.getAddlUserCheckDomainPrefixList());\n\n        DataCache dataCache = new DataCache(athenzDomain,\n                athenzDomain.getDomain().getModified().millis());\n        cacheStore.put(domainName, dataCache);\n\n        return athenzDomain;\n    }\n\n    DomainMetaList listModifiedDomains(long modifiedSince) {\n\n        // since this is the operation executed by ZTS servers to\n        // retrieve latest domain changes, we're going to use\n        // the read-write store as oppose to read-only store to\n        // get our up-to-date data\n\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            return con.listModifiedDomains(modifiedSince);\n        }\n    }\n\n    boolean auditLogSeparator(StringBuilder auditDetails, boolean firstEntry) {\n        if (!firstEntry) {\n            auditDetails.append(',');\n        }\n        // regardless of the current state, the new state is no\n        // longer the first entry so we return false\n        return false;\n    }\n\n    void auditLogStrings(StringBuilder auditDetails, String label, Collection<String> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (String value : values) {\n            firstEntry = auditLogString(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    boolean auditLogString(StringBuilder auditDetails, String value, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append('\\\"').append(value).append('\\\"');\n        return firstEntry;\n    }\n\n    void auditLogRoleMembers(StringBuilder auditDetails, String label,\n            Collection<RoleMember> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (RoleMember value : values) {\n            firstEntry = auditLogRoleMember(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    boolean auditLogRoleMember(StringBuilder auditDetails, RoleMember roleMember, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"{\\\"member\\\": \\\"\").append(roleMember.getMemberName()).append('\"');\n        if (roleMember.getExpiration() != null) {\n            auditDetails.append(\", \\\"expiration\\\": \\\"\").append(roleMember.getExpiration().toString()).append('\"');\n        }\n        if (roleMember.getReviewReminder() != null) {\n            auditDetails.append(\", \\\"reminder\\\": \\\"\").append(roleMember.getReviewReminder().toString()).append('\"');\n        }\n        auditDetails.append(\", \\\"approved\\\": \");\n        auditDetails.append(roleMember.getApproved() == Boolean.FALSE ? \"false\" : \"true\");\n        auditDetails.append(\", \\\"system-disabled\\\": \");\n        auditDetails.append(roleMember.getSystemDisabled() == null ? 0 : roleMember.getSystemDisabled());\n        auditDetails.append(\"}\");\n        return firstEntry;\n    }\n\n    void auditLogGroupMembers(StringBuilder auditDetails, String label,\n                             Collection<GroupMember> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (GroupMember value : values) {\n            firstEntry = auditLogGroupMember(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    boolean auditLogGroupMember(StringBuilder auditDetails, GroupMember groupMember, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"{\\\"member\\\": \\\"\").append(groupMember.getMemberName()).append('\"');\n        if (groupMember.getExpiration() != null) {\n            auditDetails.append(\", \\\"expiration\\\": \\\"\").append(groupMember.getExpiration().toString()).append('\"');\n        }\n        auditDetails.append(\", \\\"approved\\\": \");\n        auditDetails.append(auditLogBooleanDefault(groupMember.getApproved(), Boolean.FALSE));\n        auditDetails.append(\", \\\"system-disabled\\\": \");\n        auditDetails.append(groupMember.getSystemDisabled() == null ? 0 : groupMember.getSystemDisabled());\n        auditDetails.append(\"}\");\n        return firstEntry;\n    }\n\n    String auditLogBooleanDefault(Boolean value, Boolean checkValue) {\n        if (checkValue == Boolean.TRUE) {\n            return value == Boolean.TRUE ? \"true\" : \"false\";\n        } else {\n            return value == Boolean.FALSE ? \"false\" : \"true\";\n        }\n    }\n\n    void auditLogPublicKeyEntries(StringBuilder auditDetails, String label,\n            List<PublicKeyEntry> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (PublicKeyEntry value : values) {\n            firstEntry = auditLogPublicKeyEntry(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    void auditLogPublicKeyEntries(StringBuilder auditDetails, String label, Set<String> values,\n            Map<String, PublicKeyEntry> publicKeysMap) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (String value : values) {\n            firstEntry = auditLogPublicKeyEntry(auditDetails, publicKeysMap.get(value), firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    void auditLogPublicKeyEntries(StringBuilder auditDetails, String label, Set<String> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (String value : values) {\n            firstEntry = auditLogPublicKeyEntry(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    boolean auditLogPublicKeyEntry(StringBuilder auditDetails, PublicKeyEntry publicKey, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"{\\\"key\\\": \\\"\").append(publicKey.getKey())\n            .append(\"\\\", \\\"id\\\": \\\"\").append(publicKey.getId()).append(\"\\\"}\");\n        return firstEntry;\n    }\n\n    boolean auditLogPublicKeyEntry(StringBuilder auditDetails, String publicKeyId, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"{\\\"id\\\": \\\"\").append(publicKeyId).append(\"\\\"}\");\n        return firstEntry;\n    }\n\n    void auditLogPolicy(StringBuilder auditDetails, Policy policy, String label)  {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(policy.getName())\n                .append(\"\\\", \\\"modified\\\": \\\"\").append(policy.getModified()).append('\"');\n        if (policy.getAssertions() != null) {\n            auditLogAssertions(auditDetails, label, policy.getAssertions());\n        }\n        auditDetails.append(\"}\");\n    }\n\n    void auditLogAssertions(StringBuilder auditDetails, String label, Collection<Assertion> values) {\n        auditDetails.append(\", \\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (Assertion value : values) {\n            firstEntry = auditLogAssertion(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(']');\n    }\n\n    boolean auditLogAssertion(StringBuilder auditDetails, Assertion assertion, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        String assertionEffect = \"ALLOW\";\n        if (assertion.getEffect() != null) {\n            assertionEffect = assertion.getEffect().toString();\n        }\n        auditDetails.append(\"{\\\"role\\\": \\\"\").append(assertion.getRole())\n                .append(\"\\\", \\\"action\\\": \\\"\").append(assertion.getAction())\n                .append(\"\\\", \\\"effect\\\": \\\"\").append(assertionEffect)\n                .append(\"\\\", \\\"resource\\\": \\\"\").append(assertion.getResource())\n                .append(\"\\\"}\");\n        return firstEntry;\n    }\n\n    void auditLogDomain(StringBuilder auditDetails, Domain domain) {\n        auditDetails.append(\"{\\\"description\\\": \\\"\").append(domain.getDescription())\n                .append(\"\\\", \\\"org\\\": \\\"\").append(domain.getOrg())\n                .append(\"\\\", \\\"auditEnabled\\\": \\\"\").append(domain.getAuditEnabled())\n                .append(\"\\\", \\\"enabled\\\": \\\"\").append(domain.getEnabled())\n                .append(\"\\\", \\\"account\\\": \\\"\").append(domain.getAccount())\n                .append(\"\\\", \\\"acctId\\\": \\\"\").append(domain.getApplicationId())\n                .append(\"\\\", \\\"ypmid\\\": \\\"\").append(domain.getYpmId())\n                .append(\"\\\", \\\"id\\\": \\\"\").append(domain.getId())\n                .append(\"\\\", \\\"memberExpiryDays\\\": \\\"\").append(domain.getMemberExpiryDays())\n                .append(\"\\\", \\\"serviceExpiryDays\\\": \\\"\").append(domain.getServiceExpiryDays())\n                .append(\"\\\", \\\"tokenExpiryMins\\\": \\\"\").append(domain.getTokenExpiryMins())\n                .append(\"\\\", \\\"serviceCertExpiryMins\\\": \\\"\").append(domain.getServiceCertExpiryMins())\n                .append(\"\\\", \\\"roleCertExpiryMins\\\": \\\"\").append(domain.getRoleCertExpiryMins())\n                .append(\"\\\", \\\"signAlgorithm\\\": \\\"\").append(domain.getSignAlgorithm())\n                .append(\"\\\", \\\"userAuthorityFilter\\\": \\\"\").append(domain.getUserAuthorityFilter())\n                .append(\"\\\", \\\"businessService\\\": \\\"\").append(domain.getBusinessService())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogRoleSystemMeta(StringBuilder auditDetails, Role role, String roleName) {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(roleName)\n                .append(\"\\\", \\\"auditEnabled\\\": \\\"\").append(role.getAuditEnabled())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogGroupSystemMeta(StringBuilder auditDetails, Group group, final String groupName) {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(groupName)\n                .append(\"\\\", \\\"auditEnabled\\\": \\\"\").append(group.getAuditEnabled())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogServiceIdentitySystemMeta(StringBuilder auditDetails, ServiceIdentity service, String serviceName) {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(serviceName)\n                .append(\"\\\", \\\"providerEndpoint\\\": \\\"\").append(service.getProviderEndpoint())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogRoleMeta(StringBuilder auditDetails, Role role, String roleName) {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(roleName)\n                .append(\"\\\", \\\"selfServe\\\": \\\"\").append(role.getSelfServe())\n                .append(\"\\\", \\\"memberExpiryDays\\\": \\\"\").append(role.getMemberExpiryDays())\n                .append(\"\\\", \\\"serviceExpiryDays\\\": \\\"\").append(role.getServiceExpiryDays())\n                .append(\"\\\", \\\"groupExpiryDays\\\": \\\"\").append(role.getGroupExpiryDays())\n                .append(\"\\\", \\\"tokenExpiryMins\\\": \\\"\").append(role.getTokenExpiryMins())\n                .append(\"\\\", \\\"certExpiryMins\\\": \\\"\").append(role.getCertExpiryMins())\n                .append(\"\\\", \\\"memberReviewDays\\\": \\\"\").append(role.getMemberReviewDays())\n                .append(\"\\\", \\\"serviceReviewDays\\\": \\\"\").append(role.getServiceReviewDays())\n                .append(\"\\\", \\\"groupReviewDays\\\": \\\"\").append(role.getGroupReviewDays())\n                .append(\"\\\", \\\"reviewEnabled\\\": \\\"\").append(role.getReviewEnabled())\n                .append(\"\\\", \\\"notifyRoles\\\": \\\"\").append(role.getNotifyRoles())\n                .append(\"\\\", \\\"signAlgorithm\\\": \\\"\").append(role.getSignAlgorithm())\n                .append(\"\\\", \\\"userAuthorityFilter\\\": \\\"\").append(role.getUserAuthorityFilter())\n                .append(\"\\\", \\\"userAuthorityExpiration\\\": \\\"\").append(role.getUserAuthorityExpiration())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogGroupMeta(StringBuilder auditDetails, Group group, final String groupName) {\n        auditDetails.append(\"{\\\"name\\\": \\\"\").append(groupName)\n                .append(\"\\\", \\\"selfServe\\\": \\\"\").append(group.getSelfServe())\n                .append(\"\\\", \\\"memberExpiryDays\\\": \\\"\").append(group.getMemberExpiryDays())\n                .append(\"\\\", \\\"serviceExpiryDays\\\": \\\"\").append(group.getServiceExpiryDays())\n                .append(\"\\\", \\\"reviewEnabled\\\": \\\"\").append(group.getReviewEnabled())\n                .append(\"\\\", \\\"notifyRoles\\\": \\\"\").append(group.getNotifyRoles())\n                .append(\"\\\", \\\"userAuthorityFilter\\\": \\\"\").append(group.getUserAuthorityFilter())\n                .append(\"\\\", \\\"userAuthorityExpiration\\\": \\\"\").append(group.getUserAuthorityExpiration())\n                .append(\"\\\"}\");\n    }\n\n    void auditLogAssertionConditions(StringBuilder auditDetails, List<AssertionCondition> assertionConditions, String label)  {\n        auditDetails.append(\"{\\\"\").append(label).append(\"\\\": [\");\n        boolean firstEntry = true;\n        for (AssertionCondition value : assertionConditions) {\n            firstEntry = auditLogAssertionCondition(auditDetails, value, firstEntry);\n        }\n        auditDetails.append(\"]}\");\n    }\n\n    boolean auditLogAssertionCondition(StringBuilder auditDetails, AssertionCondition assertionCondition, boolean firstEntry) {\n\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"{\\\"conditionId\\\": \").append(assertionCondition.getId())\n                .append(\", \\\"conditionsMap\\\": {\");\n        boolean innerFirstEntry = true;\n        for (String key : assertionCondition.getConditionsMap().keySet()) {\n            innerFirstEntry = auditLogAssertionConditionData(auditDetails, assertionCondition.getConditionsMap().get(key), key, innerFirstEntry);\n        }\n        auditDetails.append(\"}}\");\n        return firstEntry;\n    }\n\n    boolean auditLogAssertionConditionData(StringBuilder auditDetails, AssertionConditionData assertionConditionData, String conditionKey, boolean firstEntry) {\n        firstEntry = auditLogSeparator(auditDetails, firstEntry);\n        auditDetails.append(\"\\\"\").append(conditionKey)\n                .append(\"\\\": {\\\"operator\\\": \\\"\").append(assertionConditionData.getOperator().name())\n                .append(\"\\\", \\\"value\\\": \\\"\").append(assertionConditionData.getValue())\n                .append(\"\\\"}\");\n        return firstEntry;\n    }\n\n    void executePutQuota(ResourceContext ctx, String domainName, Quota quota,\n            String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // process our insert quota. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (con.getQuota(domainName) != null) {\n                    con.updateQuota(domainName, quota);\n                } else {\n                    con.insertQuota(domainName, quota);\n                }\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executeDeleteQuota(ResourceContext ctx, String domainName, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // process our delete quota request - it's a single\n                // operation so no need to make it a transaction\n\n                if (!con.deleteQuota(domainName)) {\n                    throw ZMSUtils.notFoundError(caller + \": unable to delete quota: \" + domainName, caller);\n                }\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        domainName, null);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    public Quota getQuota(String domainName) {\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return quotaCheck.getDomainQuota(con, domainName);\n        }\n    }\n\n    public void executePutRoleSystemMeta(ResourceContext ctx, final String domainName, final String roleName,\n           RoleSystemMeta meta, final String attribute, final String auditRef, final String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                Domain domain = con.getDomain(domainName);\n                if (domain == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": Unknown domain: \" + domainName, caller);\n                }\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_ROLE);\n\n                if (domain.getAuditEnabled() != Boolean.TRUE) {\n                    throw ZMSUtils.requestError(caller + \": auditEnabled flag not set for domain: \" + domainName + \" to add it on the role: \" + roleName, caller);\n                }\n\n                Role originalRole = getRole(con, domainName, roleName, false, false, false);\n\n                // now process the request. first we're going to make a\n                // copy of our role\n\n                Role updatedRole = new Role()\n                        .setName(originalRole.getName())\n                        .setAuditEnabled(originalRole.getAuditEnabled())\n                        .setTrust(originalRole.getTrust())\n                        .setSelfServe(originalRole.getSelfServe())\n                        .setMemberExpiryDays(originalRole.getMemberExpiryDays())\n                        .setServiceExpiryDays(originalRole.getServiceExpiryDays())\n                        .setGroupExpiryDays(originalRole.getGroupExpiryDays())\n                        .setGroupReviewDays(originalRole.getGroupReviewDays())\n                        .setTokenExpiryMins(originalRole.getTokenExpiryMins())\n                        .setCertExpiryMins(originalRole.getCertExpiryMins())\n                        .setMemberReviewDays(originalRole.getMemberReviewDays())\n                        .setServiceReviewDays(originalRole.getServiceReviewDays())\n                        .setSignAlgorithm(originalRole.getSignAlgorithm())\n                        .setReviewEnabled(originalRole.getReviewEnabled())\n                        .setNotifyRoles(originalRole.getNotifyRoles());\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                updateRoleSystemMetaFields(con, updatedRole, originalRole, attribute, meta, ctx.getApiName());\n\n                con.updateRole(domainName, updatedRole);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogRoleSystemMeta(auditDetails, updatedRole, roleName);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    public void executePutGroupSystemMeta(ResourceContext ctx, final String domainName, final String groupName,\n                                          GroupSystemMeta meta, final String attribute, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                Domain domain = con.getDomain(domainName);\n                if (domain == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\": Unknown domain: \" + domainName, ctx.getApiName());\n                }\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domain, auditRef, ctx.getApiName(), getPrincipalName(ctx), AUDIT_TYPE_GROUP);\n\n                if (domain.getAuditEnabled() != Boolean.TRUE) {\n                    throw ZMSUtils.requestError(\"auditEnabled flag not set for domain: \" + domainName +\n                            \" to add it on the group: \" + groupName, ctx.getApiName());\n                }\n\n                Group originalGroup = getGroup(con, domainName, groupName, false, false);\n\n                // now process the request. first we're going to make a\n                // copy of our group\n\n                Group updatedGroup = new Group()\n                        .setName(originalGroup.getName())\n                        .setAuditEnabled(originalGroup.getAuditEnabled())\n                        .setSelfServe(originalGroup.getSelfServe())\n                        .setReviewEnabled(originalGroup.getReviewEnabled())\n                        .setNotifyRoles(originalGroup.getNotifyRoles())\n                        .setUserAuthorityFilter(originalGroup.getUserAuthorityFilter())\n                        .setUserAuthorityExpiration(originalGroup.getUserAuthorityExpiration())\n                        .setMemberExpiryDays(originalGroup.getMemberExpiryDays())\n                        .setServiceExpiryDays(originalGroup.getServiceExpiryDays());\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                updateGroupSystemMetaFields(updatedGroup, attribute, meta, ctx.getApiName());\n\n                con.updateGroup(domainName, updatedGroup);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogGroupSystemMeta(auditDetails, updatedGroup, groupName);\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    public void executePutServiceIdentitySystemMeta(ResourceContext ctx, String domainName, String serviceName,\n            ServiceIdentitySystemMeta meta, String attribute, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                Domain domain = con.getDomain(domainName);\n                if (domain == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(caller + \": Unknown domain: \" + domainName, caller);\n                }\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domain, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_SERVICE);\n\n                // retrieve our original service identity object\n\n                ServiceIdentity serviceIdentity = getServiceIdentity(con, domainName, serviceName, false);\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                updateServiceIdentitySystemMetaFields(serviceIdentity, attribute, meta, ctx.getApiName());\n\n                con.updateServiceIdentity(domainName, serviceIdentity);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogServiceIdentitySystemMeta(auditDetails, serviceIdentity, serviceName);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void updateRoleMetaFields(Role role, RoleMeta meta) {\n\n        if (meta.getSelfServe() != null) {\n            role.setSelfServe(meta.getSelfServe());\n        }\n        if (meta.getMemberExpiryDays() != null) {\n            role.setMemberExpiryDays(meta.getMemberExpiryDays());\n        }\n        if (meta.getServiceExpiryDays() != null) {\n            role.setServiceExpiryDays(meta.getServiceExpiryDays());\n        }\n        if (meta.getGroupExpiryDays() != null) {\n            role.setGroupExpiryDays(meta.getGroupExpiryDays());\n        }\n        if (meta.getGroupReviewDays() != null) {\n            role.setGroupReviewDays(meta.getGroupReviewDays());\n        }\n        if (meta.getTokenExpiryMins() != null) {\n            role.setTokenExpiryMins(meta.getTokenExpiryMins());\n        }\n        if (meta.getCertExpiryMins() != null) {\n            role.setCertExpiryMins(meta.getCertExpiryMins());\n        }\n        if (meta.getSignAlgorithm() != null) {\n            role.setSignAlgorithm(meta.getSignAlgorithm());\n        }\n        if (meta.getReviewEnabled() != null) {\n            role.setReviewEnabled(meta.getReviewEnabled());\n        }\n        if (meta.getNotifyRoles() != null) {\n            role.setNotifyRoles(meta.getNotifyRoles());\n        }\n        if (meta.getMemberReviewDays() != null) {\n            role.setMemberReviewDays(meta.getMemberReviewDays());\n        }\n        if (meta.getServiceReviewDays() != null) {\n            role.setServiceReviewDays(meta.getServiceReviewDays());\n        }\n        if (meta.getUserAuthorityFilter() != null) {\n            role.setUserAuthorityFilter(meta.getUserAuthorityFilter());\n        }\n        if (meta.getUserAuthorityExpiration() != null) {\n            role.setUserAuthorityExpiration(meta.getUserAuthorityExpiration());\n        }\n        if (meta.getTags() != null) {\n            role.setTags(meta.getTags());\n        }\n    }\n\n    public void executePutRoleMeta(ResourceContext ctx, String domainName, String roleName, Role originalRole,\n                                   RoleMeta meta, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                checkObjectAuditEnabled(con, originalRole.getAuditEnabled(), originalRole.getName(),\n                        auditRef, caller, getPrincipalName(ctx));\n\n                // now process the request. first we're going to make a\n                // copy of our role\n\n                Role updatedRole = new Role()\n                        .setName(originalRole.getName())\n                        .setAuditEnabled(originalRole.getAuditEnabled())\n                        .setTrust(originalRole.getTrust())\n                        .setSelfServe(originalRole.getSelfServe())\n                        .setMemberExpiryDays(originalRole.getMemberExpiryDays())\n                        .setServiceExpiryDays(originalRole.getServiceExpiryDays())\n                        .setGroupExpiryDays(originalRole.getGroupExpiryDays())\n                        .setGroupReviewDays(originalRole.getGroupReviewDays())\n                        .setTokenExpiryMins(originalRole.getTokenExpiryMins())\n                        .setCertExpiryMins(originalRole.getCertExpiryMins())\n                        .setMemberReviewDays(originalRole.getMemberReviewDays())\n                        .setServiceReviewDays(originalRole.getServiceReviewDays())\n                        .setSignAlgorithm(originalRole.getSignAlgorithm())\n                        .setReviewEnabled(originalRole.getReviewEnabled())\n                        .setNotifyRoles(originalRole.getNotifyRoles())\n                        .setUserAuthorityFilter(originalRole.getUserAuthorityFilter())\n                        .setUserAuthorityExpiration(originalRole.getUserAuthorityExpiration())\n                        .setTags(originalRole.getTags());\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                updateRoleMetaFields(updatedRole, meta);\n\n                con.updateRole(domainName, updatedRole);\n\n                processUpdateRoleTags(updatedRole, originalRole, con, roleName, domainName);\n\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogRoleMeta(auditDetails, updatedRole, roleName);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                // if the role member expiry date or review date has changed then we're going\n                // process all the members in the role and update the expiration and review\n                // date accordingly\n\n                updateRoleMembersDueDates(ctx, con, domainName, roleName, originalRole,\n                        updatedRole, auditRef, caller);\n\n                // if there was a change in the role user attribute filter then we need\n                // to make the necessary changes as well.\n\n                updateRoleMembersSystemDisabledState(ctx, con, domainName, roleName, originalRole,\n                        updatedRole, auditRef, caller);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void updateGroupMetaFields(Group group, GroupMeta meta) {\n\n        // these two fields have default values so after validation\n        // we'll never have nulls\n\n        group.setSelfServe(meta.getSelfServe());\n        group.setReviewEnabled(meta.getReviewEnabled());\n\n        if (meta.getNotifyRoles() != null) {\n            group.setNotifyRoles(meta.getNotifyRoles());\n        }\n        if (meta.getUserAuthorityFilter() != null) {\n            group.setUserAuthorityFilter(meta.getUserAuthorityFilter());\n        }\n        if (meta.getUserAuthorityExpiration() != null) {\n            group.setUserAuthorityExpiration(meta.getUserAuthorityExpiration());\n        }\n        if (meta.getMemberExpiryDays() != null) {\n            group.setMemberExpiryDays(meta.getMemberExpiryDays());\n        }\n        if (meta.getServiceExpiryDays() != null) {\n            group.setServiceExpiryDays(meta.getServiceExpiryDays());\n        }\n    }\n\n    public void executePutGroupMeta(ResourceContext ctx, final String domainName, final String groupName,\n                                    GroupMeta meta, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                Group originalGroup = getGroup(con, domainName, groupName, false, false);\n                if (originalGroup == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"Unknown group: \" + groupName, ctx.getApiName());\n                }\n\n                checkObjectAuditEnabled(con, originalGroup.getAuditEnabled(), originalGroup.getName(),\n                        auditRef, ctx.getApiName(), getPrincipalName(ctx));\n\n                // now process the request. first we're going to make a\n                // copy of our group\n\n                Group updatedGroup = new Group()\n                        .setName(originalGroup.getName())\n                        .setAuditEnabled(originalGroup.getAuditEnabled())\n                        .setSelfServe(originalGroup.getSelfServe())\n                        .setMemberExpiryDays(originalGroup.getMemberExpiryDays())\n                        .setServiceExpiryDays(originalGroup.getServiceExpiryDays())\n                        .setReviewEnabled(originalGroup.getReviewEnabled())\n                        .setNotifyRoles(originalGroup.getNotifyRoles())\n                        .setUserAuthorityFilter(originalGroup.getUserAuthorityFilter())\n                        .setUserAuthorityExpiration(originalGroup.getUserAuthorityExpiration());\n\n                // then we're going to apply the updated fields\n                // from the given object\n\n                updateGroupMetaFields(updatedGroup, meta);\n\n                // if either the filter or the expiry has been removed we need to make\n                // sure the group is not a member in a role that requires it\n\n                validateGroupUserAuthorityAttrRequirements(con, originalGroup, updatedGroup, ctx.getApiName());\n\n                // update the group in the database\n\n                con.updateGroup(domainName, updatedGroup);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogGroupMeta(auditDetails, updatedGroup, groupName);\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_PUT,\n                        domainName, auditDetails.toString());\n\n                // if the group user authority expiration attribute has changed, we're going\n                // process all the members in the group and update the expiration date accordingly\n\n                updateGroupMembersDueDates(ctx, con, domainName, groupName, originalGroup,\n                        updatedGroup, auditRef);\n\n                // if there was a change in the role user attribute filter then we need\n                // to make the necessary changes as well.\n\n                updateGroupMembersSystemDisabledState(ctx, con, domainName, groupName, originalGroup,\n                        updatedGroup, auditRef);\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    String getDomainUserAuthorityFilterFromMap(ObjectStoreConnection con, Map<String, String> domainFitlerMap, final String domainName) {\n        String domainUserAuthorityFilter = domainFitlerMap.get(domainName);\n        if (domainUserAuthorityFilter == null) {\n            final String domainFilter = getDomainUserAuthorityFilter(con, domainName);\n            domainUserAuthorityFilter = domainFilter == null ? \"\" : domainFilter;\n            domainFitlerMap.put(domainName, domainUserAuthorityFilter);\n        }\n        return domainUserAuthorityFilter;\n    }\n\n    void validateGroupUserAuthorityAttrRequirements(ObjectStoreConnection con, Group originalGroup, Group updatedGroup,\n                                                    final String caller)  {\n\n        // check to see if the attribute filter or expiration values have been removed\n\n        boolean filterRemoved = ZMSUtils.userAuthorityAttrMissing(originalGroup.getUserAuthorityFilter(),\n                updatedGroup.getUserAuthorityFilter());\n        boolean expiryRemoved = ZMSUtils.userAuthorityAttrMissing(originalGroup.getUserAuthorityExpiration(),\n                updatedGroup.getUserAuthorityExpiration());\n\n        // if nothing was removed then we're done with our checks\n\n        if (!filterRemoved && !expiryRemoved) {\n            return;\n        }\n\n        // obtain all the roles that have the given group as member\n        // if we get back 404 then the group is not a member of any\n        // role which is success otherwise we'll re-throw the exception\n\n        DomainRoleMember domainRoleMember;\n        try {\n            domainRoleMember = con.getPrincipalRoles(updatedGroup.getName(), null);\n        } catch (ResourceException ex) {\n            if (ex.getCode() == ResourceException.NOT_FOUND) {\n                return;\n            }\n            throw ex;\n        }\n\n        Map<String, String> domainFitlerMap = new HashMap<>();\n        for (MemberRole memberRole : domainRoleMember.getMemberRoles()) {\n\n            // first let's fetch the role and skip if it doesn't exist\n            // (e.g. got deleted right after we run the query)\n\n            Role role = con.getRole(memberRole.getDomainName(), memberRole.getRoleName());\n            if (role == null) {\n                continue;\n            }\n\n            // first process if the user attribute filter was removed\n\n            if (filterRemoved) {\n\n                // if the user attribute filter is removed, then we need to\n                // also obtain the domain level setting\n\n                String domainUserAuthorityFilter = getDomainUserAuthorityFilterFromMap(con, domainFitlerMap, memberRole.getDomainName());\n                final String roleUserAuthorityFilter = ZMSUtils.combineUserAuthorityFilters(role.getUserAuthorityFilter(),\n                        domainUserAuthorityFilter);\n                if (ZMSUtils.userAuthorityAttrMissing(roleUserAuthorityFilter, updatedGroup.getUserAuthorityFilter())) {\n                    throw ZMSUtils.requestError(\"Setting \" + updatedGroup.getUserAuthorityFilter() +\n                            \" user authority filter on the group will not satisfy \"\n                            + ResourceUtils.roleResourceName(memberRole.getDomainName(), memberRole.getRoleName())\n                            + \" role filter requirements\", caller);\n                }\n            }\n\n            // now process if the expiry attribute was removed\n\n            if (expiryRemoved) {\n                if (ZMSUtils.userAuthorityAttrMissing(role.getUserAuthorityExpiration(), updatedGroup.getUserAuthorityExpiration())) {\n                    throw ZMSUtils.requestError(\"Setting \" + updatedGroup.getUserAuthorityExpiration() +\n                            \" user authority expiration on the group will not satisfy \"\n                            + ResourceUtils.roleResourceName(memberRole.getDomainName(), memberRole.getRoleName())\n                            + \" role expiration requirements\", caller);\n                }\n            }\n        }\n    }\n\n    private boolean isEarlierDueDate(long newDueDateMillis, Timestamp currentDueDate) {\n        return newDueDateMillis != 0 && (currentDueDate == null || currentDueDate.millis() > newDueDateMillis);\n    }\n\n    int getMemberUserAuthorityState(final String roleMemberName, final String authorityFilter, int currentState) {\n\n        boolean bUser = ZMSUtils.isUserDomainPrincipal(roleMemberName, zmsConfig.getUserDomainPrefix(),\n                zmsConfig.getAddlUserCheckDomainPrefixList());\n\n        // if we have a user then we'll check if the filter is still valid\n        // for the user. for services, we just ignore from any checks\n\n        int newState;\n        if (bUser) {\n            if (ZMSUtils.isUserAuthorityFilterValid(zmsConfig.getUserAuthority(), authorityFilter, roleMemberName)) {\n                newState = currentState & ~ZMSConsts.ZMS_DISABLED_AUTHORITY_FILTER;\n            } else {\n                newState = currentState | ZMSConsts.ZMS_DISABLED_AUTHORITY_FILTER;\n            }\n        } else {\n            newState = currentState;\n        }\n        return newState;\n    }\n\n    boolean updateUserAuthorityFilter(RoleMember roleMember, final String userAuthorityFilter) {\n\n        int currentState = roleMember.getSystemDisabled() == null ? 0 : roleMember.getSystemDisabled();\n        int newState = getMemberUserAuthorityState(roleMember.getMemberName(), userAuthorityFilter, currentState);\n\n        if (newState != currentState) {\n            roleMember.setSystemDisabled(newState);\n            return true;\n        }\n        return false;\n    }\n\n    boolean updateUserAuthorityFilter(GroupMember groupMember, final String userAuthorityFilter) {\n\n        int currentState = groupMember.getSystemDisabled() == null ? 0 : groupMember.getSystemDisabled();\n        int newState = getMemberUserAuthorityState(groupMember.getMemberName(), userAuthorityFilter, currentState);\n\n        if (newState != currentState) {\n            groupMember.setSystemDisabled(newState);\n            return true;\n        }\n        return false;\n    }\n\n    <T> boolean updateUserAuthorityExpiry(T member, final String userAuthorityExpiry,\n                                          Function<T, Timestamp> expirationGetter,\n                                          BiConsumer<T, Timestamp> expirationSetter,\n                                          Function<T, String> nameGetter) {\n\n        // if we have a service then there is no processing taking place\n        // as the service is not managed by the user authority\n\n        if (!ZMSUtils.isUserDomainPrincipal(nameGetter.apply(member), zmsConfig.getUserDomainPrefix(),\n                zmsConfig.getAddlUserCheckDomainPrefixList())) {\n            return false;\n        }\n\n        Date authorityExpiry = zmsConfig.getUserAuthority().getDateAttribute(nameGetter.apply(member), userAuthorityExpiry);\n\n        // if we don't have a date then we'll expiry the user right away\n        // otherwise we'll set the date as imposed by the user authority\n\n        boolean expiryDateUpdated = false;\n        Timestamp memberExpiry = expirationGetter.apply(member);\n\n        if (authorityExpiry == null) {\n\n            // we'll update the expiration date to be the current time\n            // if the user doesn't have one or it's expires sometime\n            // in the future\n\n            if (memberExpiry == null || memberExpiry.millis() > System.currentTimeMillis()) {\n                expirationSetter.accept(member, Timestamp.fromCurrentTime());\n                expiryDateUpdated = true;\n            }\n        } else {\n\n            // update the expiration date if it does not match to the\n            // value specified by the user authority value\n\n            if (memberExpiry == null || memberExpiry.millis() != authorityExpiry.getTime()) {\n                expirationSetter.accept(member, Timestamp.fromDate(authorityExpiry));\n                expiryDateUpdated = true;\n            }\n        }\n        return expiryDateUpdated;\n    }\n\n    boolean updateUserAuthorityExpiry(RoleMember roleMember, final String userAuthorityExpiry) {\n        return updateUserAuthorityExpiry(roleMember,\n                userAuthorityExpiry,\n                member -> member.getExpiration(),\n                (member, timestamp) -> member.setExpiration(timestamp),\n                member -> member.getMemberName());\n    }\n\n    boolean updateUserAuthorityExpiry(GroupMember groupMember, final String userAuthorityExpiry) {\n        return updateUserAuthorityExpiry(groupMember,\n                userAuthorityExpiry,\n                member -> member.getExpiration(),\n                (member, timestamp) -> member.setExpiration(timestamp),\n                member -> member.getMemberName());\n    }\n\n    List<RoleMember> getRoleMembersWithUpdatedDisabledState(List<RoleMember> roleMembers, final String roleUserAuthorityFilter,\n                                                            final String domainUserAuthorityFilter) {\n\n        List<RoleMember> roleMembersWithUpdatedDisabledStates = new ArrayList<>();\n\n        // combine the user and domain authority lists to have a single value\n\n        final String userAuthorityFilter = ZMSUtils.combineUserAuthorityFilters(roleUserAuthorityFilter,\n                domainUserAuthorityFilter);\n\n        // if the authority filter is null or empty then we're going to go\n        // through all of the members and remove the system disabled bit\n        // set for user authority\n\n        for (RoleMember roleMember : roleMembers) {\n\n            int currentState = roleMember.getSystemDisabled() == null ? 0 : roleMember.getSystemDisabled();\n\n            // if the filter is disabled then we're going through the list and\n            // make sure the disabled bit for the filter is unset\n\n            int newState;\n            if (userAuthorityFilter == null) {\n                newState = currentState & ~ZMSConsts.ZMS_DISABLED_AUTHORITY_FILTER;\n            } else {\n                newState = getMemberUserAuthorityState(roleMember.getMemberName(), userAuthorityFilter, currentState);\n            }\n\n            if (newState != currentState) {\n                roleMember.setSystemDisabled(newState);\n                roleMembersWithUpdatedDisabledStates.add(roleMember);\n            }\n        }\n\n        return roleMembersWithUpdatedDisabledStates;\n    }\n\n    List<GroupMember> getGroupMembersWithUpdatedDisabledState(List<GroupMember> groupMembers,\n                                                              final String groupUserAuthorityFilter,\n                                                              final String domainUserAuthorityFilter) {\n\n        List<GroupMember> groupMembersWithUpdatedDisabledStates = new ArrayList<>();\n\n        // combine the user and domain authority lists to have a single value\n\n        final String userAuthorityFilter = ZMSUtils.combineUserAuthorityFilters(groupUserAuthorityFilter,\n                domainUserAuthorityFilter);\n\n        // if the authority filter is null or empty then we're going to go\n        // through all of the members and remove the system disabled bit\n        // set for user authority\n\n        for (GroupMember groupMember : groupMembers) {\n\n            int currentState = groupMember.getSystemDisabled() == null ? 0 : groupMember.getSystemDisabled();\n\n            // if the filter is disabled then we're going through the list and\n            // make sure the disabled bit for the filter is unset\n\n            int newState;\n            if (userAuthorityFilter == null) {\n                newState = currentState & ~ZMSConsts.ZMS_DISABLED_AUTHORITY_FILTER;\n            } else {\n                newState = getMemberUserAuthorityState(groupMember.getMemberName(), userAuthorityFilter, currentState);\n            }\n\n            if (newState != currentState) {\n                groupMember.setSystemDisabled(newState);\n                groupMembersWithUpdatedDisabledStates.add(groupMember);\n            }\n        }\n\n        return groupMembersWithUpdatedDisabledStates;\n    }\n\n    List<GroupMember> getGroupMembersWithUpdatedDueDates(List<GroupMember> groupMembers, Timestamp userExpiration,\n                                                         long userExpiryMillis, Timestamp serviceExpiration, long serviceExpiryMillis,\n                                                         final String userAuthorityExpiry) {\n\n        return getMembersWithUpdatedDueDates(\n                groupMembers,\n                userExpiration,\n                userExpiryMillis,\n                serviceExpiration,\n                serviceExpiryMillis,\n                null,\n                0,\n                null,\n                0,\n                null,\n                0,\n                userAuthorityExpiry,\n                null,\n                0,\n                member -> member.getExpiration(),\n                member -> null,\n                (member, timestamp) -> member.setExpiration(timestamp),\n                (member, timestamp) -> { },\n                member -> member.getMemberName());\n    }\n\n    <T> List<T> getMembersWithUpdatedDueDates(List<T> members, Timestamp userExpiration,\n                                          long userExpiryMillis, Timestamp serviceExpiration, long serviceExpiryMillis,\n                                          Timestamp groupExpiration, long groupExpiryMillis, Timestamp userReview,\n                                          long userReviewMillis, Timestamp serviceReview, long serviceReviewMillis,\n                                              final String userAuthorityExpiry, Timestamp groupReview, long groupReviewMillis,\n                                              Function<T, Timestamp> expirationGetter,\n                                              Function<T, Timestamp> reviewReminderGetter,\n                                              BiConsumer<T, Timestamp> expirationSetter,\n                                              BiConsumer<T, Timestamp> reviewReminderSetter,\n                                              Function<T, String> nameGetter) {\n        List<T> membersWithUpdatedDueDates = new ArrayList<>();\n        for (T member : members) {\n            Timestamp expiration = expirationGetter.apply(member);\n            Timestamp reviewDate = reviewReminderGetter.apply(member);\n            boolean dueDateUpdated = false;\n\n            switch (ZMSUtils.principalType(nameGetter.apply(member), zmsConfig.getUserDomainPrefix(),\n                    zmsConfig.getAddlUserCheckDomainPrefixList())) {\n\n                case USER:\n\n                    if (isEarlierDueDate(userExpiryMillis, expiration)) {\n                        expirationSetter.accept(member, userExpiration);\n                        dueDateUpdated = true;\n                    }\n                    if (isEarlierDueDate(userReviewMillis, reviewDate)) {\n                        reviewReminderSetter.accept(member, userReview);\n                        dueDateUpdated = true;\n                    }\n\n                    // if we have a user filter and/or expiry configured we need\n                    // to make sure that the user still satisfies the filter\n                    // otherwise we'll just expire the user right away\n\n                    if (userAuthorityExpiry != null && updateUserAuthorityExpiry(\n                            member,\n                            userAuthorityExpiry,\n                            expirationGetter,\n                            expirationSetter,\n                            nameGetter)) {\n                        dueDateUpdated = true;\n                    }\n\n                    break;\n\n                case GROUP:\n\n                    if (isEarlierDueDate(groupExpiryMillis, expiration)) {\n                        expirationSetter.accept(member, groupExpiration);\n                        dueDateUpdated = true;\n                    }\n                    if (isEarlierDueDate(groupReviewMillis, reviewDate)) {\n                        reviewReminderSetter.accept(member, groupReview);\n                        dueDateUpdated = true;\n                    }\n                    break;\n\n                case SERVICE:\n\n                    if (isEarlierDueDate(serviceExpiryMillis, expiration)) {\n                        expirationSetter.accept(member, serviceExpiration);\n                        dueDateUpdated = true;\n                    }\n                    if (isEarlierDueDate(serviceReviewMillis, reviewDate)) {\n                        reviewReminderSetter.accept(member, serviceReview);\n                        dueDateUpdated = true;\n                    }\n                    break;\n            }\n\n            if (dueDateUpdated) {\n                membersWithUpdatedDueDates.add(member);\n            }\n        }\n\n        return membersWithUpdatedDueDates;\n    }\n\n    List<RoleMember> getRoleMembersWithUpdatedDueDates(List<RoleMember> roleMembers, Timestamp userExpiration,\n            long userExpiryMillis, Timestamp serviceExpiration, long serviceExpiryMillis,\n            Timestamp groupExpiration, long groupExpiryMillis, Timestamp userReview,\n            long userReviewMillis, Timestamp serviceReview, long serviceReviewMillis,\n            final String userAuthorityExpiry, Timestamp groupReview, long groupReviewMillis) {\n\n        return getMembersWithUpdatedDueDates(\n                roleMembers,\n                userExpiration,\n                userExpiryMillis,\n                serviceExpiration,\n                serviceExpiryMillis,\n                groupExpiration,\n                groupExpiryMillis,\n                userReview,\n                userReviewMillis,\n                serviceReview,\n                serviceReviewMillis,\n                userAuthorityExpiry,\n                groupReview,\n                groupReviewMillis,\n                member -> member.getExpiration(),\n                member -> member.getReviewReminder(),\n                (member, timestamp) -> member.setExpiration(timestamp),\n                (member, timestamp) -> member.setReviewReminder(timestamp),\n                member -> member.getMemberName());\n    }\n\n    private boolean insertRoleMembers(ResourceContext ctx, ObjectStoreConnection con, List<RoleMember> roleMembers,\n                                      final String domainName, final String roleName, final String principal,\n                                      final String auditRef, final String caller) {\n\n        boolean bDataChanged = false;\n        for (RoleMember roleMember : roleMembers) {\n            try {\n                if (!con.insertRoleMember(domainName, roleName, roleMember, principal, auditRef)) {\n                    LOG.error(\"unable to update member {}\", roleMember.getMemberName());\n                    continue;\n                }\n            } catch (Exception ex) {\n                LOG.error(\"unable to update member {} error: {}\", roleMember.getMemberName(), ex.getMessage());\n                continue;\n            }\n\n            // audit log the request\n\n            StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n            auditLogRoleMember(auditDetails, roleMember, true);\n            auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT, roleName,\n                    auditDetails.toString());\n\n            bDataChanged = true;\n        }\n\n        return bDataChanged;\n    }\n\n    boolean insertGroupMembers(ResourceContext ctx, ObjectStoreConnection con, List<GroupMember> groupMembers,\n                               final String domainName, final String groupName, final String principal,\n                               final String auditRef, final String caller) {\n\n        boolean bDataChanged = false;\n        for (GroupMember groupMember : groupMembers) {\n            try {\n                if (!con.insertGroupMember(domainName, groupName, groupMember, principal, auditRef)) {\n                    LOG.error(\"unable to update group member {}\", groupMember.getMemberName());\n                    continue;\n                }\n            } catch (Exception ex) {\n                LOG.error(\"unable to update member {} error: {}\", groupMember.getMemberName(), ex.getMessage());\n                continue;\n            }\n\n            // audit log the request\n\n            StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n            auditLogGroupMember(auditDetails, groupMember, true);\n            auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT, groupName,\n                    auditDetails.toString());\n\n            bDataChanged = true;\n        }\n\n        return bDataChanged;\n    }\n\n    boolean updateRoleMemberDisabledState(ResourceContext ctx, ObjectStoreConnection con, List<RoleMember> roleMembers,\n                                          final String domainName, final String roleName, final String principal,\n                                          final String auditRef, final String caller) {\n\n        boolean bDataChanged = false;\n        for (RoleMember roleMember : roleMembers) {\n            try {\n                if (!con.updateRoleMemberDisabledState(domainName, roleName, roleMember.getMemberName(), principal,\n                        roleMember.getSystemDisabled(), auditRef)) {\n                    LOG.error(\"unable to update member {}\", roleMember.getMemberName());\n                    continue;\n                }\n            } catch (Exception ex) {\n                LOG.error(\"unable to update member {} error: {}\", roleMember.getMemberName(), ex.getMessage());\n                continue;\n            }\n\n            // audit log the request\n\n            StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n            auditLogRoleMember(auditDetails, roleMember, true);\n            auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT, roleName,\n                    auditDetails.toString());\n\n            bDataChanged = true;\n        }\n\n        return bDataChanged;\n    }\n\n    boolean updateGroupMemberDisabledState(ResourceContext ctx, ObjectStoreConnection con, List<GroupMember> groupMembers,\n                                           final String domainName, final String groupName, final String principal,\n                                           final String auditRef, final String caller) {\n\n        boolean bDataChanged = false;\n        for (GroupMember groupMember : groupMembers) {\n            try {\n                if (!con.updateGroupMemberDisabledState(domainName, groupName, groupMember.getMemberName(), principal,\n                        groupMember.getSystemDisabled(), auditRef)) {\n                    LOG.error(\"unable to update group member {}\", groupMember.getMemberName());\n                    continue;\n                }\n            } catch (Exception ex) {\n                LOG.error(\"unable to update group member {} error: {}\", groupMember.getMemberName(), ex.getMessage());\n                continue;\n            }\n\n            // audit log the request\n\n            StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n            auditLogGroupMember(auditDetails, groupMember, true);\n            auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT, groupName,\n                    auditDetails.toString());\n\n            bDataChanged = true;\n        }\n\n        return bDataChanged;\n    }\n\n    boolean isUserAuthorityExpiryChanged(String originalValue, String newValue) {\n\n        // if we don't have a user authority defined then\n        // we assume there are no changes\n\n        if (zmsConfig.getUserAuthority() == null) {\n            return false;\n        }\n\n        // first let's make sure if we're given empty strings\n        // we treat them as nulls\n\n        if (originalValue != null && originalValue.isEmpty()) {\n            originalValue = null;\n        }\n        if (newValue != null && newValue.isEmpty()) {\n            newValue = null;\n        }\n\n        // we're only concerned if the value was either set or changed\n        // if the value was set and now was unset, it has no impact\n        // on the existing members so we're going to treat that as\n        // if the setting was not changed\n\n        if (newValue == null) {\n            return false;\n        } else {\n            return originalValue == null || !originalValue.equalsIgnoreCase(newValue);\n        }\n    }\n\n    boolean isUserAuthorityFilterChanged(String originalValue, String newValue) {\n\n        // if we don't have a user authority defined then\n        // we assume there are no changes\n\n        if (zmsConfig.getUserAuthority() == null) {\n            return false;\n        }\n\n        // first let's make sure if we're given empty strings\n        // we treat them as nulls\n\n        if (originalValue != null && originalValue.isEmpty()) {\n            originalValue = null;\n        }\n        if (newValue != null && newValue.isEmpty()) {\n            newValue = null;\n        }\n\n        if (newValue == null && originalValue == null) {\n            return false;\n        } else if (newValue == null || originalValue == null) {\n            return true;\n        } else {\n            return !originalValue.equalsIgnoreCase(newValue);\n        }\n    }\n\n    void updateRoleMembersSystemDisabledState(ResourceContext ctx, ObjectStoreConnection con, final String domainName,\n                                              final String roleName, Role originalRole, Role updatedRole,\n                                              final String auditRef, final String caller) {\n\n        // if it's a delegated role then we have nothing to do\n\n        if (originalRole.getTrust() != null && !originalRole.getTrust().isEmpty()) {\n            return;\n        }\n\n        // if no role members, then there is nothing to do\n\n        final List<RoleMember> roleMembers = originalRole.getRoleMembers();\n        if (roleMembers == null || roleMembers.isEmpty()) {\n            return;\n        }\n\n        // check if the authority filter has changed otherwise we have\n        // nothing to do\n\n        if (!isUserAuthorityFilterChanged(originalRole.getUserAuthorityFilter(), updatedRole.getUserAuthorityFilter())) {\n            return;\n        }\n\n        final String principal = getPrincipalName(ctx);\n\n        // process our role members and if there were any changes processed then update\n        // our role and domain time-stamps, and invalidate local cache entry\n\n        List<RoleMember> roleMembersWithUpdatedDisabledState = getRoleMembersWithUpdatedDisabledState(roleMembers,\n                updatedRole.getUserAuthorityFilter(), getDomainUserAuthorityFilter(con, domainName));\n        if (updateRoleMemberDisabledState(ctx, con, roleMembersWithUpdatedDisabledState, domainName,\n                roleName, principal, auditRef, caller)) {\n\n            // update our role and domain time-stamps, and invalidate local cache entry\n\n            con.updateRoleModTimestamp(domainName, roleName);\n            con.updateDomainModTimestamp(domainName);\n            cacheStore.invalidate(domainName);\n        }\n    }\n\n    void updateGroupMembersSystemDisabledState(ResourceContext ctx, ObjectStoreConnection con, final String domainName,\n                                               final String groupName, Group originalGroup, Group updatedGroup, final String auditRef) {\n\n        // if no group members, then there is nothing to do\n\n        final List<GroupMember> groupMembers = originalGroup.getGroupMembers();\n        if (groupMembers == null || groupMembers.isEmpty()) {\n            return;\n        }\n\n        // check if the authority filter has changed otherwise we have\n        // nothing to do\n\n        if (!isUserAuthorityFilterChanged(originalGroup.getUserAuthorityFilter(), updatedGroup.getUserAuthorityFilter())) {\n            return;\n        }\n\n        final String principal = getPrincipalName(ctx);\n\n        // process our group members and if there were any changes processed then update\n        // our group and domain time-stamps, and invalidate local cache entry\n\n        List<GroupMember> groupMembersWithUpdatedDisabledState = getGroupMembersWithUpdatedDisabledState(groupMembers,\n                updatedGroup.getUserAuthorityFilter(), getDomainUserAuthorityFilter(con, domainName));\n        if (updateGroupMemberDisabledState(ctx, con, groupMembersWithUpdatedDisabledState, domainName,\n                groupName, principal, auditRef, ctx.getApiName())) {\n\n            // update our group and domain time-stamps, and invalidate local cache entry\n\n            con.updateGroupModTimestamp(domainName, groupName);\n            con.updateDomainModTimestamp(domainName);\n            cacheStore.invalidate(domainName);\n        }\n    }\n\n    String getDomainUserAuthorityFilter(ObjectStoreConnection con, final String domainName) {\n        Domain domain = con.getDomain(domainName);\n        if (domain == null) {\n            return null;\n        }\n        return domain.getUserAuthorityFilter();\n    }\n\n    void updateGroupMembersDueDates(ResourceContext ctx, ObjectStoreConnection con, final String domainName,\n                                    final String groupName, Group originalGroup, Group updatedGroup, final String auditRef) {\n\n        // if no group members, then there is nothing to do\n\n        final List<GroupMember> groupMembers = originalGroup.getGroupMembers();\n        if (groupMembers == null || groupMembers.isEmpty()) {\n            return;\n        }\n\n        // check if the user authority expiration attribute has been\n        // changed in which case we need to verify and update members\n        // accordingly\n\n        boolean userAuthorityExpiryChanged = isUserAuthorityExpiryChanged(originalGroup.getUserAuthorityExpiration(), updatedGroup.getUserAuthorityExpiration());\n\n        // we only need to process the group members if the new due date\n        // is more restrictive than what we had before\n\n        boolean userMemberExpiryDayReduced = isNumOfDaysReduced(originalGroup.getMemberExpiryDays(),\n                updatedGroup.getMemberExpiryDays());\n        boolean serviceMemberExpiryDayReduced = isNumOfDaysReduced(originalGroup.getServiceExpiryDays(),\n                updatedGroup.getServiceExpiryDays());\n\n        if (!userMemberExpiryDayReduced && !serviceMemberExpiryDayReduced && !userAuthorityExpiryChanged) {\n            return;\n        }\n\n        // we're only going to process those role members whose\n        // due date is either not set or longer than the new limit\n\n        long userExpiryMillis = userMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedGroup.getMemberExpiryDays(), TimeUnit.DAYS) : 0;\n        long serviceExpiryMillis = serviceMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedGroup.getServiceExpiryDays(), TimeUnit.DAYS) : 0;\n\n        Timestamp userExpiration = Timestamp.fromMillis(userExpiryMillis);\n        Timestamp serviceExpiration = Timestamp.fromMillis(serviceExpiryMillis);\n\n        final String principal = getPrincipalName(ctx);\n\n        // process our group members and if there were any changes processed then update\n        // our group and domain time-stamps, and invalidate local cache entry\n        final String userAuthorityExpiry = userAuthorityExpiryChanged ? updatedGroup.getUserAuthorityExpiration() : null;\n        List<GroupMember> groupMembersWithUpdatedDueDates = getGroupMembersWithUpdatedDueDates(groupMembers,\n                userExpiration, userExpiryMillis, serviceExpiration, serviceExpiryMillis, userAuthorityExpiry);\n        if (insertGroupMembers(ctx, con, groupMembersWithUpdatedDueDates, domainName,\n                groupName, principal, auditRef, ctx.getApiName())) {\n\n            // update our group and domain time-stamps, and invalidate local cache entry\n\n            con.updateGroupModTimestamp(domainName, groupName);\n            con.updateDomainModTimestamp(domainName);\n            cacheStore.invalidate(domainName);\n        }\n    }\n\n    void updateRoleMembersDueDates(ResourceContext ctx, ObjectStoreConnection con, final String domainName,\n            final String roleName, Role originalRole, Role updatedRole, final String auditRef, final String caller) {\n\n        // if it's a delegated role then we have nothing to do\n\n        if (originalRole.getTrust() != null && !originalRole.getTrust().isEmpty()) {\n            return;\n        }\n\n        // if no role members, then there is nothing to do\n\n        final List<RoleMember> roleMembers = originalRole.getRoleMembers();\n        if (roleMembers == null || roleMembers.isEmpty()) {\n            return;\n        }\n\n        // check if the user authority expiration attribute has been\n        // changed in which case we need to verify and update members\n        // accordingly\n\n        boolean userAuthorityExpiryChanged = isUserAuthorityExpiryChanged(originalRole.getUserAuthorityExpiration(),\n                updatedRole.getUserAuthorityExpiration());\n\n        // we only need to process the role members if the new due date\n        // is more restrictive than what we had before\n\n        boolean userMemberExpiryDayReduced = isNumOfDaysReduced(originalRole.getMemberExpiryDays(),\n                updatedRole.getMemberExpiryDays());\n        boolean serviceMemberExpiryDayReduced = isNumOfDaysReduced(originalRole.getServiceExpiryDays(),\n                updatedRole.getServiceExpiryDays());\n        boolean groupMemberExpiryDayReduced = isNumOfDaysReduced(originalRole.getGroupExpiryDays(),\n                updatedRole.getGroupExpiryDays());\n\n         boolean userMemberReviewDayReduced = isNumOfDaysReduced(originalRole.getMemberReviewDays(),\n                 updatedRole.getMemberReviewDays());\n         boolean serviceMemberReviewDayReduced = isNumOfDaysReduced(originalRole.getServiceReviewDays(),\n                 updatedRole.getServiceReviewDays());\n        boolean groupMemberReviewDayReduced = isNumOfDaysReduced(originalRole.getGroupReviewDays(),\n                updatedRole.getGroupReviewDays());\n\n        if (!userMemberExpiryDayReduced && !serviceMemberExpiryDayReduced &&\n                !groupMemberExpiryDayReduced && !userMemberReviewDayReduced &&\n                !serviceMemberReviewDayReduced && !userAuthorityExpiryChanged &&\n                !groupMemberReviewDayReduced) {\n            return;\n        }\n\n        // we're only going to process those role members whose\n        // due date is either not set or longer than the new limit\n\n        long userExpiryMillis = userMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedRole.getMemberExpiryDays(), TimeUnit.DAYS) : 0;\n        long serviceExpiryMillis = serviceMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedRole.getServiceExpiryDays(), TimeUnit.DAYS) : 0;\n        long groupExpiryMillis = groupMemberExpiryDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedRole.getGroupExpiryDays(), TimeUnit.DAYS) : 0;\n\n         long userReviewMillis = userMemberReviewDayReduced ? System.currentTimeMillis()\n                 + TimeUnit.MILLISECONDS.convert(updatedRole.getMemberReviewDays(), TimeUnit.DAYS) : 0;\n         long serviceReviewMillis = serviceMemberReviewDayReduced ? System.currentTimeMillis()\n                 + TimeUnit.MILLISECONDS.convert(updatedRole.getServiceReviewDays(), TimeUnit.DAYS) : 0;\n        long groupReviewMillis = groupMemberReviewDayReduced ? System.currentTimeMillis()\n                + TimeUnit.MILLISECONDS.convert(updatedRole.getGroupReviewDays(), TimeUnit.DAYS) : 0;\n\n        Timestamp userExpiration = Timestamp.fromMillis(userExpiryMillis);\n        Timestamp serviceExpiration = Timestamp.fromMillis(serviceExpiryMillis);\n        Timestamp groupExpiration = Timestamp.fromMillis(groupExpiryMillis);\n\n        Timestamp userReview = Timestamp.fromMillis(userReviewMillis);\n        Timestamp serviceReview = Timestamp.fromMillis(serviceReviewMillis);\n        Timestamp groupReview = Timestamp.fromMillis(groupReviewMillis);\n\n        final String principal = getPrincipalName(ctx);\n\n        // process our role members and if there were any changes processed then update\n        // our role and domain time-stamps, and invalidate local cache entry\n\n        final String userAuthorityExpiry = userAuthorityExpiryChanged ? updatedRole.getUserAuthorityExpiration() : null;\n        List<RoleMember> roleMembersWithUpdatedDueDates = getRoleMembersWithUpdatedDueDates(roleMembers,\n                userExpiration, userExpiryMillis, serviceExpiration, serviceExpiryMillis, groupExpiration,\n                groupExpiryMillis, userReview, userReviewMillis, serviceReview, serviceReviewMillis,\n                userAuthorityExpiry, groupReview, groupReviewMillis);\n        if (insertRoleMembers(ctx, con, roleMembersWithUpdatedDueDates, domainName,\n                roleName, principal, auditRef, caller)) {\n\n            // update our role and domain time-stamps, and invalidate local cache entry\n\n            con.updateRoleModTimestamp(domainName, roleName);\n            con.updateDomainModTimestamp(domainName);\n            cacheStore.invalidate(domainName);\n        }\n    }\n\n    boolean isNumOfDaysReduced(Integer oldNumberOfDays, Integer newNumberOfDays) {\n        if (newNumberOfDays == null || newNumberOfDays <= 0) {\n            return false;\n        }\n        if (oldNumberOfDays == null || oldNumberOfDays <= 0) {\n            return true;\n        }\n        return newNumberOfDays < oldNumberOfDays;\n    }\n\n    /**\n     * If the role has audit enabled, and user did not provide the auditRef,\n     * an exception will be thrown.\n     **/\n    void checkObjectAuditEnabled(ObjectStoreConnection con, Boolean auditEnabled, final String objectName,\n                                 final String auditRef, final String caller, final String principal) {\n\n        if (auditEnabled == Boolean.TRUE) {\n            if (auditRef == null || auditRef.length() == 0) {\n                con.rollbackChanges();\n                throw ZMSUtils.requestError(caller + \": Audit reference required for object: \" + objectName, caller);\n            }\n\n            if (auditReferenceValidator != null && !auditReferenceValidator.validateReference(auditRef, principal, caller)) {\n                con.rollbackChanges();\n                throw ZMSUtils.requestError(caller + \": Audit reference validation failed for object: \" + objectName +\n                        \", auditRef: \" + auditRef, caller);\n            }\n        }\n    }\n\n    void executePutMembershipDecision(ResourceContext ctx, String domainName, String roleName,\n            RoleMember roleMember, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // make sure the role auditing requirements are met\n\n                Role originalRole = con.getRole(domainName, roleName);\n                if (originalRole == null) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.notFoundError(\"unknown role: \" + roleName, caller);\n                }\n\n                checkObjectAuditEnabled(con, originalRole.getAuditEnabled(), originalRole.getName(),\n                        auditRef, caller, principal);\n\n                // process our confirm role member support\n\n                if (!con.confirmRoleMember(domainName, roleName, roleMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(\"unable to apply role membership decision for member: \" +\n                            roleMember.getMemberName() + \" and role: \" + roleName, caller);\n                }\n\n                // update our domain time-stamp and save changes\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogRoleMember(auditDetails, roleMember, true);\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        roleName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutGroupMembershipDecision(ResourceContext ctx, final String domainName, Group group,\n                                           GroupMember groupMember, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                String principal = getPrincipalName(ctx);\n\n                // make sure the role auditing requirements are met\n\n                checkObjectAuditEnabled(con, group.getAuditEnabled(), group.getName(),\n                        auditRef, ctx.getApiName(), principal);\n\n                // process our confirm group member support\n\n                final String groupName = ZMSUtils.extractGroupName(domainName, group.getName());\n                if (!con.confirmGroupMember(domainName, groupName, groupMember, principal, auditRef)) {\n                    con.rollbackChanges();\n                    throw ZMSUtils.requestError(\"unable to apply group membership decision for member: \" +\n                            groupMember.getMemberName() + \" and group: \" + groupName, ctx.getApiName());\n                }\n\n                // update our domain time-stamp and save changes\n\n                con.updateGroupModTimestamp(domainName, groupName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditLogGroupMember(auditDetails, groupMember, true);\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), ZMSConsts.HTTP_PUT,\n                        groupName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    DomainRoleMembership getPendingDomainRoleMembers(final String principal) {\n\n        DomainRoleMembership domainRoleMembership = new DomainRoleMembership();\n        List<DomainRoleMembers> domainRoleMembersList = new ArrayList<>();\n        DomainRoleMembers domainRoleMembers;\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            Map<String, List<DomainRoleMember>> domainRoleMembersMap = con.getPendingDomainRoleMembers(principal);\n            if (domainRoleMembersMap != null) {\n                for (String domain : domainRoleMembersMap.keySet()) {\n                    domainRoleMembers = new DomainRoleMembers();\n                    domainRoleMembers.setDomainName(domain);\n                    domainRoleMembers.setMembers(domainRoleMembersMap.get(domain));\n                    domainRoleMembersList.add(domainRoleMembers);\n                }\n                domainRoleMembership.setDomainRoleMembersList(domainRoleMembersList);\n            }\n        }\n        return domainRoleMembership;\n    }\n\n    DomainGroupMembership getPendingDomainGroupMembers(final String principal) {\n\n        DomainGroupMembership domainGroupMembership = new DomainGroupMembership();\n        List<DomainGroupMembers> domainGroupMembersList = new ArrayList<>();\n        DomainGroupMembers domainGroupMembers;\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            Map<String, List<DomainGroupMember>> domainGroupMembersMap = con.getPendingDomainGroupMembers(principal);\n            if (domainGroupMembersMap != null) {\n                for (String domain : domainGroupMembersMap.keySet()) {\n                    domainGroupMembers = new DomainGroupMembers();\n                    domainGroupMembers.setDomainName(domain);\n                    domainGroupMembers.setMembers(domainGroupMembersMap.get(domain));\n                    domainGroupMembersList.add(domainGroupMembers);\n                }\n                domainGroupMembership.setDomainGroupMembersList(domainGroupMembersList);\n            }\n        }\n        return domainGroupMembership;\n    }\n\n    public Set<String> getPendingMembershipApproverRoles(int delayDays) {\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            long updateTs = System.currentTimeMillis();\n            if (con.updatePendingRoleMembersNotificationTimestamp(zmsConfig.getServerHostName(), updateTs, delayDays)) {\n                return con.getPendingMembershipApproverRoles(zmsConfig.getServerHostName(), updateTs);\n            }\n        }\n        return null;\n    }\n\n    public Set<String> getPendingGroupMembershipApproverRoles(int delayDays) {\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            long updateTs = System.currentTimeMillis();\n            if (con.updatePendingGroupMembersNotificationTimestamp(zmsConfig.getServerHostName(), updateTs, delayDays)) {\n                return con.getPendingGroupMembershipApproverRoles(zmsConfig.getServerHostName(), updateTs);\n            }\n        }\n        return null;\n    }\n\n    public Map<String, DomainRoleMember> getRoleExpiryMembers(int delayDays) {\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            long updateTs = System.currentTimeMillis();\n            if (con.updateRoleMemberExpirationNotificationTimestamp(zmsConfig.getServerHostName(), updateTs, delayDays)) {\n                return con.getNotifyTemporaryRoleMembers(zmsConfig.getServerHostName(), updateTs);\n            }\n        }\n        return null;\n    }\n\n    public Map<String, DomainRoleMember> getRoleReviewMembers(int delayDays) {\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            long updateTs = System.currentTimeMillis();\n            if (con.updateRoleMemberReviewNotificationTimestamp(zmsConfig.getServerHostName(), updateTs, delayDays)) {\n                return con.getNotifyReviewRoleMembers(zmsConfig.getServerHostName(), updateTs);\n            }\n        }\n        return null;\n    }\n\n    public Map<String, DomainGroupMember> getGroupExpiryMembers(int delayDays) {\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n            long updateTs = System.currentTimeMillis();\n            if (con.updateGroupMemberExpirationNotificationTimestamp(zmsConfig.getServerHostName(), updateTs, delayDays)) {\n                return con.getNotifyTemporaryGroupMembers(zmsConfig.getServerHostName(), updateTs);\n            }\n        }\n        return null;\n    }\n\n    public void processExpiredPendingMembers(int pendingRoleMemberLifespan, final String monitorIdentity) {\n\n        final String auditRef = \"Expired - auto reject\";\n        final String caller = \"processExpiredPendingMembers\";\n\n        Map<String, List<DomainRoleMember>> memberList;\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            memberList = con.getExpiredPendingDomainRoleMembers(pendingRoleMemberLifespan);\n        }\n\n        // delete each member and record each expired member in audit log in a transaction\n\n        for (String domainName : memberList.keySet()) {\n            for (DomainRoleMember domainRoleMember : memberList.get(domainName)) {\n                final String principalName = domainRoleMember.getMemberName();\n                for (MemberRole memberRole : domainRoleMember.getMemberRoles()) {\n                    try (ObjectStoreConnection con = store.getConnection(true, true)) {\n                        if (con.deletePendingRoleMember(domainName, memberRole.getRoleName(),\n                                principalName, monitorIdentity, auditRef)) {\n                            auditLogRequest(monitorIdentity, domainName, auditRef, caller,\n                                    \"REJECT\", memberRole.getRoleName(),\n                                    \"{\\\"member\\\": \\\"\" + principalName + \"\\\"}\");\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    public void processExpiredPendingGroupMembers(int pendingGroupMemberLifespan, final String monitorIdentity) {\n\n        final String auditRef = \"Expired - auto reject\";\n        final String caller = \"processExpiredPendingGroupMembers\";\n\n        Map<String, List<DomainGroupMember>> memberList;\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            memberList = con.getExpiredPendingDomainGroupMembers(pendingGroupMemberLifespan);\n        }\n\n        // delete each member and record each expired member in audit log in a transaction\n\n        for (String domainName : memberList.keySet()) {\n            for (DomainGroupMember domainGroupMember : memberList.get(domainName)) {\n                final String principalName = domainGroupMember.getMemberName();\n                for (GroupMember groupMember : domainGroupMember.getMemberGroups()) {\n                    try (ObjectStoreConnection con = store.getConnection(true, true)) {\n                        if (con.deletePendingGroupMember(domainName, groupMember.getGroupName(),\n                                principalName, monitorIdentity, auditRef)) {\n                            auditLogRequest(monitorIdentity, domainName, auditRef, caller,\n                                    \"REJECT\", groupMember.getGroupName(),\n                                    \"{\\\"member\\\": \\\"\" + principalName + \"\\\"}\");\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    void executePutGroupReview(ResourceContext ctx, final String domainName, final String groupName,\n                               Group group, MemberDueDays memberExpiryDueDays, final String auditRef) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                final String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, ctx.getApiName(), principal, AUDIT_TYPE_GROUP);\n\n                // retrieve our original group\n\n                Group originalGroup = getGroup(con, domainName, groupName, false, false);\n\n                // now process the request. first we're going to make a copy of our group\n\n                Group updatedGroup = new Group().setName(originalGroup.getName());\n\n                // then we're going to apply the updated expiry and/or active status from the incoming group\n\n                List<GroupMember> noActionMembers = applyMembershipChangesGroup(updatedGroup, originalGroup,\n                        group, memberExpiryDueDays, auditRef);\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n\n                List<GroupMember> deletedMembers = new ArrayList<>();\n                List<GroupMember> extendedMembers = new ArrayList<>();\n\n                auditDetails.append(\"{\\\"name\\\": \\\"\").append(groupName).append('\\\"')\n                        .append(\", \\\"selfServe\\\": \").append(auditLogBooleanDefault(group.getSelfServe(), Boolean.TRUE))\n                        .append(\", \\\"auditEnabled\\\": \").append(auditLogBooleanDefault(group.getAuditEnabled(), Boolean.TRUE));\n\n                for (GroupMember member : updatedGroup.getGroupMembers()) {\n\n                    // if active flag is coming as false for the member, that means it's flagged for deletion\n\n                    if (member.getActive() == Boolean.FALSE) {\n                        if (!con.deleteGroupMember(domainName, groupName, member.getMemberName(), principal, auditRef)) {\n                            con.rollbackChanges();\n                            throw ZMSUtils.notFoundError(\"unable to delete group member: \" +\n                                    member.getMemberName() + \" from group: \" + groupName, ctx.getApiName());\n                        }\n                        deletedMembers.add(member);\n                    } else {\n                        // if not marked for deletion, then we are going to extend the member\n\n                        if (!con.insertGroupMember(domainName, groupName, member, principal, auditRef)) {\n                            con.rollbackChanges();\n                            throw ZMSUtils.notFoundError(\"unable to extend group member: \" +\n                                    member.getMemberName() + \" for the group: \" + groupName, ctx.getApiName());\n                        }\n                        extendedMembers.add(member);\n                    }\n                }\n\n                // construct audit log details\n\n                auditLogGroupMembers(auditDetails, \"deleted-members\", deletedMembers);\n                auditLogGroupMembers(auditDetails, \"extended-members\", extendedMembers);\n                auditLogGroupMembers(auditDetails, \"no-action-members\", noActionMembers);\n\n                auditDetails.append(\"}\");\n\n                if (!deletedMembers.isEmpty() || !extendedMembers.isEmpty()) {\n                    // we have one or more changes to the group. We should update\n                    // both lastReviewed as well as modified timestamps\n                    con.updateGroupModTimestamp(domainName, groupName);\n                }\n\n                con.updateGroupReviewTimestamp(domainName, groupName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, ctx.getApiName(), \"REVIEW\", groupName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutRoleReview(ResourceContext ctx, String domainName, String roleName, Role role,\n                              MemberDueDays memberExpiryDueDays, MemberDueDays memberReminderDueDays,\n                              String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                final String principal = getPrincipalName(ctx);\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, principal, AUDIT_TYPE_ROLE);\n\n                // retrieve our original role\n\n                Role originalRole = getRole(con, domainName, roleName, false, false, false);\n\n                if (originalRole.getTrust() != null && !originalRole.getTrust().isEmpty()) {\n                    throw ZMSUtils.requestError(caller + \": role \" + roleName + \" is delegated. Review should happen on the trusted role. \", caller);\n                }\n\n                // now process the request. first we're going to make a copy of our role\n\n                Role updatedRole = new Role().setName(originalRole.getName());\n\n                // then we're going to apply the updated expiry and/or active status from the incoming role\n\n                List<RoleMember> noActionMembers = applyMembershipChanges(updatedRole, originalRole, role,\n                        memberExpiryDueDays, memberReminderDueDays, auditRef);\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n\n                List<RoleMember> deletedMembers = new ArrayList<>();\n                List<RoleMember> extendedMembers = new ArrayList<>();\n\n                auditDetails.append(\"{\\\"name\\\": \\\"\").append(roleName).append('\\\"')\n                        .append(\", \\\"selfServe\\\": \").append(auditLogBooleanDefault(originalRole.getSelfServe(), Boolean.TRUE))\n                        .append(\", \\\"auditEnabled\\\": \").append(auditLogBooleanDefault(originalRole.getAuditEnabled(), Boolean.TRUE));\n\n                for (RoleMember member : updatedRole.getRoleMembers()) {\n\n                    // if active flag is coming as false for the member, that means it's flagged for deletion\n\n                    if (member.getActive() == Boolean.FALSE) {\n                        if (!con.deleteRoleMember(domainName, roleName, member.getMemberName(), principal, auditRef)) {\n                            con.rollbackChanges();\n                            throw ZMSUtils.notFoundError(caller + \": unable to delete role member: \" +\n                                    member.getMemberName() + \" from role: \" + roleName, caller);\n                        }\n                        deletedMembers.add(member);\n\n                    } else {\n                        // if not marked for deletion, then we are going to extend the member\n\n                        if (!con.insertRoleMember(domainName, roleName, member, principal, auditRef)) {\n                            con.rollbackChanges();\n                            throw ZMSUtils.notFoundError(caller + \": unable to extend role member: \" +\n                                    member.getMemberName() + \" for the role: \" + roleName, caller);\n                        }\n                        extendedMembers.add(member);\n                    }\n                }\n\n                // construct audit log details\n\n                auditLogRoleMembers(auditDetails, \"deleted-members\", deletedMembers);\n                auditLogRoleMembers(auditDetails, \"extended-members\", extendedMembers);\n                auditLogRoleMembers(auditDetails, \"no-action-members\", noActionMembers);\n\n                auditDetails.append(\"}\");\n\n                if (!deletedMembers.isEmpty() || !extendedMembers.isEmpty()) {\n                    // we have one or more changes to the role. We should update\n                    // both lastReviewed as well as modified timestamps\n                    con.updateRoleModTimestamp(domainName, roleName);\n                }\n\n                con.updateRoleReviewTimestamp(domainName, roleName);\n                saveChanges(con, domainName);\n\n                // audit log the request\n\n                auditLogRequest(ctx, domainName, auditRef, caller, \"REVIEW\", roleName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    /**\n     * This method takes the input role, creates a map using memberName as key,\n     * copies members from original role from DB and only adds deleted / extended members to the updatedRole.\n     * @param updatedRole updated role to be sent to DB to record changes\n     * @param originalRole original role from DB\n     * @param role incoming role containing changes from domain admin\n     * @param auditRef audit ref for the change\n     * @return List of rolemember where no action was taken\n     */\n    List<RoleMember> applyMembershipChanges(Role updatedRole, Role originalRole, Role role,\n                MemberDueDays memberExpiryDueDays, MemberDueDays memberReminderDueDays, String auditRef) {\n\n        Map<String, RoleMember> incomingMemberMap =\n                role.getRoleMembers().stream().collect(Collectors.toMap(RoleMember::getMemberName, item -> item));\n\n        List<RoleMember> noActionMembers = new ArrayList<>(originalRole.getRoleMembers().size());\n\n        // updatedMembers size is driven by input\n\n        List<RoleMember> updatedMembers = new ArrayList<>(incomingMemberMap.size());\n        updatedRole.setRoleMembers(updatedMembers);\n        RoleMember updatedMember;\n\n        // if original role is auditEnabled then all the extensions should be sent for approval again.\n\n        boolean approvalStatus = originalRole.getAuditEnabled() != Boolean.TRUE;\n        RoleMember tempMemberFromMap;\n\n        for (RoleMember originalMember : originalRole.getRoleMembers()) {\n\n            // we are only going to update the changed members\n\n            if (incomingMemberMap.containsKey(originalMember.getMemberName())) {\n\n                updatedMember = new RoleMember();\n                updatedMember.setMemberName(originalMember.getMemberName());\n\n                tempMemberFromMap = incomingMemberMap.get(updatedMember.getMemberName());\n\n                // member's approval status is determined by auditEnabled flag set on original role\n\n                updatedMember.setApproved(approvalStatus);\n\n                // member's active status is determined by action taken in UI\n\n                updatedMember.setActive(tempMemberFromMap.getActive());\n\n                // member's new expiration/review reminder date is set by\n                // role / domain level expiration setting\n\n                updatedMember.setExpiration(memberDueDateUpdateRequired(memberExpiryDueDays, tempMemberFromMap.getPrincipalType()) ?\n                        tempMemberFromMap.getExpiration() : originalMember.getExpiration());\n\n                updatedMember.setReviewReminder(memberDueDateUpdateRequired(memberReminderDueDays, tempMemberFromMap.getPrincipalType()) ?\n                        tempMemberFromMap.getReviewReminder() : originalMember.getReviewReminder());\n\n                updatedMember.setAuditRef(auditRef);\n                updatedMembers.add(updatedMember);\n            } else {\n                noActionMembers.add(originalMember);\n            }\n        }\n\n        return noActionMembers;\n    }\n\n    boolean memberDueDateUpdateRequired(MemberDueDays memberDueDays, Integer principalType) {\n        long dueDateMills = 0;\n        switch (Principal.Type.getType(principalType)) {\n            case USER:\n                dueDateMills = memberDueDays.getUserDueDateMillis();\n                break;\n            case SERVICE:\n                dueDateMills = memberDueDays.getServiceDueDateMillis();\n                break;\n            case GROUP:\n                dueDateMills = memberDueDays.getGroupDueDateMillis();\n                break;\n        }\n        return dueDateMills != 0;\n    }\n\n    /**\n     * This method takes the input group, creates a map using memberName as key,\n     * copies members from original group from DB and only adds deleted / extended members to the updatedGroup.\n     * @param updatedGroup updated group to be sent to DB to record changes\n     * @param originalGroup original group from DB\n     * @param group incoming group containing changes from domain admin\n     * @param auditRef audit ref for the change\n     * @return List of rolemember where no action was taken\n     */\n    List<GroupMember> applyMembershipChangesGroup(Group updatedGroup, Group originalGroup, Group group,\n                                                  MemberDueDays memberExpiryDueDays, String auditRef) {\n\n        Map<String, GroupMember> incomingMemberMap =\n                group.getGroupMembers().stream().collect(Collectors.toMap(GroupMember::getMemberName, item -> item));\n\n        List<GroupMember> noActionMembers = new ArrayList<>(originalGroup.getGroupMembers().size());\n\n        // updatedMembers size is driven by input\n\n        List<GroupMember> updatedMembers = new ArrayList<>(incomingMemberMap.size());\n        updatedGroup.setGroupMembers(updatedMembers);\n        GroupMember updatedMember;\n\n        // if original group is auditEnabled then all the extensions should be sent for approval again.\n\n        boolean approvalStatus = originalGroup.getAuditEnabled() != Boolean.TRUE;\n        GroupMember tempMemberFromMap;\n\n        for (GroupMember originalMember : originalGroup.getGroupMembers()) {\n\n            // we are only going to update the changed members\n\n            if (incomingMemberMap.containsKey(originalMember.getMemberName())) {\n\n                updatedMember = new GroupMember();\n                updatedMember.setMemberName(originalMember.getMemberName());\n\n                tempMemberFromMap = incomingMemberMap.get(updatedMember.getMemberName());\n\n                // member's approval status is determined by auditEnabled flag set on original role\n\n                updatedMember.setApproved(approvalStatus);\n\n                // member's active status is determined by action taken in UI\n\n                updatedMember.setActive(tempMemberFromMap.getActive());\n\n                // member's new expiration is set by role / domain level expiration setting\n\n                updatedMember.setExpiration(memberDueDateUpdateRequired(memberExpiryDueDays, tempMemberFromMap.getPrincipalType()) ?\n                        tempMemberFromMap.getExpiration() : originalMember.getExpiration());\n\n                updatedMember.setAuditRef(auditRef);\n                updatedMembers.add(updatedMember);\n            } else {\n                noActionMembers.add(originalMember);\n            }\n        }\n        return noActionMembers;\n    }\n\n    void updateDomainModTimestamp(final String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n            // update domain time-stamps, and invalidate local cache entry\n\n            con.updateDomainModTimestamp(domainName);\n            cacheStore.invalidate(domainName);\n        }\n    }\n\n    List<TemplateMetaData> getDomainTemplates(String domainName) {\n\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            return con.getDomainTemplates(domainName);\n        }\n    }\n\n    void processRoleUserAuthorityRestrictions() {\n\n        // if we don't have a user authority defined then there\n        // is no work to be done\n\n        if (zmsConfig.getUserAuthority() == null) {\n            return;\n        }\n\n        // first we need to get all the roles that have the authority\n        // filter or date expiry attributes set\n\n        List<PrincipalRole> roles;\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            roles = con.listRolesWithUserAuthorityRestrictions();\n        }\n\n        if (roles == null) {\n            return;\n        }\n\n        // for each role catch any exception and ignore since we\n        // want to process all roles and not allow a single one\n        // prevent updating others\n\n        for (PrincipalRole role : roles) {\n            try {\n                enforceRoleUserAuthorityRestrictions(role.getDomainName(), role.getRoleName(),\n                        role.getDomainUserAuthorityFilter());\n            } catch (Exception ex) {\n                LOG.error(\"Unable to process user authority restrictions for {}:role.{} - {}\",\n                        role.getDomainName(), role.getRoleName(), ex.getMessage());\n            }\n        }\n    }\n\n    void processGroupUserAuthorityRestrictions() {\n\n        // if we don't have a user authority defined then there\n        // is no work to be done\n\n        if (zmsConfig.getUserAuthority() == null) {\n            return;\n        }\n\n        // first we need to get all the groups that have the authority\n        // filter or date expiry attributes set\n\n        List<PrincipalGroup> groups;\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n            groups = con.listGroupsWithUserAuthorityRestrictions();\n        }\n\n        if (groups == null) {\n            return;\n        }\n\n        // for each group catch any exception and ignore since we\n        // want to process all group and not allow a single one\n        // prevent updating others\n\n        for (PrincipalGroup group : groups) {\n            try {\n                enforceGroupUserAuthorityRestrictions(group.getDomainName(), group.getGroupName(),\n                        group.getDomainUserAuthorityFilter());\n            } catch (Exception ex) {\n                LOG.error(\"Unable to process user authority restrictions for {}:group.{} - {}\",\n                        group.getDomainName(), group.getGroupName(), ex.getMessage());\n            }\n        }\n    }\n\n    Map<String, List<String>> applyTemplatesForListOfDomains(Map<String, Integer> templateDetails) {\n        final String caller = \"applyTemplatesForListOfDomains\";\n        final String auditRef = \"AutoApplyTemplate\";\n        Map<String, List<String>> domainTemplateListMap;\n        DomainTemplate domainTemplate = new DomainTemplate();\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n             domainTemplateListMap = con.getDomainFromTemplateName(templateDetails);\n        }\n\n        for (String domainName : domainTemplateListMap.keySet()) {\n            domainTemplate.setTemplateNames(domainTemplateListMap.get(domainName));\n            //Passing null context since it is an internal call during app start up\n            //executePutDomainTemplate can bulk apply templates given a domain hence sending domainName and templatelist\n            try {\n                this.executePutDomainTemplate(null, domainName, domainTemplate, auditRef, caller);\n            } catch (Exception ex) {\n                LOG.error(\"unable to apply template for domain {} and template {} error: {}\",\n                        domainName, domainTemplate, ex.getMessage());\n            }\n        }\n        return domainTemplateListMap;\n    }\n\n    void enforceRoleUserAuthorityRestrictions(final String domainName, final String roleName,\n                                              final String domainUserAuthorityFilter) {\n\n        final String caller = \"enforceRoleUserAuthorityRestrictions\";\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n            // get the role from the storage system\n\n            Role role = getRole(con, domainName, roleName, false, false, false);\n            if (role == null) {\n                return;\n            }\n\n            // update the role membership\n\n            List<RoleMember> roleMembers = role.getRoleMembers();\n            if (roleMembers == null) {\n                return;\n            }\n\n            // first process the authority expiration restriction\n\n            boolean expiryDBUpdated = false;\n            final String userAuthorityExpiry = role.getUserAuthorityExpiration();\n            if (userAuthorityExpiry != null) {\n                List<RoleMember> updatedMembers = new ArrayList<>();\n                for (RoleMember roleMember : roleMembers) {\n                    if (updateUserAuthorityExpiry(roleMember, userAuthorityExpiry)) {\n                        updatedMembers.add(roleMember);\n                    }\n                }\n\n                expiryDBUpdated = insertRoleMembers(null, con, updatedMembers, domainName, roleName,\n                        ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller);\n            }\n\n            // now process authority filter restriction\n\n            boolean filterDBUpdated = false;\n            final String userAuthorityFilter = ZMSUtils.combineUserAuthorityFilters(role.getUserAuthorityFilter(),\n                    domainUserAuthorityFilter);\n            if (userAuthorityFilter != null) {\n                List<RoleMember> updatedMembers = new ArrayList<>();\n\n                for (RoleMember roleMember : roleMembers) {\n                    if (updateUserAuthorityFilter(roleMember, userAuthorityFilter)) {\n                        updatedMembers.add(roleMember);\n                    }\n                }\n\n                filterDBUpdated = updateRoleMemberDisabledState(null, con, updatedMembers, domainName,\n                        roleName, ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller);\n            }\n\n            if (expiryDBUpdated || filterDBUpdated) {\n\n                // update our role and domain time-stamps, and invalidate local cache entry\n\n                con.updateRoleModTimestamp(domainName, roleName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n            }\n        }\n    }\n\n    void enforceGroupUserAuthorityRestrictions(final String domainName, final String groupName,\n                                               final String domainUserAuthorityFilter) {\n\n        final String caller = \"enforceGroupUserAuthorityRestrictions\";\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n            // get the role from the storage system\n\n            Group group = getGroup(con, domainName, groupName, false, false);\n            if (group == null) {\n                return;\n            }\n\n            // update the group membership\n\n            List<GroupMember> groupMembers = group.getGroupMembers();\n            if (groupMembers == null) {\n                return;\n            }\n\n            // first process the authority expiration restriction\n\n            boolean expiryDBUpdated = false;\n            final String userAuthorityExpiry = group.getUserAuthorityExpiration();\n            if (userAuthorityExpiry != null) {\n                List<GroupMember> updatedMembers = new ArrayList<>();\n                for (GroupMember groupMember : groupMembers) {\n                    if (updateUserAuthorityExpiry(groupMember, userAuthorityExpiry)) {\n                        updatedMembers.add(groupMember);\n                    }\n                }\n\n                expiryDBUpdated = insertGroupMembers(null, con, updatedMembers, domainName, groupName,\n                        ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller);\n            }\n\n            // now process authority filter restriction\n\n            boolean filterDBUpdated = false;\n            final String userAuthorityFilter = ZMSUtils.combineUserAuthorityFilters(group.getUserAuthorityFilter(),\n                    domainUserAuthorityFilter);\n            if (userAuthorityFilter != null) {\n                List<GroupMember> updatedMembers = new ArrayList<>();\n\n                for (GroupMember groupMember : groupMembers) {\n                    if (updateUserAuthorityFilter(groupMember, userAuthorityFilter)) {\n                        updatedMembers.add(groupMember);\n                    }\n                }\n\n                filterDBUpdated = updateGroupMemberDisabledState(null, con, updatedMembers, domainName,\n                        groupName, ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller);\n            }\n\n            if (expiryDBUpdated || filterDBUpdated) {\n\n                // update our group and domain time-stamps, and invalidate local cache entry\n\n                con.updateGroupModTimestamp(domainName, groupName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n            }\n        }\n    }\n\n    /**\n     * This method returns list of Principals based on the state parameter supplied\n     * @param queriedState state of principal\n     * @return List of Principals from DB\n     */\n    List<Principal> getPrincipals(int queriedState) {\n        List<Principal> principals = new ArrayList<>();\n        Principal principal;\n        try (ObjectStoreConnection con = store.getConnection(true, false)) {\n           List<String> dbPrincipals = con.getPrincipals(queriedState);\n            Principal.State principalState = Principal.State.getState(queriedState);\n           for (String dbPrincipal : dbPrincipals) {\n               principal = ZMSUtils.createPrincipalForName(dbPrincipal, zmsConfig.getUserDomain(), null);\n               ((SimplePrincipal) principal).setState(principalState);\n               principals.add(principal);\n           }\n        }\n        return principals;\n    }\n\n    /**\n     * This method toggles state for supplied Principals based on the flag in DB\n     * as well as modifies memberships of all roles and groups of current principal(s)\n     * @param changedPrincipals List of Principals from User Authority\n     * @param suspended boolean indicating principal's state\n     */\n    void updatePrincipalByStateFromAuthority(List<Principal> changedPrincipals, boolean suspended) {\n\n        if (changedPrincipals.isEmpty()) {\n            return;\n        }\n\n        final String caller = \"updatePrincipalByStateFromAuthority\";\n        List<Principal> updatedUsers = new ArrayList<>();\n        int newPrincipalState = suspended ? Principal.State.AUTHORITY_SYSTEM_SUSPENDED.getValue() : Principal.State.ACTIVE.getValue();\n        try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n            // first lets update the new state in DB\n            for (Principal changedPrincipal : changedPrincipals) {\n                try {\n                    if (con.updatePrincipal(changedPrincipal.getFullName(), newPrincipalState)) {\n                        updatedUsers.add(changedPrincipal);\n                    }\n                } catch (ResourceException ex) {\n                    // log the exception and continue with remaining principals\n                    LOG.error(\"Exception in updating principal state from Authority {} Moving on.\", ex.getMessage());\n                }\n            }\n            // if new state is updated successfully\n            // then we need to modify all roles and groups where given principal is member of\n            if (!updatedUsers.isEmpty()) {\n                for (Principal updatedUser : updatedUsers) {\n                    // separate try blocks to treat group and role membership 404s separately\n                    try {\n                        updateRoleMembershipsByPrincipalState(suspended, caller, con, updatedUser);\n                    } catch (ResourceException ex) {\n                        if (ex.getCode() == ResourceException.NOT_FOUND) {\n                            continue;\n                        }\n                        throw ex;\n                    }\n                    // separate try blocks to treat group and role membership 404s separately\n                    try {\n                        updateGroupMembershipByPrincipalState(suspended, caller, con, updatedUser);\n                    } catch (ResourceException ex) {\n                        if (ex.getCode() == ResourceException.NOT_FOUND) {\n                            continue;\n                        }\n                        throw ex;\n                    }\n                }\n            }\n        }\n    }\n\n    void executePutAssertionConditions(ResourceContext ctx, String domainName, String policyName, long assertionId,\n                                       AssertionConditions assertionConditions, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // now we need verify our quota check\n\n                quotaCheck.checkAssertionConditionsQuota(con, assertionId, assertionConditions, caller);\n\n                // process our insert assertion condition.\n\n                if (!con.insertAssertionConditions(assertionId, assertionConditions)) {\n                    throw ZMSUtils.requestError(String.format(\"%s: unable to insert assertion conditions for policy=%s assertionId=%d\", caller, policyName, assertionId), caller);\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                saveChanges(con, domainName);\n\n\n                // audit log the request\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n                        .append(\"\\\", \\\"assertionId\\\": \").append(assertionId)\n                        .append(\", \");\n                auditLogAssertionConditions(auditDetails, assertionConditions.getConditionsList(), \"new-assertion-conditions\");\n                auditDetails.append(\"}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    void executePutAssertionCondition(ResourceContext ctx, String domainName, String policyName, long assertionId,\n                                      AssertionCondition assertionCondition, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(false, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // process our insert assertion condition.\n\n                if (assertionCondition.getId() == null) {\n\n                    // now we need verify our quota check\n                    quotaCheck.checkAssertionConditionQuota(con, assertionId, assertionCondition, caller);\n\n                    // no condition id in the request. so we are going to generate the next condition id for\n                    // the given assertion id and then use it to insert given keys\n                    assertionCondition.setId(con.getNextConditionId(assertionId, caller));\n                    if (!con.insertAssertionCondition(assertionId, assertionCondition)) {\n                        throw ZMSUtils.requestError(String.format(\"%s: unable to insert new assertion condition for policy=%s assertionId=%d\", caller, policyName, assertionId), caller);\n                    }\n                } else {\n\n                    // existing assertion condition keys found with given condition id. so delete existing keys from DB for the given condition id\n                    if (!con.deleteAssertionCondition(assertionId, assertionCondition.getId())) {\n                        throw ZMSUtils.notFoundError(String.format(\"%s: unable to delete assertion condition during putAssertionCondition for policy=%s assertionId=%d conditionId=%d\"\n                                , caller, policyName, assertionId, assertionCondition.getId()), caller);\n                    }\n                    // now we need verify our quota check after deleting the old entries\n                    quotaCheck.checkAssertionConditionQuota(con, assertionId, assertionCondition, caller);\n\n                    // now insert the new keys against existing condition id\n                    if (!con.insertAssertionCondition(assertionId, assertionCondition)) {\n                        throw ZMSUtils.requestError(String.format(\"%s: unable to insert assertion condition for policy=%s assertionId=%d\", caller, policyName, assertionId), caller);\n                    }\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                saveChanges(con, domainName);\n\n\n                // audit log the request\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n                        .append(\"\\\", \\\"assertionId\\\": \").append(assertionId)\n                        .append(\", \\\"new-assertion-condition\\\": \");\n                auditLogAssertionCondition(auditDetails, assertionCondition, true);\n                auditDetails.append(\"}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_PUT,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    public void executeDeleteAssertionConditions(ResourceContext ctx, String domainName, String policyName, Long assertionId, String auditRef, String caller) {\n\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // fetch the assertion for our audit log\n\n                List<AssertionCondition> assertionConditions = con.getAssertionConditions(assertionId);\n                if (assertionConditions == null) {\n                    throw ZMSUtils.notFoundError(String.format(\"%s: unable to read assertion conditions for policy=%s assertionId=%d\", caller,\n                            policyName, assertionId), caller);\n                }\n\n                // process our delete assertion conditions. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.deleteAssertionConditions(assertionId)) {\n                    throw ZMSUtils.notFoundError(String.format(\"%s: unable to delete assertion conditions for policy=%s assertionId=%d\", caller,\n                            policyName, assertionId), caller);\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n                        .append(\"\\\", \\\"assertionId\\\": \").append(assertionId)\n                        .append(\", \");\n                auditLogAssertionConditions(auditDetails, assertionConditions, \"deleted-assertion-conditions\");\n                auditDetails.append(\"}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    public void executeDeleteAssertionCondition(ResourceContext ctx, String domainName, String policyName, Long assertionId, Integer conditionId, String auditRef, String caller) {\n        // our exception handling code does the check for retry count\n        // and throws the exception it had received when the retry\n        // count reaches 0\n\n        for (int retryCount = defaultRetryCount; ; retryCount--) {\n\n            try (ObjectStoreConnection con = store.getConnection(true, true)) {\n\n                // first verify that auditing requirements are met\n\n                checkDomainAuditEnabled(con, domainName, auditRef, caller, getPrincipalName(ctx), AUDIT_TYPE_POLICY);\n\n                // fetch the assertion for our audit log\n\n                AssertionCondition assertionCondition = con.getAssertionCondition(assertionId, conditionId);\n                if (assertionCondition == null) {\n                    throw ZMSUtils.notFoundError(String.format(\"%s: unable to read assertion condition for policy=%s assertionId=%d conditionId=%d\"\n                            , caller, policyName, assertionId, conditionId), caller);\n                }\n\n                // process our delete assertion condition. since this is a \"single\"\n                // operation, we are not using any transactions.\n\n                if (!con.deleteAssertionCondition(assertionId, conditionId)) {\n                    throw ZMSUtils.notFoundError(String.format(\"%s: unable to delete assertion condition for policy=%s assertionId=%d conditionId=%d\"\n                            , caller, policyName, assertionId, conditionId), caller);\n                }\n\n                // update our policy and domain time-stamps, and invalidate local cache entry\n\n                con.updatePolicyModTimestamp(domainName, policyName);\n                con.updateDomainModTimestamp(domainName);\n                cacheStore.invalidate(domainName);\n\n                // audit log the request\n\n                StringBuilder auditDetails = new StringBuilder(ZMSConsts.STRING_BLDR_SIZE_DEFAULT);\n                auditDetails.append(\"{\\\"policy\\\": \\\"\").append(policyName)\n                        .append(\"\\\", \\\"assertionId\\\": \").append(assertionId)\n                        .append(\", \\\"deleted-assertion-condition\\\": \");\n                auditLogAssertionCondition(auditDetails, assertionCondition, true);\n                auditDetails.append(\"}\");\n\n                auditLogRequest(ctx, domainName, auditRef, caller, ZMSConsts.HTTP_DELETE,\n                        policyName, auditDetails.toString());\n\n                return;\n\n            } catch (ResourceException ex) {\n\n                // otherwise check if we need to retry or return failure\n\n                if (!shouldRetryOperation(ex, retryCount)) {\n                    throw ex;\n                }\n            }\n        }\n    }\n\n    private void updateGroupMembershipByPrincipalState(boolean suspended, String caller, ObjectStoreConnection con, Principal updatedUser) {\n        List<GroupMember> groupMembersWithUpdatedState;\n        GroupMember groupMember;\n        DomainGroupMember domainGroupMember;\n        int newState, oldState;\n        Set<String> updatedDomains = new HashSet<>();\n        domainGroupMember = con.getPrincipalGroups(updatedUser.getFullName(), null);\n        if (!domainGroupMember.getMemberGroups().isEmpty()) {\n            for (GroupMember currentGroup : domainGroupMember.getMemberGroups()) {\n                groupMember = new GroupMember();\n                groupMember.setMemberName(updatedUser.getFullName());\n                oldState = 0;\n                if (groupMember.getSystemDisabled() != null) {\n                    oldState = groupMember.getSystemDisabled();\n                }\n                newState = suspended ? oldState | Principal.State.AUTHORITY_SYSTEM_SUSPENDED.getValue() :\n                        oldState & ~Principal.State.AUTHORITY_SYSTEM_SUSPENDED.getValue();\n                groupMember.setSystemDisabled(newState);\n                groupMembersWithUpdatedState = Collections.singletonList(groupMember);\n                // Following method does Audit entry as well\n                if (updateGroupMemberDisabledState(null, con, groupMembersWithUpdatedState, currentGroup.getDomainName(),\n                        currentGroup.getGroupName(), ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller)) {\n                    con.updateGroupModTimestamp(currentGroup.getDomainName(), currentGroup.getGroupName());\n                    updatedDomains.add(currentGroup.getDomainName());\n                }\n            }\n            updatedDomains.forEach(dom -> {\n                con.updateDomainModTimestamp(dom);\n                cacheStore.invalidate(dom);\n            });\n        }\n    }\n\n    private void updateRoleMembershipsByPrincipalState(boolean suspended, String caller, ObjectStoreConnection con, Principal updatedUser) {\n        RoleMember roleMember;\n        List<RoleMember> roleMembersWithUpdatedState;\n        DomainRoleMember domainRoleMember;\n        int newState, oldState;\n        Set<String> updatedDomains = new HashSet<>();\n        domainRoleMember = con.getPrincipalRoles(updatedUser.getFullName(), null);\n        if (!domainRoleMember.getMemberRoles().isEmpty()) {\n            for (MemberRole memberRole : domainRoleMember.getMemberRoles()) {\n                roleMember = new RoleMember();\n                roleMember.setMemberName(updatedUser.getFullName());\n                oldState = 0;\n                if (memberRole.getSystemDisabled() != null) {\n                    oldState = memberRole.getSystemDisabled();\n                }\n                newState = suspended ? oldState | Principal.State.AUTHORITY_SYSTEM_SUSPENDED.getValue() :\n                        oldState & ~Principal.State.AUTHORITY_SYSTEM_SUSPENDED.getValue();\n                roleMember.setSystemDisabled(newState);\n                roleMembersWithUpdatedState = Collections.singletonList(roleMember);\n\n                // Following method does Audit entry as well\n                if (updateRoleMemberDisabledState(null, con, roleMembersWithUpdatedState, memberRole.getDomainName(),\n                        memberRole.getRoleName(), ZMSConsts.SYS_AUTH_MONITOR, AUDIT_REF, caller)) {\n                    con.updateRoleModTimestamp(memberRole.getDomainName(), memberRole.getRoleName());\n                    updatedDomains.add(memberRole.getDomainName());\n                }\n            }\n            updatedDomains.forEach(dom -> {\n                con.updateDomainModTimestamp(dom);\n                cacheStore.invalidate(dom);\n            });\n        }\n    }\n\n    class UserAuthorityFilterEnforcer implements Runnable {\n\n        public UserAuthorityFilterEnforcer() {\n        }\n\n        @Override\n        public void run() {\n\n            LOG.info(\"UserAuthorityFilterEnforcer: Starting user authority filter enforcer thread...\");\n\n            try {\n                processRoleUserAuthorityRestrictions();\n            } catch (Throwable t) {\n                LOG.error(\"UserAuthorityFilterEnforcer: unable to enforce role user authority restrictions: {}\",\n                        t.getMessage());\n            }\n\n            try {\n                processGroupUserAuthorityRestrictions();\n            } catch (Throwable t) {\n                LOG.error(\"UserAuthorityFilterEnforcer: unable to enforce group user authority restrictions: {}\",\n                        t.getMessage());\n            }\n\n            LOG.info(\"UserAuthorityFilterEnforcer: Completed user authority filter enforcer thread\");\n        }\n    }\n}\n", "idx": 18, "id": 6002, "msg": "", "proj": "AthenZ-athenz", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -35,6 +35,8 @@ import org.apache.lucene.util.ArrayUtil;\n import org.apache.lucene.analysis.ja.dict.BinaryDictionary;\n \n public abstract class BinaryDictionaryWriter {\n+  private final static int ID_LIMIT = 8192;\n+\n   protected final Class<? extends BinaryDictionary> implClazz;\n   protected ByteBuffer buffer;\n   private int targetMapEndOffset = 0, lastWordId = -1, lastSourceId = -1;", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.lucene.analysis.ja.util;\n\n\nimport java.io.BufferedOutputStream;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.Channels;\nimport java.nio.channels.WritableByteChannel;\nimport java.util.ArrayList;\n\nimport org.apache.lucene.codecs.CodecUtil;\nimport org.apache.lucene.store.DataOutput;\nimport org.apache.lucene.store.OutputStreamDataOutput;\nimport org.apache.lucene.util.ArrayUtil;\n\nimport org.apache.lucene.analysis.ja.dict.BinaryDictionary;\n\npublic abstract class BinaryDictionaryWriter {\n  protected final Class<? extends BinaryDictionary> implClazz;\n  protected ByteBuffer buffer;\n  private int targetMapEndOffset = 0, lastWordId = -1, lastSourceId = -1;\n  private int[] targetMap = new int[8192];\n  private int[] targetMapOffsets = new int[8192];\n  private final ArrayList<String> posDict = new ArrayList<>();\n\n  public BinaryDictionaryWriter(Class<? extends BinaryDictionary> implClazz, int size) {\n    this.implClazz = implClazz;\n    buffer = ByteBuffer.allocate(size);\n  }\n  \n  /**\n   * put the entry in map\n   * @return current position of buffer, which will be wordId of next entry\n   */\n  public int put(String[] entry) {\n    short leftId = Short.parseShort(entry[1]);\n    short rightId = Short.parseShort(entry[2]);\n    short wordCost = Short.parseShort(entry[3]);\n    \n    StringBuilder sb = new StringBuilder();\n    \n    // build up the POS string\n    for (int i = 4; i < 8; i++) {\n      String part = entry[i];\n      assert part.length() > 0;\n      if (!\"*\".equals(part)) {\n        if (sb.length() > 0) {\n          sb.append('-');\n        }\n        sb.append(part);\n      }\n    }\n    \n    String posData = sb.toString();\n    \n    sb.setLength(0);\n    sb.append(CSVUtil.quoteEscape(posData));\n    sb.append(',');\n    if (!\"*\".equals(entry[8])) {\n      sb.append(CSVUtil.quoteEscape(entry[8]));\n    }\n    sb.append(',');\n    if (!\"*\".equals(entry[9])) {\n      sb.append(CSVUtil.quoteEscape(entry[9]));\n    }\n    String fullPOSData = sb.toString();\n    \n    String baseForm = entry[10];\n    String reading = entry[11];\n    String pronunciation = entry[12];\n    \n    // extend buffer if necessary\n    int left = buffer.remaining();\n    // worst case: two short, 3 bytes, and features (all as utf-16)\n    int worstCase = 4 + 3 + 2*(baseForm.length() + reading.length() + pronunciation.length());\n    if (worstCase > left) {\n      ByteBuffer newBuffer = ByteBuffer.allocate(ArrayUtil.oversize(buffer.limit() + worstCase - left, 1));\n      buffer.flip();\n      newBuffer.put(buffer);\n      buffer = newBuffer;\n    }\n\n    int flags = 0;\n    if (!(\"*\".equals(baseForm) || baseForm.equals(entry[0]))) {\n      flags |= BinaryDictionary.HAS_BASEFORM;\n    }\n    if (!reading.equals(toKatakana(entry[0]))) {\n      flags |= BinaryDictionary.HAS_READING;\n    }\n    if (!pronunciation.equals(reading)) {\n      flags |= BinaryDictionary.HAS_PRONUNCIATION;\n    }\n\n    assert leftId == rightId;\n    assert leftId < 4096; // there are still unused bits\n    // add pos mapping\n    int toFill = 1+leftId - posDict.size();\n    for (int i = 0; i < toFill; i++) {\n      posDict.add(null);\n    }\n    \n    String existing = posDict.get(leftId);\n    assert existing == null || existing.equals(fullPOSData);\n    posDict.set(leftId, fullPOSData);\n    \n    buffer.putShort((short)(leftId << 3 | flags));\n    buffer.putShort(wordCost);\n\n    if ((flags & BinaryDictionary.HAS_BASEFORM) != 0) {\n      assert baseForm.length() < 16;\n      int shared = sharedPrefix(entry[0], baseForm);\n      int suffix = baseForm.length() - shared;\n      buffer.put((byte) (shared << 4 | suffix));\n      for (int i = shared; i < baseForm.length(); i++) {\n        buffer.putChar(baseForm.charAt(i));\n      }\n    }\n    \n    if ((flags & BinaryDictionary.HAS_READING) != 0) {\n      if (isKatakana(reading)) {\n        buffer.put((byte) (reading.length() << 1 | 1));\n        writeKatakana(reading);\n      } else {\n        buffer.put((byte) (reading.length() << 1));\n        for (int i = 0; i < reading.length(); i++) {\n          buffer.putChar(reading.charAt(i));\n        }\n      }\n    }\n    \n    if ((flags & BinaryDictionary.HAS_PRONUNCIATION) != 0) {\n      // we can save 150KB here, but it makes the reader a little complicated.\n      // int shared = sharedPrefix(reading, pronunciation);\n      // buffer.put((byte) shared);\n      // pronunciation = pronunciation.substring(shared);\n      if (isKatakana(pronunciation)) {\n        buffer.put((byte) (pronunciation.length() << 1 | 1));\n        writeKatakana(pronunciation);\n      } else {\n        buffer.put((byte) (pronunciation.length() << 1));\n        for (int i = 0; i < pronunciation.length(); i++) {\n          buffer.putChar(pronunciation.charAt(i));\n        }\n      }\n    }\n    \n    return buffer.position();\n  }\n  \n  private boolean isKatakana(String s) {\n    for (int i = 0; i < s.length(); i++) {\n      char ch = s.charAt(i);\n      if (ch < 0x30A0 || ch > 0x30FF) {\n        return false;\n      }\n    }\n    return true;\n  }\n  \n  private void writeKatakana(String s) {\n    for (int i = 0; i < s.length(); i++) {\n      buffer.put((byte) (s.charAt(i) - 0x30A0));\n    }\n  }\n  \n  private String toKatakana(String s) {\n    char text[] = new char[s.length()];\n    for (int i = 0; i < s.length(); i++) {\n      char ch = s.charAt(i);\n      if (ch > 0x3040 && ch < 0x3097) {\n        text[i] = (char)(ch + 0x60);\n      } else {\n        text[i] = ch;\n      }\n    }\n    return new String(text);\n  }\n  \n  public static int sharedPrefix(String left, String right) {\n    int len = left.length() < right.length() ? left.length() : right.length();\n    for (int i = 0; i < len; i++)\n      if (left.charAt(i) != right.charAt(i))\n        return i;\n    return len;\n  }\n  \n  public void addMapping(int sourceId, int wordId) {\n    assert wordId > lastWordId : \"words out of order: \" + wordId + \" vs lastID: \" + lastWordId;\n    \n    if (sourceId > lastSourceId) {\n      assert sourceId > lastSourceId : \"source ids out of order: lastSourceId=\" + lastSourceId + \" vs sourceId=\" + sourceId;\n      targetMapOffsets = ArrayUtil.grow(targetMapOffsets, sourceId + 1);\n      for (int i = lastSourceId + 1; i <= sourceId; i++) {\n        targetMapOffsets[i] = targetMapEndOffset;\n      }\n    } else {\n      assert sourceId == lastSourceId;\n    }\n\n    targetMap = ArrayUtil.grow(targetMap, targetMapEndOffset + 1);\n    targetMap[targetMapEndOffset] = wordId;\n    targetMapEndOffset++;\n\n    lastSourceId = sourceId;\n    lastWordId = wordId;\n  }\n\n  protected final String getBaseFileName(String baseDir) {\n    return baseDir + File.separator + implClazz.getName().replace('.', File.separatorChar);\n  }\n  \n  /**\n   * Write dictionary in file\n   * Dictionary format is:\n   * [Size of dictionary(int)], [entry:{left id(short)}{right id(short)}{word cost(short)}{length of pos info(short)}{pos info(char)}], [entry...], [entry...].....\n   * @throws IOException if an I/O error occurs writing the dictionary files\n   */\n  public void write(String baseDir) throws IOException {\n    final String baseName = getBaseFileName(baseDir);\n    writeDictionary(baseName + BinaryDictionary.DICT_FILENAME_SUFFIX);\n    writeTargetMap(baseName + BinaryDictionary.TARGETMAP_FILENAME_SUFFIX);\n    writePosDict(baseName + BinaryDictionary.POSDICT_FILENAME_SUFFIX);\n  }\n  \n  // TODO: maybe this int[] should instead be the output to the FST...\n  protected void writeTargetMap(String filename) throws IOException {\n    new File(filename).getParentFile().mkdirs();\n    OutputStream os = new FileOutputStream(filename);\n    try {\n      os = new BufferedOutputStream(os);\n      final DataOutput out = new OutputStreamDataOutput(os);\n      CodecUtil.writeHeader(out, BinaryDictionary.TARGETMAP_HEADER, BinaryDictionary.VERSION);\n      \n      final int numSourceIds = lastSourceId + 1;\n      out.writeVInt(targetMapEndOffset); // <-- size of main array\n      out.writeVInt(numSourceIds + 1); // <-- size of offset array (+ 1 more entry)\n      int prev = 0, sourceId = 0;\n      for (int ofs = 0; ofs < targetMapEndOffset; ofs++) {\n        final int val = targetMap[ofs], delta = val - prev;\n        assert delta >= 0;\n        if (ofs == targetMapOffsets[sourceId]) {\n          out.writeVInt((delta << 1) | 0x01);\n          sourceId++;\n        } else {\n          out.writeVInt((delta << 1));\n        }\n        prev += delta;\n      }\n      assert sourceId == numSourceIds : \"sourceId:\"+sourceId+\" != numSourceIds:\"+numSourceIds;\n    } finally {\n      os.close();\n    }\n  }\n  \n  protected void writePosDict(String filename) throws IOException {\n    new File(filename).getParentFile().mkdirs();\n    OutputStream os = new FileOutputStream(filename);\n    try {\n      os = new BufferedOutputStream(os);\n      final DataOutput out = new OutputStreamDataOutput(os);\n      CodecUtil.writeHeader(out, BinaryDictionary.POSDICT_HEADER, BinaryDictionary.VERSION);\n      out.writeVInt(posDict.size());\n      for (String s : posDict) {\n        if (s == null) {\n          out.writeByte((byte)0);\n          out.writeByte((byte)0);\n          out.writeByte((byte)0);\n        } else {\n          String data[] = CSVUtil.parse(s);\n          assert data.length == 3 : \"malformed pos/inflection: \" + s;\n          out.writeString(data[0]);\n          out.writeString(data[1]);\n          out.writeString(data[2]);\n        }\n      }\n    } finally {\n      os.close();\n    }\n  }\n  \n  protected void writeDictionary(String filename) throws IOException {\n    new File(filename).getParentFile().mkdirs();\n    final FileOutputStream os = new FileOutputStream(filename);\n    try {\n      final DataOutput out = new OutputStreamDataOutput(os);\n      CodecUtil.writeHeader(out, BinaryDictionary.DICT_HEADER, BinaryDictionary.VERSION);\n      out.writeVInt(buffer.position());\n      final WritableByteChannel channel = Channels.newChannel(os);\n      // Write Buffer\n      buffer.flip();  // set position to 0, set limit to current position\n      channel.write(buffer);\n      assert buffer.remaining() == 0L;\n    } finally {\n      os.close();\n    }\n  }\n}\n", "idx": 1, "id": 29391, "msg": "", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -863,12 +863,30 @@ def define_model(model_name, dbengine, model_seed):\n             policy = policy['bindings']\n \n             def filter_etag(policy):\n-                \"\"\"Filter etag key/value out of policy map.\"\"\"\n+                \"\"\"Filter etag key/value out of policy map.\n+\n+                Args:\n+                    policy (dict): the policy to filter\n+\n+                Returns:\n+                    dict: policy without etag, <\"bindings\":[<role, members>]>\n+\n+                Raises:\n+                \"\"\"\n \n                 return {k: v for k, v in policy.iteritems() if k != 'etag'}\n \n             def calculate_diff(policy, old_policy):\n-                \"\"\"Calculate the grant/revoke difference between policies.\"\"\"\n+                \"\"\"Calculate the grant/revoke difference between policies.\n+                   The diff = policy['bindings'] - old_policy['bindings']\n+\n+                Args:\n+                    policy (dict): the new policy in dict format\n+                    old_policy (dict): the old policy in dict format\n+\n+                Returns:\n+                    dict: <role, members> diff of bindings\n+                \"\"\"\n \n                 diff = collections.defaultdict(list)\n                 for role, members in filter_etag(policy).iteritems():", "y": 0, "oldf": "# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Database abstraction objects for Forseti Server.\"\"\"\n\n# pylint: disable=too-many-lines,singleton-comparison\n\nimport datetime\nimport os\nimport binascii\nimport collections\nimport struct\nimport hmac\nimport json\nfrom threading import Lock\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import event\nfrom sqlalchemy import Integer\nfrom sqlalchemy import Boolean\nfrom sqlalchemy import String\nfrom sqlalchemy import Sequence\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import Text\nfrom sqlalchemy import create_engine as sqlalchemy_create_engine\nfrom sqlalchemy import Table\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import or_\nfrom sqlalchemy import and_\nfrom sqlalchemy import not_\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import aliased\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.orm import reconstructor\nfrom sqlalchemy.sql import select\nfrom sqlalchemy.sql import union\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom google.cloud.forseti.services.utils import mutual_exclusive\nfrom google.cloud.forseti.services.utils import to_full_resource_name\nfrom google.cloud.forseti.services import db\nfrom google.cloud.forseti.services.utils import get_sql_dialect\nfrom google.cloud.forseti.common.util import log_util\n\n# TODO: The next editor must remove this disable and correct issues.\n# pylint: disable=missing-type-doc,missing-return-type-doc,missing-return-doc\n# pylint: disable=missing-param-doc,missing-raises-doc,missing-yield-doc\n# pylint: disable=missing-yield-type-doc,too-many-branches\n\nLOGGER = log_util.get_logger(__name__)\nPOOL_RECYCLE_SECONDS = 300\nPER_YIELD = 1024\n\n\ndef generate_model_handle():\n    \"\"\"Generate random model handle.\"\"\"\n\n    return binascii.hexlify(os.urandom(16))\n\n\ndef generate_model_seed():\n    \"\"\"Generate random model seed.\"\"\"\n\n    return binascii.hexlify(os.urandom(16))\n\n\nMODEL_BASE = declarative_base()\n\n\nclass Model(MODEL_BASE):\n    \"\"\"Explain model object in database.\"\"\"\n\n    __tablename__ = 'model'\n    name = Column(String(32), primary_key=True)\n    handle = Column(String(32))\n    state = Column(String(32))\n    description = Column(Text())\n    watchdog_timer = Column(DateTime)\n    created_at = Column(DateTime)\n    etag_seed = Column(String(32), nullable=False)\n    message = Column(Text())\n    warnings = Column(Text(16777215))\n\n    def __init__(self, *args, **kwargs):\n        super(Model, self).__init__(*args, **kwargs)\n        # Non-SQL attributes\n        self.warning_store = list()\n\n    @reconstructor\n    def init_on_load(self):\n        \"\"\"Initialization of model when reconstructed from query.\"\"\"\n        self.warning_store = list()\n\n    def kick_watchdog(self):\n        \"\"\"Used during import to notify the import is still progressing.\"\"\"\n\n        self.watchdog_timer = datetime.datetime.utcnow()\n\n    def add_warning(self, warning):\n        \"\"\"Add a warning to the model.\n\n        Args:\n            warning (str): Warning message\n        \"\"\"\n        if warning:\n            self.warning_store.append(warning)\n\n    def get_warnings(self):\n        \"\"\"Returns any stored warnings.\"\"\"\n        if self.warning_store:\n            return \"\\n\".join(self.warning_store)\n        return \"\"\n\n    def set_inprogress(self):\n        \"\"\"Set state to 'in progress'.\"\"\"\n\n        self.state = 'INPROGRESS'\n\n    def add_description(self, description):\n        \"\"\"Add new description to the model\n\n        Args:\n            description (str): the description to be added in json format\n        \"\"\"\n\n        new_desc = json.loads(description)\n        model_desc = json.loads(self.description)\n\n        for new_item in new_desc:\n            model_desc[new_item] = new_desc[new_item]\n\n        self.description = json.dumps(model_desc)\n\n    def set_done(self, message=''):\n        \"\"\"Indicate a finished import.\n\n        Args:\n            message (str): Success message or ''\n        \"\"\"\n        warnings = self.get_warnings()\n        if warnings:\n            LOGGER.warn('warnings = %s', warnings)\n            self.warnings = warnings\n            self.state = 'PARTIAL_SUCCESS'\n        else:\n            self.state = 'SUCCESS'\n        self.message = message\n\n    def set_error(self, message):\n        \"\"\"Indicate a broken import.\"\"\"\n\n        self.state = 'BROKEN'\n        self.warnings = self.get_warnings()\n        self.message = message\n        LOGGER.error('warning = %s, message = %s',\n                     self.warnings, self.message)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n\n        return \"<Model(name='{}', handle='{}' state='{}')>\".format(\n            self.name, self.handle, self.state)\n\n\n# pylint: disable=too-many-locals,no-member\ndef define_model(model_name, dbengine, model_seed):\n    \"\"\"Defines table classes which point to the corresponding model.\n\n        This means, for each model being accessed this function needs to\n        be called in order to generate a full set of table definitions.\n\n        Models are name spaced via a random model seed such that multiple\n        models can exist within the same database. In order to implement\n        the name spacing in an abstract way.\n    \"\"\"\n\n    base = declarative_base()\n\n    denormed_group_in_group = '{}_group_in_group'.format(model_name)\n    bindings_tablename = '{}_bindings'.format(model_name)\n    roles_tablename = '{}_roles'.format(model_name)\n    permissions_tablename = '{}_permissions'.format(model_name)\n    members_tablename = '{}_members'.format(model_name)\n    resources_tablename = '{}_resources'.format(model_name)\n\n    role_permissions = Table('{}_role_permissions'.format(model_name),\n                             base.metadata,\n                             Column(\n                                 'roles_name', ForeignKey(\n                                     '{}.name'.format(roles_tablename)),\n                                 primary_key=True),\n                             Column(\n                                 'permissions_name', ForeignKey(\n                                     '{}.name'.format(permissions_tablename)),\n                                 primary_key=True), )\n\n    binding_members = Table('{}_binding_members'.format(model_name),\n                            base.metadata,\n                            Column(\n                                'bindings_id', ForeignKey(\n                                    '{}.id'.format(bindings_tablename)),\n                                primary_key=True),\n                            Column(\n                                'members_name', ForeignKey(\n                                    '{}.name'.format(members_tablename)),\n                                primary_key=True), )\n\n    group_members = Table('{}_group_members'.format(model_name),\n                          base.metadata,\n                          Column('group_name', ForeignKey(\n                              '{}.name'.format(members_tablename)),\n                                 primary_key=True),\n                          Column('members_name', ForeignKey(\n                              '{}.name'.format(members_tablename)),\n                                 primary_key=True), )\n\n    class Resource(base):\n        \"\"\"Row entry for a GCP resource.\"\"\"\n        __tablename__ = resources_tablename\n\n        full_name = Column(String(1024), nullable=False)\n        type_name = Column(String(256), primary_key=True)\n        name = Column(String(128), nullable=False)\n        type = Column(String(64), nullable=False)\n        policy_update_counter = Column(Integer, default=0)\n        display_name = Column(String(256), default='')\n        email = Column(String(256), default='')\n        data = Column(Text)\n\n        parent_type_name = Column(\n            String(128),\n            ForeignKey('{}.type_name'.format(resources_tablename)))\n        parent = relationship('Resource', remote_side=[type_name])\n        bindings = relationship('Binding', back_populates='resource')\n\n        def increment_update_counter(self):\n            \"\"\"Increments counter for this object's db updates.\"\"\"\n            self.policy_update_counter += 1\n\n        def get_etag(self):\n            \"\"\"Return the etag for this resource.\"\"\"\n            serialized_ctr = struct.pack('>I', self.policy_update_counter)\n            msg = binascii.hexlify(serialized_ctr)\n            msg += self.full_name\n            return hmac.new(model_seed.encode('utf-8'), msg).hexdigest()\n\n        def __repr__(self):\n            \"\"\"String representation.\"\"\"\n            return \"<Resource(full_name='{}', name='{}' type='{}')>\".format(\n                self.full_name, self.name, self.type)\n\n    Resource.children = relationship(\n        \"Resource\", order_by=Resource.full_name, back_populates=\"parent\")\n\n    class Member(base):\n        \"\"\"Row entry for a policy member.\"\"\"\n\n        __tablename__ = members_tablename\n        name = Column(String(256), primary_key=True)\n        type = Column(String(64))\n        member_name = Column(String(128))\n\n        parents = relationship(\n            'Member',\n            secondary=group_members,\n            primaryjoin=name == group_members.c.members_name,\n            secondaryjoin=name == group_members.c.group_name)\n\n        children = relationship(\n            'Member',\n            secondary=group_members,\n            primaryjoin=name == group_members.c.group_name,\n            secondaryjoin=name == group_members.c.members_name)\n\n        bindings = relationship('Binding',\n                                secondary=binding_members,\n                                back_populates='members')\n\n        def __repr__(self):\n            \"\"\"String representation.\"\"\"\n            return \"<Member(name='{}', type='{}')>\".format(\n                self.name, self.type)\n\n    class GroupInGroup(base):\n        \"\"\"Row for a group-in-group membership.\"\"\"\n\n        __tablename__ = denormed_group_in_group\n        parent = Column(String(256), primary_key=True)\n        member = Column(String(256), primary_key=True)\n\n        def __repr__(self):\n            \"\"\"String representation.\"\"\"\n            return \"<GroupInGroup(parent='{}', member='{}')>\".format(\n                self.parent,\n                self.member)\n\n    class Binding(base):\n        \"\"\"Row for a binding between resource, roles and members.\"\"\"\n\n        __tablename__ = bindings_tablename\n        id = Column(Integer, Sequence('{}_id_seq'.format(bindings_tablename)),\n                    primary_key=True)\n\n        resource_type_name = Column(String(128), ForeignKey(\n            '{}.type_name'.format(resources_tablename)))\n        role_name = Column(String(128), ForeignKey(\n            '{}.name'.format(roles_tablename)))\n\n        resource = relationship('Resource', remote_side=[resource_type_name])\n        role = relationship('Role', remote_side=[role_name])\n\n        members = relationship('Member',\n                               secondary=binding_members,\n                               back_populates='bindings')\n\n        def __repr__(self):\n            fmt_s = \"<Binding(id='{}', role='{}', resource='{}' members='{}')>\"\n            return fmt_s.format(\n                self.id,\n                self.role_name,\n                self.resource_type_name,\n                self.members)\n\n    class Role(base):\n        \"\"\"Row entry for an IAM role.\"\"\"\n\n        __tablename__ = roles_tablename\n        name = Column(String(128), primary_key=True)\n        title = Column(String(128), default='')\n        stage = Column(String(128), default='')\n        description = Column(String(256), default='')\n        custom = Column(Boolean, default=False)\n        permissions = relationship('Permission',\n                                   secondary=role_permissions,\n                                   back_populates='roles')\n\n        def __repr__(self):\n            return \"<Role(name='%s')>\" % (self.name)\n\n    class Permission(base):\n        \"\"\"Row entry for an IAM permission.\"\"\"\n\n        __tablename__ = permissions_tablename\n        name = Column(String(128), primary_key=True)\n        roles = relationship('Role',\n                             secondary=role_permissions,\n                             back_populates='permissions')\n\n        def __repr__(self):\n            return \"<Permission(name='%s')>\" % (self.name)\n\n    # pylint: disable=too-many-public-methods\n    class ModelAccess(object):\n        \"\"\"Data model facade, implement main API against database.\"\"\"\n\n        TBL_GROUP_IN_GROUP = GroupInGroup\n        TBL_BINDING = Binding\n        TBL_MEMBER = Member\n        TBL_PERMISSION = Permission\n        TBL_ROLE = Role\n        TBL_RESOURCE = Resource\n        TBL_MEMBERSHIP = group_members\n\n        @classmethod\n        def delete_all(cls, engine):\n            \"\"\"Delete all data from the model.\"\"\"\n\n            LOGGER.info(\"deleting all data \"\n                        \"from the model\")\n            role_permissions.drop(engine)\n            binding_members.drop(engine)\n            group_members.drop(engine)\n\n            Binding.__table__.drop(engine)\n            Permission.__table__.drop(engine)\n            GroupInGroup.__table__.drop(engine)\n\n            Role.__table__.drop(engine)\n            Member.__table__.drop(engine)\n            Resource.__table__.drop(engine)\n\n        @classmethod\n        def denorm_group_in_group(cls, session):\n            \"\"\"Denormalize group-in-group relation.\n\n            Args:\n                session (object): Database session to use.\n            Returns:\n                int: Number of iterations.\n            \"\"\"\n\n            tbl1 = aliased(GroupInGroup.__table__, name='alias1')\n            tbl2 = aliased(GroupInGroup.__table__, name='alias2')\n            tbl3 = aliased(GroupInGroup.__table__, name='alias3')\n\n            if get_sql_dialect(session) != 'sqlite':\n                # Lock tables for denormalization\n                # including aliases 1-3\n                locked_tables = [\n                    '`{}`'.format(GroupInGroup.__tablename__),\n                    '`{}` as {}'.format(\n                        GroupInGroup.__tablename__,\n                        tbl1.name),\n                    '`{}` as {}'.format(\n                        GroupInGroup.__tablename__,\n                        tbl2.name),\n                    '`{}` as {}'.format(\n                        GroupInGroup.__tablename__,\n                        tbl3.name),\n                    '`{}`'.format(group_members.name)]\n                lock_stmts = ['{} WRITE'.format(tbl) for tbl in locked_tables]\n                query = 'LOCK TABLES {}'.format(', '.join(lock_stmts))\n                session.execute(query)\n            try:\n                # Remove all existing rows in the denormalization\n                session.execute(GroupInGroup.__table__.delete())\n\n                # Select member relation into GroupInGroup\n                qry = (\n                    GroupInGroup.__table__.insert()\n                    .from_select(\n                        ['parent', 'member'],\n                        group_members.select()\n                        .where(\n                            group_members.c.group_name.startswith('group/')\n                            )\n                        .where(\n                            group_members.c.members_name.startswith('group/')\n                            )\n                        )\n                    )\n\n                session.execute(qry)\n\n                iterations = 0\n                rows_affected = True\n                while rows_affected:\n\n                    # Join membership on its own to find transitive\n                    expansion = tbl1.join(tbl2, tbl1.c.member == tbl2.c.parent)\n\n                    # Left outjoin to find the entries that\n                    # are already in the table to prevent\n                    # inserting already existing entries\n                    expansion = expansion.outerjoin(\n                        tbl3,\n                        and_(tbl1.c.parent == tbl3.c.parent,\n                             tbl2.c.member == tbl3.c.member))\n\n                    # Select only such elements that are not\n                    # already in the table, indicated as NULL\n                    # values through the outer-left-join\n                    stmt = (\n                        select([tbl1.c.parent, tbl2.c.member])\n                        .select_from(expansion)\n                        .where(tbl3.c.parent == None)\n                        .distinct())\n\n                    # Execute the query and insert into the table\n                    qry = (\n                        GroupInGroup.__table__.insert()\n                        .from_select(\n                            ['parent', 'member'],\n                            stmt))\n\n                    rows_affected = bool(session.execute(qry).rowcount)\n                    iterations += 1\n            except Exception:\n                LOGGER.error(Exception.message)\n                session.rollback()\n                raise\n            finally:\n                if get_sql_dialect(session) != 'sqlite':\n                    session.execute('UNLOCK TABLES')\n                session.commit()\n            return iterations\n\n        @classmethod\n        def explain_granted(cls, session, member_name, resource_type_name,\n                            role, permission):\n            \"\"\"Provide info about how the member has access to the resource.\"\"\"\n            members, member_graph = cls.reverse_expand_members(\n                session, [member_name], request_graph=True)\n            member_names = [m.name for m in members]\n            resource_type_names = [r.type_name for r in\n                                   cls.find_resource_path(session,\n                                                          resource_type_name)]\n\n            if role:\n                roles = set([role])\n                qry = session.query(Binding, Member).join(\n                    binding_members).join(Member)\n            else:\n                roles = [r.name for r in\n                         cls.get_roles_by_permission_names(\n                             session,\n                             [permission])]\n                qry = session.query(Binding, Member)\n                qry = qry.join(binding_members).join(Member)\n                qry = qry.join(Role).join(role_permissions).join(Permission)\n\n            qry = qry.filter(Binding.role_name.in_(roles))\n            qry = qry.filter(Member.name.in_(member_names))\n            qry = qry.filter(\n                Binding.resource_type_name.in_(resource_type_names))\n            result = qry.all()\n            if not result:\n                error_message = 'Grant not found: ({},{},{})'.format(\n                    member_name,\n                    resource_type_name,\n                    role if role is not None else permission)\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n            else:\n                bindings = [(b.resource_type_name, b.role_name, m.name)\n                            for b, m in result]\n                return bindings, member_graph, resource_type_names\n\n        @classmethod\n        def scanner_iter(cls, session, resource_type,\n                         parent_type_name=None):\n            \"\"\"Iterate over all resources with the specified type.\"\"\"\n\n            qry = (\n                session.query(Resource)\n                .filter(Resource.type == resource_type))\n\n            if parent_type_name:\n                qry = qry.filter(Resource.parent_type_name == parent_type_name)\n\n            for resource in qry.yield_per(PER_YIELD):\n                yield resource\n\n        @classmethod\n        def explain_denied(cls, session, member_name, resource_type_names,\n                           permission_names, role_names):\n            \"\"\"Provide information how to grant access to a member.\"\"\"\n\n            if not role_names:\n                role_names = [r.name for r in\n                              cls.get_roles_by_permission_names(\n                                  session,\n                                  permission_names)]\n                if not role_names:\n                    error_message = 'No roles covering requested permission set'\n                    LOGGER.error(error_message)\n                    raise Exception(error_message)\n\n            resource_hierarchy = (\n                cls.resource_ancestors(session,\n                                       resource_type_names))\n\n            def find_binding_candidates(resource_hierarchy):\n                \"\"\"Find the root node in the ancestors.\n\n                    From there, walk down the resource tree and add\n                    every node until a node has more than one child.\n                    This is the set of nodes which grants access to\n                    at least all of the resources requested.\n                    There is always a chain with a single node root.\n                \"\"\"\n\n                root = None\n                for parent in resource_hierarchy.iterkeys():\n                    is_root = True\n                    for children in resource_hierarchy.itervalues():\n                        if parent in children:\n                            is_root = False\n                            break\n                    if is_root:\n                        root = parent\n                chain = [root]\n                cur = root\n                while len(resource_hierarchy[cur]) == 1:\n                    cur = iter(resource_hierarchy[cur]).next()\n                    chain.append(cur)\n                return chain\n\n            bind_res_candidates = find_binding_candidates(\n                resource_hierarchy)\n\n            bindings = (\n                session.query(Binding, Member)\n                .join(binding_members).join(Member).join(Role)\n                .filter(Binding.resource_type_name.in_(bind_res_candidates))\n                .filter(Role.name.in_(role_names))\n                .filter(or_(Member.type == 'group',\n                            Member.name == member_name))\n                .filter(and_(binding_members.c.bindings_id == Binding.id,\n                             binding_members.c.members_name == Member.name))\n                .filter(Role.name == Binding.role_name)\n                .all())\n\n            strategies = []\n            for resource in bind_res_candidates:\n                for role_name in role_names:\n                    overgranting = (len(bind_res_candidates) -\n                                    bind_res_candidates.index(resource) -\n                                    1)\n                    strategies.append(\n                        (overgranting, [\n                            (role, member_name, resource)\n                            for role in [role_name]]))\n            if bindings:\n                for binding, member in bindings:\n                    overgranting = (len(bind_res_candidates) - 1 -\n                                    bind_res_candidates.index(\n                                        binding.resource_type_name))\n                    strategies.append(\n                        (overgranting, [\n                            (binding.role_name,\n                             member.name,\n                             binding.resource_type_name)]))\n\n            return strategies\n\n        @classmethod\n        def query_access_by_member(cls, session, member_name, permission_names,\n                                   expand_resources=False,\n                                   reverse_expand_members=True):\n            \"\"\"Return the set of resources the member has access to.\"\"\"\n\n            if reverse_expand_members:\n                member_names = [m.name for m in\n                                cls.reverse_expand_members(\n                                    session,\n                                    [member_name], False)]\n            else:\n                member_names = [member_name]\n\n            roles = cls.get_roles_by_permission_names(\n                session, permission_names)\n\n            qry = (\n                session.query(Binding)\n                .join(binding_members)\n                .join(Member)\n                .filter(Binding.role_name.in_([r.name for r in roles]))\n                .filter(Member.name.in_(member_names)))\n\n            bindings = qry.yield_per(1024)\n            if not expand_resources:\n                return [(binding.role_name,\n                         [binding.resource_type_name]) for binding in bindings]\n\n            r_type_names = [binding.resource_type_name for binding in bindings]\n            expansion = cls.expand_resources_by_type_names(\n                session,\n                r_type_names)\n\n            res_exp = {k.type_name:\n                       [v.type_name for v in values]\n                       for k, values in expansion.iteritems()}\n\n            return [(binding.role_name,\n                     res_exp[binding.resource_type_name])\n                    for binding in bindings]\n\n        @classmethod\n        def query_access_by_permission(cls,\n                                       session,\n                                       role_name=None,\n                                       permission_name=None,\n                                       expand_groups=False,\n                                       expand_resources=False):\n            \"\"\"Return all the (Principal, Resource) combinations allowing\n            satisfying access via the specified permission.\n\n            Args:\n                session (object): Database session.\n                permission_name (str): Permission name to query for.\n                expand_groups (bool): Whether or not to expand groups.\n                expand_resources (bool): Whether or not to expand resources.\n\n            Yields:\n                A generator of access tuples.\n\n            Raises:\n                ValueError: If neither role nor permission is set.\n            \"\"\"\n\n            if role_name:\n                role_names = [role_name]\n            elif permission_name:\n                role_names = [p.name for p in\n                              cls.get_roles_by_permission_names(\n                                  session,\n                                  [permission_name])]\n            else:\n                error_message = 'Either role or permission must be set'\n                LOGGER.error(error_message)\n                raise ValueError(error_message)\n\n            if expand_resources:\n                expanded_resources = aliased(Resource)\n                qry = (\n                    session.query(expanded_resources, Binding, Member)\n                    .filter(binding_members.c.bindings_id == Binding.id)\n                    .filter(binding_members.c.members_name == Member.name)\n                    .filter(expanded_resources.full_name.startswith(\n                        Resource.full_name))\n                    .filter(Resource.type_name == Binding.resource_type_name)\n                    .filter(Binding.role_name.in_(role_names)))\n            else:\n                qry = (\n                    session.query(Resource, Binding, Member)\n                    .filter(binding_members.c.bindings_id == Binding.id)\n                    .filter(binding_members.c.members_name == Member.name)\n                    .filter(Resource.type_name == Binding.resource_type_name)\n                    .filter(Binding.role_name.in_(role_names)))\n\n            qry = qry.order_by(Resource.name.asc(), Binding.role_name.asc())\n\n            if expand_groups:\n                to_expand = set([m.name for _, _, m in\n                                 qry.yield_per(PER_YIELD)])\n                expansion = cls.expand_members_map(session,\n                                                   to_expand,\n                                                   show_group_members=False,\n                                                   member_contain_self=True)\n\n            qry = qry.distinct()\n\n            cur_resource = None\n            cur_role = None\n            cur_members = set()\n            for resource, binding, member in qry.yield_per(PER_YIELD):\n                if cur_resource != resource.type_name:\n                    if cur_resource is not None:\n                        yield cur_role, cur_resource, cur_members\n                    cur_resource = resource.type_name\n                    cur_role = binding.role_name\n                    cur_members = set()\n                if expand_groups:\n                    for member_name in expansion[member.name]:\n                        cur_members.add(member_name)\n                else:\n                    cur_members.add(member.name)\n            if cur_resource is not None:\n                yield cur_role, cur_resource, cur_members\n\n        @classmethod\n        def query_access_by_resource(cls, session, resource_type_name,\n                                     permission_names, expand_groups=False):\n            \"\"\"Return members who have access to the given resource.\"\"\"\n\n            roles = cls.get_roles_by_permission_names(\n                session, permission_names)\n            resources = cls.find_resource_path(session, resource_type_name)\n\n            res = (session.query(Binding, Member)\n                   .filter(\n                       Binding.role_name.in_([r.name for r in roles]),\n                       Binding.resource_type_name.in_(\n                           [r.type_name for r in resources]))\n                   .join(binding_members).join(Member))\n\n            role_member_mapping = collections.defaultdict(set)\n            for binding, member in res:\n                role_member_mapping[binding.role_name].add(member.name)\n\n            if expand_groups:\n                for role in role_member_mapping:\n                    role_member_mapping[role] = (\n                        [m.name for m in cls.expand_members(\n                            session,\n                            role_member_mapping[role])])\n\n            return role_member_mapping\n\n        @classmethod\n        def query_permissions_by_roles(cls, session, role_names, role_prefixes,\n                                       _=1024):\n            \"\"\"Resolve permissions for the role.\"\"\"\n\n            if not role_names and not role_prefixes:\n                error_message = 'No roles or role prefixes specified'\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n            qry = session.query(Role, Permission).join(\n                role_permissions).join(Permission)\n            if role_names:\n                qry = qry.filter(Role.name.in_(role_names))\n            if role_prefixes:\n                qry = qry.filter(\n                    or_(*[Role.name.startswith(prefix)\n                          for prefix in role_prefixes]))\n            return qry.all()\n\n        @classmethod\n        def denormalize(cls, session):\n            \"\"\"Denormalize the model into access triples.\"\"\"\n\n            qry = (session.query(Binding)\n                   .join(binding_members)\n                   .join(Member))\n\n            members = set()\n            for binding in qry.yield_per(PER_YIELD):\n                for member in binding.members:\n                    members.add(member.name)\n\n            expanded_members = cls.expand_members_map(session, members)\n            role_permissions_map = collections.defaultdict(set)\n\n            qry = (session.query(Role, Permission)\n                   .join(role_permissions)\n                   .filter(\n                       Role.name == role_permissions.c.roles_name)\n                   .filter(\n                       Permission.name == role_permissions.c.permissions_name))\n\n            for role, permission in qry.yield_per(PER_YIELD):\n                role_permissions_map[role.name].add(permission.name)\n\n            for binding, member in (\n                    session.query(Binding, Member)\n                    .join(binding_members)\n                    .filter(binding_members.c.bindings_id == Binding.id)\n                    .filter(binding_members.c.members_name == Member.name)\n                    .yield_per(PER_YIELD)):\n\n                resource_type_name = binding.resource_type_name\n                resource_mapping = cls.expand_resources_by_type_names(\n                    session,\n                    [resource_type_name])\n\n                resource_mapping = {k.type_name: set([m.type_name for m in v])\n                                    for k, v in resource_mapping.iteritems()}\n\n                for expanded_member in expanded_members[member.name]:\n                    for permission in role_permissions_map[binding.role_name]:\n                        for res in resource_mapping[resource_type_name]:\n                            triple = (permission, res, expanded_member)\n                            yield triple\n\n        @classmethod\n        def set_iam_policy(cls, session, resource_type_name, policy):\n            \"\"\"Sets an IAM policy for the resource.\"\"\"\n\n            LOGGER.info(\"Setting IAM policy, resource_type_name = %s, policy\"\n                        \" = %s, session = %s\",\n                        resource_type_name, policy, session)\n            old_policy = cls.get_iam_policy(session, resource_type_name)\n            if policy['etag'] != old_policy['etag']:\n                error_message = 'Etags distinct, stored={}, provided={}'.format(\n                    old_policy['etag'], policy['etag'])\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n\n            old_policy = old_policy['bindings']\n            policy = policy['bindings']\n\n            def filter_etag(policy):\n                \"\"\"Filter etag key/value out of policy map.\"\"\"\n\n                return {k: v for k, v in policy.iteritems() if k != 'etag'}\n\n            def calculate_diff(policy, old_policy):\n                \"\"\"Calculate the grant/revoke difference between policies.\"\"\"\n\n                diff = collections.defaultdict(list)\n                for role, members in filter_etag(policy).iteritems():\n                    if role in old_policy:\n                        for member in members:\n                            if member not in old_policy[role]:\n                                diff[role].append(member)\n                    else:\n                        diff[role] = members\n                return diff\n\n            grants = calculate_diff(policy, old_policy)\n            revocations = calculate_diff(old_policy, policy)\n\n            for role, members in revocations.iteritems():\n                bindings = (\n                    session.query(Binding)\n                    .filter(Binding.resource_type_name == resource_type_name)\n                    .filter(Binding.role_name == role)\n                    .join(binding_members).join(Member)\n                    .filter(Member.name.in_(members)).all())\n\n                for binding in bindings:\n                    session.delete(binding)\n            for role, members in grants.iteritems():\n                inserted = False\n                existing_bindings = (\n                    session.query(Binding)\n                    .filter(Binding.resource_type_name == resource_type_name)\n                    .filter(Binding.role_name == role).all())\n\n                for binding in existing_bindings:\n                    if binding.role_name == role:\n                        inserted = True\n                        for member in members:\n                            binding.members.append(\n                                session.query(Member).filter(\n                                    Member.name == member).one())\n                if not inserted:\n                    binding = Binding(\n                        resource_type_name=resource_type_name,\n                        role=session.query(Role).filter(\n                            Role.name == role).one())\n                    binding.members = session.query(Member).filter(\n                        Member.name.in_(members)).all()\n                    session.add(binding)\n            resource = session.query(Resource).filter(\n                Resource.type_name == resource_type_name).one()\n            resource.increment_update_counter()\n            session.commit()\n\n        @classmethod\n        def get_iam_policy(cls, session, resource_type_name):\n            \"\"\"Return the IAM policy for a resource.\"\"\"\n\n            resource = session.query(Resource).filter(\n                Resource.type_name == resource_type_name).one()\n            policy = {\n                'etag': resource.get_etag(),\n                'bindings': {},\n                'resource': resource.type_name}\n            for binding in (session.query(Binding)\n                            .filter(Binding.resource_type_name ==\n                                    resource_type_name)\n                            .all()):\n                role = binding.role_name\n                members = [m.name for m in binding.members]\n                policy['bindings'][role] = members\n            return policy\n\n        @classmethod\n        def check_iam_policy(cls, session, resource_type_name, permission_name,\n                             member_name):\n            \"\"\"Check access according to the resource IAM policy.\"\"\"\n\n            member_names = [m.name for m in\n                            cls.reverse_expand_members(\n                                session,\n                                [member_name])]\n            resource_type_names = [r.type_name for r in cls.find_resource_path(\n                session,\n                resource_type_name)]\n\n            if not member_names:\n                error_message = 'Member not found: {}'.format(member_name)\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n            if not resource_type_names:\n                error_message = 'Resource not found: {}'\\\n                    .format(resource_type_name)\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n\n            return (\n                session.query(Permission)\n                .filter(Permission.name == permission_name)\n                .join(role_permissions).join(Role).join(Binding)\n                .filter(Binding.resource_type_name.in_(resource_type_names))\n                .join(binding_members).join(Member)\n                .filter(Member.name.in_(member_names)).first() is not None)\n\n        @classmethod\n        def list_roles_by_prefix(cls, session, role_prefix):\n            \"\"\"Provides a list of roles matched via name prefix.\"\"\"\n\n            return [r.name for r in session.query(Role)\n                    .filter(Role.name.startswith(role_prefix)).all()]\n\n        @classmethod\n        def add_role_by_name(cls, session, role_name, permission_names):\n            \"\"\"Creates a new role.\"\"\"\n\n            LOGGER.info(\"Creating a new role, role_name = %s, permission_names\"\n                        \" = %s, session = %s\",\n                        role_name, permission_names, session)\n            permission_names = set(permission_names)\n            existing_permissions = session.query(Permission).filter(\n                Permission.name.in_(permission_names)).all()\n            for existing_permission in existing_permissions:\n                try:\n                    permission_names.remove(existing_permission.name)\n                except KeyError:\n                    LOGGER.warn(\"existing_permissions.name = %s, KeyError\",\n                                existing_permission.name)\n\n            new_permissions = [Permission(name=n) for n in permission_names]\n            for perm in new_permissions:\n                session.add(perm)\n            cls.add_role(session, role_name,\n                         existing_permissions + new_permissions)\n            session.commit()\n\n        @classmethod\n        def delete_role_by_name(cls, session, role_name):\n            \"\"\"Deletes a role by name.\"\"\"\n            LOGGER.info(\"Deleting an existing role, role_name = %s,\"\n                        \" session = %s\", role_name, session)\n            bindings_to_be_delete = [binding for binding in\n                                     session.query(Binding)\n                                     .filter(Binding.role_name == role_name)\n                                     .all()]\n            session.delete(session.query(Role)\n                           .filter(Role.name == role_name)\n                           .first())\n            for binding in bindings_to_be_delete:\n                session.delete(binding)\n            session.commit()\n\n        @classmethod\n        def add_group_member(cls,\n                             session,\n                             member_type_name,\n                             parent_type_names,\n                             denorm=False):\n            \"\"\"Add member, optionally with parent relationship.\"\"\"\n\n            LOGGER.info(\"Adding a member, member_type_name = %s, \"\n                        \"parent_type_names = %s, denorm = %s, session = %s\",\n                        member_type_name, parent_type_names, denorm, session)\n\n            cls.add_member(session,\n                           member_type_name,\n                           parent_type_names,\n                           denorm)\n            session.commit()\n\n        @classmethod\n        def delete_group_member(cls, session, member_type_name,\n                                parent_type_name, only_delete_relationship,\n                                denorm=False):\n            \"\"\"Delete member.\"\"\"\n\n            LOGGER.info(\"Deleting a member, member_type_name = %s, \"\n                        \"parent_type_name = %s, only_delete_relationship = %s,\"\n                        \" denorm = %s, session = %s\", member_type_name,\n                        parent_type_name, only_delete_relationship,\n                        denorm, session)\n            if only_delete_relationship:\n                group_members_delete = group_members.delete(\n                    and_(group_members.c.members_name == member_type_name,\n                         group_members.c.group_name == parent_type_name))\n                session.execute(group_members_delete)\n            else:\n                member_to_be_deleted = session.query(Member).filter(\n                    Member.name == member_type_name).first()\n                session.delete(member_to_be_deleted)\n                session.commit()\n            if denorm or member_type_name.startswith('group'):\n                cls.denorm_group_in_group(session)\n\n        @classmethod\n        def list_group_members(cls, session, member_name_prefix):\n            \"\"\"Returns members filtered by prefix.\"\"\"\n\n            return [m.name for m in session.query(Member).filter(\n                Member.member_name.startswith(member_name_prefix)).all()]\n\n        @classmethod\n        def iter_groups(cls, session):\n            \"\"\"Returns iterator of all groups in model.\"\"\"\n\n            qry = session.query(Member).filter(Member.type == 'group')\n            for group in qry.yield_per(1024):\n                yield group\n\n        @classmethod\n        def iter_resources_by_prefix(cls,\n                                     session,\n                                     full_resource_name_prefix=None,\n                                     type_name_prefix=None,\n                                     type_prefix=None,\n                                     name_prefix=None):\n            \"\"\"Returns iterator to resources filtered by prefix.\"\"\"\n\n            if not any([arg is not None for arg in [full_resource_name_prefix,\n                                                    type_name_prefix,\n                                                    type_prefix,\n                                                    name_prefix]]):\n                error_message = 'At least one prefix must be set'\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n\n            qry = session.query(Resource)\n            if full_resource_name_prefix:\n                qry = qry.filter(Resource.full_name.startswith(\n                    full_resource_name_prefix))\n            if type_name_prefix:\n                qry = qry.filter(Resource.type_name.startswith(\n                    type_name_prefix))\n            if type_prefix:\n                qry = qry.filter(Resource.type.startswith(\n                    type_prefix))\n            if name_prefix:\n                qry = qry.filter(Resource.name.startswith(\n                    name_prefix))\n\n            for resource in qry.yield_per(1024):\n                yield resource\n\n        @classmethod\n        def list_resources_by_prefix(cls,\n                                     session,\n                                     full_resource_name_prefix=None,\n                                     type_name_prefix=None,\n                                     type_prefix=None,\n                                     name_prefix=None):\n            \"\"\"Returns resources filtered by prefix.\"\"\"\n\n            return list(\n                cls.iter_resources_by_prefix(session,\n                                             full_resource_name_prefix,\n                                             type_name_prefix,\n                                             type_prefix,\n                                             name_prefix))\n\n        @classmethod\n        def add_resource_by_name(cls,\n                                 session,\n                                 resource_type_name,\n                                 parent_type_name,\n                                 no_require_parent):\n            \"\"\"Adds resource specified via full name.\"\"\"\n\n            LOGGER.info(\"Adding resource via full name, resource_type_name\"\n                        \" = %s, parent_type_name = %s, no_require_parent = %s,\"\n                        \" session = %s\", resource_type_name,\n                        parent_type_name, no_require_parent, session)\n            if not no_require_parent:\n                parent = session.query(Resource).filter(\n                    Resource.type_name == parent_type_name).one()\n            else:\n                parent = None\n            return cls.add_resource(session, resource_type_name, parent)\n\n        @classmethod\n        def add_resource(cls, session, resource_type_name, parent=None):\n            \"\"\"Adds resource by name.\"\"\"\n\n            LOGGER.info(\"Adding resource by name, resource_type_name = %s,\"\n                        \" session = %s\", resource_type_name, session)\n            res_type, res_name = resource_type_name.split('/')\n            parent_full_resource_name = (\n                '' if parent is None else parent.full_name)\n\n            full_resource_name = to_full_resource_name(\n                parent_full_resource_name,\n                resource_type_name)\n\n            resource = Resource(full_name=full_resource_name,\n                                type_name=resource_type_name,\n                                name=res_name,\n                                type=res_type,\n                                parent=parent)\n            session.add(resource)\n            return resource\n\n        @classmethod\n        def add_role(cls, session, name, permissions=None):\n            \"\"\"Add role by name.\"\"\"\n\n            LOGGER.info(\"Adding role, name = %s, permissions = %s, \"\n                        \"session = %s\", name, permissions, session)\n            permissions = [] if permissions is None else permissions\n            role = Role(name=name, permissions=permissions)\n            session.add(role)\n            return role\n\n        @classmethod\n        def add_permission(cls, session, name, roles=None):\n            \"\"\"Add permission by name.\"\"\"\n\n            LOGGER.info(\"Adding permission, name = %s, roles = %s\"\n                        \" session = %s\", name, roles, session)\n            roles = [] if roles is None else roles\n            permission = Permission(name=name, roles=roles)\n            session.add(permission)\n            return permission\n\n        @classmethod\n        def add_binding(cls, session, resource, role, members):\n            \"\"\"Add a binding to the model.\"\"\"\n\n            LOGGER.info(\"Adding a binding to the model, resource = %s, \"\n                        \"role = %s, members = %s, session = %s\",\n                        resource, role, members, session)\n            binding = Binding(resource=resource, role=role, members=members)\n            session.add(binding)\n            return binding\n\n        @classmethod\n        def add_member(cls,\n                       session,\n                       type_name,\n                       parent_type_names=None,\n                       denorm=False):\n            \"\"\"Add a member to the model.\"\"\"\n\n            LOGGER.info(\"Adding a member to the model, type_name = %s, \"\n                        \"parent_type_names = %s, denorm = %s, session = %s\",\n                        type_name, parent_type_names, denorm, session)\n            if not parent_type_names:\n                parent_type_names = []\n            res_type, name = type_name.split('/', 1)\n            parents = session.query(Member).filter(\n                Member.name.in_(parent_type_names)).all()\n            if len(parents) != len(parent_type_names):\n                msg = 'parents: {}, expected: {}'.format(\n                    parents, parent_type_names)\n                error_message = 'Parent not found, {}'.format(msg)\n                LOGGER.error(error_message)\n                raise Exception(error_message)\n\n            member = Member(name=type_name,\n                            member_name=name,\n                            type=res_type,\n                            parents=parents)\n            session.add(member)\n            session.commit()\n            if denorm and res_type == 'group' and parents:\n                cls.denorm_group_in_group(session)\n            return member\n\n        @classmethod\n        def expand_resources_by_type_names(cls, session, res_type_names):\n            \"\"\"Expand resources by type/name format.\n\n                Returns: {res_type_name: Expansion(res_type_name), ... }\n            \"\"\"\n\n            res_key = aliased(Resource, name='res_key')\n            res_values = aliased(Resource, name='res_values')\n\n            expressions = []\n            for res_type_name in res_type_names:\n                expressions.append(and_(\n                    res_key.type_name == res_type_name))\n\n            res = (\n                session.query(res_key, res_values)\n                .filter(res_key.type_name.in_(res_type_names))\n                .filter(res_values.full_name.startswith(\n                    res_key.full_name)).yield_per(1024))\n\n            mapping = collections.defaultdict(set)\n            for k, value in res:\n                mapping[k].add(value)\n            return mapping\n\n        @classmethod\n        def expand_resources_by_names(cls, session, res_type_names):\n            \"\"\"Expand resources by type/name format.\"\"\"\n\n            qry = (\n                session.query(Resource)\n                .filter(Resource.type_name.in_(res_type_names))\n                )\n\n            full_resource_names = [r.full_name for r in qry.all()]\n            return cls.expand_resources(session, full_resource_names)\n\n        @classmethod\n        def expand_resources(cls, session, full_resource_names):\n            \"\"\"Expand resources towards the bottom.\"\"\"\n\n            if (not isinstance(full_resource_names, list) and\n                    not isinstance(full_resource_names, set)):\n                error_message = 'full_resource_names must be list or set'\n                LOGGER.error(error_message)\n                raise TypeError(error_message)\n\n            resources = session.query(Resource).filter(\n                Resource.full_name.in_(full_resource_names)).all()\n\n            new_resource_set = set(resources)\n            resource_set = set(resources)\n\n            def add_to_sets(resources):\n                \"\"\"Adds resources to the sets.\"\"\"\n\n                for resource in resources:\n                    if resource not in resource_set:\n                        new_resource_set.add(resource)\n                        resource_set.add(resource)\n\n            while new_resource_set:\n                resources_to_walk = new_resource_set\n                new_resource_set = set()\n                for resource in resources_to_walk:\n                    add_to_sets(resource.children)\n\n            return [r.full_name for r in resource_set]\n\n        @classmethod\n        def reverse_expand_members(cls, session, member_names,\n                                   request_graph=False):\n            \"\"\"Expand members to their groups.\"\"\"\n\n            members = session.query(Member).filter(\n                Member.name.in_(member_names)).all()\n            membership_graph = collections.defaultdict(set)\n            member_set = set()\n            new_member_set = set()\n\n            def add_to_sets(members, child):\n                \"\"\"Adds the members & children to the sets.\"\"\"\n\n                for member in members:\n                    if request_graph and child:\n                        membership_graph[child.name].add(member.name)\n                    if request_graph and not child:\n                        if member.name not in membership_graph:\n                            membership_graph[member.name] = set()\n                    if member not in member_set:\n                        new_member_set.add(member)\n                        member_set.add(member)\n\n            add_to_sets(members, None)\n            while new_member_set:\n                members_to_walk = new_member_set\n                new_member_set = set()\n                for member in members_to_walk:\n                    add_to_sets(member.parents, member)\n\n            if request_graph:\n                return member_set, membership_graph\n            return member_set\n\n        @classmethod\n        def expand_members_map(cls,\n                               session,\n                               member_names,\n                               show_group_members=True,\n                               member_contain_self=True):\n            \"\"\"Expand group membership keyed by member.\n\n            Args:\n                member_names (set): Member names to expand\n                show_group_members (bool): Whether to include subgroups\n                member_contain_self (bool): Whether to include a parent\n                                            as its own member\n            Returns:\n                dict: <Member, set(Children).\n            \"\"\"\n\n            def separate_groups(member_names):\n                \"\"\"Separate groups and other members in two lists.\"\"\"\n                groups = []\n                others = []\n                for name in member_names:\n                    if name.startswith('group/'):\n                        groups.append(name)\n                    else:\n                        others.append(name)\n                return groups, others\n\n            selectables = []\n            group_names, other_names = separate_groups(member_names)\n\n            t_ging = GroupInGroup.__table__\n            t_members = group_members\n\n            transitive_membership = (\n                # This resolves groups to its transitive non-group members\n                select([t_ging.c.parent, t_members.c.members_name])\n                .select_from(\n                    t_ging.join(t_members,\n                                t_ging.c.member == t_members.c.group_name))\n                ).where(t_ging.c.parent.in_(group_names))\n            if not show_group_members:\n                transitive_membership = transitive_membership.where(\n                    not_(t_members.c.members_name.startswith('group/')))\n\n            selectables.append(\n                transitive_membership.alias('transitive_membership'))\n\n            direct_membership = (\n                select([t_members.c.group_name, t_members.c.members_name])\n                .where(t_members.c.group_name.in_(group_names)))\n            if not show_group_members:\n                direct_membership = direct_membership.where(\n                    not_(t_members.c.members_name.startswith('group/')))\n\n            selectables.append(\n                direct_membership.alias('direct_membership'))\n\n            if show_group_members:\n                # Show groups as members of other groups\n                group_in_groups = (\n                    select([t_ging.c.parent, t_ging.c.member])\n                    .where(t_ging.c.parent.in_(group_names))\n                    )\n                selectables.append(\n                    group_in_groups.alias('group_in_groups'))\n\n            # Union all the queries\n            qry = union(*selectables)\n\n            # Build the result dict\n            result = collections.defaultdict(set)\n            for parent, child in session.execute(qry):\n                result[parent].add(child)\n            for parent in other_names:\n                result[parent] = set()\n\n            # Add each parent as its own member\n            if member_contain_self:\n                for name in member_names:\n                    result[name].add(name)\n            return result\n\n        @classmethod\n        def expand_members(cls, session, member_names):\n            \"\"\"Expand group membership towards the members.\"\"\"\n\n            members = session.query(Member).filter(\n                Member.name.in_(member_names)).all()\n\n            def is_group(member):\n                \"\"\"Returns true iff the member is a group.\"\"\"\n                return member.type == 'group'\n\n            group_set = set()\n            non_group_set = set()\n            new_group_set = set()\n\n            def add_to_sets(members):\n                \"\"\"Adds new members to the sets.\"\"\"\n                for member in members:\n                    if is_group(member):\n                        if member not in group_set:\n                            new_group_set.add(member)\n                        group_set.add(member)\n                    else:\n                        non_group_set.add(member)\n\n            add_to_sets(members)\n\n            while new_group_set:\n                groups_to_walk = new_group_set\n                new_group_set = set()\n                for group in groups_to_walk:\n                    add_to_sets(group.children)\n\n            return group_set.union(non_group_set)\n\n        @classmethod\n        def resource_ancestors(cls, session, resource_type_names):\n            \"\"\"Resolve the transitive ancestors by type/name format.\"\"\"\n\n            resource_names = resource_type_names\n            resource_graph = collections.defaultdict(set)\n\n            res_childs = aliased(Resource, name='res_childs')\n            res_anc = aliased(Resource, name='resource_parent')\n\n            resources_set = set(resource_names)\n            resources_new = set(resource_names)\n\n            for resource in resources_new:\n                resource_graph[resource] = set()\n\n            while resources_new:\n                resources_new = set()\n                for parent, child in (\n                        session.query(res_anc, res_childs)\n                        .filter(res_childs.type_name.in_(resources_set))\n                        .filter(res_childs.parent_type_name ==\n                                res_anc.type_name)\n                        .all()):\n\n                    if parent.type_name not in resources_set:\n                        resources_new.add(parent.type_name)\n\n                    resources_set.add(parent.type_name)\n                    resources_set.add(child.type_name)\n\n                    resource_graph[parent.type_name].add(child.type_name)\n\n            return resource_graph\n\n        @classmethod\n        def find_resource_path(cls, session, resource_type_name):\n            \"\"\"Find resource ancestors by type/name format.\"\"\"\n\n            qry = (\n                session.query(Resource)\n                .filter(Resource.type_name == resource_type_name))\n\n            resources = qry.all()\n            return cls._find_resource_path(session, resources)\n\n        @classmethod\n        def _find_resource_path(cls, _, resources):\n            \"\"\"Find the list of transitive ancestors for the given resource.\"\"\"\n\n            if not resources:\n                return []\n\n            path = []\n            resource = resources[0]\n\n            path.append(resource)\n            while resource.parent:\n                resource = resource.parent\n                path.append(resource)\n\n            return path\n\n        @classmethod\n        def get_roles_by_permission_names(cls, session, permission_names):\n            \"\"\"Return the list of roles covering the specified permissions.\"\"\"\n\n            permission_set = set(permission_names)\n            qry = session.query(Permission)\n            if permission_set:\n                qry = qry.filter(Permission.name.in_(permission_set))\n            permissions = qry.all()\n\n            roles = set()\n            for permission in permissions:\n                for role in permission.roles:\n                    roles.add(role)\n\n            result_set = set()\n            for role in roles:\n                role_permissions = set(\n                    [p.name for p in role.permissions])\n                if permission_set.issubset(role_permissions):\n                    result_set.add(role)\n\n            return result_set\n\n        @classmethod\n        def get_member(cls, session, name):\n            \"\"\"Get member by name.\"\"\"\n\n            return session.query(Member).filter(Member.name == name).all()\n\n    base.metadata.create_all(dbengine)\n    return sessionmaker(bind=dbengine), ModelAccess\n\n\ndef undefine_model(session_maker, data_access):\n    \"\"\"Deletes an entire model and the corresponding data in the database.\"\"\"\n\n    session = session_maker()\n    data_access.delete_all(session)\n\nLOCK = Lock()\n\n\nclass ModelManager(object):\n    \"\"\"The Central class to create,list,get and delete models.\n\n        ModelManager is mostly used to do the lookup from model name to the\n        session cache which is given in each client's request.\n    \"\"\"\n\n    def __init__(self, dbengine):\n        self.engine = dbengine\n        self.modelmaker = self._create_model_session()\n        self.sessionmakers = {}\n\n    def _create_model_session(self):\n        \"\"\"Create a session to read from the models table.\"\"\"\n\n        MODEL_BASE.metadata.create_all(self.engine)\n        return db.ScopedSessionMaker(\n            sessionmaker(\n                bind=self.engine),\n            auto_commit=True)\n\n    @mutual_exclusive(LOCK)\n    def create(self, name):\n        \"\"\"Create a new model entry in the database.\"\"\"\n\n        LOGGER.info(\"Creating a new model entry in the database, \"\n                    \"name = %s\", name)\n        handle = generate_model_handle()\n        with self.modelmaker() as session:\n            model = Model(\n                handle=handle,\n                name=name,\n                state=\"CREATED\",\n                created_at=datetime.datetime.utcnow(),\n                watchdog_timer=datetime.datetime.utcnow(),\n                etag_seed=generate_model_seed(),\n                description=\"{}\"\n                )\n            session.add(model)\n            self.sessionmakers[model.handle] = define_model(\n                model.handle, self.engine, model.etag_seed)\n            return handle\n\n    def get(self, model):\n        \"\"\"Get model data by name.\"\"\"\n\n        session_maker, data_access = self._get(model)\n        return db.ScopedSession(session_maker()), data_access\n\n    def _get(self, handle):\n        \"\"\"Get model data by name internal.\"\"\"\n\n        if handle not in [m.handle for m in self.models()]:\n            error_message = 'handle={}, available={}'\\\n                .format(handle, [m.handle for m in self.models()])\n            LOGGER.error(error_message)\n            raise KeyError(error_message)\n        try:\n            return self.sessionmakers[handle]\n        except KeyError:\n            LOGGER.warn(\"sessionmakers doesn't contain handle,\"\n                        \" handle = %s\", handle)\n            with self.modelmaker() as session:\n                model = (session.query(Model)\n                         .filter(Model.handle == handle)\n                         .one())\n                self.sessionmakers[model.handle] = define_model(\n                    model.handle, self.engine, model.etag_seed)\n                return self.sessionmakers[model.handle]\n\n    @mutual_exclusive(LOCK)\n    def delete(self, model_name):\n        \"\"\"Delete a model entry in the database by name.\"\"\"\n\n        LOGGER.info(\"Deleting model by name, model_name = %s\", model_name)\n        _, data_access = self._get(model_name)\n        if model_name in self.sessionmakers:\n            del self.sessionmakers[model_name]\n        with self.modelmaker() as session:\n            session.query(Model).filter(Model.handle == model_name).delete()\n        data_access.delete_all(self.engine)\n\n    def _models(self, expunge=False):\n        \"\"\"Return the list of models from the database.\"\"\"\n\n        with self.modelmaker() as session:\n            items = session.query(Model).all()\n            if expunge:\n                session.expunge_all()\n            return items\n\n    def models(self):\n        \"\"\"Expunging wrapper for _models.\"\"\"\n        return self._models(expunge=True)\n\n    def model(self, model_name, expunge=True, session=None):\n        \"\"\"Get model from database by name.\"\"\"\n\n        def instantiate_model(session, model_name, expunge):\n            \"\"\"Creates a model object by querying the database.\n\n            Args:\n                session (object): Database session.\n                model_name (str): Model name to instantiate.\n                expunge (bool): Whether or not to detach the object from\n                                the session for use in another session.\n            \"\"\"\n\n            item = session.query(Model).filter(\n                Model.handle == model_name).one()\n            if expunge:\n                session.expunge(item)\n            return item\n\n        if not session:\n            with self.modelmaker() as scoped_session:\n                return instantiate_model(scoped_session, model_name, expunge)\n        else:\n            return instantiate_model(session, model_name, expunge)\n\n    def get_model(self, model, expunge=True, session=None):\n        \"\"\"Get model from database by name or handle.\"\"\"\n\n        def query_model(session, model, expunge):\n            \"\"\"Get a model object by querying the database.\n\n            Args:\n                session (object): Database session.\n                model (str): Model name or handle.\n                expunge (bool): Whether or not to detach the object from\n                                the session for use in another session.\n            \"\"\"\n\n            item = session.query(Model).filter(or_(\n                Model.handle == model,\n                Model.name == model)).first()\n            if expunge and item:\n                session.expunge(item)\n            return item\n\n        if not session:\n            with self.modelmaker() as scoped_session:\n                return query_model(scoped_session, model, expunge)\n        else:\n            return query_model(session, model, expunge)\n\n    def add_description(self, model_name, new_description, session=None):\n        \"\"\"Add description to a model.\n\n        Args:\n            model_name (str): Model name\n            new_description(str): The description in json format.\n            session (object): Database session.\n        \"\"\"\n\n        if not session:\n            with self.modelmaker() as scoped_session:\n                model = scoped_session.query(Model).filter(\n                    Model.handle == model_name).one()\n        else:\n            model = session.query(Model).filter(\n                Model.handle == model_name).one()\n        model.add_description(new_description)\n\ndef create_engine(*args, **kwargs):\n    \"\"\"Create engine wrapper to patch database options.\n\n    Args:\n        *args (list): Arguments.\n        **kwargs (dict): Arguments.\n\n    Returns:\n        object: Engine.\n    \"\"\"\n\n    sqlite_enforce_fks = 'sqlite_enforce_fks'\n    forward_kwargs = {k: v for k, v in kwargs.iteritems()}\n    if sqlite_enforce_fks in forward_kwargs:\n        del forward_kwargs[sqlite_enforce_fks]\n\n    engine = sqlalchemy_create_engine(*args, **forward_kwargs)\n    dialect = engine.dialect.name\n    if dialect == 'sqlite':\n        @event.listens_for(engine, \"connect\")\n        def do_connect(dbapi_connection, _):\n            \"\"\"Hooking database connect.\n\n            Args:\n                dbapi_connection (object): Database connection.\n                _ (object): Unknown.\n            \"\"\"\n            # Fix for nested transaction problems\n            dbapi_connection.isolation_level = None\n            if kwargs.get(sqlite_enforce_fks, False):\n                # Enable foreign key constraints\n                dbapi_connection.execute('pragma foreign_keys=ON')\n\n        @event.listens_for(engine, \"begin\")\n        def do_begin(conn):\n            \"\"\"Hooking database transaction begin.\n\n            Args:\n                conn (object): Database connection.\n            \"\"\"\n            # Fix for nested transaction problems\n            conn.execute(\"BEGIN\")\n\n        engine.__explain_hooks = [do_connect, do_begin] # pylint: disable=protected-access\n\n    return engine\n\n\ndef session_creator(model_name, filename=None, seed=None, echo=False):\n    \"\"\"Create a session maker for the model and db file.\"\"\"\n    LOGGER.info(\"Creating session maker, model_name = %s, filename = %s\",\n                model_name, filename)\n    if filename:\n        engine = create_engine('sqlite:///{}'.format(filename),\n                               pool_recycle=POOL_RECYCLE_SECONDS)\n    else:\n        engine = create_engine('sqlite:///:memory:',\n                               pool_recycle=POOL_RECYCLE_SECONDS, echo=echo)\n    if seed is None:\n        seed = generate_model_seed()\n    session_maker, data_access = define_model(model_name, engine, seed)\n    return session_maker, data_access\n", "idx": 27, "id": 28589, "msg": "", "proj": "forseti-security-forseti-security", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,13 @@\n+class Unsubscriber\n+  def initialize(subscription)\n+    @subscription = subscription\n+  end\n+\n+  def process\n+    Subscription.transaction do\n+      @subscription.destroy\n+      stripe_user = Stripe::Customer.retrieve(@subscription.stripe_customer)\n+      stripe_user.cancel_subscription\n+    end\n+  end\n+end", "y": 1, "oldf": "", "idx": 1, "id": 7016, "msg": "I think the code needs to raise an error to have the `transaction` block roll back. Does `stripe_user.cancel_subscription` raise an error if it fails?", "proj": "thoughtbot-upcase", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -88,26 +88,47 @@ func newFolderBlockManager(config Config, fb FolderBranch,\n \ttlfStringFull := fb.Tlf.String()\n \tlog := config.MakeLogger(fmt.Sprintf(\"FBM %s\", tlfStringFull[:8]))\n \tfbm := &folderBlockManager{\n-\t\tconfig:                   config,\n-\t\tlog:                      log,\n-\t\tshutdownChan:             make(chan struct{}),\n-\t\tid:                       fb.Tlf,\n-\t\tarchiveChan:              make(chan *RootMetadata, 25),\n-\t\tarchivePauseChan:         make(chan (<-chan struct{})),\n-\t\tblocksToDeleteAfterError: make(map[*RootMetadata][]BlockPointer),\n-\t\tforceReclamationChan:     make(chan struct{}, 1),\n-\t\thelper:                   helper,\n+\t\tconfig:                  config,\n+\t\tlog:                     log,\n+\t\tshutdownChan:            make(chan struct{}),\n+\t\tid:                      fb.Tlf,\n+\t\tarchiveChan:             make(chan *RootMetadata, 25),\n+\t\tarchivePauseChan:        make(chan (<-chan struct{})),\n+\t\tblocksToDeleteChan:      make(chan blocksToDelete, 25),\n+\t\tblocksToDeletePauseChan: make(chan (<-chan struct{})),\n+\t\tforceReclamationChan:    make(chan struct{}, 1),\n+\t\thelper:                  helper,\n \t}\n \t// Pass in the BlockOps here so that the archive goroutine\n \t// doesn't do possibly-racy-in-tests access to\n \t// fbm.config.BlockOps().\n \tgo fbm.archiveBlocksInBackground()\n+\tgo fbm.deleteBlocksInBackground()\n \tif fb.Branch == MasterBranch {\n \t\tgo fbm.reclaimQuotaInBackground()\n \t}\n \treturn fbm\n }\n \n+func (fbm *folderBlockManager) setBlocksToDeleteCancel(cancel context.CancelFunc) {\n+\tfbm.blocksToDeleteCancelLock.Lock()\n+\tdefer fbm.blocksToDeleteCancelLock.Unlock()\n+\tfbm.blocksToDeleteCancel = cancel\n+}\n+\n+func (fbm *folderBlockManager) cancelBlocksToDelete() {\n+\tblocksToDeleteCancel := func() context.CancelFunc {\n+\t\tfbm.blocksToDeleteCancelLock.Lock()\n+\t\tdefer fbm.blocksToDeleteCancelLock.Unlock()\n+\t\tblocksToDeleteCancel := fbm.blocksToDeleteCancel\n+\t\tfbm.blocksToDeleteCancel = nil\n+\t\treturn blocksToDeleteCancel\n+\t}()\n+\tif blocksToDeleteCancel != nil {\n+\t\tblocksToDeleteCancel()\n+\t}\n+}\n+\n func (fbm *folderBlockManager) setArchiveCancel(cancel context.CancelFunc) {\n \tfbm.archiveCancelLock.Lock()\n \tdefer fbm.archiveCancelLock.Unlock()", "y": 0, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/logger\"\n\t\"golang.org/x/net/context\"\n)\n\ntype fbmHelper interface {\n\tgetMDForFBM(ctx context.Context) (*RootMetadata, error)\n\tfinalizeGCOp(ctx context.Context, gco *gcOp) error\n}\n\nconst (\n\t// How many pointers to downgrade in a single Archive/Delete call.\n\tnumPointersToDowngradePerChunk = 20\n\t// Once the number of pointers being deleted in a single gc op\n\t// passes this threshold, we'll stop garbage collection at the\n\t// current revision.\n\tnumPointersPerGCThreshold = 100\n\t// The most revisions to consider for each QR run.\n\tnumMaxRevisionsPerQR = 100\n)\n\n// folderBlockManager is a helper class for managing the blocks in a\n// particular TLF.  It archives historical blocks and reclaims quota\n// usage, all in the background.\ntype folderBlockManager struct {\n\tconfig       Config\n\tlog          logger.Logger\n\tshutdownChan chan struct{}\n\tid           TlfID\n\n\t// A queue of MD updates for this folder that need to have their\n\t// unref's blocks archived\n\tarchiveChan chan *RootMetadata\n\n\tarchivePauseChan chan (<-chan struct{})\n\n\t// archiveGroup tracks the outstanding archives.\n\tarchiveGroup RepeatedWaitGroup\n\n\tarchiveCancelLock sync.Mutex\n\tarchiveCancel     context.CancelFunc\n\n\t// blocksToDeleteAfterError is a list of blocks, for a given\n\t// metadata revision, that may have been Put as part of a failed\n\t// MD write.  These blocks should be deleted as soon as we know\n\t// for sure that the MD write isn't visible to others.\n\t// The lock should only be held immediately around accessing the\n\t// list.  TODO: Persist these to disk?\n\tblocksToDeleteLock       sync.Mutex\n\tblocksToDeleteAfterError map[*RootMetadata][]BlockPointer\n\n\t// forceReclamation forces the manager to start a reclamation\n\t// process.\n\tforceReclamationChan chan struct{}\n\n\t// reclamationGroup tracks the outstanding quota reclamations.\n\treclamationGroup RepeatedWaitGroup\n\n\treclamationCancelLock sync.Mutex\n\treclamationCancel     context.CancelFunc\n\n\thelper fbmHelper\n\n\t// Keep track of the last reclamation time, for testing.\n\tlastReclamationTimeLock sync.Mutex\n\tlastReclamationTime     time.Time\n\n\t// Remembers what happened last time during quota reclamation;\n\t// should only be accessed by the QR goroutine.\n\tlastQRHeadRev      MetadataRevision\n\tlastQROldEnoughRev MetadataRevision\n\twasLastQRComplete  bool\n}\n\nfunc newFolderBlockManager(config Config, fb FolderBranch,\n\thelper fbmHelper) *folderBlockManager {\n\ttlfStringFull := fb.Tlf.String()\n\tlog := config.MakeLogger(fmt.Sprintf(\"FBM %s\", tlfStringFull[:8]))\n\tfbm := &folderBlockManager{\n\t\tconfig:                   config,\n\t\tlog:                      log,\n\t\tshutdownChan:             make(chan struct{}),\n\t\tid:                       fb.Tlf,\n\t\tarchiveChan:              make(chan *RootMetadata, 25),\n\t\tarchivePauseChan:         make(chan (<-chan struct{})),\n\t\tblocksToDeleteAfterError: make(map[*RootMetadata][]BlockPointer),\n\t\tforceReclamationChan:     make(chan struct{}, 1),\n\t\thelper:                   helper,\n\t}\n\t// Pass in the BlockOps here so that the archive goroutine\n\t// doesn't do possibly-racy-in-tests access to\n\t// fbm.config.BlockOps().\n\tgo fbm.archiveBlocksInBackground()\n\tif fb.Branch == MasterBranch {\n\t\tgo fbm.reclaimQuotaInBackground()\n\t}\n\treturn fbm\n}\n\nfunc (fbm *folderBlockManager) setArchiveCancel(cancel context.CancelFunc) {\n\tfbm.archiveCancelLock.Lock()\n\tdefer fbm.archiveCancelLock.Unlock()\n\tfbm.archiveCancel = cancel\n}\n\nfunc (fbm *folderBlockManager) cancelArchive() {\n\tarchiveCancel := func() context.CancelFunc {\n\t\tfbm.archiveCancelLock.Lock()\n\t\tdefer fbm.archiveCancelLock.Unlock()\n\t\tarchiveCancel := fbm.archiveCancel\n\t\tfbm.archiveCancel = nil\n\t\treturn archiveCancel\n\t}()\n\tif archiveCancel != nil {\n\t\tarchiveCancel()\n\t}\n}\n\nfunc (fbm *folderBlockManager) setReclamationCancel(cancel context.CancelFunc) {\n\tfbm.reclamationCancelLock.Lock()\n\tdefer fbm.reclamationCancelLock.Unlock()\n\tfbm.reclamationCancel = cancel\n}\n\nfunc (fbm *folderBlockManager) cancelReclamation() {\n\treclamationCancel := func() context.CancelFunc {\n\t\tfbm.reclamationCancelLock.Lock()\n\t\tdefer fbm.reclamationCancelLock.Unlock()\n\t\treclamationCancel := fbm.reclamationCancel\n\t\tfbm.reclamationCancel = nil\n\t\treturn reclamationCancel\n\t}()\n\tif reclamationCancel != nil {\n\t\treclamationCancel()\n\t}\n}\n\nfunc (fbm *folderBlockManager) shutdown() {\n\tclose(fbm.shutdownChan)\n\tfbm.cancelArchive()\n\tfbm.cancelReclamation()\n}\n\n// cleanUpBlockState cleans up any blocks that may have been orphaned\n// by a failure during or after blocks have been sent to the\n// server. This is usually used in a defer right before a call to\n// fbo.doBlockPuts like so:\n//\n//  defer func() {\n//    if err != nil {\n//      ...cleanUpBlockState(md, bps)\n//    }\n//  }()\n//\n//  ... = ...doBlockPuts(ctx, md, *bps)\nfunc (fbm *folderBlockManager) cleanUpBlockState(\n\tmd *RootMetadata, bps *blockPutState) {\n\tfbm.blocksToDeleteLock.Lock()\n\tdefer fbm.blocksToDeleteLock.Unlock()\n\tfbm.log.CDebugf(nil, \"Clean up md %d %s\", md.Revision, md.MergedStatus())\n\tfor _, bs := range bps.blockStates {\n\t\tfbm.blocksToDeleteAfterError[md] =\n\t\t\tappend(fbm.blocksToDeleteAfterError[md], bs.blockPtr)\n\t}\n}\n\nfunc (fbm *folderBlockManager) archiveUnrefBlocks(md *RootMetadata) {\n\t// Don't archive for unmerged revisions, because conflict\n\t// resolution might undo some of the unreferences.\n\tif md.MergedStatus() != Merged {\n\t\treturn\n\t}\n\n\tfbm.archiveGroup.Add(1)\n\tfbm.archiveChan <- md\n}\n\n// archiveUnrefBlocksNoWait enqueues the MD for archiving without\n// blocking.  By the time it returns, the archive group has been\n// incremented so future waits will block on this archive.  This\n// method is for internal use within folderBlockManager only.\nfunc (fbm *folderBlockManager) archiveUnrefBlocksNoWait(md *RootMetadata) {\n\t// Don't archive for unmerged revisions, because conflict\n\t// resolution might undo some of the unreferences.\n\tif md.MergedStatus() != Merged {\n\t\treturn\n\t}\n\n\tfbm.archiveGroup.Add(1)\n\n\t// Don't block if the channel is full; instead do the send in a\n\t// background goroutine.  We've already done the Add above, so the\n\t// wait calls should all work just fine.\n\tselect {\n\tcase fbm.archiveChan <- md:\n\t\treturn\n\tdefault:\n\t\tgo func() { fbm.archiveChan <- md }()\n\t}\n}\n\nfunc (fbm *folderBlockManager) waitForArchives(ctx context.Context) error {\n\treturn fbm.archiveGroup.Wait(ctx)\n}\n\nfunc (fbm *folderBlockManager) waitForQuotaReclamations(\n\tctx context.Context) error {\n\treturn fbm.reclamationGroup.Wait(ctx)\n}\n\nfunc (fbm *folderBlockManager) forceQuotaReclamation() {\n\tfbm.reclamationGroup.Add(1)\n\tselect {\n\tcase fbm.forceReclamationChan <- struct{}{}:\n\tdefault:\n\t\tfbm.reclamationGroup.Done()\n\t}\n}\n\n// doChunkedDowngrades sends batched archive or delete messages to the\n// block server for the given block pointers.  For deletes, it returns\n// a list of block IDs that no longer have any references.\nfunc (fbm *folderBlockManager) doChunkedDowngrades(ctx context.Context,\n\tmd *RootMetadata, ptrs []BlockPointer, archive bool) (\n\t[]BlockID, error) {\n\tfbm.log.CDebugf(ctx, \"Downgrading %d pointers (archive=%t)\",\n\t\tlen(ptrs), archive)\n\tbops := fbm.config.BlockOps()\n\n\t// Round up to find the number of chunks.\n\tnumChunks := (len(ptrs) + numPointersToDowngradePerChunk - 1) /\n\t\tnumPointersToDowngradePerChunk\n\tnumWorkers := numChunks\n\tif numWorkers > maxParallelBlockPuts {\n\t\tnumWorkers = maxParallelBlockPuts\n\t}\n\tchunks := make(chan []BlockPointer, numChunks)\n\n\tvar wg sync.WaitGroup\n\tdefer wg.Wait()\n\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\ttype workerResult struct {\n\t\tzeroRefCounts []BlockID\n\t\terr           error\n\t}\n\n\tchunkResults := make(chan workerResult, numChunks)\n\tworker := func() {\n\t\tdefer wg.Done()\n\t\tfor chunk := range chunks {\n\t\t\tvar res workerResult\n\t\t\tfbm.log.CDebugf(ctx, \"Downgrading chunk of %d pointers\", len(chunk))\n\t\t\tif archive {\n\t\t\t\tres.err = bops.Archive(ctx, md, chunk)\n\t\t\t} else {\n\t\t\t\tvar liveCounts map[BlockID]int\n\t\t\t\tliveCounts, res.err = bops.Delete(ctx, md, chunk)\n\t\t\t\tif res.err == nil {\n\t\t\t\t\tfor id, count := range liveCounts {\n\t\t\t\t\t\tif count == 0 {\n\t\t\t\t\t\t\tres.zeroRefCounts = append(res.zeroRefCounts, id)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tchunkResults <- res\n\t\t\tselect {\n\t\t\t// return early if the context has been canceled\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t}\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo worker()\n\t}\n\n\tfor start := 0; start < len(ptrs); start += numPointersToDowngradePerChunk {\n\t\tend := start + numPointersToDowngradePerChunk\n\t\tif end > len(ptrs) {\n\t\t\tend = len(ptrs)\n\t\t}\n\t\tchunks <- ptrs[start:end]\n\t}\n\tclose(chunks)\n\n\tvar zeroRefCounts []BlockID\n\tfor i := 0; i < numChunks; i++ {\n\t\tresult := <-chunkResults\n\t\tif result.err != nil {\n\t\t\t// deferred cancel will stop the other workers.\n\t\t\treturn nil, result.err\n\t\t}\n\t\tzeroRefCounts = append(zeroRefCounts, result.zeroRefCounts...)\n\t}\n\treturn zeroRefCounts, nil\n}\n\n// deleteBlockRefs sends batched delete messages to the block server\n// for the given block pointers.  It returns a list of block IDs that\n// no longer have any references.\nfunc (fbm *folderBlockManager) deleteBlockRefs(ctx context.Context,\n\tmd *RootMetadata, ptrs []BlockPointer) ([]BlockID, error) {\n\treturn fbm.doChunkedDowngrades(ctx, md, ptrs, false)\n}\n\nfunc (fbm *folderBlockManager) processBlocksToDelete(ctx context.Context) error {\n\t// also attempt to delete any error references\n\tvar toDelete map[*RootMetadata][]BlockPointer\n\tfunc() {\n\t\tfbm.blocksToDeleteLock.Lock()\n\t\tdefer fbm.blocksToDeleteLock.Unlock()\n\t\ttoDelete = fbm.blocksToDeleteAfterError\n\t\tfbm.blocksToDeleteAfterError =\n\t\t\tmake(map[*RootMetadata][]BlockPointer)\n\t}()\n\n\tif len(toDelete) == 0 {\n\t\treturn nil\n\t}\n\n\ttoDeleteAgain := make(map[*RootMetadata][]BlockPointer)\n\tfor md, ptrs := range toDelete {\n\t\tfbm.log.CDebugf(ctx, \"Checking deleted blocks for revision %d\",\n\t\t\tmd.Revision)\n\t\t// Make sure that the MD didn't actually become\n\t\t// part of the folder history.  (This could happen\n\t\t// if the Sync was canceled while the MD put was\n\t\t// outstanding.)\n\t\trmds, err := getMDRange(ctx, fbm.config, fbm.id, md.BID,\n\t\t\tmd.Revision, md.Revision, md.MergedStatus())\n\t\tif err != nil || len(rmds) == 0 {\n\t\t\ttoDeleteAgain[md] = ptrs\n\t\t\tcontinue\n\t\t}\n\t\tdirsEqual, err := CodecEqual(fbm.config.Codec(),\n\t\t\trmds[0].data.Dir, md.data.Dir)\n\t\tif err != nil {\n\t\t\tfbm.log.CErrorf(ctx, \"Error when comparing dirs: %v\", err)\n\t\t} else if dirsEqual {\n\t\t\t// This md is part of the history of the folder,\n\t\t\t// so we shouldn't delete the blocks.\n\t\t\tfbm.log.CDebugf(ctx, \"Not deleting blocks from revision %d\",\n\t\t\t\tmd.Revision)\n\t\t\t// But, since this MD put seems to have succeeded, we\n\t\t\t// should archive it.\n\t\t\tfbm.log.CDebugf(ctx, \"Archiving successful MD revision %d\",\n\t\t\t\trmds[0].Revision)\n\t\t\t// Don't block on archiving the MD, because that could\n\t\t\t// lead to deadlock.\n\t\t\tfbm.archiveUnrefBlocksNoWait(rmds[0])\n\t\t\tcontinue\n\t\t}\n\n\t\t// Otherwise something else has been written over\n\t\t// this MD, so get rid of the blocks.\n\t\tfbm.log.CDebugf(ctx, \"Cleaning up blocks for failed revision %d\",\n\t\t\tmd.Revision)\n\n\t\t_, err = fbm.deleteBlockRefs(ctx, md, ptrs)\n\t\t// Ignore permanent errors\n\t\t_, isPermErr := err.(BServerError)\n\t\t_, isNonceNonExistentErr := err.(BServerErrorNonceNonExistent)\n\t\tif err != nil {\n\t\t\tfbm.log.CWarningf(ctx, \"Couldn't delete some ref in batch %v: %v\", ptrs, err)\n\t\t\tif !isPermErr && !isNonceNonExistentErr {\n\t\t\t\ttoDeleteAgain[md] = ptrs\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(toDeleteAgain) > 0 {\n\t\tfunc() {\n\t\t\tfbm.blocksToDeleteLock.Lock()\n\t\t\tdefer fbm.blocksToDeleteLock.Unlock()\n\t\t\tfor md, ptrs := range toDeleteAgain {\n\t\t\t\tfbm.blocksToDeleteAfterError[md] =\n\t\t\t\t\tappend(fbm.blocksToDeleteAfterError[md], ptrs...)\n\t\t\t}\n\t\t}()\n\t}\n\n\treturn nil\n}\n\n// CtxFBMTagKey is the type used for unique context tags within\n// folderBlockManager\ntype CtxFBMTagKey int\n\nconst (\n\t// CtxFBMIDKey is the type of the tag for unique operation IDs\n\t// within folderBlockManager.\n\tCtxFBMIDKey CtxFBMTagKey = iota\n)\n\n// CtxFBMOpID is the display name for the unique operation\n// folderBlockManager ID tag.\nconst CtxFBMOpID = \"FBMID\"\n\nfunc (fbm *folderBlockManager) ctxWithFBMID(\n\tctx context.Context) context.Context {\n\treturn ctxWithRandomID(ctx, CtxFBMIDKey, CtxFBMOpID, fbm.log)\n}\n\n// Run the passed function with a context that's canceled on shutdown.\nfunc (fbm *folderBlockManager) runUnlessShutdown(\n\tfn func(ctx context.Context) error) error {\n\tctx := fbm.ctxWithFBMID(context.Background())\n\tctx, cancelFunc := context.WithCancel(ctx)\n\tdefer cancelFunc()\n\terrChan := make(chan error, 1)\n\tgo func() {\n\t\terrChan <- fn(ctx)\n\t}()\n\n\tselect {\n\tcase err := <-errChan:\n\t\treturn err\n\tcase <-fbm.shutdownChan:\n\t\treturn errors.New(\"shutdown received\")\n\t}\n}\n\nfunc (fbm *folderBlockManager) archiveBlockRefs(ctx context.Context,\n\tmd *RootMetadata, ptrs []BlockPointer) error {\n\t_, err := fbm.doChunkedDowngrades(ctx, md, ptrs, true)\n\treturn err\n}\n\nfunc (fbm *folderBlockManager) archiveBlocksInBackground() {\n\tfor {\n\t\tselect {\n\t\tcase md := <-fbm.archiveChan:\n\t\t\tvar ptrs []BlockPointer\n\t\t\tfor _, op := range md.data.Changes.Ops {\n\t\t\t\tptrs = append(ptrs, op.Unrefs()...)\n\t\t\t\tfor _, update := range op.AllUpdates() {\n\t\t\t\t\t// It's legal for there to be an \"update\" between\n\t\t\t\t\t// two identical pointers (usually because of\n\t\t\t\t\t// conflict resolution), so ignore that for\n\t\t\t\t\t// archival purposes.\n\t\t\t\t\tif update.Ref != update.Unref {\n\t\t\t\t\t\tptrs = append(ptrs, update.Unref)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tfbm.runUnlessShutdown(func(ctx context.Context) (err error) {\n\t\t\t\tdefer fbm.archiveGroup.Done()\n\t\t\t\t// This func doesn't take any locks, though it can\n\t\t\t\t// block md writes due to the buffered channel.  So\n\t\t\t\t// use the long timeout to make sure things get\n\t\t\t\t// unblocked eventually, but no need for a short timeout.\n\t\t\t\tctx, cancel := context.WithTimeout(ctx, backgroundTaskTimeout)\n\t\t\t\tfbm.setArchiveCancel(cancel)\n\t\t\t\tdefer fbm.cancelArchive()\n\n\t\t\t\tfbm.log.CDebugf(ctx, \"Archiving %d block pointers as a result \"+\n\t\t\t\t\t\"of revision %d\", len(ptrs), md.Revision)\n\t\t\t\terr = fbm.archiveBlockRefs(ctx, md, ptrs)\n\t\t\t\tif err != nil {\n\t\t\t\t\tfbm.log.CWarningf(ctx, \"Couldn't archive blocks: %v\", err)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Also see if we can delete any blocks.\n\t\t\t\tif err := fbm.processBlocksToDelete(ctx); err != nil {\n\t\t\t\t\tfbm.log.CDebugf(ctx, \"Error deleting blocks: %v\", err)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\t\tcase unpause := <-fbm.archivePauseChan:\n\t\t\tfbm.runUnlessShutdown(func(ctx context.Context) (err error) {\n\t\t\t\tfbm.log.CInfof(ctx, \"Archives paused\")\n\t\t\t\t// wait to be unpaused\n\t\t\t\tselect {\n\t\t\t\tcase <-unpause:\n\t\t\t\t\tfbm.log.CInfof(ctx, \"Archives unpaused\")\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\tcase <-fbm.shutdownChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (fbm *folderBlockManager) isOldEnough(rmd *RootMetadata) bool {\n\t// Trust the client-provided timestamp -- it's\n\t// possible that a writer with a bad clock could cause\n\t// another writer to clear out quotas early.  That's\n\t// ok, there's nothing we can really do about that.\n\t//\n\t// TODO: rmd.data.Dir.Mtime does not necessarily reflect when the\n\t// MD was made, since it only gets updated if the root directory\n\t// mtime needs to be updated.  As a result, some updates may be\n\t// cleaned up earlier than desired.  We need to find a more stable\n\t// way to record MD update time (KBFS-821).\n\tmtime := time.Unix(0, rmd.data.Dir.Mtime)\n\tunrefAge := fbm.config.QuotaReclamationMinUnrefAge()\n\treturn mtime.Add(unrefAge).Before(fbm.config.Clock().Now())\n}\n\n// getMostRecentOldEnoughAndGCRevisions returns the most recent MD\n// that's older than the unref age, as well as the latest revision\n// that was scrubbed by the previous gc op.\nfunc (fbm *folderBlockManager) getMostRecentOldEnoughAndGCRevisions(\n\tctx context.Context, head *RootMetadata) (\n\tmostRecentOldEnoughRev, lastGCRev MetadataRevision, err error) {\n\t// Walk backwards until we find one that is old enough.  Also,\n\t// look out for the previous gcOp.\n\tcurrHead := head.Revision\n\tmostRecentOldEnoughRev = MetadataRevisionUninitialized\n\tlastGCRev = MetadataRevisionUninitialized\n\tfor {\n\t\tstartRev := currHead - maxMDsAtATime + 1 // (MetadataRevision is signed)\n\t\tif startRev < MetadataRevisionInitial {\n\t\t\tstartRev = MetadataRevisionInitial\n\t\t}\n\n\t\trmds, err := getMDRange(ctx, fbm.config, fbm.id, NullBranchID, startRev,\n\t\t\tcurrHead, Merged)\n\t\tif err != nil {\n\t\t\treturn MetadataRevisionUninitialized,\n\t\t\t\tMetadataRevisionUninitialized, err\n\t\t}\n\n\t\tnumNew := len(rmds)\n\t\tfor i := len(rmds) - 1; i >= 0; i-- {\n\t\t\trmd := rmds[i]\n\t\t\tif mostRecentOldEnoughRev == MetadataRevisionUninitialized &&\n\t\t\t\tfbm.isOldEnough(rmd) {\n\t\t\t\tfbm.log.CDebugf(ctx, \"Revision %d is older than the unref \"+\n\t\t\t\t\t\"age %s\", rmd.Revision,\n\t\t\t\t\tfbm.config.QuotaReclamationMinUnrefAge())\n\t\t\t\tmostRecentOldEnoughRev = rmd.Revision\n\t\t\t}\n\n\t\t\tif lastGCRev == MetadataRevisionUninitialized {\n\t\t\t\tfor j := len(rmd.data.Changes.Ops) - 1; j >= 0; j-- {\n\t\t\t\t\tgcOp, ok := rmd.data.Changes.Ops[j].(*gcOp)\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tfbm.log.CDebugf(ctx, \"Found last gc op: %s\", gcOp)\n\t\t\t\t\tlastGCRev = gcOp.LatestRev\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Once both return values are set, we are done\n\t\t\tif mostRecentOldEnoughRev != MetadataRevisionUninitialized &&\n\t\t\t\tlastGCRev != MetadataRevisionUninitialized {\n\t\t\t\treturn mostRecentOldEnoughRev, lastGCRev, nil\n\t\t\t}\n\t\t}\n\n\t\tif numNew > 0 {\n\t\t\tcurrHead = rmds[0].Revision - 1\n\t\t}\n\n\t\tif numNew < maxMDsAtATime || currHead < MetadataRevisionInitial {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn mostRecentOldEnoughRev, lastGCRev, nil\n}\n\n// getUnrefBlocks returns a slice containing all the block pointers\n// that were unreferenced after the earliestRev, up to and including\n// those in latestRev.  If the number of pointers is too large, it\n// will shorten the range of the revisions being reclaimed, and return\n// the latest revision represented in the returned slice of pointers.\nfunc (fbm *folderBlockManager) getUnreferencedBlocks(\n\tctx context.Context, latestRev, earliestRev MetadataRevision) (\n\tptrs []BlockPointer, lastRevConsidered MetadataRevision,\n\tcomplete bool, err error) {\n\tfbm.log.CDebugf(ctx, \"Getting unreferenced blocks between revisions \"+\n\t\t\"%d and %d\", earliestRev, latestRev)\n\tdefer func() {\n\t\tif err == nil {\n\t\t\tfbm.log.CDebugf(ctx, \"Found %d pointers to clean between \"+\n\t\t\t\t\"revisions %d and %d\", len(ptrs), earliestRev, latestRev)\n\t\t}\n\t}()\n\n\tif latestRev <= earliestRev {\n\t\t// Nothing to do.\n\t\tfbm.log.CDebugf(ctx, \"Latest rev %d is included in the previous \"+\n\t\t\t\"gc op (%d)\", latestRev, earliestRev)\n\t\treturn nil, MetadataRevisionUninitialized, true, nil\n\t}\n\n\t// Walk backward, starting from latestRev, until just after\n\t// earliestRev, gathering block pointers.\n\tcurrHead := latestRev\n\trevStartPositions := make(map[MetadataRevision]int)\nouter:\n\tfor {\n\t\tstartRev := currHead - maxMDsAtATime + 1 // (MetadataRevision is signed)\n\t\tif startRev < MetadataRevisionInitial {\n\t\t\tstartRev = MetadataRevisionInitial\n\t\t}\n\n\t\trmds, err := getMDRange(ctx, fbm.config, fbm.id, NullBranchID, startRev,\n\t\t\tcurrHead, Merged)\n\t\tif err != nil {\n\t\t\treturn nil, MetadataRevisionUninitialized, false, err\n\t\t}\n\n\t\tnumNew := len(rmds)\n\t\tfor i := len(rmds) - 1; i >= 0; i-- {\n\t\t\trmd := rmds[i]\n\t\t\tif rmd.Revision <= earliestRev {\n\t\t\t\tbreak outer\n\t\t\t}\n\t\t\t// Save the latest revision starting at this position:\n\t\t\trevStartPositions[rmd.Revision] = len(ptrs)\n\t\t\tfor _, op := range rmd.data.Changes.Ops {\n\t\t\t\tif _, ok := op.(*gcOp); ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tptrs = append(ptrs, op.Unrefs()...)\n\t\t\t\tfor _, update := range op.AllUpdates() {\n\t\t\t\t\t// It's legal for there to be an \"update\" between\n\t\t\t\t\t// two identical pointers (usually because of\n\t\t\t\t\t// conflict resolution), so ignore that for quota\n\t\t\t\t\t// reclamation purposes.\n\t\t\t\t\tif update.Ref != update.Unref {\n\t\t\t\t\t\tptrs = append(ptrs, update.Unref)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// TODO: when can we clean up the MD's unembedded block\n\t\t\t// changes pointer?  It's not safe until we know for sure\n\t\t\t// that all existing clients have received the latest\n\t\t\t// update (and also that there are no outstanding staged\n\t\t\t// branches).  Let's do that as part of the bigger issue\n\t\t\t// KBFS-793 -- for now we have to leak those blocks.\n\t\t}\n\n\t\tif numNew > 0 {\n\t\t\tcurrHead = rmds[0].Revision - 1\n\t\t}\n\n\t\tif numNew < maxMDsAtATime || currHead < MetadataRevisionInitial {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tcomplete = true\n\tif len(ptrs) > numPointersPerGCThreshold {\n\t\t// Find the earliest revision to clean up that lets us send at\n\t\t// least numPointersPerGCThreshold pointers.  The earliest\n\t\t// pointers are at the end of the list, so subtract the\n\t\t// threshold from the back.\n\t\tthreshStart := len(ptrs) - numPointersPerGCThreshold\n\t\torigLatestRev := latestRev\n\t\torigPtrsLen := len(ptrs)\n\t\t// TODO: optimize by keeping rev->pos mappings in sorted order.\n\t\tfor rev, i := range revStartPositions {\n\t\t\tif i < threshStart && rev < latestRev {\n\t\t\t\tlatestRev = rev\n\t\t\t}\n\t\t}\n\t\tif latestRev < origLatestRev {\n\t\t\tptrs = ptrs[revStartPositions[latestRev]:]\n\t\t\tfbm.log.CDebugf(ctx, \"Shortening GC range from [%d:%d] to [%d:%d],\"+\n\t\t\t\t\" reducing pointers from %d to %d\", earliestRev, origLatestRev,\n\t\t\t\tearliestRev, latestRev, origPtrsLen, len(ptrs))\n\t\t\tcomplete = false\n\t\t}\n\t}\n\n\treturn ptrs, latestRev, complete, nil\n}\n\nfunc (fbm *folderBlockManager) finalizeReclamation(ctx context.Context,\n\tptrs []BlockPointer, zeroRefCounts []BlockID,\n\tlatestRev MetadataRevision) error {\n\tgco := newGCOp(latestRev)\n\tfor _, id := range zeroRefCounts {\n\t\tgco.AddUnrefBlock(BlockPointer{ID: id})\n\t}\n\tfbm.log.CDebugf(ctx, \"Finalizing reclamation %s with %d ptrs\", gco,\n\t\tlen(ptrs))\n\t// finalizeGCOp could wait indefinitely on locks, so run it in a\n\t// goroutine.\n\treturn runUnlessCanceled(ctx,\n\t\tfunc() error { return fbm.helper.finalizeGCOp(ctx, gco) })\n}\n\nfunc (fbm *folderBlockManager) isQRNecessary(head *RootMetadata) bool {\n\tif head == nil {\n\t\treturn false\n\t}\n\n\t// Do QR if:\n\t//   * The head has changed since last time, OR\n\t//   * The last QR did not completely clean every available thing\n\tif head.Revision != fbm.lastQRHeadRev || !fbm.wasLastQRComplete {\n\t\treturn true\n\t}\n\n\t// Do QR if the head was not reclaimable at the last QR time, but\n\t// is old enough now.\n\treturn fbm.lastQRHeadRev > fbm.lastQROldEnoughRev && fbm.isOldEnough(head)\n}\n\nfunc (fbm *folderBlockManager) doReclamation(timer *time.Timer) (err error) {\n\tctx, cancel := context.WithCancel(fbm.ctxWithFBMID(context.Background()))\n\tfbm.setReclamationCancel(cancel)\n\tdefer fbm.cancelReclamation()\n\tdefer timer.Reset(fbm.config.QuotaReclamationPeriod())\n\tdefer fbm.reclamationGroup.Done()\n\n\t// Don't set a context deadline.  For users that have written a\n\t// lot of updates since their last QR, this might involve fetching\n\t// a lot of MD updates in small chunks.  It doesn't hold locks for\n\t// any considerable amount of time, so it should be safe to let it\n\t// run indefinitely.\n\n\t// First get the current head, and see if we're staged or not.\n\thead, err := fbm.helper.getMDForFBM(ctx)\n\tif err != nil {\n\t\treturn err\n\t} else if err := head.isReadableOrError(ctx, fbm.config); err != nil {\n\t\treturn err\n\t} else if head.MergedStatus() != Merged {\n\t\treturn errors.New(\"Skipping quota reclamation while unstaged\")\n\t}\n\n\t// Make sure we're a writer\n\tusername, uid, err := fbm.config.KBPKI().GetCurrentUserInfo(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !head.GetTlfHandle().IsWriter(uid) {\n\t\treturn NewWriteAccessError(head.GetTlfHandle(), username)\n\t}\n\n\tif !fbm.isQRNecessary(head) {\n\t\t// Nothing has changed since last time, so no need to do any QR.\n\t\treturn nil\n\t}\n\tvar mostRecentOldEnoughRev MetadataRevision\n\tvar complete bool\n\tdefer func() {\n\t\t// Remember the QR we just performed.\n\t\tif err == nil && head != nil {\n\t\t\tfbm.lastQRHeadRev = head.Revision\n\t\t\tfbm.lastQROldEnoughRev = mostRecentOldEnoughRev\n\t\t\tfbm.wasLastQRComplete = complete\n\t\t}\n\t}()\n\n\t// Then grab the lock for this folder, so we're the only one doing\n\t// garbage collection for a while.\n\tlocked, err := fbm.config.MDServer().TruncateLock(ctx, fbm.id)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !locked {\n\t\tfbm.log.CDebugf(ctx, \"Couldn't get the truncate lock\")\n\t\treturn fmt.Errorf(\"Couldn't get the truncate lock for folder %d\",\n\t\t\tfbm.id)\n\t}\n\tdefer func() {\n\t\tunlocked, unlockErr := fbm.config.MDServer().TruncateUnlock(ctx, fbm.id)\n\t\tif unlockErr != nil {\n\t\t\tfbm.log.CDebugf(ctx, \"Couldn't release the truncate lock: %v\",\n\t\t\t\tunlockErr)\n\t\t}\n\t\tif !unlocked {\n\t\t\tfbm.log.CDebugf(ctx, \"Couldn't unlock the truncate lock\")\n\t\t}\n\t}()\n\n\tmostRecentOldEnoughRev, lastGCRev, err :=\n\t\tfbm.getMostRecentOldEnoughAndGCRevisions(ctx, head)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif mostRecentOldEnoughRev == MetadataRevisionUninitialized ||\n\t\tmostRecentOldEnoughRev <= lastGCRev {\n\t\t// TODO: need a log level more fine-grained than Debug to\n\t\t// print out that we're not doing reclamation.\n\t\tcomplete = true\n\t\treturn nil\n\t}\n\n\t// Don't try to do too many at a time.\n\tshortened := false\n\tif mostRecentOldEnoughRev-lastGCRev > numMaxRevisionsPerQR {\n\t\tmostRecentOldEnoughRev = lastGCRev + numMaxRevisionsPerQR\n\t\tshortened = true\n\t}\n\n\t// Don't print these until we know for sure that we'll be\n\t// reclaiming some quota, to avoid log pollution.\n\tfbm.log.CDebugf(ctx, \"Starting quota reclamation process\")\n\tdefer func() {\n\t\tfbm.log.CDebugf(ctx, \"Ending quota reclamation process: %v\", err)\n\t\tfbm.lastReclamationTimeLock.Lock()\n\t\tdefer fbm.lastReclamationTimeLock.Unlock()\n\t\tfbm.lastReclamationTime = fbm.config.Clock().Now()\n\t}()\n\n\tptrs, latestRev, complete, err :=\n\t\tfbm.getUnreferencedBlocks(ctx, mostRecentOldEnoughRev, lastGCRev)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(ptrs) == 0 && !shortened {\n\t\tcomplete = true\n\t\treturn nil\n\t}\n\n\tzeroRefCounts, err := fbm.deleteBlockRefs(ctx, head, ptrs)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn fbm.finalizeReclamation(ctx, ptrs, zeroRefCounts, latestRev)\n}\n\nfunc (fbm *folderBlockManager) reclaimQuotaInBackground() {\n\ttimer := time.NewTimer(fbm.config.QuotaReclamationPeriod())\n\ttimerChan := timer.C\n\tfor {\n\t\t// Don't let the timer fire if auto-reclamation is turned off.\n\t\tif fbm.config.QuotaReclamationPeriod().Seconds() == 0 {\n\t\t\ttimer.Stop()\n\t\t\t// Use a channel that will never fire instead.\n\t\t\ttimerChan = make(chan time.Time)\n\t\t}\n\t\tselect {\n\t\tcase <-fbm.shutdownChan:\n\t\t\treturn\n\t\tcase <-timerChan:\n\t\t\tfbm.reclamationGroup.Add(1)\n\t\tcase <-fbm.forceReclamationChan:\n\t\t}\n\n\t\terr := fbm.doReclamation(timer)\n\t\tif _, ok := err.(WriteAccessError); ok {\n\t\t\t// If we got a write access error, don't bother with the\n\t\t\t// timer anymore. Don't completely shut down, since we\n\t\t\t// don't want forced reclamations to hang.\n\t\t\ttimer.Stop()\n\t\t\ttimerChan = make(chan time.Time)\n\t\t}\n\t}\n}\n\nfunc (fbm *folderBlockManager) getLastReclamationTime() time.Time {\n\tfbm.lastReclamationTimeLock.Lock()\n\tdefer fbm.lastReclamationTimeLock.Unlock()\n\treturn fbm.lastReclamationTime\n}\n", "idx": 3, "id": 11988, "msg": "", "proj": "keybase-kbfs", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -24,6 +24,19 @@\n \n const char h2o_httpclient_error_is_eos[] = \"end of stream\";\n const char h2o_httpclient_error_refused_stream[] = \"refused stream\";\n+const char h2o_httpclient_error_unknown_alpn_protocol[] = \"unknown alpn protocol\";\n+const char h2o_httpclient_error_io[] = \"I/O error\";\n+const char h2o_httpclient_error_connection_timeout[] = \"connection timeout\";\n+const char h2o_httpclient_error_first_byte_timeout[] = \"first byte timeout\";\n+const char h2o_httpclient_error_io_timeout[] = \"I/O timeout\";\n+const char h2o_httpclient_error_http1_line_folding[] = \"line folding of header fields is not supported\";\n+const char h2o_httpclient_error_http1_unexpected_transfer_encoding[] = \"unexpected type of transfer-encoding\";\n+const char h2o_httpclient_error_http1_invalid_content_length[] = \"invalid content-length\";\n+const char h2o_httpclient_error_http1_parse_failed[] = \"failed to parse the response\";\n+const char h2o_httpclient_error_http2_upstream_protocol[] = \"upstream protocol error\";\n+const char h2o_httpclient_error_http2_goaway_received[] = \"GOAWAY received\";\n+const char h2o_httpclient_error_http2_flow_control_window_overflow[] = \"flow control window overflow\";\n+const char h2o_httpclient_error_internal[] = \"internal error\";\n \n void h2o_httpclient_connection_pool_init(h2o_httpclient_connection_pool_t *connpool, h2o_socketpool_t *sockpool)\n {", "y": 1, "oldf": "/*\n * Copyright (c) 2018 Ichito Nagata, Fastly, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to\n * deal in the Software without restriction, including without limitation the\n * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n * sell copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n * IN THE SOFTWARE.\n */\n\n#include \"h2o/httpclient.h\"\n\nconst char h2o_httpclient_error_is_eos[] = \"end of stream\";\nconst char h2o_httpclient_error_refused_stream[] = \"refused stream\";\n\nvoid h2o_httpclient_connection_pool_init(h2o_httpclient_connection_pool_t *connpool, h2o_socketpool_t *sockpool)\n{\n    connpool->socketpool = sockpool;\n    h2o_linklist_init_anchor(&connpool->http2.conns);\n}\n\nstatic void close_client(h2o_httpclient_t *client)\n{\n    if (client->_connect_req != NULL) {\n        h2o_socketpool_cancel_connect(client->_connect_req);\n        client->_connect_req = NULL;\n    }\n\n    if (h2o_timer_is_linked(&client->_timeout))\n        h2o_timer_unlink(&client->_timeout);\n\n    free(client);\n}\n\nstatic void on_connect_error(h2o_httpclient_t *client, const char *errstr)\n{\n    assert(errstr != NULL);\n    client->_cb.on_connect(client, errstr, NULL, NULL, NULL, 0, NULL, NULL, NULL, NULL);\n    close_client(client);\n}\n\nstatic void on_connect_timeout(h2o_timer_t *entry)\n{\n    h2o_httpclient_t *client = H2O_STRUCT_FROM_MEMBER(h2o_httpclient_t, _timeout, entry);\n    on_connect_error(client, \"connection timeout\");\n}\n\nstatic void do_cancel(h2o_httpclient_t *_client)\n{\n    h2o_httpclient_t *client = (void *)_client;\n    close_client(client);\n}\n\nstatic h2o_httpclient_t *create_client(h2o_mem_pool_t *pool, void *data, h2o_httpclient_ctx_t *ctx, h2o_httpclient_connect_cb cb)\n{\n#define SZ_MAX(x, y) ((x) > (y) ? (x) : (y))\n    size_t sz = SZ_MAX(h2o_httpclient__h1_size, h2o_httpclient__h2_size);\n#undef SZ_MAX\n    h2o_httpclient_t *client = h2o_mem_alloc(sz);\n    memset(client, 0, sz);\n    client->pool = pool;\n    client->ctx = ctx;\n    client->data = data;\n    client->cancel = do_cancel;\n    client->steal_socket = NULL;\n    client->get_socket = NULL;\n    client->update_window = NULL;\n    client->write_req = NULL;\n    client->_cb.on_connect = cb;\n    client->_timeout.cb = on_connect_timeout;\n\n    return client;\n}\n\nstatic void on_pool_connect(h2o_socket_t *sock, const char *errstr, void *data, h2o_url_t *origin)\n{\n    h2o_httpclient_t *client = data;\n\n    h2o_timer_unlink(&client->_timeout);\n\n    client->_connect_req = NULL;\n\n    if (sock == NULL) {\n        assert(errstr != NULL);\n        on_connect_error(client, errstr);\n        return;\n    }\n\n    h2o_iovec_t alpn_proto;\n    if (sock->ssl == NULL || (alpn_proto = h2o_socket_ssl_get_selected_protocol(sock)).len == 0) {\n        h2o_httpclient__h1_on_connect(client, sock, origin);\n    } else {\n        if (h2o_memis(alpn_proto.base, alpn_proto.len, H2O_STRLIT(\"h2\"))) {\n            /* detach this socket from the socketpool to count the number of h1 connections correctly */\n            h2o_socketpool_detach(client->connpool->socketpool, sock);\n            h2o_httpclient__h2_on_connect(client, sock, origin);\n        } else if (memcmp(alpn_proto.base, \"http/1.1\", alpn_proto.len) == 0) {\n            h2o_httpclient__h1_on_connect(client, sock, origin);\n        } else {\n            on_connect_error(client, \"unknown alpn protocol\");\n        }\n    }\n}\n\nstatic int should_use_h2(int8_t ratio, int8_t *counter)\n{\n    /* weighted fair queueing */\n    if (*counter < 0)\n        *counter = ratio == 0 ? 0 : 50 / ratio; /* set initial counter value */\n    int use_h2 = (((int)ratio * *counter) % 100) + ratio >= 100;\n    if (++*counter == 100)\n        *counter = 0;\n    return use_h2;\n}\n\nvoid h2o_httpclient_connect(h2o_httpclient_t **_client, h2o_mem_pool_t *pool, void *data, h2o_httpclient_ctx_t *ctx,\n                            h2o_httpclient_connection_pool_t *connpool, h2o_url_t *origin, h2o_httpclient_connect_cb cb)\n{\n    static const h2o_iovec_t both_protos = {H2O_STRLIT(\"\\x02\"\n                                                       \"h2\"\n                                                       \"\\x08\"\n                                                       \"http/1.1\")};\n    assert(connpool != NULL);\n    h2o_iovec_t alpn_protos = h2o_iovec_init(NULL, 0);\n\n    h2o_httpclient_t *client = create_client(pool, data, ctx, cb);\n    client->connpool = connpool;\n    if (_client != NULL)\n        *_client = client;\n\n    client->timings.start_at = h2o_gettimeofday(ctx->loop);\n\n    h2o_httpclient__h2_conn_t *http2_conn = NULL;\n    if (!h2o_linklist_is_empty(&connpool->http2.conns)) {\n        http2_conn = H2O_STRUCT_FROM_MEMBER(h2o_httpclient__h2_conn_t, link, connpool->http2.conns.next);\n        if (http2_conn->num_streams >= h2o_httpclient__h2_get_max_concurrent_streams(http2_conn))\n            http2_conn = NULL;\n    }\n\n    if (ctx->http2.ratio < 0) {\n        /* mix mode */\n\n        if (http2_conn != NULL && connpool->socketpool->_shared.pooled_count != 0) {\n            /* both of h1 and h2 connections exist, compare in-use ratio */\n            double http1_ratio = (double)(connpool->socketpool->_shared.count - connpool->socketpool->_shared.pooled_count) /\n                                 connpool->socketpool->_shared.count;\n            double http2_ratio = http2_conn->num_streams / h2o_httpclient__h2_get_max_concurrent_streams(http2_conn);\n            if (http2_ratio <= http1_ratio) {\n                h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n            } else {\n                goto UseSocketPool;\n            }\n        } else if (http2_conn != NULL) {\n            /* h2 connection exists */\n            h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n        } else if (connpool->socketpool->_shared.pooled_count != 0) {\n            /* h1 connection exists */\n            goto UseSocketPool;\n        } else {\n            /* no connections, connect using ALPN */\n            alpn_protos = both_protos;\n            goto UseSocketPool;\n        }\n    } else {\n        /* fixed ratio mode */\n\n        if (should_use_h2(ctx->http2.ratio, &ctx->http2.counter)) {\n            if (http2_conn != NULL) {\n                h2o_httpclient__h2_on_connect(client, http2_conn->sock, &http2_conn->origin_url);\n            } else {\n                alpn_protos = both_protos;\n                goto UseSocketPool;\n            }\n        } else {\n            goto UseSocketPool;\n        }\n    }\n\n    return;\n\nUseSocketPool:\n    h2o_timer_link(client->ctx->loop, client->ctx->connect_timeout, &client->_timeout);\n    h2o_socketpool_connect(&client->_connect_req, connpool->socketpool, origin, ctx->loop, ctx->getaddr_receiver, alpn_protos,\n                           on_pool_connect, client);\n}\n", "idx": 1, "id": 13828, "msg": "I think that this error is version agnostic, and that therefore \"http1\" should be omitted. Also, I would appreciate it if you could check what we do for H2, and if the behavior is different between the protocol versions, align them.", "proj": "h2o-h2o", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -91,6 +91,7 @@ type DockerClient interface {\n \tVersion() (string, error)\n \tInspectImage(string) (*docker.Image, error)\n \tRemoveImage(string, time.Duration) error\n+\tLoadImage(io.Reader, time.Duration) error\n }\n \n // DockerGoClient wraps the underlying go-dockerclient library.", "y": 0, "oldf": "// Copyright 2014-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n// not use this file except in compliance with the License. A copy of the\n// License is located at\n//\n//\thttp://aws.amazon.com/apache2.0/\n//\n// or in the \"license\" file accompanying this file. This file is distributed\n// on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n// express or implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage engine\n\nimport (\n\t\"archive/tar\"\n\t\"bufio\"\n\t\"encoding/json\"\n\t\"io\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/net/context\"\n\n\t\"github.com/aws/amazon-ecs-agent/agent/api\"\n\t\"github.com/aws/amazon-ecs-agent/agent/config\"\n\t\"github.com/aws/amazon-ecs-agent/agent/ecr\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/dockerauth\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/dockerclient\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/dockeriface\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/emptyvolume\"\n\t\"github.com/aws/amazon-ecs-agent/agent/utils/ttime\"\n\t\"github.com/cihub/seelog\"\n\n\tdocker \"github.com/fsouza/go-dockerclient\"\n)\n\nconst (\n\tdockerDefaultTag = \"latest\"\n)\n\n// Timelimits for docker operations enforced above docker\nconst (\n\t// ListContainersTimeout is the timeout for the ListContainers API.\n\tListContainersTimeout   = 10 * time.Minute\n\tpullImageTimeout        = 2 * time.Hour\n\tcreateContainerTimeout  = 4 * time.Minute\n\tstartContainerTimeout   = 3 * time.Minute\n\tstopContainerTimeout    = 30 * time.Second\n\tremoveContainerTimeout  = 5 * time.Minute\n\tinspectContainerTimeout = 30 * time.Second\n\tremoveImageTimeout      = 3 * time.Minute\n\n\t// dockerPullBeginTimeout is the timeout from when a 'pull' is called to when\n\t// we expect to see output on the pull progress stream. This is to work\n\t// around a docker bug which sometimes results in pulls not progressing.\n\tdockerPullBeginTimeout = 5 * time.Minute\n\n\t// pullStatusSuppressDelay controls the time where pull status progress bar\n\t// output will be suppressed in debug mode\n\tpullStatusSuppressDelay = 2 * time.Second\n\n\t// StatsInactivityTimeout controls the amount of time we hold open a\n\t// connection to the Docker daemon waiting for stats data\n\tStatsInactivityTimeout = 5 * time.Second\n)\n\n// DockerClient interface to make testing it easier\ntype DockerClient interface {\n\t// SupportedVersions returns a slice of the supported docker versions (or at least supposedly supported).\n\tSupportedVersions() []dockerclient.DockerVersion\n\t// WithVersion returns a new DockerClient for which all operations will use the given remote api version.\n\t// A default version will be used for a client not produced via this method.\n\tWithVersion(dockerclient.DockerVersion) DockerClient\n\tContainerEvents(ctx context.Context) (<-chan DockerContainerChangeEvent, error)\n\n\tPullImage(image string, authData *api.RegistryAuthenticationData) DockerContainerMetadata\n\n\tCreateContainer(*docker.Config, *docker.HostConfig, string, time.Duration) DockerContainerMetadata\n\tStartContainer(string, time.Duration) DockerContainerMetadata\n\tStopContainer(string, time.Duration) DockerContainerMetadata\n\tDescribeContainer(string) (api.ContainerStatus, DockerContainerMetadata)\n\tRemoveContainer(string, time.Duration) error\n\n\tInspectContainer(string, time.Duration) (*docker.Container, error)\n\tListContainers(bool, time.Duration) ListContainersResponse\n\tStats(string, context.Context) (<-chan *docker.Stats, error)\n\n\tVersion() (string, error)\n\tInspectImage(string) (*docker.Image, error)\n\tRemoveImage(string, time.Duration) error\n}\n\n// DockerGoClient wraps the underlying go-dockerclient library.\n// It exists primarily for the following three purposes:\n// 1) Provide an abstraction over inputs and outputs,\n//    a) Inputs: Trims them down to what we actually need (largely unchanged tbh)\n//    b) Outputs: Unifies error handling and the common 'start->inspect'\n//       pattern by having a consistent error output. This error output\n//       contains error data with a given Name that aims to be presentable as a\n//       'reason' in state changes. It also filters out the information about a\n//       container that is of interest, such as network bindings, while\n//       ignoring the rest.\n// 2) Timeouts: It adds timeouts everywhere, mostly as a reaction to\n//    pull-related issues in the Docker daemon.\n// 3) Versioning: It abstracts over multiple client versions to allow juggling\n//    appropriately there.\n// Implements DockerClient\ntype dockerGoClient struct {\n\tclientFactory    dockerclient.Factory\n\tversion          dockerclient.DockerVersion\n\tauth             dockerauth.DockerAuthProvider\n\tecrClientFactory ecr.ECRFactory\n\tconfig           *config.Config\n\n\t_time     ttime.Time\n\t_timeOnce sync.Once\n}\n\nfunc (dg *dockerGoClient) WithVersion(version dockerclient.DockerVersion) DockerClient {\n\treturn &dockerGoClient{\n\t\tclientFactory: dg.clientFactory,\n\t\tversion:       version,\n\t\tauth:          dg.auth,\n\t\tconfig:        dg.config,\n\t}\n}\n\n// scratchCreateLock guards against multiple 'scratch' image creations at once\nvar scratchCreateLock sync.Mutex\n\n// NewDockerGoClient creates a new DockerGoClient\nfunc NewDockerGoClient(clientFactory dockerclient.Factory, cfg *config.Config) (DockerClient, error) {\n\tclient, err := clientFactory.GetDefaultClient()\n\tif err != nil {\n\t\tlog.Error(\"Unable to connect to docker daemon. Ensure docker is running.\", \"err\", err)\n\t\treturn nil, err\n\t}\n\n\t// Even if we have a dockerclient, the daemon might not be running. Ping it\n\t// to ensure it's up.\n\terr = client.Ping()\n\tif err != nil {\n\t\tlog.Error(\"Unable to ping docker daemon. Ensure docker is running.\", \"err\", err)\n\t\treturn nil, err\n\t}\n\n\tvar dockerAuthData json.RawMessage\n\tif cfg.EngineAuthData != nil {\n\t\tdockerAuthData = cfg.EngineAuthData.Contents()\n\t}\n\treturn &dockerGoClient{\n\t\tclientFactory:    clientFactory,\n\t\tauth:             dockerauth.NewDockerAuthProvider(cfg.EngineAuthType, dockerAuthData),\n\t\tecrClientFactory: ecr.NewECRFactory(cfg.AcceptInsecureCert),\n\t\tconfig:           cfg,\n\t}, nil\n}\n\nfunc (dg *dockerGoClient) dockerClient() (dockeriface.Client, error) {\n\tif dg.version == \"\" {\n\t\treturn dg.clientFactory.GetDefaultClient()\n\t}\n\treturn dg.clientFactory.GetClient(dg.version)\n}\n\nfunc (dg *dockerGoClient) time() ttime.Time {\n\tdg._timeOnce.Do(func() {\n\t\tif dg._time == nil {\n\t\t\tdg._time = &ttime.DefaultTime{}\n\t\t}\n\t})\n\treturn dg._time\n}\n\nfunc (dg *dockerGoClient) PullImage(image string, authData *api.RegistryAuthenticationData) DockerContainerMetadata {\n\ttimeout := dg.time().After(pullImageTimeout)\n\n\tresponse := make(chan DockerContainerMetadata, 1)\n\tgo func() { response <- dg.pullImage(image, authData) }()\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-timeout:\n\t\treturn DockerContainerMetadata{Error: &DockerTimeoutError{pullImageTimeout, \"pulled\"}}\n\t}\n}\n\nfunc (dg *dockerGoClient) pullImage(image string, authData *api.RegistryAuthenticationData) DockerContainerMetadata {\n\tlog.Debug(\"Pulling image\", \"image\", image)\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotGetDockerClientError{version: dg.version, err: err}}\n\t}\n\n\t// Special case; this image is not one that should be pulled, but rather\n\t// should be created locally if necessary\n\tif image == emptyvolume.Image+\":\"+emptyvolume.Tag {\n\t\tscratchErr := dg.createScratchImageIfNotExists()\n\t\tif scratchErr != nil {\n\t\t\treturn DockerContainerMetadata{Error: &api.DefaultNamedError{Name: \"CreateEmptyVolumeError\", Err: \"Could not create empty volume \" + scratchErr.Error()}}\n\t\t}\n\t\treturn DockerContainerMetadata{}\n\t}\n\n\tauthConfig, err := dg.getAuthdata(image, authData)\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotPullContainerError{err}}\n\t}\n\n\tpullDebugOut, pullWriter := io.Pipe()\n\tdefer pullWriter.Close()\n\n\trepository, tag := parseRepositoryTag(image)\n\tif tag == \"\" {\n\t\trepository = repository + \":\" + dockerDefaultTag\n\t} else {\n\t\trepository = image\n\t}\n\n\topts := docker.PullImageOptions{\n\t\tRepository:   repository,\n\t\tOutputStream: pullWriter,\n\t}\n\ttimeout := dg.time().After(dockerPullBeginTimeout)\n\t// pullBegan is a channel indicating that we have seen at least one line of data on the 'OutputStream' above.\n\t// It is here to guard against a bug wherin docker never writes anything to that channel and hangs in pulling forever.\n\tpullBegan := make(chan bool, 1)\n\t// pullBeganOnce ensures we only indicate it began once (since our channel will only be read 0 or 1 times)\n\tpullBeganOnce := sync.Once{}\n\n\tgo func() {\n\t\treader := bufio.NewReader(pullDebugOut)\n\t\tvar line string\n\t\tvar pullErr error\n\t\tvar statusDisplayed time.Time\n\t\tfor pullErr == nil {\n\t\t\tline, pullErr = reader.ReadString('\\n')\n\t\t\tif pullErr != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tpullBeganOnce.Do(func() {\n\t\t\t\tpullBegan <- true\n\t\t\t})\n\n\t\t\tnow := time.Now()\n\t\t\tif !strings.Contains(line, \"[=\") || now.After(statusDisplayed.Add(pullStatusSuppressDelay)) {\n\t\t\t\t// skip most of the progress bar lines, but retain enough for debugging\n\t\t\t\tlog.Debug(\"Pulling image\", \"image\", image, \"status\", line)\n\t\t\t\tstatusDisplayed = now\n\t\t\t}\n\n\t\t\tif strings.Contains(line, \"already being pulled by another client. Waiting.\") {\n\t\t\t\t// This can mean the daemon is 'hung' in pulling status for this image, but we can't be sure.\n\t\t\t\tlog.Error(\"Image 'pull' status marked as already being pulled\", \"image\", image, \"status\", line)\n\t\t\t}\n\t\t}\n\t\tif pullErr != nil && pullErr != io.EOF {\n\t\t\tlog.Warn(\"Error reading pull image status\", \"image\", image, \"err\", pullErr)\n\t\t}\n\t}()\n\tpullFinished := make(chan error, 1)\n\tgo func() {\n\t\tpullFinished <- client.PullImage(opts, authConfig)\n\t\tlog.Debug(\"Pulling image complete\", \"image\", image)\n\t}()\n\n\tselect {\n\tcase <-pullBegan:\n\t\tbreak\n\tcase pullErr := <-pullFinished:\n\t\tif pullErr != nil {\n\t\t\treturn DockerContainerMetadata{Error: CannotPullContainerError{pullErr}}\n\t\t}\n\t\treturn DockerContainerMetadata{}\n\tcase <-timeout:\n\t\treturn DockerContainerMetadata{Error: &DockerTimeoutError{dockerPullBeginTimeout, \"pullBegin\"}}\n\t}\n\tlog.Debug(\"Pull began for image\", \"image\", image)\n\tdefer log.Debug(\"Pull completed for image\", \"image\", image)\n\n\terr = <-pullFinished\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotPullContainerError{err}}\n\t}\n\treturn DockerContainerMetadata{}\n}\n\nfunc (dg *dockerGoClient) createScratchImageIfNotExists() error {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tscratchCreateLock.Lock()\n\tdefer scratchCreateLock.Unlock()\n\n\t_, err = client.InspectImage(emptyvolume.Image + \":\" + emptyvolume.Tag)\n\tif err == nil {\n\t\t// Already exists; assume that it's okay to use it\n\t\treturn nil\n\t}\n\n\treader, writer := io.Pipe()\n\n\temptytarball := tar.NewWriter(writer)\n\tgo func() {\n\t\temptytarball.Close()\n\t\twriter.Close()\n\t}()\n\n\t// Create it from an empty tarball\n\terr = client.ImportImage(docker.ImportImageOptions{\n\t\tRepository:  emptyvolume.Image,\n\t\tTag:         emptyvolume.Tag,\n\t\tSource:      \"-\",\n\t\tInputStream: reader,\n\t})\n\treturn err\n}\n\nfunc (dg *dockerGoClient) InspectImage(image string) (*docker.Image, error) {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn client.InspectImage(image)\n}\n\nfunc (dg *dockerGoClient) getAuthdata(image string, authData *api.RegistryAuthenticationData) (docker.AuthConfiguration, error) {\n\tif authData == nil || authData.Type != \"ecr\" {\n\t\treturn dg.auth.GetAuthconfig(image)\n\t}\n\tprovider := dockerauth.NewECRAuthProvider(authData.ECRAuthData, dg.ecrClientFactory)\n\tauthConfig, err := provider.GetAuthconfig(image)\n\tif err != nil {\n\t\treturn authConfig, CannotPullECRContainerError{err}\n\t}\n\treturn authConfig, nil\n}\n\nfunc (dg *dockerGoClient) CreateContainer(config *docker.Config, hostConfig *docker.HostConfig, name string, timeout time.Duration) DockerContainerMetadata {\n\t// Create a context that times out after the 'timeout' duration\n\t// This is defined by the const 'createContainerTimeout'. Injecting the 'timeout'\n\t// makes it easier to write tests.\n\t// Eventually, the context should be initialized from a parent root context\n\t// instead of TODO.\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan DockerContainerMetadata, 1)\n\tgo func() { response <- dg.createContainer(ctx, config, hostConfig, name) }()\n\n\t// Wait until we get a response or for the 'done' context channel\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\t// Context has either expired or canceled. If it has timed out,\n\t\t// send back the DockerTimeoutError\n\t\terr := ctx.Err()\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn DockerContainerMetadata{Error: &DockerTimeoutError{timeout, \"created\"}}\n\t\t}\n\t\t// Context was canceled even though there was no timeout. Send\n\t\t// back an error.\n\t\treturn DockerContainerMetadata{Error: &CannotCreateContainerError{err}}\n\t}\n}\n\nfunc (dg *dockerGoClient) createContainer(ctx context.Context, config *docker.Config, hostConfig *docker.HostConfig, name string) DockerContainerMetadata {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotGetDockerClientError{version: dg.version, err: err}}\n\t}\n\n\tcontainerOptions := docker.CreateContainerOptions{\n\t\tConfig:     config,\n\t\tHostConfig: hostConfig,\n\t\tName:       name,\n\t\tContext:    ctx,\n\t}\n\tdockerContainer, err := client.CreateContainer(containerOptions)\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotCreateContainerError{err}}\n\t}\n\treturn dg.containerMetadata(dockerContainer.ID)\n}\n\nfunc (dg *dockerGoClient) StartContainer(id string, timeout time.Duration) DockerContainerMetadata {\n\t// Create a context that times out after the 'timeout' duration\n\t// This is defined by the const 'startContainerTimeout'. Injecting the 'timeout'\n\t// makes it easier to write tests.\n\t// Eventually, the context should be initialized from a parent root context\n\t// instead of TODO.\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan DockerContainerMetadata, 1)\n\tgo func() { response <- dg.startContainer(ctx, id) }()\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\t// Context has either expired or canceled. If it has timed out,\n\t\t// send back the DockerTimeoutError\n\t\terr := ctx.Err()\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn DockerContainerMetadata{Error: &DockerTimeoutError{timeout, \"started\"}}\n\t\t}\n\t\treturn DockerContainerMetadata{Error: CannotStartContainerError{err}}\n\t}\n}\n\nfunc (dg *dockerGoClient) startContainer(ctx context.Context, id string) DockerContainerMetadata {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotGetDockerClientError{version: dg.version, err: err}}\n\t}\n\n\terr = client.StartContainerWithContext(id, nil, ctx)\n\tmetadata := dg.containerMetadata(id)\n\tif err != nil {\n\t\tmetadata.Error = CannotStartContainerError{err}\n\t}\n\n\treturn metadata\n}\n\nfunc dockerStateToState(state docker.State) api.ContainerStatus {\n\tif state.Running {\n\t\treturn api.ContainerRunning\n\t}\n\treturn api.ContainerStopped\n}\n\nfunc (dg *dockerGoClient) DescribeContainer(dockerID string) (api.ContainerStatus, DockerContainerMetadata) {\n\tdockerContainer, err := dg.InspectContainer(dockerID, inspectContainerTimeout)\n\tif err != nil {\n\t\treturn api.ContainerStatusNone, DockerContainerMetadata{Error: CannotDescribeContainerError{err}}\n\t}\n\treturn dockerStateToState(dockerContainer.State), metadataFromContainer(dockerContainer)\n}\n\nfunc (dg *dockerGoClient) InspectContainer(dockerID string, timeout time.Duration) (*docker.Container, error) {\n\ttype inspectResponse struct {\n\t\tcontainer *docker.Container\n\t\terr       error\n\t}\n\t// Create a context that times out after the 'timeout' duration\n\t// This is defined by the const 'inspectContainerTimeout'. Injecting the 'timeout'\n\t// makes it easier to write tests.\n\t// Eventually, the context should be initialized from a parent root context\n\t// instead of TODO.\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan inspectResponse, 1)\n\tgo func() {\n\t\tcontainer, err := dg.inspectContainer(dockerID, ctx)\n\t\tresponse <- inspectResponse{container, err}\n\t}()\n\n\t// Wait until we get a response or for the 'done' context channel\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp.container, resp.err\n\tcase <-ctx.Done():\n\t\terr := ctx.Err()\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn nil, &DockerTimeoutError{timeout, \"inspecting\"}\n\t\t}\n\n\t\treturn nil, &CannotInspectContainerError{err}\n\t}\n}\n\nfunc (dg *dockerGoClient) inspectContainer(dockerID string, ctx context.Context) (*docker.Container, error) {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn client.InspectContainerWithContext(dockerID, ctx)\n}\n\nfunc (dg *dockerGoClient) StopContainer(dockerID string, timeout time.Duration) DockerContainerMetadata {\n\ttimeout = timeout + dg.config.DockerStopTimeout\n\n\t// Create a context that times out after the 'timeout' duration\n\t// This is defined by the const 'stopContainerTimeout' and the\n\t// 'DockerStopTimeout' in the config. Injecting the 'timeout'\n\t// makes it easier to write tests.\n\t// Eventually, the context should be initialized from a parent root context\n\t// instead of TODO.\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan DockerContainerMetadata, 1)\n\tgo func() { response <- dg.stopContainer(ctx, dockerID) }()\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\t// Context has either expired or canceled. If it has timed out,\n\t\t// send back the DockerTimeoutError\n\t\terr := ctx.Err()\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn DockerContainerMetadata{Error: &DockerTimeoutError{timeout, \"stopped\"}}\n\t\t}\n\t\treturn DockerContainerMetadata{Error: &CannotStopContainerError{err}}\n\t}\n}\n\nfunc (dg *dockerGoClient) stopContainer(ctx context.Context, dockerID string) DockerContainerMetadata {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn DockerContainerMetadata{Error: CannotGetDockerClientError{version: dg.version, err: err}}\n\t}\n\n\terr = client.StopContainerWithContext(dockerID, uint(dg.config.DockerStopTimeout/time.Second), ctx)\n\tmetadata := dg.containerMetadata(dockerID)\n\tif err != nil {\n\t\tlog.Debug(\"Error stopping container\", \"err\", err, \"id\", dockerID)\n\t\tif metadata.Error == nil {\n\t\t\tmetadata.Error = CannotStopContainerError{err}\n\t\t}\n\t}\n\treturn metadata\n}\n\nfunc (dg *dockerGoClient) RemoveContainer(dockerID string, timeout time.Duration) error {\n\t// Remove a context that times out after the 'timeout' duration\n\t// This is defined by 'removeContainerTimeout'. 'timeout' makes it\n\t// easier to write tests\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan error, 1)\n\tgo func() { response <- dg.removeContainer(dockerID, ctx) }()\n\t// Wait until we get a response or for the 'done' context channel\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\terr := ctx.Err()\n\t\t// Context has either expired or canceled. If it has timed out,\n\t\t// send back the DockerTimeoutError\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn &DockerTimeoutError{removeContainerTimeout, \"removing\"}\n\t\t}\n\t\treturn &CannotRemoveContainerError{err}\n\t}\n}\n\nfunc (dg *dockerGoClient) removeContainer(dockerID string, ctx context.Context) error {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn client.RemoveContainer(docker.RemoveContainerOptions{\n\t\tID:            dockerID,\n\t\tRemoveVolumes: true,\n\t\tForce:         false,\n\t\tContext:       ctx,\n\t})\n}\n\nfunc (dg *dockerGoClient) containerMetadata(id string) DockerContainerMetadata {\n\tdockerContainer, err := dg.InspectContainer(id, inspectContainerTimeout)\n\tif err != nil {\n\t\treturn DockerContainerMetadata{DockerID: id, Error: CannotInspectContainerError{err}}\n\t}\n\treturn metadataFromContainer(dockerContainer)\n}\n\nfunc metadataFromContainer(dockerContainer *docker.Container) DockerContainerMetadata {\n\tvar bindings []api.PortBinding\n\tvar err api.NamedError\n\tif dockerContainer.NetworkSettings != nil {\n\t\t// Convert port bindings into the format our container expects\n\t\tbindings, err = api.PortBindingFromDockerPortBinding(dockerContainer.NetworkSettings.Ports)\n\t\tif err != nil {\n\t\t\tlog.Crit(\"Docker had network bindings we couldn't understand\", \"err\", err)\n\t\t\treturn DockerContainerMetadata{Error: api.NamedError(err)}\n\t\t}\n\t}\n\tmetadata := DockerContainerMetadata{\n\t\tDockerID:     dockerContainer.ID,\n\t\tPortBindings: bindings,\n\t\tVolumes:      dockerContainer.Volumes,\n\t}\n\t// Workaround for https://github.com/docker/docker/issues/27601\n\t// See https://github.com/docker/docker/blob/v1.12.2/daemon/inspect_unix.go#L38-L43\n\t// for how Docker handles API compatibility on Linux\n\tif len(metadata.Volumes) == 0 {\n\t\tmetadata.Volumes = make(map[string]string)\n\t\tfor _, m := range dockerContainer.Mounts {\n\t\t\tmetadata.Volumes[m.Destination] = m.Source\n\t\t}\n\t}\n\tif !dockerContainer.State.Running && !dockerContainer.State.FinishedAt.IsZero() {\n\t\t// Only record an exitcode if it has exited\n\t\tmetadata.ExitCode = &dockerContainer.State.ExitCode\n\t}\n\tif dockerContainer.State.Error != \"\" {\n\t\tmetadata.Error = NewDockerStateError(dockerContainer.State.Error)\n\t}\n\tif dockerContainer.State.OOMKilled {\n\t\tmetadata.Error = OutOfMemoryError{}\n\t}\n\n\treturn metadata\n}\n\n// Listen to the docker event stream for container changes and pass them up\nfunc (dg *dockerGoClient) ContainerEvents(ctx context.Context) (<-chan DockerContainerChangeEvent, error) {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tevents := make(chan *docker.APIEvents)\n\n\terr = client.AddEventListener(events)\n\tif err != nil {\n\t\tlog.Error(\"Unable to add a docker event listener\", \"err\", err)\n\t\treturn nil, err\n\t}\n\tgo func() {\n\t\t<-ctx.Done()\n\t\tclient.RemoveEventListener(events)\n\t}()\n\n\tchangedContainers := make(chan DockerContainerChangeEvent)\n\n\tgo func() {\n\t\tfor event := range events {\n\t\t\t// currently only container events type needs to be handled\n\t\t\tif event.Type != \"container\" || event.ID == \"\" {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tcontainerID := event.ID\n\t\t\tlog.Debug(\"Got event from docker daemon\", \"event\", event)\n\n\t\t\tvar status api.ContainerStatus\n\t\t\tswitch event.Status {\n\t\t\tcase \"create\":\n\t\t\t\tstatus = api.ContainerCreated\n\t\t\tcase \"start\":\n\t\t\t\tstatus = api.ContainerRunning\n\t\t\tcase \"stop\":\n\t\t\t\tfallthrough\n\t\t\tcase \"die\":\n\t\t\t\tstatus = api.ContainerStopped\n\t\t\tcase \"kill\":\n\t\t\t\tfallthrough\n\t\t\tcase \"rename\":\n\t\t\t\t// TODO, ensure this wasn't one of our containers. This isn't critical\n\t\t\t\t// because we typically have the docker id stored too and a wrong name\n\t\t\t\t// won't be fatal once we do\n\t\t\t\tcontinue\n\t\t\tcase \"restart\":\n\t\t\tcase \"resize\":\n\t\t\tcase \"destroy\":\n\t\t\tcase \"unpause\":\n\t\t\t// These result in us falling through to inspect the container, some\n\t\t\t// out of caution, some because it's a form of state change\n\n\t\t\tcase \"oom\":\n\t\t\t\tseelog.Infof(\"process within container %v died due to OOM\", event.ID)\n\t\t\t\t// \"oom\" can either means any process got OOM'd, but doesn't always\n\t\t\t\t// mean the container dies (non-init processes). If the container also\n\t\t\t\t// dies, you see a \"die\" status as well; we'll update suitably there\n\t\t\t\tfallthrough\n\t\t\tcase \"pause\":\n\t\t\t\t// non image events that aren't of interest currently\n\t\t\t\tfallthrough\n\t\t\tcase \"exec_create\":\n\t\t\t\tfallthrough\n\t\t\tcase \"exec_start\":\n\t\t\t\tfallthrough\n\t\t\tcase \"top\":\n\t\t\t\tfallthrough\n\t\t\tcase \"attach\":\n\t\t\t\tfallthrough\n\t\t\t// image events\n\t\t\tcase \"export\":\n\t\t\t\tfallthrough\n\t\t\tcase \"pull\":\n\t\t\t\tfallthrough\n\t\t\tcase \"push\":\n\t\t\t\tfallthrough\n\t\t\tcase \"tag\":\n\t\t\t\tfallthrough\n\t\t\tcase \"untag\":\n\t\t\t\tfallthrough\n\t\t\tcase \"import\":\n\t\t\t\tfallthrough\n\t\t\tcase \"delete\":\n\t\t\t\t// No interest in image events\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\tif strings.HasPrefix(event.Status, \"exec_create:\") || strings.HasPrefix(event.Status, \"exec_start:\") {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Because docker emits new events even when you use an old event api\n\t\t\t\t// version, it's not that big a deal\n\t\t\t\tseelog.Debugf(\"Unknown status event from docker: %s\", event.Status)\n\t\t\t}\n\n\t\t\tmetadata := dg.containerMetadata(containerID)\n\n\t\t\tchangedContainers <- DockerContainerChangeEvent{\n\t\t\t\tStatus:                  status,\n\t\t\t\tDockerContainerMetadata: metadata,\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn changedContainers, nil\n}\n\n// ListContainers returns a slice of container IDs.\nfunc (dg *dockerGoClient) ListContainers(all bool, timeout time.Duration) ListContainersResponse {\n\t// Create a context that times out after the 'timeout' duration\n\t// This is defined by the const 'listContainersTimeout'. Injecting the 'timeout'\n\t// makes it easier to write tests.\n\t// Eventually, the context should be initialized from a parent root context\n\t// instead of TODO.\n\tctx, cancel := context.WithTimeout(context.TODO(), timeout)\n\tdefer cancel()\n\n\t// Buffered channel so in the case of timeout it takes one write, never gets\n\t// read, and can still be GC'd\n\tresponse := make(chan ListContainersResponse, 1)\n\tgo func() { response <- dg.listContainers(all, ctx) }()\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\t// Context has either expired or canceled. If it has timed out,\n\t\t// send back the DockerTimeoutError\n\t\terr := ctx.Err()\n\t\tif err == context.DeadlineExceeded {\n\t\t\treturn ListContainersResponse{Error: &DockerTimeoutError{timeout, \"listing\"}}\n\t\t}\n\t\treturn ListContainersResponse{Error: &CannotListContainersError{err}}\n\t}\n}\n\nfunc (dg *dockerGoClient) listContainers(all bool, ctx context.Context) ListContainersResponse {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn ListContainersResponse{Error: err}\n\t}\n\n\tcontainers, err := client.ListContainers(docker.ListContainersOptions{\n\t\tAll:     all,\n\t\tContext: ctx,\n\t})\n\tif err != nil {\n\t\treturn ListContainersResponse{Error: err}\n\t}\n\n\t// We get an empty slice if there are no containers to be listed.\n\t// Extract container IDs from this list.\n\tcontainerIDs := make([]string, len(containers))\n\tfor i, container := range containers {\n\t\tcontainerIDs[i] = container.ID\n\t}\n\n\treturn ListContainersResponse{DockerIDs: containerIDs, Error: nil}\n}\n\nfunc (dg *dockerGoClient) SupportedVersions() []dockerclient.DockerVersion {\n\treturn dg.clientFactory.FindAvailableVersions()\n}\n\nfunc (dg *dockerGoClient) Version() (string, error) {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tinfo, err := client.Version()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn info.Get(\"Version\"), nil\n}\n\n// Stats returns a channel of *docker.Stats entries for the container.\nfunc (dg *dockerGoClient) Stats(id string, ctx context.Context) (<-chan *docker.Stats, error) {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstats := make(chan *docker.Stats)\n\toptions := docker.StatsOptions{\n\t\tID:                id,\n\t\tStats:             stats,\n\t\tStream:            true,\n\t\tContext:           ctx,\n\t\tInactivityTimeout: StatsInactivityTimeout,\n\t}\n\n\tgo func() {\n\t\tstatsErr := client.Stats(options)\n\t\tif statsErr != nil {\n\t\t\tseelog.Infof(\"Error retrieving stats for container %s: %v\", id, statsErr)\n\t\t}\n\t}()\n\n\treturn stats, nil\n}\n\nfunc (dg *dockerGoClient) RemoveImage(imageName string, imageRemovalTimeout time.Duration) error {\n\tctx, cancel := context.WithTimeout(context.Background(), imageRemovalTimeout)\n\tdefer cancel()\n\n\tresponse := make(chan error, 1)\n\tgo func() { response <- dg.removeImage(imageName) }()\n\tselect {\n\tcase resp := <-response:\n\t\treturn resp\n\tcase <-ctx.Done():\n\t\treturn &DockerTimeoutError{imageRemovalTimeout, \"removing image\"}\n\t}\n}\n\nfunc (dg *dockerGoClient) removeImage(imageName string) error {\n\tclient, err := dg.dockerClient()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn client.RemoveImage(imageName)\n}\n", "idx": 2, "id": 15551, "msg": "", "proj": "aws-amazon-ecs-agent", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -121,7 +121,11 @@ class NotesControllerTest < ActionController::TestCase\n \n     assert_routing(\n       { :path => \"/user/username/notes\", :method => :get },\n-      { :controller => \"notes\", :action => \"mine\", :display_name => \"username\" }\n+      { :controller => \"notes\", :action => \"user\", :display_name => \"username\" }\n+    )\n+    assert_routing(\n+      { :path => \"/traces/mine\", :method => :get },\n+      { :controller => \"traces\", :action => \"mine\" }\n     )\n   end\n ", "y": 1, "oldf": "require \"test_helper\"\n\nclass NotesControllerTest < ActionController::TestCase\n  def setup\n    # Stub nominatim response for note locations\n    stub_request(:get, %r{^https://nominatim\\.openstreetmap\\.org/reverse\\?})\n      .to_return(:status => 404)\n  end\n\n  ##\n  # test all routes which lead to this controller\n  def test_routes\n    assert_routing(\n      { :path => \"/api/0.6/notes\", :method => :post },\n      { :controller => \"notes\", :action => \"create\", :format => \"xml\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1\", :method => :get },\n      { :controller => \"notes\", :action => \"show\", :id => \"1\", :format => \"xml\" }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"show\", :id => \"1\", :format => \"xml\" },\n      { :path => \"/api/0.6/notes/1.xml\", :method => :get }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1.rss\", :method => :get },\n      { :controller => \"notes\", :action => \"show\", :id => \"1\", :format => \"rss\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1.json\", :method => :get },\n      { :controller => \"notes\", :action => \"show\", :id => \"1\", :format => \"json\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1.gpx\", :method => :get },\n      { :controller => \"notes\", :action => \"show\", :id => \"1\", :format => \"gpx\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1/comment\", :method => :post },\n      { :controller => \"notes\", :action => \"comment\", :id => \"1\", :format => \"xml\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1/close\", :method => :post },\n      { :controller => \"notes\", :action => \"close\", :id => \"1\", :format => \"xml\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1/reopen\", :method => :post },\n      { :controller => \"notes\", :action => \"reopen\", :id => \"1\", :format => \"xml\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/1\", :method => :delete },\n      { :controller => \"notes\", :action => \"destroy\", :id => \"1\", :format => \"xml\" }\n    )\n\n    assert_routing(\n      { :path => \"/api/0.6/notes\", :method => :get },\n      { :controller => \"notes\", :action => \"index\", :format => \"xml\" }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"index\", :format => \"xml\" },\n      { :path => \"/api/0.6/notes.xml\", :method => :get }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes.rss\", :method => :get },\n      { :controller => \"notes\", :action => \"index\", :format => \"rss\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes.json\", :method => :get },\n      { :controller => \"notes\", :action => \"index\", :format => \"json\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes.gpx\", :method => :get },\n      { :controller => \"notes\", :action => \"index\", :format => \"gpx\" }\n    )\n\n    assert_routing(\n      { :path => \"/api/0.6/notes/search\", :method => :get },\n      { :controller => \"notes\", :action => \"search\", :format => \"xml\" }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"search\", :format => \"xml\" },\n      { :path => \"/api/0.6/notes/search.xml\", :method => :get }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/search.rss\", :method => :get },\n      { :controller => \"notes\", :action => \"search\", :format => \"rss\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/search.json\", :method => :get },\n      { :controller => \"notes\", :action => \"search\", :format => \"json\" }\n    )\n    assert_routing(\n      { :path => \"/api/0.6/notes/search.gpx\", :method => :get },\n      { :controller => \"notes\", :action => \"search\", :format => \"gpx\" }\n    )\n\n    assert_routing(\n      { :path => \"/api/0.6/notes/feed\", :method => :get },\n      { :controller => \"notes\", :action => \"feed\", :format => \"rss\" }\n    )\n\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"create\" },\n      { :path => \"/api/0.6/notes/addPOIexec\", :method => :post }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"close\" },\n      { :path => \"/api/0.6/notes/closePOIexec\", :method => :post }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"comment\" },\n      { :path => \"/api/0.6/notes/editPOIexec\", :method => :post }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"index\", :format => \"gpx\" },\n      { :path => \"/api/0.6/notes/getGPX\", :method => :get }\n    )\n    assert_recognizes(\n      { :controller => \"notes\", :action => \"feed\", :format => \"rss\" },\n      { :path => \"/api/0.6/notes/getRSSfeed\", :method => :get }\n    )\n\n    assert_routing(\n      { :path => \"/user/username/notes\", :method => :get },\n      { :controller => \"notes\", :action => \"mine\", :display_name => \"username\" }\n    )\n  end\n\n  def test_create_success\n    assert_difference \"Note.count\", 1 do\n      assert_difference \"NoteComment.count\", 1 do\n        post :create, :params => { :lat => -1.0, :lon => -1.0, :text => \"This is a comment\", :format => \"json\" }\n      end\n    end\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal \"Point\", js[\"geometry\"][\"type\"]\n    assert_equal [-1.0, -1.0], js[\"geometry\"][\"coordinates\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 1, js[\"properties\"][\"comments\"].count\n    assert_equal \"opened\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n    id = js[\"properties\"][\"id\"]\n\n    get :show, :params => { :id => id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal \"Point\", js[\"geometry\"][\"type\"]\n    assert_equal [-1.0, -1.0], js[\"geometry\"][\"coordinates\"]\n    assert_equal id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 1, js[\"properties\"][\"comments\"].count\n    assert_equal \"opened\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n  end\n\n  def test_create_fail\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lon => -1.0, :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :lon => -1.0 }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :lon => -1.0, :text => \"\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -100.0, :lon => -1.0, :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :lon => -200.0, :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => \"abc\", :lon => -1.0, :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :lon => \"abc\", :text => \"This is a comment\" }\n      end\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"Note.count\" do\n      assert_no_difference \"NoteComment.count\" do\n        post :create, :params => { :lat => -1.0, :lon => -1.0, :text => \"x\\u0000y\" }\n      end\n    end\n    assert_response :bad_request\n  end\n\n  def test_comment_success\n    open_note_with_comment = create(:note_with_comments)\n    assert_difference \"NoteComment.count\", 1 do\n      assert_no_difference \"ActionMailer::Base.deliveries.size\" do\n        post :comment, :params => { :id => open_note_with_comment.id, :text => \"This is an additional comment\", :format => \"json\" }\n      end\n    end\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal open_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n\n    get :show, :params => { :id => open_note_with_comment.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal open_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n\n    # Ensure that emails are sent to users\n    first_user = create(:user)\n    second_user = create(:user)\n    third_user = create(:user)\n\n    note_with_comments_by_users = create(:note) do |note|\n      create(:note_comment, :note => note, :author => first_user)\n      create(:note_comment, :note => note, :author => second_user)\n    end\n    assert_difference \"NoteComment.count\", 1 do\n      assert_difference \"ActionMailer::Base.deliveries.size\", 2 do\n        post :comment, :params => { :id => note_with_comments_by_users.id, :text => \"This is an additional comment\", :format => \"json\" }\n      end\n    end\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal note_with_comments_by_users.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 3, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n\n    email = ActionMailer::Base.deliveries.find { |e| e.to.first == first_user.email }\n    assert_not_nil email\n    assert_equal 1, email.to.length\n    assert_equal \"[OpenStreetMap] An anonymous user has commented on one of your notes\", email.subject\n\n    email = ActionMailer::Base.deliveries.find { |e| e.to.first == second_user.email }\n    assert_not_nil email\n    assert_equal 1, email.to.length\n    assert_equal \"[OpenStreetMap] An anonymous user has commented on a note you are interested in\", email.subject\n\n    get :show, :params => { :id => note_with_comments_by_users.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal note_with_comments_by_users.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 3, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_nil js[\"properties\"][\"comments\"].last[\"user\"]\n\n    ActionMailer::Base.deliveries.clear\n\n    basic_authorization third_user.email, \"test\"\n\n    assert_difference \"NoteComment.count\", 1 do\n      assert_difference \"ActionMailer::Base.deliveries.size\", 2 do\n        post :comment, :params => { :id => note_with_comments_by_users.id, :text => \"This is an additional comment\", :format => \"json\" }\n      end\n    end\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal note_with_comments_by_users.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 4, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal third_user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n\n    email = ActionMailer::Base.deliveries.find { |e| e.to.first == first_user.email }\n    assert_not_nil email\n    assert_equal 1, email.to.length\n    assert_equal \"[OpenStreetMap] #{third_user.display_name} has commented on one of your notes\", email.subject\n    assert_equal first_user.email, email.to.first\n\n    email = ActionMailer::Base.deliveries.find { |e| e.to.first == second_user.email }\n    assert_not_nil email\n    assert_equal 1, email.to.length\n    assert_equal \"[OpenStreetMap] #{third_user.display_name} has commented on a note you are interested in\", email.subject\n\n    get :show, :params => { :id => note_with_comments_by_users.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal note_with_comments_by_users.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 4, js[\"properties\"][\"comments\"].count\n    assert_equal \"commented\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is an additional comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal third_user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n\n    ActionMailer::Base.deliveries.clear\n  end\n\n  def test_comment_fail\n    open_note_with_comment = create(:note_with_comments)\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :text => \"This is an additional comment\" }\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => open_note_with_comment.id }\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => open_note_with_comment.id, :text => \"\" }\n    end\n    assert_response :bad_request\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => 12345, :text => \"This is an additional comment\" }\n    end\n    assert_response :not_found\n\n    hidden_note_with_comment = create(:note_with_comments, :status => \"hidden\")\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => hidden_note_with_comment.id, :text => \"This is an additional comment\" }\n    end\n    assert_response :gone\n\n    closed_note_with_comment = create(:note_with_comments, :status => \"closed\", :closed_at => Time.now)\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => closed_note_with_comment.id, :text => \"This is an additional comment\" }\n    end\n    assert_response :conflict\n\n    assert_no_difference \"NoteComment.count\" do\n      post :comment, :params => { :id => open_note_with_comment.id, :text => \"x\\u0000y\" }\n    end\n    assert_response :bad_request\n  end\n\n  def test_close_success\n    open_note_with_comment = create(:note_with_comments)\n    user = create(:user)\n\n    post :close, :params => { :id => open_note_with_comment.id, :text => \"This is a close comment\", :format => \"json\" }\n    assert_response :unauthorized\n\n    basic_authorization user.email, \"test\"\n\n    post :close, :params => { :id => open_note_with_comment.id, :text => \"This is a close comment\", :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal open_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"closed\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"closed\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a close comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n\n    get :show, :params => { :id => open_note_with_comment.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal open_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"closed\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"closed\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a close comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n  end\n\n  def test_close_fail\n    post :close\n    assert_response :unauthorized\n\n    basic_authorization create(:user).email, \"test\"\n\n    post :close\n    assert_response :bad_request\n\n    post :close, :params => { :id => 12345 }\n    assert_response :not_found\n\n    hidden_note_with_comment = create(:note_with_comments, :status => \"hidden\")\n\n    post :close, :params => { :id => hidden_note_with_comment.id }\n    assert_response :gone\n\n    closed_note_with_comment = create(:note_with_comments, :status => \"closed\", :closed_at => Time.now)\n\n    post :close, :params => { :id => closed_note_with_comment.id }\n    assert_response :conflict\n  end\n\n  def test_reopen_success\n    closed_note_with_comment = create(:note_with_comments, :status => \"closed\", :closed_at => Time.now)\n    user = create(:user)\n\n    post :reopen, :params => { :id => closed_note_with_comment.id, :text => \"This is a reopen comment\", :format => \"json\" }\n    assert_response :unauthorized\n\n    basic_authorization user.email, \"test\"\n\n    post :reopen, :params => { :id => closed_note_with_comment.id, :text => \"This is a reopen comment\", :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal closed_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"reopened\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a reopen comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n\n    get :show, :params => { :id => closed_note_with_comment.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal closed_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"open\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"reopened\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a reopen comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n  end\n\n  def test_reopen_fail\n    hidden_note_with_comment = create(:note_with_comments, :status => \"hidden\")\n\n    post :reopen, :params => { :id => hidden_note_with_comment.id }\n    assert_response :unauthorized\n\n    basic_authorization create(:user).email, \"test\"\n\n    post :reopen, :params => { :id => 12345 }\n    assert_response :not_found\n\n    post :reopen, :params => { :id => hidden_note_with_comment.id }\n    assert_response :gone\n\n    open_note_with_comment = create(:note_with_comments)\n\n    post :reopen, :params => { :id => open_note_with_comment.id }\n    assert_response :conflict\n  end\n\n  def test_show_success\n    open_note = create(:note_with_comments)\n\n    get :show, :params => { :id => open_note.id, :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note[lat='#{open_note.lat}'][lon='#{open_note.lon}']\", :count => 1 do\n        assert_select \"id\", open_note.id.to_s\n        assert_select \"url\", note_url(open_note, :format => \"xml\")\n        assert_select \"comment_url\", comment_note_url(open_note, :format => \"xml\")\n        assert_select \"close_url\", close_note_url(open_note, :format => \"xml\")\n        assert_select \"date_created\", open_note.created_at.to_s\n        assert_select \"status\", open_note.status\n        assert_select \"comments\", :count => 1 do\n          assert_select \"comment\", :count => 1\n        end\n      end\n    end\n\n    get :show, :params => { :id => open_note.id, :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 1 do\n          assert_select \"link\", browse_note_url(open_note)\n          assert_select \"guid\", note_url(open_note)\n          assert_select \"pubDate\", open_note.created_at.to_s(:rfc822)\n          #          assert_select \"geo:lat\", open_note.lat.to_s\n          #          assert_select \"geo:long\", open_note.lon\n          #          assert_select \"georss:point\", \"#{open_note.lon} #{open_note.lon}\"\n        end\n      end\n    end\n\n    get :show, :params => { :id => open_note.id, :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal \"Point\", js[\"geometry\"][\"type\"]\n    assert_equal open_note.lat, js[\"geometry\"][\"coordinates\"][0]\n    assert_equal open_note.lon, js[\"geometry\"][\"coordinates\"][1]\n    assert_equal open_note.id, js[\"properties\"][\"id\"]\n    assert_equal note_url(open_note, :format => \"json\"), js[\"properties\"][\"url\"]\n    assert_equal comment_note_url(open_note, :format => \"json\"), js[\"properties\"][\"comment_url\"]\n    assert_equal close_note_url(open_note, :format => \"json\"), js[\"properties\"][\"close_url\"]\n    assert_equal open_note.created_at.to_s, js[\"properties\"][\"date_created\"]\n    assert_equal open_note.status, js[\"properties\"][\"status\"]\n\n    get :show, :params => { :id => open_note.id, :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt[lat='#{open_note.lat}'][lon='#{open_note.lon}']\", :count => 1 do\n        assert_select \"time\", :count => 1\n        assert_select \"name\", \"Note: #{open_note.id}\"\n        assert_select \"desc\", :count => 1\n        assert_select \"link[href='http://test.host/note/#{open_note.id}']\", :count => 1\n        assert_select \"extensions\", :count => 1 do\n          assert_select \"id\", open_note.id.to_s\n          assert_select \"url\", note_url(open_note, :format => \"gpx\")\n          assert_select \"comment_url\", comment_note_url(open_note, :format => \"gpx\")\n          assert_select \"close_url\", close_note_url(open_note, :format => \"gpx\")\n        end\n      end\n    end\n  end\n\n  def test_show_hidden_comment\n    note_with_hidden_comment = create(:note) do |note|\n      create(:note_comment, :note => note, :body => \"Valid comment for hidden note\")\n      create(:note_comment, :note => note, :visible => false)\n      create(:note_comment, :note => note, :body => \"Another valid comment for hidden note\")\n    end\n\n    get :show, :params => { :id => note_with_hidden_comment.id, :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal note_with_hidden_comment.id, js[\"properties\"][\"id\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"Valid comment for hidden note\", js[\"properties\"][\"comments\"][0][\"text\"]\n    assert_equal \"Another valid comment for hidden note\", js[\"properties\"][\"comments\"][1][\"text\"]\n  end\n\n  def test_show_fail\n    get :show, :params => { :id => 12345 }\n    assert_response :not_found\n\n    get :show, :params => { :id => create(:note, :status => \"hidden\").id }\n    assert_response :gone\n  end\n\n  def test_destroy_success\n    open_note_with_comment = create(:note_with_comments)\n    user = create(:user)\n    moderator_user = create(:moderator_user)\n\n    delete :destroy, :params => { :id => open_note_with_comment.id, :text => \"This is a hide comment\", :format => \"json\" }\n    assert_response :unauthorized\n\n    basic_authorization user.email, \"test\"\n\n    delete :destroy, :params => { :id => open_note_with_comment.id, :text => \"This is a hide comment\", :format => \"json\" }\n    assert_response :forbidden\n\n    basic_authorization moderator_user.email, \"test\"\n\n    delete :destroy, :params => { :id => open_note_with_comment.id, :text => \"This is a hide comment\", :format => \"json\" }\n    assert_response :success\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"Feature\", js[\"type\"]\n    assert_equal open_note_with_comment.id, js[\"properties\"][\"id\"]\n    assert_equal \"hidden\", js[\"properties\"][\"status\"]\n    assert_equal 2, js[\"properties\"][\"comments\"].count\n    assert_equal \"hidden\", js[\"properties\"][\"comments\"].last[\"action\"]\n    assert_equal \"This is a hide comment\", js[\"properties\"][\"comments\"].last[\"text\"]\n    assert_equal moderator_user.display_name, js[\"properties\"][\"comments\"].last[\"user\"]\n\n    get :show, :params => { :id => open_note_with_comment.id, :format => \"json\" }\n    assert_response :success\n\n    basic_authorization user.email, \"test\"\n    get :show, :params => { :id => open_note_with_comment.id, :format => \"json\" }\n    assert_response :gone\n  end\n\n  def test_destroy_fail\n    user = create(:user)\n    moderator_user = create(:moderator_user)\n\n    delete :destroy, :params => { :id => 12345, :format => \"json\" }\n    assert_response :unauthorized\n\n    basic_authorization user.email, \"test\"\n\n    delete :destroy, :params => { :id => 12345, :format => \"json\" }\n    assert_response :forbidden\n\n    basic_authorization moderator_user.email, \"test\"\n\n    delete :destroy, :params => { :id => 12345, :format => \"json\" }\n    assert_response :not_found\n\n    hidden_note_with_comment = create(:note_with_comments, :status => \"hidden\")\n\n    delete :destroy, :params => { :id => hidden_note_with_comment.id, :format => \"json\" }\n    assert_response :gone\n  end\n\n  def test_index_success\n    position = (1.1 * GeoRecord::SCALE).to_i\n    create(:note_with_comments, :latitude => position, :longitude => position)\n    create(:note_with_comments, :latitude => position, :longitude => position)\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 2\n      end\n    end\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 2, js[\"features\"].count\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note\", :count => 2\n    end\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt\", :count => 2\n    end\n  end\n\n  def test_index_limit\n    position = (1.1 * GeoRecord::SCALE).to_i\n    create(:note_with_comments, :latitude => position, :longitude => position)\n    create(:note_with_comments, :latitude => position, :longitude => position)\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :limit => 1, :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 1\n      end\n    end\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :limit => 1, :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 1, js[\"features\"].count\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :limit => 1, :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note\", :count => 1\n    end\n\n    get :index, :params => { :bbox => \"1,1,1.2,1.2\", :limit => 1, :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt\", :count => 1\n    end\n  end\n\n  def test_index_empty_area\n    get :index, :params => { :bbox => \"5,5,5.1,5.1\", :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 0\n      end\n    end\n\n    get :index, :params => { :bbox => \"5,5,5.1,5.1\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 0, js[\"features\"].count\n\n    get :index, :params => { :bbox => \"5,5,5.1,5.1\", :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note\", :count => 0\n    end\n\n    get :index, :params => { :bbox => \"5,5,5.1,5.1\", :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt\", :count => 0\n    end\n  end\n\n  def test_index_large_area\n    get :index, :params => { :bbox => \"-2.5,-2.5,2.5,2.5\", :format => :json }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n\n    get :index, :params => { :l => \"-2.5\", :b => \"-2.5\", :r => \"2.5\", :t => \"2.5\", :format => :json }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n\n    get :index, :params => { :bbox => \"-10,-10,12,12\", :format => :json }\n    assert_response :bad_request\n    assert_equal \"application/json\", @response.content_type\n\n    get :index, :params => { :l => \"-10\", :b => \"-10\", :r => \"12\", :t => \"12\", :format => :json }\n    assert_response :bad_request\n    assert_equal \"application/json\", @response.content_type\n  end\n\n  def test_index_closed\n    create(:note_with_comments, :status => \"closed\", :closed_at => Time.now - 5.days)\n    create(:note_with_comments, :status => \"closed\", :closed_at => Time.now - 100.days)\n    create(:note_with_comments, :status => \"hidden\")\n    create(:note_with_comments)\n\n    # Open notes + closed in last 7 days\n    get :index, :params => { :bbox => \"1,1,1.7,1.7\", :closed => \"7\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 2, js[\"features\"].count\n\n    # Only open notes\n    get :index, :params => { :bbox => \"1,1,1.7,1.7\", :closed => \"0\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 1, js[\"features\"].count\n\n    # Open notes + all closed notes\n    get :index, :params => { :bbox => \"1,1,1.7,1.7\", :closed => \"-1\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 3, js[\"features\"].count\n  end\n\n  def test_index_bad_params\n    get :index, :params => { :bbox => \"-2.5,-2.5,2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :bbox => \"-2.5,-2.5,2.5,2.5,2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :b => \"-2.5\", :r => \"2.5\", :t => \"2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :l => \"-2.5\", :r => \"2.5\", :t => \"2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :l => \"-2.5\", :b => \"-2.5\", :t => \"2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :l => \"-2.5\", :b => \"-2.5\", :r => \"2.5\" }\n    assert_response :bad_request\n\n    get :index, :params => { :bbox => \"1,1,1.7,1.7\", :limit => \"0\", :format => \"json\" }\n    assert_response :bad_request\n\n    get :index, :params => { :bbox => \"1,1,1.7,1.7\", :limit => \"10001\", :format => \"json\" }\n    assert_response :bad_request\n  end\n\n  def test_search_success\n    create(:note_with_comments)\n\n    get :search, :params => { :q => \"note comment\", :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note\", :count => 1\n    end\n\n    get :search, :params => { :q => \"note comment\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 1, js[\"features\"].count\n\n    get :search, :params => { :q => \"note comment\", :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 1\n      end\n    end\n\n    get :search, :params => { :q => \"note comment\", :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt\", :count => 1\n    end\n  end\n\n  def test_search_no_match\n    create(:note_with_comments)\n\n    get :search, :params => { :q => \"no match\", :format => \"xml\" }\n    assert_response :success\n    assert_equal \"application/xml\", @response.content_type\n    assert_select \"osm\", :count => 1 do\n      assert_select \"note\", :count => 0\n    end\n\n    get :search, :params => { :q => \"no match\", :format => \"json\" }\n    assert_response :success\n    assert_equal \"application/json\", @response.content_type\n    js = ActiveSupport::JSON.decode(@response.body)\n    assert_not_nil js\n    assert_equal \"FeatureCollection\", js[\"type\"]\n    assert_equal 0, js[\"features\"].count\n\n    get :search, :params => { :q => \"no match\", :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 0\n      end\n    end\n\n    get :search, :params => { :q => \"no match\", :format => \"gpx\" }\n    assert_response :success\n    assert_equal \"application/gpx+xml\", @response.content_type\n    assert_select \"gpx\", :count => 1 do\n      assert_select \"wpt\", :count => 0\n    end\n  end\n\n  def test_search_bad_params\n    get :search\n    assert_response :bad_request\n\n    get :search, :params => { :q => \"no match\", :limit => \"0\", :format => \"json\" }\n    assert_response :bad_request\n\n    get :search, :params => { :q => \"no match\", :limit => \"10001\", :format => \"json\" }\n    assert_response :bad_request\n  end\n\n  def test_feed_success\n    position = (1.1 * GeoRecord::SCALE).to_i\n    create(:note_with_comments, :latitude => position, :longitude => position)\n    create(:note_with_comments, :latitude => position, :longitude => position)\n    position = (1.5 * GeoRecord::SCALE).to_i\n    create(:note_with_comments, :latitude => position, :longitude => position)\n    create(:note_with_comments, :latitude => position, :longitude => position)\n\n    get :feed, :params => { :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 4\n      end\n    end\n\n    get :feed, :params => { :bbox => \"1,1,1.2,1.2\", :format => \"rss\" }\n    assert_response :success\n    assert_equal \"application/rss+xml\", @response.content_type\n    assert_select \"rss\", :count => 1 do\n      assert_select \"channel\", :count => 1 do\n        assert_select \"item\", :count => 2\n      end\n    end\n  end\n\n  def test_feed_fail\n    get :feed, :params => { :bbox => \"1,1,1.2\", :format => \"rss\" }\n    assert_response :bad_request\n\n    get :feed, :params => { :bbox => \"1,1,1.2,1.2,1.2\", :format => \"rss\" }\n    assert_response :bad_request\n\n    get :feed, :params => { :bbox => \"1,1,1.2,1.2\", :limit => \"0\", :format => \"rss\" }\n    assert_response :bad_request\n\n    get :feed, :params => { :bbox => \"1,1,1.2,1.2\", :limit => \"10001\", :format => \"rss\" }\n    assert_response :bad_request\n  end\n\n  def test_mine_success\n    first_user = create(:user)\n    second_user = create(:user)\n    moderator_user = create(:moderator_user)\n\n    create(:note) do |note|\n      create(:note_comment, :note => note, :author => first_user)\n    end\n    create(:note) do |note|\n      create(:note_comment, :note => note, :author => second_user)\n    end\n    create(:note, :status => \"hidden\") do |note|\n      create(:note_comment, :note => note, :author => second_user)\n    end\n\n    # Note that the table rows include a header row\n    get :mine, :params => { :display_name => first_user.display_name }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 2\n\n    get :mine, :params => { :display_name => second_user.display_name }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 2\n\n    get :mine, :params => { :display_name => \"non-existent\" }\n    assert_response :not_found\n\n    session[:user] = moderator_user.id\n\n    get :mine, :params => { :display_name => first_user.display_name }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 2\n\n    get :mine, :params => { :display_name => second_user.display_name }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 3\n\n    get :mine, :params => { :display_name => \"non-existent\" }\n    assert_response :not_found\n  end\n\n  def test_mine_paged\n    user = create(:user)\n\n    create_list(:note, 50) do |note|\n      create(:note_comment, :note => note, :author => user)\n    end\n\n    get :mine, :params => { :display_name => user.display_name }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 11\n\n    get :mine, :params => { :display_name => user.display_name, :page => 2 }\n    assert_response :success\n    assert_select \"table.note_list tr\", :count => 11\n  end\nend\n", "idx": 1, "id": 11516, "msg": "I think you mean `notes` here ;-)", "proj": "openstreetmap-openstreetmap-website", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -54,6 +54,15 @@ flatbuffers.FILE_IDENTIFIER_LENGTH = 4;\n  */\n flatbuffers.SIZE_PREFIX_LENGTH = 4;\n \n+/**\n+ * @param {number} low\n+ * @param {number} high\n+ * @returns {flatbuffers.Long}\n+ */\n+flatbuffers.createLong = function(low, high) {\n+  return flatbuffers.Long.create(low, high);\n+};\n+\n /**\n  * @enum {number}\n  */", "y": 1, "oldf": "/// @file\n/// @addtogroup flatbuffers_javascript_api\n/// @{\n/// @cond FLATBUFFERS_INTERNAL\n\n/**\n * @fileoverview\n *\n * Need to suppress 'global this' error so the Node.js export line doesn't cause\n * closure compile to error out.\n * @suppress {globalThis}\n */\n\n/**\n * @const\n * @namespace\n */\nvar flatbuffers = {};\n\n/**\n * @typedef {number}\n */\nflatbuffers.Offset;\n\n/**\n * @typedef {{\n *   bb: flatbuffers.ByteBuffer,\n *   bb_pos: number\n * }}\n */\nflatbuffers.Table;\n\n/**\n * @type {number}\n * @const\n */\nflatbuffers.SIZEOF_SHORT = 2;\n\n/**\n * @type {number}\n * @const\n */\nflatbuffers.SIZEOF_INT = 4;\n\n/**\n * @type {number}\n * @const\n */\nflatbuffers.FILE_IDENTIFIER_LENGTH = 4;\n\n/**\n * @type {number}\n * @const\n */\nflatbuffers.SIZE_PREFIX_LENGTH = 4;\n\n/**\n * @enum {number}\n */\nflatbuffers.Encoding = {\n  UTF8_BYTES: 1,\n  UTF16_STRING: 2\n};\n\n/**\n * @type {Int32Array}\n * @const\n */\nflatbuffers.int32 = new Int32Array(2);\n\n/**\n * @type {Float32Array}\n * @const\n */\nflatbuffers.float32 = new Float32Array(flatbuffers.int32.buffer);\n\n/**\n * @type {Float64Array}\n * @const\n */\nflatbuffers.float64 = new Float64Array(flatbuffers.int32.buffer);\n\n/**\n * @type {boolean}\n * @const\n */\nflatbuffers.isLittleEndian = new Uint16Array(new Uint8Array([1, 0]).buffer)[0] === 1;\n\n////////////////////////////////////////////////////////////////////////////////\n\n/**\n * @constructor\n * @param {number} low\n * @param {number} high\n */\nflatbuffers.Long = function(low, high) {\n  /**\n   * @type {number}\n   * @const\n   */\n  this.low = low | 0;\n\n  /**\n   * @type {number}\n   * @const\n   */\n  this.high = high | 0;\n};\n\n/**\n * @param {number} low\n * @param {number} high\n * @returns {flatbuffers.Long}\n */\nflatbuffers.Long.create = function(low, high) {\n  // Special-case zero to avoid GC overhead for default values\n  return low == 0 && high == 0 ? flatbuffers.Long.ZERO : new flatbuffers.Long(low, high);\n};\n\n/**\n * @returns {number}\n */\nflatbuffers.Long.prototype.toFloat64 = function() {\n  return (this.low >>> 0) + this.high * 0x100000000;\n};\n\n/**\n * @param {flatbuffers.Long} other\n * @returns {boolean}\n */\nflatbuffers.Long.prototype.equals = function(other) {\n  return this.low == other.low && this.high == other.high;\n};\n\n/**\n * @type {flatbuffers.Long}\n * @const\n */\nflatbuffers.Long.ZERO = new flatbuffers.Long(0, 0);\n\n/// @endcond\n////////////////////////////////////////////////////////////////////////////////\n/**\n * Create a FlatBufferBuilder.\n *\n * @constructor\n * @param {number=} opt_initial_size\n */\nflatbuffers.Builder = function(opt_initial_size) {\n  if (!opt_initial_size) {\n    var initial_size = 1024;\n  } else {\n    var initial_size = opt_initial_size;\n  }\n\n  /**\n   * @type {flatbuffers.ByteBuffer}\n   * @private\n   */\n  this.bb = flatbuffers.ByteBuffer.allocate(initial_size);\n\n  /**\n   * Remaining space in the ByteBuffer.\n   *\n   * @type {number}\n   * @private\n   */\n  this.space = initial_size;\n\n  /**\n   * Minimum alignment encountered so far.\n   *\n   * @type {number}\n   * @private\n   */\n  this.minalign = 1;\n\n  /**\n   * The vtable for the current table.\n   *\n   * @type {Array.<number>}\n   * @private\n   */\n  this.vtable = null;\n\n  /**\n   * The amount of fields we're actually using.\n   *\n   * @type {number}\n   * @private\n   */\n  this.vtable_in_use = 0;\n\n  /**\n   * Whether we are currently serializing a table.\n   *\n   * @type {boolean}\n   * @private\n   */\n  this.isNested = false;\n\n  /**\n   * Starting offset of the current struct/table.\n   *\n   * @type {number}\n   * @private\n   */\n  this.object_start = 0;\n\n  /**\n   * List of offsets of all vtables.\n   *\n   * @type {Array.<number>}\n   * @private\n   */\n  this.vtables = [];\n\n  /**\n   * For the current vector being built.\n   *\n   * @type {number}\n   * @private\n   */\n  this.vector_num_elems = 0;\n\n  /**\n   * False omits default values from the serialized data\n   *\n   * @type {boolean}\n   * @private\n   */\n  this.force_defaults = false;\n};\n\nflatbuffers.Builder.prototype.clear = function() {\n  this.bb.clear();\n  this.space = this.bb.capacity();\n  this.minalign = 1;\n  this.vtable = null;\n  this.vtable_in_use = 0;\n  this.isNested = false;\n  this.object_start = 0;\n  this.vtables = [];\n  this.vector_num_elems = 0;\n  this.force_defaults = false;\n};\n\n/**\n * In order to save space, fields that are set to their default value\n * don't get serialized into the buffer. Forcing defaults provides a\n * way to manually disable this optimization.\n *\n * @param {boolean} forceDefaults true always serializes default values\n */\nflatbuffers.Builder.prototype.forceDefaults = function(forceDefaults) {\n  this.force_defaults = forceDefaults;\n};\n\n/**\n * Get the ByteBuffer representing the FlatBuffer. Only call this after you've\n * called finish(). The actual data starts at the ByteBuffer's current position,\n * not necessarily at 0.\n *\n * @returns {flatbuffers.ByteBuffer}\n */\nflatbuffers.Builder.prototype.dataBuffer = function() {\n  return this.bb;\n};\n\n/**\n * Get the bytes representing the FlatBuffer. Only call this after you've\n * called finish().\n *\n * @returns {Uint8Array}\n */\nflatbuffers.Builder.prototype.asUint8Array = function() {\n  return this.bb.bytes().subarray(this.bb.position(), this.bb.position() + this.offset());\n};\n\n/// @cond FLATBUFFERS_INTERNAL\n/**\n * Prepare to write an element of `size` after `additional_bytes` have been\n * written, e.g. if you write a string, you need to align such the int length\n * field is aligned to 4 bytes, and the string data follows it directly. If all\n * you need to do is alignment, `additional_bytes` will be 0.\n *\n * @param {number} size This is the of the new element to write\n * @param {number} additional_bytes The padding size\n */\nflatbuffers.Builder.prototype.prep = function(size, additional_bytes) {\n  // Track the biggest thing we've ever aligned to.\n  if (size > this.minalign) {\n    this.minalign = size;\n  }\n\n  // Find the amount of alignment needed such that `size` is properly\n  // aligned after `additional_bytes`\n  var align_size = ((~(this.bb.capacity() - this.space + additional_bytes)) + 1) & (size - 1);\n\n  // Reallocate the buffer if needed.\n  while (this.space < align_size + size + additional_bytes) {\n    var old_buf_size = this.bb.capacity();\n    this.bb = flatbuffers.Builder.growByteBuffer(this.bb);\n    this.space += this.bb.capacity() - old_buf_size;\n  }\n\n  this.pad(align_size);\n};\n\n/**\n * @param {number} byte_size\n */\nflatbuffers.Builder.prototype.pad = function(byte_size) {\n  for (var i = 0; i < byte_size; i++) {\n    this.bb.writeInt8(--this.space, 0);\n  }\n};\n\n/**\n * @param {number} value\n */\nflatbuffers.Builder.prototype.writeInt8 = function(value) {\n  this.bb.writeInt8(this.space -= 1, value);\n};\n\n/**\n * @param {number} value\n */\nflatbuffers.Builder.prototype.writeInt16 = function(value) {\n  this.bb.writeInt16(this.space -= 2, value);\n};\n\n/**\n * @param {number} value\n */\nflatbuffers.Builder.prototype.writeInt32 = function(value) {\n  this.bb.writeInt32(this.space -= 4, value);\n};\n\n/**\n * @param {flatbuffers.Long} value\n */\nflatbuffers.Builder.prototype.writeInt64 = function(value) {\n  this.bb.writeInt64(this.space -= 8, value);\n};\n\n/**\n * @param {number} value\n */\nflatbuffers.Builder.prototype.writeFloat32 = function(value) {\n  this.bb.writeFloat32(this.space -= 4, value);\n};\n\n/**\n * @param {number} value\n */\nflatbuffers.Builder.prototype.writeFloat64 = function(value) {\n  this.bb.writeFloat64(this.space -= 8, value);\n};\n/// @endcond\n\n/**\n * Add an `int8` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {number} value The `int8` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addInt8 = function(value) {\n  this.prep(1, 0);\n  this.writeInt8(value);\n};\n\n/**\n * Add an `int16` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {number} value The `int16` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addInt16 = function(value) {\n  this.prep(2, 0);\n  this.writeInt16(value);\n};\n\n/**\n * Add an `int32` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {number} value The `int32` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addInt32 = function(value) {\n  this.prep(4, 0);\n  this.writeInt32(value);\n};\n\n/**\n * Add an `int64` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {flatbuffers.Long} value The `int64` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addInt64 = function(value) {\n  this.prep(8, 0);\n  this.writeInt64(value);\n};\n\n/**\n * Add a `float32` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {number} value The `float32` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addFloat32 = function(value) {\n  this.prep(4, 0);\n  this.writeFloat32(value);\n};\n\n/**\n * Add a `float64` to the buffer, properly aligned, and grows the buffer (if necessary).\n * @param {number} value The `float64` to add the the buffer.\n */\nflatbuffers.Builder.prototype.addFloat64 = function(value) {\n  this.prep(8, 0);\n  this.writeFloat64(value);\n};\n\n/// @cond FLATBUFFERS_INTERNAL\n/**\n * @param {number} voffset\n * @param {number} value\n * @param {number} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldInt8 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addInt8(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {number} value\n * @param {number} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldInt16 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addInt16(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {number} value\n * @param {number} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldInt32 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addInt32(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {flatbuffers.Long} value\n * @param {flatbuffers.Long} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldInt64 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || !value.equals(defaultValue)) {\n    this.addInt64(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {number} value\n * @param {number} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldFloat32 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addFloat32(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {number} value\n * @param {number} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldFloat64 = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addFloat64(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * @param {number} voffset\n * @param {flatbuffers.Offset} value\n * @param {flatbuffers.Offset} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldOffset = function(voffset, value, defaultValue) {\n  if (this.force_defaults || value != defaultValue) {\n    this.addOffset(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * Structs are stored inline, so nothing additional is being added. `d` is always 0.\n *\n * @param {number} voffset\n * @param {flatbuffers.Offset} value\n * @param {flatbuffers.Offset} defaultValue\n */\nflatbuffers.Builder.prototype.addFieldStruct = function(voffset, value, defaultValue) {\n  if (value != defaultValue) {\n    this.nested(value);\n    this.slot(voffset);\n  }\n};\n\n/**\n * Structures are always stored inline, they need to be created right\n * where they're used.  You'll get this assertion failure if you\n * created it elsewhere.\n *\n * @param {flatbuffers.Offset} obj The offset of the created object\n */\nflatbuffers.Builder.prototype.nested = function(obj) {\n  if (obj != this.offset()) {\n    throw new Error('FlatBuffers: struct must be serialized inline.');\n  }\n};\n\n/**\n * Should not be creating any other object, string or vector\n * while an object is being constructed\n */\nflatbuffers.Builder.prototype.notNested = function() {\n  if (this.isNested) {\n    throw new Error('FlatBuffers: object serialization must not be nested.');\n  }\n};\n\n/**\n * Set the current vtable at `voffset` to the current location in the buffer.\n *\n * @param {number} voffset\n */\nflatbuffers.Builder.prototype.slot = function(voffset) {\n  this.vtable[voffset] = this.offset();\n};\n\n/**\n * @returns {flatbuffers.Offset} Offset relative to the end of the buffer.\n */\nflatbuffers.Builder.prototype.offset = function() {\n  return this.bb.capacity() - this.space;\n};\n\n/**\n * Doubles the size of the backing ByteBuffer and copies the old data towards\n * the end of the new buffer (since we build the buffer backwards).\n *\n * @param {flatbuffers.ByteBuffer} bb The current buffer with the existing data\n * @returns {flatbuffers.ByteBuffer} A new byte buffer with the old data copied\n * to it. The data is located at the end of the buffer.\n *\n * uint8Array.set() formally takes {Array<number>|ArrayBufferView}, so to pass\n * it a uint8Array we need to suppress the type check:\n * @suppress {checkTypes}\n */\nflatbuffers.Builder.growByteBuffer = function(bb) {\n  var old_buf_size = bb.capacity();\n\n  // Ensure we don't grow beyond what fits in an int.\n  if (old_buf_size & 0xC0000000) {\n    throw new Error('FlatBuffers: cannot grow buffer beyond 2 gigabytes.');\n  }\n\n  var new_buf_size = old_buf_size << 1;\n  var nbb = flatbuffers.ByteBuffer.allocate(new_buf_size);\n  nbb.setPosition(new_buf_size - old_buf_size);\n  nbb.bytes().set(bb.bytes(), new_buf_size - old_buf_size);\n  return nbb;\n};\n/// @endcond\n\n/**\n * Adds on offset, relative to where it will be written.\n *\n * @param {flatbuffers.Offset} offset The offset to add.\n */\nflatbuffers.Builder.prototype.addOffset = function(offset) {\n  this.prep(flatbuffers.SIZEOF_INT, 0); // Ensure alignment is already done.\n  this.writeInt32(this.offset() - offset + flatbuffers.SIZEOF_INT);\n};\n\n/// @cond FLATBUFFERS_INTERNAL\n/**\n * Start encoding a new object in the buffer.  Users will not usually need to\n * call this directly. The FlatBuffers compiler will generate helper methods\n * that call this method internally.\n *\n * @param {number} numfields\n */\nflatbuffers.Builder.prototype.startObject = function(numfields) {\n  this.notNested();\n  if (this.vtable == null) {\n    this.vtable = [];\n  }\n  this.vtable_in_use = numfields;\n  for (var i = 0; i < numfields; i++) {\n    this.vtable[i] = 0; // This will push additional elements as needed\n  }\n  this.isNested = true;\n  this.object_start = this.offset();\n};\n\n/**\n * Finish off writing the object that is under construction.\n *\n * @returns {flatbuffers.Offset} The offset to the object inside `dataBuffer`\n */\nflatbuffers.Builder.prototype.endObject = function() {\n  if (this.vtable == null || !this.isNested) {\n    throw new Error('FlatBuffers: endObject called without startObject');\n  }\n\n  this.addInt32(0);\n  var vtableloc = this.offset();\n\n  // Trim trailing zeroes.\n  var i = this.vtable_in_use - 1;\n  for (; i >= 0 && this.vtable[i] == 0; i--) {}\n  var trimmed_size = i + 1;\n\n  // Write out the current vtable.\n  for (; i >= 0; i--) {\n    // Offset relative to the start of the table.\n    this.addInt16(this.vtable[i] != 0 ? vtableloc - this.vtable[i] : 0);\n  }\n\n  var standard_fields = 2; // The fields below:\n  this.addInt16(vtableloc - this.object_start);\n  var len = (trimmed_size + standard_fields) * flatbuffers.SIZEOF_SHORT;\n  this.addInt16(len);\n\n  // Search for an existing vtable that matches the current one.\n  var existing_vtable = 0;\n  var vt1 = this.space;\nouter_loop:\n  for (i = 0; i < this.vtables.length; i++) {\n    var vt2 = this.bb.capacity() - this.vtables[i];\n    if (len == this.bb.readInt16(vt2)) {\n      for (var j = flatbuffers.SIZEOF_SHORT; j < len; j += flatbuffers.SIZEOF_SHORT) {\n        if (this.bb.readInt16(vt1 + j) != this.bb.readInt16(vt2 + j)) {\n          continue outer_loop;\n        }\n      }\n      existing_vtable = this.vtables[i];\n      break;\n    }\n  }\n\n  if (existing_vtable) {\n    // Found a match:\n    // Remove the current vtable.\n    this.space = this.bb.capacity() - vtableloc;\n\n    // Point table to existing vtable.\n    this.bb.writeInt32(this.space, existing_vtable - vtableloc);\n  } else {\n    // No match:\n    // Add the location of the current vtable to the list of vtables.\n    this.vtables.push(this.offset());\n\n    // Point table to current vtable.\n    this.bb.writeInt32(this.bb.capacity() - vtableloc, this.offset() - vtableloc);\n  }\n\n  this.isNested = false;\n  return vtableloc;\n};\n/// @endcond\n\n/**\n * Finalize a buffer, poiting to the given `root_table`.\n *\n * @param {flatbuffers.Offset} root_table\n * @param {string=} opt_file_identifier\n * @param {boolean=} opt_size_prefix\n */\nflatbuffers.Builder.prototype.finish = function(root_table, opt_file_identifier, opt_size_prefix) {\n  var size_prefix = opt_size_prefix ? flatbuffers.SIZE_PREFIX_LENGTH : 0;\n  if (opt_file_identifier) {\n    var file_identifier = opt_file_identifier;\n    this.prep(this.minalign, flatbuffers.SIZEOF_INT +\n      flatbuffers.FILE_IDENTIFIER_LENGTH + size_prefix);\n    if (file_identifier.length != flatbuffers.FILE_IDENTIFIER_LENGTH) {\n      throw new Error('FlatBuffers: file identifier must be length ' +\n        flatbuffers.FILE_IDENTIFIER_LENGTH);\n    }\n    for (var i = flatbuffers.FILE_IDENTIFIER_LENGTH - 1; i >= 0; i--) {\n      this.writeInt8(file_identifier.charCodeAt(i));\n    }\n  }\n  this.prep(this.minalign, flatbuffers.SIZEOF_INT + size_prefix);\n  this.addOffset(root_table);\n  if (size_prefix) {\n    this.addInt32(this.bb.capacity() - this.space);\n  }\n  this.bb.setPosition(this.space);\n};\n\n/**\n * Finalize a size prefixed buffer, pointing to the given `root_table`.\n *\n * @param {flatbuffers.Offset} root_table\n * @param {string=} opt_file_identifier\n */\nflatbuffers.Builder.prototype.finishSizePrefixed = function (root_table, opt_file_identifier) {\n  this.finish(root_table, opt_file_identifier, true);\n};\n\n/// @cond FLATBUFFERS_INTERNAL\n/**\n * This checks a required field has been set in a given table that has\n * just been constructed.\n *\n * @param {flatbuffers.Offset} table\n * @param {number} field\n */\nflatbuffers.Builder.prototype.requiredField = function(table, field) {\n  var table_start = this.bb.capacity() - table;\n  var vtable_start = table_start - this.bb.readInt32(table_start);\n  var ok = this.bb.readInt16(vtable_start + field) != 0;\n\n  // If this fails, the caller will show what field needs to be set.\n  if (!ok) {\n    throw new Error('FlatBuffers: field ' + field + ' must be set');\n  }\n};\n\n/**\n * Start a new array/vector of objects.  Users usually will not call\n * this directly. The FlatBuffers compiler will create a start/end\n * method for vector types in generated code.\n *\n * @param {number} elem_size The size of each element in the array\n * @param {number} num_elems The number of elements in the array\n * @param {number} alignment The alignment of the array\n */\nflatbuffers.Builder.prototype.startVector = function(elem_size, num_elems, alignment) {\n  this.notNested();\n  this.vector_num_elems = num_elems;\n  this.prep(flatbuffers.SIZEOF_INT, elem_size * num_elems);\n  this.prep(alignment, elem_size * num_elems); // Just in case alignment > int.\n};\n\n/**\n * Finish off the creation of an array and all its elements. The array must be\n * created with `startVector`.\n *\n * @returns {flatbuffers.Offset} The offset at which the newly created array\n * starts.\n */\nflatbuffers.Builder.prototype.endVector = function() {\n  this.writeInt32(this.vector_num_elems);\n  return this.offset();\n};\n/// @endcond\n\n/**\n * Encode the string `s` in the buffer using UTF-8. If a Uint8Array is passed\n * instead of a string, it is assumed to contain valid UTF-8 encoded data.\n *\n * @param {string|Uint8Array} s The string to encode\n * @return {flatbuffers.Offset} The offset in the buffer where the encoded string starts\n */\nflatbuffers.Builder.prototype.createString = function(s) {\n  if (s instanceof Uint8Array) {\n    var utf8 = s;\n  } else {\n    var utf8 = [];\n    var i = 0;\n\n    while (i < s.length) {\n      var codePoint;\n\n      // Decode UTF-16\n      var a = s.charCodeAt(i++);\n      if (a < 0xD800 || a >= 0xDC00) {\n        codePoint = a;\n      } else {\n        var b = s.charCodeAt(i++);\n        codePoint = (a << 10) + b + (0x10000 - (0xD800 << 10) - 0xDC00);\n      }\n\n      // Encode UTF-8\n      if (codePoint < 0x80) {\n        utf8.push(codePoint);\n      } else {\n        if (codePoint < 0x800) {\n          utf8.push(((codePoint >> 6) & 0x1F) | 0xC0);\n        } else {\n          if (codePoint < 0x10000) {\n            utf8.push(((codePoint >> 12) & 0x0F) | 0xE0);\n          } else {\n            utf8.push(\n              ((codePoint >> 18) & 0x07) | 0xF0,\n              ((codePoint >> 12) & 0x3F) | 0x80);\n          }\n          utf8.push(((codePoint >> 6) & 0x3F) | 0x80);\n        }\n        utf8.push((codePoint & 0x3F) | 0x80);\n      }\n    }\n  }\n\n  this.addInt8(0);\n  this.startVector(1, utf8.length, 1);\n  this.bb.setPosition(this.space -= utf8.length);\n  for (var i = 0, offset = this.space, bytes = this.bb.bytes(); i < utf8.length; i++) {\n    bytes[offset++] = utf8[i];\n  }\n  return this.endVector();\n};\n\n/**\n * A helper function to avoid generated code depending on this file directly.\n *\n * @param {number} low\n * @param {number} high\n * @returns {flatbuffers.Long}\n */\nflatbuffers.Builder.prototype.createLong = function(low, high) {\n  return flatbuffers.Long.create(low, high);\n};\n////////////////////////////////////////////////////////////////////////////////\n/// @cond FLATBUFFERS_INTERNAL\n/**\n * Create a new ByteBuffer with a given array of bytes (`Uint8Array`).\n *\n * @constructor\n * @param {Uint8Array} bytes\n */\nflatbuffers.ByteBuffer = function(bytes) {\n  /**\n   * @type {Uint8Array}\n   * @private\n   */\n  this.bytes_ = bytes;\n\n  /**\n   * @type {number}\n   * @private\n   */\n  this.position_ = 0;\n};\n\n/**\n * Create and allocate a new ByteBuffer with a given size.\n *\n * @param {number} byte_size\n * @returns {flatbuffers.ByteBuffer}\n */\nflatbuffers.ByteBuffer.allocate = function(byte_size) {\n  return new flatbuffers.ByteBuffer(new Uint8Array(byte_size));\n};\n\nflatbuffers.ByteBuffer.prototype.clear = function() {\n  this.position_ = 0;\n};\n\n/**\n * Get the underlying `Uint8Array`.\n *\n * @returns {Uint8Array}\n */\nflatbuffers.ByteBuffer.prototype.bytes = function() {\n  return this.bytes_;\n};\n\n/**\n * Get the buffer's position.\n *\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.position = function() {\n  return this.position_;\n};\n\n/**\n * Set the buffer's position.\n *\n * @param {number} position\n */\nflatbuffers.ByteBuffer.prototype.setPosition = function(position) {\n  this.position_ = position;\n};\n\n/**\n * Get the buffer's capacity.\n *\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.capacity = function() {\n  return this.bytes_.length;\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readInt8 = function(offset) {\n  return this.readUint8(offset) << 24 >> 24;\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readUint8 = function(offset) {\n  return this.bytes_[offset];\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readInt16 = function(offset) {\n  return this.readUint16(offset) << 16 >> 16;\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readUint16 = function(offset) {\n  return this.bytes_[offset] | this.bytes_[offset + 1] << 8;\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readInt32 = function(offset) {\n  return this.bytes_[offset] | this.bytes_[offset + 1] << 8 | this.bytes_[offset + 2] << 16 | this.bytes_[offset + 3] << 24;\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readUint32 = function(offset) {\n  return this.readInt32(offset) >>> 0;\n};\n\n/**\n * @param {number} offset\n * @returns {flatbuffers.Long}\n */\nflatbuffers.ByteBuffer.prototype.readInt64 = function(offset) {\n  return new flatbuffers.Long(this.readInt32(offset), this.readInt32(offset + 4));\n};\n\n/**\n * @param {number} offset\n * @returns {flatbuffers.Long}\n */\nflatbuffers.ByteBuffer.prototype.readUint64 = function(offset) {\n  return new flatbuffers.Long(this.readUint32(offset), this.readUint32(offset + 4));\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readFloat32 = function(offset) {\n  flatbuffers.int32[0] = this.readInt32(offset);\n  return flatbuffers.float32[0];\n};\n\n/**\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.readFloat64 = function(offset) {\n  flatbuffers.int32[flatbuffers.isLittleEndian ? 0 : 1] = this.readInt32(offset);\n  flatbuffers.int32[flatbuffers.isLittleEndian ? 1 : 0] = this.readInt32(offset + 4);\n  return flatbuffers.float64[0];\n};\n\n/**\n * @param {number} offset\n * @param {number|boolean} value\n */\nflatbuffers.ByteBuffer.prototype.writeInt8 = function(offset, value) {\n  this.bytes_[offset] = /** @type {number} */(value);\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeUint8 = function(offset, value) {\n  this.bytes_[offset] = value;\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeInt16 = function(offset, value) {\n  this.bytes_[offset] = value;\n  this.bytes_[offset + 1] = value >> 8;\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeUint16 = function(offset, value) {\n    this.bytes_[offset] = value;\n    this.bytes_[offset + 1] = value >> 8;\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeInt32 = function(offset, value) {\n  this.bytes_[offset] = value;\n  this.bytes_[offset + 1] = value >> 8;\n  this.bytes_[offset + 2] = value >> 16;\n  this.bytes_[offset + 3] = value >> 24;\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeUint32 = function(offset, value) {\n    this.bytes_[offset] = value;\n    this.bytes_[offset + 1] = value >> 8;\n    this.bytes_[offset + 2] = value >> 16;\n    this.bytes_[offset + 3] = value >> 24;\n};\n\n/**\n * @param {number} offset\n * @param {flatbuffers.Long} value\n */\nflatbuffers.ByteBuffer.prototype.writeInt64 = function(offset, value) {\n  this.writeInt32(offset, value.low);\n  this.writeInt32(offset + 4, value.high);\n};\n\n/**\n * @param {number} offset\n * @param {flatbuffers.Long} value\n */\nflatbuffers.ByteBuffer.prototype.writeUint64 = function(offset, value) {\n    this.writeUint32(offset, value.low);\n    this.writeUint32(offset + 4, value.high);\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeFloat32 = function(offset, value) {\n  flatbuffers.float32[0] = value;\n  this.writeInt32(offset, flatbuffers.int32[0]);\n};\n\n/**\n * @param {number} offset\n * @param {number} value\n */\nflatbuffers.ByteBuffer.prototype.writeFloat64 = function(offset, value) {\n  flatbuffers.float64[0] = value;\n  this.writeInt32(offset, flatbuffers.int32[flatbuffers.isLittleEndian ? 0 : 1]);\n  this.writeInt32(offset + 4, flatbuffers.int32[flatbuffers.isLittleEndian ? 1 : 0]);\n};\n\n/**\n * Return the file identifier.   Behavior is undefined for FlatBuffers whose\n * schema does not include a file_identifier (likely points at padding or the\n * start of a the root vtable).\n * @returns {string}\n */\nflatbuffers.ByteBuffer.prototype.getBufferIdentifier = function() {\n  if (this.bytes_.length < this.position_ + flatbuffers.SIZEOF_INT +\n      flatbuffers.FILE_IDENTIFIER_LENGTH) {\n    throw new Error(\n        'FlatBuffers: ByteBuffer is too short to contain an identifier.');\n  }\n  var result = \"\";\n  for (var i = 0; i < flatbuffers.FILE_IDENTIFIER_LENGTH; i++) {\n    result += String.fromCharCode(\n        this.readInt8(this.position_ + flatbuffers.SIZEOF_INT + i));\n  }\n  return result;\n};\n\n/**\n * Look up a field in the vtable, return an offset into the object, or 0 if the\n * field is not present.\n *\n * @param {number} bb_pos\n * @param {number} vtable_offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.__offset = function(bb_pos, vtable_offset) {\n  var vtable = bb_pos - this.readInt32(bb_pos);\n  return vtable_offset < this.readInt16(vtable) ? this.readInt16(vtable + vtable_offset) : 0;\n};\n\n/**\n * Initialize any Table-derived type to point to the union at the given offset.\n *\n * @param {flatbuffers.Table} t\n * @param {number} offset\n * @returns {flatbuffers.Table}\n */\nflatbuffers.ByteBuffer.prototype.__union = function(t, offset) {\n  t.bb_pos = offset + this.readInt32(offset);\n  t.bb = this;\n  return t;\n};\n\n/**\n * Create a JavaScript string from UTF-8 data stored inside the FlatBuffer.\n * This allocates a new string and converts to wide chars upon each access.\n *\n * To avoid the conversion to UTF-16, pass flatbuffers.Encoding.UTF8_BYTES as\n * the \"optionalEncoding\" argument. This is useful for avoiding conversion to\n * and from UTF-16 when the data will just be packaged back up in another\n * FlatBuffer later on.\n *\n * @param {number} offset\n * @param {flatbuffers.Encoding=} opt_encoding Defaults to UTF16_STRING\n * @returns {string|Uint8Array}\n */\nflatbuffers.ByteBuffer.prototype.__string = function(offset, opt_encoding) {\n  offset += this.readInt32(offset);\n\n  var length = this.readInt32(offset);\n  var result = '';\n  var i = 0;\n\n  offset += flatbuffers.SIZEOF_INT;\n\n  if (opt_encoding === flatbuffers.Encoding.UTF8_BYTES) {\n    return this.bytes_.subarray(offset, offset + length);\n  }\n\n  while (i < length) {\n    var codePoint;\n\n    // Decode UTF-8\n    var a = this.readUint8(offset + i++);\n    if (a < 0xC0) {\n      codePoint = a;\n    } else {\n      var b = this.readUint8(offset + i++);\n      if (a < 0xE0) {\n        codePoint =\n          ((a & 0x1F) << 6) |\n          (b & 0x3F);\n      } else {\n        var c = this.readUint8(offset + i++);\n        if (a < 0xF0) {\n          codePoint =\n            ((a & 0x0F) << 12) |\n            ((b & 0x3F) << 6) |\n            (c & 0x3F);\n        } else {\n          var d = this.readUint8(offset + i++);\n          codePoint =\n            ((a & 0x07) << 18) |\n            ((b & 0x3F) << 12) |\n            ((c & 0x3F) << 6) |\n            (d & 0x3F);\n        }\n      }\n    }\n\n    // Encode UTF-16\n    if (codePoint < 0x10000) {\n      result += String.fromCharCode(codePoint);\n    } else {\n      codePoint -= 0x10000;\n      result += String.fromCharCode(\n        (codePoint >> 10) + 0xD800,\n        (codePoint & ((1 << 10) - 1)) + 0xDC00);\n    }\n  }\n\n  return result;\n};\n\n/**\n * Retrieve the relative offset stored at \"offset\"\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.__indirect = function(offset) {\n  return offset + this.readInt32(offset);\n};\n\n/**\n * Get the start of data of a vector whose offset is stored at \"offset\" in this object.\n *\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.__vector = function(offset) {\n  return offset + this.readInt32(offset) + flatbuffers.SIZEOF_INT; // data starts after the length\n};\n\n/**\n * Get the length of a vector whose offset is stored at \"offset\" in this object.\n *\n * @param {number} offset\n * @returns {number}\n */\nflatbuffers.ByteBuffer.prototype.__vector_len = function(offset) {\n  return this.readInt32(offset + this.readInt32(offset));\n};\n\n/**\n * @param {string} ident\n * @returns {boolean}\n */\nflatbuffers.ByteBuffer.prototype.__has_identifier = function(ident) {\n  if (ident.length != flatbuffers.FILE_IDENTIFIER_LENGTH) {\n    throw new Error('FlatBuffers: file identifier must be length ' +\n                    flatbuffers.FILE_IDENTIFIER_LENGTH);\n  }\n  for (var i = 0; i < flatbuffers.FILE_IDENTIFIER_LENGTH; i++) {\n    if (ident.charCodeAt(i) != this.readInt8(this.position_ + flatbuffers.SIZEOF_INT + i)) {\n      return false;\n    }\n  }\n  return true;\n};\n\n/**\n * A helper function to avoid generated code depending on this file directly.\n *\n * @param {number} low\n * @param {number} high\n * @returns {flatbuffers.Long}\n */\nflatbuffers.ByteBuffer.prototype.createLong = function(low, high) {\n  return flatbuffers.Long.create(low, high);\n};\n\n// Exports for Node.js and RequireJS\nthis.flatbuffers = flatbuffers;\n\n/// @endcond\n/// @}\n", "idx": 1, "id": 17894, "msg": "Why do we need this function? It appears to save a `.`", "proj": "google-flatbuffers", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -55,6 +55,7 @@ return [\n         ]\n     ],\n     'theme' => [\n+        'label' => 'Theme',\n         'unnamed' => 'Unnamed theme',\n         'name' => [\n             'label' => 'Theme Name',", "y": 0, "oldf": "<?php\n\nreturn [\n    'app' => [\n        'name' => 'October CMS',\n        'tagline' => 'Getting back to basics'\n    ],\n    'locale' => [\n        'en' => 'English',\n        'de' => 'German',\n        'es' => 'Spanish',\n        'es-ar' => 'Spanish (Argentina)',\n        'fa' => 'Persian',\n        'fr' => 'French',\n        'hu' => 'Hungarian',\n        'id' => 'Bahasa Indonesia',\n        'it' => 'Italian',\n        'ja' => 'Japanese',\n        'nb-no' => 'Norwegian (Bokm\u00e5l)',\n        'nl' => 'Dutch',\n        'pl' => 'Polish',\n        'pt-br' => 'Portuguese (Brazil)',\n        'ro' => 'Romanian',\n        'ru' => 'Russian',\n        'se' => 'Swedish',\n        'sk' => 'Slovak (Slovakia)',\n        'tr' => 'Turkish',\n        'zh-cn' => 'Chinese (China)'\n    ],\n    'directory' => [\n        'create_fail' => 'Cannot create directory: :name'\n    ],\n    'file' => [\n        'create_fail' => 'Cannot create file: :name'\n    ],\n    'combiner' => [\n        'not_found' => \"The combiner file ':name' is not found.\"\n    ],\n    'system' => [\n        'name' => 'System',\n        'menu_label' => 'System',\n        'categories' => [\n            'cms' => 'CMS',\n            'misc' => 'Misc',\n            'logs' => 'Logs',\n            'mail' => 'Mail',\n            'shop' => 'Shop',\n            'team' => 'Team',\n            'users' => 'Users',\n            'system' => 'System',\n            'social' => 'Social',\n            'events' => 'Events',\n            'customers' => 'Customers',\n            'my_settings' => 'My Settings'\n        ]\n    ],\n    'theme' => [\n        'unnamed' => 'Unnamed theme',\n        'name' => [\n            'label' => 'Theme Name',\n            'help' => 'Name the theme by its unique code. For example, RainLab.Vanilla'\n        ],\n    ],\n    'themes' => [\n        'install' => 'Install themes',\n        'search' => 'search themes to install...',\n        'installed' => 'Installed themes',\n        'no_themes' => 'There are no themes installed from the marketplace.',\n        'recommended' => 'Recommended',\n        'remove_confirm' => 'Are you sure you want to remove this theme?'\n    ],\n    'plugin' => [\n        'unnamed' => 'Unnamed plugin',\n        'name' => [\n            'label' => 'Plugin Name',\n            'help' => 'Name the plugin by its unique code. For example, RainLab.Blog'\n        ]\n    ],\n    'plugins' => [\n        'manage' => 'Manage plugins',\n        'enable_or_disable' => 'Enable or disable',\n        'enable_or_disable_title' => 'Enable or Disable Plugins',\n        'install' => 'Install plugins',\n        'install_products' => 'Install products',\n        'search' => 'search plugins to install...',\n        'installed' => 'Installed plugins',\n        'no_plugins' => 'There are no plugins installed from the marketplace.',\n        'recommended' => 'Recommended',\n        'remove' => 'Remove',\n        'refresh' => 'Refresh',\n        'disabled_label' => 'Disabled',\n        'disabled_help' => 'Plugins that are disabled are ignored by the application.',\n        'selected_amount' => 'Plugins selected: :amount',\n        'remove_confirm' => 'Are you sure you want to remove this plugin?',\n        'remove_success' => 'Successfully removed those plugins from the system.',\n        'refresh_confirm' => 'Are you sure?',\n        'refresh_success' => 'Successfully refreshed those plugins in the system.',\n        'disable_confirm' => 'Are you sure?',\n        'disable_success' => 'Successfully disabled those plugins.',\n        'enable_success' => 'Successfully enabled those plugins.',\n        'unknown_plugin' => 'Plugin has been removed from the file system.'\n    ],\n    'project' => [\n        'name' => 'Project',\n        'owner_label' => 'Owner',\n        'attach' => 'Attach Project',\n        'detach' => 'Detach Project',\n        'none' => 'None',\n        'id' => [\n            'label' => 'Project ID',\n            'help' => 'How to find your Project ID',\n            'missing' => 'Please specify a Project ID to use.'\n        ],\n        'detach_confirm' => 'Are you sure you want to detach this project?',\n        'unbind_success' => 'Project has been detached successfully.'\n    ],\n    'settings' => [\n        'menu_label' => 'Settings',\n        'not_found' => 'Unable to find the specified settings.',\n        'missing_model' => 'The settings page is missing a Model definition.',\n        'update_success' => 'Settings for :name have been updated successfully.',\n        'return' => 'Return to system settings',\n        'search' => 'Search'\n    ],\n    'mail' => [\n        'log_file' => 'Log file',\n        'menu_label' => 'Mail configuration',\n        'menu_description' => 'Manage email configuration.',\n        'general' => 'General',\n        'method' => 'Mail Method',\n        'sender_name' => 'Sender Name',\n        'sender_email' => 'Sender Email',\n        'php_mail' => 'PHP mail',\n        'smtp' => 'SMTP',\n        'smtp_address' => 'SMTP Address',\n        'smtp_authorization' => 'SMTP authorization required',\n        'smtp_authorization_comment' => 'Use this checkbox if your SMTP server requires authorization.',\n        'smtp_username' => 'Username',\n        'smtp_password' => 'Password',\n        'smtp_port' => 'SMTP Port',\n        'smtp_ssl' => 'SSL connection required',\n        'sendmail' => 'Sendmail',\n        'sendmail_path' => 'Sendmail Path',\n        'sendmail_path_comment' => 'Please specify the path of the sendmail program.',\n        'mailgun' => 'Mailgun',\n        'mailgun_domain' => 'Mailgun Domain',\n        'mailgun_domain_comment' => 'Please specify the Mailgun domain name.',\n        'mailgun_secret' => 'Mailgun Secret',\n        'mailgun_secret_comment' => 'Enter your Mailgun API key.',\n        'mandrill' => 'Mandrill',\n        'mandrill_secret' => 'Mandrill Secret',\n        'mandrill_secret_comment' => 'Enter your Mandrill API key.'\n    ],\n    'mail_templates' => [\n        'menu_label' => 'Mail templates',\n        'menu_description' => 'Modify the mail templates that are sent to users and administrators, manage email layouts.',\n        'new_template' => 'New Template',\n        'new_layout' => 'New Layout',\n        'template' => 'Template',\n        'templates' => 'Templates',\n        'menu_layouts_label' => 'Mail Layouts',\n        'layout' => 'Layout',\n        'layouts' => 'Layouts',\n        'name' => 'Name',\n        'name_comment' => 'Unique name used to refer to this template',\n        'code' => 'Code',\n        'code_comment' => 'Unique code used to refer to this template',\n        'subject' => 'Subject',\n        'subject_comment' => 'Email message subject',\n        'description' => 'Description',\n        'content_html' => 'HTML',\n        'content_css' => 'CSS',\n        'content_text' => 'Plaintext',\n        'test_send' => 'Send test message',\n        'test_success' => 'The test message has been successfully sent.',\n        'return' => 'Return to template list'\n    ],\n    'install' => [\n        'project_label' => 'Attach to Project',\n        'plugin_label' => 'Install Plugin',\n        'theme_label' => 'Install Theme',\n        'missing_plugin_name' => 'Please specify a Plugin name to install.',\n        'missing_theme_name' => 'Please specify a Theme name to install.',\n        'install_completing' => 'Finishing installation process',\n        'install_success' => 'The plugin has been installed successfully.'\n    ],\n    'updates' => [\n        'title' => 'Manage Updates',\n        'name' => 'Software update',\n        'menu_label' => 'Updates',\n        'menu_description' => 'Update the system, manage and install plugins and themes.',\n        'check_label' => 'Check for updates',\n        'retry_label' => 'Try again',\n        'plugin_name' => 'Name',\n        'plugin_description' => 'Description',\n        'plugin_version' => 'Version',\n        'plugin_author' => 'Author',\n        'core_build' => 'Current build',\n        'core_build_old' => 'Current build :build',\n        'core_build_new' => 'Build :build',\n        'core_build_new_help' => 'Latest build is available.',\n        'core_downloading' => 'Downloading application files',\n        'core_extracting' => 'Unpacking application files',\n        'plugins' => 'Plugins',\n        'themes' => 'Themes',\n        'disabled' => 'Disabled',\n        'plugin_downloading' => 'Downloading plugin: :name',\n        'plugin_extracting' => 'Unpacking plugin: :name',\n        'plugin_version_none' => 'New plugin',\n        'plugin_version_old' => 'Current v:version',\n        'plugin_version_new' => 'v:version',\n        'theme_label' => 'Theme',\n        'theme_new_install' => 'New theme installation.',\n        'theme_downloading' => 'Downloading theme: :name',\n        'theme_extracting' => 'Unpacking theme: :name',\n        'update_label' => 'Update software',\n        'update_completing' => 'Finishing update process',\n        'update_loading' => 'Loading available updates...',\n        'update_success' => 'The update process was performed successfully.',\n        'update_failed_label' => 'Update failed',\n        'force_label' => 'Force update',\n        'found' => [\n            'label' => 'Found new updates!',\n            'help' => 'Click Update software to begin the update process.'\n        ],\n        'none' => [\n            'label' => 'No updates',\n            'help' => 'No new updates were found.'\n        ]\n    ],\n    'server' => [\n        'connect_error' => 'Error connecting to the server.',\n        'response_not_found' => 'The update server could not be found.',\n        'response_invalid' => 'Invalid response from the server.',\n        'response_empty' => 'Empty response from the server.',\n        'file_error' => 'Server failed to deliver the package.',\n        'file_corrupt' => 'File from server is corrupt.'\n    ],\n    'behavior' => [\n        'missing_property' => 'Class :class must define property $:property used by :behavior behavior.'\n    ],\n    'config' => [\n        'not_found' => 'Unable to find configuration file :file defined for :location.',\n        'required' => \"Configuration used in :location must supply a value ':property'.\"\n    ],\n    'zip' => [\n        'extract_failed' => \"Unable to extract core file ':file'.\"\n    ],\n    'event_log' => [\n        'hint' => 'This log displays a list of potential errors that occur in the application, such as exceptions and debugging information.',\n        'menu_label' => 'Event log',\n        'menu_description' => 'View system log messages with their recorded time and details.',\n        'empty_link' => 'Empty event log',\n        'empty_loading' => 'Emptying event log...',\n        'empty_success' => 'Successfully emptied the event log.',\n        'return_link' => 'Return to event log',\n        'id' => 'ID',\n        'id_label' => 'Event ID',\n        'created_at' => 'Date & Time',\n        'message' => 'Message',\n        'level' => 'Level'\n    ],\n    'request_log' => [\n        'hint' => 'This log displays a list of browser requests that may require attention. For example, if a visitor opens a CMS page that cannot be found, a record is created with the status code 404.',\n        'menu_label' => 'Request log',\n        'menu_description' => 'View bad or redirected requests, such as Page not found (404).',\n        'empty_link' => 'Empty request log',\n        'empty_loading' => 'Emptying request log...',\n        'empty_success' => 'Successfully emptied the request log.',\n        'return_link' => 'Return to request log',\n        'id' => 'ID',\n        'id_label' => 'Log ID',\n        'count' => 'Counter',\n        'referer' => 'Referers',\n        'url' => 'URL',\n        'status_code' => 'Status'\n    ],\n    'permissions' => [\n        'name' => 'System',\n        'manage_system_settings' => 'Manage system settings',\n        'manage_software_updates' => 'Manage software updates',\n        'access_logs' => 'View system logs',\n        'manage_mail_templates' => 'Manage mail templates',\n        'manage_mail_settings' => 'Manage mail settings',\n        'manage_other_administrators' => 'Manage other administrators',\n        'view_the_dashboard' => 'View the dashboard',\n        'manage_branding' => 'Customize the back-end'\n    ]\n];\n", "idx": 4, "id": 10959, "msg": "", "proj": "octobercms-october", "lang": "php", "sampling_weight": 0.0739150857232327}
{"patch": "@@ -407,9 +407,9 @@ func (c *Cache) UpdateSVIDs(update *UpdateSVIDs) {\n \n \t\t// Registration entry is updated, remove it from stale map\n \t\tdelete(c.staleEntries, entryID)\n+\t\tc.notifyBySelectorSet(notifySet)\n+\t\tclearSelectorSet(notifySet)\n \t}\n-\n-\tc.notifyBySelectors(notifySet)\n }\n \n // GetStaleEntries obtains a list of stale entries", "y": 0, "oldf": "package cache\n\nimport (\n\t\"crypto\"\n\t\"crypto/x509\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/spiffe/go-spiffe/v2/spiffeid\"\n\t\"github.com/spiffe/spire/pkg/common/bundleutil\"\n\t\"github.com/spiffe/spire/pkg/common/telemetry\"\n\t\"github.com/spiffe/spire/proto/spire/common\"\n)\n\ntype Selectors []*common.Selector\ntype Bundle = bundleutil.Bundle\n\n// Identity holds the data for a single workload identity\ntype Identity struct {\n\tEntry      *common.RegistrationEntry\n\tSVID       []*x509.Certificate\n\tPrivateKey crypto.Signer\n}\n\n// WorkloadUpdate is used to convey workload information to cache subscribers\ntype WorkloadUpdate struct {\n\tIdentities       []Identity\n\tBundle           *bundleutil.Bundle\n\tFederatedBundles map[spiffeid.TrustDomain]*bundleutil.Bundle\n}\n\nfunc (u *WorkloadUpdate) HasIdentity() bool {\n\treturn len(u.Identities) > 0\n}\n\n// Update holds information for an entries update to the cache.\ntype UpdateEntries struct {\n\t// Bundles is a set of ALL trust bundles available to the agent, keyed by trust domain\n\tBundles map[spiffeid.TrustDomain]*bundleutil.Bundle\n\n\t// RegistrationEntries is a set of ALL registration entries available to the\n\t// agent, keyed by registration entry id.\n\tRegistrationEntries map[string]*common.RegistrationEntry\n}\n\n// Update holds information for an SVIDs update to the cache.\ntype UpdateSVIDs struct {\n\t// X509SVIDs is a set of updated X509-SVIDs that should be merged into\n\t// the cache, keyed by registration entry id.\n\tX509SVIDs map[string]*X509SVID\n}\n\n// X509SVID holds onto the SVID certificate chain and private key.\ntype X509SVID struct {\n\tChain      []*x509.Certificate\n\tPrivateKey crypto.Signer\n}\n\n// Cache caches each registration entry, signed X509-SVIDs for those entries,\n// bundles, and JWT SVIDs for the agent. It allows subscriptions by (workload)\n// selector sets and notifies subscribers when:\n//\n// 1) a registration entry related to the selectors:\n//   * is modified\n//   * has a new X509-SVID signed for it\n//   * federates with a federated bundle that is updated\n// 2) the trust bundle for the agent trust domain is updated\n//\n// When notified, the subscriber is given a WorkloadUpdate containing\n// related identities and trust bundles.\n//\n// The cache does this efficiently by building an index for each unique\n// selector it encounters. Each selector index tracks the subscribers (i.e\n// workloads) and registration entries that have that selector.\n//\n// When registration entries are added/updated/removed, the set of relevant\n// selectors are gathered and the indexes for those selectors are combed for\n// all relevant subscribers.\n//\n// For each relevant subscriber, the selector index for each selector of the\n// subscriber is combed for registration whose selectors are a subset of the\n// subscriber selector set. Identities for those entries are added to the\n// workload update returned to the subscriber.\n//\n// NOTE: The cache is intended to be able to handle thousands of workload\n// subscriptions, which can involve thousands of certificates, keys, bundles,\n// and registration entries, etc. The selector index itself is intended to be\n// scalable, but the objects themselves can take a considerable amount of\n// memory. For maximal safety, the objects should be cloned both coming in and\n// leaving the cache. However, during global updates (e.g. trust bundle is\n// updated for the agent trust domain) in particular, cloning all of the\n// relevant objects for each subscriber causes HUGE amounts of memory pressure\n// which adds non-trivial amounts of latency and causes a giant memory spike\n// that could OOM the agent on smaller VMs. For this reason, the cache is\n// presumed to own ALL data passing in and out of the cache. Producers and\n// consumers MUST NOT mutate the data.\ntype Cache struct {\n\t*BundleCache\n\t*JWTSVIDCache\n\n\tlog         logrus.FieldLogger\n\ttrustDomain spiffeid.TrustDomain\n\n\tmetrics telemetry.Metrics\n\n\tmu sync.RWMutex\n\n\t// records holds the records for registration entries, keyed by registration entry ID\n\trecords map[string]*cacheRecord\n\n\t// selectors holds the selector indices, keyed by a selector key\n\tselectors map[selector]*selectorIndex\n\n\t// staleEntries holds stale registration entries\n\tstaleEntries map[string]bool\n\n\t// bundles holds the trust bundles, keyed by trust domain id (i.e. \"spiffe://domain.test\")\n\tbundles map[spiffeid.TrustDomain]*bundleutil.Bundle\n}\n\n// StaleEntry holds stale entries with SVIDs expiration time\ntype StaleEntry struct {\n\t// Entry stale registration entry\n\tEntry *common.RegistrationEntry\n\t// SVIDs expiration time\n\tExpiresAt time.Time\n}\n\nfunc New(log logrus.FieldLogger, trustDomain spiffeid.TrustDomain, bundle *Bundle, metrics telemetry.Metrics) *Cache {\n\treturn &Cache{\n\t\tBundleCache:  NewBundleCache(trustDomain, bundle),\n\t\tJWTSVIDCache: NewJWTSVIDCache(),\n\n\t\tlog:          log,\n\t\tmetrics:      metrics,\n\t\ttrustDomain:  trustDomain,\n\t\trecords:      make(map[string]*cacheRecord),\n\t\tselectors:    make(map[selector]*selectorIndex),\n\t\tstaleEntries: make(map[string]bool),\n\t\tbundles: map[spiffeid.TrustDomain]*bundleutil.Bundle{\n\t\t\ttrustDomain: bundle,\n\t\t},\n\t}\n}\n\n// Identities is only used by manager tests\n// TODO: We should remove this and find a better way\nfunc (c *Cache) Identities() []Identity {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\tout := make([]Identity, 0, len(c.records))\n\tfor _, record := range c.records {\n\t\tif record.svid == nil {\n\t\t\t// The record does not have an SVID yet and should not be returned\n\t\t\t// from the cache.\n\t\t\tcontinue\n\t\t}\n\t\tout = append(out, makeIdentity(record))\n\t}\n\tsortIdentities(out)\n\treturn out\n}\n\nfunc (c *Cache) CountSVIDs() int {\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\n\tvar records int\n\tfor _, record := range c.records {\n\t\tif record.svid == nil {\n\t\t\t// The record does not have an SVID yet and should not be returned\n\t\t\t// from the cache.\n\t\t\tcontinue\n\t\t}\n\t\trecords++\n\t}\n\n\treturn records\n}\n\nfunc (c *Cache) MatchingIdentities(selectors []*common.Selector) []Identity {\n\tset, setDone := allocSelectorSet(selectors...)\n\tdefer setDone()\n\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\treturn c.matchingIdentities(set)\n}\n\nfunc (c *Cache) FetchWorkloadUpdate(selectors []*common.Selector) *WorkloadUpdate {\n\tset, setDone := allocSelectorSet(selectors...)\n\tdefer setDone()\n\n\tc.mu.RLock()\n\tdefer c.mu.RUnlock()\n\treturn c.buildWorkloadUpdate(set)\n}\n\nfunc (c *Cache) SubscribeToWorkloadUpdates(selectors []*common.Selector) Subscriber {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tsub := newSubscriber(c, selectors)\n\tfor s := range sub.set {\n\t\tc.addSelectorIndexSub(s, sub)\n\t}\n\tc.notify(sub)\n\treturn sub\n}\n\n// UpdateEntries updates the cache with the provided registration entries and bundles and\n// notifies impacted subscribers. The checkSVID callback, if provided, is used to determine\n// if the SVID for the entry is stale, or otherwise in need of rotation. Entries marked stale\n// through the checkSVID callback are returned from GetStaleEntries() until the SVID is\n// updated through a call to UpdateSVIDs.\nfunc (c *Cache) UpdateEntries(update *UpdateEntries, checkSVID func(*common.RegistrationEntry, *common.RegistrationEntry, *X509SVID) bool) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Remove bundles that no longer exist. The bundle for the agent trust\n\t// domain should NOT be removed even if not present (which should only be\n\t// the case if there is a bug on the server) since it is necessary to\n\t// authenticate the server.\n\tbundleRemoved := false\n\tfor id := range c.bundles {\n\t\tif _, ok := update.Bundles[id]; !ok && id != c.trustDomain {\n\t\t\tbundleRemoved = true\n\t\t\t// bundle no longer exists.\n\t\t\tc.log.WithField(telemetry.TrustDomainID, id).Debug(\"Bundle removed\")\n\t\t\tdelete(c.bundles, id)\n\t\t}\n\t}\n\n\t// Update bundles with changes, populating a \"changed\" set that we can\n\t// check when processing registration entries to know if they need to spawn\n\t// a notification.\n\tbundleChanged := make(map[spiffeid.TrustDomain]bool)\n\tfor id, bundle := range update.Bundles {\n\t\texisting, ok := c.bundles[id]\n\t\tif !(ok && existing.EqualTo(bundle)) {\n\t\t\tif !ok {\n\t\t\t\tc.log.WithField(telemetry.TrustDomainID, id).Debug(\"Bundle added\")\n\t\t\t} else {\n\t\t\t\tc.log.WithField(telemetry.TrustDomainID, id).Debug(\"Bundle updated\")\n\t\t\t}\n\t\t\tbundleChanged[id] = true\n\t\t\tc.bundles[id] = bundle\n\t\t}\n\t}\n\ttrustDomainBundleChanged := bundleChanged[c.trustDomain]\n\n\t// Allocate a set of selectors that\n\tnotifySet, selSetDone := allocSelectorSet()\n\tdefer selSetDone()\n\n\t// Allocate sets from the pool to track changes to selectors and\n\t// federatesWith declarations. These sets must be cleared after EACH use\n\t// and returned to their respective pools when done processing the\n\t// updates.\n\tselAdd, selAddDone := allocSelectorSet()\n\tdefer selAddDone()\n\tselRem, selRemDone := allocSelectorSet()\n\tdefer selRemDone()\n\tfedAdd, fedAddDone := allocStringSet()\n\tdefer fedAddDone()\n\tfedRem, fedRemDone := allocStringSet()\n\tdefer fedRemDone()\n\n\t// Remove records for registration entries that no longer exist\n\tfor id, record := range c.records {\n\t\tif _, ok := update.RegistrationEntries[id]; !ok {\n\t\t\tc.log.WithFields(logrus.Fields{\n\t\t\t\ttelemetry.Entry:    id,\n\t\t\t\ttelemetry.SPIFFEID: record.entry.SpiffeId,\n\t\t\t}).Debug(\"Entry removed\")\n\n\t\t\t// built a set of selectors for the record being removed, drop the\n\t\t\t// record for each selector index, and add the entry selectors to\n\t\t\t// the notify set.\n\t\t\tclearSelectorSet(selRem)\n\t\t\tselRem.Merge(record.entry.Selectors...)\n\t\t\tc.delSelectorIndicesRecord(selRem, record)\n\t\t\tnotifySet.MergeSet(selRem)\n\t\t\tdelete(c.records, id)\n\t\t\t// Remove stale entry since, registration entry is no longer on cache.\n\t\t\tdelete(c.staleEntries, id)\n\t\t}\n\t}\n\n\t// Add/update records for registration entries in the update\n\tfor _, newEntry := range update.RegistrationEntries {\n\t\tclearSelectorSet(selAdd)\n\t\tclearSelectorSet(selRem)\n\t\tclearStringSet(fedAdd)\n\t\tclearStringSet(fedRem)\n\n\t\trecord, existingEntry := c.updateOrCreateRecord(newEntry)\n\n\t\t// Calculate the difference in selectors, add/remove the record\n\t\t// from impacted selector indices, and add the selector diff to the\n\t\t// notify set.\n\t\tc.diffSelectors(existingEntry, newEntry, selAdd, selRem)\n\t\tc.addSelectorIndicesRecord(selAdd, record)\n\t\tc.delSelectorIndicesRecord(selRem, record)\n\t\tnotifySet.MergeSet(selAdd)\n\t\tnotifySet.MergeSet(selRem)\n\n\t\t// Determine if there were changes to FederatesWith declarations or\n\t\t// if any federated bundles related to the entry were updated.\n\t\tc.diffFederatesWith(existingEntry, newEntry, fedAdd, fedRem)\n\t\tfederatedBundlesChanged := len(fedAdd) > 0 || len(fedRem) > 0\n\t\tif !federatedBundlesChanged {\n\t\t\tfor _, id := range newEntry.FederatesWith {\n\t\t\t\ttd, err := spiffeid.TrustDomainFromString(id)\n\t\t\t\tif err != nil {\n\t\t\t\t\tc.log.WithFields(logrus.Fields{\n\t\t\t\t\t\ttelemetry.TrustDomainID: id,\n\t\t\t\t\t\tlogrus.ErrorKey:         err,\n\t\t\t\t\t}).Warn(\"Invalid federated trust domain\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif bundleChanged[td] {\n\t\t\t\t\tfederatedBundlesChanged = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Determine if something related to this record changed outside of the\n\t\t// selectors and if so, make sure subscribers for all entry selectors\n\t\t// are notified.\n\t\tif federatedBundlesChanged {\n\t\t\tnotifySet.Merge(newEntry.Selectors...)\n\t\t}\n\n\t\t// Invoke the svid checker callback for this record\n\t\tif checkSVID != nil && checkSVID(existingEntry, newEntry, record.svid) {\n\t\t\tc.staleEntries[newEntry.EntryId] = true\n\t\t}\n\n\t\t// Log all the details of the update to the DEBUG log\n\t\t//\n\t\t// TODO: This is a bit verbose and could be trimmed in the future\n\t\t// when the cache implementation has stabilized.\n\t\tif len(selAdd) > 0 || len(selRem) > 0 || len(fedAdd) > 0 || len(fedRem) > 0 {\n\t\t\tlog := c.log.WithFields(logrus.Fields{\n\t\t\t\ttelemetry.Entry:    newEntry.EntryId,\n\t\t\t\ttelemetry.SPIFFEID: newEntry.SpiffeId,\n\t\t\t})\n\t\t\tif len(selAdd) > 0 {\n\t\t\t\tlog = log.WithField(telemetry.SelectorsAdded, len(selAdd))\n\t\t\t}\n\t\t\tif len(selRem) > 0 {\n\t\t\t\tlog = log.WithField(telemetry.SelectorsRemoved, len(selRem))\n\t\t\t}\n\t\t\tif len(fedAdd) > 0 {\n\t\t\t\tlog = log.WithField(telemetry.FederatedAdded, len(fedAdd))\n\t\t\t}\n\t\t\tif len(fedRem) > 0 {\n\t\t\t\tlog = log.WithField(telemetry.FederatedRemoved, len(fedRem))\n\t\t\t}\n\t\t\tif existingEntry != nil {\n\t\t\t\tlog.Debug(\"Entry updated\")\n\t\t\t} else {\n\t\t\t\tlog.Debug(\"Entry created\")\n\t\t\t}\n\t\t}\n\t}\n\n\tif bundleRemoved || len(bundleChanged) > 0 {\n\t\tc.BundleCache.Update(c.bundles)\n\t}\n\n\tif trustDomainBundleChanged {\n\t\tc.notifyAll()\n\t} else {\n\t\tc.notifyBySelectors(notifySet)\n\t}\n}\n\nfunc (c *Cache) UpdateSVIDs(update *UpdateSVIDs) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Allocate a set of selectors that\n\tnotifySet, selSetDone := allocSelectorSet()\n\tdefer selSetDone()\n\n\t// Add/update records for registration entries in the update\n\tfor entryID, svid := range update.X509SVIDs {\n\t\trecord, existingEntry := c.records[entryID]\n\t\tif !existingEntry {\n\t\t\tc.log.WithField(telemetry.RegistrationID, entryID).Error(\"Entry not found\")\n\t\t\tcontinue\n\t\t}\n\n\t\trecord.svid = svid\n\t\tnotifySet.Merge(record.entry.Selectors...)\n\t\tlog := c.log.WithFields(logrus.Fields{\n\t\t\ttelemetry.Entry:    record.entry.EntryId,\n\t\t\ttelemetry.SPIFFEID: record.entry.SpiffeId,\n\t\t})\n\t\tlog.Debug(\"SVID updated\")\n\n\t\t// Registration entry is updated, remove it from stale map\n\t\tdelete(c.staleEntries, entryID)\n\t}\n\n\tc.notifyBySelectors(notifySet)\n}\n\n// GetStaleEntries obtains a list of stale entries\nfunc (c *Cache) GetStaleEntries() []*StaleEntry {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tvar staleEntries []*StaleEntry\n\tfor entryID := range c.staleEntries {\n\t\tcachedEntry, ok := c.records[entryID]\n\t\tif !ok {\n\t\t\tc.log.WithField(telemetry.RegistrationID, entryID).Debug(\"Stale marker found for unknown entry. Please fill a bug\")\n\t\t\tdelete(c.staleEntries, entryID)\n\t\t\tcontinue\n\t\t}\n\n\t\tvar expiresAt time.Time\n\t\tif cachedEntry.svid != nil {\n\t\t\texpiresAt = cachedEntry.svid.Chain[0].NotAfter\n\t\t}\n\n\t\tstaleEntries = append(staleEntries, &StaleEntry{\n\t\t\tEntry:     cachedEntry.entry,\n\t\t\tExpiresAt: expiresAt,\n\t\t})\n\t}\n\n\treturn staleEntries\n}\n\nfunc (c *Cache) updateOrCreateRecord(newEntry *common.RegistrationEntry) (*cacheRecord, *common.RegistrationEntry) {\n\tvar existingEntry *common.RegistrationEntry\n\trecord, recordExists := c.records[newEntry.EntryId]\n\tif !recordExists {\n\t\trecord = newCacheRecord()\n\t\tc.records[newEntry.EntryId] = record\n\t} else {\n\t\texistingEntry = record.entry\n\t}\n\trecord.entry = newEntry\n\treturn record, existingEntry\n}\n\nfunc (c *Cache) diffSelectors(existingEntry, newEntry *common.RegistrationEntry, added, removed selectorSet) {\n\t// Make a set of all the selectors being added\n\tif newEntry != nil {\n\t\tadded.Merge(newEntry.Selectors...)\n\t}\n\n\t// Make a set of all the selectors that are being removed\n\tif existingEntry != nil {\n\t\tfor _, selector := range existingEntry.Selectors {\n\t\t\ts := makeSelector(selector)\n\t\t\tif _, ok := added[s]; ok {\n\t\t\t\t// selector already exists in entry\n\t\t\t\tdelete(added, s)\n\t\t\t} else {\n\t\t\t\t// selector has been removed from entry\n\t\t\t\tremoved[s] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (c *Cache) diffFederatesWith(existingEntry, newEntry *common.RegistrationEntry, added, removed stringSet) {\n\t// Make a set of all the selectors being added\n\tif newEntry != nil {\n\t\tadded.Merge(newEntry.FederatesWith...)\n\t}\n\n\t// Make a set of all the selectors that are being removed\n\tif existingEntry != nil {\n\t\tfor _, id := range existingEntry.FederatesWith {\n\t\t\tif _, ok := added[id]; ok {\n\t\t\t\t// Bundle already exists in entry\n\t\t\t\tdelete(added, id)\n\t\t\t} else {\n\t\t\t\t// Bundle has been removed from entry\n\t\t\t\tremoved[id] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (c *Cache) addSelectorIndicesRecord(selectors selectorSet, record *cacheRecord) {\n\tfor selector := range selectors {\n\t\tc.addSelectorIndexRecord(selector, record)\n\t}\n}\n\nfunc (c *Cache) addSelectorIndexRecord(s selector, record *cacheRecord) {\n\tindex := c.getSelectorIndex(s)\n\tindex.records[record] = struct{}{}\n}\n\nfunc (c *Cache) delSelectorIndicesRecord(selectors selectorSet, record *cacheRecord) {\n\tfor selector := range selectors {\n\t\tc.delSelectorIndexRecord(selector, record)\n\t}\n}\n\n// delSelectorIndexRecord removes the record from the selector index. If\n// the selector index is empty afterwards, it is also removed.\nfunc (c *Cache) delSelectorIndexRecord(s selector, record *cacheRecord) {\n\tindex, ok := c.selectors[s]\n\tif ok {\n\t\tdelete(index.records, record)\n\t\tif index.isEmpty() {\n\t\t\tdelete(c.selectors, s)\n\t\t}\n\t}\n}\n\nfunc (c *Cache) addSelectorIndexSub(s selector, sub *subscriber) {\n\tindex := c.getSelectorIndex(s)\n\tindex.subs[sub] = struct{}{}\n}\n\n// delSelectorIndexSub removes the subscription from the selector index. If\n// the selector index is empty afterwards, it is also removed.\nfunc (c *Cache) delSelectorIndexSub(s selector, sub *subscriber) {\n\tindex, ok := c.selectors[s]\n\tif ok {\n\t\tdelete(index.subs, sub)\n\t\tif index.isEmpty() {\n\t\t\tdelete(c.selectors, s)\n\t\t}\n\t}\n}\n\nfunc (c *Cache) unsubscribe(sub *subscriber) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tfor selector := range sub.set {\n\t\tc.delSelectorIndexSub(selector, sub)\n\t}\n}\n\nfunc (c *Cache) notifyAll() {\n\tsubs, subsDone := c.allSubscribers()\n\tdefer subsDone()\n\tfor sub := range subs {\n\t\tc.notify(sub)\n\t}\n}\n\nfunc (c *Cache) notifyBySelectors(set selectorSet) {\n\tsubs, subsDone := c.getSubscribers(set)\n\tdefer subsDone()\n\tfor sub := range subs {\n\t\tc.notify(sub)\n\t}\n}\n\nfunc (c *Cache) notify(sub *subscriber) {\n\tupdate := c.buildWorkloadUpdate(sub.set)\n\tsub.notify(update)\n}\n\nfunc (c *Cache) allSubscribers() (subscriberSet, func()) {\n\tsubs, subsDone := allocSubscriberSet()\n\tfor _, index := range c.selectors {\n\t\tfor sub := range index.subs {\n\t\t\tsubs[sub] = struct{}{}\n\t\t}\n\t}\n\treturn subs, subsDone\n}\n\nfunc (c *Cache) getSubscribers(set selectorSet) (subscriberSet, func()) {\n\tsubs, subsDone := allocSubscriberSet()\n\tfor s := range set {\n\t\tindex := c.getSelectorIndex(s)\n\t\tfor sub := range index.subs {\n\t\t\tsubs[sub] = struct{}{}\n\t\t}\n\t}\n\treturn subs, subsDone\n}\n\nfunc (c *Cache) matchingIdentities(set selectorSet) []Identity {\n\trecords, recordsDone := c.getRecordsForSelectors(set)\n\tdefer recordsDone()\n\n\tif len(records) == 0 {\n\t\treturn nil\n\t}\n\n\t// Return identities in ascending \"entry id\" order to maintain a consistent\n\t// ordering.\n\t// TODO: figure out how to determine the \"default\" identity\n\tout := make([]Identity, 0, len(records))\n\tfor record := range records {\n\t\tout = append(out, makeIdentity(record))\n\t}\n\tsortIdentities(out)\n\treturn out\n}\n\nfunc (c *Cache) buildWorkloadUpdate(set selectorSet) *WorkloadUpdate {\n\tw := &WorkloadUpdate{\n\t\tBundle:           c.bundles[c.trustDomain],\n\t\tFederatedBundles: make(map[spiffeid.TrustDomain]*bundleutil.Bundle),\n\t\tIdentities:       c.matchingIdentities(set),\n\t}\n\n\t// Add in the bundles the workload is federated with.\n\tfor _, identity := range w.Identities {\n\t\tfor _, federatesWith := range identity.Entry.FederatesWith {\n\t\t\ttd, err := spiffeid.TrustDomainFromString(federatesWith)\n\t\t\tif err != nil {\n\t\t\t\tc.log.WithFields(logrus.Fields{\n\t\t\t\t\ttelemetry.TrustDomainID: federatesWith,\n\t\t\t\t\tlogrus.ErrorKey:         err,\n\t\t\t\t}).Warn(\"Invalid federated trust domain\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif federatedBundle := c.bundles[td]; federatedBundle != nil {\n\t\t\t\tw.FederatedBundles[td] = federatedBundle\n\t\t\t} else {\n\t\t\t\tc.log.WithFields(logrus.Fields{\n\t\t\t\t\ttelemetry.RegistrationID:  identity.Entry.EntryId,\n\t\t\t\t\ttelemetry.SPIFFEID:        identity.Entry.SpiffeId,\n\t\t\t\t\ttelemetry.FederatedBundle: federatesWith,\n\t\t\t\t}).Warn(\"Federated bundle contents missing\")\n\t\t\t}\n\t\t}\n\t}\n\n\treturn w\n}\n\nfunc (c *Cache) getRecordsForSelectors(set selectorSet) (recordSet, func()) {\n\t// Build and dedup a list of candidate entries. Ignore those without an\n\t// SVID but otherwise don't check for selector set inclusion yet, since\n\t// that is a more expensive operation and we could easily have duplicate\n\t// entries to check.\n\trecords, recordsDone := allocRecordSet()\n\tfor selector := range set {\n\t\tindex := c.getSelectorIndex(selector)\n\t\tfor record := range index.records {\n\t\t\tif record.svid == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trecords[record] = struct{}{}\n\t\t}\n\t}\n\n\t// Filter out records whose registration entry selectors are not within\n\t// inside the selector set.\n\tfor record := range records {\n\t\tfor _, s := range record.entry.Selectors {\n\t\t\tif !set.In(s) {\n\t\t\t\tdelete(records, record)\n\t\t\t}\n\t\t}\n\t}\n\treturn records, recordsDone\n}\n\n// getSelectorIndex gets the selector index for the selector. If one doesn't\n// exist, it is created.\nfunc (c *Cache) getSelectorIndex(s selector) *selectorIndex {\n\tindex, ok := c.selectors[s]\n\tif !ok {\n\t\tindex = newSelectorIndex()\n\t\tc.selectors[s] = index\n\t}\n\treturn index\n}\n\ntype cacheRecord struct {\n\tentry *common.RegistrationEntry\n\tsvid  *X509SVID\n\tsubs  map[*subscriber]struct{}\n}\n\nfunc newCacheRecord() *cacheRecord {\n\treturn &cacheRecord{\n\t\tsubs: make(map[*subscriber]struct{}),\n\t}\n}\n\ntype selectorIndex struct {\n\t// subs holds the subscriptions related to this selector\n\tsubs map[*subscriber]struct{}\n\n\t// records holds the cache records related to this selector\n\trecords map[*cacheRecord]struct{}\n}\n\nfunc (x *selectorIndex) isEmpty() bool {\n\treturn len(x.subs) == 0 && len(x.records) == 0\n}\n\nfunc newSelectorIndex() *selectorIndex {\n\treturn &selectorIndex{\n\t\tsubs:    make(map[*subscriber]struct{}),\n\t\trecords: make(map[*cacheRecord]struct{}),\n\t}\n}\n\nfunc sortIdentities(identities []Identity) {\n\tsort.Slice(identities, func(a, b int) bool {\n\t\treturn identities[a].Entry.EntryId < identities[b].Entry.EntryId\n\t})\n}\n\nfunc makeIdentity(record *cacheRecord) Identity {\n\treturn Identity{\n\t\tEntry:      record.entry,\n\t\tSVID:       record.svid.Chain,\n\t\tPrivateKey: record.svid.PrivateKey,\n\t}\n}\n", "idx": 6, "id": 16786, "msg": "", "proj": "spiffe-spire", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -8,12 +8,17 @@\n #include <LightGBM/utils/array_args.h>\n #include <LightGBM/utils/openmp_wrapper.h>\n #include <LightGBM/utils/threading.h>\n+#include <LightGBM/json11.hpp>\n \n #include <limits>\n #include <chrono>\n #include <cstdio>\n #include <sstream>\n #include <unordered_map>\n+#include <fstream>\n+\n+using namespace json11;\n+\n \n namespace LightGBM {\n ", "y": 1, "oldf": "/*!\n * Copyright (c) 2016 Microsoft Corporation. All rights reserved.\n * Licensed under the MIT License. See LICENSE file in the project root for license information.\n */\n#include <LightGBM/dataset.h>\n\n#include <LightGBM/feature_group.h>\n#include <LightGBM/utils/array_args.h>\n#include <LightGBM/utils/openmp_wrapper.h>\n#include <LightGBM/utils/threading.h>\n\n#include <limits>\n#include <chrono>\n#include <cstdio>\n#include <sstream>\n#include <unordered_map>\n\nnamespace LightGBM {\n\nconst char* Dataset::binary_file_token = \"______LightGBM_Binary_File_Token______\\n\";\n\nDataset::Dataset() {\n  data_filename_ = \"noname\";\n  num_data_ = 0;\n  is_finish_load_ = false;\n}\n\nDataset::Dataset(data_size_t num_data) {\n  CHECK(num_data > 0);\n  data_filename_ = \"noname\";\n  num_data_ = num_data;\n  metadata_.Init(num_data_, NO_SPECIFIC, NO_SPECIFIC);\n  is_finish_load_ = false;\n  group_bin_boundaries_.push_back(0);\n}\n\nDataset::~Dataset() {\n}\n\nstd::vector<std::vector<int>> NoGroup(\n  const std::vector<int>& used_features) {\n  std::vector<std::vector<int>> features_in_group;\n  features_in_group.resize(used_features.size());\n  for (size_t i = 0; i < used_features.size(); ++i) {\n    features_in_group[i].emplace_back(used_features[i]);\n  }\n  return features_in_group;\n}\n\nint GetConfilctCount(const std::vector<bool>& mark, const int* indices, int num_indices, int max_cnt) {\n  int ret = 0;\n  for (int i = 0; i < num_indices; ++i) {\n    if (mark[indices[i]]) {\n      ++ret;\n      if (ret > max_cnt) {\n        return -1;\n      }\n    }\n  }\n  return ret;\n}\nvoid MarkUsed(std::vector<bool>& mark, const int* indices, int num_indices) {\n  for (int i = 0; i < num_indices; ++i) {\n    mark[indices[i]] = true;\n  }\n}\n\nstd::vector<std::vector<int>> FindGroups(const std::vector<std::unique_ptr<BinMapper>>& bin_mappers,\n                                         const std::vector<int>& find_order,\n                                         int** sample_indices,\n                                         const int* num_per_col,\n                                         size_t total_sample_cnt,\n                                         data_size_t max_error_cnt,\n                                         data_size_t filter_cnt,\n                                         data_size_t num_data,\n                                         bool is_use_gpu) {\n  const int max_search_group = 100;\n  const int gpu_max_bin_per_group = 256;\n  Random rand(num_data);\n  std::vector<std::vector<int>> features_in_group;\n  std::vector<std::vector<bool>> conflict_marks;\n  std::vector<int> group_conflict_cnt;\n  std::vector<size_t> group_non_zero_cnt;\n  std::vector<int> group_num_bin;\n\n  for (auto fidx : find_order) {\n    const size_t cur_non_zero_cnt = num_per_col[fidx];\n    bool need_new_group = true;\n    std::vector<int> available_groups;\n    for (int gid = 0; gid < static_cast<int>(features_in_group.size()); ++gid) {\n      if (group_non_zero_cnt[gid] + cur_non_zero_cnt <= total_sample_cnt + max_error_cnt) {\n        if (!is_use_gpu || group_num_bin[gid] + bin_mappers[fidx]->num_bin() + (bin_mappers[fidx]->GetDefaultBin() == 0 ? -1 : 0)\n            <= gpu_max_bin_per_group) {\n          available_groups.push_back(gid);\n        }\n      }\n    }\n    std::vector<int> search_groups;\n    if (!available_groups.empty()) {\n      int last = static_cast<int>(available_groups.size()) - 1;\n      auto indices = rand.Sample(last, std::min(last, max_search_group - 1));\n      search_groups.push_back(available_groups.back());\n      for (auto idx : indices) {\n        search_groups.push_back(available_groups[idx]);\n      }\n    }\n    for (auto gid : search_groups) {\n      const int rest_max_cnt = max_error_cnt - group_conflict_cnt[gid];\n      int cnt = GetConfilctCount(conflict_marks[gid], sample_indices[fidx], num_per_col[fidx], rest_max_cnt);\n      if (cnt >= 0 && cnt <= rest_max_cnt) {\n        data_size_t rest_non_zero_data = static_cast<data_size_t>(\n          static_cast<double>(cur_non_zero_cnt - cnt) * num_data / total_sample_cnt);\n        if (rest_non_zero_data < filter_cnt) { continue; }\n        need_new_group = false;\n        features_in_group[gid].push_back(fidx);\n        group_conflict_cnt[gid] += cnt;\n        group_non_zero_cnt[gid] += cur_non_zero_cnt - cnt;\n        MarkUsed(conflict_marks[gid], sample_indices[fidx], num_per_col[fidx]);\n        if (is_use_gpu) {\n          group_num_bin[gid] += bin_mappers[fidx]->num_bin() + (bin_mappers[fidx]->GetDefaultBin() == 0 ? -1 : 0);\n        }\n        break;\n      }\n    }\n    if (need_new_group) {\n      features_in_group.emplace_back();\n      features_in_group.back().push_back(fidx);\n      group_conflict_cnt.push_back(0);\n      conflict_marks.emplace_back(total_sample_cnt, false);\n      MarkUsed(conflict_marks.back(), sample_indices[fidx], num_per_col[fidx]);\n      group_non_zero_cnt.emplace_back(cur_non_zero_cnt);\n      if (is_use_gpu) {\n        group_num_bin.push_back(1 + bin_mappers[fidx]->num_bin() + (bin_mappers[fidx]->GetDefaultBin() == 0 ? -1 : 0));\n      }\n    }\n  }\n  return features_in_group;\n}\n\nstd::vector<std::vector<int>> FastFeatureBundling(std::vector<std::unique_ptr<BinMapper>>& bin_mappers,\n                                                  int** sample_indices,\n                                                  const int* num_per_col,\n                                                  size_t total_sample_cnt,\n                                                  const std::vector<int>& used_features,\n                                                  double max_conflict_rate,\n                                                  data_size_t num_data,\n                                                  data_size_t min_data,\n                                                  double sparse_threshold,\n                                                  bool is_enable_sparse,\n                                                  bool is_use_gpu) {\n  // filter is based on sampling data, so decrease its range\n  const data_size_t filter_cnt = static_cast<data_size_t>(static_cast<double>(0.95 * min_data) / num_data * total_sample_cnt);\n  const data_size_t max_error_cnt = static_cast<data_size_t>(total_sample_cnt * max_conflict_rate);\n  std::vector<size_t> feature_non_zero_cnt;\n  feature_non_zero_cnt.reserve(used_features.size());\n  // put dense feature first\n  for (auto fidx : used_features) {\n    feature_non_zero_cnt.emplace_back(num_per_col[fidx]);\n  }\n  // sort by non zero cnt\n  std::vector<int> sorted_idx;\n  sorted_idx.reserve(used_features.size());\n  for (int i = 0; i < static_cast<int>(used_features.size()); ++i) {\n    sorted_idx.emplace_back(i);\n  }\n  // sort by non zero cnt, bigger first\n  std::stable_sort(sorted_idx.begin(), sorted_idx.end(),\n                   [&feature_non_zero_cnt](int a, int b) {\n    return feature_non_zero_cnt[a] > feature_non_zero_cnt[b];\n  });\n\n  std::vector<int> feature_order_by_cnt;\n  feature_order_by_cnt.reserve(sorted_idx.size());\n  for (auto sidx : sorted_idx) {\n    feature_order_by_cnt.push_back(used_features[sidx]);\n  }\n  auto features_in_group = FindGroups(bin_mappers, used_features, sample_indices, num_per_col, total_sample_cnt, max_error_cnt, filter_cnt, num_data, is_use_gpu);\n  auto group2 = FindGroups(bin_mappers, feature_order_by_cnt, sample_indices, num_per_col, total_sample_cnt, max_error_cnt, filter_cnt, num_data, is_use_gpu);\n  if (features_in_group.size() > group2.size()) {\n    features_in_group = group2;\n  }\n  std::vector<std::vector<int>> ret;\n  for (size_t i = 0; i < features_in_group.size(); ++i) {\n    if (features_in_group[i].size() <= 1 || features_in_group[i].size() >= 5) {\n      ret.push_back(features_in_group[i]);\n    } else {\n      int cnt_non_zero = 0;\n      for (size_t j = 0; j < features_in_group[i].size(); ++j) {\n        const int fidx = features_in_group[i][j];\n        cnt_non_zero += static_cast<int>(num_data * (1.0f - bin_mappers[fidx]->sparse_rate()));\n      }\n      double sparse_rate = 1.0f - static_cast<double>(cnt_non_zero) / (num_data);\n      // take apart small sparse group, due it will not gain on speed\n      if (sparse_rate >= sparse_threshold && is_enable_sparse) {\n        for (size_t j = 0; j < features_in_group[i].size(); ++j) {\n          const int fidx = features_in_group[i][j];\n          ret.emplace_back();\n          ret.back().push_back(fidx);\n        }\n      } else {\n        ret.push_back(features_in_group[i]);\n      }\n    }\n  }\n  // shuffle groups\n  int num_group = static_cast<int>(ret.size());\n  Random tmp_rand(12);\n  for (int i = 0; i < num_group - 1; ++i) {\n    int j = tmp_rand.NextShort(i + 1, num_group);\n    std::swap(ret[i], ret[j]);\n  }\n  return ret;\n}\n\nvoid Dataset::Construct(\n  std::vector<std::unique_ptr<BinMapper>>& bin_mappers,\n  int** sample_non_zero_indices,\n  const int* num_per_col,\n  size_t total_sample_cnt,\n  const Config& io_config) {\n  num_total_features_ = static_cast<int>(bin_mappers.size());\n  sparse_threshold_ = io_config.sparse_threshold;\n  // get num_features\n  std::vector<int> used_features;\n  for (int i = 0; i < static_cast<int>(bin_mappers.size()); ++i) {\n    if (bin_mappers[i] != nullptr && !bin_mappers[i]->is_trivial()) {\n      used_features.emplace_back(i);\n    }\n  }\n  if (used_features.empty()) {\n    Log::Warning(\"There are no meaningful features, as all feature values are constant.\");\n  }\n  auto features_in_group = NoGroup(used_features);\n\n  if (io_config.enable_bundle && !used_features.empty()) {\n    features_in_group = FastFeatureBundling(bin_mappers,\n                                            sample_non_zero_indices, num_per_col, total_sample_cnt,\n                                            used_features, io_config.max_conflict_rate,\n                                            num_data_, io_config.min_data_in_leaf,\n                                            sparse_threshold_, io_config.is_enable_sparse, io_config.device_type == std::string(\"gpu\"));\n  }\n\n  num_features_ = 0;\n  for (const auto& fs : features_in_group) {\n    num_features_ += static_cast<int>(fs.size());\n  }\n  int cur_fidx = 0;\n  used_feature_map_ = std::vector<int>(num_total_features_, -1);\n  num_groups_ = static_cast<int>(features_in_group.size());\n  real_feature_idx_.resize(num_features_);\n  feature2group_.resize(num_features_);\n  feature2subfeature_.resize(num_features_);\n  for (int i = 0; i < num_groups_; ++i) {\n    auto cur_features = features_in_group[i];\n    int cur_cnt_features = static_cast<int>(cur_features.size());\n    // get bin_mappers\n    std::vector<std::unique_ptr<BinMapper>> cur_bin_mappers;\n    for (int j = 0; j < cur_cnt_features; ++j) {\n      int real_fidx = cur_features[j];\n      used_feature_map_[real_fidx] = cur_fidx;\n      real_feature_idx_[cur_fidx] = real_fidx;\n      feature2group_[cur_fidx] = i;\n      feature2subfeature_[cur_fidx] = j;\n      cur_bin_mappers.emplace_back(bin_mappers[real_fidx].release());\n      ++cur_fidx;\n    }\n    feature_groups_.emplace_back(std::unique_ptr<FeatureGroup>(\n      new FeatureGroup(cur_cnt_features, cur_bin_mappers, num_data_, sparse_threshold_,\n                       io_config.is_enable_sparse)));\n  }\n  feature_groups_.shrink_to_fit();\n  group_bin_boundaries_.clear();\n  uint64_t num_total_bin = 0;\n  group_bin_boundaries_.push_back(num_total_bin);\n  for (int i = 0; i < num_groups_; ++i) {\n    num_total_bin += feature_groups_[i]->num_total_bin_;\n    group_bin_boundaries_.push_back(num_total_bin);\n  }\n  int last_group = 0;\n  group_feature_start_.reserve(num_groups_);\n  group_feature_cnt_.reserve(num_groups_);\n  group_feature_start_.push_back(0);\n  group_feature_cnt_.push_back(1);\n  for (int i = 1; i < num_features_; ++i) {\n    const int group = feature2group_[i];\n    if (group == last_group) {\n      group_feature_cnt_.back() = group_feature_cnt_.back() + 1;\n    } else {\n      group_feature_start_.push_back(i);\n      group_feature_cnt_.push_back(1);\n      last_group = group;\n    }\n  }\n\n  if (!io_config.monotone_constraints.empty()) {\n    CHECK(static_cast<size_t>(num_total_features_) == io_config.monotone_constraints.size());\n    monotone_types_.resize(num_features_);\n    for (int i = 0; i < num_total_features_; ++i) {\n      int inner_fidx = InnerFeatureIndex(i);\n      if (inner_fidx >= 0) {\n        monotone_types_[inner_fidx] = io_config.monotone_constraints[i];\n      }\n    }\n    if (ArrayArgs<int8_t>::CheckAllZero(monotone_types_)) {\n      monotone_types_.clear();\n    }\n  }\n  if (!io_config.feature_contri.empty()) {\n    CHECK(static_cast<size_t>(num_total_features_) == io_config.feature_contri.size());\n    feature_penalty_.resize(num_features_);\n    for (int i = 0; i < num_total_features_; ++i) {\n      int inner_fidx = InnerFeatureIndex(i);\n      if (inner_fidx >= 0) {\n        feature_penalty_[inner_fidx] = std::max(0.0, io_config.feature_contri[i]);\n      }\n    }\n    if (ArrayArgs<double>::CheckAll(feature_penalty_, 1.0)) {\n      feature_penalty_.clear();\n    }\n  }\n  if (!io_config.max_bin_by_feature.empty()) {\n    CHECK(static_cast<size_t>(num_total_features_) == io_config.max_bin_by_feature.size());\n    CHECK(*(std::min_element(io_config.max_bin_by_feature.begin(), io_config.max_bin_by_feature.end())) > 1);\n    max_bin_by_feature_.resize(num_total_features_);\n    max_bin_by_feature_.assign(io_config.max_bin_by_feature.begin(), io_config.max_bin_by_feature.end());\n  }\n  max_bin_ = io_config.max_bin;\n  min_data_in_bin_ = io_config.min_data_in_bin;\n  bin_construct_sample_cnt_ = io_config.bin_construct_sample_cnt;\n  use_missing_ = io_config.use_missing;\n  zero_as_missing_ = io_config.zero_as_missing;\n}\n\nvoid Dataset::ResetConfig(const char* parameters) {\n  auto param = Config::Str2Map(parameters);\n  Config io_config;\n  io_config.Set(param);\n  if (param.count(\"max_bin\") && io_config.max_bin != max_bin_) {\n    Log::Warning(\"Cannot change max_bin after constructed Dataset handle.\");\n  }\n  if (param.count(\"max_bin_by_feature\") && io_config.max_bin_by_feature != max_bin_by_feature_) {\n    Log::Warning(\"Cannot change max_bin_by_feature after constructed Dataset handle.\");\n  }\n  if (param.count(\"bin_construct_sample_cnt\") && io_config.bin_construct_sample_cnt != bin_construct_sample_cnt_) {\n    Log::Warning(\"Cannot change bin_construct_sample_cnt after constructed Dataset handle.\");\n  }\n  if (param.count(\"min_data_in_bin\") && io_config.min_data_in_bin != min_data_in_bin_) {\n    Log::Warning(\"Cannot change min_data_in_bin after constructed Dataset handle.\");\n  }\n  if (param.count(\"use_missing\") && io_config.use_missing != use_missing_) {\n    Log::Warning(\"Cannot change use_missing after constructed Dataset handle.\");\n  }\n  if (param.count(\"zero_as_missing\") && io_config.zero_as_missing != zero_as_missing_) {\n    Log::Warning(\"Cannot change zero_as_missing after constructed Dataset handle.\");\n  }\n  if (param.count(\"sparse_threshold\") && io_config.sparse_threshold != sparse_threshold_) {\n    Log::Warning(\"Cannot change sparse_threshold after constructed Dataset handle.\");\n  }\n\n  if (!io_config.monotone_constraints.empty()) {\n    CHECK(static_cast<size_t>(num_total_features_) == io_config.monotone_constraints.size());\n    monotone_types_.resize(num_features_);\n    for (int i = 0; i < num_total_features_; ++i) {\n      int inner_fidx = InnerFeatureIndex(i);\n      if (inner_fidx >= 0) {\n        monotone_types_[inner_fidx] = io_config.monotone_constraints[i];\n      }\n    }\n    if (ArrayArgs<int8_t>::CheckAllZero(monotone_types_)) {\n      monotone_types_.clear();\n    }\n  }\n  if (!io_config.feature_contri.empty()) {\n    CHECK(static_cast<size_t>(num_total_features_) == io_config.feature_contri.size());\n    feature_penalty_.resize(num_features_);\n    for (int i = 0; i < num_total_features_; ++i) {\n      int inner_fidx = InnerFeatureIndex(i);\n      if (inner_fidx >= 0) {\n        feature_penalty_[inner_fidx] = std::max(0.0, io_config.feature_contri[i]);\n      }\n    }\n    if (ArrayArgs<double>::CheckAll(feature_penalty_, 1.0)) {\n      feature_penalty_.clear();\n    }\n  }\n}\n\nvoid Dataset::FinishLoad() {\n  if (is_finish_load_) { return; }\n  if (num_groups_ > 0) {\n    OMP_INIT_EX();\n#pragma omp parallel for schedule(guided)\n    for (int i = 0; i < num_groups_; ++i) {\n      OMP_LOOP_EX_BEGIN();\n      feature_groups_[i]->bin_data_->FinishLoad();\n      OMP_LOOP_EX_END();\n    }\n    OMP_THROW_EX();\n  }\n  is_finish_load_ = true;\n}\n\nvoid Dataset::CopyFeatureMapperFrom(const Dataset* dataset) {\n  feature_groups_.clear();\n  num_features_ = dataset->num_features_;\n  num_groups_ = dataset->num_groups_;\n  sparse_threshold_ = dataset->sparse_threshold_;\n  // copy feature bin mapper data\n  for (int i = 0; i < num_groups_; ++i) {\n    std::vector<std::unique_ptr<BinMapper>> bin_mappers;\n    for (int j = 0; j < dataset->feature_groups_[i]->num_feature_; ++j) {\n      bin_mappers.emplace_back(new BinMapper(*(dataset->feature_groups_[i]->bin_mappers_[j])));\n    }\n    feature_groups_.emplace_back(new FeatureGroup(\n      dataset->feature_groups_[i]->num_feature_,\n      bin_mappers,\n      num_data_,\n      dataset->feature_groups_[i]->is_sparse_));\n  }\n  feature_groups_.shrink_to_fit();\n  used_feature_map_ = dataset->used_feature_map_;\n  num_total_features_ = dataset->num_total_features_;\n  feature_names_ = dataset->feature_names_;\n  label_idx_ = dataset->label_idx_;\n  real_feature_idx_ = dataset->real_feature_idx_;\n  feature2group_ = dataset->feature2group_;\n  feature2subfeature_ = dataset->feature2subfeature_;\n  group_bin_boundaries_ = dataset->group_bin_boundaries_;\n  group_feature_start_ = dataset->group_feature_start_;\n  group_feature_cnt_ = dataset->group_feature_cnt_;\n  monotone_types_ = dataset->monotone_types_;\n  feature_penalty_ = dataset->feature_penalty_;\n}\n\nvoid Dataset::CreateValid(const Dataset* dataset) {\n  feature_groups_.clear();\n  num_features_ = dataset->num_features_;\n  num_groups_ = num_features_;\n  sparse_threshold_ = dataset->sparse_threshold_;\n  bool is_enable_sparse = true;\n  feature2group_.clear();\n  feature2subfeature_.clear();\n  // copy feature bin mapper data\n  for (int i = 0; i < num_features_; ++i) {\n    std::vector<std::unique_ptr<BinMapper>> bin_mappers;\n    bin_mappers.emplace_back(new BinMapper(*(dataset->FeatureBinMapper(i))));\n    feature_groups_.emplace_back(new FeatureGroup(\n      1,\n      bin_mappers,\n      num_data_,\n      sparse_threshold_,\n      is_enable_sparse));\n    feature2group_.push_back(i);\n    feature2subfeature_.push_back(0);\n  }\n\n  feature_groups_.shrink_to_fit();\n  used_feature_map_ = dataset->used_feature_map_;\n  num_total_features_ = dataset->num_total_features_;\n  feature_names_ = dataset->feature_names_;\n  label_idx_ = dataset->label_idx_;\n  real_feature_idx_ = dataset->real_feature_idx_;\n  group_bin_boundaries_.clear();\n  uint64_t num_total_bin = 0;\n  group_bin_boundaries_.push_back(num_total_bin);\n  for (int i = 0; i < num_groups_; ++i) {\n    num_total_bin += feature_groups_[i]->num_total_bin_;\n    group_bin_boundaries_.push_back(num_total_bin);\n  }\n  int last_group = 0;\n  group_feature_start_.reserve(num_groups_);\n  group_feature_cnt_.reserve(num_groups_);\n  group_feature_start_.push_back(0);\n  group_feature_cnt_.push_back(1);\n  for (int i = 1; i < num_features_; ++i) {\n    const int group = feature2group_[i];\n    if (group == last_group) {\n      group_feature_cnt_.back() = group_feature_cnt_.back() + 1;\n    } else {\n      group_feature_start_.push_back(i);\n      group_feature_cnt_.push_back(1);\n      last_group = group;\n    }\n  }\n  monotone_types_ = dataset->monotone_types_;\n  feature_penalty_ = dataset->feature_penalty_;\n}\n\nvoid Dataset::ReSize(data_size_t num_data) {\n  if (num_data_ != num_data) {\n    num_data_ = num_data;\n    OMP_INIT_EX();\n    #pragma omp parallel for schedule(static)\n    for (int group = 0; group < num_groups_; ++group) {\n      OMP_LOOP_EX_BEGIN();\n      feature_groups_[group]->bin_data_->ReSize(num_data_);\n      OMP_LOOP_EX_END();\n    }\n    OMP_THROW_EX();\n  }\n}\n\nvoid Dataset::CopySubset(const Dataset* fullset, const data_size_t* used_indices, data_size_t num_used_indices, bool need_meta_data) {\n  CHECK(num_used_indices == num_data_);\n  OMP_INIT_EX();\n  #pragma omp parallel for schedule(static)\n  for (int group = 0; group < num_groups_; ++group) {\n    OMP_LOOP_EX_BEGIN();\n    feature_groups_[group]->CopySubset(fullset->feature_groups_[group].get(), used_indices, num_used_indices);\n    OMP_LOOP_EX_END();\n  }\n  OMP_THROW_EX();\n  if (need_meta_data) {\n    metadata_.Init(fullset->metadata_, used_indices, num_used_indices);\n  }\n  is_finish_load_ = true;\n}\n\nbool Dataset::SetFloatField(const char* field_name, const float* field_data, data_size_t num_element) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"label\") || name == std::string(\"target\")) {\n    #ifdef LABEL_T_USE_DOUBLE\n    Log::Fatal(\"Don't support LABEL_T_USE_DOUBLE\");\n    #else\n    metadata_.SetLabel(field_data, num_element);\n    #endif\n  } else if (name == std::string(\"weight\") || name == std::string(\"weights\")) {\n    #ifdef LABEL_T_USE_DOUBLE\n    Log::Fatal(\"Don't support LABEL_T_USE_DOUBLE\");\n    #else\n    metadata_.SetWeights(field_data, num_element);\n    #endif\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::SetDoubleField(const char* field_name, const double* field_data, data_size_t num_element) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"init_score\")) {\n    metadata_.SetInitScore(field_data, num_element);\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::SetIntField(const char* field_name, const int* field_data, data_size_t num_element) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"query\") || name == std::string(\"group\")) {\n    metadata_.SetQuery(field_data, num_element);\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::GetFloatField(const char* field_name, data_size_t* out_len, const float** out_ptr) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"label\") || name == std::string(\"target\")) {\n    #ifdef LABEL_T_USE_DOUBLE\n    Log::Fatal(\"Don't support LABEL_T_USE_DOUBLE\");\n    #else\n    *out_ptr = metadata_.label();\n    *out_len = num_data_;\n    #endif\n  } else if (name == std::string(\"weight\") || name == std::string(\"weights\")) {\n    #ifdef LABEL_T_USE_DOUBLE\n    Log::Fatal(\"Don't support LABEL_T_USE_DOUBLE\");\n    #else\n    *out_ptr = metadata_.weights();\n    *out_len = num_data_;\n    #endif\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::GetDoubleField(const char* field_name, data_size_t* out_len, const double** out_ptr) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"init_score\")) {\n    *out_ptr = metadata_.init_score();\n    *out_len = static_cast<data_size_t>(metadata_.num_init_score());\n  } else if (name == std::string(\"feature_penalty\")) {\n    *out_ptr = feature_penalty_.data();\n    *out_len = static_cast<data_size_t>(feature_penalty_.size());\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::GetIntField(const char* field_name, data_size_t* out_len, const int** out_ptr) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"query\") || name == std::string(\"group\")) {\n    *out_ptr = metadata_.query_boundaries();\n    *out_len = metadata_.num_queries() + 1;\n  } else {\n    return false;\n  }\n  return true;\n}\n\nbool Dataset::GetInt8Field(const char* field_name, data_size_t* out_len, const int8_t** out_ptr) {\n  std::string name(field_name);\n  name = Common::Trim(name);\n  if (name == std::string(\"monotone_constraints\")) {\n    *out_ptr = monotone_types_.data();\n    *out_len = static_cast<data_size_t>(monotone_types_.size());\n  } else {\n    return false;\n  }\n  return true;\n}\n\nvoid Dataset::SaveBinaryFile(const char* bin_filename) {\n  if (bin_filename != nullptr\n      && std::string(bin_filename) == data_filename_) {\n    Log::Warning(\"Bianry file %s already exists\", bin_filename);\n    return;\n  }\n  // if not pass a filename, just append \".bin\" of original file\n  std::string bin_filename_str(data_filename_);\n  if (bin_filename == nullptr || bin_filename[0] == '\\0') {\n    bin_filename_str.append(\".bin\");\n    bin_filename = bin_filename_str.c_str();\n  }\n  bool is_file_existed = false;\n\n  if (VirtualFileWriter::Exists(bin_filename)) {\n    is_file_existed = true;\n    Log::Warning(\"File %s exists, cannot save binary to it\", bin_filename);\n  }\n\n  if (!is_file_existed) {\n    auto writer = VirtualFileWriter::Make(bin_filename);\n    if (!writer->Init()) {\n      Log::Fatal(\"Cannot write binary data to %s \", bin_filename);\n    }\n    Log::Info(\"Saving data to binary file %s\", bin_filename);\n    size_t size_of_token = std::strlen(binary_file_token);\n    writer->Write(binary_file_token, size_of_token);\n    // get size of header\n    size_t size_of_header = sizeof(num_data_) + sizeof(num_features_) + sizeof(num_total_features_)\n      + sizeof(int) * num_total_features_ + sizeof(label_idx_) + sizeof(num_groups_) + sizeof(sparse_threshold_)\n      + 3 * sizeof(int) * num_features_ + sizeof(uint64_t) * (num_groups_ + 1) + 2 * sizeof(int) * num_groups_ + sizeof(int8_t) * num_features_\n      + sizeof(double) * num_features_ + sizeof(int32_t) * num_total_features_ + sizeof(int) * 3 + sizeof(bool) * 2;\n    // size of feature names\n    for (int i = 0; i < num_total_features_; ++i) {\n      size_of_header += feature_names_[i].size() + sizeof(int);\n    }\n    writer->Write(&size_of_header, sizeof(size_of_header));\n    // write header\n    writer->Write(&num_data_, sizeof(num_data_));\n    writer->Write(&num_features_, sizeof(num_features_));\n    writer->Write(&num_total_features_, sizeof(num_total_features_));\n    writer->Write(&label_idx_, sizeof(label_idx_));\n    writer->Write(&max_bin_, sizeof(max_bin_));\n    writer->Write(&bin_construct_sample_cnt_, sizeof(bin_construct_sample_cnt_));\n    writer->Write(&min_data_in_bin_, sizeof(min_data_in_bin_));\n    writer->Write(&use_missing_, sizeof(use_missing_));\n    writer->Write(&zero_as_missing_, sizeof(zero_as_missing_));\n    writer->Write(&sparse_threshold_, sizeof(sparse_threshold_));\n    writer->Write(used_feature_map_.data(), sizeof(int) * num_total_features_);\n    writer->Write(&num_groups_, sizeof(num_groups_));\n    writer->Write(real_feature_idx_.data(), sizeof(int) * num_features_);\n    writer->Write(feature2group_.data(), sizeof(int) * num_features_);\n    writer->Write(feature2subfeature_.data(), sizeof(int) * num_features_);\n    writer->Write(group_bin_boundaries_.data(), sizeof(uint64_t) * (num_groups_ + 1));\n    writer->Write(group_feature_start_.data(), sizeof(int) * num_groups_);\n    writer->Write(group_feature_cnt_.data(), sizeof(int) * num_groups_);\n    if (monotone_types_.empty()) {\n      ArrayArgs<int8_t>::Assign(&monotone_types_, 0, num_features_);\n    }\n    writer->Write(monotone_types_.data(), sizeof(int8_t) * num_features_);\n    if (ArrayArgs<int8_t>::CheckAllZero(monotone_types_)) {\n      monotone_types_.clear();\n    }\n    if (feature_penalty_.empty()) {\n      ArrayArgs<double>::Assign(&feature_penalty_, 1.0, num_features_);\n    }\n    writer->Write(feature_penalty_.data(), sizeof(double) * num_features_);\n    if (ArrayArgs<double>::CheckAll(feature_penalty_, 1.0)) {\n      feature_penalty_.clear();\n    }\n    if (max_bin_by_feature_.empty()) {\n      ArrayArgs<int32_t>::Assign(&max_bin_by_feature_, -1, num_total_features_);\n    }\n    writer->Write(max_bin_by_feature_.data(), sizeof(int32_t) * num_total_features_);\n    if (ArrayArgs<int32_t>::CheckAll(max_bin_by_feature_, -1)) {\n      max_bin_by_feature_.clear();\n    }\n    // write feature names\n    for (int i = 0; i < num_total_features_; ++i) {\n      int str_len = static_cast<int>(feature_names_[i].size());\n      writer->Write(&str_len, sizeof(int));\n      const char* c_str = feature_names_[i].c_str();\n      writer->Write(c_str, sizeof(char) * str_len);\n    }\n\n    // get size of meta data\n    size_t size_of_metadata = metadata_.SizesInByte();\n    writer->Write(&size_of_metadata, sizeof(size_of_metadata));\n    // write meta data\n    metadata_.SaveBinaryToFile(writer.get());\n\n    // write feature data\n    for (int i = 0; i < num_groups_; ++i) {\n      // get size of feature\n      size_t size_of_feature = feature_groups_[i]->SizesInByte();\n      writer->Write(&size_of_feature, sizeof(size_of_feature));\n      // write feature\n      feature_groups_[i]->SaveBinaryToFile(writer.get());\n    }\n  }\n}\n\nvoid Dataset::DumpTextFile(const char* text_filename) {\n  FILE* file = NULL;\n#if _MSC_VER\n  fopen_s(&file, text_filename, \"wt\");\n#else\n  file = fopen(text_filename, \"wt\");\n#endif\n  fprintf(file, \"num_features: %d\\n\", num_features_);\n  fprintf(file, \"num_total_features: %d\\n\", num_total_features_);\n  fprintf(file, \"num_groups: %d\\n\", num_groups_);\n  fprintf(file, \"num_data: %d\\n\", num_data_);\n  fprintf(file, \"feature_names: \");\n  for (auto n : feature_names_) {\n    fprintf(file, \"%s, \", n.c_str());\n  }\n  fprintf(file, \"\\nmonotone_constraints: \");\n  for (auto i : monotone_types_) {\n    fprintf(file, \"%d, \", i);\n  }\n  fprintf(file, \"\\nfeature_penalty: \");\n  for (auto i : feature_penalty_) {\n    fprintf(file, \"%lf, \", i);\n  }\n  fprintf(file, \"\\nmax_bin_by_feature: \");\n  for (auto i : max_bin_by_feature_) {\n    fprintf(file, \"%d, \", i);\n  }\n  fprintf(file, \"\\n\");\n  for (auto n : feature_names_) {\n    fprintf(file, \"%s, \", n.c_str());\n  }\n  std::vector<std::unique_ptr<BinIterator>> iterators;\n  iterators.reserve(num_features_);\n  for (int j = 0; j < num_features_; ++j) {\n    auto group_idx = feature2group_[j];\n    auto sub_idx = feature2subfeature_[j];\n    iterators.emplace_back(feature_groups_[group_idx]->SubFeatureIterator(sub_idx));\n  }\n  for (data_size_t i = 0; i < num_data_; ++i) {\n    fprintf(file, \"\\n\");\n    for (int j = 0; j < num_total_features_; ++j) {\n      auto inner_feature_idx = used_feature_map_[j];\n      if (inner_feature_idx < 0) {\n        fprintf(file, \"NA, \");\n      } else {\n        fprintf(file, \"%d, \", iterators[inner_feature_idx]->RawGet(i));\n      }\n    }\n  }\n  fclose(file);\n}\n\nvoid Dataset::ConstructHistograms(const std::vector<int8_t>& is_feature_used,\n                                  const data_size_t* data_indices, data_size_t num_data,\n                                  int leaf_idx,\n                                  std::vector<std::unique_ptr<OrderedBin>>& ordered_bins,\n                                  const score_t* gradients, const score_t* hessians,\n                                  score_t* ordered_gradients, score_t* ordered_hessians,\n                                  bool is_constant_hessian,\n                                  HistogramBinEntry* hist_data) const {\n  if (leaf_idx < 0 || num_data < 0 || hist_data == nullptr) {\n    return;\n  }\n\n  std::vector<int> used_group;\n  used_group.reserve(num_groups_);\n  for (int group = 0; group < num_groups_; ++group) {\n    const int f_cnt = group_feature_cnt_[group];\n    bool is_group_used = false;\n    for (int j = 0; j < f_cnt; ++j) {\n      const int fidx = group_feature_start_[group] + j;\n      if (is_feature_used[fidx]) {\n        is_group_used = true;\n        break;\n      }\n    }\n    if (is_group_used) {\n      used_group.push_back(group);\n    }\n  }\n  int num_used_group = static_cast<int>(used_group.size());\n  auto ptr_ordered_grad = gradients;\n  auto ptr_ordered_hess = hessians;\n  if (data_indices != nullptr && num_data < num_data_) {\n    if (!is_constant_hessian) {\n      #pragma omp parallel for schedule(static)\n      for (data_size_t i = 0; i < num_data; ++i) {\n        ordered_gradients[i] = gradients[data_indices[i]];\n        ordered_hessians[i] = hessians[data_indices[i]];\n      }\n    } else {\n      #pragma omp parallel for schedule(static)\n      for (data_size_t i = 0; i < num_data; ++i) {\n        ordered_gradients[i] = gradients[data_indices[i]];\n      }\n    }\n    ptr_ordered_grad = ordered_gradients;\n    ptr_ordered_hess = ordered_hessians;\n    if (!is_constant_hessian) {\n      OMP_INIT_EX();\n      #pragma omp parallel for schedule(static)\n      for (int gi = 0; gi < num_used_group; ++gi) {\n        OMP_LOOP_EX_BEGIN();\n        int group = used_group[gi];\n        // feature is not used\n        auto data_ptr = hist_data + group_bin_boundaries_[group];\n        const int num_bin = feature_groups_[group]->num_total_bin_;\n        std::memset((void*)(data_ptr + 1), 0, (num_bin - 1) * sizeof(HistogramBinEntry));\n        // construct histograms for smaller leaf\n        if (ordered_bins[group] == nullptr) {\n          // if not use ordered bin\n          feature_groups_[group]->bin_data_->ConstructHistogram(\n            data_indices,\n            num_data,\n            ptr_ordered_grad,\n            ptr_ordered_hess,\n            data_ptr);\n        } else {\n          // used ordered bin\n          ordered_bins[group]->ConstructHistogram(leaf_idx,\n                                                  gradients,\n                                                  hessians,\n                                                  data_ptr);\n        }\n        OMP_LOOP_EX_END();\n      }\n      OMP_THROW_EX();\n    } else {\n      OMP_INIT_EX();\n      #pragma omp parallel for schedule(static)\n      for (int gi = 0; gi < num_used_group; ++gi) {\n        OMP_LOOP_EX_BEGIN();\n        int group = used_group[gi];\n        // feature is not used\n        auto data_ptr = hist_data + group_bin_boundaries_[group];\n        const int num_bin = feature_groups_[group]->num_total_bin_;\n        std::memset((void*)(data_ptr + 1), 0, (num_bin - 1) * sizeof(HistogramBinEntry));\n        // construct histograms for smaller leaf\n        if (ordered_bins[group] == nullptr) {\n          // if not use ordered bin\n          feature_groups_[group]->bin_data_->ConstructHistogram(\n            data_indices,\n            num_data,\n            ptr_ordered_grad,\n            data_ptr);\n        } else {\n          // used ordered bin\n          ordered_bins[group]->ConstructHistogram(leaf_idx,\n                                                  gradients,\n                                                  data_ptr);\n        }\n        // fixed hessian.\n        for (int i = 0; i < num_bin; ++i) {\n          data_ptr[i].sum_hessians = data_ptr[i].cnt * hessians[0];\n        }\n        OMP_LOOP_EX_END();\n      }\n      OMP_THROW_EX();\n    }\n  } else {\n    if (!is_constant_hessian) {\n      OMP_INIT_EX();\n      #pragma omp parallel for schedule(static)\n      for (int gi = 0; gi < num_used_group; ++gi) {\n        OMP_LOOP_EX_BEGIN();\n        int group = used_group[gi];\n        // feature is not used\n        auto data_ptr = hist_data + group_bin_boundaries_[group];\n        const int num_bin = feature_groups_[group]->num_total_bin_;\n        std::memset((void*)(data_ptr + 1), 0, (num_bin - 1) * sizeof(HistogramBinEntry));\n        // construct histograms for smaller leaf\n        if (ordered_bins[group] == nullptr) {\n          // if not use ordered bin\n          feature_groups_[group]->bin_data_->ConstructHistogram(\n            num_data,\n            ptr_ordered_grad,\n            ptr_ordered_hess,\n            data_ptr);\n        } else {\n          // used ordered bin\n          ordered_bins[group]->ConstructHistogram(leaf_idx,\n                                                  gradients,\n                                                  hessians,\n                                                  data_ptr);\n        }\n        OMP_LOOP_EX_END();\n      }\n      OMP_THROW_EX();\n    } else {\n      OMP_INIT_EX();\n      #pragma omp parallel for schedule(static)\n      for (int gi = 0; gi < num_used_group; ++gi) {\n        OMP_LOOP_EX_BEGIN();\n        int group = used_group[gi];\n        // feature is not used\n        auto data_ptr = hist_data + group_bin_boundaries_[group];\n        const int num_bin = feature_groups_[group]->num_total_bin_;\n        std::memset((void*)(data_ptr + 1), 0, (num_bin - 1) * sizeof(HistogramBinEntry));\n        // construct histograms for smaller leaf\n        if (ordered_bins[group] == nullptr) {\n          // if not use ordered bin\n          feature_groups_[group]->bin_data_->ConstructHistogram(\n            num_data,\n            ptr_ordered_grad,\n            data_ptr);\n        } else {\n          // used ordered bin\n          ordered_bins[group]->ConstructHistogram(leaf_idx,\n                                                  gradients,\n                                                  data_ptr);\n        }\n        // fixed hessian.\n        for (int i = 0; i < num_bin; ++i) {\n          data_ptr[i].sum_hessians = data_ptr[i].cnt * hessians[0];\n        }\n        OMP_LOOP_EX_END();\n      }\n      OMP_THROW_EX();\n    }\n  }\n}\n\nvoid Dataset::FixHistogram(int feature_idx, double sum_gradient, double sum_hessian, data_size_t num_data,\n                           HistogramBinEntry* data) const {\n  const int group = feature2group_[feature_idx];\n  const int sub_feature = feature2subfeature_[feature_idx];\n  const BinMapper* bin_mapper = feature_groups_[group]->bin_mappers_[sub_feature].get();\n  const int default_bin = bin_mapper->GetDefaultBin();\n  if (default_bin > 0) {\n    const int num_bin = bin_mapper->num_bin();\n    data[default_bin].sum_gradients = sum_gradient;\n    data[default_bin].sum_hessians = sum_hessian;\n    data[default_bin].cnt = num_data;\n    for (int i = 0; i < num_bin; ++i) {\n      if (i != default_bin) {\n        data[default_bin].sum_gradients -= data[i].sum_gradients;\n        data[default_bin].sum_hessians -= data[i].sum_hessians;\n        data[default_bin].cnt -= data[i].cnt;\n      }\n    }\n  }\n}\n\ntemplate<typename T>\nvoid PushVector(std::vector<T>& dest, const std::vector<T>& src) {\n  dest.reserve(dest.size() + src.size());\n  for (auto i : src) {\n    dest.push_back(i);\n  }\n}\n\ntemplate<typename T>\nvoid PushOffset(std::vector<T>& dest, const std::vector<T>& src, const T& offset) {\n  dest.reserve(dest.size() + src.size());\n  for (auto i : src) {\n    dest.push_back(i + offset);\n  }\n}\n\ntemplate<typename T>\nvoid PushClearIfEmpty(std::vector<T>& dest, const size_t dest_len, const std::vector<T>& src, const size_t src_len, const T& deflt) {\n  if (!dest.empty() && !src.empty()) {\n    PushVector(dest, src);\n  } else if (!dest.empty() && src.empty()) {\n    for (size_t i = 0; i < src_len; ++i) {\n      dest.push_back(deflt);\n    }\n  } else if (dest.empty() && !src.empty()) {\n    for (size_t i = 0; i < dest_len; ++i) {\n      dest.push_back(deflt);\n    }\n    PushVector(dest, src);\n  }\n}\n\nvoid Dataset::addFeaturesFrom(Dataset* other) {\n  if (other->num_data_ != num_data_) {\n    throw std::runtime_error(\"Cannot add features from other Dataset with a different number of rows\");\n  }\n  PushVector(feature_names_, other->feature_names_);\n  PushVector(feature2subfeature_, other->feature2subfeature_);\n  PushVector(group_feature_cnt_, other->group_feature_cnt_);\n  feature_groups_.reserve(other->feature_groups_.size());\n  for (auto& fg : other->feature_groups_) {\n    feature_groups_.emplace_back(new FeatureGroup(*fg));\n  }\n  for (auto feature_idx : other->used_feature_map_) {\n    if (feature_idx >= 0) {\n      used_feature_map_.push_back(feature_idx + num_features_);\n    } else {\n      used_feature_map_.push_back(-1);  // Unused feature.\n    }\n  }\n  PushOffset(real_feature_idx_, other->real_feature_idx_, num_total_features_);\n  PushOffset(feature2group_, other->feature2group_, num_groups_);\n  auto bin_offset = group_bin_boundaries_.back();\n  // Skip the leading 0 when copying group_bin_boundaries.\n  for (auto i = other->group_bin_boundaries_.begin()+1; i < other->group_bin_boundaries_.end(); ++i) {\n    group_bin_boundaries_.push_back(*i + bin_offset);\n  }\n  PushOffset(group_feature_start_, other->group_feature_start_, num_features_);\n\n  PushClearIfEmpty(monotone_types_, num_total_features_, other->monotone_types_, other->num_total_features_, (int8_t)0);\n  PushClearIfEmpty(feature_penalty_, num_total_features_, other->feature_penalty_, other->num_total_features_, 1.0);\n\n  num_features_ += other->num_features_;\n  num_total_features_ += other->num_total_features_;\n  num_groups_ += other->num_groups_;\n}\n\n}  // namespace LightGBM\n", "idx": 1, "id": 20877, "msg": "Please keep alphabetical order, and move this after `feature_group.h`", "proj": "microsoft-LightGBM", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -0,0 +1,5 @@\n+class FillTrailsDescriptions < ActiveRecord::Migration\n+  def change\n+    update(\"UPDATE trails SET description = 'A method of improving code quality and minimizing time required to add new features to software by ensuring that each facet of the program works as expected.'\")\n+  end\n+end", "y": 1, "oldf": "", "idx": 1, "id": 12244, "msg": "This description seems like it was taken from a specific trail? What about reusing the subheading text? > These new trails are focused exercises targeting your interests. They help you learn by doing and keep your skillset sharp.", "proj": "thoughtbot-upcase", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -525,28 +525,31 @@ func assembleByte(ops *OpStream, spec *OpSpec, args []string) error {\n \tvar val []byte\n \tvar err error\n \tif len(args) == 0 {\n-\t\treturn ops.error(\"byte operation needs byte literal argument\")\n+\t\tops.error(\"byte operation needs byte literal argument\")\n+\t\targs = []string{\"0x00\"} // By continuing, ByteLiteral will maintain type stack.\n \t}\n \tval, _, err = parseBinaryArgs(args)\n \tif err != nil {\n-\t\treturn ops.error(err)\n+\t\tops.error(err)\n+\t\tval = []byte{} // By continuing, ByteLiteral will maintain type stack.\n \t}\n-\treturn ops.ByteLiteral(val)\n+\tops.ByteLiteral(val)\n+\treturn nil\n }\n \n func assembleIntCBlock(ops *OpStream, spec *OpSpec, args []string) error {\n-\tops.Out.WriteByte(0x20) // intcblock\n+\tops.pending.WriteByte(0x20) // intcblock\n \tvar scratch [binary.MaxVarintLen64]byte\n \tl := binary.PutUvarint(scratch[:], uint64(len(args)))\n-\tops.Out.Write(scratch[:l])\n+\tops.pending.Write(scratch[:l])\n \tops.intc = make([]uint64, len(args))\n \tfor i, xs := range args {\n \t\tcu, err := strconv.ParseUint(xs, 0, 64)\n \t\tif err != nil {\n-\t\t\treturn ops.error(err)\n+\t\t\tops.error(err)\n \t\t}\n \t\tl = binary.PutUvarint(scratch[:], cu)\n-\t\tops.Out.Write(scratch[:l])\n+\t\tops.pending.Write(scratch[:l])\n \t\tops.intc[i] = cu\n \t}\n \tops.noIntcBlock = true", "y": 0, "oldf": "// Copyright (C) 2019-2020 Algorand, Inc.\n// This file is part of go-algorand\n//\n// go-algorand is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as\n// published by the Free Software Foundation, either version 3 of the\n// License, or (at your option) any later version.\n//\n// go-algorand is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with go-algorand.  If not, see <https://www.gnu.org/licenses/>.\n\npackage logic\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base32\"\n\t\"encoding/base64\"\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/algorand/go-algorand/data/basics\"\n)\n\n// Writer is what we want here. Satisfied by bufio.Buffer\ntype Writer interface {\n\tWrite([]byte) (int, error)\n\tWriteByte(c byte) error\n}\n\ntype labelReference struct {\n\tsourceLine int\n\n\t// position of the opcode start that refers to the label\n\tposition int\n\n\tlabel string\n}\n\n// OpStream is destination for program and scratch space\ntype OpStream struct {\n\tOut     bytes.Buffer\n\tVersion uint64\n\tTrace   io.Writer\n\tStderr  io.Writer\n\tvubytes [9]byte\n\n\tintc        []uint64\n\tnoIntcBlock bool\n\n\tbytec        [][]byte\n\tnoBytecBlock bool\n\n\t// Keep a stack of the types of what we would push and pop to typecheck a program\n\ttypeStack []StackType\n\n\t// current sourceLine during assembly\n\tsourceLine int\n\n\t// map label string to position within Out buffer\n\tlabels map[string]int\n\n\tlabelReferences []labelReference\n\n\t// map opcode offsets to source line\n\toffsetToLine map[int]int\n}\n\n// GetVersion returns the LogicSigVersion we're building to\nfunc (ops *OpStream) GetVersion() uint64 {\n\tif ops.Version == 0 {\n\t\tops.Version = AssemblerDefaultVersion\n\t}\n\treturn ops.Version\n}\n\n// SetLabelHere inserts a label reference to point to the next instruction\nfunc (ops *OpStream) SetLabelHere(label string) error {\n\tif ops.labels == nil {\n\t\tops.labels = make(map[string]int)\n\t}\n\tif _, ok := ops.labels[label]; ok {\n\t\treturn ops.errorf(\"duplicate label %s\", label)\n\t}\n\tops.labels[label] = ops.Out.Len()\n\treturn nil\n}\n\n// RecordSourceLine adds an entry to pc to line mapping\nfunc (ops *OpStream) RecordSourceLine() {\n\tif ops.offsetToLine == nil {\n\t\tops.offsetToLine = make(map[int]int)\n\t}\n\tops.offsetToLine[ops.Out.Len()] = ops.sourceLine - 1\n}\n\n// ReferToLabel records an opcode label refence to resolve later\nfunc (ops *OpStream) ReferToLabel(pc int, label string) {\n\tops.labelReferences = append(ops.labelReferences, labelReference{ops.sourceLine, pc, label})\n}\n\nfunc (ops *OpStream) tpush(argType StackType) {\n\tops.typeStack = append(ops.typeStack, argType)\n}\n\nfunc (ops *OpStream) tpusha(argType []StackType) {\n\tops.typeStack = append(ops.typeStack, argType...)\n}\n\nfunc (ops *OpStream) tpop() (argType StackType) {\n\tif len(ops.typeStack) == 0 {\n\t\targType = StackNone\n\t\treturn\n\t}\n\tlast := len(ops.typeStack) - 1\n\targType = ops.typeStack[last]\n\tops.typeStack = ops.typeStack[:last]\n\treturn\n}\n\n// Intc writes opcodes for loading a uint64 constant onto the stack.\nfunc (ops *OpStream) Intc(constIndex uint) error {\n\tswitch constIndex {\n\tcase 0:\n\t\tops.Out.WriteByte(0x22) // intc_0\n\tcase 1:\n\t\tops.Out.WriteByte(0x23) // intc_1\n\tcase 2:\n\t\tops.Out.WriteByte(0x24) // intc_2\n\tcase 3:\n\t\tops.Out.WriteByte(0x25) // intc_3\n\tdefault:\n\t\tif constIndex > 0xff {\n\t\t\treturn ops.error(\"cannot have more than 256 int constants\")\n\t\t}\n\t\tops.Out.WriteByte(0x21) // intc\n\t\tops.Out.WriteByte(uint8(constIndex))\n\t}\n\tif constIndex >= uint(len(ops.intc)) {\n\t\treturn ops.errorf(\"intc %d is not defined\", constIndex)\n\t}\n\tops.tpush(StackUint64)\n\treturn nil\n}\n\n// Uint writes opcodes for loading a uint literal\nfunc (ops *OpStream) Uint(val uint64) error {\n\tfound := false\n\tvar constIndex uint\n\tfor i, cv := range ops.intc {\n\t\tif cv == val {\n\t\t\tconstIndex = uint(i)\n\t\t\tfound = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !found {\n\t\tconstIndex = uint(len(ops.intc))\n\t\tops.intc = append(ops.intc, val)\n\t}\n\treturn ops.Intc(constIndex)\n}\n\n// Bytec writes opcodes for loading a []byte constant onto the stack.\nfunc (ops *OpStream) Bytec(constIndex uint) error {\n\tswitch constIndex {\n\tcase 0:\n\t\tops.Out.WriteByte(0x28) // bytec_0\n\tcase 1:\n\t\tops.Out.WriteByte(0x29) // bytec_1\n\tcase 2:\n\t\tops.Out.WriteByte(0x2a) // bytec_2\n\tcase 3:\n\t\tops.Out.WriteByte(0x2b) // bytec_3\n\tdefault:\n\t\tif constIndex > 0xff {\n\t\t\treturn ops.error(\"cannot have more than 256 byte constants\")\n\t\t}\n\t\tops.Out.WriteByte(0x27) // bytec\n\t\tops.Out.WriteByte(uint8(constIndex))\n\t}\n\tif constIndex >= uint(len(ops.bytec)) {\n\t\treturn ops.errorf(\"bytec %d is not defined\", constIndex)\n\t}\n\tops.trace(\"bytec %d %s\", constIndex, hex.EncodeToString(ops.bytec[constIndex]))\n\tops.tpush(StackBytes)\n\treturn nil\n}\n\n// ByteLiteral writes opcodes and data for loading a []byte literal\n// Values are accumulated so that they can be put into a bytecblock\nfunc (ops *OpStream) ByteLiteral(val []byte) error {\n\tfound := false\n\tvar constIndex uint\n\tfor i, cv := range ops.bytec {\n\t\tif bytes.Compare(cv, val) == 0 {\n\t\t\tfound = true\n\t\t\tconstIndex = uint(i)\n\t\t\tbreak\n\t\t}\n\t}\n\tif !found {\n\t\tconstIndex = uint(len(ops.bytec))\n\t\tops.bytec = append(ops.bytec, val)\n\t}\n\treturn ops.Bytec(constIndex)\n}\n\n// Arg writes opcodes for loading from Lsig.Args\nfunc (ops *OpStream) Arg(val uint64) error {\n\tswitch val {\n\tcase 0:\n\t\tops.Out.WriteByte(0x2d) // arg_0\n\tcase 1:\n\t\tops.Out.WriteByte(0x2e) // arg_1\n\tcase 2:\n\t\tops.Out.WriteByte(0x2f) // arg_2\n\tcase 3:\n\t\tops.Out.WriteByte(0x30) // arg_3\n\tdefault:\n\t\tif val > 0xff {\n\t\t\treturn ops.error(\"cannot have more than 256 args\")\n\t\t}\n\t\tops.Out.WriteByte(0x2c)\n\t\tops.Out.WriteByte(uint8(val))\n\t}\n\tops.tpush(StackBytes)\n\treturn nil\n}\n\n// Txn writes opcodes for loading a field from the current transaction\nfunc (ops *OpStream) Txn(val uint64) error {\n\tif val >= uint64(len(TxnFieldNames)) {\n\t\treturn ops.errorf(\"invalid txn field: %d\", val)\n\t}\n\tops.Out.WriteByte(0x31)\n\tops.Out.WriteByte(uint8(val))\n\tops.tpush(TxnFieldTypes[val])\n\treturn nil\n}\n\n// Txna writes opcodes for loading array field from the current transaction\nfunc (ops *OpStream) Txna(fieldNum uint64, arrayFieldIdx uint64) error {\n\tif fieldNum >= uint64(len(TxnFieldNames)) {\n\t\treturn ops.errorf(\"invalid txn field: %d\", fieldNum)\n\t}\n\tif arrayFieldIdx > 255 {\n\t\treturn ops.errorf(\"txna array index beyond 255: %d\", arrayFieldIdx)\n\t}\n\tops.Out.WriteByte(0x36)\n\tops.Out.WriteByte(uint8(fieldNum))\n\tops.Out.WriteByte(uint8(arrayFieldIdx))\n\tops.tpush(TxnFieldTypes[fieldNum])\n\treturn nil\n}\n\n// Gtxn writes opcodes for loading a field from the current transaction\nfunc (ops *OpStream) Gtxn(gid, val uint64) error {\n\tif val >= uint64(len(TxnFieldNames)) {\n\t\treturn ops.errorf(\"invalid txn field: %d\", val)\n\t}\n\tif gid > 255 {\n\t\treturn ops.errorf(\"gtxn transaction index beyond 255: %d\", gid)\n\t}\n\tops.Out.WriteByte(0x33)\n\tops.Out.WriteByte(uint8(gid))\n\tops.Out.WriteByte(uint8(val))\n\tops.tpush(TxnFieldTypes[val])\n\treturn nil\n}\n\n// Gtxna writes opcodes for loading an array field from the current transaction\nfunc (ops *OpStream) Gtxna(gid, fieldNum uint64, arrayFieldIdx uint64) error {\n\tif fieldNum >= uint64(len(TxnFieldNames)) {\n\t\treturn ops.errorf(\"invalid txn field: %d\", fieldNum)\n\t}\n\tif gid > 255 {\n\t\treturn ops.errorf(\"gtxna group index beyond 255: %d\", gid)\n\t}\n\tif arrayFieldIdx > 255 {\n\t\treturn ops.errorf(\"gtxna array index beyond 255: %d\", arrayFieldIdx)\n\t}\n\tops.Out.WriteByte(0x37)\n\tops.Out.WriteByte(uint8(gid))\n\tops.Out.WriteByte(uint8(fieldNum))\n\tops.Out.WriteByte(uint8(arrayFieldIdx))\n\tops.tpush(TxnFieldTypes[fieldNum])\n\treturn nil\n}\n\n// Global writes opcodes for loading an evaluator-global field\nfunc (ops *OpStream) Global(val uint64) error {\n\tif val >= uint64(len(GlobalFieldNames)) {\n\t\treturn ops.errorf(\"invalid global field: %d\", val)\n\t}\n\tops.Out.WriteByte(0x32)\n\tops.Out.WriteByte(uint8(val))\n\tops.trace(\"%s (%s)\", GlobalFieldNames[val], GlobalFieldTypes[val].String())\n\tops.tpush(GlobalFieldTypes[val])\n\treturn nil\n}\n\n// AssetHolding writes opcodes for accessing data from AssetHolding\nfunc (ops *OpStream) AssetHolding(val uint64) error {\n\tif val >= uint64(len(AssetHoldingFieldNames)) {\n\t\treturn ops.errorf(\"invalid asset holding field: %d\", val)\n\t}\n\tops.Out.WriteByte(opsByName[ops.Version][\"asset_holding_get\"].Opcode)\n\tops.Out.WriteByte(uint8(val))\n\tops.tpush(AssetHoldingFieldTypes[val])\n\tops.tpush(StackUint64)\n\treturn nil\n}\n\n// AssetParams writes opcodes for accessing data from AssetParams\nfunc (ops *OpStream) AssetParams(val uint64) error {\n\tif val >= uint64(len(AssetParamsFieldNames)) {\n\t\treturn ops.errorf(\"invalid asset params field: %d\", val)\n\t}\n\tops.Out.WriteByte(opsByName[ops.Version][\"asset_params_get\"].Opcode)\n\tops.Out.WriteByte(uint8(val))\n\tops.tpush(AssetParamsFieldTypes[val])\n\tops.tpush(StackUint64)\n\treturn nil\n}\n\nfunc assembleInt(ops *OpStream, spec *OpSpec, args []string) error {\n\t// check friendly TypeEnum constants\n\tte, isTypeEnum := txnTypeConstToUint64[args[0]]\n\tif isTypeEnum {\n\t\treturn ops.Uint(uint64(te))\n\t}\n\t// check raw transaction type strings\n\ttt, isTypeStr := txnTypeIndexes[args[0]]\n\tif isTypeStr {\n\t\treturn ops.Uint(uint64(tt))\n\t}\n\t// check OnCompetion constants\n\toc, isOCStr := onCompletionConstToUint64[args[0]]\n\tif isOCStr {\n\t\treturn ops.Uint(uint64(oc))\n\t}\n\tval, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.Uint(val)\n}\n\n// Explicit invocation of const lookup and push\nfunc assembleIntC(ops *OpStream, spec *OpSpec, args []string) error {\n\tconstIndex, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.Intc(uint(constIndex))\n}\nfunc assembleByteC(ops *OpStream, spec *OpSpec, args []string) error {\n\tconstIndex, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.Bytec(uint(constIndex))\n}\n\nfunc base32DecdodeAnyPadding(x string) (val []byte, err error) {\n\tval, err = base32.StdEncoding.WithPadding(base32.NoPadding).DecodeString(x)\n\tif err != nil {\n\t\t// try again with standard padding\n\t\tvar e2 error\n\t\tval, e2 = base32.StdEncoding.DecodeString(x)\n\t\tif e2 == nil {\n\t\t\terr = nil\n\t\t}\n\t}\n\treturn\n}\n\nfunc parseBinaryArgs(args []string) (val []byte, consumed int, err error) {\n\targ := args[0]\n\tif strings.HasPrefix(arg, \"base32(\") || strings.HasPrefix(arg, \"b32(\") {\n\t\topen := strings.IndexRune(arg, '(')\n\t\tclose := strings.IndexRune(arg, ')')\n\t\tif close == -1 {\n\t\t\terr = errors.New(\"byte base32 arg lacks close paren\")\n\t\t\treturn\n\t\t}\n\t\tval, err = base32DecdodeAnyPadding(arg[open+1 : close])\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tconsumed = 1\n\t} else if strings.HasPrefix(arg, \"base64(\") || strings.HasPrefix(arg, \"b64(\") {\n\t\topen := strings.IndexRune(arg, '(')\n\t\tclose := strings.IndexRune(arg, ')')\n\t\tif close == -1 {\n\t\t\terr = errors.New(\"byte base64 arg lacks close paren\")\n\t\t\treturn\n\t\t}\n\t\tval, err = base64.StdEncoding.DecodeString(arg[open+1 : close])\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tconsumed = 1\n\t} else if strings.HasPrefix(arg, \"0x\") {\n\t\tval, err = hex.DecodeString(arg[2:])\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tconsumed = 1\n\t} else if arg == \"base32\" || arg == \"b32\" {\n\t\tif len(args) < 2 {\n\t\t\terr = fmt.Errorf(\"need literal after 'byte %s'\", arg)\n\t\t\treturn\n\t\t}\n\t\tval, err = base32DecdodeAnyPadding(args[1])\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tconsumed = 2\n\t} else if arg == \"base64\" || arg == \"b64\" {\n\t\tif len(args) < 2 {\n\t\t\terr = fmt.Errorf(\"need literal after 'byte %s'\", arg)\n\t\t\treturn\n\t\t}\n\t\tval, err = base64.StdEncoding.DecodeString(args[1])\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tconsumed = 2\n\t} else if len(arg) > 1 && arg[0] == '\"' && arg[len(arg)-1] == '\"' {\n\t\tval, err = parseStringLiteral(arg)\n\t\tconsumed = 1\n\t} else {\n\t\terr = fmt.Errorf(\"byte arg did not parse: %v\", arg)\n\t\treturn\n\t}\n\treturn\n}\n\nfunc parseStringLiteral(input string) (result []byte, err error) {\n\tstart := 0\n\tend := len(input) - 1\n\tif input[start] != '\"' || input[end] != '\"' {\n\t\treturn nil, fmt.Errorf(\"no quotes\")\n\t}\n\tstart++\n\n\tescapeSeq := false\n\thexSeq := false\n\tresult = make([]byte, 0, end-start+1)\n\n\t// skip first and last quotes\n\tpos := start\n\tfor pos < end {\n\t\tchar := input[pos]\n\t\tif char == '\\\\' && !escapeSeq {\n\t\t\tif hexSeq {\n\t\t\t\treturn nil, fmt.Errorf(\"escape seq inside hex number\")\n\t\t\t}\n\t\t\tescapeSeq = true\n\t\t\tpos++\n\t\t\tcontinue\n\t\t}\n\t\tif escapeSeq {\n\t\t\tescapeSeq = false\n\t\t\tswitch char {\n\t\t\tcase 'n':\n\t\t\t\tchar = '\\n'\n\t\t\tcase 'r':\n\t\t\t\tchar = '\\r'\n\t\t\tcase 't':\n\t\t\t\tchar = '\\t'\n\t\t\tcase '\\\\':\n\t\t\t\tchar = '\\\\'\n\t\t\tcase '\"':\n\t\t\t\tchar = '\"'\n\t\t\tcase 'x':\n\t\t\t\thexSeq = true\n\t\t\t\tpos++\n\t\t\t\tcontinue\n\t\t\tdefault:\n\t\t\t\treturn nil, fmt.Errorf(\"invalid escape seq \\\\%c\", char)\n\t\t\t}\n\t\t}\n\t\tif hexSeq {\n\t\t\thexSeq = false\n\t\t\tif pos >= len(input)-2 { // count a closing quote\n\t\t\t\treturn nil, fmt.Errorf(\"non-terminated hex seq\")\n\t\t\t}\n\t\t\tnum, err := strconv.ParseUint(input[pos:pos+2], 16, 8)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tchar = uint8(num)\n\t\t\tpos++\n\t\t}\n\n\t\tresult = append(result, char)\n\t\tpos++\n\t}\n\tif escapeSeq || hexSeq {\n\t\treturn nil, fmt.Errorf(\"non-terminated escape seq\")\n\t}\n\n\treturn\n}\n\n// byte {base64,b64,base32,b32}(...)\n// byte {base64,b64,base32,b32} ...\n// byte 0x....\n// byte \"this is a string\\n\"\nfunc assembleByte(ops *OpStream, spec *OpSpec, args []string) error {\n\tvar val []byte\n\tvar err error\n\tif len(args) == 0 {\n\t\treturn ops.error(\"byte operation needs byte literal argument\")\n\t}\n\tval, _, err = parseBinaryArgs(args)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.ByteLiteral(val)\n}\n\nfunc assembleIntCBlock(ops *OpStream, spec *OpSpec, args []string) error {\n\tops.Out.WriteByte(0x20) // intcblock\n\tvar scratch [binary.MaxVarintLen64]byte\n\tl := binary.PutUvarint(scratch[:], uint64(len(args)))\n\tops.Out.Write(scratch[:l])\n\tops.intc = make([]uint64, len(args))\n\tfor i, xs := range args {\n\t\tcu, err := strconv.ParseUint(xs, 0, 64)\n\t\tif err != nil {\n\t\t\treturn ops.error(err)\n\t\t}\n\t\tl = binary.PutUvarint(scratch[:], cu)\n\t\tops.Out.Write(scratch[:l])\n\t\tops.intc[i] = cu\n\t}\n\tops.noIntcBlock = true\n\treturn nil\n}\n\nfunc assembleByteCBlock(ops *OpStream, spec *OpSpec, args []string) error {\n\tops.Out.WriteByte(0x26) // bytecblock\n\tbvals := make([][]byte, 0, len(args))\n\trest := args\n\tfor len(rest) > 0 {\n\t\tval, consumed, err := parseBinaryArgs(rest)\n\t\tif err != nil {\n\t\t\treturn ops.error(err)\n\t\t}\n\t\tbvals = append(bvals, val)\n\t\trest = rest[consumed:]\n\t}\n\tvar scratch [binary.MaxVarintLen64]byte\n\tl := binary.PutUvarint(scratch[:], uint64(len(bvals)))\n\tops.Out.Write(scratch[:l])\n\tfor _, bv := range bvals {\n\t\tl := binary.PutUvarint(scratch[:], uint64(len(bv)))\n\t\tops.Out.Write(scratch[:l])\n\t\tops.Out.Write(bv)\n\t}\n\tops.bytec = bvals\n\tops.noBytecBlock = true\n\treturn nil\n}\n\n// addr A1EU...\n// parses base32-with-checksum account address strings into a byte literal\nfunc assembleAddr(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"addr operation needs one argument\")\n\t}\n\taddr, err := basics.UnmarshalChecksumAddress(args[0])\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.ByteLiteral(addr[:])\n}\n\nfunc assembleArg(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"arg operation needs one argument\")\n\t}\n\tval, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn ops.Arg(val)\n}\n\nfunc assembleBranch(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"branch operation needs label argument\")\n\t}\n\tops.ReferToLabel(ops.Out.Len(), args[0])\n\terr := ops.checkArgs(*spec)\n\tif err != nil {\n\t\treturn err\n\t}\n\tops.Out.WriteByte(spec.Opcode)\n\t// zero bytes will get replaced with actual offset in resolveLabels()\n\tops.Out.WriteByte(0)\n\tops.Out.WriteByte(0)\n\treturn nil\n}\n\nfunc assembleLoad(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"load operation needs one argument\")\n\t}\n\tval, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tif val > EvalMaxScratchSize {\n\t\treturn ops.errorf(\"load outside 0..255: %d\", val)\n\t}\n\tops.Out.WriteByte(0x34)\n\tops.Out.WriteByte(byte(val))\n\tops.tpush(StackAny)\n\treturn nil\n}\n\nfunc assembleStore(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"store operation needs one argument\")\n\t}\n\tval, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tif val > EvalMaxScratchSize {\n\t\treturn ops.errorf(\"store outside 0..255: %d\", val)\n\t}\n\terr = ops.checkArgs(*spec)\n\tif err != nil {\n\t\treturn err\n\t}\n\tops.Out.WriteByte(spec.Opcode)\n\tops.Out.WriteByte(byte(val))\n\treturn nil\n}\n\nfunc assembleSubstring(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 2 {\n\t\treturn ops.error(\"substring expects 2 args\")\n\t}\n\tstart, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tif start > EvalMaxScratchSize {\n\t\treturn ops.error(\"substring limited to 0..255\")\n\t}\n\tend, err := strconv.ParseUint(args[1], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tif end > EvalMaxScratchSize {\n\t\treturn ops.error(\"substring limited to 0..255\")\n\t}\n\n\tif end < start {\n\t\treturn ops.error(\"substring end is before start\")\n\t}\n\topcode := byte(0x51)\n\terr = ops.checkArgs(*spec)\n\tif err != nil {\n\t\treturn err\n\t}\n\tops.Out.WriteByte(opcode)\n\tops.Out.WriteByte(byte(start))\n\tops.Out.WriteByte(byte(end))\n\tops.trace(\" pushes([]byte)\")\n\tops.tpush(StackBytes)\n\treturn nil\n}\n\nfunc disSubstring(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 2\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tstart := uint(dis.program[dis.pc+1])\n\tend := uint(dis.program[dis.pc+2])\n\tdis.nextpc = dis.pc + 3\n\t_, dis.err = fmt.Fprintf(dis.out, \"substring %d %d\\n\", start, end)\n}\n\nfunc assembleTxn(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"txn expects one argument\")\n\t}\n\tfs, ok := txnFieldSpecByName[args[0]]\n\tif !ok {\n\t\treturn ops.errorf(\"txn unknown arg: %v\", args[0])\n\t}\n\t_, ok = txnaFieldSpecByField[fs.field]\n\tif ok {\n\t\treturn ops.errorf(\"found txna field %v in txn op\", args[0])\n\t}\n\tif fs.version > ops.Version {\n\t\treturn ops.errorf(\"txn %s available in version %d. Missed #pragma version?\", args[0], fs.version)\n\t}\n\tval := fs.field\n\treturn ops.Txn(uint64(val))\n}\n\n// assembleTxn2 delegates to assembleTxn or assembleTxna depending on number of operands\nfunc assembleTxn2(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) == 1 {\n\t\treturn assembleTxn(ops, spec, args)\n\t}\n\tif len(args) == 2 {\n\t\treturn assembleTxna(ops, spec, args)\n\t}\n\treturn ops.error(\"txn expects one or two arguments\")\n}\n\nfunc assembleTxna(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 2 {\n\t\treturn ops.error(\"txna expects two arguments\")\n\t}\n\tfs, ok := txnFieldSpecByName[args[0]]\n\tif !ok {\n\t\treturn ops.errorf(\"txna unknown arg: %v\", args[0])\n\t}\n\t_, ok = txnaFieldSpecByField[fs.field]\n\tif !ok {\n\t\treturn ops.errorf(\"txna unknown arg: %v\", args[0])\n\t}\n\tif fs.version > ops.Version {\n\t\treturn ops.errorf(\"txna %s available in version %d. Missed #pragma version?\", args[0], fs.version)\n\t}\n\tarrayFieldIdx, err := strconv.ParseUint(args[1], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tfieldNum := fs.field\n\treturn ops.Txna(uint64(fieldNum), uint64(arrayFieldIdx))\n}\n\nfunc assembleGtxn(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 2 {\n\t\treturn ops.error(\"gtxn expects two arguments\")\n\t}\n\tgtid, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tfs, ok := txnFieldSpecByName[args[1]]\n\tif !ok {\n\t\treturn ops.errorf(\"gtxn unknown arg: %v\", args[1])\n\t}\n\t_, ok = txnaFieldSpecByField[fs.field]\n\tif ok {\n\t\treturn ops.errorf(\"found gtxna field %v in gtxn op\", args[1])\n\t}\n\tif fs.version > ops.Version {\n\t\treturn ops.errorf(\"gtxn %s available in version %d. Missed #pragma version?\", args[1], fs.version)\n\t}\n\tval := fs.field\n\treturn ops.Gtxn(gtid, uint64(val))\n}\n\nfunc assembleGtxn2(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) == 2 {\n\t\treturn assembleGtxn(ops, spec, args)\n\t}\n\tif len(args) == 3 {\n\t\treturn assembleGtxna(ops, spec, args)\n\t}\n\treturn ops.error(\"gtxn expects two or three arguments\")\n}\n\nfunc assembleGtxna(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 3 {\n\t\treturn ops.error(\"gtxna expects three arguments\")\n\t}\n\tgtid, err := strconv.ParseUint(args[0], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tfs, ok := txnFieldSpecByName[args[1]]\n\tif !ok {\n\t\treturn ops.errorf(\"gtxna unknown arg: %v\", args[1])\n\t}\n\t_, ok = txnaFieldSpecByField[fs.field]\n\tif !ok {\n\t\treturn ops.errorf(\"gtxna unknown arg: %v\", args[1])\n\t}\n\tif fs.version > ops.Version {\n\t\treturn ops.errorf(\"gtxna %s available in version %d. Missed #pragma version?\", args[1], fs.version)\n\t}\n\tarrayFieldIdx, err := strconv.ParseUint(args[2], 0, 64)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\tfieldNum := fs.field\n\treturn ops.Gtxna(gtid, uint64(fieldNum), uint64(arrayFieldIdx))\n}\n\nfunc assembleGlobal(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"global expects one argument\")\n\t}\n\tfs, ok := globalFieldSpecByName[args[0]]\n\tif !ok {\n\t\treturn ops.errorf(\"global unknown arg: %v\", args[0])\n\t}\n\tif fs.version > ops.Version {\n\t\treturn ops.errorf(\"global %s available in version %d. Missed #pragma version?\", args[0], fs.version)\n\t}\n\tval := fs.gfield\n\treturn ops.Global(uint64(val))\n}\n\nfunc assembleAssetHolding(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"asset_holding_get expects one argument\")\n\t}\n\tval, ok := assetHoldingFields[args[0]]\n\tif !ok {\n\t\treturn ops.errorf(\"asset_holding_get unknown arg: %v\", args[0])\n\t}\n\treturn ops.AssetHolding(uint64(val))\n}\n\nfunc assembleAssetParams(ops *OpStream, spec *OpSpec, args []string) error {\n\tif len(args) != 1 {\n\t\treturn ops.error(\"asset_params_get expects one argument\")\n\t}\n\tval, ok := assetParamsFields[args[0]]\n\tif !ok {\n\t\treturn ops.errorf(\"asset_params_get unknown arg: %v\", args[0])\n\t}\n\treturn ops.AssetParams(uint64(val))\n}\n\ntype assembleFunc func(*OpStream, *OpSpec, []string) error\n\nfunc asmDefault(ops *OpStream, spec *OpSpec, args []string) error {\n\terr := ops.checkArgs(*spec)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(spec.Returns) > 0 {\n\t\tops.tpusha(spec.Returns)\n\t\tops.trace(\" pushes(%s\", spec.Returns[0].String())\n\t\tif len(spec.Returns) > 1 {\n\t\t\tfor _, rt := range spec.Returns[1:] {\n\t\t\t\tops.trace(\", %s\", rt.String())\n\t\t\t}\n\t\t}\n\t\tops.trace(\")\")\n\t}\n\terr = ops.Out.WriteByte(spec.Opcode)\n\tif err != nil {\n\t\treturn ops.error(err)\n\t}\n\treturn nil\n}\n\n// keywords handle parsing and assembling special asm language constructs like 'addr'\nvar keywords map[string]assembleFunc\n\nfunc init() {\n\t// WARNING: special case op assembly by argOps functions must do their own type stack maintenance via ops.tpop() ops.tpush()/ops.tpusha()\n\tkeywords = make(map[string]assembleFunc)\n\tkeywords[\"int\"] = assembleInt\n\tkeywords[\"byte\"] = assembleByte\n\tkeywords[\"addr\"] = assembleAddr // parse basics.Address, actually just another []byte constant\n\t// WARNING: special case op assembly by argOps functions must do their own type stack maintenance via ops.tpop() ops.tpush()/ops.tpusha()\n}\n\ntype lineError struct {\n\tLine int\n\tErr  error\n}\n\nfunc fmtLineError(line int, format string, args ...interface{}) error {\n\treturn &lineError{Line: line, Err: fmt.Errorf(format, args...)}\n}\n\nfunc (le *lineError) Error() string {\n\treturn fmt.Sprintf(\"%d: %s\", le.Line, le.Err.Error())\n}\n\nfunc (le *lineError) Unwrap() error {\n\treturn le.Err\n}\n\nfunc typecheck(expected, got StackType) bool {\n\t// Some ops push 'any' and we wait for run time to see what it is.\n\t// Some of those 'any' are based on fields that we _could_ know now but haven't written a more detailed system of typecheck for (yet).\n\tif (expected == StackAny) || (got == StackAny) {\n\t\treturn true\n\t}\n\treturn expected == got\n}\n\nvar spaces = [256]uint8{'\\t': 1, ' ': 1}\n\nfunc fieldsFromLine(line string) []string {\n\tvar fields []string\n\n\ti := 0\n\tfor i < len(line) && spaces[line[i]] != 0 {\n\t\ti++\n\t}\n\n\tstart := i\n\tinString := false\n\tinBase64 := false\n\tfor i < len(line) {\n\t\tif spaces[line[i]] == 0 { // if not space\n\t\t\tswitch line[i] {\n\t\t\tcase '\"': // is a string literal?\n\t\t\t\tif !inString {\n\t\t\t\t\tif i == 0 || i > 0 && spaces[line[i-1]] != 0 {\n\t\t\t\t\t\tinString = true\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif line[i-1] != '\\\\' { // if not escape symbol\n\t\t\t\t\t\tinString = false\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase '/': // is a comment?\n\t\t\t\tif i < len(line)-1 && line[i+1] == '/' && !inBase64 && !inString {\n\t\t\t\t\tif start != i { // if a comment without whitespace\n\t\t\t\t\t\tfields = append(fields, line[start:i])\n\t\t\t\t\t}\n\t\t\t\t\treturn fields\n\t\t\t\t}\n\t\t\tcase '(': // is base64( seq?\n\t\t\t\tprefix := line[start:i]\n\t\t\t\tif prefix == \"base64\" || prefix == \"b64\" {\n\t\t\t\t\tinBase64 = true\n\t\t\t\t}\n\t\t\tcase ')': // is ) as base64( completion\n\t\t\t\tif inBase64 {\n\t\t\t\t\tinBase64 = false\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t}\n\t\t\ti++\n\t\t\tcontinue\n\t\t}\n\t\tif !inString {\n\t\t\tfield := line[start:i]\n\t\t\tfields = append(fields, field)\n\t\t\tif field == \"base64\" || field == \"b64\" {\n\t\t\t\tinBase64 = true\n\t\t\t} else if inBase64 {\n\t\t\t\tinBase64 = false\n\t\t\t}\n\t\t}\n\t\ti++\n\n\t\tif !inString {\n\t\t\tfor i < len(line) && spaces[line[i]] != 0 {\n\t\t\t\ti++\n\t\t\t}\n\t\t\tstart = i\n\t\t}\n\t}\n\n\t// add rest of the string if any\n\tif start < len(line) {\n\t\tfields = append(fields, line[start:i])\n\t}\n\n\treturn fields\n}\n\nfunc (ops *OpStream) trace(format string, args ...interface{}) {\n\tif ops.Trace == nil {\n\t\treturn\n\t}\n\tfmt.Fprintf(ops.Trace, format, args...)\n}\n\n// checks (and pops) arg types from arg type stack\nfunc (ops *OpStream) checkArgs(spec OpSpec) error {\n\tfirstPop := true\n\tfor i := len(spec.Args) - 1; i >= 0; i-- {\n\t\targType := spec.Args[i]\n\t\tstype := ops.tpop()\n\t\tif firstPop {\n\t\t\tfirstPop = false\n\t\t\tops.trace(\"pops(%s\", argType.String())\n\t\t} else {\n\t\t\tops.trace(\", %s\", argType.String())\n\t\t}\n\t\tif !typecheck(argType, stype) {\n\t\t\tmsg := fmt.Sprintf(\"%s arg %d wanted type %s got %s\", spec.Name, i, argType.String(), stype.String())\n\t\t\tif len(ops.labelReferences) > 0 {\n\t\t\t\tfmt.Fprintf(ops.Stderr, \"warning: %d: %s; but branches have happened and assembler does not precisely track types in this case\\n\", ops.sourceLine, msg)\n\t\t\t} else {\n\t\t\t\treturn ops.error(msg)\n\t\t\t}\n\t\t}\n\t}\n\tif !firstPop {\n\t\tops.trace(\")\")\n\t}\n\treturn nil\n}\n\n// assemble reads text from an input and accumulates the program\nfunc (ops *OpStream) assemble(fin io.Reader) error {\n\tscanner := bufio.NewScanner(fin)\n\tops.sourceLine = 0\n\tfor scanner.Scan() {\n\t\tops.sourceLine++\n\t\tline := scanner.Text()\n\t\tif len(line) == 0 {\n\t\t\tops.trace(\"%d: 0 line\\n\", ops.sourceLine)\n\t\t\tcontinue\n\t\t}\n\t\tif strings.HasPrefix(line, \"//\") {\n\t\t\tops.trace(\"%d: // line\\n\", ops.sourceLine)\n\t\t\tcontinue\n\t\t}\n\t\tif strings.HasPrefix(line, \"#pragma\") {\n\t\t\t// all pragmas must be be already processed in advance\n\t\t\tops.trace(\"%d: #pragma line\\n\", ops.sourceLine)\n\t\t\tcontinue\n\t\t}\n\t\tfields := fieldsFromLine(line)\n\t\tif len(fields) == 0 {\n\t\t\tops.trace(\"%d: no fields\\n\", ops.sourceLine)\n\t\t\tcontinue\n\t\t}\n\t\topstring := fields[0]\n\t\tspec, ok := opsByName[ops.Version][opstring]\n\t\tvar asmFunc assembleFunc\n\t\tif ok {\n\t\t\tasmFunc = spec.asm\n\t\t} else {\n\t\t\tkwFunc, ok := keywords[opstring]\n\t\t\tif ok {\n\t\t\t\tasmFunc = kwFunc\n\t\t\t}\n\t\t}\n\t\tif asmFunc != nil {\n\t\t\tops.trace(\"%3d: %s\\t\", ops.sourceLine, opstring)\n\t\t\tops.RecordSourceLine()\n\t\t\terr := asmFunc(ops, &spec, fields[1:])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tops.trace(\"\\n\")\n\t\t\tcontinue\n\t\t}\n\t\tif opstring[len(opstring)-1] == ':' {\n\t\t\t// create a label\n\t\t\terr := ops.SetLabelHere(opstring[:len(opstring)-1])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\treturn ops.errorf(\"unknown opcode: %v\", opstring)\n\t}\n\n\t// backward compatibility: do not allow jumps behind last instruction in TEAL v1\n\tif ops.Version <= 1 {\n\t\tfor label, dest := range ops.labels {\n\t\t\tif dest == ops.Out.Len() {\n\t\t\t\treturn ops.errorf(\"label %v is too far away\", label)\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: warn if expected resulting stack is not len==1 ?\n\treturn ops.resolveLabels()\n}\n\nfunc (ops *OpStream) resolveLabels() (err error) {\n\tif len(ops.labelReferences) == 0 {\n\t\treturn nil\n\t}\n\traw := ops.Out.Bytes()\n\tfor _, lr := range ops.labelReferences {\n\t\tdest, ok := ops.labels[lr.label]\n\t\tif !ok {\n\t\t\treturn fmtLineError(lr.sourceLine, \"reference to undefined label %v\", lr.label)\n\t\t}\n\t\t// all branch instructions (currently) are opcode byte and 2 offset bytes, and the destination is relative to the next pc as if the branch was a no-op\n\t\tnaturalPc := lr.position + 3\n\t\tif dest < naturalPc {\n\t\t\treturn fmtLineError(lr.sourceLine, \"label %v is before reference but only forward jumps are allowed\", lr.label)\n\t\t}\n\t\tjump := dest - naturalPc\n\t\tif jump > 0x7fff {\n\t\t\treturn fmtLineError(lr.sourceLine, \"label %v is too far away\", lr.label)\n\t\t}\n\t\traw[lr.position+1] = uint8(jump >> 8)\n\t\traw[lr.position+2] = uint8(jump & 0x0ff)\n\t}\n\tops.Out.Reset()\n\tops.Out.Write(raw)\n\treturn nil\n}\n\n// AssemblerDefaultVersion what version of code do we emit by default\n// AssemblerDefaultVersion is set to 1 on puprose\n// to prevent accidental building of v1 official templates with version 2\n// because these templates are not aware of rekeying.\nconst AssemblerDefaultVersion = 1\n\n// AssemblerMaxVersion is a maximum supported assembler version\nconst AssemblerMaxVersion = LogicVersion\nconst assemblerNoVersion = (^uint64(0))\n\n// Bytes returns the finished program bytes\nfunc (ops *OpStream) Bytes() (program []byte, err error) {\n\tvar scratch [binary.MaxVarintLen64]byte\n\tprebytes := bytes.Buffer{}\n\tvlen := binary.PutUvarint(scratch[:], ops.GetVersion())\n\tprebytes.Write(scratch[:vlen])\n\tif len(ops.intc) > 0 && !ops.noIntcBlock {\n\t\tprebytes.WriteByte(0x20) // intcblock\n\t\tvlen := binary.PutUvarint(scratch[:], uint64(len(ops.intc)))\n\t\tprebytes.Write(scratch[:vlen])\n\t\tfor _, iv := range ops.intc {\n\t\t\tvlen = binary.PutUvarint(scratch[:], iv)\n\t\t\tprebytes.Write(scratch[:vlen])\n\t\t}\n\t}\n\tif len(ops.bytec) > 0 && !ops.noBytecBlock {\n\t\tprebytes.WriteByte(0x26) // bytecblock\n\t\tvlen := binary.PutUvarint(scratch[:], uint64(len(ops.bytec)))\n\t\tprebytes.Write(scratch[:vlen])\n\t\tfor _, bv := range ops.bytec {\n\t\t\tvlen = binary.PutUvarint(scratch[:], uint64(len(bv)))\n\t\t\tprebytes.Write(scratch[:vlen])\n\t\t\tprebytes.Write(bv)\n\t\t}\n\t}\n\tif prebytes.Len() == 0 {\n\t\tprogram = ops.Out.Bytes()\n\t\treturn\n\t}\n\tpbl := prebytes.Len()\n\toutl := ops.Out.Len()\n\tout := make([]byte, pbl+outl)\n\tpl, err := prebytes.Read(out)\n\tif pl != pbl || err != nil {\n\t\terr = fmt.Errorf(\"wat: %d prebytes, %d to buffer? err=%s\", pbl, pl, err)\n\t\treturn\n\t}\n\tol, err := ops.Out.Read(out[pl:])\n\tif ol != outl || err != nil {\n\t\terr = fmt.Errorf(\"%d program bytes but %d to buffer. err=%s\", outl, ol, err)\n\t\treturn\n\t}\n\n\t// fixup offset to line mapping\n\tnewOffsetToLine := make(map[int]int, len(ops.offsetToLine))\n\tfor o, l := range ops.offsetToLine {\n\t\tnewOffsetToLine[o+pbl] = l\n\t}\n\tops.offsetToLine = newOffsetToLine\n\n\tprogram = out\n\treturn\n}\n\nfunc (ops *OpStream) error(problem interface{}) error {\n\tswitch p := problem.(type) {\n\tcase string:\n\t\treturn &lineError{Line: ops.sourceLine, Err: errors.New(p)}\n\tcase error:\n\t\treturn &lineError{Line: ops.sourceLine, Err: p}\n\tdefault:\n\t\treturn &lineError{Line: ops.sourceLine, Err: fmt.Errorf(\"%#v\", p)}\n\t}\n}\n\nfunc (ops *OpStream) errorf(format string, a ...interface{}) error {\n\treturn ops.error(fmt.Errorf(format, a...))\n}\n\n// AssembleString takes an entire program in a string and assembles it to bytecode using AssemblerDefaultVersion\nfunc AssembleString(text string) ([]byte, error) {\n\treturn AssembleStringWithVersion(text, assemblerNoVersion)\n}\n\n// AssembleStringV1 takes an entire program in a string and assembles it to bytecode using TEAL v1\nfunc AssembleStringV1(text string) ([]byte, error) {\n\treturn AssembleStringWithVersion(text, 1)\n}\n\n// AssembleStringV2 takes an entire program in a string and assembles it to bytecode using TEAL v2\nfunc AssembleStringV2(text string) ([]byte, error) {\n\treturn AssembleStringWithVersion(text, 2)\n}\n\n// AssembleStringWithVersion takes an entire program in a string and assembles it to bytecode using the assembler version specified\nfunc AssembleStringWithVersion(text string, version uint64) ([]byte, error) {\n\tprogram, _, err := AssembleStringWithVersionEx(text, version)\n\treturn program, err\n}\n\n// AssembleStringWithVersionEx takes an entire program in a string and assembles it to bytecode\n// using the assembler version specified.\n// If version is assemblerNoVersion it uses #pragma version or fallbacks to AssemblerDefaultVersion.\n// It also returns PC to source line mapping.\nfunc AssembleStringWithVersionEx(text string, version uint64) ([]byte, map[int]int, error) {\n\tsr := strings.NewReader(text)\n\tps := PragmaStream{}\n\terr := ps.Process(sr)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\t// If version not set yet then set either default or #pragma version.\n\t// We have to use assemblerNoVersion as a marker for non-specified version\n\t// because version 0 is valid version for TEAL v1\n\tif version == assemblerNoVersion {\n\t\tif ps.Version != 0 {\n\t\t\tversion = ps.Version\n\t\t} else {\n\t\t\tversion = AssemblerDefaultVersion\n\t\t}\n\t} else if ps.Version != 0 && version != ps.Version {\n\t\terr = fmt.Errorf(\"version mismatch: assembling v%d with v%d assembler\", ps.Version, version)\n\t\treturn nil, nil, err\n\t} else {\n\t\t// otherwise the passed version matches the pragma and we are ok\n\t}\n\n\tsr = strings.NewReader(text)\n\tops := OpStream{Version: version, Stderr: os.Stderr}\n\terr = ops.assemble(sr)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tprogram, err := ops.Bytes()\n\treturn program, ops.offsetToLine, err\n}\n\n// PragmaStream represents all parsed pragmas from the program\ntype PragmaStream struct {\n\tVersion uint64\n}\n\n// Process all pragmas in the input stream\nfunc (ps *PragmaStream) Process(fin io.Reader) (err error) {\n\tscanner := bufio.NewScanner(fin)\n\tsourceLine := 0\n\tfor scanner.Scan() {\n\t\tsourceLine++\n\t\tline := scanner.Text()\n\t\tif len(line) == 0 || !strings.HasPrefix(line, \"#pragma\") {\n\t\t\tcontinue\n\t\t}\n\n\t\tfields := strings.Split(line, \" \")\n\t\tif fields[0] != \"#pragma\" {\n\t\t\treturn fmtLineError(sourceLine, \"invalid syntax: %s\", fields[0])\n\t\t}\n\t\tif len(fields) < 2 {\n\t\t\treturn fmtLineError(sourceLine, \"empty pragma\")\n\t\t}\n\t\tkey := fields[1]\n\t\tswitch key {\n\t\tcase \"version\":\n\t\t\tif len(fields) < 3 {\n\t\t\t\treturn fmtLineError(sourceLine, \"no version value\")\n\t\t\t}\n\t\t\tvalue := fields[2]\n\t\t\tvar ver uint64\n\t\t\tif sourceLine != 1 {\n\t\t\t\treturn fmtLineError(sourceLine, \"#pragma version is only allowed on 1st line\")\n\t\t\t}\n\t\t\tver, err = strconv.ParseUint(value, 0, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn &lineError{Line: sourceLine, Err: err}\n\t\t\t}\n\t\t\tif ver < 1 || ver > AssemblerMaxVersion {\n\t\t\t\treturn fmtLineError(sourceLine, \"unsupported version: %d\", ver)\n\t\t\t}\n\t\t\tps.Version = ver\n\t\tdefault:\n\t\t\treturn fmtLineError(sourceLine, \"unsupported pragma directive: %s\", key)\n\t\t}\n\t}\n\treturn\n}\n\ntype disassembleState struct {\n\tprogram       []byte\n\tpc            int\n\tout           io.Writer\n\tlabelCount    int\n\tpendingLabels map[int]string\n\n\tnextpc int\n\terr    error\n}\n\nfunc (dis *disassembleState) putLabel(label string, target int) {\n\tif dis.pendingLabels == nil {\n\t\tdis.pendingLabels = make(map[int]string)\n\t}\n\tdis.pendingLabels[target] = label\n}\n\nfunc (dis *disassembleState) outputLabelIfNeeded() (err error) {\n\tif label, hasLabel := dis.pendingLabels[dis.pc]; hasLabel {\n\t\t_, err = fmt.Fprintf(dis.out, \"%s:\\n\", label)\n\t}\n\treturn\n}\n\ntype disassembleFunc func(dis *disassembleState, spec *OpSpec)\n\nfunc disDefault(dis *disassembleState, spec *OpSpec) {\n\tdis.nextpc = dis.pc + 1\n\t_, dis.err = fmt.Fprintf(dis.out, \"%s\\n\", spec.Name)\n}\n\nvar errShortIntcblock = errors.New(\"intcblock ran past end of program\")\nvar errTooManyIntc = errors.New(\"intcblock with too many items\")\n\nfunc parseIntcblock(program []byte, pc int) (intc []uint64, nextpc int, err error) {\n\tpos := pc + 1\n\tnumInts, bytesUsed := binary.Uvarint(program[pos:])\n\tif bytesUsed <= 0 {\n\t\terr = fmt.Errorf(\"could not decode int const block size at pc=%d\", pos)\n\t\treturn\n\t}\n\tpos += bytesUsed\n\tif numInts > uint64(len(program)) {\n\t\terr = errTooManyIntc\n\t\treturn\n\t}\n\tintc = make([]uint64, numInts)\n\tfor i := uint64(0); i < numInts; i++ {\n\t\tif pos >= len(program) {\n\t\t\terr = errShortIntcblock\n\t\t\treturn\n\t\t}\n\t\tintc[i], bytesUsed = binary.Uvarint(program[pos:])\n\t\tif bytesUsed <= 0 {\n\t\t\terr = fmt.Errorf(\"could not decode int const[%d] at pc=%d\", i, pos)\n\t\t\treturn\n\t\t}\n\t\tpos += bytesUsed\n\t}\n\tnextpc = pos\n\treturn\n}\n\nfunc checkIntConstBlock(cx *evalContext) int {\n\tpos := cx.pc + 1\n\tnumInts, bytesUsed := binary.Uvarint(cx.program[pos:])\n\tif bytesUsed <= 0 {\n\t\tcx.err = fmt.Errorf(\"could not decode int const block size at pc=%d\", pos)\n\t\treturn 1\n\t}\n\tpos += bytesUsed\n\tif numInts > uint64(len(cx.program)) {\n\t\tcx.err = errTooManyIntc\n\t\treturn 0\n\t}\n\t//intc = make([]uint64, numInts)\n\tfor i := uint64(0); i < numInts; i++ {\n\t\tif pos >= len(cx.program) {\n\t\t\tcx.err = errShortIntcblock\n\t\t\treturn 0\n\t\t}\n\t\t_, bytesUsed = binary.Uvarint(cx.program[pos:])\n\t\tif bytesUsed <= 0 {\n\t\t\tcx.err = fmt.Errorf(\"could not decode int const[%d] at pc=%d\", i, pos)\n\t\t\treturn 1\n\t\t}\n\t\tpos += bytesUsed\n\t}\n\tcx.nextpc = pos\n\treturn 1\n}\n\nvar errShortBytecblock = errors.New(\"bytecblock ran past end of program\")\nvar errTooManyItems = errors.New(\"bytecblock with too many items\")\n\nfunc parseBytecBlock(program []byte, pc int) (bytec [][]byte, nextpc int, err error) {\n\tpos := pc + 1\n\tnumItems, bytesUsed := binary.Uvarint(program[pos:])\n\tif bytesUsed <= 0 {\n\t\terr = fmt.Errorf(\"could not decode []byte const block size at pc=%d\", pos)\n\t\treturn\n\t}\n\tpos += bytesUsed\n\tif numItems > uint64(len(program)) {\n\t\terr = errTooManyItems\n\t\treturn\n\t}\n\tbytec = make([][]byte, numItems)\n\tfor i := uint64(0); i < numItems; i++ {\n\t\tif pos >= len(program) {\n\t\t\terr = errShortBytecblock\n\t\t\treturn\n\t\t}\n\t\titemLen, bytesUsed := binary.Uvarint(program[pos:])\n\t\tif bytesUsed <= 0 {\n\t\t\terr = fmt.Errorf(\"could not decode []byte const[%d] at pc=%d\", i, pos)\n\t\t\treturn\n\t\t}\n\t\tpos += bytesUsed\n\t\tif pos >= len(program) {\n\t\t\terr = errShortBytecblock\n\t\t\treturn\n\t\t}\n\t\tend := uint64(pos) + itemLen\n\t\tif end > uint64(len(program)) || end < uint64(pos) {\n\t\t\terr = errShortBytecblock\n\t\t\treturn\n\t\t}\n\t\tbytec[i] = program[pos : pos+int(itemLen)]\n\t\tpos += int(itemLen)\n\t}\n\tnextpc = pos\n\treturn\n}\n\nfunc checkByteConstBlock(cx *evalContext) int {\n\tpos := cx.pc + 1\n\tnumItems, bytesUsed := binary.Uvarint(cx.program[pos:])\n\tif bytesUsed <= 0 {\n\t\tcx.err = fmt.Errorf(\"could not decode []byte const block size at pc=%d\", pos)\n\t\treturn 1\n\t}\n\tpos += bytesUsed\n\tif numItems > uint64(len(cx.program)) {\n\t\tcx.err = errTooManyItems\n\t\treturn 0\n\t}\n\t//bytec = make([][]byte, numItems)\n\tfor i := uint64(0); i < numItems; i++ {\n\t\tif pos >= len(cx.program) {\n\t\t\tcx.err = errShortBytecblock\n\t\t\treturn 0\n\t\t}\n\t\titemLen, bytesUsed := binary.Uvarint(cx.program[pos:])\n\t\tif bytesUsed <= 0 {\n\t\t\tcx.err = fmt.Errorf(\"could not decode []byte const[%d] at pc=%d\", i, pos)\n\t\t\treturn 1\n\t\t}\n\t\tpos += bytesUsed\n\t\tif pos >= len(cx.program) {\n\t\t\tcx.err = errShortBytecblock\n\t\t\treturn 0\n\t\t}\n\t\tend := uint64(pos) + itemLen\n\t\tif end > uint64(len(cx.program)) || end < uint64(pos) {\n\t\t\tcx.err = errShortBytecblock\n\t\t\treturn 0\n\t\t}\n\t\t//bytec[i] = program[pos : pos+int(itemLen)]\n\t\tpos += int(itemLen)\n\t}\n\tcx.nextpc = pos\n\treturn 1\n}\n\nfunc disIntcblock(dis *disassembleState, spec *OpSpec) {\n\tvar intc []uint64\n\tintc, dis.nextpc, dis.err = parseIntcblock(dis.program, dis.pc)\n\tif dis.err != nil {\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"intcblock\")\n\tif dis.err != nil {\n\t\treturn\n\t}\n\tfor _, iv := range intc {\n\t\t_, dis.err = fmt.Fprintf(dis.out, \" %d\", iv)\n\t\tif dis.err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\t_, dis.err = dis.out.Write([]byte(\"\\n\"))\n}\n\nfunc disIntc(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\t_, dis.err = fmt.Fprintf(dis.out, \"intc %d\\n\", dis.program[dis.pc+1])\n}\n\nfunc disBytecblock(dis *disassembleState, spec *OpSpec) {\n\tvar bytec [][]byte\n\tbytec, dis.nextpc, dis.err = parseBytecBlock(dis.program, dis.pc)\n\tif dis.err != nil {\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"bytecblock\")\n\tif dis.err != nil {\n\t\treturn\n\t}\n\tfor _, bv := range bytec {\n\t\t_, dis.err = fmt.Fprintf(dis.out, \" 0x%s\", hex.EncodeToString(bv))\n\t\tif dis.err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\t_, dis.err = dis.out.Write([]byte(\"\\n\"))\n}\n\nfunc disBytec(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\t_, dis.err = fmt.Fprintf(dis.out, \"bytec %d\\n\", dis.program[dis.pc+1])\n}\n\nfunc disArg(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\t_, dis.err = fmt.Fprintf(dis.out, \"arg %d\\n\", dis.program[dis.pc+1])\n}\n\nfunc disTxn(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\ttxarg := dis.program[dis.pc+1]\n\tif int(txarg) >= len(TxnFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid txn arg index %d at pc=%d\", txarg, dis.pc)\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"txn %s\\n\", TxnFieldNames[txarg])\n}\n\nfunc disTxna(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 2\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 3\n\ttxarg := dis.program[dis.pc+1]\n\tif int(txarg) >= len(TxnFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid txn arg index %d at pc=%d\", txarg, dis.pc)\n\t\treturn\n\t}\n\tarrayFieldIdx := dis.program[dis.pc+2]\n\t_, dis.err = fmt.Fprintf(dis.out, \"txna %s %d\\n\", TxnFieldNames[txarg], arrayFieldIdx)\n}\n\nfunc disGtxn(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 2\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 3\n\tgi := dis.program[dis.pc+1]\n\ttxarg := dis.program[dis.pc+2]\n\tif int(txarg) >= len(TxnFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid txn arg index %d at pc=%d\", txarg, dis.pc)\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"gtxn %d %s\\n\", gi, TxnFieldNames[txarg])\n}\n\nfunc disGtxna(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 3\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 4\n\tgi := dis.program[dis.pc+1]\n\ttxarg := dis.program[dis.pc+2]\n\tif int(txarg) >= len(TxnFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid txn arg index %d at pc=%d\", txarg, dis.pc)\n\t\treturn\n\t}\n\tarrayFieldIdx := dis.program[dis.pc+3]\n\t_, dis.err = fmt.Fprintf(dis.out, \"gtxna %d %s %d\\n\", gi, TxnFieldNames[txarg], arrayFieldIdx)\n}\n\nfunc disGlobal(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\tgarg := dis.program[dis.pc+1]\n\tif int(garg) >= len(GlobalFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid global arg index %d at pc=%d\", garg, dis.pc)\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"global %s\\n\", GlobalFieldNames[garg])\n}\n\nfunc disBranch(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 2\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\n\tdis.nextpc = dis.pc + 3\n\toffset := (uint(dis.program[dis.pc+1]) << 8) | uint(dis.program[dis.pc+2])\n\ttarget := int(offset) + dis.pc + 3\n\tlabel, labelExists := dis.pendingLabels[target]\n\tif !labelExists {\n\t\tdis.labelCount++\n\t\tlabel = fmt.Sprintf(\"label%d\", dis.labelCount)\n\t\tdis.putLabel(label, target)\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"%s %s\\n\", spec.Name, label)\n}\n\nfunc disLoad(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tn := uint(dis.program[dis.pc+1])\n\tdis.nextpc = dis.pc + 2\n\t_, dis.err = fmt.Fprintf(dis.out, \"load %d\\n\", n)\n}\n\nfunc disStore(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tn := uint(dis.program[dis.pc+1])\n\tdis.nextpc = dis.pc + 2\n\t_, dis.err = fmt.Fprintf(dis.out, \"store %d\\n\", n)\n}\n\nfunc disAssetHolding(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\targ := dis.program[dis.pc+1]\n\tif int(arg) >= len(AssetHoldingFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid asset holding arg index %d at pc=%d\", arg, dis.pc)\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"asset_holding_get %s\\n\", AssetHoldingFieldNames[arg])\n}\n\nfunc disAssetParams(dis *disassembleState, spec *OpSpec) {\n\tlastIdx := dis.pc + 1\n\tif len(dis.program) <= lastIdx {\n\t\tmissing := lastIdx - len(dis.program) + 1\n\t\tdis.err = fmt.Errorf(\"unexpected %s opcode end: missing %d bytes\", spec.Name, missing)\n\t\treturn\n\t}\n\tdis.nextpc = dis.pc + 2\n\targ := dis.program[dis.pc+1]\n\tif int(arg) >= len(AssetParamsFieldNames) {\n\t\tdis.err = fmt.Errorf(\"invalid asset params arg index %d at pc=%d\", arg, dis.pc)\n\t\treturn\n\t}\n\t_, dis.err = fmt.Fprintf(dis.out, \"asset_params_get %s\\n\", AssetParamsFieldNames[arg])\n}\n\ntype disInfo struct {\n\tpcOffset       []PCOffset\n\thasStatefulOps bool\n}\n\n// disassembleInstrumented is like Disassemble, but additionally returns where\n// each program counter value maps in the disassembly\nfunc disassembleInstrumented(program []byte) (text string, ds disInfo, err error) {\n\tout := strings.Builder{}\n\tdis := disassembleState{program: program, out: &out}\n\tversion, vlen := binary.Uvarint(program)\n\tif vlen <= 0 {\n\t\tfmt.Fprintf(dis.out, \"// invalid version\\n\")\n\t\ttext = out.String()\n\t\treturn\n\t}\n\tif version > LogicVersion {\n\t\tfmt.Fprintf(dis.out, \"// unsupported version %d\\n\", version)\n\t\ttext = out.String()\n\t\treturn\n\t}\n\tfmt.Fprintf(dis.out, \"// version %d\\n\", version)\n\tdis.pc = vlen\n\tfor dis.pc < len(program) {\n\t\terr = dis.outputLabelIfNeeded()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\top := opsByOpcode[version][program[dis.pc]]\n\t\tif op.Modes == runModeApplication {\n\t\t\tds.hasStatefulOps = true\n\t\t}\n\t\tif op.Name == \"\" {\n\t\t\tds.pcOffset = append(ds.pcOffset, PCOffset{dis.pc, out.Len()})\n\t\t\tmsg := fmt.Sprintf(\"invalid opcode %02x at pc=%d\", program[dis.pc], dis.pc)\n\t\t\tout.WriteString(msg)\n\t\t\tout.WriteRune('\\n')\n\t\t\ttext = out.String()\n\t\t\terr = errors.New(msg)\n\t\t\treturn\n\t\t}\n\n\t\t// ds.pcOffset tracks where in the output each opcode maps to assembly\n\t\tds.pcOffset = append(ds.pcOffset, PCOffset{dis.pc, out.Len()})\n\n\t\t// Actually do the disassembly\n\t\top.dis(&dis, &op)\n\t\tif dis.err != nil {\n\t\t\terr = dis.err\n\t\t\treturn\n\t\t}\n\t\tdis.pc = dis.nextpc\n\t}\n\terr = dis.outputLabelIfNeeded()\n\tif err != nil {\n\t\treturn\n\t}\n\n\ttext = out.String()\n\treturn\n}\n\n// Disassemble produces a text form of program bytes.\n// AssembleString(Disassemble()) should result in the same program bytes.\nfunc Disassemble(program []byte) (text string, err error) {\n\ttext, _, err = disassembleInstrumented(program)\n\treturn\n}\n\n// HasStatefulOps checks if the program has stateful opcodes\nfunc HasStatefulOps(program []byte) (bool, error) {\n\t_, ds, err := disassembleInstrumented(program)\n\treturn ds.hasStatefulOps, err\n}\n", "idx": 7, "id": 40924, "msg": "", "proj": "algorand-go-algorand", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1,5 +1,9 @@\n class IndividualPlan < ActiveRecord::Base\n   PRIME_BASIC_SKU = 'prime-basic'\n+  PRIME_249_SKU = 'prime-249'\n+  PRIME_99_SKU = 'prime-99'\n+  PRIME_49_SKU = 'prime-49'\n+  PRIME_29_SKU = 'prime-29'\n \n   has_many :announcements, as: :announceable, dependent: :destroy\n   has_many :purchases, as: :purchaseable", "y": 1, "oldf": "class IndividualPlan < ActiveRecord::Base\n  PRIME_BASIC_SKU = 'prime-basic'\n\n  has_many :announcements, as: :announceable, dependent: :destroy\n  has_many :purchases, as: :purchaseable\n  has_many :subscriptions, as: :plan\n\n  validates :description, presence: true\n  validates :individual_price, presence: true\n  validates :name, presence: true\n  validates :short_description, presence: true\n  validates :sku, presence: true\n\n  include PlanForPublicListing\n\n  def self.active\n    where active: true\n  end\n\n  def self.default\n    active.featured.ordered.first\n  end\n\n  def self.basic\n    where(sku: PRIME_BASIC_SKU).first\n  end\n\n  def purchase_for(user)\n    purchases.paid.where(user_id: user).first\n  end\n\n  def starts_on(purchase_date)\n    purchase_date\n  end\n\n  def ends_on(purchase_date)\n    purchase_date\n  end\n\n  def subscription?\n    true\n  end\n\n  def subscription_interval\n    stripe_plan.interval\n  end\n\n  def offering_type\n    'subscription'\n  end\n\n  def fulfilled_with_github?\n    false\n  end\n\n  def announcement\n    @announcement ||= announcements.current\n  end\n\n  def fulfill(purchase, user)\n    SubscriptionFulfillment.new(purchase, user).fulfill\n  end\n\n  def after_purchase_url(controller, purchase)\n    controller.dashboard_path\n  end\n\n  private\n\n  def stripe_plan\n    @stripe_plan ||= Stripe::Plan.retrieve(sku)\n  end\nend\n", "idx": 1, "id": 9984, "msg": "Prefer double-quoted strings unless you need single quotes to avoid extra backslashes for escaping.", "proj": "thoughtbot-upcase", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -206,11 +206,18 @@ func (s *Service) retrieveChunk(ctx context.Context, addr swarm.Address, sp *ski\n \t// compute the peer's price for this chunk for price header\n \tchunkPrice := s.pricer.PeerPrice(peer, addr)\n \n+\t// Reserve to see whether we can request the chunk\n+\terr = s.accounting.Reserve(ctx, peer, chunkPrice)\n+\tif err != nil {\n+\t\treturn nil, peer, false, err\n+\t}\n+\tdefer s.accounting.Release(peer, chunkPrice)\n+\n \ts.logger.Tracef(\"retrieval: requesting chunk %s from peer %s\", addr, peer)\n \tstream, err := s.streamer.NewStream(ctx, peer, nil, protocolName, protocolVersion, streamName)\n \tif err != nil {\n \t\ts.metrics.TotalErrors.Inc()\n-\t\treturn nil, peer, fmt.Errorf(\"new stream: %w\", err)\n+\t\treturn nil, peer, false, fmt.Errorf(\"new stream: %w\", err)\n \t}\n \n \tdefer func() {", "y": 0, "oldf": "// Copyright 2020 The Swarm Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\n// Package retrieval provides the retrieval protocol\n// implementation. The protocol is used to retrieve\n// chunks over the network using forwarding-kademlia\n// routing.\npackage retrieval\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/ethersphere/bee/pkg/accounting\"\n\t\"github.com/ethersphere/bee/pkg/cac\"\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/p2p\"\n\t\"github.com/ethersphere/bee/pkg/p2p/protobuf\"\n\t\"github.com/ethersphere/bee/pkg/postage\"\n\t\"github.com/ethersphere/bee/pkg/pricer\"\n\tpb \"github.com/ethersphere/bee/pkg/retrieval/pb\"\n\t\"github.com/ethersphere/bee/pkg/soc\"\n\t\"github.com/ethersphere/bee/pkg/storage\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n\t\"github.com/ethersphere/bee/pkg/topology\"\n\t\"github.com/ethersphere/bee/pkg/tracing\"\n\t\"github.com/opentracing/opentracing-go\"\n\t\"golang.org/x/sync/singleflight\"\n)\n\ntype requestSourceContextKey struct{}\n\nconst (\n\tprotocolName    = \"retrieval\"\n\tprotocolVersion = \"1.0.0\"\n\tstreamName      = \"retrieval\"\n)\n\nvar _ Interface = (*Service)(nil)\n\ntype Interface interface {\n\tRetrieveChunk(ctx context.Context, addr swarm.Address) (chunk swarm.Chunk, err error)\n}\n\ntype Service struct {\n\taddr          swarm.Address\n\tstreamer      p2p.Streamer\n\tpeerSuggester topology.EachPeerer\n\tstorer        storage.Storer\n\tsingleflight  singleflight.Group\n\tlogger        logging.Logger\n\taccounting    accounting.Interface\n\tmetrics       metrics\n\tpricer        pricer.Interface\n\ttracer        *tracing.Tracer\n}\n\nfunc New(addr swarm.Address, storer storage.Storer, streamer p2p.Streamer, chunkPeerer topology.EachPeerer, logger logging.Logger, accounting accounting.Interface, pricer pricer.Interface, tracer *tracing.Tracer) *Service {\n\treturn &Service{\n\t\taddr:          addr,\n\t\tstreamer:      streamer,\n\t\tpeerSuggester: chunkPeerer,\n\t\tstorer:        storer,\n\t\tlogger:        logger,\n\t\taccounting:    accounting,\n\t\tpricer:        pricer,\n\t\tmetrics:       newMetrics(),\n\t\ttracer:        tracer,\n\t}\n}\n\nfunc (s *Service) Protocol() p2p.ProtocolSpec {\n\treturn p2p.ProtocolSpec{\n\t\tName:    protocolName,\n\t\tVersion: protocolVersion,\n\t\tStreamSpecs: []p2p.StreamSpec{\n\t\t\t{\n\t\t\t\tName:    streamName,\n\t\t\t\tHandler: s.handler,\n\t\t\t},\n\t\t},\n\t}\n}\n\nconst (\n\tmaxPeers             = 5\n\tretrieveChunkTimeout = 10 * time.Second\n\n\tretrieveRetryIntervalDuration = 5 * time.Second\n)\n\nfunc (s *Service) RetrieveChunk(ctx context.Context, addr swarm.Address) (swarm.Chunk, error) {\n\ts.metrics.RequestCounter.Inc()\n\n\tv, err, _ := s.singleflight.Do(addr.String(), func() (interface{}, error) {\n\t\tspan, logger, ctx := s.tracer.StartSpanFromContext(ctx, \"retrieve-chunk\", s.logger, opentracing.Tag{Key: \"address\", Value: addr.String()})\n\t\tdefer span.Finish()\n\n\t\tsp := newSkipPeers()\n\n\t\tticker := time.NewTicker(retrieveRetryIntervalDuration)\n\t\tdefer ticker.Stop()\n\n\t\tvar (\n\t\t\tpeerAttempt  int\n\t\t\tpeersResults int\n\t\t\tresultC      = make(chan swarm.Chunk, maxPeers)\n\t\t\terrC         = make(chan error, maxPeers)\n\t\t)\n\n\t\tfor {\n\t\t\tif peerAttempt < maxPeers {\n\t\t\t\tpeerAttempt++\n\n\t\t\t\ts.metrics.PeerRequestCounter.Inc()\n\n\t\t\t\tgo func() {\n\t\t\t\t\tchunk, peer, err := s.retrieveChunk(ctx, addr, sp)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tif !peer.IsZero() {\n\t\t\t\t\t\t\tlogger.Debugf(\"retrieval: failed to get chunk %s from peer %s: %v\", addr, peer, err)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\terrC <- err\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\n\t\t\t\t\tresultC <- chunk\n\t\t\t\t}()\n\t\t\t} else {\n\t\t\t\tticker.Stop()\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\t// break\n\t\t\tcase chunk := <-resultC:\n\t\t\t\treturn chunk, nil\n\t\t\tcase <-errC:\n\t\t\t\tpeersResults++\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlogger.Tracef(\"retrieval: failed to get chunk %s: %v\", addr, ctx.Err())\n\t\t\t\treturn nil, fmt.Errorf(\"retrieval: %w\", ctx.Err())\n\t\t\t}\n\n\t\t\t// all results received\n\t\t\tif peersResults >= maxPeers {\n\t\t\t\tlogger.Tracef(\"retrieval: failed to get chunk %s\", addr)\n\t\t\t\treturn nil, storage.ErrNotFound\n\t\t\t}\n\t\t}\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn v.(swarm.Chunk), nil\n}\n\nfunc (s *Service) retrieveChunk(ctx context.Context, addr swarm.Address, sp *skipPeers) (chunk swarm.Chunk, peer swarm.Address, err error) {\n\tstartTimer := time.Now()\n\n\tv := ctx.Value(requestSourceContextKey{})\n\tsourcePeerAddr := swarm.Address{}\n\t// allow upstream requests if this node is the source of the request\n\t// i.e. the request was not forwarded, to improve retrieval\n\t// if this node is the closest to he chunk but still does not contain it\n\tallowUpstream := true\n\tif src, ok := v.(string); ok {\n\t\tsourcePeerAddr, err = swarm.ParseHexAddress(src)\n\t\tif err == nil {\n\t\t\tsp.Add(sourcePeerAddr)\n\t\t}\n\t\t// do not allow upstream requests if the request was forwarded to this node\n\t\t// to avoid the request loops\n\t\tallowUpstream = false\n\t}\n\n\tctx, cancel := context.WithTimeout(ctx, retrieveChunkTimeout)\n\tdefer cancel()\n\tpeer, err = s.closestPeer(addr, sp.All(), allowUpstream)\n\tif err != nil {\n\t\treturn nil, peer, fmt.Errorf(\"get closest for address %s, allow upstream %v: %w\", addr.String(), allowUpstream, err)\n\t}\n\n\tpeerPO := swarm.Proximity(s.addr.Bytes(), peer.Bytes())\n\n\tif !sourcePeerAddr.IsZero() {\n\t\t// is forwarded request\n\t\tsourceAddrPO := swarm.Proximity(sourcePeerAddr.Bytes(), addr.Bytes())\n\t\taddrPO := swarm.Proximity(peer.Bytes(), addr.Bytes())\n\n\t\tpoGain := int(addrPO) - int(sourceAddrPO)\n\n\t\ts.metrics.RetrieveChunkPOGainCounter.\n\t\t\tWithLabelValues(strconv.Itoa(poGain)).\n\t\t\tInc()\n\t}\n\n\tsp.Add(peer)\n\n\t// compute the peer's price for this chunk for price header\n\tchunkPrice := s.pricer.PeerPrice(peer, addr)\n\n\ts.logger.Tracef(\"retrieval: requesting chunk %s from peer %s\", addr, peer)\n\tstream, err := s.streamer.NewStream(ctx, peer, nil, protocolName, protocolVersion, streamName)\n\tif err != nil {\n\t\ts.metrics.TotalErrors.Inc()\n\t\treturn nil, peer, fmt.Errorf(\"new stream: %w\", err)\n\t}\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = stream.Reset()\n\t\t} else {\n\t\t\tgo stream.FullClose()\n\t\t}\n\t}()\n\n\t// Reserve to see whether we can request the chunk\n\terr = s.accounting.Reserve(ctx, peer, chunkPrice)\n\tif err != nil {\n\t\treturn nil, peer, err\n\t}\n\tdefer s.accounting.Release(peer, chunkPrice)\n\n\tw, r := protobuf.NewWriterAndReader(stream)\n\tif err := w.WriteMsgWithContext(ctx, &pb.Request{\n\t\tAddr: addr.Bytes(),\n\t}); err != nil {\n\t\ts.metrics.TotalErrors.Inc()\n\t\treturn nil, peer, fmt.Errorf(\"write request: %w peer %s\", err, peer.String())\n\t}\n\n\tvar d pb.Delivery\n\tif err := r.ReadMsgWithContext(ctx, &d); err != nil {\n\t\ts.metrics.TotalErrors.Inc()\n\t\treturn nil, peer, fmt.Errorf(\"read delivery: %w peer %s\", err, peer.String())\n\t}\n\ts.metrics.RetrieveChunkPeerPOTimer.\n\t\tWithLabelValues(strconv.Itoa(int(peerPO))).\n\t\tObserve(time.Since(startTimer).Seconds())\n\ts.metrics.TotalRetrieved.Inc()\n\n\tstamp := new(postage.Stamp)\n\terr = stamp.UnmarshalBinary(d.Stamp)\n\tif err != nil {\n\t\treturn nil, peer, fmt.Errorf(\"stamp unmarshal: %w\", err)\n\t}\n\tchunk = swarm.NewChunk(addr, d.Data).WithStamp(stamp)\n\tif !cac.Valid(chunk) {\n\t\tif !soc.Valid(chunk) {\n\t\t\ts.metrics.InvalidChunkRetrieved.Inc()\n\t\t\ts.metrics.TotalErrors.Inc()\n\t\t\treturn nil, peer, swarm.ErrInvalidChunk\n\t\t}\n\t}\n\n\t// credit the peer after successful delivery\n\terr = s.accounting.Credit(peer, chunkPrice)\n\tif err != nil {\n\t\treturn nil, peer, err\n\t}\n\ts.metrics.ChunkPrice.Observe(float64(chunkPrice))\n\n\treturn chunk, peer, err\n}\n\n// closestPeer returns address of the peer that is closest to the chunk with\n// provided address addr. This function will ignore peers with addresses\n// provided in skipPeers and if allowUpstream is true, peers that are further of\n// the chunk than this node is, could also be returned, allowing the upstream\n// retrieve request.\nfunc (s *Service) closestPeer(addr swarm.Address, skipPeers []swarm.Address, allowUpstream bool) (swarm.Address, error) {\n\tclosest := swarm.Address{}\n\terr := s.peerSuggester.EachPeerRev(func(peer swarm.Address, po uint8) (bool, bool, error) {\n\t\tfor _, a := range skipPeers {\n\t\t\tif a.Equal(peer) {\n\t\t\t\treturn false, false, nil\n\t\t\t}\n\t\t}\n\t\tif closest.IsZero() {\n\t\t\tclosest = peer\n\t\t\treturn false, false, nil\n\t\t}\n\t\tdcmp, err := swarm.DistanceCmp(addr.Bytes(), closest.Bytes(), peer.Bytes())\n\t\tif err != nil {\n\t\t\treturn false, false, fmt.Errorf(\"distance compare error. addr %s closest %s peer %s: %w\", addr.String(), closest.String(), peer.String(), err)\n\t\t}\n\t\tswitch dcmp {\n\t\tcase 0:\n\t\t\t// do nothing\n\t\tcase -1:\n\t\t\t// current peer is closer\n\t\t\tclosest = peer\n\t\tcase 1:\n\t\t\t// closest is already closer to chunk\n\t\t\t// do nothing\n\t\t}\n\t\treturn false, false, nil\n\t})\n\tif err != nil {\n\t\treturn swarm.Address{}, err\n\t}\n\n\t// check if found\n\tif closest.IsZero() {\n\t\treturn swarm.Address{}, topology.ErrNotFound\n\t}\n\tif allowUpstream {\n\t\treturn closest, nil\n\t}\n\n\tdcmp, err := swarm.DistanceCmp(addr.Bytes(), closest.Bytes(), s.addr.Bytes())\n\tif err != nil {\n\t\treturn swarm.Address{}, fmt.Errorf(\"distance compare addr %s closest %s base address %s: %w\", addr.String(), closest.String(), s.addr.String(), err)\n\t}\n\tif dcmp != 1 {\n\t\treturn swarm.Address{}, topology.ErrNotFound\n\t}\n\n\treturn closest, nil\n}\n\nfunc (s *Service) handler(ctx context.Context, p p2p.Peer, stream p2p.Stream) (err error) {\n\tw, r := protobuf.NewWriterAndReader(stream)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = stream.Reset()\n\t\t} else {\n\t\t\t_ = stream.FullClose()\n\t\t}\n\t}()\n\tvar req pb.Request\n\tif err := r.ReadMsgWithContext(ctx, &req); err != nil {\n\t\treturn fmt.Errorf(\"read request: %w peer %s\", err, p.Address.String())\n\t}\n\n\tspan, _, ctx := s.tracer.StartSpanFromContext(ctx, \"handle-retrieve-chunk\", s.logger, opentracing.Tag{Key: \"address\", Value: swarm.NewAddress(req.Addr).String()})\n\tdefer span.Finish()\n\n\tctx = context.WithValue(ctx, requestSourceContextKey{}, p.Address.String())\n\taddr := swarm.NewAddress(req.Addr)\n\tchunk, err := s.storer.Get(ctx, storage.ModeGetRequest, addr)\n\tif err != nil {\n\t\tif errors.Is(err, storage.ErrNotFound) {\n\t\t\t// forward the request\n\t\t\tchunk, err = s.RetrieveChunk(ctx, addr)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"retrieve chunk: %w\", err)\n\t\t\t}\n\t\t} else {\n\t\t\treturn fmt.Errorf(\"get from store: %w\", err)\n\t\t}\n\t}\n\n\tstamp, err := chunk.Stamp().MarshalBinary()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"stamp marshal: %w\", err)\n\t}\n\n\tchunkPrice := s.pricer.Price(chunk.Address())\n\tdebit := s.accounting.PrepareDebit(p.Address, chunkPrice)\n\tdefer debit.Cleanup()\n\n\tif err := w.WriteMsgWithContext(ctx, &pb.Delivery{\n\t\tData:  chunk.Data(),\n\t\tStamp: stamp,\n\t}); err != nil {\n\t\treturn fmt.Errorf(\"write delivery: %w peer %s\", err, p.Address.String())\n\t}\n\n\ts.logger.Tracef(\"retrieval protocol debiting peer %s\", p.Address.String())\n\n\t// debit price from p's balance\n\treturn debit.Apply()\n}\n", "idx": 8, "id": 14777, "msg": "", "proj": "ethersphere-bee", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1376,8 +1376,6 @@ void DataReaderImpl::release_payload_pool()\n \n     PoolConfig config = PoolConfig::from_history_attributes(history_.m_att);\n     payload_pool_->release_history(config, true);\n-\n-    TopicPayloadPoolRegistry::release(payload_pool_);\n }\n \n ReturnCode_t DataReaderImpl::check_datasharing_compatible(", "y": 1, "oldf": "// Copyright 2019 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n/**\n * @file DataReaderImpl.cpp\n *\n */\n\n#include <fastrtps/config.h>\n\n#include <fastdds/subscriber/DataReaderImpl.hpp>\n\n#include <fastdds/dds/log/Log.hpp>\n#include <fastdds/dds/domain/DomainParticipant.hpp>\n#include <fastdds/dds/subscriber/DataReader.hpp>\n#include <fastdds/dds/subscriber/SampleInfo.hpp>\n#include <fastdds/dds/subscriber/Subscriber.hpp>\n#include <fastdds/dds/subscriber/SubscriberListener.hpp>\n#include <fastdds/dds/topic/TypeSupport.hpp>\n#include <fastdds/dds/topic/Topic.hpp>\n\n#include <fastdds/rtps/RTPSDomain.h>\n#include <fastdds/rtps/participant/RTPSParticipant.h>\n#include <fastdds/rtps/reader/RTPSReader.h>\n#include <fastdds/rtps/resources/ResourceEvent.h>\n#include <fastdds/rtps/resources/TimedEvent.h>\n\n#include <fastdds/subscriber/SubscriberImpl.hpp>\n#include <fastdds/subscriber/DataReaderImpl/ReadTakeCommand.hpp>\n#include <fastdds/subscriber/DataReaderImpl/StateFilter.hpp>\n\n#include <fastrtps/utils/TimeConversion.h>\n#include <utils/Host.hpp>\n#include <fastrtps/subscriber/SampleInfo.h>\n\n#include <rtps/history/TopicPayloadPoolRegistry.hpp>\n\nusing namespace eprosima::fastrtps;\nusing namespace eprosima::fastrtps::rtps;\nusing namespace std::chrono;\n\nnamespace eprosima {\nnamespace fastdds {\nnamespace dds {\n\nstatic void sample_info_to_dds (\n        const SampleInfo_t& rtps_info,\n        SampleInfo* dds_info)\n{\n    dds_info->sample_state = NOT_READ_SAMPLE_STATE;\n    dds_info->view_state = NOT_NEW_VIEW_STATE;\n    dds_info->disposed_generation_count = 0;\n    dds_info->no_writers_generation_count = 1;\n    dds_info->sample_rank = 0;\n    dds_info->generation_rank = 0;\n    dds_info->absoulte_generation_rank = 0;\n    dds_info->source_timestamp = rtps_info.sourceTimestamp;\n    dds_info->instance_handle = rtps_info.iHandle;\n    dds_info->publication_handle = fastrtps::rtps::InstanceHandle_t(rtps_info.sample_identity.writer_guid());\n    dds_info->sample_identity = rtps_info.sample_identity;\n    dds_info->related_sample_identity = rtps_info.related_sample_identity;\n    dds_info->valid_data = rtps_info.sampleKind == eprosima::fastrtps::rtps::ALIVE ? true : false;\n\n    switch (rtps_info.sampleKind)\n    {\n        case eprosima::fastrtps::rtps::ALIVE:\n            dds_info->instance_state = ALIVE_INSTANCE_STATE;\n            break;\n        case eprosima::fastrtps::rtps::NOT_ALIVE_DISPOSED:\n            dds_info->instance_state = NOT_ALIVE_DISPOSED_INSTANCE_STATE;\n            break;\n        default:\n            //TODO [ILG] change this if the other kinds ever get implemented\n            dds_info->instance_state = ALIVE_INSTANCE_STATE;\n            break;\n    }\n}\n\nstatic bool collections_have_same_properties(\n        const LoanableCollection& data_values,\n        const SampleInfoSeq& sample_infos)\n{\n    return ((data_values.has_ownership() == sample_infos.has_ownership()) &&\n           (data_values.maximum() == sample_infos.maximum()) &&\n           (data_values.length() == sample_infos.length()));\n}\n\nDataReaderImpl::DataReaderImpl(\n        SubscriberImpl* s,\n        TypeSupport& type,\n        TopicDescription* topic,\n        const DataReaderQos& qos,\n        DataReaderListener* listener)\n    : subscriber_(s)\n    , type_(type)\n    , topic_(topic)\n    , qos_(&qos == &DATAREADER_QOS_DEFAULT ? subscriber_->get_default_datareader_qos() : qos)\n#pragma warning (disable : 4355 )\n    , history_(topic_attributes(),\n            type_.get(),\n            qos_.get_readerqos(subscriber_->get_qos()),\n            type_->m_typeSize + 3,    /* Possible alignment */\n            qos_.endpoint().history_memory_policy)\n    , listener_(listener)\n    , reader_listener_(this)\n    , deadline_duration_us_(qos_.deadline().period.to_ns() * 1e-3)\n    , lifespan_duration_us_(qos_.lifespan().duration.to_ns() * 1e-3)\n    , sample_info_pool_(qos)\n    , loan_manager_(qos)\n{\n}\n\nReturnCode_t DataReaderImpl::enable()\n{\n    assert(reader_ == nullptr);\n\n    fastrtps::rtps::ReaderAttributes att;\n\n    att.endpoint.durabilityKind = qos_.durability().durabilityKind();\n    att.endpoint.endpointKind = READER;\n    att.endpoint.multicastLocatorList = qos_.endpoint().multicast_locator_list;\n    att.endpoint.reliabilityKind = qos_.reliability().kind == RELIABLE_RELIABILITY_QOS ? RELIABLE : BEST_EFFORT;\n    att.endpoint.topicKind = type_->m_isGetKeyDefined ? WITH_KEY : NO_KEY;\n    att.endpoint.unicastLocatorList = qos_.endpoint().unicast_locator_list;\n    att.endpoint.remoteLocatorList = qos_.endpoint().remote_locator_list;\n    att.endpoint.properties = qos_.properties();\n\n    if (qos_.endpoint().entity_id > 0)\n    {\n        att.endpoint.setEntityID(static_cast<uint8_t>(qos_.endpoint().entity_id));\n    }\n\n    if (qos_.endpoint().user_defined_id > 0)\n    {\n        att.endpoint.setUserDefinedID(static_cast<uint8_t>(qos_.endpoint().user_defined_id));\n    }\n\n    att.times = qos_.reliable_reader_qos().times;\n    att.liveliness_lease_duration = qos_.liveliness().lease_duration;\n    att.liveliness_kind_ = qos_.liveliness().kind;\n    att.matched_writers_allocation = qos_.reader_resource_limits().matched_publisher_allocation;\n    att.expectsInlineQos = qos_.expects_inline_qos();\n    att.disable_positive_acks = qos_.reliable_reader_qos().disable_positive_ACKs.enabled;\n\n\n    // TODO(Ricardo) Remove in future\n    // Insert topic_name and partitions\n    Property property;\n    property.name(\"topic_name\");\n    property.value(topic_->get_name().c_str());\n    att.endpoint.properties.properties().push_back(std::move(property));\n    if (subscriber_->get_qos().partition().names().size() > 0)\n    {\n        property.name(\"partitions\");\n        std::string partitions;\n        bool is_first_partition = true;\n        for (auto partition : subscriber_->get_qos().partition().names())\n        {\n            partitions += (is_first_partition ? \"\" : \";\") + partition;\n            is_first_partition = false;\n        }\n        property.value(std::move(partitions));\n        att.endpoint.properties.properties().push_back(std::move(property));\n    }\n\n    bool is_datasharing_compatible = false;\n    ReturnCode_t ret_code = check_datasharing_compatible(att, is_datasharing_compatible);\n    if (ret_code != ReturnCode_t::RETCODE_OK)\n    {\n        return ret_code;\n    }\n    if (is_datasharing_compatible)\n    {\n        DataSharingQosPolicy datasharing(qos_.data_sharing());\n        if (datasharing.domain_ids().empty())\n        {\n            uint64_t id = 0;\n            Host::uint48 mac_id = Host::instance().mac_id();\n            for (size_t i = 0; i < Host::mac_id_length; ++i)\n            {\n                id |= mac_id.value[i] << (64 - i);\n            }\n            datasharing.add_domain_id(id);\n        }\n        att.endpoint.set_data_sharing_configuration(datasharing);\n    }\n    else\n    {\n        DataSharingQosPolicy datasharing;\n        datasharing.off();\n        att.endpoint.set_data_sharing_configuration(datasharing);\n    }\n\n    std::shared_ptr<IPayloadPool> pool = get_payload_pool();\n    RTPSReader* reader = RTPSDomain::createRTPSReader(\n        subscriber_->rtps_participant(),\n        att, pool,\n        static_cast<ReaderHistory*>(&history_),\n        static_cast<ReaderListener*>(&reader_listener_));\n\n    if (reader == nullptr)\n    {\n        release_payload_pool();\n        logError(DATA_READER, \"Problem creating associated Reader\");\n        return ReturnCode_t::RETCODE_ERROR;\n    }\n\n    reader_ = reader;\n\n    deadline_timer_ = new TimedEvent(subscriber_->get_participant()->get_resource_event(),\n                    [&]() -> bool\n                    {\n                        return deadline_missed();\n                    },\n                    qos_.deadline().period.to_ns() * 1e-6);\n\n    lifespan_timer_ = new TimedEvent(subscriber_->get_participant()->get_resource_event(),\n                    [&]() -> bool\n                    {\n                        return lifespan_expired();\n                    },\n                    qos_.lifespan().duration.to_ns() * 1e-6);\n\n    // Register the reader\n    ReaderQos rqos = qos_.get_readerqos(subscriber_->get_qos());\n    if (!is_datasharing_compatible)\n    {\n        rqos.data_sharing.off();\n    }\n    subscriber_->rtps_participant()->registerReader(reader_, topic_attributes(), rqos);\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nvoid DataReaderImpl::disable()\n{\n    set_listener(nullptr);\n    if (reader_ != nullptr)\n    {\n        reader_->setListener(nullptr);\n    }\n}\n\nDataReaderImpl::~DataReaderImpl()\n{\n    delete lifespan_timer_;\n    delete deadline_timer_;\n\n    if (reader_ != nullptr)\n    {\n        logInfo(DATA_READER, guid().entityId << \" in topic: \" << topic_->get_name());\n        RTPSDomain::removeRTPSReader(reader_);\n        release_payload_pool();\n    }\n\n    delete user_datareader_;\n}\n\nbool DataReaderImpl::can_be_deleted() const\n{\n    if (reader_ != nullptr)\n    {\n        std::lock_guard<RecursiveTimedMutex> lock(reader_->getMutex());\n        return !loan_manager_.has_outstanding_loans();\n    }\n\n    return true;\n}\n\nbool DataReaderImpl::wait_for_unread_message(\n        const fastrtps::Duration_t& timeout)\n{\n    return reader_ ? reader_->wait_for_unread_cache(timeout) : false;\n}\n\nReturnCode_t DataReaderImpl::check_collection_preconditions_and_calc_max_samples(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t& max_samples)\n{\n    // Properties should be the same on both collections\n    if (!collections_have_same_properties(data_values, sample_infos))\n    {\n        return ReturnCode_t::RETCODE_PRECONDITION_NOT_MET;\n    }\n\n    // Check if a loan is required\n    if (0 < data_values.maximum())\n    {\n        // Loan not required, input collections should not be already loaned\n        if (false == data_values.has_ownership())\n        {\n            return ReturnCode_t::RETCODE_PRECONDITION_NOT_MET;\n        }\n\n        int32_t collection_max = data_values.maximum();\n\n        // We consider all negative value to be LENGTH_UNLIMITED\n        if (0 > max_samples)\n        {\n            // When max_samples is LENGTH_UNLIMITED, the collection imposes the maximum number of samples\n            max_samples = collection_max;\n        }\n        else\n        {\n            if (max_samples > collection_max)\n            {\n                return ReturnCode_t::RETCODE_PRECONDITION_NOT_MET;\n            }\n        }\n    }\n\n    // All preconditions have been checked. Now apply resource limits on max_samples.\n    if ((0 > max_samples) || (max_samples > qos_.reader_resource_limits().max_samples_per_read))\n    {\n        max_samples = qos_.reader_resource_limits().max_samples_per_read;\n    }\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nReturnCode_t DataReaderImpl::prepare_loan(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t& max_samples)\n{\n    if (0 < data_values.maximum())\n    {\n        // A loan was not requested\n        return ReturnCode_t::RETCODE_OK;\n    }\n\n    if (max_samples > 0)\n    {\n        // Check if there are enough sample_infos\n        size_t num_infos = sample_info_pool_.num_allocated();\n        if (num_infos == qos_.reader_resource_limits().sample_infos_allocation.maximum)\n        {\n            return ReturnCode_t::RETCODE_OUT_OF_RESOURCES;\n        }\n\n        // Limit max_samples to available sample_infos\n        num_infos += max_samples;\n        if (num_infos > qos_.reader_resource_limits().sample_infos_allocation.maximum)\n        {\n            size_t exceed = num_infos - qos_.reader_resource_limits().sample_infos_allocation.maximum;\n            max_samples -= static_cast<uint32_t>(exceed);\n        }\n    }\n\n    if (max_samples > 0)\n    {\n        // Check if there are enough samples\n        int32_t num_samples = sample_pool_->num_allocated();\n        int32_t max_resource_samples = qos_.resource_limits().max_samples;\n        if (max_resource_samples <= 0)\n        {\n            max_resource_samples = std::numeric_limits<int32_t>::max();\n        }\n        if (num_samples == max_resource_samples)\n        {\n            return ReturnCode_t::RETCODE_OUT_OF_RESOURCES;\n        }\n\n        // Limit max_samples to available samples\n        num_samples += max_samples;\n        if (num_samples > max_resource_samples)\n        {\n            int32_t exceed = num_samples - max_resource_samples;\n            max_samples -= exceed;\n        }\n    }\n\n    // Check if there are enough loans\n    ReturnCode_t code = loan_manager_.get_loan(data_values, sample_infos);\n    if (!code)\n    {\n        return code;\n    }\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nReturnCode_t DataReaderImpl::read_or_take(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        const InstanceHandle_t& handle,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states,\n        bool exact_instance,\n        bool single_instance,\n        bool should_take)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    ReturnCode_t code = check_collection_preconditions_and_calc_max_samples(data_values, sample_infos, max_samples);\n    if (!code)\n    {\n        return code;\n    }\n\n    std::lock_guard<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    auto it = history_.lookup_instance(handle, exact_instance);\n    if (!it.first)\n    {\n        return exact_instance ? ReturnCode_t::RETCODE_BAD_PARAMETER : ReturnCode_t::RETCODE_NO_DATA;\n    }\n\n    code = prepare_loan(data_values, sample_infos, max_samples);\n    if (!code)\n    {\n        return code;\n    }\n\n    detail::StateFilter states{ sample_states, view_states, instance_states };\n    detail::ReadTakeCommand cmd(*this, data_values, sample_infos, max_samples, states, it.second, single_instance);\n    while (!cmd.is_finished())\n    {\n        cmd.add_instance(should_take);\n    }\n    return cmd.return_value();\n}\n\nReturnCode_t DataReaderImpl::read(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, HANDLE_NIL,\n                   sample_states, view_states, instance_states, false, false, false);\n}\n\nReturnCode_t DataReaderImpl::read_instance(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        const InstanceHandle_t& a_handle,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, a_handle,\n                   sample_states, view_states, instance_states, true, true, false);\n}\n\nReturnCode_t DataReaderImpl::read_next_instance(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        const InstanceHandle_t& previous_handle,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, previous_handle,\n                   sample_states, view_states, instance_states, false, true, false);\n}\n\nReturnCode_t DataReaderImpl::take(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, HANDLE_NIL,\n                   sample_states, view_states, instance_states, false, false, true);\n}\n\nReturnCode_t DataReaderImpl::take_instance(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        const InstanceHandle_t& a_handle,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, a_handle,\n                   sample_states, view_states, instance_states, true, true, true);\n}\n\nReturnCode_t DataReaderImpl::take_next_instance(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos,\n        int32_t max_samples,\n        const InstanceHandle_t& previous_handle,\n        SampleStateMask sample_states,\n        ViewStateMask view_states,\n        InstanceStateMask instance_states)\n{\n    return read_or_take(data_values, sample_infos, max_samples, previous_handle,\n                   sample_states, view_states, instance_states, false, true, true);\n}\n\nReturnCode_t DataReaderImpl::return_loan(\n        LoanableCollection& data_values,\n        SampleInfoSeq& sample_infos)\n{\n    static_cast<void>(data_values);\n    static_cast<void>(sample_infos);\n\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    // Properties should be the same on both collections\n    if (!collections_have_same_properties(data_values, sample_infos))\n    {\n        return ReturnCode_t::RETCODE_PRECONDITION_NOT_MET;\n    }\n\n    // They should have a loan\n    if (data_values.has_ownership() == true)\n    {\n        return ReturnCode_t::RETCODE_PRECONDITION_NOT_MET;\n    }\n\n    std::lock_guard<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    // Check if they were loaned by this reader\n    ReturnCode_t code = loan_manager_.return_loan(data_values, sample_infos);\n    if (!code)\n    {\n        return code;\n    }\n\n    // Return samples and infos\n    LoanableCollection::size_type n = sample_infos.length();\n    while (n > 0)\n    {\n        --n;\n        if (sample_infos[n].valid_data)\n        {\n            sample_pool_->return_loan(data_values.buffer()[n]);\n        }\n\n        sample_info_pool_.return_item(&sample_infos[n]);\n    }\n\n    data_values.unloan();\n    sample_infos.unloan();\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nReturnCode_t DataReaderImpl::read_next_sample(\n        void* data,\n        SampleInfo* info)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    if (history_.getHistorySize() == 0)\n    {\n        return ReturnCode_t::RETCODE_NO_DATA;\n    }\n\n    auto max_blocking_time = std::chrono::steady_clock::now() +\n#if HAVE_STRICT_REALTIME\n            std::chrono::microseconds(::TimeConv::Time_t2MicroSecondsInt64(qos_.reliability().max_blocking_time));\n#else\n            std::chrono::hours(24);\n#endif // if HAVE_STRICT_REALTIME\n    SampleInfo_t rtps_info;\n    if (history_.readNextData(data, &rtps_info, max_blocking_time))\n    {\n        sample_info_to_dds(rtps_info, info);\n        return ReturnCode_t::RETCODE_OK;\n    }\n    return ReturnCode_t::RETCODE_ERROR;\n}\n\nReturnCode_t DataReaderImpl::take_next_sample(\n        void* data,\n        SampleInfo* info)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    if (history_.getHistorySize() == 0)\n    {\n        return ReturnCode_t::RETCODE_NO_DATA;\n    }\n\n    auto max_blocking_time = std::chrono::steady_clock::now() +\n#if HAVE_STRICT_REALTIME\n            std::chrono::microseconds(::TimeConv::Time_t2MicroSecondsInt64(qos_.reliability().max_blocking_time));\n#else\n            std::chrono::hours(24);\n#endif // if HAVE_STRICT_REALTIME\n\n    SampleInfo_t rtps_info;\n    if (history_.takeNextData(data, &rtps_info, max_blocking_time))\n    {\n        sample_info_to_dds(rtps_info, info);\n        return ReturnCode_t::RETCODE_OK;\n    }\n    return ReturnCode_t::RETCODE_ERROR;\n}\n\nReturnCode_t DataReaderImpl::get_first_untaken_info(\n        SampleInfo* info)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    SampleInfo_t rtps_info;\n    if (history_.get_first_untaken_info(&rtps_info))\n    {\n        sample_info_to_dds(rtps_info, info);\n        return ReturnCode_t::RETCODE_OK;\n    }\n    return ReturnCode_t::RETCODE_NO_DATA;\n}\n\nconst GUID_t& DataReaderImpl::guid() const\n{\n    return reader_ ? reader_->getGuid() : c_Guid_Unknown;\n}\n\nInstanceHandle_t DataReaderImpl::get_instance_handle() const\n{\n    return guid();\n}\n\nvoid DataReaderImpl::subscriber_qos_updated()\n{\n    if (reader_)\n    {\n        //NOTIFY THE BUILTIN PROTOCOLS THAT THE READER HAS CHANGED\n        ReaderQos rqos = qos_.get_readerqos(get_subscriber()->get_qos());\n        subscriber_->rtps_participant()->updateReader(reader_, topic_attributes(), rqos);\n    }\n}\n\nReturnCode_t DataReaderImpl::set_qos(\n        const DataReaderQos& qos)\n{\n    bool enabled = reader_ != nullptr;\n    const DataReaderQos& qos_to_set = (&qos == &DATAREADER_QOS_DEFAULT) ?\n            subscriber_->get_default_datareader_qos() : qos;\n\n    // Default qos is always considered consistent\n    if (&qos != &DATAREADER_QOS_DEFAULT)\n    {\n        if (subscriber_->get_participant()->get_qos().allocation().data_limits.max_user_data != 0 &&\n                subscriber_->get_participant()->get_qos().allocation().data_limits.max_user_data <\n                qos_to_set.user_data().getValue().size())\n        {\n            return ReturnCode_t::RETCODE_INCONSISTENT_POLICY;\n        }\n\n        ReturnCode_t check_result = check_qos(qos_to_set);\n        if (!check_result)\n        {\n            return check_result;\n        }\n    }\n\n    if (enabled && !can_qos_be_updated(qos_, qos_to_set))\n    {\n        return ReturnCode_t::RETCODE_IMMUTABLE_POLICY;\n    }\n\n    set_qos(qos_, qos_to_set, !enabled);\n\n    if (enabled)\n    {\n        //NOTIFY THE BUILTIN PROTOCOLS THAT THE READER HAS CHANGED\n        ReaderQos rqos = qos.get_readerqos(get_subscriber()->get_qos());\n        subscriber_->rtps_participant()->updateReader(reader_, topic_attributes(), rqos);\n\n        // Deadline\n        if (qos_.deadline().period != c_TimeInfinite)\n        {\n            deadline_duration_us_ = duration<double, std::ratio<1, 1000000>>(qos_.deadline().period.to_ns() * 1e-3);\n            deadline_timer_->update_interval_millisec(qos_.deadline().period.to_ns() * 1e-6);\n        }\n        else\n        {\n            deadline_timer_->cancel_timer();\n        }\n\n        // Lifespan\n        if (qos_.lifespan().duration != c_TimeInfinite)\n        {\n            lifespan_duration_us_ =\n                    std::chrono::duration<double, std::ratio<1, 1000000>>(qos_.lifespan().duration.to_ns() * 1e-3);\n            lifespan_timer_->update_interval_millisec(qos_.lifespan().duration.to_ns() * 1e-6);\n        }\n        else\n        {\n            lifespan_timer_->cancel_timer();\n        }\n    }\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nconst DataReaderQos& DataReaderImpl::get_qos() const\n{\n    return qos_;\n}\n\nvoid DataReaderImpl::InnerDataReaderListener::onNewCacheChangeAdded(\n        RTPSReader* /*reader*/,\n        const CacheChange_t* const change_in)\n{\n    if (data_reader_->on_new_cache_change_added(change_in))\n    {\n        //First check if we can handle with on_data_on_readers\n        SubscriberListener* subscriber_listener =\n                data_reader_->subscriber_->get_listener_for(StatusMask::data_on_readers());\n        if (subscriber_listener != nullptr)\n        {\n            subscriber_listener->on_data_on_readers(data_reader_->subscriber_->user_subscriber_);\n        }\n        else\n        {\n            // If not, try with on_data_available\n            DataReaderListener* listener = data_reader_->get_listener_for(StatusMask::data_available());\n            if (listener != nullptr)\n            {\n                listener->on_data_available(data_reader_->user_datareader_);\n            }\n        }\n    }\n}\n\nvoid DataReaderImpl::InnerDataReaderListener::onReaderMatched(\n        RTPSReader* /*reader*/,\n        const SubscriptionMatchedStatus& info)\n{\n    DataReaderListener* listener = data_reader_->get_listener_for(StatusMask::subscription_matched());\n    if (listener != nullptr)\n    {\n        listener->on_subscription_matched(data_reader_->user_datareader_, info);\n    }\n}\n\nvoid DataReaderImpl::InnerDataReaderListener::on_liveliness_changed(\n        RTPSReader* /*reader*/,\n        const fastrtps::LivelinessChangedStatus& status)\n{\n    data_reader_->update_liveliness_status(status);\n    DataReaderListener* listener = data_reader_->get_listener_for(StatusMask::liveliness_changed());\n    if (listener != nullptr)\n    {\n        LivelinessChangedStatus callback_status;\n        if (data_reader_->get_liveliness_changed_status(callback_status) == ReturnCode_t::RETCODE_OK)\n        {\n            listener->on_liveliness_changed(data_reader_->user_datareader_, callback_status);\n        }\n    }\n}\n\nvoid DataReaderImpl::InnerDataReaderListener::on_requested_incompatible_qos(\n        RTPSReader* /*reader*/,\n        fastdds::dds::PolicyMask qos)\n{\n    data_reader_->update_requested_incompatible_qos(qos);\n    DataReaderListener* listener = data_reader_->get_listener_for(StatusMask::requested_incompatible_qos());\n    if (listener != nullptr)\n    {\n        RequestedIncompatibleQosStatus callback_status;\n        if (data_reader_->get_requested_incompatible_qos_status(callback_status) == ReturnCode_t::RETCODE_OK)\n        {\n            listener->on_requested_incompatible_qos(data_reader_->user_datareader_, callback_status);\n        }\n    }\n}\n\nbool DataReaderImpl::on_new_cache_change_added(\n        const CacheChange_t* const change)\n{\n    if (qos_.deadline().period != c_TimeInfinite)\n    {\n        std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n        if (!history_.set_next_deadline(\n                    change->instanceHandle,\n                    steady_clock::now() + duration_cast<system_clock::duration>(deadline_duration_us_)))\n        {\n            logError(SUBSCRIBER, \"Could not set next deadline in the history\");\n        }\n        else if (timer_owner_ == change->instanceHandle || timer_owner_ == InstanceHandle_t())\n        {\n            if (deadline_timer_reschedule())\n            {\n                deadline_timer_->cancel_timer();\n                deadline_timer_->restart_timer();\n            }\n        }\n    }\n\n    CacheChange_t* new_change = const_cast<CacheChange_t*>(change);\n\n    if (qos_.lifespan().duration == c_TimeInfinite)\n    {\n        return true;\n    }\n\n    auto source_timestamp = system_clock::time_point() + nanoseconds(change->sourceTimestamp.to_ns());\n    auto now = system_clock::now();\n\n    // The new change could have expired if it arrived too late\n    // If so, remove it from the history and return false to avoid notifying the listener\n    if (now - source_timestamp >= lifespan_duration_us_)\n    {\n        history_.remove_change_sub(new_change);\n        return false;\n    }\n\n    CacheChange_t* earliest_change;\n    if (history_.get_earliest_change(&earliest_change))\n    {\n        if (earliest_change == change)\n        {\n            // The new change has been added at the beginning of the the history\n            // As the history is sorted by timestamp, this means that the new change has the smallest timestamp\n            // We have to stop the timer as this will be the next change to expire\n            lifespan_timer_->cancel_timer();\n        }\n    }\n    else\n    {\n        logError(SUBSCRIBER, \"A change was added to history that could not be retrieved\");\n    }\n\n    auto interval = source_timestamp - now + duration_cast<nanoseconds>(lifespan_duration_us_);\n\n    // Update and restart the timer\n    // If the timer is already running this will not have any effect\n    lifespan_timer_->update_interval_millisec(interval.count() * 1e-6);\n    lifespan_timer_->restart_timer();\n    return true;\n}\n\nbool DataReaderImpl::deadline_timer_reschedule()\n{\n    assert(qos_.deadline().period != c_TimeInfinite);\n\n    std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    steady_clock::time_point next_deadline_us;\n    if (!history_.get_next_deadline(timer_owner_, next_deadline_us))\n    {\n        logError(SUBSCRIBER, \"Could not get the next deadline from the history\");\n        return false;\n    }\n    auto interval_ms = duration_cast<milliseconds>(next_deadline_us - steady_clock::now());\n\n    deadline_timer_->update_interval_millisec(static_cast<double>(interval_ms.count()));\n    return true;\n}\n\nbool DataReaderImpl::deadline_missed()\n{\n    assert(qos_.deadline().period != c_TimeInfinite);\n\n    std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    deadline_missed_status_.total_count++;\n    deadline_missed_status_.total_count_change++;\n    deadline_missed_status_.last_instance_handle = timer_owner_;\n    listener_->on_requested_deadline_missed(user_datareader_, deadline_missed_status_);\n    subscriber_->subscriber_listener_.on_requested_deadline_missed(user_datareader_, deadline_missed_status_);\n    deadline_missed_status_.total_count_change = 0;\n\n    if (!history_.set_next_deadline(\n                timer_owner_,\n                steady_clock::now() + duration_cast<system_clock::duration>(deadline_duration_us_)))\n    {\n        logError(SUBSCRIBER, \"Could not set next deadline in the history\");\n        return false;\n    }\n    return deadline_timer_reschedule();\n}\n\nReturnCode_t DataReaderImpl::get_requested_deadline_missed_status(\n        RequestedDeadlineMissedStatus& status)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    status = deadline_missed_status_;\n    deadline_missed_status_.total_count_change = 0;\n    return ReturnCode_t::RETCODE_OK;\n}\n\nbool DataReaderImpl::lifespan_expired()\n{\n    std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    CacheChange_t* earliest_change;\n    while (history_.get_earliest_change(&earliest_change))\n    {\n        auto source_timestamp = system_clock::time_point() + nanoseconds(earliest_change->sourceTimestamp.to_ns());\n        auto now = system_clock::now();\n\n        // Check that the earliest change has expired (the change which started the timer could have been removed from the history)\n        if (now - source_timestamp < lifespan_duration_us_)\n        {\n            auto interval = source_timestamp - now + lifespan_duration_us_;\n            lifespan_timer_->update_interval_millisec(static_cast<double>(duration_cast<milliseconds>(interval).count()));\n            return true;\n        }\n\n        // The earliest change has expired\n        history_.remove_change_sub(earliest_change);\n\n        // Set the timer for the next change if there is one\n        if (!history_.get_earliest_change(&earliest_change))\n        {\n            return false;\n        }\n\n        // Calculate when the next change is due to expire and restart\n        source_timestamp = system_clock::time_point() + nanoseconds(earliest_change->sourceTimestamp.to_ns());\n        now = system_clock::now();\n        auto interval = source_timestamp - now + lifespan_duration_us_;\n\n        if (interval.count() > 0)\n        {\n            lifespan_timer_->update_interval_millisec(static_cast<double>(duration_cast<milliseconds>(interval).count()));\n            return true;\n        }\n    }\n\n    return false;\n}\n\n/* TODO\n   bool DataReaderImpl::read(\n        std::vector<void *>& data_values,\n        std::vector<SampleInfo_t>& sample_infos,\n        uint32_t max_samples)\n   {\n    (void)data_values;\n    (void)sample_infos;\n    (void)max_samples;\n    // TODO Implement\n    return false;\n   }\n\n   bool DataReaderImpl::take(\n        std::vector<void *>& data_values,\n        std::vector<SampleInfo_t>& sample_infos,\n        uint32_t max_samples)\n   {\n    (void)data_values;\n    (void)sample_infos;\n    (void)max_samples;\n    // TODO Implement\n    return false;\n   }\n */\n\nReturnCode_t DataReaderImpl::set_listener(\n        DataReaderListener* listener)\n{\n    listener_ = listener;\n    return ReturnCode_t::RETCODE_OK;\n}\n\nconst DataReaderListener* DataReaderImpl::get_listener() const\n{\n    return listener_;\n}\n\n/* TODO\n   bool DataReaderImpl::get_key_value(\n        void* data,\n        const rtps::InstanceHandle_t& handle)\n   {\n    (void)data;\n    (void)handle;\n    // TODO Implement\n    return false;\n   }\n */\n\nReturnCode_t DataReaderImpl::get_liveliness_changed_status(\n        LivelinessChangedStatus& status)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    std::lock_guard<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    status = liveliness_changed_status_;\n    liveliness_changed_status_.alive_count_change = 0u;\n    liveliness_changed_status_.not_alive_count_change = 0u;\n\n    return ReturnCode_t::RETCODE_OK;\n}\n\nReturnCode_t DataReaderImpl::get_requested_incompatible_qos_status(\n        RequestedIncompatibleQosStatus& status)\n{\n    if (reader_ == nullptr)\n    {\n        return ReturnCode_t::RETCODE_NOT_ENABLED;\n    }\n\n    std::unique_lock<RecursiveTimedMutex> lock(reader_->getMutex());\n\n    status = requested_incompatible_qos_status_;\n    requested_incompatible_qos_status_.total_count_change = 0u;\n    return ReturnCode_t::RETCODE_OK;\n}\n\n/* TODO\n   bool DataReaderImpl::get_sample_lost_status(\n        SampleLostStatus& status) const\n   {\n    (void)status;\n    // TODO Implement\n    // TODO add callback call subscriber_->subscriber_listener_->on_sample_lost\n    return false;\n   }\n */\n\n/* TODO\n   bool DataReaderImpl::get_sample_rejected_status(\n        SampleRejectedStatus& status) const\n   {\n    (void)status;\n    // TODO Implement\n    // TODO add callback call subscriber_->subscriber_listener_->on_sample_rejected\n    return false;\n   }\n */\n\nconst Subscriber* DataReaderImpl::get_subscriber() const\n{\n    return subscriber_->get_subscriber();\n}\n\n/* TODO\n   bool DataReaderImpl::wait_for_historical_data(\n        const Duration_t& max_wait) const\n   {\n    (void)max_wait;\n    // TODO Implement\n    return false;\n   }\n */\n\nconst TopicDescription* DataReaderImpl::get_topicdescription() const\n{\n    return topic_;\n}\n\nTypeSupport DataReaderImpl::type()\n{\n    return type_;\n}\n\nRequestedIncompatibleQosStatus& DataReaderImpl::update_requested_incompatible_qos(\n        PolicyMask incompatible_policies)\n{\n    ++requested_incompatible_qos_status_.total_count;\n    ++requested_incompatible_qos_status_.total_count_change;\n    for (fastrtps::rtps::octet id = 1; id < NEXT_QOS_POLICY_ID; ++id)\n    {\n        if (incompatible_policies.test(id))\n        {\n            ++requested_incompatible_qos_status_.policies[static_cast<QosPolicyId_t>(id)].count;\n            requested_incompatible_qos_status_.last_policy_id = static_cast<QosPolicyId_t>(id);\n        }\n    }\n    return requested_incompatible_qos_status_;\n}\n\nLivelinessChangedStatus& DataReaderImpl::update_liveliness_status(\n        const fastrtps::LivelinessChangedStatus& status)\n{\n    liveliness_changed_status_.alive_count = status.alive_count;\n    liveliness_changed_status_.not_alive_count = status.not_alive_count;\n    liveliness_changed_status_.alive_count_change += status.alive_count_change;\n    liveliness_changed_status_.not_alive_count_change += status.not_alive_count_change;\n    liveliness_changed_status_.last_publication_handle = status.last_publication_handle;\n\n    return liveliness_changed_status_;\n}\n\nReturnCode_t DataReaderImpl::check_qos (\n        const DataReaderQos& qos)\n{\n    if (qos.durability().kind == PERSISTENT_DURABILITY_QOS)\n    {\n        logError(DDS_QOS_CHECK, \"PERSISTENT Durability not supported\");\n        return ReturnCode_t::RETCODE_UNSUPPORTED;\n    }\n    if (qos.destination_order().kind == BY_SOURCE_TIMESTAMP_DESTINATIONORDER_QOS)\n    {\n        logError(DDS_QOS_CHECK, \"BY SOURCE TIMESTAMP DestinationOrder not supported\");\n        return ReturnCode_t::RETCODE_UNSUPPORTED;\n    }\n    if (qos.reliability().kind == BEST_EFFORT_RELIABILITY_QOS && qos.ownership().kind == EXCLUSIVE_OWNERSHIP_QOS)\n    {\n        logError(DDS_QOS_CHECK, \"BEST_EFFORT incompatible with EXCLUSIVE ownership\");\n        return ReturnCode_t::RETCODE_INCONSISTENT_POLICY;\n    }\n    if (qos.reader_resource_limits().max_samples_per_read <= 0)\n    {\n        logError(DDS_QOS_CHECK, \"max_samples_per_read should be strictly possitive\");\n        return ReturnCode_t::RETCODE_INCONSISTENT_POLICY;\n    }\n    return ReturnCode_t::RETCODE_OK;\n}\n\nbool DataReaderImpl::can_qos_be_updated(\n        const DataReaderQos& to,\n        const DataReaderQos& from)\n{\n    bool updatable = true;\n    if (!(to.resource_limits() == from.resource_limits()))\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"resource_limits cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.history().kind != from.history().kind ||\n            to.history().depth != from.history().depth)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"History cannot be changed after the creation of a DataReader.\");\n    }\n\n    if (to.durability().kind != from.durability().kind)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"Durability kind cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.liveliness().kind != from.liveliness().kind ||\n            to.liveliness().lease_duration != from.liveliness().lease_duration ||\n            to.liveliness().announcement_period != from.liveliness().announcement_period)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"Liveliness cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.reliability().kind != from.reliability().kind)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"Reliability Kind cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.ownership().kind != from.ownership().kind)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"Ownership Kind cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.destination_order().kind != from.destination_order().kind)\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"Destination order Kind cannot be changed after the creation of a DataReader.\");\n    }\n    if (!(to.reader_resource_limits() == from.reader_resource_limits()))\n    {\n        updatable = false;\n        logWarning(DDS_QOS_CHECK, \"reader_resource_limits cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.data_sharing().kind() != from.data_sharing().kind())\n    {\n        updatable = false;\n        logWarning(RTPS_QOS_CHECK, \"Data sharing configuration cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.data_sharing().shm_directory() != from.data_sharing().shm_directory())\n    {\n        updatable = false;\n        logWarning(RTPS_QOS_CHECK, \"Data sharing configuration cannot be changed after the creation of a DataReader.\");\n    }\n    if (to.data_sharing().domain_ids() != from.data_sharing().domain_ids())\n    {\n        updatable = false;\n        logWarning(RTPS_QOS_CHECK, \"Data sharing configuration cannot be changed after the creation of a DataReader.\");\n    }\n    return updatable;\n}\n\nvoid DataReaderImpl::set_qos(\n        DataReaderQos& to,\n        const DataReaderQos& from,\n        bool first_time)\n{\n    if (first_time && to.durability().kind != from.durability().kind)\n    {\n        to.durability() = from.durability();\n        to.durability().hasChanged = true;\n    }\n    if (to.deadline().period != from.deadline().period)\n    {\n        to.deadline() = from.deadline();\n        to.deadline().hasChanged = true;\n    }\n    if (to.latency_budget().duration != from.latency_budget().duration)\n    {\n        to.latency_budget() = from.latency_budget();\n        to.latency_budget().hasChanged = true;\n    }\n    if (first_time && !(to.liveliness() == from.liveliness()))\n    {\n        to.liveliness() = from.liveliness();\n        to.liveliness().hasChanged = true;\n    }\n    if (first_time && !(to.reliability() == from.reliability()))\n    {\n        to.reliability() = from.reliability();\n        to.reliability().hasChanged = true;\n    }\n    if (first_time && to.ownership().kind != from.ownership().kind)\n    {\n        to.ownership() = from.ownership();\n        to.ownership().hasChanged = true;\n    }\n    if (first_time && to.destination_order().kind != from.destination_order().kind)\n    {\n        to.destination_order() = from.destination_order();\n        to.destination_order().hasChanged = true;\n    }\n    if (to.user_data().data_vec() != from.user_data().data_vec())\n    {\n        to.user_data() = from.user_data();\n        to.user_data().hasChanged = true;\n    }\n    if (to.time_based_filter().minimum_separation != from.time_based_filter().minimum_separation )\n    {\n        to.time_based_filter() = from.time_based_filter();\n        to.time_based_filter().hasChanged = true;\n    }\n    if (first_time || !(to.durability_service() == from.durability_service()))\n    {\n        to.durability_service() = from.durability_service();\n        to.durability_service().hasChanged = true;\n    }\n    if (to.lifespan().duration != from.lifespan().duration )\n    {\n        to.lifespan() = from.lifespan();\n        to.lifespan().hasChanged = true;\n    }\n    if (first_time && !(to.reliable_reader_qos() == from.reliable_reader_qos()))\n    {\n        to.reliable_reader_qos() = from.reliable_reader_qos();\n    }\n    if (first_time || !(to.type_consistency() == from.type_consistency()))\n    {\n        to.type_consistency() = from.type_consistency();\n        to.type_consistency().hasChanged = true;\n    }\n    if (first_time && (to.history().kind != from.history().kind ||\n            to.history().depth != from.history().depth))\n    {\n        to.history() = from.history();\n        to.history().hasChanged = true;\n    }\n    if (first_time && !(to.resource_limits() == from.resource_limits()))\n    {\n        to.resource_limits() = from.resource_limits();\n        to.resource_limits().hasChanged = true;\n    }\n    if (!(to.reader_data_lifecycle() == from.reader_data_lifecycle()))\n    {\n        to.reader_data_lifecycle() = from.reader_data_lifecycle();\n    }\n\n    if (to.expects_inline_qos() != from.expects_inline_qos())\n    {\n        to.expects_inline_qos(from.expects_inline_qos());\n    }\n\n    if (first_time && !(to.properties() == from.properties()))\n    {\n        to.properties() = from.properties();\n    }\n\n    if (first_time && !(to.endpoint() == from.endpoint()))\n    {\n        to.endpoint() = from.endpoint();\n    }\n\n    if (first_time && !(to.reader_resource_limits() == from.reader_resource_limits()))\n    {\n        to.reader_resource_limits() = from.reader_resource_limits();\n    }\n}\n\nfastrtps::TopicAttributes DataReaderImpl::topic_attributes() const\n{\n    fastrtps::TopicAttributes topic_att;\n    topic_att.topicKind = type_->m_isGetKeyDefined ? WITH_KEY : NO_KEY;\n    topic_att.topicName = topic_->get_name();\n    topic_att.topicDataType = topic_->get_type_name();\n    topic_att.historyQos = qos_.history();\n    topic_att.resourceLimitsQos = qos_.resource_limits();\n    if (type_->type_object())\n    {\n        topic_att.type = *type_->type_object();\n    }\n    if (type_->type_identifier())\n    {\n        topic_att.type_id = *type_->type_identifier();\n    }\n    if (type_->type_information())\n    {\n        topic_att.type_information = *type_->type_information();\n    }\n    topic_att.auto_fill_type_object = type_->auto_fill_type_object();\n    topic_att.auto_fill_type_information = type_->auto_fill_type_information();\n\n    return topic_att;\n}\n\nDataReaderListener* DataReaderImpl::get_listener_for(\n        const StatusMask& status)\n{\n    if (listener_ != nullptr &&\n            user_datareader_->get_status_mask().is_active(status))\n    {\n        return listener_;\n    }\n    return subscriber_->get_listener_for(status);\n}\n\nstd::shared_ptr<IPayloadPool> DataReaderImpl::get_payload_pool()\n{\n    PoolConfig config = PoolConfig::from_history_attributes(history_.m_att );\n\n    if (!payload_pool_)\n    {\n        payload_pool_ = TopicPayloadPoolRegistry::get(topic_->get_name(), config);\n        sample_pool_ = std::make_shared<detail::SampleLoanManager>(config, type_);\n    }\n\n    payload_pool_->reserve_history(config, true);\n    return payload_pool_;\n}\n\nvoid DataReaderImpl::release_payload_pool()\n{\n    assert(payload_pool_);\n\n    PoolConfig config = PoolConfig::from_history_attributes(history_.m_att);\n    payload_pool_->release_history(config, true);\n\n    TopicPayloadPoolRegistry::release(payload_pool_);\n}\n\nReturnCode_t DataReaderImpl::check_datasharing_compatible(\n        const ReaderAttributes& reader_attributes,\n        bool& is_datasharing_compatible) const\n{\n#if HAVE_SECURITY\n    bool has_security_enabled = subscriber_->rtps_participant()->is_security_enabled_for_reader(reader_attributes);\n#else\n    (void) reader_attributes;\n#endif // HAVE_SECURITY\n\n    bool has_key = type_->m_isGetKeyDefined;\n\n    is_datasharing_compatible = false;\n    switch (qos_.data_sharing().kind())\n    {\n        case DataSharingKind::OFF:\n            return ReturnCode_t::RETCODE_OK;\n            break;\n        case DataSharingKind::ON:\n#if HAVE_SECURITY\n            if (has_security_enabled)\n            {\n                logError(DATA_READER, \"Data sharing cannot be used with security protection.\");\n                return ReturnCode_t::RETCODE_NOT_ALLOWED_BY_SECURITY;\n            }\n#endif // if HAVE_SECURITY\n            if (!type_.is_bounded())\n            {\n                logInfo(DATA_READER, \"Data sharing cannot be used with unbounded data types\");\n                return ReturnCode_t::RETCODE_BAD_PARAMETER;\n            }\n\n            if (has_key)\n            {\n                logError(DATA_READER, \"Data sharing cannot be used with keyed data types\");\n                return ReturnCode_t::RETCODE_BAD_PARAMETER;\n            }\n\n            is_datasharing_compatible = true;\n            return ReturnCode_t::RETCODE_OK;\n            break;\n        case DataSharingKind::AUTO:\n#if HAVE_SECURITY\n            if (has_security_enabled)\n            {\n                logInfo(DATA_READER, \"Data sharing disabled due to security configuration.\");\n                return ReturnCode_t::RETCODE_OK;\n            }\n#endif // if HAVE_SECURITY\n\n            if (!type_.is_bounded())\n            {\n                logInfo(DATA_READER, \"Data sharing disabled because data type is not bounded\");\n                return ReturnCode_t::RETCODE_OK;\n            }\n\n            if (has_key)\n            {\n                logInfo(DATA_READER, \"Data sharing disabled because data type is keyed\");\n                return ReturnCode_t::RETCODE_OK;\n            }\n\n            is_datasharing_compatible = true;\n            return ReturnCode_t::RETCODE_OK;\n            break;\n        default:\n            logError(DATA_WRITER, \"Unknown data sharing kind.\");\n            return ReturnCode_t::RETCODE_BAD_PARAMETER;\n    }\n}\n\nbool DataReaderImpl::is_sample_valid(\n        const void* data,\n        const SampleInfo* info) const\n{\n    return reader_->is_sample_valid(data, info->sample_identity.writer_guid(), info->sample_identity.sequence_number());\n}\n\n} /* namespace dds */\n} /* namespace fastdds */\n} /* namespace eprosima */\n", "idx": 1, "id": 21517, "msg": "Shouldn't we do `payload_pool_.reset()` here ?", "proj": "eProsima-Fast-DDS", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -84,10 +84,18 @@ static CALI_BPF_INLINE int calico_xdp(struct xdp_md *xdp)\n \t\tgoto allow;\n \t}\n \n-\t// Allow a packet if it hits an entry in the failsafe map\n+\t// Allow a packet if it hits an entry in the inbound ports failsafe map\n \tif (is_failsafe_in(ctx.state->ip_proto, ctx.state->dport, ctx.state->ip_src)) {\n \t\tCALI_DEBUG(\"Inbound failsafe port: %d. Skip policy\\n\", ctx.state->dport);\n-\t\tgoto allow_with_metadata;\n+\t\tctx.state->pol_rc = CALI_POL_ALLOW;\n+\t\tgoto allow;\n+\t}\n+\n+\t// Allow a packet if it hits an entry in the outbound ports failsafe map\n+\tif (is_failsafe_out(ctx.state->ip_proto, ctx.state->sport, ctx.state->ip_src)) {\n+\t\tCALI_DEBUG(\"Outbound failsafe port: %d. Skip policy\\n\", ctx.state->sport);\n+\t\tctx.state->pol_rc = CALI_POL_ALLOW;\n+\t\tgoto allow;\n \t}\n \n \t// Jump to the policy program", "y": 1, "oldf": "// Project Calico BPF dataplane programs.\n// Copyright (c) 2020-2021 Tigera, Inc. All rights reserved.\n//\n// This program is free software; you can redistribute it and/or modify\n// it under the terms of the GNU General Public License as published by\n// the Free Software Foundation; either version 2 of the License, or\n// (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU General Public License for more details.\n//\n// You should have received a copy of the GNU General Public License along\n// with this program; if not, write to the Free Software Foundation, Inc.,\n// 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n\n// NOTE: THIS FILE IS NOT YET IN ACTIVE USE.\n\n#include <linux/if_ether.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/icmp.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n\n// stdbool.h has no deps so it's OK to include; stdint.h pulls in parts\n// of the std lib that aren't compatible with BPF.\n#include <stdbool.h>\n\n#include \"bpf.h\"\n#include \"types.h\"\n#include \"log.h\"\n#include \"skb.h\"\n#include \"routes.h\"\n#include \"reasons.h\"\n#include \"icmp.h\"\n#include \"fib.h\"\n#include \"parsing.h\"\n#include \"failsafe.h\"\n#include \"jump.h\"\n#include \"metadata.h\"\n\n/* calico_xdp is the main function used in all of the xdp programs */\nstatic CALI_BPF_INLINE int calico_xdp(struct xdp_md *xdp)\n{\n\t/* Initialise the context, which is stored on the stack, and the state, which\n\t * we use to pass data from one program to the next via tail calls. */\n\tstruct cali_tc_ctx ctx = {\n\t\t.state = state_get(),\n\t\t.xdp = xdp,\n\t\t.fwd = {\n\t\t\t.res = XDP_PASS, // TODO: Adjust based on the design\n\t\t\t.reason = CALI_REASON_UNKNOWN,\n\t\t},\n\t};\n\n\tif (!ctx.state) {\n\t\tCALI_DEBUG(\"State map lookup failed: PASS\\n\");\n\t\treturn XDP_PASS; // TODO: Adjust base on the design\n\t}\n\n\t__builtin_memset(ctx.state, 0, sizeof(*ctx.state));\n\n\tif (CALI_LOG_LEVEL >= CALI_LOG_LEVEL_INFO) {\n\t\tctx.state->prog_start_time = bpf_ktime_get_ns();\n\t}\n\n\t// Parse packets and drop malformed and unsupported ones\n\tswitch (parse_packet_ip(&ctx)) {\n\tcase PARSING_ERROR:\n\t\tgoto deny;\n\tcase PARSING_ALLOW_WITHOUT_ENFORCING_POLICY:\n\t\tgoto allow;\n\t}\n\n\ttc_state_fill_from_iphdr(&ctx);\n\n\tswitch(tc_state_fill_from_nexthdr(&ctx)) {\n\tcase PARSING_ERROR:\n\t\tgoto deny;\n\tcase PARSING_ALLOW_WITHOUT_ENFORCING_POLICY:\n\t\tgoto allow;\n\t}\n\n\t// Allow a packet if it hits an entry in the failsafe map\n\tif (is_failsafe_in(ctx.state->ip_proto, ctx.state->dport, ctx.state->ip_src)) {\n\t\tCALI_DEBUG(\"Inbound failsafe port: %d. Skip policy\\n\", ctx.state->dport);\n\t\tgoto allow_with_metadata;\n\t}\n\n\t// Jump to the policy program\n\tCALI_DEBUG(\"About to jump to policy program.\\n\");\n\tbpf_tail_call(xdp, &cali_jump, PROG_INDEX_POLICY);\n\nallow:\n\treturn XDP_PASS;\n\nallow_with_metadata:\n\tif (xdp2tc_set_metadata(xdp, CALI_META_ACCEPTED_BY_XDP)) {\n\t\tCALI_DEBUG(\"Failed to set metadata for TC\\n\");\n\t}\n\treturn XDP_PASS;\n\ndeny:\n\treturn XDP_DROP;\n}\n\n/* This program contains \"default\" implementations of the policy program\n * which ip will load for us when we're attaching a program to a xdp hook.\n * This allows us to control the behaviour in the window before Felix replaces\n * the policy program with its generated version.*/\n__attribute__((section(\"1/0\")))\nint calico_xdp_norm_pol_tail(struct xdp_md *xdp)\n{\n\tCALI_DEBUG(\"Entering normal policy tail call: PASS\\n\");\n\treturn XDP_PASS;\n}\n\n__attribute__((section(\"1/1\")))\nint calico_xdp_accepted_entrypoint(struct xdp_md *xdp)\n{\n\tCALI_DEBUG(\"Entring calico_xdp_accepted_entrypoint\\n\");\n\t// Share with TC the packet is already accepted and accept it there too.\n\tif (xdp2tc_set_metadata(xdp, CALI_META_ACCEPTED_BY_XDP)) {\n\t\tCALI_DEBUG(\"Failed to set metadata for TC\\n\");\n\t}\n\treturn XDP_PASS;\n}\n\n#ifndef CALI_ENTRYPOINT_NAME_XDP\n#define CALI_ENTRYPOINT_NAME_XDP calico_entrypoint_xdp\n#endif\n\n// Entrypoint with definable name.  It's useful to redefine the name for each entrypoint\n// because the name is exposed by bpftool et al.\n__attribute__((section(XSTR(CALI_ENTRYPOINT_NAME_XDP))))\nint xdp_calico_entry(struct xdp_md *xdp)\n{\n\treturn calico_xdp(xdp);\n}\n\nchar ____license[] __attribute__((section(\"license\"), used)) = \"GPL\";\n", "idx": 1, "id": 19290, "msg": "the XDP program runs only in RX path. I am not totally 100% sure how the failsafe works, but is checking \"is_failsafe_out\" valid for RX path?", "proj": "projectcalico-felix", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -161,13 +161,13 @@ final class Stemmer {\n   }\n \n   /** folds lowercase variant of word (title cased) to lowerBuffer */\n-  private void caseFoldLower(char word[], int length) {\n+  private void caseFoldLower(char[] word, int length) {\n     lowerBuffer = ArrayUtil.grow(lowerBuffer, length);\n     System.arraycopy(word, 0, lowerBuffer, 0, length);\n     lowerBuffer[0] = dictionary.caseFold(lowerBuffer[0]);\n   }\n \n-  private List<CharsRef> doStem(char word[], int length, boolean caseVariant) {\n+  private List<CharsRef> doStem(char[] word, int length, boolean caseVariant) {\n     List<CharsRef> stems = new ArrayList<>();\n     IntsRef forms = dictionary.lookupWord(word, 0, length);\n     if (forms != null) {", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.lucene.analysis.hunspell;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport org.apache.lucene.analysis.CharArraySet;\nimport org.apache.lucene.store.ByteArrayDataInput;\nimport org.apache.lucene.util.ArrayUtil;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.CharsRef;\nimport org.apache.lucene.util.IntsRef;\nimport org.apache.lucene.util.automaton.CharacterRunAutomaton;\nimport org.apache.lucene.util.fst.FST;\nimport org.apache.lucene.util.fst.Outputs;\n\n/**\n * Stemmer uses the affix rules declared in the Dictionary to generate one or more stems for a word.\n * It conforms to the algorithm in the original hunspell algorithm, including recursive suffix\n * stripping.\n */\nfinal class Stemmer {\n  private final Dictionary dictionary;\n  private final BytesRef scratch = new BytesRef();\n  private final StringBuilder segment = new StringBuilder();\n  private final ByteArrayDataInput affixReader;\n\n  // used for normalization\n  private final StringBuilder scratchSegment = new StringBuilder();\n  private char scratchBuffer[] = new char[32];\n\n  // it's '1' if we have no stem exceptions, otherwise every other form\n  // is really an ID pointing to the exception table\n  private final int formStep;\n\n  /**\n   * Constructs a new Stemmer which will use the provided Dictionary to create its stems.\n   *\n   * @param dictionary Dictionary that will be used to create the stems\n   */\n  public Stemmer(Dictionary dictionary) {\n    this.dictionary = dictionary;\n    this.affixReader = new ByteArrayDataInput(dictionary.affixData);\n    for (int level = 0; level < 3; level++) {\n      if (dictionary.prefixes != null) {\n        prefixArcs[level] = new FST.Arc<>();\n        prefixReaders[level] = dictionary.prefixes.getBytesReader();\n      }\n      if (dictionary.suffixes != null) {\n        suffixArcs[level] = new FST.Arc<>();\n        suffixReaders[level] = dictionary.suffixes.getBytesReader();\n      }\n    }\n    formStep = dictionary.hasStemExceptions ? 2 : 1;\n  }\n\n  /**\n   * Find the stem(s) of the provided word.\n   *\n   * @param word Word to find the stems for\n   * @return List of stems for the word\n   */\n  public List<CharsRef> stem(String word) {\n    return stem(word.toCharArray(), word.length());\n  }\n\n  /**\n   * Find the stem(s) of the provided word\n   *\n   * @param word Word to find the stems for\n   * @return List of stems for the word\n   */\n  public List<CharsRef> stem(char word[], int length) {\n\n    if (dictionary.needsInputCleaning) {\n      scratchSegment.setLength(0);\n      scratchSegment.append(word, 0, length);\n      CharSequence cleaned = dictionary.cleanInput(scratchSegment, segment);\n      scratchBuffer = ArrayUtil.grow(scratchBuffer, cleaned.length());\n      length = segment.length();\n      segment.getChars(0, length, scratchBuffer, 0);\n      word = scratchBuffer;\n    }\n\n    int caseType = caseOf(word, length);\n    if (caseType == UPPER_CASE) {\n      // upper: union exact, title, lower\n      caseFoldTitle(word, length);\n      caseFoldLower(titleBuffer, length);\n      List<CharsRef> list = doStem(word, length, false);\n      list.addAll(doStem(titleBuffer, length, true));\n      list.addAll(doStem(lowerBuffer, length, true));\n      return list;\n    } else if (caseType == TITLE_CASE) {\n      // title: union exact, lower\n      caseFoldLower(word, length);\n      List<CharsRef> list = doStem(word, length, false);\n      list.addAll(doStem(lowerBuffer, length, true));\n      return list;\n    } else {\n      // exact match only\n      return doStem(word, length, false);\n    }\n  }\n\n  // temporary buffers for case variants\n  private char[] lowerBuffer = new char[8];\n  private char[] titleBuffer = new char[8];\n\n  private static final int EXACT_CASE = 0;\n  private static final int TITLE_CASE = 1;\n  private static final int UPPER_CASE = 2;\n\n  /** returns EXACT_CASE,TITLE_CASE, or UPPER_CASE type for the word */\n  private int caseOf(char word[], int length) {\n    if (dictionary.ignoreCase || length == 0 || !Character.isUpperCase(word[0])) {\n      return EXACT_CASE;\n    }\n\n    // determine if we are title or lowercase (or something funky, in which it's exact)\n    boolean seenUpper = false;\n    boolean seenLower = false;\n    for (int i = 1; i < length; i++) {\n      boolean v = Character.isUpperCase(word[i]);\n      seenUpper |= v;\n      seenLower |= !v;\n    }\n\n    if (!seenLower) {\n      return UPPER_CASE;\n    } else if (!seenUpper) {\n      return TITLE_CASE;\n    } else {\n      return EXACT_CASE;\n    }\n  }\n\n  /** folds titlecase variant of word to titleBuffer */\n  private void caseFoldTitle(char word[], int length) {\n    titleBuffer = ArrayUtil.grow(titleBuffer, length);\n    System.arraycopy(word, 0, titleBuffer, 0, length);\n    for (int i = 1; i < length; i++) {\n      titleBuffer[i] = dictionary.caseFold(titleBuffer[i]);\n    }\n  }\n\n  /** folds lowercase variant of word (title cased) to lowerBuffer */\n  private void caseFoldLower(char word[], int length) {\n    lowerBuffer = ArrayUtil.grow(lowerBuffer, length);\n    System.arraycopy(word, 0, lowerBuffer, 0, length);\n    lowerBuffer[0] = dictionary.caseFold(lowerBuffer[0]);\n  }\n\n  private List<CharsRef> doStem(char word[], int length, boolean caseVariant) {\n    List<CharsRef> stems = new ArrayList<>();\n    IntsRef forms = dictionary.lookupWord(word, 0, length);\n    if (forms != null) {\n      for (int i = 0; i < forms.length; i += formStep) {\n        boolean checkKeepCase = caseVariant && dictionary.keepcase != -1;\n        boolean checkNeedAffix = dictionary.needaffix != -1;\n        boolean checkOnlyInCompound = dictionary.onlyincompound != -1;\n        if (checkKeepCase || checkNeedAffix || checkOnlyInCompound) {\n          dictionary.flagLookup.get(forms.ints[forms.offset + i], scratch);\n          char wordFlags[] = Dictionary.decodeFlags(scratch);\n          // we are looking for a case variant, but this word does not allow it\n          if (checkKeepCase && Dictionary.hasFlag(wordFlags, (char) dictionary.keepcase)) {\n            continue;\n          }\n          // we can't add this form, it's a pseudostem requiring an affix\n          if (checkNeedAffix && Dictionary.hasFlag(wordFlags, (char) dictionary.needaffix)) {\n            continue;\n          }\n          // we can't add this form, it only belongs inside a compound word\n          if (checkOnlyInCompound\n              && Dictionary.hasFlag(wordFlags, (char) dictionary.onlyincompound)) {\n            continue;\n          }\n        }\n        stems.add(newStem(word, length, forms, i));\n      }\n    }\n    try {\n      boolean v =\n          stems.addAll(stem(word, length, -1, -1, -1, 0, true, true, false, false, caseVariant));\n    } catch (IOException bogus) {\n      throw new RuntimeException(bogus);\n    }\n    return stems;\n  }\n\n  /**\n   * Find the unique stem(s) of the provided word\n   *\n   * @param word Word to find the stems for\n   * @return List of stems for the word\n   */\n  public List<CharsRef> uniqueStems(char word[], int length) {\n    List<CharsRef> stems = stem(word, length);\n    if (stems.size() < 2) {\n      return stems;\n    }\n    CharArraySet terms = new CharArraySet(8, dictionary.ignoreCase);\n    List<CharsRef> deduped = new ArrayList<>();\n    for (CharsRef s : stems) {\n      if (!terms.contains(s)) {\n        deduped.add(s);\n        terms.add(s);\n      }\n    }\n    return deduped;\n  }\n\n  private CharsRef newStem(char buffer[], int length, IntsRef forms, int formID) {\n    final String exception;\n    if (dictionary.hasStemExceptions) {\n      int exceptionID = forms.ints[forms.offset + formID + 1];\n      if (exceptionID > 0) {\n        exception = dictionary.getStemException(exceptionID);\n      } else {\n        exception = null;\n      }\n    } else {\n      exception = null;\n    }\n\n    if (dictionary.needsOutputCleaning) {\n      scratchSegment.setLength(0);\n      if (exception != null) {\n        scratchSegment.append(exception);\n      } else {\n        scratchSegment.append(buffer, 0, length);\n      }\n      try {\n        Dictionary.applyMappings(dictionary.oconv, scratchSegment);\n      } catch (IOException bogus) {\n        throw new RuntimeException(bogus);\n      }\n      char cleaned[] = new char[scratchSegment.length()];\n      scratchSegment.getChars(0, cleaned.length, cleaned, 0);\n      return new CharsRef(cleaned, 0, cleaned.length);\n    } else {\n      if (exception != null) {\n        return new CharsRef(exception);\n      } else {\n        return new CharsRef(buffer, 0, length);\n      }\n    }\n  }\n\n  // some state for traversing FSTs\n  final FST.BytesReader prefixReaders[] = new FST.BytesReader[3];\n\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  final FST.Arc<IntsRef> prefixArcs[] = new FST.Arc[3];\n\n  final FST.BytesReader suffixReaders[] = new FST.BytesReader[3];\n\n  @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n  final FST.Arc<IntsRef> suffixArcs[] = new FST.Arc[3];\n\n  /**\n   * Generates a list of stems for the provided word\n   *\n   * @param word Word to generate the stems for\n   * @param previous previous affix that was removed (so we dont remove same one twice)\n   * @param prevFlag Flag from a previous stemming step that need to be cross-checked with any\n   *     affixes in this recursive step\n   * @param prefixFlag flag of the most inner removed prefix, so that when removing a suffix, it's\n   *     also checked against the word\n   * @param recursionDepth current recursiondepth\n   * @param doPrefix true if we should remove prefixes\n   * @param doSuffix true if we should remove suffixes\n   * @param previousWasPrefix true if the previous removal was a prefix: if we are removing a\n   *     suffix, and it has no continuation requirements, it's ok. but two prefixes\n   *     (COMPLEXPREFIXES) or two suffixes must have continuation requirements to recurse.\n   * @param circumfix true if the previous prefix removal was signed as a circumfix this means inner\n   *     most suffix must also contain circumfix flag.\n   * @param caseVariant true if we are searching for a case variant. if the word has KEEPCASE flag\n   *     it cannot succeed.\n   * @return List of stems, or empty list if no stems are found\n   */\n  private List<CharsRef> stem(\n      char word[],\n      int length,\n      int previous,\n      int prevFlag,\n      int prefixFlag,\n      int recursionDepth,\n      boolean doPrefix,\n      boolean doSuffix,\n      boolean previousWasPrefix,\n      boolean circumfix,\n      boolean caseVariant)\n      throws IOException {\n\n    // TODO: allow this stuff to be reused by tokenfilter\n    List<CharsRef> stems = new ArrayList<>();\n\n    if (doPrefix && dictionary.prefixes != null) {\n      FST<IntsRef> fst = dictionary.prefixes;\n      Outputs<IntsRef> outputs = fst.outputs;\n      FST.BytesReader bytesReader = prefixReaders[recursionDepth];\n      FST.Arc<IntsRef> arc = prefixArcs[recursionDepth];\n      fst.getFirstArc(arc);\n      IntsRef NO_OUTPUT = outputs.getNoOutput();\n      IntsRef output = NO_OUTPUT;\n      int limit = dictionary.fullStrip ? length : length - 1;\n      for (int i = 0; i < limit; i++) {\n        if (i > 0) {\n          int ch = word[i - 1];\n          if (fst.findTargetArc(ch, arc, arc, bytesReader) == null) {\n            break;\n          } else if (arc.output() != NO_OUTPUT) {\n            output = fst.outputs.add(output, arc.output());\n          }\n        }\n        IntsRef prefixes = null;\n        if (!arc.isFinal()) {\n          continue;\n        } else {\n          prefixes = fst.outputs.add(output, arc.nextFinalOutput());\n        }\n\n        for (int j = 0; j < prefixes.length; j++) {\n          int prefix = prefixes.ints[prefixes.offset + j];\n          if (prefix == previous) {\n            continue;\n          }\n          affixReader.setPosition(8 * prefix);\n          char flag = (char) (affixReader.readShort() & 0xffff);\n          char stripOrd = (char) (affixReader.readShort() & 0xffff);\n          int condition = (char) (affixReader.readShort() & 0xffff);\n          boolean crossProduct = (condition & 1) == 1;\n          condition >>>= 1;\n          char append = (char) (affixReader.readShort() & 0xffff);\n\n          final boolean compatible;\n          if (recursionDepth == 0) {\n            if (dictionary.onlyincompound == -1) {\n              compatible = true;\n            } else {\n              // check if affix is allowed in a non-compound word\n              dictionary.flagLookup.get(append, scratch);\n              char appendFlags[] = Dictionary.decodeFlags(scratch);\n              compatible = !Dictionary.hasFlag(appendFlags, (char) dictionary.onlyincompound);\n            }\n          } else if (crossProduct) {\n            // cross check incoming continuation class (flag of previous affix) against list.\n            dictionary.flagLookup.get(append, scratch);\n            char appendFlags[] = Dictionary.decodeFlags(scratch);\n            assert prevFlag >= 0;\n            boolean allowed =\n                dictionary.onlyincompound == -1\n                    || !Dictionary.hasFlag(appendFlags, (char) dictionary.onlyincompound);\n            compatible = allowed && hasCrossCheckedFlag((char) prevFlag, appendFlags, false);\n          } else {\n            compatible = false;\n          }\n\n          if (compatible) {\n            int deAffixedStart = i;\n            int deAffixedLength = length - deAffixedStart;\n\n            int stripStart = dictionary.stripOffsets[stripOrd];\n            int stripEnd = dictionary.stripOffsets[stripOrd + 1];\n            int stripLength = stripEnd - stripStart;\n\n            if (!checkCondition(\n                condition,\n                dictionary.stripData,\n                stripStart,\n                stripLength,\n                word,\n                deAffixedStart,\n                deAffixedLength)) {\n              continue;\n            }\n\n            char strippedWord[] = new char[stripLength + deAffixedLength];\n            System.arraycopy(dictionary.stripData, stripStart, strippedWord, 0, stripLength);\n            System.arraycopy(word, deAffixedStart, strippedWord, stripLength, deAffixedLength);\n\n            List<CharsRef> stemList =\n                applyAffix(\n                    strippedWord,\n                    strippedWord.length,\n                    prefix,\n                    -1,\n                    recursionDepth,\n                    true,\n                    circumfix,\n                    caseVariant);\n\n            stems.addAll(stemList);\n          }\n        }\n      }\n    }\n\n    if (doSuffix && dictionary.suffixes != null) {\n      FST<IntsRef> fst = dictionary.suffixes;\n      Outputs<IntsRef> outputs = fst.outputs;\n      FST.BytesReader bytesReader = suffixReaders[recursionDepth];\n      FST.Arc<IntsRef> arc = suffixArcs[recursionDepth];\n      fst.getFirstArc(arc);\n      IntsRef NO_OUTPUT = outputs.getNoOutput();\n      IntsRef output = NO_OUTPUT;\n      int limit = dictionary.fullStrip ? 0 : 1;\n      for (int i = length; i >= limit; i--) {\n        if (i < length) {\n          int ch = word[i];\n          if (fst.findTargetArc(ch, arc, arc, bytesReader) == null) {\n            break;\n          } else if (arc.output() != NO_OUTPUT) {\n            output = fst.outputs.add(output, arc.output());\n          }\n        }\n        IntsRef suffixes = null;\n        if (!arc.isFinal()) {\n          continue;\n        } else {\n          suffixes = fst.outputs.add(output, arc.nextFinalOutput());\n        }\n\n        for (int j = 0; j < suffixes.length; j++) {\n          int suffix = suffixes.ints[suffixes.offset + j];\n          if (suffix == previous) {\n            continue;\n          }\n          affixReader.setPosition(8 * suffix);\n          char flag = (char) (affixReader.readShort() & 0xffff);\n          char stripOrd = (char) (affixReader.readShort() & 0xffff);\n          int condition = (char) (affixReader.readShort() & 0xffff);\n          boolean crossProduct = (condition & 1) == 1;\n          condition >>>= 1;\n          char append = (char) (affixReader.readShort() & 0xffff);\n\n          final boolean compatible;\n          if (recursionDepth == 0) {\n            if (dictionary.onlyincompound == -1) {\n              compatible = true;\n            } else {\n              // check if affix is allowed in a non-compound word\n              dictionary.flagLookup.get(append, scratch);\n              char appendFlags[] = Dictionary.decodeFlags(scratch);\n              compatible = !Dictionary.hasFlag(appendFlags, (char) dictionary.onlyincompound);\n            }\n          } else if (crossProduct) {\n            // cross check incoming continuation class (flag of previous affix) against list.\n            dictionary.flagLookup.get(append, scratch);\n            char appendFlags[] = Dictionary.decodeFlags(scratch);\n            assert prevFlag >= 0;\n            boolean allowed =\n                dictionary.onlyincompound == -1\n                    || !Dictionary.hasFlag(appendFlags, (char) dictionary.onlyincompound);\n            compatible =\n                allowed && hasCrossCheckedFlag((char) prevFlag, appendFlags, previousWasPrefix);\n          } else {\n            compatible = false;\n          }\n\n          if (compatible) {\n            int appendLength = length - i;\n            int deAffixedLength = length - appendLength;\n\n            int stripStart = dictionary.stripOffsets[stripOrd];\n            int stripEnd = dictionary.stripOffsets[stripOrd + 1];\n            int stripLength = stripEnd - stripStart;\n\n            if (!checkCondition(\n                condition,\n                word,\n                0,\n                deAffixedLength,\n                dictionary.stripData,\n                stripStart,\n                stripLength)) {\n              continue;\n            }\n\n            char strippedWord[] = new char[stripLength + deAffixedLength];\n            System.arraycopy(word, 0, strippedWord, 0, deAffixedLength);\n            System.arraycopy(\n                dictionary.stripData, stripStart, strippedWord, deAffixedLength, stripLength);\n\n            List<CharsRef> stemList =\n                applyAffix(\n                    strippedWord,\n                    strippedWord.length,\n                    suffix,\n                    prefixFlag,\n                    recursionDepth,\n                    false,\n                    circumfix,\n                    caseVariant);\n\n            stems.addAll(stemList);\n          }\n        }\n      }\n    }\n\n    return stems;\n  }\n\n  /** checks condition of the concatenation of two strings */\n  // note: this is pretty stupid, we really should subtract strip from the condition up front and\n  // just check the stem\n  // but this is a little bit more complicated.\n  private boolean checkCondition(\n      int condition, char c1[], int c1off, int c1len, char c2[], int c2off, int c2len) {\n    if (condition != 0) {\n      CharacterRunAutomaton pattern = dictionary.patterns.get(condition);\n      int state = 0;\n      for (int i = c1off; i < c1off + c1len; i++) {\n        state = pattern.step(state, c1[i]);\n        if (state == -1) {\n          return false;\n        }\n      }\n      for (int i = c2off; i < c2off + c2len; i++) {\n        state = pattern.step(state, c2[i]);\n        if (state == -1) {\n          return false;\n        }\n      }\n      return pattern.isAccept(state);\n    }\n    return true;\n  }\n\n  /**\n   * Applies the affix rule to the given word, producing a list of stems if any are found\n   *\n   * @param strippedWord Word the affix has been removed and the strip added\n   * @param length valid length of stripped word\n   * @param affix HunspellAffix representing the affix rule itself\n   * @param prefixFlag when we already stripped a prefix, we cant simply recurse and check the\n   *     suffix, unless both are compatible so we must check dictionary form against both to add it\n   *     as a stem!\n   * @param recursionDepth current recursion depth\n   * @param prefix true if we are removing a prefix (false if it's a suffix)\n   * @return List of stems for the word, or an empty list if none are found\n   */\n  List<CharsRef> applyAffix(\n      char strippedWord[],\n      int length,\n      int affix,\n      int prefixFlag,\n      int recursionDepth,\n      boolean prefix,\n      boolean circumfix,\n      boolean caseVariant)\n      throws IOException {\n    // TODO: just pass this in from before, no need to decode it twice\n    affixReader.setPosition(8 * affix);\n    char flag = (char) (affixReader.readShort() & 0xffff);\n    affixReader.skipBytes(2); // strip\n    int condition = (char) (affixReader.readShort() & 0xffff);\n    boolean crossProduct = (condition & 1) == 1;\n    condition >>>= 1;\n    char append = (char) (affixReader.readShort() & 0xffff);\n\n    List<CharsRef> stems = new ArrayList<>();\n\n    IntsRef forms = dictionary.lookupWord(strippedWord, 0, length);\n    if (forms != null) {\n      for (int i = 0; i < forms.length; i += formStep) {\n        dictionary.flagLookup.get(forms.ints[forms.offset + i], scratch);\n        char wordFlags[] = Dictionary.decodeFlags(scratch);\n        if (Dictionary.hasFlag(wordFlags, flag)) {\n          // confusing: in this one exception, we already chained the first prefix against the\n          // second,\n          // so it doesnt need to be checked against the word\n          boolean chainedPrefix = dictionary.complexPrefixes && recursionDepth == 1 && prefix;\n          if (chainedPrefix == false\n              && prefixFlag >= 0\n              && !Dictionary.hasFlag(wordFlags, (char) prefixFlag)) {\n            // see if we can chain prefix thru the suffix continuation class (only if it has any!)\n            dictionary.flagLookup.get(append, scratch);\n            char appendFlags[] = Dictionary.decodeFlags(scratch);\n            if (!hasCrossCheckedFlag((char) prefixFlag, appendFlags, false)) {\n              continue;\n            }\n          }\n\n          // if circumfix was previously set by a prefix, we must check this suffix,\n          // to ensure it has it, and vice versa\n          if (dictionary.circumfix != -1) {\n            dictionary.flagLookup.get(append, scratch);\n            char appendFlags[] = Dictionary.decodeFlags(scratch);\n            boolean suffixCircumfix = Dictionary.hasFlag(appendFlags, (char) dictionary.circumfix);\n            if (circumfix != suffixCircumfix) {\n              continue;\n            }\n          }\n\n          // we are looking for a case variant, but this word does not allow it\n          if (caseVariant\n              && dictionary.keepcase != -1\n              && Dictionary.hasFlag(wordFlags, (char) dictionary.keepcase)) {\n            continue;\n          }\n          // we aren't decompounding (yet)\n          if (dictionary.onlyincompound != -1\n              && Dictionary.hasFlag(wordFlags, (char) dictionary.onlyincompound)) {\n            continue;\n          }\n          stems.add(newStem(strippedWord, length, forms, i));\n        }\n      }\n    }\n\n    // if a circumfix flag is defined in the dictionary, and we are a prefix, we need to check if we\n    // have that flag\n    if (dictionary.circumfix != -1 && !circumfix && prefix) {\n      dictionary.flagLookup.get(append, scratch);\n      char appendFlags[] = Dictionary.decodeFlags(scratch);\n      circumfix = Dictionary.hasFlag(appendFlags, (char) dictionary.circumfix);\n    }\n\n    if (crossProduct) {\n      if (recursionDepth == 0) {\n        if (prefix) {\n          // we took away the first prefix.\n          // COMPLEXPREFIXES = true:  combine with a second prefix and another suffix\n          // COMPLEXPREFIXES = false: combine with a suffix\n          stems.addAll(\n              stem(\n                  strippedWord,\n                  length,\n                  affix,\n                  flag,\n                  flag,\n                  ++recursionDepth,\n                  dictionary.complexPrefixes && dictionary.twoStageAffix,\n                  true,\n                  true,\n                  circumfix,\n                  caseVariant));\n        } else if (dictionary.complexPrefixes == false && dictionary.twoStageAffix) {\n          // we took away a suffix.\n          // COMPLEXPREFIXES = true: we don't recurse! only one suffix allowed\n          // COMPLEXPREFIXES = false: combine with another suffix\n          stems.addAll(\n              stem(\n                  strippedWord,\n                  length,\n                  affix,\n                  flag,\n                  prefixFlag,\n                  ++recursionDepth,\n                  false,\n                  true,\n                  false,\n                  circumfix,\n                  caseVariant));\n        }\n      } else if (recursionDepth == 1) {\n        if (prefix && dictionary.complexPrefixes) {\n          // we took away the second prefix: go look for another suffix\n          stems.addAll(\n              stem(\n                  strippedWord,\n                  length,\n                  affix,\n                  flag,\n                  flag,\n                  ++recursionDepth,\n                  false,\n                  true,\n                  true,\n                  circumfix,\n                  caseVariant));\n        } else if (prefix == false\n            && dictionary.complexPrefixes == false\n            && dictionary.twoStageAffix) {\n          // we took away a prefix, then a suffix: go look for another suffix\n          stems.addAll(\n              stem(\n                  strippedWord,\n                  length,\n                  affix,\n                  flag,\n                  prefixFlag,\n                  ++recursionDepth,\n                  false,\n                  true,\n                  false,\n                  circumfix,\n                  caseVariant));\n        }\n      }\n    }\n\n    return stems;\n  }\n\n  /**\n   * Checks if the given flag cross checks with the given array of flags\n   *\n   * @param flag Flag to cross check with the array of flags\n   * @param flags Array of flags to cross check against. Can be {@code null}\n   * @return {@code true} if the flag is found in the array or the array is {@code null}, {@code\n   *     false} otherwise\n   */\n  private boolean hasCrossCheckedFlag(char flag, char[] flags, boolean matchEmpty) {\n    return (flags.length == 0 && matchEmpty) || Arrays.binarySearch(flags, flag) >= 0;\n  }\n}\n", "idx": 5, "id": 39421, "msg": "", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -167,24 +167,24 @@ define([\"appSettings\", \"browser\", \"events\", \"htmlMediaHelper\"], function (appSet\n         if (browser.mobile) {\n             return false;\n         }\n-\n-        var savedResult = appSettings.get(htmlMediaAutoplayAppStorageKey);\n-        return \"true\" === savedResult || \"false\" !== savedResult && null;\n     }\n \n-    function cueSupported() {\n+    function supportsCue() {\n         try {\n             var video = document.createElement(\"video\");\n             var style = document.createElement(\"style\");\n+\n             style.textContent = \"video::cue {background: inherit}\";\n             document.body.appendChild(style);\n             document.body.appendChild(video);\n+\n             var cue = window.getComputedStyle(video, \"::cue\").background;\n             document.body.removeChild(style);\n             document.body.removeChild(video);\n+\n             return !!cue.length;\n         } catch (err) {\n-            console.log(\"Error detecting cue support:\" + err);\n+            console.log(\"error detecting cue support: \" + err);\n             return false;\n         }\n     }", "y": 1, "oldf": "define([\"appSettings\", \"browser\", \"events\", \"htmlMediaHelper\"], function (appSettings, browser, events, htmlMediaHelper) {\n    \"use strict\";\n\n    function getBaseProfileOptions(item) {\n        var disableHlsVideoAudioCodecs = [];\n\n        if (item && htmlMediaHelper.enableHlsJsPlayer(item.RunTimeTicks, item.MediaType)) {\n            if (browser.edge || browser.msie) {\n                disableHlsVideoAudioCodecs.push(\"mp3\");\n            }\n\n            disableHlsVideoAudioCodecs.push(\"ac3\");\n            disableHlsVideoAudioCodecs.push(\"eac3\");\n            disableHlsVideoAudioCodecs.push(\"opus\");\n        }\n\n        return {\n            enableMkvProgressive: false,\n            disableHlsVideoAudioCodecs: disableHlsVideoAudioCodecs\n        };\n    }\n\n    function getDeviceProfileForWindowsUwp(item) {\n        return new Promise(function (resolve, reject) {\n            require([\"browserdeviceprofile\", \"environments/windows-uwp/mediacaps\"], function (profileBuilder, uwpMediaCaps) {\n                var profileOptions = getBaseProfileOptions(item);\n                profileOptions.supportsDts = uwpMediaCaps.supportsDTS();\n                profileOptions.supportsTrueHd = uwpMediaCaps.supportsDolby();\n                profileOptions.audioChannels = uwpMediaCaps.getAudioChannels();\n                resolve(profileBuilder(profileOptions));\n            });\n        });\n    }\n\n    function getDeviceProfile(item, options) {\n        options = options || {};\n\n        if (self.Windows) {\n            return getDeviceProfileForWindowsUwp(item);\n        }\n\n        return new Promise(function (resolve) {\n            require([\"browserdeviceprofile\"], function (profileBuilder) {\n                var profile;\n\n                if (window.NativeShell) {\n                    profile = window.NativeShell.AppHost.getDeviceProfile(profileBuilder);\n                } else {\n                    profile = profileBuilder(getBaseProfileOptions(item));\n\n                    if (item && !options.isRetry && \"allcomplexformats\" !== appSettings.get(\"subtitleburnin\")) {\n                        if (!browser.orsay && !browser.tizen) {\n                            profile.SubtitleProfiles.push({\n                                Format: \"ass\",\n                                Method: \"External\"\n                            });\n                            profile.SubtitleProfiles.push({\n                                Format: \"ssa\",\n                                Method: \"External\"\n                            });\n                        }\n                    }\n                }\n\n                resolve(profile);\n            });\n        });\n    }\n\n    function escapeRegExp(str) {\n        return str.replace(/([.*+?^=!:${}()|\\[\\]\\/\\\\])/g, \"\\\\$1\");\n    }\n\n    function replaceAll(originalString, strReplace, strWith) {\n        var strReplace2 = escapeRegExp(strReplace);\n        var reg = new RegExp(strReplace2, \"ig\");\n        return originalString.replace(reg, strWith);\n    }\n\n    function generateDeviceId() {\n        var keys = [];\n\n        if (keys.push(navigator.userAgent), keys.push(new Date().getTime()), self.btoa) {\n            var result = replaceAll(btoa(keys.join(\"|\")), \"=\", \"1\");\n            return Promise.resolve(result);\n        }\n\n        return Promise.resolve(new Date().getTime());\n    }\n\n    function getDeviceId() {\n        var key = \"_deviceId2\";\n        var deviceId = appSettings.get(key);\n\n        if (deviceId) {\n            return Promise.resolve(deviceId);\n        }\n\n        return generateDeviceId().then(function (deviceId) {\n            appSettings.set(key, deviceId);\n            return deviceId;\n        });\n    }\n\n    function getDeviceName() {\n        var deviceName;\n        deviceName = browser.tizen ? \"Samsung Smart TV\" : browser.web0s ? \"LG Smart TV\" : browser.operaTv ? \"Opera TV\" : browser.xboxOne ? \"Xbox One\" : browser.ps4 ? \"Sony PS4\" : browser.chrome ? \"Chrome\" : browser.edge ? \"Edge\" : browser.firefox ? \"Firefox\" : browser.msie ? \"Internet Explorer\" : browser.opera ? \"Opera\" : \"Web Browser\";\n\n        if (browser.ipad) {\n            deviceName += \" iPad\";\n        } else {\n            if (browser.iphone) {\n                deviceName += \" iPhone\";\n            } else {\n                if (browser.android) {\n                    deviceName += \" Android\";\n                }\n            }\n        }\n\n        return deviceName;\n    }\n\n    function supportsVoiceInput() {\n        if (!browser.tv) {\n            return window.SpeechRecognition || window.webkitSpeechRecognition || window.mozSpeechRecognition || window.oSpeechRecognition || window.msSpeechRecognition;\n        }\n\n        return false;\n    }\n\n    function supportsFullscreen() {\n        if (browser.tv) {\n            return false;\n        }\n\n        var element = document.documentElement;\n        return (element.requestFullscreen || element.mozRequestFullScreen || element.webkitRequestFullscreen || element.msRequestFullscreen) || document.createElement(\"video\").webkitEnterFullscreen;\n    }\n\n    function getSyncProfile() {\n        return new Promise(function (resolve) {\n            require([\"browserdeviceprofile\", \"appSettings\"], function (profileBuilder, appSettings) {\n                var profile;\n\n                if (window.NativeShell) {\n                    profile = window.NativeShell.AppHost.getSyncProfile(profileBuilder, appSettings);\n                } else {\n                    profile = profileBuilder();\n                    profile.MaxStaticMusicBitrate = appSettings.maxStaticMusicBitrate();\n                }\n\n                resolve(profile);\n            });\n        });\n    }\n\n    function getDefaultLayout() {\n        return \"desktop\";\n    }\n\n    function supportsHtmlMediaAutoplay() {\n        if (browser.edgeUwp || browser.tizen || browser.web0s || browser.orsay || browser.operaTv || browser.ps4 || browser.xboxOne) {\n            return true;\n        }\n\n        if (browser.mobile) {\n            return false;\n        }\n\n        var savedResult = appSettings.get(htmlMediaAutoplayAppStorageKey);\n        return \"true\" === savedResult || \"false\" !== savedResult && null;\n    }\n\n    function cueSupported() {\n        try {\n            var video = document.createElement(\"video\");\n            var style = document.createElement(\"style\");\n            style.textContent = \"video::cue {background: inherit}\";\n            document.body.appendChild(style);\n            document.body.appendChild(video);\n            var cue = window.getComputedStyle(video, \"::cue\").background;\n            document.body.removeChild(style);\n            document.body.removeChild(video);\n            return !!cue.length;\n        } catch (err) {\n            console.log(\"Error detecting cue support:\" + err);\n            return false;\n        }\n    }\n\n    function onAppVisible() {\n        if (isHidden) {\n            isHidden = false;\n            console.log(\"triggering app resume event\");\n            events.trigger(appHost, \"resume\");\n        }\n    }\n\n    function onAppHidden() {\n        if (!isHidden) {\n            isHidden = true;\n            console.log(\"app is hidden\");\n        }\n    }\n\n    var htmlMediaAutoplayAppStorageKey = \"supportshtmlmediaautoplay0\";\n\n    var supportedFeatures = function () {\n        var features = [];\n\n        if (navigator.share) {\n            features.push(\"sharing\");\n        }\n\n        if (!browser.edgeUwp && !browser.tv && !browser.xboxOne && !browser.ps4) {\n            features.push(\"filedownload\");\n        }\n\n        if (browser.operaTv || browser.tizen || browser.orsay || browser.web0s) {\n            features.push(\"exit\");\n        } else {\n            features.push(\"exitmenu\");\n            features.push(\"plugins\");\n        }\n\n        if (!browser.operaTv && !browser.tizen && !browser.orsay && !browser.web0s && !browser.ps4) {\n            features.push(\"externallinks\");\n            features.push(\"externalpremium\");\n        }\n\n        if (!browser.operaTv) {\n            features.push(\"externallinkdisplay\");\n        }\n\n        if (supportsVoiceInput()) {\n            features.push(\"voiceinput\");\n        }\n\n        if (!browser.tv && !browser.xboxOne) {\n            browser.ps4;\n        }\n\n        if (supportsHtmlMediaAutoplay()) {\n            features.push(\"htmlaudioautoplay\");\n            features.push(\"htmlvideoautoplay\");\n        }\n\n        if (browser.edgeUwp) {\n            features.push(\"sync\");\n        }\n\n        if (supportsFullscreen()) {\n            features.push(\"fullscreenchange\");\n        }\n\n        if (browser.chrome || browser.edge && !browser.slow) {\n            if (!browser.noAnimation && !browser.edgeUwp && !browser.xboxOne) {\n                features.push(\"imageanalysis\");\n            }\n        }\n\n        if (browser.tv || browser.xboxOne || browser.ps4 || browser.mobile) {\n            features.push(\"physicalvolumecontrol\");\n        }\n\n        if (!browser.tv && !browser.xboxOne && !browser.ps4) {\n            features.push(\"remotecontrol\");\n        }\n\n        if (!browser.operaTv && !browser.tizen && !browser.orsay && !browser.web0s && !browser.edgeUwp) {\n            features.push(\"remotevideo\");\n        }\n\n        features.push(\"displaylanguage\");\n        features.push(\"otherapppromotions\");\n        features.push(\"displaymode\");\n        features.push(\"targetblank\");\n        // allows users to connect to more than one server\n        //features.push(\"multiserver\");\n        features.push(\"screensaver\");\n\n        if (!browser.orsay && !browser.tizen && !browser.msie && (browser.firefox || browser.ps4 || browser.edge || cueSupported())) {\n            features.push(\"subtitleappearancesettings\");\n        }\n\n        if (!browser.orsay && !browser.tizen) {\n            features.push(\"subtitleburnsettings\");\n        }\n\n        if (!browser.tv && !browser.ps4 && !browser.xboxOne) {\n            features.push(\"fileinput\");\n        }\n\n        if (browser.chrome) {\n            features.push(\"chromecast\");\n        }\n\n        return features;\n    }();\n\n    if (supportedFeatures.indexOf(\"htmlvideoautoplay\") === -1 && supportsHtmlMediaAutoplay() !== false) {\n        require([\"autoPlayDetect\"], function (autoPlayDetect) {\n            autoPlayDetect.supportsHtmlMediaAutoplay().then(function () {\n                appSettings.set(htmlMediaAutoplayAppStorageKey, \"true\");\n                supportedFeatures.push(\"htmlvideoautoplay\");\n                supportedFeatures.push(\"htmlaudioautoplay\");\n            }, function () {\n                appSettings.set(htmlMediaAutoplayAppStorageKey, \"false\");\n            });\n        });\n    }\n\n    var deviceId;\n    var deviceName;\n    var appName = \"Jellyfin Web\";\n    var appVersion = \"10.5.0\";\n    var visibilityChange;\n    var visibilityState;\n\n    var appHost = {\n        getWindowState: function () {\n            return document.windowState || \"Normal\";\n        },\n        setWindowState: function (state) {\n            alert(\"setWindowState is not supported and should not be called\");\n        },\n        exit: function () {\n            if (window.NativeShell) {\n                window.NativeShell.AppHost.exit();\n            } else if (browser.tizen) {\n                try {\n                    tizen.application.getCurrentApplication().exit();\n                } catch (err) {\n                    console.log(\"error closing application: \" + err);\n                }\n            } else {\n                window.close();\n            }\n        },\n        supports: function (command) {\n            if (window.NativeShell) {\n                return window.NativeShell.AppHost.supports(command);\n            }\n\n            return -1 !== supportedFeatures.indexOf(command.toLowerCase());\n        },\n        preferVisualCards: browser.android || browser.chrome,\n        moreIcon: browser.android ? \"dots-vert\" : \"dots-horiz\",\n        getSyncProfile: getSyncProfile,\n        getDefaultLayout: function () {\n            if (window.NativeShell) {\n                return window.NativeShell.AppHost.getDefaultLayout();\n            }\n\n            return getDefaultLayout()\n        },\n        getDeviceProfile: getDeviceProfile,\n        init: function () {\n            if (window.NativeShell) {\n                return window.NativeShell.AppHost.init();\n            }\n\n            deviceName = getDeviceName();\n            getDeviceId().then(function (id) {\n                deviceId = id;\n            });\n        },\n        deviceName: function () {\n            return window.NativeShell ? window.NativeShell.AppHost.deviceName() : deviceName;\n        },\n        deviceId: function () {\n            return window.NativeShell ? window.NativeShell.AppHost.deviceId() : deviceId;\n        },\n        appName: function () {\n            return window.NativeShell ? window.NativeShell.AppHost.appName() : appName;\n        },\n        appVersion: function () {\n            return window.NativeShell ? window.NativeShell.AppHost.appVersion() : appVersion;\n        },\n        getPushTokenInfo: function () {\n            return {};\n        },\n        setThemeColor: function (color) {\n            var metaThemeColor = document.querySelector(\"meta[name=theme-color]\");\n\n            if (metaThemeColor) {\n                metaThemeColor.setAttribute(\"content\", color);\n            }\n        },\n        setUserScalable: function (scalable) {\n            if (!browser.tv) {\n                var att = scalable ? \"width=device-width, initial-scale=1, minimum-scale=1, user-scalable=yes\" : \"width=device-width, initial-scale=1, minimum-scale=1, maximum-scale=1, user-scalable=no\";\n                document.querySelector(\"meta[name=viewport]\").setAttribute(\"content\", att);\n            }\n        }\n    };\n    var doc = self.document;\n\n    if (doc) {\n        if (void 0 !== doc.visibilityState) {\n            visibilityChange = \"visibilitychange\";\n            visibilityState = \"hidden\";\n        } else {\n            if (void 0 !== doc.mozHidden) {\n                visibilityChange = \"mozvisibilitychange\";\n                visibilityState = \"mozVisibilityState\";\n            } else {\n                if (void 0 !== doc.msHidden) {\n                    visibilityChange = \"msvisibilitychange\";\n                    visibilityState = \"msVisibilityState\";\n                } else {\n                    if (void 0 !== doc.webkitHidden) {\n                        visibilityChange = \"webkitvisibilitychange\";\n                        visibilityState = \"webkitVisibilityState\";\n                    }\n                }\n            }\n        }\n    }\n\n    var isHidden = false;\n\n    if (doc) {\n        doc.addEventListener(visibilityChange, function () {\n            if (document[visibilityState]) {\n                onAppHidden();\n            } else {\n                onAppVisible();\n            }\n        });\n    }\n\n    if (self.addEventListener) {\n        self.addEventListener(\"focus\", onAppVisible);\n        self.addEventListener(\"blur\", onAppHidden);\n    }\n\n    return appHost;\n});\n", "idx": 1, "id": 12858, "msg": "This function will no longer return any value by default. That could cause issues for anything that calls this expecting a return value.", "proj": "jellyfin-jellyfin-web", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -3575,32 +3575,32 @@ drx_avx512_gather_sequence_state_machine(void *drcontext,\n                     opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                     if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                         params->restore_dest_mask_start_pc = params->pc;\n-                        advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5,\n+                        advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4,\n                                       params);\n                         break;\n                     }\n                 }\n             }\n         }\n-        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n+        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1, params);\n         break;\n-    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5:\n+    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4:\n         if (instr_get_opcode(&params->inst) == OP_vextracti32x4) {\n             opnd_t dst0 = instr_get_dst(&params->inst, 0);\n             if (opnd_is_reg(dst0)) {\n                 reg_id_t tmp_reg = opnd_get_reg(dst0);\n                 if (!reg_is_strictly_xmm(tmp_reg))\n                     break;\n-                ASSERT(params->spilled_xmm == tmp_reg,\n+                ASSERT(reg_resize_to_opsz(params->spilled_mm, OPSZ_16) == tmp_reg,\n                        \"Only the spilled xmm should be used as scratch\");\n                 params->the_scratch_xmm = tmp_reg;\n-                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6, params);\n+                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5, params);\n                 break;\n             }\n         }\n-        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n+        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1, params);\n         break;\n-    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6:\n+    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5:\n         ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n                \"internal error: expected xmm register to be recorded in state \"\n                \"machine.\");", "y": 0, "oldf": "/* **********************************************************\n * Copyright (c) 2013-2021 Google, Inc.   All rights reserved.\n * **********************************************************/\n\n/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and/or other materials provided with the distribution.\n *\n * * Neither the name of Google, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL GOOGLE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/* DynamoRio eXtension utilities */\n\n#include \"dr_api.h\"\n#include \"drx.h\"\n#include \"hashtable.h\"\n#include \"../ext_utils.h\"\n#include <string.h>\n\n/* We use drmgr but only internally.  A user of drx will end up loading in\n * the drmgr library, but it won't affect the user's code.\n */\n#include \"drmgr.h\"\n\n#include \"drreg.h\"\n\n#ifdef UNIX\n#    ifdef LINUX\n#        include \"../../core/unix/include/syscall.h\"\n#    else\n#        include <sys/syscall.h>\n#    endif\n#    include <signal.h> /* SIGKILL */\n#endif\n\n#include <limits.h>\n\n#ifdef DEBUG\n#    define ASSERT(x, msg) DR_ASSERT_MSG(x, msg)\n#    define IF_DEBUG(x) x\n#else\n#    define ASSERT(x, msg) /* nothing */\n#    define IF_DEBUG(x)    /* nothing */\n#endif                     /* DEBUG */\n\n#define XMM_REG_SIZE 16\n#define YMM_REG_SIZE 32\n#define XMM_ALIGNMENT 32\n\n#define MAX(x, y) ((x) >= (y) ? (x) : (y))\n\n#ifdef WINDOWS\n#    define IF_WINDOWS_ELSE(x, y) (x)\n#else\n#    define IF_WINDOWS_ELSE(x, y) (y)\n#endif\n\n#ifdef X86\n/* TODO i#2985: add ARM SIMD. */\n#    define PLATFORM_SUPPORTS_SCATTER_GATHER\n#endif\n\n#define MINSERT instrlist_meta_preinsert\n/* For inserting an app instruction, which must have a translation (\"xl8\") field. */\n#define PREXL8 instrlist_preinsert\n\n#define VERBOSE 0\n\n/* Reserved note range values */\nenum {\n    DRX_NOTE_AFLAGS_RESTORE_BEGIN,\n    DRX_NOTE_AFLAGS_RESTORE_SAHF,\n    DRX_NOTE_AFLAGS_RESTORE_END,\n    DRX_NOTE_COUNT,\n};\n\nstatic ptr_uint_t note_base;\n#define NOTE_VAL(enum_val) ((void *)(ptr_int_t)(note_base + (enum_val)))\n\nstatic bool soft_kills_enabled;\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\nstatic int tls_idx;\ntypedef struct _per_thread_t {\n    void *scratch_xmm_spill_slot;\n} per_thread_t;\n\nstatic per_thread_t *\nget_tls_data(void *drcontext);\n\nstatic per_thread_t init_pt;\n#endif\nstatic void\nsoft_kills_exit(void);\n\n/* For debugging */\nstatic uint verbose = 0;\n\n#undef NOTIFY\n#define NOTIFY(n, ...)                       \\\n    do {                                     \\\n        if (verbose >= (n)) {                \\\n            dr_fprintf(STDERR, __VA_ARGS__); \\\n        }                                    \\\n    } while (0)\n\n/* defined in drx_buf.c */\nbool\ndrx_buf_init_library(void);\nvoid\ndrx_buf_exit_library(void);\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n\nstatic int drx_scatter_gather_expanded;\n\nstatic bool\ndrx_event_restore_state(void *drcontext, bool restore_memory,\n                        dr_restore_state_info_t *info);\n#endif\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\nstatic per_thread_t *\nget_tls_data(void *drcontext)\n{\n    per_thread_t *pt = (per_thread_t *)drmgr_get_tls_field(drcontext, tls_idx);\n    /* Support use during init (i#2910). */\n    if (pt == NULL)\n        return &init_pt;\n    return pt;\n}\n#endif\n\n/***************************************************************************\n * INIT\n */\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\nstatic void\ndrx_thread_init(void *drcontext)\n{\n    per_thread_t *pt = (per_thread_t *)dr_thread_alloc(drcontext, sizeof(*pt));\n    pt->scratch_xmm_spill_slot =\n        dr_thread_alloc(drcontext, XMM_REG_SIZE + (XMM_ALIGNMENT - 1));\n    drmgr_set_tls_field(drcontext, tls_idx, (void *)pt);\n}\n\nstatic void\ndrx_thread_exit(void *drcontext)\n{\n    per_thread_t *pt = (per_thread_t *)drmgr_get_tls_field(drcontext, tls_idx);\n    dr_thread_free(drcontext, pt->scratch_xmm_spill_slot,\n                   XMM_REG_SIZE + (XMM_ALIGNMENT - 1));\n    dr_thread_free(drcontext, pt, sizeof(*pt));\n}\n#endif\n\nstatic int drx_init_count;\n\nDR_EXPORT\nbool\ndrx_init(void)\n{\n    /* drx_insert_counter_update() needs 1 slot on x86 plus the 1 slot\n     * drreg uses for aflags, and 2 reg slots on aarch, so 2 on both.\n     * drx_expand_scatter_gather() needs 4 slots in app2app phase, which\n     * cannot be reused by other phases. So, ideally we should reserve\n     * 6 slots for drx. But we settle with 4 to avoid stealing too many\n     * slots from other clients/libs. When drx needs more for instrumenting\n     * scatter/gather instrs, we fall back on DR slots. As scatter/gather\n     * instrs are spilt into their own bbs, this effect will be limited.\n     * On Windows however we reserve even fewer slots, as they are\n     * shared with the application and reserving even one slot can result\n     * in failure to initialize for certain applications (e.g. i#1163).\n     * On Linux, we set do_not_sum_slots to false so that we get at least\n     * as many slots for drx use.\n     */\n    drreg_options_t ops = { sizeof(ops), IF_WINDOWS_ELSE(2, 4), false, NULL,\n                            IF_WINDOWS_ELSE(true, false) };\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n    drmgr_priority_t fault_priority = { sizeof(fault_priority),\n                                        DRMGR_PRIORITY_NAME_DRX_FAULT, NULL, NULL,\n                                        DRMGR_PRIORITY_FAULT_DRX };\n#endif\n\n    int count = dr_atomic_add32_return_sum(&drx_init_count, 1);\n    if (count > 1)\n        return true;\n\n    drmgr_init();\n    note_base = drmgr_reserve_note_range(DRX_NOTE_COUNT);\n    ASSERT(note_base != DRMGR_NOTE_NONE, \"failed to reserve note range\");\n\n    if (drreg_init(&ops) != DRREG_SUCCESS)\n        return false;\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n    if (!drmgr_register_restore_state_ex_event_ex(drx_event_restore_state,\n                                                  &fault_priority))\n        return false;\n    if (!drmgr_register_thread_init_event(drx_thread_init) ||\n        !drmgr_register_thread_exit_event(drx_thread_exit))\n        return false;\n    tls_idx = drmgr_register_tls_field();\n    if (tls_idx == -1)\n        return false;\n#endif\n\n    return drx_buf_init_library();\n}\n\nDR_EXPORT\nvoid\ndrx_exit()\n{\n    int count = dr_atomic_add32_return_sum(&drx_init_count, -1);\n    if (count != 0)\n        return;\n\n    if (soft_kills_enabled) {\n        soft_kills_exit();\n        soft_kills_enabled = false;\n    }\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n    drmgr_unregister_tls_field(tls_idx);\n#endif\n    drx_buf_exit_library();\n    drreg_exit();\n    drmgr_exit();\n}\n\n/***************************************************************************\n * INSTRUCTION NOTE FIELD\n */\n\n/* For historical reasons we have this routine exported by drx.\n * We just forward to drmgr.\n */\nDR_EXPORT\nptr_uint_t\ndrx_reserve_note_range(size_t size)\n{\n    return drmgr_reserve_note_range(size);\n}\n\n/***************************************************************************\n * ANALYSIS\n */\n\nDR_EXPORT\nbool\ndrx_aflags_are_dead(instr_t *where)\n{\n    bool dead = false;\n    IF_DEBUG(drreg_status_t res =)\n    drreg_are_aflags_dead(dr_get_current_drcontext(), where, &dead);\n    ASSERT(res == DRREG_SUCCESS, \"drreg_are_aflags_dead failed!\");\n    return dead;\n}\n\n/***************************************************************************\n * INSTRUMENTATION\n */\n\n#ifdef AARCHXX\n/* XXX i#1603: add liveness analysis and pick dead regs */\n#    define SCRATCH_REG0 DR_REG_R0\n#    define SCRATCH_REG1 DR_REG_R1\n#endif\n\n/* insert a label instruction with note */\nstatic void\nilist_insert_note_label(void *drcontext, instrlist_t *ilist, instr_t *where, void *note)\n{\n    instr_t *instr = INSTR_CREATE_label(drcontext);\n    instr_set_note(instr, note);\n    MINSERT(ilist, where, instr);\n}\n\n#ifdef X86 /* not yet used on ARM but we may export */\n\n/* Insert arithmetic flags saving code with more control.\n * For x86:\n * - skip %eax save if !save_reg\n * - save %eax to reg if reg is not DR_REG_NULL,\n * - save %eax to slot otherwise\n * For ARM:\n * - saves flags to reg\n * - saves reg first to slot, unless !save_reg.\n */\nstatic void\ndrx_save_arith_flags(void *drcontext, instrlist_t *ilist, instr_t *where, bool save_reg,\n                     bool save_oflag, dr_spill_slot_t slot, reg_id_t reg)\n{\n#    ifdef X86\n    instr_t *instr;\n    /* save %eax if necessary */\n    if (save_reg) {\n        if (reg != DR_REG_NULL) {\n            ASSERT(reg >= DR_REG_START_GPR && reg <= DR_REG_STOP_GPR && reg != DR_REG_XAX,\n                   \"wrong dead reg\");\n            MINSERT(ilist, where,\n                    INSTR_CREATE_mov_st(drcontext, opnd_create_reg(reg),\n                                        opnd_create_reg(DR_REG_XAX)));\n        } else {\n            ASSERT(slot >= SPILL_SLOT_1 && slot <= SPILL_SLOT_MAX, \"wrong spill slot\");\n            dr_save_reg(drcontext, ilist, where, DR_REG_XAX, slot);\n        }\n    }\n    /* lahf */\n    instr = INSTR_CREATE_lahf(drcontext);\n    MINSERT(ilist, where, instr);\n    if (save_oflag) {\n        /* seto %al */\n        instr = INSTR_CREATE_setcc(drcontext, OP_seto, opnd_create_reg(DR_REG_AL));\n        MINSERT(ilist, where, instr);\n    }\n#    elif defined(AARCHXX)\n    ASSERT(reg >= DR_REG_START_GPR && reg <= DR_REG_STOP_GPR, \"reg must be a GPR\");\n    if (save_reg) {\n        ASSERT(slot >= SPILL_SLOT_1 && slot <= SPILL_SLOT_MAX, \"wrong spill slot\");\n        dr_save_reg(drcontext, ilist, where, reg, slot);\n    }\n    MINSERT(ilist, where,\n            INSTR_CREATE_msr(drcontext, opnd_create_reg(DR_REG_CPSR),\n                             OPND_CREATE_INT_MSR_NZCVQG(), opnd_create_reg(reg)));\n#    endif\n}\n\n/* Insert arithmetic flags restore code with more control.\n * For x86:\n * - skip %eax restore if !restore_reg\n * - restore %eax from reg if reg is not DR_REG_NULL\n * - restore %eax from slot otherwise\n * For ARM:\n * - restores flags from reg\n * - restores reg to slot, unless !restore_reg.\n * Routine merge_prev_drx_aflags_switch looks for labels inserted by\n * drx_restore_arith_flags, so changes to this routine may affect\n * merge_prev_drx_aflags_switch.\n */\nstatic void\ndrx_restore_arith_flags(void *drcontext, instrlist_t *ilist, instr_t *where,\n                        bool restore_reg, bool restore_oflag, dr_spill_slot_t slot,\n                        reg_id_t reg)\n{\n    instr_t *instr;\n    ilist_insert_note_label(drcontext, ilist, where,\n                            NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_BEGIN));\n#    ifdef X86\n    if (restore_oflag) {\n        /* add 0x7f, %al */\n        instr = INSTR_CREATE_add(drcontext, opnd_create_reg(DR_REG_AL),\n                                 OPND_CREATE_INT8(0x7f));\n        MINSERT(ilist, where, instr);\n    }\n    /* sahf */\n    instr = INSTR_CREATE_sahf(drcontext);\n    instr_set_note(instr, NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_SAHF));\n    MINSERT(ilist, where, instr);\n    /* restore eax if necessary */\n    if (restore_reg) {\n        if (reg != DR_REG_NULL) {\n            ASSERT(reg >= DR_REG_START_GPR && reg <= DR_REG_STOP_GPR && reg != DR_REG_XAX,\n                   \"wrong dead reg\");\n            MINSERT(ilist, where,\n                    INSTR_CREATE_mov_st(drcontext, opnd_create_reg(DR_REG_XAX),\n                                        opnd_create_reg(reg)));\n        } else {\n            ASSERT(slot >= SPILL_SLOT_1 && slot <= SPILL_SLOT_MAX, \"wrong spill slot\");\n            dr_restore_reg(drcontext, ilist, where, DR_REG_XAX, slot);\n        }\n    }\n#    elif defined(AARCHXX)\n    ASSERT(reg >= DR_REG_START_GPR && reg <= DR_REG_STOP_GPR, \"reg must be a GPR\");\n    instr =\n        INSTR_CREATE_mrs(drcontext, opnd_create_reg(reg), opnd_create_reg(DR_REG_CPSR));\n    instr_set_note(instr, NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_SAHF));\n    MINSERT(ilist, where, instr);\n    if (restore_reg) {\n        ASSERT(slot >= SPILL_SLOT_1 && slot <= SPILL_SLOT_MAX, \"wrong spill slot\");\n        dr_restore_reg(drcontext, ilist, where, reg, slot);\n    }\n#    endif\n    ilist_insert_note_label(drcontext, ilist, where,\n                            NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_END));\n}\n#endif /* X86 */\n\n/* Check if current instrumentation can be merged into previous aflags\n * (or on ARM, GPR) save/restore inserted by drx_restore_arith_flags.\n * Returns NULL if cannot merge. Otherwise, returns the right insertion point,\n * i.e., DRX_NOTE_AFLAGS_RESTORE_BEGIN label instr.\n *\n * This routine looks for labels inserted by drx_restore_arith_flags,\n * so changes to drx_restore_arith_flags may affect this routine.\n * On ARM the labels are from drx_insert_counter_update.\n */\nstatic instr_t *\nmerge_prev_drx_spill(instrlist_t *ilist, instr_t *where, bool aflags)\n{\n    instr_t *instr;\n#ifdef DEBUG\n    bool has_sahf = false;\n#endif\n\n    if (where == NULL)\n        return NULL;\n    instr = instr_get_prev(where);\n    if (instr == NULL)\n        return NULL;\n    if (!instr_is_label(instr))\n        return NULL;\n    /* Check if prev instr is DRX_NOTE_AFLAGS_RESTORE_END.\n     * We bail even there is only a label instr in between, which\n     * might be a target of internal cti.\n     */\n    if (instr_get_note(instr) != NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_END))\n        return NULL;\n    /* On ARM we do not want to merge two drx spills if they are\n     * predicated differently.\n     */\n    if (instr_get_predicate(instr) != instrlist_get_auto_predicate(ilist))\n        return NULL;\n\n    /* find DRX_NOTE_AFLAGS_RESTORE_BEGIN */\n    for (instr = instr_get_prev(instr); instr != NULL; instr = instr_get_prev(instr)) {\n        if (instr_is_app(instr)) {\n            /* we do not expect any app instr */\n            ASSERT(false, \"drx aflags restore is corrupted\");\n            return NULL;\n        }\n        if (instr_is_label(instr)) {\n            if (instr_get_note(instr) == NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_BEGIN)) {\n                ASSERT(!aflags || has_sahf, \"missing sahf\");\n                return instr;\n            }\n            /* we do not expect any other label instr */\n            ASSERT(false, \"drx aflags restore is corrupted\");\n            return NULL;\n#ifdef DEBUG\n        } else {\n            if (instr_get_note(instr) == NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_SAHF))\n                has_sahf = true;\n#endif\n        }\n    }\n    return NULL;\n}\n\nstatic bool\ncounter_crosses_cache_line(byte *addr, size_t size)\n{\n    size_t cache_line_size = proc_get_cache_line_size();\n    if (ALIGN_BACKWARD(addr, cache_line_size) ==\n        ALIGN_BACKWARD(addr + size - 1, cache_line_size))\n        return false;\n    return true;\n}\n\nDR_EXPORT\nbool\ndrx_insert_counter_update(void *drcontext, instrlist_t *ilist, instr_t *where,\n                          dr_spill_slot_t slot,\n                          IF_NOT_X86_(dr_spill_slot_t slot2) void *addr, int value,\n                          uint flags)\n{\n    instr_t *instr;\n    bool use_drreg = false;\n#ifdef X86\n    bool save_aflags = true;\n#elif defined(AARCHXX)\n    bool save_regs = true;\n    reg_id_t reg1, reg2;\n#endif\n    bool is_64 = TEST(DRX_COUNTER_64BIT, flags);\n    /* Requires drx_init(), where it didn't when first added. */\n    if (drx_init_count == 0) {\n        ASSERT(false, \"drx_insert_counter_update requires drx_init\");\n        return false;\n    }\n    if (drcontext == NULL) {\n        ASSERT(false, \"drcontext cannot be NULL\");\n        return false;\n    }\n    if (drmgr_current_bb_phase(drcontext) == DRMGR_PHASE_INSERTION) {\n        use_drreg = true;\n        if (drmgr_current_bb_phase(drcontext) == DRMGR_PHASE_INSERTION &&\n            (slot != SPILL_SLOT_MAX + 1 IF_NOT_X86(|| slot2 != SPILL_SLOT_MAX + 1))) {\n            ASSERT(false, \"with drmgr, SPILL_SLOT_MAX+1 must be passed\");\n            return false;\n        }\n    } else if (!(slot >= SPILL_SLOT_1 && slot <= SPILL_SLOT_MAX)) {\n        ASSERT(false, \"wrong spill slot\");\n        return false;\n    }\n\n    /* check whether we can add lock */\n    if (TEST(DRX_COUNTER_LOCK, flags)) {\n#ifdef ARM\n        /* FIXME i#1551: implement for ARM */\n        ASSERT(false, \"DRX_COUNTER_LOCK not implemented for ARM\");\n        return false;\n#endif\n        if (IF_NOT_X64(is_64 ||) /* 64-bit counter in 32-bit mode */\n            counter_crosses_cache_line((byte *)addr, is_64 ? 8 : 4))\n            return false;\n    }\n\n#ifdef X86\n    if (use_drreg) {\n        if (drreg_reserve_aflags(drcontext, ilist, where) != DRREG_SUCCESS)\n            return false;\n    } else {\n        /* if save_aflags, check if we can merge with the prev aflags save */\n        save_aflags = !drx_aflags_are_dead(where);\n        if (save_aflags) {\n            instr = merge_prev_drx_spill(ilist, where, true /*aflags*/);\n            if (instr != NULL) {\n                save_aflags = false;\n                where = instr;\n            }\n        }\n        /* save aflags if necessary */\n        if (save_aflags) {\n            drx_save_arith_flags(drcontext, ilist, where, true /* save eax */,\n                                 true /* save oflag */, slot, DR_REG_NULL);\n        }\n    }\n    /* update counter */\n    instr = INSTR_CREATE_add(\n        drcontext,\n        OPND_CREATE_ABSMEM(addr, IF_X64_ELSE((is_64 ? OPSZ_8 : OPSZ_4), OPSZ_4)),\n        OPND_CREATE_INT_32OR8(value));\n    if (TEST(DRX_COUNTER_LOCK, flags))\n        instr = LOCK(instr);\n    MINSERT(ilist, where, instr);\n\n#    ifndef X64\n    if (is_64) {\n        MINSERT(ilist, where,\n                INSTR_CREATE_adc(\n                    drcontext, OPND_CREATE_ABSMEM((void *)((ptr_int_t)addr + 4), OPSZ_4),\n                    OPND_CREATE_INT32(0)));\n    }\n#    endif /* !X64 */\n    if (use_drreg) {\n        if (drreg_unreserve_aflags(drcontext, ilist, where) != DRREG_SUCCESS)\n            return false;\n    } else {\n        /* restore aflags if necessary */\n        if (save_aflags) {\n            drx_restore_arith_flags(drcontext, ilist, where, true /* restore eax */,\n                                    true /* restore oflag */, slot, DR_REG_NULL);\n        }\n    }\n#elif defined(AARCHXX)\n#    ifdef ARM\n    /* FIXME i#1551: implement 64-bit counter support */\n    ASSERT(!is_64, \"DRX_COUNTER_64BIT is not implemented for ARM_32\");\n#    endif /* ARM */\n\n    if (use_drreg) {\n        if (drreg_reserve_register(drcontext, ilist, where, NULL, &reg1) !=\n                DRREG_SUCCESS ||\n            drreg_reserve_register(drcontext, ilist, where, NULL, &reg2) != DRREG_SUCCESS)\n            return false;\n    } else {\n        reg1 = SCRATCH_REG0;\n        reg2 = SCRATCH_REG1;\n        /* merge w/ prior restore */\n        if (save_regs) {\n            instr = merge_prev_drx_spill(ilist, where, false /*!aflags*/);\n            if (instr != NULL) {\n                save_regs = false;\n                where = instr;\n            }\n        }\n        if (save_regs) {\n            dr_save_reg(drcontext, ilist, where, reg1, slot);\n            dr_save_reg(drcontext, ilist, where, reg2, slot2);\n        }\n    }\n    /* XXX: another optimization is to look for the prior increment's\n     * address being near this one, and add to reg1 instead of\n     * taking 2 instrs to load it fresh.\n     */\n    /* Update the counter either with release-acquire semantics (when the\n     * DRX_COUNTER_REL_ACQ flag is on) or without any barriers.\n     */\n    instrlist_insert_mov_immed_ptrsz(drcontext, (ptr_int_t)addr, opnd_create_reg(reg1),\n                                     ilist, where, NULL, NULL);\n    if (TEST(DRX_COUNTER_REL_ACQ, flags)) {\n#    ifdef AARCH64\n        MINSERT(ilist, where,\n                INSTR_CREATE_ldar(drcontext, opnd_create_reg(reg2),\n                                  OPND_CREATE_MEMPTR(reg1, 0)));\n        if (value >= 0) {\n            MINSERT(ilist, where,\n                    XINST_CREATE_add(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(value)));\n        } else {\n            MINSERT(ilist, where,\n                    XINST_CREATE_sub(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(-value)));\n        }\n        MINSERT(ilist, where,\n                INST_CREATE_stlr(drcontext, OPND_CREATE_MEMPTR(reg1, 0),\n                                 opnd_create_reg(reg2)));\n#    else /* ARM */\n        /* TODO: This counter update has not been tested on a ARM_32 machine. */\n        MINSERT(ilist, where,\n                XINST_CREATE_load(drcontext, opnd_create_reg(reg2),\n                                  OPND_CREATE_MEMPTR(reg1, 0)));\n        MINSERT(ilist, where, INSTR_CREATE_dmb(drcontext, OPND_CREATE_INT(DR_DMB_ISH)));\n        if (value >= 0) {\n            MINSERT(ilist, where,\n                    XINST_CREATE_add(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(value)));\n        } else {\n            MINSERT(ilist, where,\n                    XINST_CREATE_add(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(-value)));\n        }\n        MINSERT(ilist, where, INSTR_CREATE_dmb(drcontext, OPND_CREATE_INT(DR_DMB_ISH)));\n        MINSERT(ilist, where,\n                XINST_CREATE_store(drcontext, OPND_CREATE_MEMPTR(reg1, 0),\n                                   opnd_create_reg(reg2)));\n#    endif\n    } else {\n        MINSERT(ilist, where,\n                XINST_CREATE_load(drcontext, opnd_create_reg(reg2),\n                                  OPND_CREATE_MEMPTR(reg1, 0)));\n        if (value >= 0) {\n            MINSERT(ilist, where,\n                    XINST_CREATE_add(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(value)));\n        } else {\n            MINSERT(ilist, where,\n                    XINST_CREATE_sub(drcontext, opnd_create_reg(reg2),\n                                     OPND_CREATE_INT(-value)));\n        }\n        MINSERT(ilist, where,\n                XINST_CREATE_store(drcontext, OPND_CREATE_MEMPTR(reg1, 0),\n                                   opnd_create_reg(reg2)));\n    }\n    if (use_drreg) {\n        if (drreg_unreserve_register(drcontext, ilist, where, reg1) != DRREG_SUCCESS ||\n            drreg_unreserve_register(drcontext, ilist, where, reg2) != DRREG_SUCCESS)\n            return false;\n    } else if (save_regs) {\n        ilist_insert_note_label(drcontext, ilist, where,\n                                NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_BEGIN));\n        dr_restore_reg(drcontext, ilist, where, reg2, slot2);\n        dr_restore_reg(drcontext, ilist, where, reg1, slot);\n        ilist_insert_note_label(drcontext, ilist, where,\n                                NOTE_VAL(DRX_NOTE_AFLAGS_RESTORE_END));\n    }\n#endif\n    return true;\n}\n\n/***************************************************************************\n * SOFT KILLS\n */\n\n/* Track callbacks in a simple list protected by a lock */\ntypedef struct _cb_entry_t {\n    /* XXX: the bool return value is complex to support in some situations.  We\n     * ignore the return value and always skip the app's termination of the\n     * child process for jobs containing multiple pids and for\n     * JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE.  If we wanted to not skip those we'd\n     * have to emulate the kill via NtTerminateProcess, which doesn't seem worth\n     * it when our two use cases (DrMem and drcov) don't need that kind of\n     * control.\n     */\n    bool (*cb)(process_id_t, int);\n    struct _cb_entry_t *next;\n} cb_entry_t;\n\nstatic cb_entry_t *cb_list;\nstatic void *cb_lock;\n\nstatic bool\nsoft_kills_invoke_cbs(process_id_t pid, int exit_code)\n{\n    cb_entry_t *e;\n    bool skip = false;\n    NOTIFY(1, \"--drx-- parent %d soft killing pid %d code %d\\n\", dr_get_process_id(), pid,\n           exit_code);\n    dr_mutex_lock(cb_lock);\n    for (e = cb_list; e != NULL; e = e->next) {\n        /* If anyone wants to skip, we skip */\n        skip = e->cb(pid, exit_code) || skip;\n    }\n    dr_mutex_unlock(cb_lock);\n    return skip;\n}\n\n#ifdef WINDOWS\n\n/* The system calls we need to watch for soft kills.\n * These are are in ntoskrnl so we get away without drsyscall.\n */\nenum {\n    SYS_NUM_PARAMS_TerminateProcess = 2,\n    SYS_NUM_PARAMS_TerminateJobObject = 2,\n    SYS_NUM_PARAMS_SetInformationJobObject = 4,\n    SYS_NUM_PARAMS_Close = 1,\n    SYS_NUM_PARAMS_DuplicateObject = 7,\n};\n\nenum {\n    SYS_WOW64_IDX_TerminateProcess = 0,\n    SYS_WOW64_IDX_TerminateJobObject = 0,\n    SYS_WOW64_IDX_SetInformationJobObject = 7,\n    SYS_WOW64_IDX_Close = 0,\n    SYS_WOW64_IDX_DuplicateObject = 0,\n};\n\nstatic int sysnum_TerminateProcess;\nstatic int sysnum_TerminateJobObject;\nstatic int sysnum_SetInformationJobObject;\nstatic int sysnum_Close;\nstatic int sysnum_DuplicateObject;\n\n/* Table of job handles for which the app set JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE */\n#    define JOB_TABLE_HASH_BITS 6\nstatic hashtable_t job_table;\n\n/* Entry in job_table.  If it is present in the table, it should only be\n * accessed while holding the table lock.\n */\ntypedef struct _job_info_t {\n    /* So far just a reference count.  We don't need store a duplicated handle\n     * b/c we always have a valid app handle for this job.\n     */\n    uint ref_count;\n} job_info_t;\n\n/* We need CLS as we track data across syscalls, where TLS is not sufficient */\nstatic int cls_idx_soft;\n\ntypedef struct _cls_soft_t {\n    /* For NtSetInformationJobObject */\n    DWORD job_limit_flags_orig;\n    DWORD *job_limit_flags_loc;\n    /* For NtDuplicateObject */\n    bool dup_proc_src_us;\n    bool dup_proc_dst_us;\n    ULONG dup_options;\n    HANDLE dup_src;\n    HANDLE *dup_dst;\n    job_info_t *dup_jinfo;\n    /* If we add data for more syscalls, we could use a union to save space */\n} cls_soft_t;\n\n/* XXX: should we have some kind of shared wininc/ dir for these common defines?\n * I don't really want to include core/win32/ntdll.h here.\n */\n\ntypedef LONG NTSTATUS;\n#    define NT_SUCCESS(status) (((NTSTATUS)(status)) >= 0)\n\n/* Since we invoke only in a client/privlib context, we can statically link\n * with ntdll to call these syscall wrappers:\n */\n#    define GET_NTDLL(NtFunction, signature) NTSYSAPI NTSTATUS NTAPI NtFunction signature\n\nGET_NTDLL(NtQueryInformationJobObject,\n          (IN HANDLE JobHandle, IN JOBOBJECTINFOCLASS JobInformationClass,\n           OUT PVOID JobInformation, IN ULONG JobInformationLength,\n           OUT PULONG ReturnLength OPTIONAL));\n\n#    define STATUS_BUFFER_OVERFLOW ((NTSTATUS)0x80000005L)\n\n#    define NT_CURRENT_PROCESS ((HANDLE)(ptr_int_t)-1)\n\ntypedef LONG KPRIORITY;\n\ntypedef enum _PROCESSINFOCLASS {\n    ProcessBasicInformation,\n} PROCESSINFOCLASS;\n\ntypedef struct _PROCESS_BASIC_INFORMATION {\n    NTSTATUS ExitStatus;\n    void *PebBaseAddress;\n    ULONG_PTR AffinityMask;\n    KPRIORITY BasePriority;\n    ULONG_PTR UniqueProcessId;\n    ULONG_PTR InheritedFromUniqueProcessId;\n} PROCESS_BASIC_INFORMATION;\ntypedef PROCESS_BASIC_INFORMATION *PPROCESS_BASIC_INFORMATION;\n\nGET_NTDLL(NtQueryInformationProcess,\n          (IN HANDLE ProcessHandle, IN PROCESSINFOCLASS ProcessInformationClass,\n           OUT PVOID ProcessInformation, IN ULONG ProcessInformationLength,\n           OUT PULONG ReturnLength OPTIONAL));\nGET_NTDLL(NtTerminateProcess, (IN HANDLE ProcessHandle, IN NTSTATUS ExitStatus));\n\nstatic ssize_t\nnum_job_object_pids(HANDLE job)\n{\n    /* i#1401: despite what Nebbett says and MSDN hints at, on Win7 at least\n     * JobObjectBasicProcessIdList returning STATUS_BUFFER_OVERFLOW does NOT\n     * fill in any data at all.  We thus have to query through a different\n     * mechanism.\n     */\n    JOBOBJECT_BASIC_ACCOUNTING_INFORMATION info;\n    NTSTATUS res;\n    DWORD len;\n    res = NtQueryInformationJobObject(job, JobObjectBasicAccountingInformation, &info,\n                                      sizeof(info), &len);\n    NOTIFY(1, \"--drx-- job 0x%x => %d pids len=%d res=0x%08x\\n\", job,\n           info.ActiveProcesses, len, res);\n    if (NT_SUCCESS(res))\n        return info.ActiveProcesses;\n    else\n        return -1;\n}\n\nstatic bool\nget_job_object_pids(HANDLE job, JOBOBJECT_BASIC_PROCESS_ID_LIST *list, size_t list_sz)\n{\n    NTSTATUS res;\n    res = NtQueryInformationJobObject(job, JobObjectBasicProcessIdList, list,\n                                      (ULONG)list_sz, NULL);\n    return NT_SUCCESS(res);\n}\n\n/* XXX: should DR provide a routine to query this? */\nstatic bool\nget_app_exit_code(int *exit_code)\n{\n    ULONG got;\n    PROCESS_BASIC_INFORMATION info;\n    NTSTATUS res;\n    memset(&info, 0, sizeof(PROCESS_BASIC_INFORMATION));\n    res = NtQueryInformationProcess(NT_CURRENT_PROCESS, ProcessBasicInformation, &info,\n                                    sizeof(PROCESS_BASIC_INFORMATION), &got);\n    if (!NT_SUCCESS(res) || got != sizeof(PROCESS_BASIC_INFORMATION))\n        return false;\n    *exit_code = (int)info.ExitStatus;\n    return true;\n}\n\nstatic void\nsoft_kills_context_init(void *drcontext, bool new_depth)\n{\n    cls_soft_t *cls;\n    if (new_depth) {\n        cls = (cls_soft_t *)dr_thread_alloc(drcontext, sizeof(*cls));\n        drmgr_set_cls_field(drcontext, cls_idx_soft, cls);\n    } else {\n        cls = (cls_soft_t *)drmgr_get_cls_field(drcontext, cls_idx_soft);\n    }\n    memset(cls, 0, sizeof(*cls));\n}\n\nstatic void\nsoft_kills_context_exit(void *drcontext, bool thread_exit)\n{\n    if (thread_exit) {\n        cls_soft_t *cls = (cls_soft_t *)drmgr_get_cls_field(drcontext, cls_idx_soft);\n        dr_thread_free(drcontext, cls, sizeof(*cls));\n    }\n    /* else, nothing to do: we leave the struct for re-use on next callback */\n}\n\nstatic int\nsoft_kills_get_sysnum(const char *name, int num_params, int wow64_idx)\n{\n    static module_handle_t ntdll;\n    app_pc wrapper;\n    int sysnum;\n\n    if (ntdll == NULL) {\n        module_data_t *data = dr_lookup_module_by_name(\"ntdll.dll\");\n        if (data == NULL)\n            return -1;\n        ntdll = data->handle;\n        dr_free_module_data(data);\n    }\n    wrapper = (app_pc)dr_get_proc_address(ntdll, name);\n    if (wrapper == NULL)\n        return -1;\n    sysnum = drmgr_decode_sysnum_from_wrapper(wrapper);\n    if (sysnum == -1)\n        return -1;\n    /* Ensure that DR intercepts these if we go native.\n     * XXX: better to only do this if client plans to use native execution\n     * to reduce the hook count and shrink chance of hook conflicts?\n     */\n    if (!dr_syscall_intercept_natively(name, sysnum, num_params, wow64_idx))\n        return -1;\n    return sysnum;\n}\n\nstatic void\nsoft_kills_handle_job_termination(void *drcontext, HANDLE job, int exit_code)\n{\n    ssize_t num_jobs = num_job_object_pids(job);\n    NOTIFY(1, \"--drx-- for job 0x%x got %d jobs\\n\", job, num_jobs);\n    if (num_jobs > 0) {\n        JOBOBJECT_BASIC_PROCESS_ID_LIST *list;\n        size_t sz = sizeof(*list) + (num_jobs - 1) * sizeof(list->ProcessIdList[0]);\n        byte *buf = dr_thread_alloc(drcontext, sz);\n        list = (JOBOBJECT_BASIC_PROCESS_ID_LIST *)buf;\n        if (get_job_object_pids(job, list, sz)) {\n            uint i;\n            NOTIFY(1, \"--drx-- for job 0x%x got %d jobs in list\\n\", job,\n                   list->NumberOfProcessIdsInList);\n            for (i = 0; i < list->NumberOfProcessIdsInList; i++) {\n                process_id_t pid = list->ProcessIdList[i];\n                if (!soft_kills_invoke_cbs(pid, exit_code)) {\n                    /* Client is not terminating and requests not to skip the action.\n                     * But since we have multiple pids, we go with a local decision\n                     * here and emulate the kill.\n                     */\n                    HANDLE phandle = dr_convert_pid_to_handle(pid);\n                    if (phandle != INVALID_HANDLE_VALUE)\n                        NtTerminateProcess(phandle, exit_code);\n                    /* else, child stays alive: not much we can do */\n                }\n            }\n        }\n        dr_thread_free(drcontext, buf, sz);\n    } /* else query failed: I'd issue a warning log msg if not inside drx library */\n}\n\nstatic void\nsoft_kills_free_job_info(void *ptr)\n{\n    job_info_t *jinfo = (job_info_t *)ptr;\n    if (jinfo->ref_count == 0)\n        dr_global_free(jinfo, sizeof(*jinfo));\n}\n\n/* Called when the app closes a job handle \"job\".\n * Caller must hold job_table lock.\n * If \"remove\" is true, removes from the hashtable and de-allocates \"jinfo\",\n * if refcount is 0.\n */\nstatic void\nsoft_kills_handle_close(void *drcontext, job_info_t *jinfo, HANDLE job, int exit_code,\n                        bool remove)\n{\n    ASSERT(jinfo->ref_count > 0, \"invalid ref count\");\n    jinfo->ref_count--;\n    if (jinfo->ref_count == 0) {\n        NOTIFY(1, \"--drx-- closing kill-on-close handle 0x%x in pid %d\\n\", job,\n               dr_get_process_id());\n        /* XXX: It's possible for us to miss a handle being closed from another\n         * process.  In such a case, our ref count won't reach 0 and we'll\n         * fail to kill the child at all.\n         * If that handle value is re-used as a job object (else our job queryies\n         * will get STATUS_OBJECT_TYPE_MISMATCH) with no kill-on-close, we could\n         * incorrectly kill a job when the app is just closing its handle, but\n         * this would only happen when a job is being controlled from multiple\n         * processes.  We'll have to live with the risk.  We could watch\n         * NtCreateJobObject but it doesn't seem worth it.\n         */\n        soft_kills_handle_job_termination(drcontext, job, exit_code);\n    }\n    if (remove)\n        hashtable_remove(&job_table, (void *)job);\n}\n\nstatic bool\nsoft_kills_filter_syscall(void *drcontext, int sysnum)\n{\n    return (sysnum == sysnum_TerminateProcess || sysnum == sysnum_TerminateJobObject ||\n            sysnum == sysnum_SetInformationJobObject || sysnum == sysnum_Close ||\n            sysnum == sysnum_DuplicateObject);\n}\n\nstatic bool\nsoft_kills_pre_SetInformationJobObject(void *drcontext, cls_soft_t *cls)\n{\n    HANDLE job = (HANDLE)dr_syscall_get_param(drcontext, 0);\n    JOBOBJECTINFOCLASS class = (JOBOBJECTINFOCLASS)dr_syscall_get_param(drcontext, 1);\n    ULONG sz = (ULONG)dr_syscall_get_param(drcontext, 3);\n    /* MSDN claims that JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE requires an\n     * extended info struct, which we trust, though it seems odd as it's\n     * a flag in the basic struct.\n     */\n    JOBOBJECT_EXTENDED_LIMIT_INFORMATION info;\n    if (class == JobObjectExtendedLimitInformation && sz >= sizeof(info) &&\n        dr_safe_read((byte *)dr_syscall_get_param(drcontext, 2), sizeof(info), &info,\n                     NULL)) {\n        if (TEST(JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE,\n                 info.BasicLimitInformation.LimitFlags)) {\n            /* Remove the kill-on-close flag from the syscall arg.\n             * We restore in post-syscall in case app uses the memory\n             * for something else.  There is of course a race where another\n             * thread could use it and get the wrong value: -soft_kills isn't\n             * perfect.\n             */\n            JOBOBJECT_EXTENDED_LIMIT_INFORMATION *ptr =\n                (JOBOBJECT_EXTENDED_LIMIT_INFORMATION *)dr_syscall_get_param(drcontext,\n                                                                             2);\n            ULONG new_flags = info.BasicLimitInformation.LimitFlags &\n                (~JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE);\n            bool isnew;\n            job_info_t *jinfo;\n            cls->job_limit_flags_orig = info.BasicLimitInformation.LimitFlags;\n            cls->job_limit_flags_loc = &ptr->BasicLimitInformation.LimitFlags;\n            ASSERT(sizeof(cls->job_limit_flags_orig) ==\n                       sizeof(ptr->BasicLimitInformation.LimitFlags),\n                   \"size mismatch\");\n            if (!dr_safe_write(cls->job_limit_flags_loc,\n                               sizeof(ptr->BasicLimitInformation.LimitFlags), &new_flags,\n                               NULL)) {\n                /* XXX: Any way we can send a WARNING on our failure to write? */\n                NOTIFY(1,\n                       \"--drx-- FAILED to remove kill-on-close from job 0x%x \"\n                       \"in pid %d\\n\",\n                       job, dr_get_process_id());\n            } else {\n                NOTIFY(1, \"--drx-- removed kill-on-close from job 0x%x in pid %d\\n\", job,\n                       dr_get_process_id());\n            }\n            /* Track the handle so we can notify the client on close or exit */\n            hashtable_lock(&job_table);\n            /* See if already there (in case app called Set 2x) */\n            if (hashtable_lookup(&job_table, (void *)job) == NULL) {\n                jinfo = (job_info_t *)dr_global_alloc(sizeof(*jinfo));\n                jinfo->ref_count = 1;\n                isnew = hashtable_add(&job_table, (void *)job, (void *)jinfo);\n                ASSERT(isnew, \"missed an NtClose\");\n            }\n            hashtable_unlock(&job_table);\n        }\n    }\n    return true;\n}\n\n/* We must do two things on NtDuplicateObject:\n * 1) Update our job table: adding a new entry for the duplicate,\n *    and removing the source handle if it is closed.\n * 2) Process a handle being closed but a new one not being\n *    created (in this process): corner case that triggers a kill.\n */\nstatic bool\nsoft_kills_pre_DuplicateObject(void *drcontext, cls_soft_t *cls)\n{\n    HANDLE proc_src = (HANDLE)dr_syscall_get_param(drcontext, 0);\n    process_id_t id_src = dr_convert_handle_to_pid(proc_src);\n    cls->dup_proc_src_us = (id_src == dr_get_process_id());\n    cls->dup_jinfo = NULL;\n    if (cls->dup_proc_src_us) {\n        /* NtDuplicateObject seems more likely than NtClose to fail, so we\n         * shift as much handling as possible post-syscall.\n         */\n        HANDLE proc_dst = (HANDLE)dr_syscall_get_param(drcontext, 2);\n        process_id_t id_dst = dr_convert_handle_to_pid(proc_dst);\n        cls->dup_proc_dst_us = (id_dst == dr_get_process_id());\n        cls->dup_src = (HANDLE)dr_syscall_get_param(drcontext, 1);\n        cls->dup_dst = (HANDLE *)dr_syscall_get_param(drcontext, 3);\n        cls->dup_options = (ULONG)dr_syscall_get_param(drcontext, 6);\n        hashtable_lock(&job_table);\n        /* We have to save jinfo b/c dup_src will be gone */\n        cls->dup_jinfo = (job_info_t *)hashtable_lookup(&job_table, (void *)cls->dup_src);\n        if (cls->dup_jinfo != NULL) {\n            if (TEST(DUPLICATE_CLOSE_SOURCE, cls->dup_options)) {\n                /* \"This occurs regardless of any error status returned\"\n                 * according to MSDN DuplicateHandle, and Nebbett.\n                 * Thus, we act on this here, which avoids any handle value\n                 * reuse race, and we don't have to undo in post.\n                 * If this weren't true, we'd have to reinstate in the table\n                 * on failure, and we'd have to duplicate the handle\n                 * (dr_dup_file_handle would do it -- at least w/ current impl)\n                 * to call soft_kills_handle_close() in post.\n                 */\n                if (!cls->dup_proc_dst_us) {\n                    NOTIFY(1, \"--drx-- job 0x%x closed in pid %d w/ dst outside proc\\n\",\n                           cls->dup_src, dr_get_process_id());\n                    /* The exit code is set to 0 by the kernel for this case */\n                    soft_kills_handle_close(drcontext, cls->dup_jinfo, cls->dup_src, 0,\n                                            true /*remove*/);\n                } else {\n                    hashtable_remove(&job_table, (void *)cls->dup_src);\n                    /* Adjust refcount after removing to avoid freeing prematurely.\n                     * The refcount may be sitting at 0, but no other thread should\n                     * be able to affect it as there is no hashtable entry.\n                     */\n                    ASSERT(cls->dup_jinfo->ref_count > 0, \"invalid ref count\");\n                    cls->dup_jinfo->ref_count--;\n                }\n            }\n        }\n        hashtable_unlock(&job_table);\n    }\n    return true;\n}\n\nstatic void\nsoft_kills_post_DuplicateObject(void *drcontext)\n{\n    cls_soft_t *cls = (cls_soft_t *)drmgr_get_cls_field(drcontext, cls_idx_soft);\n    HANDLE dup_dst;\n    if (cls->dup_jinfo == NULL)\n        return;\n    if (!NT_SUCCESS(dr_syscall_get_result(drcontext)))\n        return;\n    ASSERT(cls->dup_proc_src_us, \"shouldn't get here\");\n    if (!cls->dup_proc_dst_us)\n        return; /* already handled in pre */\n    /* At this point we have a successful intra-process duplication.  If\n     * DUPLICATE_CLOSE_SOURCE, we already removed from the table in pre.\n     */\n    hashtable_lock(&job_table);\n    if (cls->dup_dst != NULL &&\n        dr_safe_read(cls->dup_dst, sizeof(dup_dst), &dup_dst, NULL)) {\n        NOTIFY(1, \"--drx-- job 0x%x duplicated as 0x%x in pid %d\\n\", cls->dup_src,\n               dup_dst, dr_get_process_id());\n        cls->dup_jinfo->ref_count++;\n        hashtable_add(&job_table, (void *)dup_dst, (void *)cls->dup_jinfo);\n    }\n    hashtable_unlock(&job_table);\n}\n\n/* Returns whether to execute the system call */\nstatic bool\nsoft_kills_pre_syscall(void *drcontext, int sysnum)\n{\n    cls_soft_t *cls = (cls_soft_t *)drmgr_get_cls_field(drcontext, cls_idx_soft);\n    /* Xref DrMem i#544, DrMem i#1297, and DRi#1231: give child\n     * processes a chance for clean exit for dumping of data or other\n     * actions.\n     *\n     * XXX: a child under DR but not a supporting client will be left\n     * alive: but that's a risk we can live with.\n     */\n    if (sysnum == sysnum_TerminateProcess) {\n        HANDLE proc = (HANDLE)dr_syscall_get_param(drcontext, 0);\n        process_id_t pid = dr_convert_handle_to_pid(proc);\n        if (pid != INVALID_PROCESS_ID && pid != dr_get_process_id()) {\n            int exit_code = (int)dr_syscall_get_param(drcontext, 1);\n            NOTIFY(1, \"--drx-- NtTerminateProcess in pid %d\\n\", dr_get_process_id());\n            if (soft_kills_invoke_cbs(pid, exit_code)) {\n                dr_syscall_set_result(drcontext, 0 /*success*/);\n                return false; /* skip syscall */\n            } else\n                return true; /* execute syscall */\n        }\n    } else if (sysnum == sysnum_TerminateJobObject) {\n        /* There are several ways a process in a job can be killed:\n         *\n         *   1) NtTerminateJobObject\n         *   2) The last handle is closed + JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE is set\n         *   3) JOB_OBJECT_LIMIT_ACTIVE_PROCESS is hit\n         *   4) Time limit and JOB_OBJECT_TERMINATE_AT_END_OF_JOB is hit\n         *\n         * XXX: we only handle #1 and #2.\n         */\n        HANDLE job = (HANDLE)dr_syscall_get_param(drcontext, 0);\n        NTSTATUS exit_code = (NTSTATUS)dr_syscall_get_param(drcontext, 1);\n        NOTIFY(1, \"--drx-- NtTerminateJobObject job 0x%x in pid %d\\n\", job,\n               dr_get_process_id());\n        soft_kills_handle_job_termination(drcontext, job, exit_code);\n        /* We always skip this syscall.  If individual processes were requested\n         * to not be skipped, we emulated via NtTerminateProcess in\n         * soft_kills_handle_job_termination().\n         */\n        dr_syscall_set_result(drcontext, 0 /*success*/);\n        return false; /* skip syscall */\n    } else if (sysnum == sysnum_SetInformationJobObject) {\n        return soft_kills_pre_SetInformationJobObject(drcontext, cls);\n    } else if (sysnum == sysnum_Close) {\n        /* If a job object, act on it, and remove from our table */\n        HANDLE handle = (HANDLE)dr_syscall_get_param(drcontext, 0);\n        job_info_t *jinfo;\n        hashtable_lock(&job_table);\n        jinfo = (job_info_t *)hashtable_lookup(&job_table, (void *)handle);\n        if (jinfo != NULL) {\n            NOTIFY(1, \"--drx-- explicit close of job 0x%x in pid %d\\n\", handle,\n                   dr_get_process_id());\n            /* The exit code is set to 0 by the kernel for this case */\n            soft_kills_handle_close(drcontext, jinfo, handle, 0, true /*remove*/);\n        }\n        hashtable_unlock(&job_table);\n    } else if (sysnum == sysnum_DuplicateObject) {\n        return soft_kills_pre_DuplicateObject(drcontext, cls);\n    }\n    return true;\n}\n\nstatic void\nsoft_kills_post_syscall(void *drcontext, int sysnum)\n{\n    if (sysnum == sysnum_SetInformationJobObject) {\n        cls_soft_t *cls = (cls_soft_t *)drmgr_get_cls_field(drcontext, cls_idx_soft);\n        if (cls->job_limit_flags_loc != NULL) {\n            /* Restore the app's memory */\n            if (!dr_safe_write(cls->job_limit_flags_loc,\n                               sizeof(cls->job_limit_flags_orig),\n                               &cls->job_limit_flags_orig, NULL)) {\n                /* If we weren't in drx lib I'd log a warning */\n            }\n            cls->job_limit_flags_loc = NULL;\n        }\n    } else if (sysnum == sysnum_DuplicateObject) {\n        soft_kills_post_DuplicateObject(drcontext);\n    }\n}\n#else /* WINDOWS */\n\nstatic bool\nsoft_kills_filter_syscall(void *drcontext, int sysnum)\n{\n    return (sysnum == SYS_kill);\n}\n\n/* Returns whether to execute the system call */\nstatic bool\nsoft_kills_pre_syscall(void *drcontext, int sysnum)\n{\n    if (sysnum == SYS_kill) {\n        process_id_t pid = (process_id_t)dr_syscall_get_param(drcontext, 0);\n        int sig = (int)dr_syscall_get_param(drcontext, 1);\n        if (sig == SIGKILL && pid != INVALID_PROCESS_ID && pid != dr_get_process_id()) {\n            /* Pass exit code << 8 for use with dr_exit_process() */\n            int exit_code = sig << 8;\n            if (soft_kills_invoke_cbs(pid, exit_code)) {\n                /* set result to 0 (success) and use_high and use_errno to false */\n                dr_syscall_result_info_t info = {\n                    sizeof(info),\n                };\n                info.succeeded = true;\n                dr_syscall_set_result_ex(drcontext, &info);\n                return false; /* skip syscall */\n            } else\n                return true; /* execute syscall */\n        }\n    }\n    return true;\n}\n\nstatic void\nsoft_kills_post_syscall(void *drcontext, int sysnum)\n{\n    /* nothing yet */\n}\n\n#endif /* UNIX */\n\nstatic bool\nsoft_kills_init(void)\n{\n#ifdef WINDOWS\n    IF_DEBUG(bool ok;)\n#endif\n    /* XXX: would be nice to fail if it's not still process init,\n     * but we don't have an easy way to check.\n     */\n    soft_kills_enabled = true;\n\n    NOTIFY(1, \"--drx-- init pid %d %s\\n\", dr_get_process_id(), dr_get_application_name());\n\n    cb_lock = dr_mutex_create();\n\n#ifdef WINDOWS\n    hashtable_init_ex(&job_table, JOB_TABLE_HASH_BITS, HASH_INTPTR, false /*!strdup*/,\n                      false /*!synch*/, soft_kills_free_job_info, NULL, NULL);\n\n    sysnum_TerminateProcess =\n        soft_kills_get_sysnum(\"NtTerminateProcess\", SYS_NUM_PARAMS_TerminateProcess,\n                              SYS_WOW64_IDX_TerminateProcess);\n    if (sysnum_TerminateProcess == -1)\n        return false;\n    sysnum_TerminateJobObject =\n        soft_kills_get_sysnum(\"NtTerminateJobObject\", SYS_NUM_PARAMS_TerminateJobObject,\n                              SYS_WOW64_IDX_TerminateJobObject);\n    if (sysnum_TerminateJobObject == -1)\n        return false;\n    sysnum_SetInformationJobObject = soft_kills_get_sysnum(\n        \"NtSetInformationJobObject\", SYS_NUM_PARAMS_SetInformationJobObject,\n        SYS_WOW64_IDX_SetInformationJobObject);\n    if (sysnum_SetInformationJobObject == -1)\n        return false;\n    sysnum_Close =\n        soft_kills_get_sysnum(\"NtClose\", SYS_NUM_PARAMS_Close, SYS_WOW64_IDX_Close);\n    if (sysnum_Close == -1)\n        return false;\n    sysnum_DuplicateObject =\n        soft_kills_get_sysnum(\"NtDuplicateObject\", SYS_NUM_PARAMS_DuplicateObject,\n                              SYS_WOW64_IDX_DuplicateObject);\n    if (sysnum_DuplicateObject == -1)\n        return false;\n\n    cls_idx_soft =\n        drmgr_register_cls_field(soft_kills_context_init, soft_kills_context_exit);\n    if (cls_idx_soft == -1)\n        return false;\n\n    /* Ensure that DR intercepts these when we're native */\n    IF_DEBUG(ok =)\n    dr_syscall_intercept_natively(\"NtTerminateProcess\", sysnum_TerminateProcess,\n                                  SYS_NUM_PARAMS_TerminateProcess,\n                                  SYS_WOW64_IDX_TerminateProcess);\n    ASSERT(ok, \"failure to watch syscall while native\");\n    IF_DEBUG(ok =)\n    dr_syscall_intercept_natively(\"NtTerminateJobObject\", sysnum_TerminateJobObject,\n                                  SYS_NUM_PARAMS_TerminateJobObject,\n                                  SYS_WOW64_IDX_TerminateJobObject);\n    ASSERT(ok, \"failure to watch syscall while native\");\n    IF_DEBUG(ok =)\n    dr_syscall_intercept_natively(\n        \"NtSetInformationJobObject\", sysnum_SetInformationJobObject,\n        SYS_NUM_PARAMS_SetInformationJobObject, SYS_WOW64_IDX_SetInformationJobObject);\n    ASSERT(ok, \"failure to watch syscall while native\");\n    IF_DEBUG(ok =)\n    dr_syscall_intercept_natively(\"NtClose\", sysnum_Close, SYS_NUM_PARAMS_Close,\n                                  SYS_WOW64_IDX_Close);\n    ASSERT(ok, \"failure to watch syscall while native\");\n    IF_DEBUG(ok =)\n    dr_syscall_intercept_natively(\"NtDuplicateObject\", sysnum_DuplicateObject,\n                                  SYS_NUM_PARAMS_DuplicateObject,\n                                  SYS_WOW64_IDX_DuplicateObject);\n    ASSERT(ok, \"failure to watch syscall while native\");\n#endif\n\n    if (!drmgr_register_pre_syscall_event(soft_kills_pre_syscall) ||\n        !drmgr_register_post_syscall_event(soft_kills_post_syscall))\n        return false;\n    dr_register_filter_syscall_event(soft_kills_filter_syscall);\n\n    return true;\n}\n\nstatic void\nsoft_kills_exit(void)\n{\n    cb_entry_t *e;\n#ifdef WINDOWS\n    /* Any open job handles will be closed, triggering\n     * JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE\n     */\n    uint i;\n    /* The exit code used is the exit code for this process */\n    int exit_code;\n    if (!get_app_exit_code(&exit_code))\n        exit_code = 0;\n    hashtable_lock(&job_table);\n    for (i = 0; i < HASHTABLE_SIZE(job_table.table_bits); i++) {\n        hash_entry_t *he;\n        for (he = job_table.table[i]; he != NULL; he = he->next) {\n            HANDLE job = (HANDLE)he->key;\n            job_info_t *jinfo = (job_info_t *)he->payload;\n            NOTIFY(1, \"--drx-- implicit close of job 0x%x in pid %d\\n\", job,\n                   dr_get_process_id());\n            soft_kills_handle_close(dr_get_current_drcontext(), jinfo, job, exit_code,\n                                    false /*do not remove*/);\n        }\n    }\n    hashtable_unlock(&job_table);\n\n    hashtable_delete(&job_table);\n\n    drmgr_unregister_cls_field(soft_kills_context_init, soft_kills_context_exit,\n                               cls_idx_soft);\n#endif\n\n    dr_mutex_lock(cb_lock);\n    while (cb_list != NULL) {\n        e = cb_list;\n        cb_list = e->next;\n        dr_global_free(e, sizeof(*e));\n    }\n    dr_mutex_unlock(cb_lock);\n\n    dr_mutex_destroy(cb_lock);\n}\n\nbool\ndrx_register_soft_kills(bool (*event_cb)(process_id_t pid, int exit_code))\n{\n    /* We split our init from drx_init() to avoid extra work when nobody\n     * requests this feature.\n     */\n    static int soft_kills_init_count;\n    cb_entry_t *e;\n    int count = dr_atomic_add32_return_sum(&soft_kills_init_count, 1);\n    if (count == 1) {\n        soft_kills_init();\n    }\n\n    e = dr_global_alloc(sizeof(*e));\n    e->cb = event_cb;\n\n    dr_mutex_lock(cb_lock);\n    e->next = cb_list;\n    cb_list = e;\n    dr_mutex_unlock(cb_lock);\n    return true;\n}\n\n/***************************************************************************\n * INSTRUCTION LIST\n */\n\nDR_EXPORT\nsize_t\ndrx_instrlist_size(instrlist_t *ilist)\n{\n    instr_t *instr;\n    size_t size = 0;\n\n    for (instr = instrlist_first(ilist); instr != NULL; instr = instr_get_next(instr))\n        size++;\n\n    return size;\n}\n\nDR_EXPORT\nsize_t\ndrx_instrlist_app_size(instrlist_t *ilist)\n{\n    instr_t *instr;\n    size_t size = 0;\n\n    for (instr = instrlist_first_app(ilist); instr != NULL;\n         instr = instr_get_next_app(instr))\n        size++;\n\n    return size;\n}\n\n/***************************************************************************\n * LOGGING\n */\n#ifdef WINDOWS\n#    define DIRSEP '\\\\'\n#else\n#    define DIRSEP '/'\n#endif\n\nfile_t\ndrx_open_unique_file(const char *dir, const char *prefix, const char *suffix,\n                     uint extra_flags, char *result OUT, size_t result_len)\n{\n    char buf[MAXIMUM_PATH];\n    file_t f = INVALID_FILE;\n    int i;\n    ssize_t len;\n    for (i = 0; i < 10000; i++) {\n        len = dr_snprintf(\n            buf, BUFFER_SIZE_ELEMENTS(buf), \"%s%c%s.%04d.%s\", dir, DIRSEP, prefix,\n            (extra_flags == DRX_FILE_SKIP_OPEN) ? dr_get_random_value(9999) : i, suffix);\n        if (len < 0)\n            return INVALID_FILE;\n        NULL_TERMINATE_BUFFER(buf);\n        if (extra_flags != DRX_FILE_SKIP_OPEN)\n            f = dr_open_file(buf, DR_FILE_WRITE_REQUIRE_NEW | extra_flags);\n        if (f != INVALID_FILE || extra_flags == DRX_FILE_SKIP_OPEN) {\n            if (result != NULL)\n                dr_snprintf(result, result_len, \"%s\", buf);\n            return f;\n        }\n    }\n    return INVALID_FILE;\n}\n\nfile_t\ndrx_open_unique_appid_file(const char *dir, ptr_int_t id, const char *prefix,\n                           const char *suffix, uint extra_flags, char *result OUT,\n                           size_t result_len)\n{\n    int len;\n    char appid[MAXIMUM_PATH];\n    const char *app_name = dr_get_application_name();\n    if (app_name == NULL)\n        app_name = \"<unknown-app>\";\n    len = dr_snprintf(appid, BUFFER_SIZE_ELEMENTS(appid), \"%s.%s.%05d\", prefix, app_name,\n                      id);\n    if (len < 0 || (size_t)len >= BUFFER_SIZE_ELEMENTS(appid))\n        return INVALID_FILE;\n    NULL_TERMINATE_BUFFER(appid);\n\n    return drx_open_unique_file(dir, appid, suffix, extra_flags, result, result_len);\n}\n\nbool\ndrx_open_unique_appid_dir(const char *dir, ptr_int_t id, const char *prefix,\n                          const char *suffix, char *result OUT, size_t result_len)\n{\n    char buf[MAXIMUM_PATH];\n    int i;\n    ssize_t len;\n    for (i = 0; i < 10000; i++) {\n        const char *app_name = dr_get_application_name();\n        if (app_name == NULL)\n            app_name = \"<unknown-app>\";\n        len = dr_snprintf(buf, BUFFER_SIZE_ELEMENTS(buf), \"%s%c%s.%s.%05d.%04d.%s\", dir,\n                          DIRSEP, prefix, app_name, id, i, suffix);\n        if (len < 0 || (size_t)len >= BUFFER_SIZE_ELEMENTS(buf))\n            return false;\n        NULL_TERMINATE_BUFFER(buf);\n        if (dr_create_dir(buf)) {\n            if (result != NULL)\n                dr_snprintf(result, result_len, \"%s\", buf);\n            return true;\n        }\n    }\n    return false;\n}\n\nbool\ndrx_tail_pad_block(void *drcontext, instrlist_t *ilist)\n{\n    instr_t *last = instrlist_last_app(ilist);\n\n    if (instr_is_cti(last) || instr_is_syscall(last)) {\n        /* This basic block is already branch or syscall-terminated */\n        return false;\n    }\n    instrlist_meta_postinsert(ilist, last, INSTR_CREATE_label(drcontext));\n    return true;\n}\n\n/***************************************************************************\n * drx_expand_scatter_gather() related auxiliary functions and structures.\n */\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n\ntypedef struct _scatter_gather_info_t {\n    bool is_evex;\n    bool is_load;\n    opnd_size_t scalar_index_size;\n    opnd_size_t scalar_value_size;\n    opnd_size_t scatter_gather_size;\n    reg_id_t mask_reg;\n    reg_id_t base_reg;\n    reg_id_t index_reg;\n    union {\n        reg_id_t gather_dst_reg;\n        reg_id_t scatter_src_reg;\n    };\n    int disp;\n    int scale;\n} scatter_gather_info_t;\n\nstatic void\nget_scatter_gather_info(instr_t *instr, scatter_gather_info_t *sg_info)\n{\n    /* We detect whether the instruction is EVEX by looking at its potential mask\n     * operand.\n     */\n    opnd_t dst0 = instr_get_dst(instr, 0);\n    opnd_t src0 = instr_get_src(instr, 0);\n    opnd_t src1 = instr_get_src(instr, 1);\n    sg_info->is_evex = opnd_is_reg(src0) && reg_is_opmask(opnd_get_reg(src0));\n    sg_info->mask_reg = sg_info->is_evex ? opnd_get_reg(src0) : opnd_get_reg(src1);\n    ASSERT(!sg_info->is_evex ||\n               (opnd_get_reg(instr_get_dst(instr, 1)) == opnd_get_reg(src0)),\n           \"Invalid gather instruction.\");\n    int opc = instr_get_opcode(instr);\n    opnd_t memopnd;\n    switch (opc) {\n    case OP_vgatherdpd:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = true;\n        break;\n    case OP_vgatherqpd:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = true;\n        break;\n    case OP_vgatherdps:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = true;\n        break;\n    case OP_vgatherqps:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = true;\n        break;\n    case OP_vpgatherdd:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = true;\n        break;\n    case OP_vpgatherqd:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = true;\n        break;\n    case OP_vpgatherdq:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = true;\n        break;\n    case OP_vpgatherqq:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = true;\n        break;\n    case OP_vscatterdpd:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = false;\n        break;\n    case OP_vscatterqpd:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = false;\n        break;\n    case OP_vscatterdps:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = false;\n        break;\n    case OP_vscatterqps:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = false;\n        break;\n    case OP_vpscatterdd:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = false;\n        break;\n    case OP_vpscatterqd:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_4;\n        sg_info->is_load = false;\n        break;\n    case OP_vpscatterdq:\n        sg_info->scalar_index_size = OPSZ_4;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = false;\n        break;\n    case OP_vpscatterqq:\n        sg_info->scalar_index_size = OPSZ_8;\n        sg_info->scalar_value_size = OPSZ_8;\n        sg_info->is_load = false;\n        break;\n    default:\n        ASSERT(false, \"Incorrect opcode.\");\n        memopnd = opnd_create_null();\n        break;\n    }\n    if (sg_info->is_load) {\n        sg_info->scatter_gather_size = opnd_get_size(dst0);\n        sg_info->gather_dst_reg = opnd_get_reg(dst0);\n        memopnd = sg_info->is_evex ? src1 : src0;\n    } else {\n        sg_info->scatter_gather_size = opnd_get_size(src1);\n        sg_info->scatter_src_reg = opnd_get_reg(src1);\n        memopnd = dst0;\n    }\n    sg_info->index_reg = opnd_get_index(memopnd);\n    sg_info->base_reg = opnd_get_base(memopnd);\n    sg_info->disp = opnd_get_disp(memopnd);\n    sg_info->scale = opnd_get_scale(memopnd);\n}\n\nstatic bool\nexpand_gather_insert_scalar(void *drcontext, instrlist_t *bb, instr_t *sg_instr, int el,\n                            scatter_gather_info_t *sg_info, reg_id_t simd_reg,\n                            reg_id_t scalar_reg, reg_id_t scratch_xmm, bool is_avx512,\n                            app_pc orig_app_pc)\n{\n    /* Used by both AVX2 and AVX-512. */\n    ASSERT(instr_is_gather(sg_instr), \"Internal error: only gather instructions.\");\n    reg_id_t simd_reg_zmm = reg_resize_to_opsz(simd_reg, OPSZ_64);\n    reg_id_t simd_reg_ymm = reg_resize_to_opsz(simd_reg, OPSZ_32);\n    uint scalar_value_bytes = opnd_size_in_bytes(sg_info->scalar_value_size);\n    int scalarxmmimm = el * scalar_value_bytes / XMM_REG_SIZE;\n    if (is_avx512) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vextracti32x4_mask(\n                             drcontext, opnd_create_reg(scratch_xmm),\n                             opnd_create_reg(DR_REG_K0),\n                             opnd_create_immed_int(scalarxmmimm, OPSZ_1),\n                             opnd_create_reg(simd_reg_zmm)),\n                         orig_app_pc));\n    } else {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(\n                   INSTR_CREATE_vextracti128(drcontext, opnd_create_reg(scratch_xmm),\n                                             opnd_create_reg(simd_reg_ymm),\n                                             opnd_create_immed_int(scalarxmmimm, OPSZ_1)),\n                   orig_app_pc));\n    }\n    if (sg_info->scalar_value_size == OPSZ_4) {\n        PREXL8(\n            bb, sg_instr,\n            INSTR_XL8(\n                INSTR_CREATE_vpinsrd(\n                    drcontext, opnd_create_reg(scratch_xmm), opnd_create_reg(scratch_xmm),\n                    opnd_create_reg(IF_X64_ELSE(reg_64_to_32(scalar_reg), scalar_reg)),\n                    opnd_create_immed_int((el * scalar_value_bytes) % XMM_REG_SIZE /\n                                              opnd_size_in_bytes(OPSZ_4),\n                                          OPSZ_1)),\n                orig_app_pc));\n    } else if (sg_info->scalar_value_size == OPSZ_8) {\n        ASSERT(reg_is_64bit(scalar_reg),\n               \"The qword index versions are unsupported in 32-bit mode.\");\n        PREXL8(\n            bb, sg_instr,\n            INSTR_XL8(INSTR_CREATE_vpinsrq(\n                          drcontext, opnd_create_reg(scratch_xmm),\n                          opnd_create_reg(scratch_xmm), opnd_create_reg(scalar_reg),\n                          opnd_create_immed_int((el * scalar_value_bytes) % XMM_REG_SIZE /\n                                                    opnd_size_in_bytes(OPSZ_8),\n                                                OPSZ_1)),\n                      orig_app_pc));\n\n    } else {\n        ASSERT(false, \"Unexpected index size.\");\n    }\n    if (is_avx512) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vinserti32x4_mask(\n                             drcontext, opnd_create_reg(simd_reg_zmm),\n                             opnd_create_reg(DR_REG_K0),\n                             opnd_create_immed_int(scalarxmmimm, OPSZ_1),\n                             opnd_create_reg(simd_reg_zmm), opnd_create_reg(scratch_xmm)),\n                         orig_app_pc));\n    } else {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vinserti128(\n                             drcontext, opnd_create_reg(simd_reg_ymm),\n                             opnd_create_reg(simd_reg_ymm), opnd_create_reg(scratch_xmm),\n                             opnd_create_immed_int(scalarxmmimm, OPSZ_1)),\n                         orig_app_pc));\n    }\n    return true;\n}\n\nstatic bool\nexpand_avx512_gather_insert_scalar_value(void *drcontext, instrlist_t *bb,\n                                         instr_t *sg_instr, int el,\n                                         scatter_gather_info_t *sg_info,\n                                         reg_id_t scalar_value_reg, reg_id_t scratch_xmm,\n                                         app_pc orig_app_pc)\n{\n    return expand_gather_insert_scalar(drcontext, bb, sg_instr, el, sg_info,\n                                       sg_info->gather_dst_reg, scalar_value_reg,\n                                       scratch_xmm, true /* AVX-512 */, orig_app_pc);\n}\n\nstatic bool\nexpand_avx2_gather_insert_scalar_value(void *drcontext, instrlist_t *bb,\n                                       instr_t *sg_instr, int el,\n                                       scatter_gather_info_t *sg_info,\n                                       reg_id_t scalar_value_reg, reg_id_t scratch_xmm,\n                                       app_pc orig_app_pc)\n{\n    return expand_gather_insert_scalar(drcontext, bb, sg_instr, el, sg_info,\n                                       sg_info->gather_dst_reg, scalar_value_reg,\n                                       scratch_xmm, false /* AVX2 */, orig_app_pc);\n}\n\nstatic bool\nexpand_avx2_gather_insert_scalar_mask(void *drcontext, instrlist_t *bb, instr_t *sg_instr,\n                                      int el, scatter_gather_info_t *sg_info,\n                                      reg_id_t scalar_index_reg, reg_id_t scratch_xmm,\n                                      app_pc orig_app_pc)\n{\n    return expand_gather_insert_scalar(drcontext, bb, sg_instr, el, sg_info,\n                                       sg_info->mask_reg, scalar_index_reg, scratch_xmm,\n                                       false /* AVX2 */, orig_app_pc);\n}\n\nstatic bool\nexpand_scatter_gather_extract_scalar(void *drcontext, instrlist_t *bb, instr_t *sg_instr,\n                                     int el, scatter_gather_info_t *sg_info,\n                                     opnd_size_t scalar_size, uint scalar_bytes,\n                                     reg_id_t from_simd_reg, reg_id_t scratch_xmm,\n                                     reg_id_t scratch_reg, bool is_avx512,\n                                     app_pc orig_app_pc)\n{\n    reg_id_t from_simd_reg_zmm = reg_resize_to_opsz(from_simd_reg, OPSZ_64);\n    reg_id_t from_simd_reg_ymm = reg_resize_to_opsz(from_simd_reg, OPSZ_32);\n    int scalarxmmimm = el * scalar_bytes / XMM_REG_SIZE;\n    if (is_avx512) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vextracti32x4_mask(\n                             drcontext, opnd_create_reg(scratch_xmm),\n                             opnd_create_reg(DR_REG_K0),\n                             opnd_create_immed_int(scalarxmmimm, OPSZ_1),\n                             opnd_create_reg(from_simd_reg_zmm)),\n                         orig_app_pc));\n    } else {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(\n                   INSTR_CREATE_vextracti128(drcontext, opnd_create_reg(scratch_xmm),\n                                             opnd_create_reg(from_simd_reg_ymm),\n                                             opnd_create_immed_int(scalarxmmimm, OPSZ_1)),\n                   orig_app_pc));\n    }\n    if (scalar_size == OPSZ_4) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vpextrd(\n                             drcontext,\n                             opnd_create_reg(\n                                 IF_X64_ELSE(reg_64_to_32(scratch_reg), scratch_reg)),\n                             opnd_create_reg(scratch_xmm),\n                             opnd_create_immed_int((el * scalar_bytes) % XMM_REG_SIZE /\n                                                       opnd_size_in_bytes(OPSZ_4),\n                                                   OPSZ_1)),\n                         orig_app_pc));\n    } else if (scalar_size == OPSZ_8) {\n        ASSERT(reg_is_64bit(scratch_reg),\n               \"The qword index versions are unsupported in 32-bit mode.\");\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vpextrq(\n                             drcontext, opnd_create_reg(scratch_reg),\n                             opnd_create_reg(scratch_xmm),\n                             opnd_create_immed_int((el * scalar_bytes) % XMM_REG_SIZE /\n                                                       opnd_size_in_bytes(OPSZ_8),\n                                                   OPSZ_1)),\n                         orig_app_pc));\n    } else {\n        ASSERT(false, \"Unexpected scalar size.\");\n        return false;\n    }\n    return true;\n}\n\nstatic bool\nexpand_avx512_scatter_extract_scalar_value(void *drcontext, instrlist_t *bb,\n                                           instr_t *sg_instr, int el,\n                                           scatter_gather_info_t *sg_info,\n                                           reg_id_t scratch_xmm, reg_id_t scratch_reg,\n                                           app_pc orig_app_pc)\n{\n    return expand_scatter_gather_extract_scalar(\n        drcontext, bb, sg_instr, el, sg_info, sg_info->scalar_value_size,\n        opnd_size_in_bytes(sg_info->scalar_value_size), sg_info->scatter_src_reg,\n        scratch_xmm, scratch_reg, true /* AVX-512 */, orig_app_pc);\n}\n\nstatic bool\nexpand_avx512_scatter_gather_extract_scalar_index(void *drcontext, instrlist_t *bb,\n                                                  instr_t *sg_instr, int el,\n                                                  scatter_gather_info_t *sg_info,\n                                                  reg_id_t scratch_xmm,\n                                                  reg_id_t scratch_reg,\n                                                  app_pc orig_app_pc)\n{\n    return expand_scatter_gather_extract_scalar(\n        drcontext, bb, sg_instr, el, sg_info, sg_info->scalar_index_size,\n        opnd_size_in_bytes(sg_info->scalar_index_size), sg_info->index_reg, scratch_xmm,\n        scratch_reg, true /* AVX-512 */, orig_app_pc);\n}\n\nstatic bool\nexpand_avx2_gather_extract_scalar_index(void *drcontext, instrlist_t *bb,\n                                        instr_t *sg_instr, int el,\n                                        scatter_gather_info_t *sg_info,\n                                        reg_id_t scratch_xmm, reg_id_t scratch_reg,\n                                        app_pc orig_app_pc)\n{\n    return expand_scatter_gather_extract_scalar(\n        drcontext, bb, sg_instr, el, sg_info, sg_info->scalar_index_size,\n        opnd_size_in_bytes(sg_info->scalar_index_size), sg_info->index_reg, scratch_xmm,\n        scratch_reg, false /* AVX2 */, orig_app_pc);\n}\n\nstatic bool\nexpand_avx512_scatter_gather_update_mask(void *drcontext, instrlist_t *bb,\n                                         instr_t *sg_instr, int el,\n                                         scatter_gather_info_t *sg_info,\n                                         reg_id_t scratch_reg, app_pc orig_app_pc,\n                                         drvector_t *allowed)\n{\n    reg_id_t save_mask_reg;\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_mov_imm(drcontext,\n                                          opnd_create_reg(IF_X64_ELSE(\n                                              reg_64_to_32(scratch_reg), scratch_reg)),\n                                          OPND_CREATE_INT32(1 << el)),\n                     orig_app_pc));\n    if (drreg_reserve_register(drcontext, bb, sg_instr, allowed, &save_mask_reg) !=\n        DRREG_SUCCESS)\n        return false;\n    /* The scratch k register we're using here is always k0, because it is never\n     * used for scatter/gather.\n     */\n    MINSERT(bb, sg_instr,\n            INSTR_CREATE_kmovw(\n                drcontext,\n                opnd_create_reg(IF_X64_ELSE(reg_64_to_32(save_mask_reg), save_mask_reg)),\n                opnd_create_reg(DR_REG_K0)));\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_kmovw(drcontext, opnd_create_reg(DR_REG_K0),\n                                        opnd_create_reg(IF_X64_ELSE(\n                                            reg_64_to_32(scratch_reg), scratch_reg))),\n                     orig_app_pc));\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_kandnw(drcontext, opnd_create_reg(sg_info->mask_reg),\n                                         opnd_create_reg(DR_REG_K0),\n                                         opnd_create_reg(sg_info->mask_reg)),\n                     orig_app_pc));\n    MINSERT(bb, sg_instr,\n            INSTR_CREATE_kmovw(drcontext, opnd_create_reg(DR_REG_K0),\n                               opnd_create_reg(IF_X64_ELSE(reg_64_to_32(save_mask_reg),\n                                                           save_mask_reg))));\n    if (drreg_unreserve_register(drcontext, bb, sg_instr, save_mask_reg) !=\n        DRREG_SUCCESS) {\n        ASSERT(false, \"drreg_unreserve_register should not fail\");\n        return false;\n    }\n    return true;\n}\n\nstatic bool\nexpand_avx2_gather_update_mask(void *drcontext, instrlist_t *bb, instr_t *sg_instr,\n                               int el, scatter_gather_info_t *sg_info,\n                               reg_id_t scratch_xmm, reg_id_t scratch_reg,\n                               app_pc orig_app_pc)\n{\n    /* The width of the mask element and data element is identical per definition of the\n     * instruction.\n     */\n    if (sg_info->scalar_value_size == OPSZ_4) {\n        PREXL8(\n            bb, sg_instr,\n            INSTR_XL8(\n                INSTR_CREATE_xor(\n                    drcontext,\n                    opnd_create_reg(IF_X64_ELSE(reg_64_to_32(scratch_reg), scratch_reg)),\n                    opnd_create_reg(IF_X64_ELSE(reg_64_to_32(scratch_reg), scratch_reg))),\n                orig_app_pc));\n    } else if (sg_info->scalar_value_size == OPSZ_8) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_xor(drcontext, opnd_create_reg(scratch_reg),\n                                          opnd_create_reg(scratch_reg)),\n                         orig_app_pc));\n    }\n    reg_id_t null_index_reg = scratch_reg;\n    if (!expand_avx2_gather_insert_scalar_mask(drcontext, bb, sg_instr, el, sg_info,\n                                               null_index_reg, scratch_xmm, orig_app_pc))\n        return false;\n    return true;\n}\n\nstatic bool\nexpand_avx2_gather_make_test(void *drcontext, instrlist_t *bb, instr_t *sg_instr, int el,\n                             scatter_gather_info_t *sg_info, reg_id_t scratch_xmm,\n                             reg_id_t scratch_reg, instr_t *skip_label,\n                             app_pc orig_app_pc)\n{\n    /* The width of the mask element and data element is identical per definition of the\n     * instruction.\n     */\n    expand_scatter_gather_extract_scalar(\n        drcontext, bb, sg_instr, el, sg_info, sg_info->scalar_value_size,\n        opnd_size_in_bytes(sg_info->scalar_value_size), sg_info->mask_reg, scratch_xmm,\n        scratch_reg, false /* AVX2 */, orig_app_pc);\n    if (sg_info->scalar_value_size == OPSZ_4) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_shr(drcontext,\n                                          opnd_create_reg(IF_X64_ELSE(\n                                              reg_64_to_32(scratch_reg), scratch_reg)),\n                                          OPND_CREATE_INT8(31)),\n                         orig_app_pc));\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_and(drcontext,\n                                          opnd_create_reg(IF_X64_ELSE(\n                                              reg_64_to_32(scratch_reg), scratch_reg)),\n                                          OPND_CREATE_INT32(1)),\n                         orig_app_pc));\n    } else if (sg_info->scalar_value_size == OPSZ_8) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_shr(drcontext, opnd_create_reg(scratch_reg),\n                                          OPND_CREATE_INT8(63)),\n                         orig_app_pc));\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_and(drcontext, opnd_create_reg(scratch_reg),\n                                          OPND_CREATE_INT32(1)),\n                         orig_app_pc));\n    }\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_jcc(drcontext, OP_jz, opnd_create_instr(skip_label)),\n                     orig_app_pc));\n    return true;\n}\n\nstatic bool\nexpand_avx512_scatter_gather_make_test(void *drcontext, instrlist_t *bb,\n                                       instr_t *sg_instr, int el,\n                                       scatter_gather_info_t *sg_info,\n                                       reg_id_t scratch_reg, instr_t *skip_label,\n                                       app_pc orig_app_pc)\n{\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_kmovw(drcontext,\n                                        opnd_create_reg(IF_X64_ELSE(\n                                            reg_64_to_32(scratch_reg), scratch_reg)),\n                                        opnd_create_reg(sg_info->mask_reg)),\n                     orig_app_pc));\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_test(drcontext,\n                                       opnd_create_reg(IF_X64_ELSE(\n                                           reg_64_to_32(scratch_reg), scratch_reg)),\n                                       OPND_CREATE_INT32(1 << el)),\n                     orig_app_pc));\n    PREXL8(bb, sg_instr,\n           INSTR_XL8(INSTR_CREATE_jcc(drcontext, OP_jz, opnd_create_instr(skip_label)),\n                     orig_app_pc));\n    return true;\n}\n\nstatic bool\nexpand_avx512_scatter_store_scalar_value(void *drcontext, instrlist_t *bb,\n                                         instr_t *sg_instr,\n                                         scatter_gather_info_t *sg_info,\n                                         reg_id_t scalar_index_reg,\n                                         reg_id_t scalar_value_reg, app_pc orig_app_pc)\n{\n    if (sg_info->base_reg == IF_X64_ELSE(DR_REG_RAX, DR_REG_EAX)) {\n        /* We need the app's base register value. If it's xax, then it may be used to\n         * store flags by drreg.\n         */\n        drreg_get_app_value(drcontext, bb, sg_instr, sg_info->base_reg,\n                            sg_info->base_reg);\n    }\n#    ifdef X64\n    if (sg_info->scalar_index_size == OPSZ_4) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(\n                   INSTR_CREATE_movsxd(drcontext, opnd_create_reg(scalar_index_reg),\n                                       opnd_create_reg(reg_64_to_32(scalar_index_reg))),\n                   orig_app_pc));\n    }\n#    endif\n    if (sg_info->scalar_value_size == OPSZ_4) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_mov_st(\n                             drcontext,\n                             opnd_create_base_disp(sg_info->base_reg, scalar_index_reg,\n                                                   sg_info->scale, sg_info->disp, OPSZ_4),\n                             opnd_create_reg(IF_X64_ELSE(reg_64_to_32(scalar_value_reg),\n                                                         scalar_value_reg))),\n                         orig_app_pc));\n    } else if (sg_info->scalar_value_size == OPSZ_8) {\n        ASSERT(reg_is_64bit(scalar_index_reg),\n               \"Internal error: scratch index register not 64-bit.\");\n        ASSERT(reg_is_64bit(scalar_value_reg),\n               \"Internal error: scratch value register not 64-bit.\");\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_mov_st(\n                             drcontext,\n                             opnd_create_base_disp(sg_info->base_reg, scalar_index_reg,\n                                                   sg_info->scale, sg_info->disp, OPSZ_8),\n                             opnd_create_reg(scalar_value_reg)),\n                         orig_app_pc));\n    } else {\n        ASSERT(false, \"Unexpected index size.\");\n        return false;\n    }\n    return true;\n}\n\nstatic bool\nexpand_gather_load_scalar_value(void *drcontext, instrlist_t *bb, instr_t *sg_instr,\n                                scatter_gather_info_t *sg_info, reg_id_t scalar_index_reg,\n                                app_pc orig_app_pc)\n{\n    if (sg_info->base_reg == IF_X64_ELSE(DR_REG_RAX, DR_REG_EAX)) {\n        /* We need the app's base register value. If it's xax, then it may be used to\n         * store flags by drreg.\n         */\n        drreg_get_app_value(drcontext, bb, sg_instr, sg_info->base_reg,\n                            sg_info->base_reg);\n    }\n#    ifdef X64\n    if (sg_info->scalar_index_size == OPSZ_4) {\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(\n                   INSTR_CREATE_movsxd(drcontext, opnd_create_reg(scalar_index_reg),\n                                       opnd_create_reg(reg_64_to_32(scalar_index_reg))),\n                   orig_app_pc));\n    }\n#    endif\n    if (sg_info->scalar_value_size == OPSZ_4) {\n        PREXL8(\n            bb, sg_instr,\n            INSTR_XL8(INSTR_CREATE_mov_ld(\n                          drcontext,\n                          opnd_create_reg(IF_X64_ELSE(reg_64_to_32(scalar_index_reg),\n                                                      scalar_index_reg)),\n                          opnd_create_base_disp(sg_info->base_reg, scalar_index_reg,\n                                                sg_info->scale, sg_info->disp, OPSZ_4)),\n                      orig_app_pc));\n    } else if (sg_info->scalar_value_size == OPSZ_8) {\n        ASSERT(reg_is_64bit(scalar_index_reg),\n               \"Internal error: scratch register not 64-bit.\");\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_mov_ld(drcontext, opnd_create_reg(scalar_index_reg),\n                                             opnd_create_base_disp(\n                                                 sg_info->base_reg, scalar_index_reg,\n                                                 sg_info->scale, sg_info->disp, OPSZ_8)),\n                         orig_app_pc));\n    } else {\n        ASSERT(false, \"Unexpected index size.\");\n        return false;\n    }\n    return true;\n}\n\nuint\nmov_xmm_aligned32_opcode()\n{\n    ASSERT(proc_avx_enabled(), \"running on unsupported processor\");\n    return OP_vmovdqa;\n}\n\n#endif\n\n/*****************************************************************************************\n * drx_expand_scatter_gather()\n *\n * The function expands scatter and gather instructions to a sequence of equivalent\n * scalar operations. Gather instructions are expanded into a sequence of mask register\n * bit tests, extracting the index value, a scalar load, inserting the scalar value into\n * the destination simd register, and mask register bit updates. Scatter instructions\n * are similarly expanded into a sequence, but deploy a scalar store. Registers spilled\n * and restored by drreg are not illustrated in the sequence below.\n *\n * ------------------------------------------------------------------------------\n * AVX2 vpgatherdd, vgatherdps, vpgatherdq, vgatherdpd, vpgatherqd, vgatherqps, |\n * vpgatherqq, vgatherqpd:                                                      |\n * ------------------------------------------------------------------------------\n *\n * vpgatherdd (%rax,%ymm1,4)[4byte] %ymm2 -> %ymm0 %ymm2 sequence laid out here,\n * others are similar:\n *\n * Extract mask dword. qword versions use vpextrq:\n *   vextracti128   %ymm2 $0x00 -> %xmm3\n *   vpextrd        %xmm3 $0x00 -> %ecx\n * Test mask bit:\n *   shr            $0x0000001f %ecx -> %ecx\n *   and            $0x00000001 %ecx -> %ecx\n * Skip element if mask not set:\n *   jz             <skip0>\n * Extract index dword. qword versions use vpextrq:\n *   vextracti128   %ymm1 $0x00 -> %xmm3\n *   vpextrd        %xmm3 $0x00 -> %ecx\n * Restore app's base register value (may not be present):\n *   mov            %rax -> %gs:0x00000090[8byte]\n *   mov            %gs:0x00000098[8byte] -> %rax\n * Load scalar value:\n *   mov            (%rax,%rcx,4)[4byte] -> %ecx\n * Insert scalar value in destination register:\n *   vextracti128   %ymm0 $0x00 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x00 -> %xmm3\n *   vinserti128    %ymm0 %xmm3 $0x00 -> %ymm0\n * Set mask dword to zero:\n *   xor            %ecx %ecx -> %ecx\n *   vextracti128   %ymm2 $0x00 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x00 -> %xmm3\n *   vinserti128    %ymm2 %xmm3 $0x00 -> %ymm2\n *   skip0:\n * Do the same as above for the next element:\n *   vextracti128   %ymm2 $0x00 -> %xmm3\n *   vpextrd        %xmm3 $0x01 -> %ecx\n *   shr            $0x0000001f %ecx -> %ecx\n *   and            $0x00000001 %ecx -> %ecx\n *   jz             <skip1>\n *   vextracti128   %ymm1 $0x00 -> %xmm3\n *   vpextrd        %xmm3 $0x01 -> %ecx\n *   mov            (%rax,%rcx,4)[4byte] -> %ecx\n *   vextracti128   %ymm0 $0x00 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x01 -> %xmm3\n *   vinserti128    %ymm0 %xmm3 $0x00 -> %ymm0\n *   xor            %ecx %ecx -> %ecx\n *   vextracti128   %ymm2 $0x00 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x01 -> %xmm3\n *   vinserti128    %ymm2 %xmm3 $0x00 -> %ymm2\n *   skip1:\n *   [..]\n * Do the same as above for the last element:\n *   vextracti128   %ymm2 $0x01 -> %xmm3\n *   vpextrd        %xmm3 $0x03 -> %ecx\n *   shr            $0x0000001f %ecx -> %ecx\n *   and            $0x00000001 %ecx -> %ecx\n *   jz             <skip7>\n *   vextracti128   %ymm1 $0x01 -> %xmm3\n *   vpextrd        %xmm3 $0x03 -> %ecx\n *   mov            (%rax,%rcx,4)[4byte] -> %ecx\n *   vextracti128   %ymm0 $0x01 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x03 -> %xmm3\n *   vinserti128    %ymm0 %xmm3 $0x01 -> %ymm0\n *   xor            %ecx %ecx -> %ecx\n *   vextracti128   %ymm2 $0x01 -> %xmm3\n *   vpinsrd        %xmm3 %ecx $0x03 -> %xmm3\n *   vinserti128    %ymm2 %xmm3 $0x01 -> %ymm2\n *   skip7:\n * Finally, clear the entire mask register, even\n * the parts that are not used as a mask:\n *   vpxor          %ymm2 %ymm2 -> %ymm2\n *\n * ---------------------------------------------------------------------------------\n * AVX-512 vpgatherdd, vgatherdps, vpgatherdq, vgatherdpd, vpgatherqd, vgatherqps, |\n * vpgatherqq, vgatherqpd:                                                         |\n * ---------------------------------------------------------------------------------\n *\n * vpgatherdd {%k1} (%rax,%zmm1,4)[4byte] -> %zmm0 %k1 sequence laid out here,\n * others are similar:\n *\n * Extract mask bit:\n *   kmovw           %k1 -> %ecx\n * Test mask bit:\n *   test            %ecx $0x00000001\n * Skip element if mask not set:\n *   jz              <skip0>\n * Extract index dword. qword versions use vpextrq:\n *   vextracti32x4   {%k0} $0x00 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x00 -> %ecx\n * Restore app's base register value (may not be present):\n *   mov             %rax -> %gs:0x00000090[8byte]\n *   mov             %gs:0x00000098[8byte] -> %rax\n * Load scalar value:\n *   mov             (%rax,%rcx,4)[4byte] -> %ecx\n * Insert scalar value in destination register:\n *   vextracti32x4   {%k0} $0x00 %zmm0 -> %xmm2\n *   vpinsrd         %xmm2 %ecx $0x00 -> %xmm2\n *   vinserti32x4    {%k0} $0x00 %zmm0 %xmm2 -> %zmm0\n * Set mask bit to zero:\n *   mov             $0x00000001 -> %ecx\n * %k0 is saved to a gpr here, while the gpr\n * is managed by drreg. This is not further\n * layed out in this example.\n *   kmovw           %ecx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n * It is not illustrated that %k0 is restored here.\n *   skip0:\n * Do the same as above for the next element:\n *   kmovw           %k1 -> %ecx\n *   test            %ecx $0x00000002\n *   jz              <skip1>\n *   vextracti32x4   {%k0} $0x00 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x01 -> %ecx\n *   mov             (%rax,%rcx,4)[4byte] -> %ecx\n *   vextracti32x4   {%k0} $0x00 %zmm0 -> %xmm2\n *   vpinsrd         %xmm2 %ecx $0x01 -> %xmm2\n *   vinserti32x4    {%k0} $0x00 %zmm0 %xmm2 -> %zmm0\n *   mov             $0x00000002 -> %ecx\n *   kmovw           %ecx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n *   skip1:\n *   [..]\n * Do the same as above for the last element:\n *   kmovw           %k1 -> %ecx\n *   test            %ecx $0x00008000\n *   jz              <skip15>\n *   vextracti32x4   {%k0} $0x03 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x03 -> %ecx\n *   mov             (%rax,%rcx,4)[4byte] -> %ecx\n *   vextracti32x4   {%k0} $0x03 %zmm0 -> %xmm2\n *   vpinsrd         %xmm2 %ecx $0x03 -> %xmm2\n *   vinserti32x4    {%k0} $0x03 %zmm0 %xmm2 -> %zmm0\n *   mov             $0x00008000 -> %ecx\n *   kmovw           %ecx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n *   skip15:\n * Finally, clear the entire mask register, even\n * the parts that are not used as a mask:\n *   kxorq           %k1 %k1 -> %k1\n *\n * --------------------------------------------------------------------------\n * AVX-512 vpscatterdd, vscatterdps, vpscatterdq, vscatterdpd, vpscatterqd, |\n * vscatterqps, vpscatterqq, vscatterqpd:                                   |\n * --------------------------------------------------------------------------\n *\n * vpscatterdd {%k1} %zmm0 -> (%rcx,%zmm1,4)[4byte] %k1 sequence laid out here,\n * others are similar:\n *\n * Extract mask bit:\n *   kmovw           %k1 -> %edx\n * Test mask bit:\n *   test            %edx $0x00000001\n * Skip element if mask not set:\n *   jz              <skip0>\n * Extract index dword. qword versions use vpextrq:\n *   vextracti32x4   {%k0} $0x00 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x00 -> %edx\n * Extract scalar value dword. qword versions use vpextrq:\n *   vextracti32x4   {%k0} $0x00 %zmm0 -> %xmm2\n *   vpextrd         %xmm2 $0x00 -> %ebx\n * Store scalar value:\n *   mov             %ebx -> (%rcx,%rdx,4)[4byte]\n * Set mask bit to zero:\n *   mov             $0x00000001 -> %edx\n *   kmovw           %edx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n *   skip0:\n * Do the same as above for the next element:\n *   kmovw           %k1 -> %edx\n *   test            %edx $0x00000002\n *   jz              <skip1>\n *   vextracti32x4   {%k0} $0x00 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x01 -> %edx\n *   vextracti32x4   {%k0} $0x00 %zmm0 -> %xmm2\n *   vpextrd         %xmm2 $0x01 -> %ebx\n *   mov             %ebx -> (%rcx,%rdx,4)[4byte]\n *   mov             $0x00000002 -> %edx\n *   kmovw           %edx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n *   skip1:\n *   [..]\n * Do the same as above for the last element:\n *   kmovw           %k1 -> %edx\n *   test            %edx $0x00008000\n *   jz              <skip15>\n *   vextracti32x4   {%k0} $0x03 %zmm1 -> %xmm2\n *   vpextrd         %xmm2 $0x03 -> %edx\n *   vextracti32x4   {%k0} $0x03 %zmm0 -> %xmm2\n *   vpextrd         %xmm2 $0x03 -> %ebx\n *   mov             %ebx -> (%rcx,%rdx,4)[4byte]\n *   mov             $0x00008000 -> %edx\n *   kmovw           %edx -> %k0\n *   kandnw          %k0 %k1 -> %k1\n *   skip15:\n * Finally, clear the entire mask register, even\n * the parts that are not used as a mask:\n *   kxorq           %k1 %k1 -> %k1\n */\nbool\ndrx_expand_scatter_gather(void *drcontext, instrlist_t *bb, OUT bool *expanded)\n{\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n    instr_t *instr, *next_instr, *first_app = NULL;\n    bool delete_rest = false;\n#endif\n    if (expanded != NULL)\n        *expanded = false;\n    if (drmgr_current_bb_phase(drcontext) != DRMGR_PHASE_APP2APP) {\n        return false;\n    }\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n    /* Make each scatter or gather instruction be in their own basic block.\n     * TODO i#3837: cross-platform code like the following bb splitting can be shared\n     * with other architectures in the future.\n     */\n    for (instr = instrlist_first(bb); instr != NULL; instr = next_instr) {\n        next_instr = instr_get_next(instr);\n        if (delete_rest) {\n            instrlist_remove(bb, instr);\n            instr_destroy(drcontext, instr);\n        } else if (instr_is_app(instr)) {\n            if (first_app == NULL)\n                first_app = instr;\n            if (instr_is_gather(instr) || instr_is_scatter(instr)) {\n                delete_rest = true;\n                if (instr != first_app) {\n                    instrlist_remove(bb, instr);\n                    instr_destroy(drcontext, instr);\n                }\n            }\n        }\n    }\n    if (first_app == NULL)\n        return true;\n    if (!instr_is_gather(first_app) && !instr_is_scatter(first_app))\n        return true;\n\n    /* We want to avoid spill slot conflicts with later instrumentation passes. */\n    drreg_status_t res_bb_props =\n        drreg_set_bb_properties(drcontext, DRREG_HANDLE_MULTI_PHASE_SLOT_RESERVATIONS);\n    DR_ASSERT(res_bb_props == DRREG_SUCCESS);\n\n    dr_atomic_store32(&drx_scatter_gather_expanded, 1);\n\n    instr_t *sg_instr = first_app;\n    scatter_gather_info_t sg_info;\n    bool res = false;\n    /* XXX: we may want to make this function public, as it may be useful to clients. */\n    get_scatter_gather_info(sg_instr, &sg_info);\n#    ifndef X64\n    if (sg_info.scalar_index_size == OPSZ_8 || sg_info.scalar_value_size == OPSZ_8) {\n        /* FIXME i#2985: we do not yet support expansion of the qword index and value\n         * scatter/gather versions in 32-bit mode.\n         */\n        return false;\n    }\n#    endif\n    uint no_of_elements = opnd_size_in_bytes(sg_info.scatter_gather_size) /\n        MAX(opnd_size_in_bytes(sg_info.scalar_index_size),\n            opnd_size_in_bytes(sg_info.scalar_value_size));\n    reg_id_t scratch_reg0 = DR_REG_INVALID, scratch_reg1 = DR_REG_INVALID;\n    drvector_t allowed;\n    drreg_init_and_fill_vector(&allowed, true);\n    /* We need the scratch registers and base register app's value to be available at the\n     * same time. Do not use.\n     */\n    drreg_set_vector_entry(&allowed, sg_info.base_reg, false);\n    if (drreg_reserve_aflags(drcontext, bb, sg_instr) != DRREG_SUCCESS)\n        goto drx_expand_scatter_gather_exit;\n    if (drreg_reserve_register(drcontext, bb, sg_instr, &allowed, &scratch_reg0) !=\n        DRREG_SUCCESS)\n        goto drx_expand_scatter_gather_exit;\n    if (instr_is_scatter(sg_instr)) {\n        if (drreg_reserve_register(drcontext, bb, sg_instr, &allowed, &scratch_reg1) !=\n            DRREG_SUCCESS)\n            goto drx_expand_scatter_gather_exit;\n    }\n    app_pc orig_app_pc = instr_get_app_pc(sg_instr);\n    reg_id_t scratch_xmm;\n    /* Search the instruction for an unused xmm register we will use as a temp.\n     * Modify scatter-gather tests if the criteria for picking the scratch xmm changes.\n     */\n    for (scratch_xmm = DR_REG_START_XMM; scratch_xmm <= DR_REG_STOP_XMM; ++scratch_xmm) {\n        if ((sg_info.is_evex ||\n             scratch_xmm != reg_resize_to_opsz(sg_info.mask_reg, OPSZ_16)) &&\n            scratch_xmm != reg_resize_to_opsz(sg_info.index_reg, OPSZ_16) &&\n            /* redundant with scatter_src_reg */\n            scratch_xmm != reg_resize_to_opsz(sg_info.gather_dst_reg, OPSZ_16))\n            break;\n    }\n    /* Spill the scratch xmm.\n     * TODO i#3844: drreg does not support spilling xmm regs yet, so we do it ourselves.\n     * When that support is available, replace the following with the required drreg API\n     * calls.\n     */\n    per_thread_t *pt = get_tls_data(drcontext);\n    instrlist_insert_mov_immed_ptrsz(\n        drcontext, ALIGN_FORWARD(pt->scratch_xmm_spill_slot, XMM_ALIGNMENT),\n        opnd_create_reg(scratch_reg0), bb, sg_instr, NULL, NULL);\n    uint mov_xmm_opcode = mov_xmm_aligned32_opcode();\n    instrlist_meta_preinsert(\n        bb, sg_instr,\n        instr_create_1dst_1src(\n            drcontext, mov_xmm_opcode,\n            opnd_create_base_disp(scratch_reg0, DR_REG_NULL, 0, 0, OPSZ_16),\n            opnd_create_reg(scratch_xmm)));\n    emulated_instr_t emulated_instr;\n    emulated_instr.size = sizeof(emulated_instr);\n    emulated_instr.pc = instr_get_app_pc(sg_instr);\n    emulated_instr.instr = sg_instr;\n    /* Tools should instrument the data operations in the sequence. */\n    emulated_instr.flags = DR_EMULATE_INSTR_ONLY;\n    drmgr_insert_emulation_start(drcontext, bb, sg_instr, &emulated_instr);\n\n    if (sg_info.is_evex) {\n        if (/* AVX-512 */ instr_is_gather(sg_instr)) {\n            for (uint el = 0; el < no_of_elements; ++el) {\n                instr_t *skip_label = INSTR_CREATE_label(drcontext);\n                if (!expand_avx512_scatter_gather_make_test(drcontext, bb, sg_instr, el,\n                                                            &sg_info, scratch_reg0,\n                                                            skip_label, orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                if (!expand_avx512_scatter_gather_extract_scalar_index(\n                        drcontext, bb, sg_instr, el, &sg_info, scratch_xmm, scratch_reg0,\n                        orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                reg_id_t scalar_index_reg = scratch_reg0;\n                if (!expand_gather_load_scalar_value(drcontext, bb, sg_instr, &sg_info,\n                                                     scalar_index_reg, orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                reg_id_t scalar_value_reg = scratch_reg0;\n                if (!expand_avx512_gather_insert_scalar_value(drcontext, bb, sg_instr, el,\n                                                              &sg_info, scalar_value_reg,\n                                                              scratch_xmm, orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                if (!expand_avx512_scatter_gather_update_mask(drcontext, bb, sg_instr, el,\n                                                              &sg_info, scratch_reg0,\n                                                              orig_app_pc, &allowed))\n                    goto drx_expand_scatter_gather_exit;\n                MINSERT(bb, sg_instr, skip_label);\n            }\n        } else /* AVX-512 instr_is_scatter(sg_instr) */ {\n            for (uint el = 0; el < no_of_elements; ++el) {\n                instr_t *skip_label = INSTR_CREATE_label(drcontext);\n                expand_avx512_scatter_gather_make_test(drcontext, bb, sg_instr, el,\n                                                       &sg_info, scratch_reg0, skip_label,\n                                                       orig_app_pc);\n                if (!expand_avx512_scatter_gather_extract_scalar_index(\n                        drcontext, bb, sg_instr, el, &sg_info, scratch_xmm, scratch_reg0,\n                        orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                reg_id_t scalar_index_reg = scratch_reg0;\n                reg_id_t scalar_value_reg = scratch_reg1;\n                if (!expand_avx512_scatter_extract_scalar_value(\n                        drcontext, bb, sg_instr, el, &sg_info, scratch_xmm,\n                        scalar_value_reg, orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                if (!expand_avx512_scatter_store_scalar_value(\n                        drcontext, bb, sg_instr, &sg_info, scalar_index_reg,\n                        scalar_value_reg, orig_app_pc))\n                    goto drx_expand_scatter_gather_exit;\n                if (!expand_avx512_scatter_gather_update_mask(drcontext, bb, sg_instr, el,\n                                                              &sg_info, scratch_reg0,\n                                                              orig_app_pc, &allowed))\n                    goto drx_expand_scatter_gather_exit;\n                MINSERT(bb, sg_instr, skip_label);\n            }\n        }\n        /* The mask register is zeroed completely when instruction finishes. */\n        if (proc_has_feature(FEATURE_AVX512BW)) {\n            PREXL8(\n                bb, sg_instr,\n                INSTR_XL8(INSTR_CREATE_kxorq(drcontext, opnd_create_reg(sg_info.mask_reg),\n                                             opnd_create_reg(sg_info.mask_reg),\n                                             opnd_create_reg(sg_info.mask_reg)),\n                          orig_app_pc));\n        } else {\n            PREXL8(\n                bb, sg_instr,\n                INSTR_XL8(INSTR_CREATE_kxorw(drcontext, opnd_create_reg(sg_info.mask_reg),\n                                             opnd_create_reg(sg_info.mask_reg),\n                                             opnd_create_reg(sg_info.mask_reg)),\n                          orig_app_pc));\n        }\n    } else {\n        /* AVX2 instr_is_gather(sg_instr) */\n        for (uint el = 0; el < no_of_elements; ++el) {\n            instr_t *skip_label = INSTR_CREATE_label(drcontext);\n            if (!expand_avx2_gather_make_test(drcontext, bb, sg_instr, el, &sg_info,\n                                              scratch_xmm, scratch_reg0, skip_label,\n                                              orig_app_pc))\n                goto drx_expand_scatter_gather_exit;\n            if (!expand_avx2_gather_extract_scalar_index(drcontext, bb, sg_instr, el,\n                                                         &sg_info, scratch_xmm,\n                                                         scratch_reg0, orig_app_pc))\n                goto drx_expand_scatter_gather_exit;\n            reg_id_t scalar_index_reg = scratch_reg0;\n            if (!expand_gather_load_scalar_value(drcontext, bb, sg_instr, &sg_info,\n                                                 scalar_index_reg, orig_app_pc))\n                goto drx_expand_scatter_gather_exit;\n            reg_id_t scalar_value_reg = scratch_reg0;\n            if (!expand_avx2_gather_insert_scalar_value(drcontext, bb, sg_instr, el,\n                                                        &sg_info, scalar_value_reg,\n                                                        scratch_xmm, orig_app_pc))\n                goto drx_expand_scatter_gather_exit;\n            if (!expand_avx2_gather_update_mask(drcontext, bb, sg_instr, el, &sg_info,\n                                                scratch_xmm, scratch_reg0, orig_app_pc))\n                goto drx_expand_scatter_gather_exit;\n            MINSERT(bb, sg_instr, skip_label);\n        }\n        /* The mask register is zeroed completely when instruction finishes. */\n        PREXL8(bb, sg_instr,\n               INSTR_XL8(INSTR_CREATE_vpxor(drcontext, opnd_create_reg(sg_info.mask_reg),\n                                            opnd_create_reg(sg_info.mask_reg),\n                                            opnd_create_reg(sg_info.mask_reg)),\n                         orig_app_pc));\n    }\n    /* Restore the scratch xmm. */\n    instrlist_insert_mov_immed_ptrsz(\n        drcontext, ALIGN_FORWARD(pt->scratch_xmm_spill_slot, XMM_ALIGNMENT),\n        opnd_create_reg(scratch_reg0), bb, sg_instr, NULL, NULL);\n    instrlist_meta_preinsert(\n        bb, sg_instr,\n        instr_create_1dst_1src(\n            drcontext, mov_xmm_opcode, opnd_create_reg(scratch_xmm),\n            opnd_create_base_disp(scratch_reg0, DR_REG_NULL, 0, 0, OPSZ_16)));\n    ASSERT(scratch_reg0 != scratch_reg1,\n           \"Internal error: scratch registers must be different\");\n    if (drreg_unreserve_register(drcontext, bb, sg_instr, scratch_reg0) !=\n        DRREG_SUCCESS) {\n        ASSERT(false, \"drreg_unreserve_register should not fail\");\n        goto drx_expand_scatter_gather_exit;\n    }\n    if (instr_is_scatter(sg_instr)) {\n        if (drreg_unreserve_register(drcontext, bb, sg_instr, scratch_reg1) !=\n            DRREG_SUCCESS) {\n            ASSERT(false, \"drreg_unreserve_register should not fail\");\n            goto drx_expand_scatter_gather_exit;\n        }\n    }\n    if (drreg_unreserve_aflags(drcontext, bb, sg_instr) != DRREG_SUCCESS)\n        goto drx_expand_scatter_gather_exit;\n#    if VERBOSE\n    dr_print_instr(drcontext, STDERR, sg_instr, \"\\tThe instruction\\n\");\n#    endif\n\n    drmgr_insert_emulation_end(drcontext, bb, sg_instr);\n    /* Remove and destroy the original scatter/gather. */\n    instrlist_remove(bb, sg_instr);\n#    if VERBOSE\n    dr_fprintf(STDERR, \"\\twas expanded to the following sequence:\\n\");\n    for (instr = instrlist_first(bb); instr != NULL; instr = instr_get_next(instr)) {\n        dr_print_instr(drcontext, STDERR, instr, \"\");\n    }\n#    endif\n\n    if (expanded != NULL)\n        *expanded = true;\n    res = true;\n\ndrx_expand_scatter_gather_exit:\n    drvector_delete(&allowed);\n    return res;\n\n#else /* !PLATFORM_SUPPORTS_SCATTER_GATHER */\n    /* TODO i#3837: add support for AArch64. */\n    if (expanded != NULL)\n        *expanded = false;\n    return true;\n#endif\n}\n\n/***************************************************************************\n * RESTORE STATE\n */\n\n#ifdef PLATFORM_SUPPORTS_SCATTER_GATHER\n\n/*\n * x86 scatter/gather emulation sequence support\n *\n * The following state machines exist in order to detect restore events that need\n * additional attention by drx in order to fix the application state on top of the\n * fixes that drreg already makes. For the AVX-512 scatter/gather sequences these are\n * instruction windows where a scratch mask is being used, and the windows after\n * each scalar load/store but before the destination mask register update. For AVX2,\n * the scratch mask is an xmm register and will be handled by drreg directly (future\n * update, xref #3844).\n *\n * The state machines allow for instructions like drreg spill/restore and instrumentation\n * in between recognized states. This is an approximation and could be broken in many\n * ways, e.g. by a client adding more than DRX_RESTORE_EVENT_SKIP_UNKNOWN_INSTR_MAX\n * number of instructions as instrumentation, or by altering the emulation sequence's\n * code.\n * TODO i#5005: A more safe way to do this would be along the lines of xref i#3801: if\n * we had instruction lists available, we could see and pass down emulation labels\n * instead of guessing the sequence based on decoding the code cache.\n *\n * AVX-512 gather sequence detection example:\n *\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_0\n *         mov           <xmm_spill_addr> -> %ecx\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1\n *         vmovdqa       %xmm2 -> (%ecx)[16byte]\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2\n *         vextracti32x4 {%k0} $0x00 %zmm1 -> %xmm2\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_3\n *         vpextrd       %xmm2 $0x00 -> %ecx\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4\n *         mov           (%rax,%rcx,4)[4byte] -> %ecx\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5\n * (a)     vextracti32x4 {%k0} $0x00 %zmm0 -> %xmm2\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6\n * (a)     vpinsrd       %xmm2 %ecx $0x00 -> %xmm2\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_7\n * (a)     vinserti32x4  {%k0} $0x00 %zmm0 %xmm2 -> %zmm0\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_8\n * (a)     mov           $0x00000001 -> %ecx\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_9\n * (a)     kmovw         %k0 -> %edx\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_10\n * (a)     kmovw         %ecx -> %k0\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_11\n * (a) (b) kandnw        %k0 %k1 -> %k1\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_12\n *     (b) kmovw         %edx -> %k0\n *         DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2\n *\n * (a): The instruction window where the destination mask state hadn't been updated yet.\n * (b): The instruction window where the scratch mask is clobbered w/o support by drreg.\n *\n * AVX-512 scatter sequence detection example:\n *\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_0\n *         mov           <xmm_spill_addr> -> %edx\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_1\n *         vmovdqa       %xmm2 -> (%edx)[16byte]\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2\n *         vextracti32x4 {%k0} $0x00 %zmm1 -> %xmm2\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_3\n *         vpextrd       %xmm2 $0x00 -> %edx\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_4\n *         vextracti32x4 {%k0} $0x00 %zmm0 -> %xmm2\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_5\n *         vpextrd       %xmm2 $0x00 -> %ebx\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_6\n *         mov           %ebx -> (%rcx,%rdx,4)[4byte]\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_7\n * (a)     mov           $0x00000001 -> %edx\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_8\n * (a)     kmovw         %k0 -> %ebp\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_9\n * (a)     kmovw         %edx -> %k0\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_10\n * (a) (b) kandnw        %k0 %k1 -> %k1\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_11\n *     (b) kmovw         %ebp -> %k0\n *         DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2\n *\n * (a): The instruction window where the destination mask state hadn't been updated yet.\n * (b): The instruction window where the scratch mask is clobbered w/o support by drreg.\n *\n * AVX2 gather sequence detection example:\n *\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_0\n *         mov           <xmm_spill_addr> -> %ecx\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_1\n *         vmovdqa       %xmm3 -> (%ecx)[16byte]\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2\n *         vextracti128  %ymm2 $0x00 -> %xmm3\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_3\n *         vpextrd       %xmm3 $0x00 -> %ecx\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_4\n *         mov           (%rax,%rcx,4)[4byte] -> %ecx\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_5\n * (a)     vextracti128  %ymm0 $0x00 -> %xmm3\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_6\n * (a)     vpinsrd       %xmm3 %ecx $0x00 -> %xmm3\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_7\n * (a)     vinserti128   %ymm0 %xmm3 $0x00 -> %ymm0\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_8\n * (a)     xor           %ecx %ecx -> %ecx\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_9\n * (a)     vextracti128  %ymm2 $0x00 -> %xmm3\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_10\n * (a)     vpinsrd       %xmm3 %ecx $0x00 -> %xmm3\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_11\n * (a)     vinserti128   %ymm2 %xmm3 $0x00 -> %ymm2\n *         DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2\n *\n * (a): The instruction window where the destination mask state hadn't been updated yet.\n *\n */\n\n#    define DRX_RESTORE_EVENT_SKIP_UNKNOWN_INSTR_MAX 32\n\n/* States of the AVX-512 gather detection state machine. */\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_0 0\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1 1\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2 2\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_3 3\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4 4\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5 5\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6 6\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_7 7\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_8 8\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_9 9\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_10 10\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_11 11\n#    define DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_12 12\n\n/* States of the AVX-512 scatter detection state machine. */\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_0 0\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_1 1\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2 2\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_3 3\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_4 4\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_5 5\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_6 6\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_7 7\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_8 8\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_9 9\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_10 10\n#    define DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_11 11\n\n/* States of the AVX2 gather detection state machine. */\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_0 0\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_1 1\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2 2\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_3 3\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_4 4\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_5 5\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_6 6\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_7 7\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_8 8\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_9 9\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_10 10\n#    define DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_11 11\n\ntypedef struct _drx_state_machine_params_t {\n    byte *pc;\n    byte *prev_pc;\n    /* state machine's state */\n    int detect_state;\n    /* detected start pc of destination mask update */\n    byte *restore_dest_mask_start_pc;\n    /* detected start pc of scratch mask usage */\n    byte *restore_scratch_mask_start_pc;\n    /* counter to allow for skipping unknown instructions */\n    int skip_unknown_instr_count;\n    /* The spilled xmm register. When the_scratch_xmm is set,\n     * it is expected to be equal to this.\n     */\n    reg_id_t spilled_xmm;\n    /* The address where the spilled xmm reg's app value is\n     * stored.\n     */\n    void *spilled_xmm_slot_addr;\n    /* The reg with spilled_xmm_slot_addr, so that we can use it\n     * in the actual spill instr.\n     */\n    reg_id_t spilled_xmm_slot_addr_reg;\n    /* detected scratch xmm register for mask update */\n    reg_id_t the_scratch_xmm;\n    /* detected gpr register that holds the mask update immediate */\n    reg_id_t gpr_bit_mask;\n    /* detected gpr register that holds the app's mask state */\n    reg_id_t gpr_save_scratch_mask;\n    /* counter of scalar element in the scatter/gather sequence */\n    uint scalar_mask_update_no;\n    /* temporary scratch gpr for the AVX-512 scatter value */\n    reg_id_t gpr_scratch_index;\n    /* temporary scratch gpr for the AVX-512 scatter index */\n    reg_id_t gpr_scratch_value;\n    instr_t inst;\n    dr_restore_state_info_t *info;\n    scatter_gather_info_t *sg_info;\n} drx_state_machine_params_t;\n\nstatic void\nadvance_state(int new_detect_state, drx_state_machine_params_t *params)\n{\n    params->detect_state = new_detect_state;\n    params->skip_unknown_instr_count = 0;\n}\n\n/* Advances to state 0 if counter has exceeded threshold, returns otherwise. */\nstatic inline void\nskip_unknown_instr_inc(int reset_state, drx_state_machine_params_t *params)\n{\n    if (params->skip_unknown_instr_count++ >= DRX_RESTORE_EVENT_SKIP_UNKNOWN_INSTR_MAX) {\n        advance_state(reset_state, params);\n    }\n}\n\nstatic void\nrestore_spilled_xmm_value(drx_state_machine_params_t *params)\n{\n    byte xmm_val[XMM_REG_SIZE];\n    ASSERT(params->spilled_xmm_slot_addr != NULL,\n           \"No spill address recorded for the app xmm value\");\n    ASSERT(params->spilled_xmm != DR_REG_NULL && reg_is_strictly_xmm(params->spilled_xmm),\n           \"No spilled xmm reg recorded\");\n    memcpy(xmm_val, params->spilled_xmm_slot_addr, XMM_REG_SIZE);\n    reg_set_value_ex(params->spilled_xmm, params->info->mcontext, xmm_val);\n}\n\n/* Run the state machines and decode the code cache. The state machines will search the\n * code for whether the translation pc is in one of the instruction windows that need\n * additional handling by drx in order to restore specific state of the application's mask\n * registers. We consider this sufficiently accurate, but this is still an approximation.\n */\nstatic bool\ndrx_restore_state_scatter_gather(\n    void *drcontext, dr_restore_state_info_t *info, scatter_gather_info_t *sg_info,\n    bool (*state_machine_func)(void *drcontext, drx_state_machine_params_t *params))\n{\n    drx_state_machine_params_t params;\n    params.restore_dest_mask_start_pc = NULL;\n    params.restore_scratch_mask_start_pc = NULL;\n    params.detect_state = 0;\n    params.skip_unknown_instr_count = 0;\n    params.the_scratch_xmm = DR_REG_NULL;\n    params.spilled_xmm = DR_REG_NULL;\n    params.spilled_xmm_slot_addr = NULL;\n    params.spilled_xmm_slot_addr_reg = DR_REG_NULL;\n    params.gpr_bit_mask = DR_REG_NULL;\n    params.gpr_save_scratch_mask = DR_REG_NULL;\n    params.scalar_mask_update_no = 0;\n    params.info = info;\n    params.sg_info = sg_info;\n    params.pc = params.info->fragment_info.cache_start_pc;\n    instr_init(drcontext, &params.inst);\n    /* As the state machine is looking for blocks of code that the fault may hit, the 128\n     * bytes is a conservative approximation of the block's size, see (a) and (b) above.\n     */\n    while (params.pc <= params.info->raw_mcontext->pc + 128) {\n        instr_reset(drcontext, &params.inst);\n        params.prev_pc = params.pc;\n        params.pc = decode(drcontext, params.pc, &params.inst);\n        if (params.pc == NULL) {\n            /* Upon a decoding error we simply give up. */\n            break;\n        }\n        /* If there is a gather or scatter instruction in the code cache, then it is wise\n         * to assume that this is not an emulated sequence that we need to examine\n         * further.\n         */\n        if (instr_is_gather(&params.inst))\n            break;\n        if (instr_is_scatter(&params.inst))\n            break;\n        if ((*state_machine_func)(drcontext, &params))\n            break;\n    }\n    instr_free(drcontext, &params.inst);\n    return true;\n}\n\n/* Returns true if done, false otherwise. */\nstatic bool\ndrx_avx2_gather_sequence_state_machine(void *drcontext,\n                                       drx_state_machine_params_t *params)\n{\n    switch (params->detect_state) {\n    /* First we detect the spill address for the scratch xmm reg. */\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_0: {\n        params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n        params->spilled_xmm_slot_addr = NULL;\n        ptr_int_t spilled_xmm_slot_addr;\n        if (instr_is_mov_constant(&params->inst, &spilled_xmm_slot_addr) &&\n            opnd_is_reg(instr_get_dst(&params->inst, 0)) &&\n            reg_is_gpr(opnd_get_reg(instr_get_dst(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr = (void *)spilled_xmm_slot_addr;\n            params->spilled_xmm_slot_addr_reg =\n                opnd_get_reg(instr_get_dst(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_1, params);\n        }\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_1:\n        ASSERT(params->spilled_xmm == DR_REG_NULL,\n               \"Spilled xmm reg must be undetermined yet\");\n        ASSERT(params->spilled_xmm_slot_addr != NULL &&\n                   params->spilled_xmm_slot_addr_reg != DR_REG_NULL,\n               \"xmm spill address must be determined already\");\n        if (instr_get_opcode(&params->inst) == OP_vmovdqa &&\n            opnd_is_base_disp(instr_get_dst(&params->inst, 0)) &&\n            opnd_get_base(instr_get_dst(&params->inst, 0)) ==\n                params->spilled_xmm_slot_addr_reg &&\n            opnd_is_reg(instr_get_src(&params->inst, 0)) &&\n            reg_is_strictly_xmm(opnd_get_reg(instr_get_src(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n            params->spilled_xmm = opnd_get_reg(instr_get_src(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n            break;\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_0, params);\n        break;\n    /* We come back to DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2 for each\n     * scalar load sequence of the expanded gather instr.\n     */\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2:\n        if (instr_get_opcode(&params->inst) == OP_vextracti128) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_3, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_0, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_3:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_index_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpextrd) ||\n            (params->sg_info->scalar_index_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpextrq)) {\n            ASSERT(opnd_is_reg(instr_get_src(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_src(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                    params->the_scratch_xmm = DR_REG_NULL;\n                    params->gpr_scratch_index = opnd_get_reg(dst0);\n                    advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_4, params);\n                    break;\n                }\n            }\n        }\n        /* Intentionally not else if */\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_4:\n        if (!instr_is_reg_spill_or_restore(drcontext, &params->inst, NULL, NULL, NULL,\n                                           NULL)) {\n            if (instr_reads_memory(&params->inst)) {\n                opnd_t src0 = instr_get_src(&params->inst, 0);\n                if (opnd_is_memory_reference(src0)) {\n                    if (opnd_uses_reg(src0, params->gpr_scratch_index)) {\n                        opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                        if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                            params->restore_dest_mask_start_pc = params->pc;\n                            advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_5,\n                                          params);\n                            break;\n                        }\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_5:\n        if (instr_get_opcode(&params->inst) == OP_vextracti128) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_6, params);\n                break;\n            }\n        }\n        /* Intentionally not else if */\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_6:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_value_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrd) ||\n            (params->sg_info->scalar_value_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrq)) {\n            ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_dst(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                params->the_scratch_xmm = DR_REG_NULL;\n                advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_7, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_7:\n        if (instr_get_opcode(&params->inst) == OP_vinserti128) {\n            ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_dst(&params->inst, 0));\n            if (tmp_reg == params->sg_info->gather_dst_reg) {\n                advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_8, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_8:\n        if (instr_get_opcode(&params->inst) == OP_xor) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            opnd_t src1 = instr_get_src(&params->inst, 1);\n            if (opnd_is_reg(dst0) && opnd_is_reg(src0) && opnd_is_reg(src1)) {\n                reg_id_t reg_dst0 = opnd_get_reg(dst0);\n                reg_id_t reg_src0 = opnd_get_reg(src0);\n                reg_id_t reg_src1 = opnd_get_reg(src1);\n                ASSERT(reg_is_gpr(reg_dst0) && reg_is_gpr(reg_src0) &&\n                           reg_is_gpr(reg_src1),\n                       \"internal error: unexpected instruction format\");\n                if (reg_dst0 == reg_src0 && reg_src0 == reg_src1) {\n                    params->gpr_bit_mask = reg_dst0;\n                    advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_9, params);\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_9:\n        if (instr_get_opcode(&params->inst) == OP_vextracti128) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            if (opnd_is_reg(src0)) {\n                if (opnd_get_reg(src0) == params->sg_info->mask_reg) {\n                    opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                    if (opnd_is_reg(dst0)) {\n                        reg_id_t tmp_reg = opnd_get_reg(dst0);\n                        if (!reg_is_strictly_xmm(tmp_reg))\n                            break;\n                        ASSERT(params->spilled_xmm == tmp_reg,\n                               \"Only the spilled xmm should be used as scratch\");\n                        params->the_scratch_xmm = tmp_reg;\n                        advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_10,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_10:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_value_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrd) ||\n            (params->sg_info->scalar_value_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrq)) {\n            opnd_t src1 = instr_get_src(&params->inst, 1);\n            if (opnd_is_reg(src1)) {\n                if (opnd_get_reg(src1) == params->gpr_bit_mask) {\n                    ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)),\n                           \"internal error: unexpected instruction format\");\n                    reg_id_t tmp_reg = opnd_get_reg(instr_get_dst(&params->inst, 0));\n                    if (tmp_reg == params->the_scratch_xmm) {\n                        advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_11,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_11:\n        if (instr_get_opcode(&params->inst) == OP_vinserti128) {\n            ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)) &&\n                       opnd_is_reg(instr_get_src(&params->inst, 0)) &&\n                       opnd_is_reg(instr_get_src(&params->inst, 1)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t dst0 = opnd_get_reg(instr_get_dst(&params->inst, 0));\n            reg_id_t src0 = opnd_get_reg(instr_get_src(&params->inst, 0));\n            reg_id_t src1 = opnd_get_reg(instr_get_src(&params->inst, 1));\n            if (src1 == params->the_scratch_xmm) {\n                if (src0 == params->sg_info->mask_reg) {\n                    if (dst0 == params->sg_info->mask_reg) {\n                        /* Check if we are already past the fault point. */\n                        if (params->info->raw_mcontext->pc <= params->prev_pc) {\n                            if (params->restore_dest_mask_start_pc <=\n                                params->info->raw_mcontext->pc) {\n                                /* Fix the gather's destination mask here and zero out\n                                 * the bit that the emulation sequence hadn't done\n                                 * before the fault hit.\n                                 */\n                                ASSERT(reg_is_strictly_xmm(params->sg_info->mask_reg) ||\n                                           reg_is_strictly_ymm(params->sg_info->mask_reg),\n                                       \"internal error: unexpected instruction format\");\n                                byte val[YMM_REG_SIZE];\n                                if (!reg_get_value_ex(params->sg_info->mask_reg,\n                                                      params->info->raw_mcontext, val)) {\n                                    ASSERT(false,\n                                           \"internal error: can't read mcontext's mask \"\n                                           \"value\");\n                                }\n                                uint mask_byte = opnd_size_in_bytes(\n                                                     params->sg_info->scalar_index_size) *\n                                        (params->scalar_mask_update_no + 1) -\n                                    1;\n                                val[mask_byte] &= ~(byte)128;\n                                reg_set_value_ex(params->sg_info->mask_reg,\n                                                 params->info->mcontext, val);\n                            }\n                            restore_spilled_xmm_value(params);\n                            /* We are done. */\n                            return true;\n                        }\n                        params->scalar_mask_update_no++;\n                        uint no_of_elements =\n                            opnd_size_in_bytes(params->sg_info->scatter_gather_size) /\n                            MAX(opnd_size_in_bytes(params->sg_info->scalar_index_size),\n                                opnd_size_in_bytes(params->sg_info->scalar_value_size));\n                        if (params->scalar_mask_update_no > no_of_elements) {\n                            /* Unlikely that something looks identical to an emulation\n                             * sequence for this long, but we safely can return here.\n                             */\n                            return true;\n                        }\n                        advance_state(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX2_GATHER_EVENT_STATE_2, params);\n        break;\n    default: ASSERT(false, \"internal error: invalid state.\");\n    }\n    return false;\n}\n\n/* Returns true if done, false otherwise. */\nstatic bool\ndrx_avx512_scatter_sequence_state_machine(void *drcontext,\n                                          drx_state_machine_params_t *params)\n{\n    switch (params->detect_state) {\n    /* First we detect the spill address for the scratch xmm reg. */\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_0: {\n        params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n        params->spilled_xmm_slot_addr = NULL;\n        ptr_int_t spilled_xmm_slot_addr;\n        if (instr_is_mov_constant(&params->inst, &spilled_xmm_slot_addr) &&\n            opnd_is_reg(instr_get_dst(&params->inst, 0)) &&\n            reg_is_gpr(opnd_get_reg(instr_get_dst(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr = (void *)spilled_xmm_slot_addr;\n            params->spilled_xmm_slot_addr_reg =\n                opnd_get_reg(instr_get_dst(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_1, params);\n        }\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_1:\n        ASSERT(params->spilled_xmm == DR_REG_NULL,\n               \"Spilled xmm reg must be undetermined yet\");\n        ASSERT(params->spilled_xmm_slot_addr != NULL &&\n                   params->spilled_xmm_slot_addr_reg != DR_REG_NULL,\n               \"xmm spill address must be determined already\");\n        if (instr_get_opcode(&params->inst) == OP_vmovdqa &&\n            opnd_is_base_disp(instr_get_dst(&params->inst, 0)) &&\n            opnd_get_base(instr_get_dst(&params->inst, 0)) ==\n                params->spilled_xmm_slot_addr_reg &&\n            opnd_is_reg(instr_get_src(&params->inst, 0)) &&\n            reg_is_strictly_xmm(opnd_get_reg(instr_get_src(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n            params->spilled_xmm = opnd_get_reg(instr_get_src(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n            break;\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_0, params);\n        break;\n    /* We come back to DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2 for each\n     * scalar store sequence of the expanded scatter instr.\n     */\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2:\n        if (instr_get_opcode(&params->inst) == OP_vextracti32x4) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_3, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_0, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_3:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_index_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpextrd) ||\n            (params->sg_info->scalar_index_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpextrq)) {\n            ASSERT(opnd_is_reg(instr_get_src(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_src(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                    params->the_scratch_xmm = DR_REG_NULL;\n                    params->gpr_scratch_index = opnd_get_reg(dst0);\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_4,\n                                  params);\n                    break;\n                }\n            }\n        }\n        /* Intentionally not else if */\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_4:\n        if (instr_get_opcode(&params->inst) == OP_vextracti32x4) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_5, params);\n                break;\n            }\n        }\n        /* Intentionally not else if */\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_5:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_value_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpextrd) ||\n            (params->sg_info->scalar_value_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpextrq)) {\n            ASSERT(opnd_is_reg(instr_get_src(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_src(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                    params->the_scratch_xmm = DR_REG_NULL;\n                    params->gpr_scratch_value = opnd_get_reg(dst0);\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_6,\n                                  params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_6: {\n        if (!instr_is_reg_spill_or_restore(drcontext, &params->inst, NULL, NULL, NULL,\n                                           NULL)) {\n            if (instr_writes_memory(&params->inst)) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_memory_reference(dst0)) {\n                    opnd_t src0 = instr_get_src(&params->inst, 0);\n                    if (opnd_is_reg(src0) &&\n                        opnd_uses_reg(src0, params->gpr_scratch_value) &&\n                        opnd_uses_reg(dst0, params->gpr_scratch_index)) {\n                        params->restore_dest_mask_start_pc = params->pc;\n                        advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_7,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_7: {\n        ptr_int_t val;\n        if (instr_is_mov_constant(&params->inst, &val)) {\n            /* If more than one bit is set, this is not what we're looking for. */\n            if (val == 0 || (val & (val - 1)) != 0)\n                break;\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_gpr = opnd_get_reg(dst0);\n                if (reg_is_gpr(tmp_gpr)) {\n                    params->gpr_bit_mask = tmp_gpr;\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_8,\n                                  params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_8:\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == DR_REG_K0) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0)) {\n                    reg_id_t tmp_gpr = opnd_get_reg(dst0);\n                    if (reg_is_gpr(tmp_gpr)) {\n                        params->gpr_save_scratch_mask = tmp_gpr;\n                        advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_9,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_9:\n        ASSERT(params->gpr_bit_mask != DR_REG_NULL,\n               \"internal error: expected gpr register to be recorded in state \"\n               \"machine.\");\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == params->gpr_bit_mask) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && opnd_get_reg(dst0) == DR_REG_K0) {\n                    params->restore_scratch_mask_start_pc = params->pc;\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_10,\n                                  params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_10:\n        if (instr_get_opcode(&params->inst) == OP_kandnw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            opnd_t src1 = instr_get_src(&params->inst, 1);\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == DR_REG_K0) {\n                if (opnd_is_reg(src1) &&\n                    opnd_get_reg(src1) == params->sg_info->mask_reg &&\n                    opnd_is_reg(dst0) &&\n                    opnd_get_reg(dst0) == params->sg_info->mask_reg) {\n                    if (params->restore_dest_mask_start_pc <=\n                            params->info->raw_mcontext->pc &&\n                        params->info->raw_mcontext->pc <= params->prev_pc) {\n                        /* Fix the scatter's destination mask here and zero out\n                         * the bit that the emulation sequence hadn't done\n                         * before the fault hit.\n                         */\n                        params->info->mcontext\n                            ->opmask[params->sg_info->mask_reg - DR_REG_K0] &=\n                            ~(1 << params->scalar_mask_update_no);\n                        /* We are not done yet, we have to fix up the scratch\n                         * mask as well.\n                         */\n                    }\n                    /* We are counting the scalar load number in the sequence\n                     * here.\n                     */\n                    params->scalar_mask_update_no++;\n                    uint no_of_elements =\n                        opnd_size_in_bytes(params->sg_info->scatter_gather_size) /\n                        MAX(opnd_size_in_bytes(params->sg_info->scalar_index_size),\n                            opnd_size_in_bytes(params->sg_info->scalar_value_size));\n                    if (params->scalar_mask_update_no > no_of_elements) {\n                        /* Unlikely that something looks identical to an emulation\n                         * sequence for this long, but we safely can return here.\n                         */\n                        return true;\n                    }\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_11,\n                                  params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_11:\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0) && opnd_get_reg(dst0) == DR_REG_K0) {\n                opnd_t src0 = instr_get_src(&params->inst, 0);\n                if (opnd_is_reg(src0)) {\n                    if (reg_is_gpr(opnd_get_reg(src0)) &&\n                        /* Check if we are already past the fault point. */\n                        params->info->raw_mcontext->pc <= params->prev_pc) {\n                        if (params->restore_scratch_mask_start_pc <=\n                            params->info->raw_mcontext->pc) {\n                            /* The scratch mask is always k0. This is hard-coded\n                             * in drx. We carefully only update the lowest 16 bits\n                             * because the mask was saved with kmovw.\n                             */\n                            ASSERT(sizeof(params->info->mcontext->opmask[0]) ==\n                                       sizeof(long long),\n                                   \"internal error: unexpected opmask slot size\");\n                            params->info->mcontext->opmask[0] &= ~0xffffLL;\n                            params->info->mcontext->opmask[0] |=\n                                reg_get_value(params->gpr_save_scratch_mask,\n                                              params->info->raw_mcontext) &\n                                0xffff;\n                        }\n                        restore_spilled_xmm_value(params);\n                        /* We are done. If we did fix up the scatter's destination\n                         * mask, this already has happened.\n                         */\n                        return true;\n                    }\n                    advance_state(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2,\n                                  params);\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_SCATTER_EVENT_STATE_2, params);\n        break;\n    default: ASSERT(false, \"internal error: invalid state.\");\n    }\n    return false;\n}\n\n/* Returns true if done, false otherwise. */\nstatic bool\ndrx_avx512_gather_sequence_state_machine(void *drcontext,\n                                         drx_state_machine_params_t *params)\n{\n    switch (params->detect_state) {\n    /* First we detect the spill address for the scratch xmm reg. */\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_0: {\n        params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n        params->spilled_xmm_slot_addr = NULL;\n        ptr_int_t spilled_xmm_slot_addr;\n        if (instr_is_mov_constant(&params->inst, &spilled_xmm_slot_addr) &&\n            opnd_is_reg(instr_get_dst(&params->inst, 0)) &&\n            reg_is_gpr(opnd_get_reg(instr_get_dst(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr = (void *)spilled_xmm_slot_addr;\n            params->spilled_xmm_slot_addr_reg =\n                opnd_get_reg(instr_get_dst(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1, params);\n        }\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_1:\n        ASSERT(params->spilled_xmm == DR_REG_NULL,\n               \"Spilled xmm reg must be undetermined yet\");\n        ASSERT(params->spilled_xmm_slot_addr != NULL &&\n                   params->spilled_xmm_slot_addr_reg != DR_REG_NULL,\n               \"xmm spill address must be determined already\");\n        if (instr_get_opcode(&params->inst) == OP_vmovdqa &&\n            opnd_is_base_disp(instr_get_dst(&params->inst, 0)) &&\n            opnd_get_base(instr_get_dst(&params->inst, 0)) ==\n                params->spilled_xmm_slot_addr_reg &&\n            opnd_is_reg(instr_get_src(&params->inst, 0)) &&\n            reg_is_strictly_xmm(opnd_get_reg(instr_get_src(&params->inst, 0)))) {\n            params->spilled_xmm_slot_addr_reg = DR_REG_NULL;\n            params->spilled_xmm = opnd_get_reg(instr_get_src(&params->inst, 0));\n            advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n            break;\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_0, params);\n        break;\n    /* We come back to DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2 for each\n     * scalar load sequence of the expanded gather instr.\n     */\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2:\n        if (instr_get_opcode(&params->inst) == OP_vextracti32x4) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_3, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_0, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_3:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_index_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpextrd) ||\n            (params->sg_info->scalar_index_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpextrq)) {\n            ASSERT(opnd_is_reg(instr_get_src(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_src(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                    params->the_scratch_xmm = DR_REG_NULL;\n                    params->gpr_scratch_index = opnd_get_reg(dst0);\n                    advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4, params);\n                    break;\n                }\n            }\n        }\n        /* Intentionally not else if */\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_4:\n        if (!instr_is_reg_spill_or_restore(drcontext, &params->inst, NULL, NULL, NULL,\n                                           NULL)) {\n            if (instr_reads_memory(&params->inst)) {\n                opnd_t src0 = instr_get_src(&params->inst, 0);\n                if (opnd_is_memory_reference(src0) &&\n                    opnd_uses_reg(src0, params->gpr_scratch_index)) {\n                    opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                    if (opnd_is_reg(dst0) && reg_is_gpr(opnd_get_reg(dst0))) {\n                        params->restore_dest_mask_start_pc = params->pc;\n                        advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_5:\n        if (instr_get_opcode(&params->inst) == OP_vextracti32x4) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_reg = opnd_get_reg(dst0);\n                if (!reg_is_strictly_xmm(tmp_reg))\n                    break;\n                ASSERT(params->spilled_xmm == tmp_reg,\n                       \"Only the spilled xmm should be used as scratch\");\n                params->the_scratch_xmm = tmp_reg;\n                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_6:\n        ASSERT(params->the_scratch_xmm != DR_REG_NULL,\n               \"internal error: expected xmm register to be recorded in state \"\n               \"machine.\");\n        if ((params->sg_info->scalar_value_size == OPSZ_4 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrd) ||\n            (params->sg_info->scalar_value_size == OPSZ_8 &&\n             instr_get_opcode(&params->inst) == OP_vpinsrq)) {\n            ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_dst(&params->inst, 0));\n            if (tmp_reg == params->the_scratch_xmm) {\n                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_7, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_7:\n        if (instr_get_opcode(&params->inst) == OP_vinserti32x4) {\n            ASSERT(opnd_is_reg(instr_get_dst(&params->inst, 0)),\n                   \"internal error: unexpected instruction format\");\n            reg_id_t tmp_reg = opnd_get_reg(instr_get_dst(&params->inst, 0));\n            if (tmp_reg == params->sg_info->gather_dst_reg) {\n                advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_8, params);\n                break;\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_8: {\n        ptr_int_t val;\n        if (instr_is_mov_constant(&params->inst, &val)) {\n            /* If more than one bit is set, this is not what we're looking for. */\n            if (val == 0 || (val & (val - 1)) != 0)\n                break;\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0)) {\n                reg_id_t tmp_gpr = opnd_get_reg(dst0);\n                if (reg_is_gpr(tmp_gpr)) {\n                    params->gpr_bit_mask = tmp_gpr;\n                    advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_9, params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    }\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_9:\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == DR_REG_K0) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0)) {\n                    reg_id_t tmp_gpr = opnd_get_reg(dst0);\n                    if (reg_is_gpr(tmp_gpr)) {\n                        params->gpr_save_scratch_mask = tmp_gpr;\n                        advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_10,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_10:\n        ASSERT(params->gpr_bit_mask != DR_REG_NULL,\n               \"internal error: expected gpr register to be recorded in state \"\n               \"machine.\");\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == params->gpr_bit_mask) {\n                opnd_t dst0 = instr_get_dst(&params->inst, 0);\n                if (opnd_is_reg(dst0) && opnd_get_reg(dst0) == DR_REG_K0) {\n                    params->restore_scratch_mask_start_pc = params->pc;\n                    advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_11,\n                                  params);\n                    break;\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_11:\n        if (instr_get_opcode(&params->inst) == OP_kandnw) {\n            opnd_t src0 = instr_get_src(&params->inst, 0);\n            opnd_t src1 = instr_get_src(&params->inst, 1);\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(src0) && opnd_get_reg(src0) == DR_REG_K0) {\n                if (opnd_is_reg(src1) &&\n                    opnd_get_reg(src1) == params->sg_info->mask_reg) {\n                    if (opnd_is_reg(dst0) &&\n                        opnd_get_reg(dst0) == params->sg_info->mask_reg) {\n                        if (params->restore_dest_mask_start_pc <=\n                                params->info->raw_mcontext->pc &&\n                            params->info->raw_mcontext->pc <= params->prev_pc) {\n                            /* Fix the gather's destination mask here and zero out\n                             * the bit that the emulation sequence hadn't done\n                             * before the fault hit.\n                             */\n                            params->info->mcontext\n                                ->opmask[params->sg_info->mask_reg - DR_REG_K0] &=\n                                ~(1 << params->scalar_mask_update_no);\n                            /* We are not done yet, we have to fix up the scratch\n                             * mask as well.\n                             */\n                        }\n                        /* We are counting the scalar load number in the sequence\n                         * here.\n                         */\n                        params->scalar_mask_update_no++;\n                        uint no_of_elements =\n                            opnd_size_in_bytes(params->sg_info->scatter_gather_size) /\n                            MAX(opnd_size_in_bytes(params->sg_info->scalar_index_size),\n                                opnd_size_in_bytes(params->sg_info->scalar_value_size));\n                        if (params->scalar_mask_update_no > no_of_elements) {\n                            /* Unlikely that something looks identical to an emulation\n                             * sequence for this long, but we safely can return here.\n                             */\n                            return true;\n                        }\n                        advance_state(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_12,\n                                      params);\n                        break;\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    case DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_12:\n        if (instr_get_opcode(&params->inst) == OP_kmovw) {\n            opnd_t dst0 = instr_get_dst(&params->inst, 0);\n            if (opnd_is_reg(dst0) && opnd_get_reg(dst0) == DR_REG_K0) {\n                opnd_t src0 = instr_get_src(&params->inst, 0);\n                if (opnd_is_reg(src0)) {\n                    reg_id_t tmp_gpr = opnd_get_reg(src0);\n                    if (reg_is_gpr(tmp_gpr)) {\n                        /* Check if we are already past the fault point. */\n                        if (params->info->raw_mcontext->pc <= params->prev_pc) {\n                            if (params->restore_scratch_mask_start_pc <=\n                                params->info->raw_mcontext->pc) {\n                                /* The scratch mask is always k0. This is hard-coded\n                                 * in drx. We carefully only update the lowest 16 bits\n                                 * because the mask was saved with kmovw.\n                                 */\n                                ASSERT(sizeof(params->info->mcontext->opmask[0]) ==\n                                           sizeof(long long),\n                                       \"internal error: unexpected opmask slot size\");\n                                params->info->mcontext->opmask[0] &= ~0xffffLL;\n                                params->info->mcontext->opmask[0] |=\n                                    reg_get_value(params->gpr_save_scratch_mask,\n                                                  params->info->raw_mcontext) &\n                                    0xffff;\n                            }\n                            restore_spilled_xmm_value(params);\n                            /* We are done. If we did fix up the gather's destination\n                             * mask, this already has happened.\n                             */\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n        skip_unknown_instr_inc(DRX_DETECT_RESTORE_AVX512_GATHER_EVENT_STATE_2, params);\n        break;\n    default: ASSERT(false, \"internal error: invalid state.\");\n    }\n    return false;\n}\n\nstatic bool\ndrx_restore_state_for_avx512_gather(void *drcontext, dr_restore_state_info_t *info,\n                                    scatter_gather_info_t *sg_info)\n{\n    return drx_restore_state_scatter_gather(drcontext, info, sg_info,\n                                            drx_avx512_gather_sequence_state_machine);\n}\n\nstatic bool\ndrx_restore_state_for_avx512_scatter(void *drcontext, dr_restore_state_info_t *info,\n                                     scatter_gather_info_t *sg_info)\n{\n    return drx_restore_state_scatter_gather(drcontext, info, sg_info,\n                                            drx_avx512_scatter_sequence_state_machine);\n}\n\nstatic bool\ndrx_restore_state_for_avx2_gather(void *drcontext, dr_restore_state_info_t *info,\n                                  scatter_gather_info_t *sg_info)\n{\n    return drx_restore_state_scatter_gather(drcontext, info, sg_info,\n                                            drx_avx2_gather_sequence_state_machine);\n}\n\nstatic bool\ndrx_event_restore_state(void *drcontext, bool restore_memory,\n                        dr_restore_state_info_t *info)\n{\n    instr_t inst;\n    bool success = true;\n    if (info->fragment_info.cache_start_pc == NULL)\n        return true; /* fault not in cache */\n    if (dr_atomic_load32(&drx_scatter_gather_expanded) == 0) {\n        /* Nothing to do if nobody had never called expand_scatter_gather() before. */\n        return true;\n    }\n    if (!info->fragment_info.app_code_consistent) {\n        /* Can't verify application code.\n         * XXX i#2985: is it better to keep searching?\n         */\n        return true;\n    }\n    instr_init(drcontext, &inst);\n    byte *pc = decode(drcontext, dr_fragment_app_pc(info->fragment_info.tag), &inst);\n    if (pc != NULL) {\n        scatter_gather_info_t sg_info;\n        if (instr_is_gather(&inst)) {\n            get_scatter_gather_info(&inst, &sg_info);\n            if (sg_info.is_evex) {\n                success = success &&\n                    drx_restore_state_for_avx512_gather(drcontext, info, &sg_info);\n            } else {\n                success = success &&\n                    drx_restore_state_for_avx2_gather(drcontext, info, &sg_info);\n            }\n        } else if (instr_is_scatter(&inst)) {\n            get_scatter_gather_info(&inst, &sg_info);\n            success = success &&\n                drx_restore_state_for_avx512_scatter(drcontext, info, &sg_info);\n        }\n    }\n    instr_free(drcontext, &inst);\n    return success;\n}\n\n#endif\n", "idx": 41, "id": 25702, "msg": "", "proj": "DynamoRIO-dynamorio", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -39,16 +39,12 @@ var (\n // Once the callback has been called with isForwardSecure = true, it is guarantueed to not be called with isForwardSecure = false after that\n type cryptoChangeCallback func(session Session, isForwardSecure bool)\n \n-// closeCallback is called when a session is closed\n-type closeCallback func(id protocol.ConnectionID)\n-\n // A Session is a QUIC session\n type session struct {\n \tconnectionID protocol.ConnectionID\n \tperspective  protocol.Perspective\n \tversion      protocol.VersionNumber\n \n-\tcloseCallback        closeCallback\n \tcryptoChangeCallback cryptoChangeCallback\n \n \tconn connection", "y": 0, "oldf": "package quic\n\nimport (\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/lucas-clemente/quic-go/ackhandler\"\n\t\"github.com/lucas-clemente/quic-go/congestion\"\n\t\"github.com/lucas-clemente/quic-go/flowcontrol\"\n\t\"github.com/lucas-clemente/quic-go/frames\"\n\t\"github.com/lucas-clemente/quic-go/handshake\"\n\t\"github.com/lucas-clemente/quic-go/protocol\"\n\t\"github.com/lucas-clemente/quic-go/qerr\"\n\t\"github.com/lucas-clemente/quic-go/utils\"\n)\n\ntype unpacker interface {\n\tUnpack(publicHeaderBinary []byte, hdr *PublicHeader, data []byte) (*unpackedPacket, error)\n}\n\ntype receivedPacket struct {\n\tremoteAddr   net.Addr\n\tpublicHeader *PublicHeader\n\tdata         []byte\n\trcvTime      time.Time\n}\n\nvar (\n\terrRstStreamOnInvalidStream   = errors.New(\"RST_STREAM received for unknown stream\")\n\terrWindowUpdateOnClosedStream = errors.New(\"WINDOW_UPDATE received for an already closed stream\")\n\terrSessionAlreadyClosed       = errors.New(\"cannot close session; it was already closed before\")\n)\n\n// cryptoChangeCallback is called every time the encryption level changes\n// Once the callback has been called with isForwardSecure = true, it is guarantueed to not be called with isForwardSecure = false after that\ntype cryptoChangeCallback func(session Session, isForwardSecure bool)\n\n// closeCallback is called when a session is closed\ntype closeCallback func(id protocol.ConnectionID)\n\n// A Session is a QUIC session\ntype session struct {\n\tconnectionID protocol.ConnectionID\n\tperspective  protocol.Perspective\n\tversion      protocol.VersionNumber\n\n\tcloseCallback        closeCallback\n\tcryptoChangeCallback cryptoChangeCallback\n\n\tconn connection\n\n\tstreamsMap *streamsMap\n\n\trttStats *congestion.RTTStats\n\n\tsentPacketHandler     ackhandler.SentPacketHandler\n\treceivedPacketHandler ackhandler.ReceivedPacketHandler\n\tstreamFramer          *streamFramer\n\n\tflowControlManager flowcontrol.FlowControlManager\n\n\tunpacker unpacker\n\tpacker   *packetPacker\n\n\tcryptoSetup handshake.CryptoSetup\n\n\treceivedPackets  chan *receivedPacket\n\tsendingScheduled chan struct{}\n\t// closeChan is used to notify the run loop that it should terminate.\n\t// If the value is not nil, the error is sent as a CONNECTION_CLOSE.\n\tcloseChan chan *qerr.QuicError\n\trunClosed chan struct{}\n\tclosed    uint32 // atomic bool\n\n\t// when we receive too many undecryptable packets during the handshake, we send a Public reset\n\t// but only after a time of protocol.PublicResetTimeout has passed\n\tundecryptablePackets                   []*receivedPacket\n\treceivedTooManyUndecrytablePacketsTime time.Time\n\n\taeadChanged chan protocol.EncryptionLevel\n\n\tnextAckScheduledTime time.Time\n\n\tconnectionParameters handshake.ConnectionParametersManager\n\n\tlastRcvdPacketNumber protocol.PacketNumber\n\t// Used to calculate the next packet number from the truncated wire\n\t// representation, and sent back in public reset packets\n\tlargestRcvdPacketNumber protocol.PacketNumber\n\n\tsessionCreationTime     time.Time\n\tlastNetworkActivityTime time.Time\n\n\ttimer           *time.Timer\n\tcurrentDeadline time.Time\n\ttimerRead       bool\n}\n\nvar _ Session = &session{}\n\n// newSession makes a new session\nfunc newSession(conn connection, v protocol.VersionNumber, connectionID protocol.ConnectionID, sCfg *handshake.ServerConfig, closeCallback closeCallback, cryptoChangeCallback cryptoChangeCallback) (packetHandler, error) {\n\ts := &session{\n\t\tconn:         conn,\n\t\tconnectionID: connectionID,\n\t\tperspective:  protocol.PerspectiveServer,\n\t\tversion:      v,\n\n\t\tcloseCallback:        closeCallback,\n\t\tcryptoChangeCallback: cryptoChangeCallback,\n\t\tconnectionParameters: handshake.NewConnectionParamatersManager(protocol.PerspectiveServer, v),\n\t}\n\n\ts.setup()\n\tcryptoStream, _ := s.GetOrOpenStream(1)\n\t_, _ = s.AcceptStream() // don't expose the crypto stream\n\tvar sourceAddr []byte\n\tif udpAddr, ok := conn.RemoteAddr().(*net.UDPAddr); ok {\n\t\tsourceAddr = udpAddr.IP\n\t} else {\n\t\tsourceAddr = []byte(conn.RemoteAddr().String())\n\t}\n\tvar err error\n\ts.cryptoSetup, err = handshake.NewCryptoSetup(connectionID, sourceAddr, v, sCfg, cryptoStream, s.connectionParameters, s.aeadChanged)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.packer = newPacketPacker(connectionID, s.cryptoSetup, s.connectionParameters, s.streamFramer, s.perspective, s.version)\n\ts.unpacker = &packetUnpacker{aead: s.cryptoSetup, version: s.version}\n\n\treturn s, err\n}\n\nfunc newClientSession(conn connection, hostname string, v protocol.VersionNumber, connectionID protocol.ConnectionID, tlsConfig *tls.Config, closeCallback closeCallback, cryptoChangeCallback cryptoChangeCallback, negotiatedVersions []protocol.VersionNumber) (*session, error) {\n\ts := &session{\n\t\tconn:         conn,\n\t\tconnectionID: connectionID,\n\t\tperspective:  protocol.PerspectiveClient,\n\t\tversion:      v,\n\n\t\tcloseCallback:        closeCallback,\n\t\tcryptoChangeCallback: cryptoChangeCallback,\n\t\tconnectionParameters: handshake.NewConnectionParamatersManager(protocol.PerspectiveClient, v),\n\t}\n\n\ts.receivedPacketHandler = ackhandler.NewReceivedPacketHandler(s.ackAlarmChanged)\n\ts.setup()\n\n\tcryptoStream, _ := s.OpenStream()\n\tvar err error\n\ts.cryptoSetup, err = handshake.NewCryptoSetupClient(hostname, connectionID, v, cryptoStream, tlsConfig, s.connectionParameters, s.aeadChanged, negotiatedVersions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.packer = newPacketPacker(connectionID, s.cryptoSetup, s.connectionParameters, s.streamFramer, s.perspective, s.version)\n\ts.unpacker = &packetUnpacker{aead: s.cryptoSetup, version: s.version}\n\n\treturn s, err\n}\n\n// setup is called from newSession and newClientSession and initializes values that are independent of the perspective\nfunc (s *session) setup() {\n\ts.rttStats = &congestion.RTTStats{}\n\tflowControlManager := flowcontrol.NewFlowControlManager(s.connectionParameters, s.rttStats)\n\n\tsentPacketHandler := ackhandler.NewSentPacketHandler(s.rttStats)\n\n\tnow := time.Now()\n\n\ts.sentPacketHandler = sentPacketHandler\n\ts.flowControlManager = flowControlManager\n\ts.receivedPacketHandler = ackhandler.NewReceivedPacketHandler(s.ackAlarmChanged)\n\n\ts.receivedPackets = make(chan *receivedPacket, protocol.MaxSessionUnprocessedPackets)\n\ts.closeChan = make(chan *qerr.QuicError, 1)\n\ts.sendingScheduled = make(chan struct{}, 1)\n\ts.undecryptablePackets = make([]*receivedPacket, 0, protocol.MaxUndecryptablePackets)\n\ts.aeadChanged = make(chan protocol.EncryptionLevel, 2)\n\ts.runClosed = make(chan struct{}, 1)\n\n\ts.timer = time.NewTimer(0)\n\ts.lastNetworkActivityTime = now\n\ts.sessionCreationTime = now\n\n\ts.streamsMap = newStreamsMap(s.newStream, s.perspective, s.connectionParameters)\n\ts.streamFramer = newStreamFramer(s.streamsMap, s.flowControlManager)\n}\n\n// run the session main loop\nfunc (s *session) run() {\n\t// Start the crypto stream handler\n\tgo func() {\n\t\tif err := s.cryptoSetup.HandleCryptoStream(); err != nil {\n\t\t\ts.Close(err)\n\t\t}\n\t}()\n\nrunLoop:\n\tfor {\n\t\t// Close immediately if requested\n\t\tselect {\n\t\tcase errForConnClose := <-s.closeChan:\n\t\t\tif errForConnClose != nil {\n\t\t\t\ts.sendConnectionClose(errForConnClose)\n\t\t\t}\n\t\t\tbreak runLoop\n\t\tdefault:\n\t\t}\n\n\t\ts.maybeResetTimer()\n\n\t\tvar err error\n\t\tselect {\n\t\tcase errForConnClose := <-s.closeChan:\n\t\t\tif errForConnClose != nil {\n\t\t\t\ts.sendConnectionClose(errForConnClose)\n\t\t\t}\n\t\t\tbreak runLoop\n\t\tcase <-s.timer.C:\n\t\t\ts.timerRead = true\n\t\t\t// We do all the interesting stuff after the switch statement, so\n\t\t\t// nothing to see here.\n\t\tcase <-s.sendingScheduled:\n\t\t\t// We do all the interesting stuff after the switch statement, so\n\t\t\t// nothing to see here.\n\t\tcase p := <-s.receivedPackets:\n\t\t\terr = s.handlePacketImpl(p)\n\t\t\tif qErr, ok := err.(*qerr.QuicError); ok && qErr.ErrorCode == qerr.DecryptionFailure {\n\t\t\t\ts.tryQueueingUndecryptablePacket(p)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// This is a bit unclean, but works properly, since the packet always\n\t\t\t// begins with the public header and we never copy it.\n\t\t\tputPacketBuffer(p.publicHeader.Raw)\n\t\tcase l := <-s.aeadChanged:\n\t\t\tif l == protocol.EncryptionForwardSecure {\n\t\t\t\ts.packer.SetForwardSecure()\n\t\t\t}\n\t\t\ts.tryDecryptingQueuedPackets()\n\t\t\ts.cryptoChangeCallback(s, l == protocol.EncryptionForwardSecure)\n\t\t}\n\n\t\tif err != nil {\n\t\t\ts.close(err)\n\t\t}\n\n\t\tnow := time.Now()\n\t\tif s.sentPacketHandler.GetAlarmTimeout().Before(now) {\n\t\t\t// This could cause packets to be retransmitted, so check it before trying\n\t\t\t// to send packets.\n\t\t\ts.sentPacketHandler.OnAlarm()\n\t\t}\n\n\t\tif err := s.sendPacket(); err != nil {\n\t\t\ts.close(err)\n\t\t}\n\t\tif !s.receivedTooManyUndecrytablePacketsTime.IsZero() && s.receivedTooManyUndecrytablePacketsTime.Add(protocol.PublicResetTimeout).Before(now) && len(s.undecryptablePackets) != 0 {\n\t\t\ts.close(qerr.Error(qerr.DecryptionFailure, \"too many undecryptable packets received\"))\n\t\t}\n\t\tif now.Sub(s.lastNetworkActivityTime) >= s.idleTimeout() {\n\t\t\ts.close(qerr.Error(qerr.NetworkIdleTimeout, \"No recent network activity.\"))\n\t\t}\n\t\tif !s.cryptoSetup.HandshakeComplete() && now.Sub(s.sessionCreationTime) >= protocol.MaxTimeForCryptoHandshake {\n\t\t\ts.close(qerr.Error(qerr.NetworkIdleTimeout, \"Crypto handshake did not complete in time.\"))\n\t\t}\n\t\ts.garbageCollectStreams()\n\t}\n\n\ts.closeCallback(s.connectionID)\n\ts.runClosed <- struct{}{}\n}\n\nfunc (s *session) maybeResetTimer() {\n\tnextDeadline := s.lastNetworkActivityTime.Add(s.idleTimeout())\n\n\tif !s.nextAckScheduledTime.IsZero() {\n\t\tnextDeadline = utils.MinTime(nextDeadline, s.nextAckScheduledTime)\n\t}\n\tif lossTime := s.sentPacketHandler.GetAlarmTimeout(); !lossTime.IsZero() {\n\t\tnextDeadline = utils.MinTime(nextDeadline, lossTime)\n\t}\n\tif !s.cryptoSetup.HandshakeComplete() {\n\t\thandshakeDeadline := s.sessionCreationTime.Add(protocol.MaxTimeForCryptoHandshake)\n\t\tnextDeadline = utils.MinTime(nextDeadline, handshakeDeadline)\n\t}\n\tif !s.receivedTooManyUndecrytablePacketsTime.IsZero() {\n\t\tnextDeadline = utils.MinTime(nextDeadline, s.receivedTooManyUndecrytablePacketsTime.Add(protocol.PublicResetTimeout))\n\t}\n\n\tif nextDeadline.Equal(s.currentDeadline) {\n\t\t// No need to reset the timer\n\t\treturn\n\t}\n\n\t// We need to drain the timer if the value from its channel was not read yet.\n\t// See https://groups.google.com/forum/#!topic/golang-dev/c9UUfASVPoU\n\tif !s.timer.Stop() && !s.timerRead {\n\t\t<-s.timer.C\n\t}\n\ts.timer.Reset(nextDeadline.Sub(time.Now()))\n\n\ts.timerRead = false\n\ts.currentDeadline = nextDeadline\n}\n\nfunc (s *session) idleTimeout() time.Duration {\n\tif s.cryptoSetup.HandshakeComplete() {\n\t\treturn s.connectionParameters.GetIdleConnectionStateLifetime()\n\t}\n\treturn protocol.InitialIdleTimeout\n}\n\nfunc (s *session) handlePacketImpl(p *receivedPacket) error {\n\tif s.perspective == protocol.PerspectiveClient {\n\t\tdiversificationNonce := p.publicHeader.DiversificationNonce\n\t\tif len(diversificationNonce) > 0 {\n\t\t\ts.cryptoSetup.SetDiversificationNonce(diversificationNonce)\n\t\t}\n\t}\n\n\tif p.rcvTime.IsZero() {\n\t\t// To simplify testing\n\t\tp.rcvTime = time.Now()\n\t}\n\n\ts.lastNetworkActivityTime = p.rcvTime\n\thdr := p.publicHeader\n\tdata := p.data\n\n\t// Calculate packet number\n\thdr.PacketNumber = protocol.InferPacketNumber(\n\t\thdr.PacketNumberLen,\n\t\ts.largestRcvdPacketNumber,\n\t\thdr.PacketNumber,\n\t)\n\n\tpacket, err := s.unpacker.Unpack(hdr.Raw, hdr, data)\n\tif utils.Debug() {\n\t\tif err != nil {\n\t\t\tutils.Debugf(\"<- Reading packet 0x%x (%d bytes) for connection %x @ %s\", hdr.PacketNumber, len(data)+len(hdr.Raw), hdr.ConnectionID, time.Now().Format(\"15:04:05.000\"))\n\t\t} else {\n\t\t\tutils.Debugf(\"<- Reading packet 0x%x (%d bytes) for connection %x, %s @ %s\", hdr.PacketNumber, len(data)+len(hdr.Raw), hdr.ConnectionID, packet.encryptionLevel, time.Now().Format(\"15:04:05.000\"))\n\t\t}\n\t}\n\t// if the decryption failed, this might be a packet sent by an attacker\n\t// don't update the remote address\n\tif quicErr, ok := err.(*qerr.QuicError); ok && quicErr.ErrorCode == qerr.DecryptionFailure {\n\t\treturn err\n\t}\n\tif s.perspective == protocol.PerspectiveServer {\n\t\t// update the remote address, even if unpacking failed for any other reason than a decryption error\n\t\ts.conn.SetCurrentRemoteAddr(p.remoteAddr)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ts.lastRcvdPacketNumber = hdr.PacketNumber\n\t// Only do this after decrypting, so we are sure the packet is not attacker-controlled\n\ts.largestRcvdPacketNumber = utils.MaxPacketNumber(s.largestRcvdPacketNumber, hdr.PacketNumber)\n\n\terr = s.receivedPacketHandler.ReceivedPacket(hdr.PacketNumber, packet.IsRetransmittable())\n\t// ignore duplicate packets\n\tif err == ackhandler.ErrDuplicatePacket {\n\t\tutils.Infof(\"Ignoring packet 0x%x due to ErrDuplicatePacket\", hdr.PacketNumber)\n\t\treturn nil\n\t}\n\t// ignore packets with packet numbers smaller than the LeastUnacked of a StopWaiting\n\tif err == ackhandler.ErrPacketSmallerThanLastStopWaiting {\n\t\tutils.Infof(\"Ignoring packet 0x%x due to ErrPacketSmallerThanLastStopWaiting\", hdr.PacketNumber)\n\t\treturn nil\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn s.handleFrames(packet.frames)\n}\n\nfunc (s *session) handleFrames(fs []frames.Frame) error {\n\tfor _, ff := range fs {\n\t\tvar err error\n\t\tframes.LogFrame(ff, false)\n\t\tswitch frame := ff.(type) {\n\t\tcase *frames.StreamFrame:\n\t\t\terr = s.handleStreamFrame(frame)\n\t\tcase *frames.AckFrame:\n\t\t\terr = s.handleAckFrame(frame)\n\t\tcase *frames.ConnectionCloseFrame:\n\t\t\ts.closeImpl(qerr.Error(frame.ErrorCode, frame.ReasonPhrase), true)\n\t\tcase *frames.GoawayFrame:\n\t\t\terr = errors.New(\"unimplemented: handling GOAWAY frames\")\n\t\tcase *frames.StopWaitingFrame:\n\t\t\terr = s.receivedPacketHandler.ReceivedStopWaiting(frame)\n\t\tcase *frames.RstStreamFrame:\n\t\t\terr = s.handleRstStreamFrame(frame)\n\t\tcase *frames.WindowUpdateFrame:\n\t\t\terr = s.handleWindowUpdateFrame(frame)\n\t\tcase *frames.BlockedFrame:\n\t\tcase *frames.PingFrame:\n\t\tdefault:\n\t\t\treturn errors.New(\"Session BUG: unexpected frame type\")\n\t\t}\n\n\t\tif err != nil {\n\t\t\tswitch err {\n\t\t\tcase ackhandler.ErrDuplicateOrOutOfOrderAck:\n\t\t\t\t// Can happen e.g. when packets thought missing arrive late\n\t\t\tcase errRstStreamOnInvalidStream:\n\t\t\t\t// Can happen when RST_STREAMs arrive early or late (?)\n\t\t\t\tutils.Errorf(\"Ignoring error in session: %s\", err.Error())\n\t\t\tcase errWindowUpdateOnClosedStream:\n\t\t\t\t// Can happen when we already sent the last StreamFrame with the FinBit, but the client already sent a WindowUpdate for this Stream\n\t\t\tdefault:\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// handlePacket is called by the server with a new packet\nfunc (s *session) handlePacket(p *receivedPacket) {\n\t// Discard packets once the amount of queued packets is larger than\n\t// the channel size, protocol.MaxSessionUnprocessedPackets\n\tselect {\n\tcase s.receivedPackets <- p:\n\tdefault:\n\t}\n}\n\nfunc (s *session) handleStreamFrame(frame *frames.StreamFrame) error {\n\tstr, err := s.streamsMap.GetOrOpenStream(frame.StreamID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif str == nil {\n\t\t// Stream is closed and already garbage collected\n\t\t// ignore this StreamFrame\n\t\treturn nil\n\t}\n\treturn str.AddStreamFrame(frame)\n}\n\nfunc (s *session) handleWindowUpdateFrame(frame *frames.WindowUpdateFrame) error {\n\tif frame.StreamID != 0 {\n\t\tstr, err := s.streamsMap.GetOrOpenStream(frame.StreamID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif str == nil {\n\t\t\treturn errWindowUpdateOnClosedStream\n\t\t}\n\t}\n\t_, err := s.flowControlManager.UpdateWindow(frame.StreamID, frame.ByteOffset)\n\treturn err\n}\n\nfunc (s *session) handleRstStreamFrame(frame *frames.RstStreamFrame) error {\n\tstr, err := s.streamsMap.GetOrOpenStream(frame.StreamID)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif str == nil {\n\t\treturn errRstStreamOnInvalidStream\n\t}\n\n\tstr.RegisterRemoteError(fmt.Errorf(\"RST_STREAM received with code %d\", frame.ErrorCode))\n\treturn s.flowControlManager.ResetStream(frame.StreamID, frame.ByteOffset)\n}\n\nfunc (s *session) handleAckFrame(frame *frames.AckFrame) error {\n\treturn s.sentPacketHandler.ReceivedAck(frame, s.lastRcvdPacketNumber, s.lastNetworkActivityTime)\n}\n\n// Close the connection. If err is nil it will be set to qerr.PeerGoingAway.\n// It waits until the run loop has stopped before returning\nfunc (s *session) Close(e error) error {\n\terr := s.closeImpl(e, false)\n\tif err == errSessionAlreadyClosed {\n\t\treturn nil\n\t}\n\n\t// wait for the run loop to finish\n\t<-s.runClosed\n\treturn err\n}\n\n// close the connection. Use this when called from the run loop\nfunc (s *session) close(e error) error {\n\terr := s.closeImpl(e, false)\n\tif err == errSessionAlreadyClosed {\n\t\treturn nil\n\t}\n\treturn err\n}\n\nfunc (s *session) closeImpl(e error, remoteClose bool) error {\n\t// Only close once\n\tif !atomic.CompareAndSwapUint32(&s.closed, 0, 1) {\n\t\treturn errSessionAlreadyClosed\n\t}\n\n\tif e == errCloseSessionForNewVersion {\n\t\ts.streamsMap.CloseWithError(e)\n\t\ts.closeStreamsWithError(e)\n\t\t// when the run loop exits, it will call the closeCallback\n\t\t// replace it with an noop function to make sure this doesn't have any effect\n\t\ts.closeCallback = func(protocol.ConnectionID) {}\n\t\ts.closeChan <- nil\n\t\treturn nil\n\t}\n\n\tif e == nil {\n\t\te = qerr.PeerGoingAway\n\t}\n\n\tquicErr := qerr.ToQuicError(e)\n\n\t// Don't log 'normal' reasons\n\tif quicErr.ErrorCode == qerr.PeerGoingAway || quicErr.ErrorCode == qerr.NetworkIdleTimeout {\n\t\tutils.Infof(\"Closing connection %x\", s.connectionID)\n\t} else {\n\t\tutils.Errorf(\"Closing session with error: %s\", e.Error())\n\t}\n\n\ts.streamsMap.CloseWithError(quicErr)\n\ts.closeStreamsWithError(quicErr)\n\n\tif remoteClose {\n\t\t// If this is a remote close we don't need to send a CONNECTION_CLOSE\n\t\ts.closeChan <- nil\n\t\treturn nil\n\t}\n\n\tif quicErr.ErrorCode == qerr.DecryptionFailure || quicErr == handshake.ErrHOLExperiment {\n\t\t// If we send a public reset, don't send a CONNECTION_CLOSE\n\t\ts.closeChan <- nil\n\t\treturn s.sendPublicReset(s.lastRcvdPacketNumber)\n\t}\n\ts.closeChan <- quicErr\n\treturn nil\n}\n\nfunc (s *session) closeStreamsWithError(err error) {\n\ts.streamsMap.Iterate(func(str *stream) (bool, error) {\n\t\tstr.Cancel(err)\n\t\treturn true, nil\n\t})\n}\n\nfunc (s *session) sendPacket() error {\n\t// Repeatedly try sending until we don't have any more data, or run out of the congestion window\n\tfor {\n\t\tif !s.sentPacketHandler.SendingAllowed() {\n\t\t\treturn nil\n\t\t}\n\n\t\tvar controlFrames []frames.Frame\n\n\t\t// get WindowUpdate frames\n\t\t// this call triggers the flow controller to increase the flow control windows, if necessary\n\t\twindowUpdateFrames, err := s.getWindowUpdateFrames()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, wuf := range windowUpdateFrames {\n\t\t\tcontrolFrames = append(controlFrames, wuf)\n\t\t}\n\n\t\t// check for retransmissions first\n\t\tfor {\n\t\t\tretransmitPacket := s.sentPacketHandler.DequeuePacketForRetransmission()\n\t\t\tif retransmitPacket == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tutils.Debugf(\"\\tDequeueing retransmission for packet 0x%x\", retransmitPacket.PacketNumber)\n\n\t\t\tif retransmitPacket.EncryptionLevel != protocol.EncryptionForwardSecure {\n\t\t\t\tutils.Debugf(\"\\tDequeueing handshake retransmission for packet 0x%x\", retransmitPacket.PacketNumber)\n\t\t\t\tstopWaitingFrame := s.sentPacketHandler.GetStopWaitingFrame(true)\n\t\t\t\tvar packet *packedPacket\n\t\t\t\tpacket, err = s.packer.RetransmitNonForwardSecurePacket(stopWaitingFrame, retransmitPacket)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif packet == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\terr = s.sendPackedPacket(packet)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\t// resend the frames that were in the packet\n\t\t\t\tfor _, frame := range retransmitPacket.GetFramesForRetransmission() {\n\t\t\t\t\tswitch frame.(type) {\n\t\t\t\t\tcase *frames.StreamFrame:\n\t\t\t\t\t\ts.streamFramer.AddFrameForRetransmission(frame.(*frames.StreamFrame))\n\t\t\t\t\tcase *frames.WindowUpdateFrame:\n\t\t\t\t\t\t// only retransmit WindowUpdates if the stream is not yet closed and the we haven't sent another WindowUpdate with a higher ByteOffset for the stream\n\t\t\t\t\t\tvar currentOffset protocol.ByteCount\n\t\t\t\t\t\tf := frame.(*frames.WindowUpdateFrame)\n\t\t\t\t\t\tcurrentOffset, err = s.flowControlManager.GetReceiveWindow(f.StreamID)\n\t\t\t\t\t\tif err == nil && f.ByteOffset >= currentOffset {\n\t\t\t\t\t\t\tcontrolFrames = append(controlFrames, frame)\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tcontrolFrames = append(controlFrames, frame)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tack := s.receivedPacketHandler.GetAckFrame()\n\t\tif ack != nil {\n\t\t\tcontrolFrames = append(controlFrames, ack)\n\t\t}\n\t\thasRetransmission := s.streamFramer.HasFramesForRetransmission()\n\t\tvar stopWaitingFrame *frames.StopWaitingFrame\n\t\tif ack != nil || hasRetransmission {\n\t\t\tstopWaitingFrame = s.sentPacketHandler.GetStopWaitingFrame(hasRetransmission)\n\t\t}\n\t\tpacket, err := s.packer.PackPacket(stopWaitingFrame, controlFrames, s.sentPacketHandler.GetLeastUnacked())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif packet == nil {\n\t\t\treturn nil\n\t\t}\n\t\t// send every window update twice\n\t\tfor _, f := range windowUpdateFrames {\n\t\t\ts.packer.QueueControlFrameForNextPacket(f)\n\t\t}\n\n\t\terr = s.sendPackedPacket(packet)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts.nextAckScheduledTime = time.Time{}\n\t}\n}\n\nfunc (s *session) sendPackedPacket(packet *packedPacket) error {\n\terr := s.sentPacketHandler.SentPacket(&ackhandler.Packet{\n\t\tPacketNumber:    packet.number,\n\t\tFrames:          packet.frames,\n\t\tLength:          protocol.ByteCount(len(packet.raw)),\n\t\tEncryptionLevel: packet.encryptionLevel,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ts.logPacket(packet)\n\n\terr = s.conn.Write(packet.raw)\n\tputPacketBuffer(packet.raw)\n\treturn err\n}\n\nfunc (s *session) sendConnectionClose(quicErr *qerr.QuicError) error {\n\tpacket, err := s.packer.PackConnectionClose(&frames.ConnectionCloseFrame{ErrorCode: quicErr.ErrorCode, ReasonPhrase: quicErr.ErrorMessage}, s.sentPacketHandler.GetLeastUnacked())\n\tif err != nil {\n\t\treturn err\n\t}\n\tif packet == nil {\n\t\treturn errors.New(\"Session BUG: expected packet not to be nil\")\n\t}\n\ts.logPacket(packet)\n\treturn s.conn.Write(packet.raw)\n}\n\nfunc (s *session) logPacket(packet *packedPacket) {\n\tif !utils.Debug() {\n\t\t// We don't need to allocate the slices for calling the format functions\n\t\treturn\n\t}\n\tif utils.Debug() {\n\t\tutils.Debugf(\"-> Sending packet 0x%x (%d bytes), %s, @ %s\", packet.number, len(packet.raw), packet.encryptionLevel, time.Now().Format(\"15:04:05.000\"))\n\t\tfor _, frame := range packet.frames {\n\t\t\tframes.LogFrame(frame, true)\n\t\t}\n\t}\n}\n\n// GetOrOpenStream either returns an existing stream, a newly opened stream, or nil if a stream with the provided ID is already closed.\n// Newly opened streams should only originate from the client. To open a stream from the server, OpenStream should be used.\nfunc (s *session) GetOrOpenStream(id protocol.StreamID) (Stream, error) {\n\tstr, err := s.streamsMap.GetOrOpenStream(id)\n\tif str != nil {\n\t\treturn str, err\n\t}\n\t// make sure to return an actual nil value here, not an Stream with value nil\n\treturn nil, err\n}\n\n// AcceptStream returns the next stream openend by the peer\nfunc (s *session) AcceptStream() (Stream, error) {\n\treturn s.streamsMap.AcceptStream()\n}\n\n// OpenStream opens a stream\nfunc (s *session) OpenStream() (Stream, error) {\n\treturn s.streamsMap.OpenStream()\n}\n\nfunc (s *session) OpenStreamSync() (Stream, error) {\n\treturn s.streamsMap.OpenStreamSync()\n}\n\nfunc (s *session) queueResetStreamFrame(id protocol.StreamID, offset protocol.ByteCount) {\n\ts.packer.QueueControlFrameForNextPacket(&frames.RstStreamFrame{\n\t\tStreamID:   id,\n\t\tByteOffset: offset,\n\t})\n\ts.scheduleSending()\n}\n\nfunc (s *session) newStream(id protocol.StreamID) (*stream, error) {\n\tstream, err := newStream(id, s.scheduleSending, s.queueResetStreamFrame, s.flowControlManager)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO: find a better solution for determining which streams contribute to connection level flow control\n\tif id == 1 || id == 3 {\n\t\ts.flowControlManager.NewStream(id, false)\n\t} else {\n\t\ts.flowControlManager.NewStream(id, true)\n\t}\n\n\treturn stream, nil\n}\n\n// garbageCollectStreams goes through all streams and removes EOF'ed streams\n// from the streams map.\nfunc (s *session) garbageCollectStreams() {\n\ts.streamsMap.Iterate(func(str *stream) (bool, error) {\n\t\tid := str.StreamID()\n\t\tif str.finished() {\n\t\t\terr := s.streamsMap.RemoveStream(id)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\ts.flowControlManager.RemoveStream(id)\n\t\t}\n\t\treturn true, nil\n\t})\n}\n\nfunc (s *session) sendPublicReset(rejectedPacketNumber protocol.PacketNumber) error {\n\tutils.Infof(\"Sending public reset for connection %x, packet number %d\", s.connectionID, rejectedPacketNumber)\n\treturn s.conn.Write(writePublicReset(s.connectionID, rejectedPacketNumber, 0))\n}\n\n// scheduleSending signals that we have data for sending\nfunc (s *session) scheduleSending() {\n\tselect {\n\tcase s.sendingScheduled <- struct{}{}:\n\tdefault:\n\t}\n}\n\nfunc (s *session) tryQueueingUndecryptablePacket(p *receivedPacket) {\n\tif s.cryptoSetup.HandshakeComplete() {\n\t\treturn\n\t}\n\tif len(s.undecryptablePackets)+1 > protocol.MaxUndecryptablePackets {\n\t\t// if this is the first time the undecryptablePackets runs full, start the timer to send a Public Reset\n\t\tif s.receivedTooManyUndecrytablePacketsTime.IsZero() {\n\t\t\ts.receivedTooManyUndecrytablePacketsTime = time.Now()\n\t\t\ts.maybeResetTimer()\n\t\t}\n\t\tutils.Infof(\"Dropping undecrytable packet 0x%x (undecryptable packet queue full)\", p.publicHeader.PacketNumber)\n\t\treturn\n\t}\n\tutils.Infof(\"Queueing packet 0x%x for later decryption\", p.publicHeader.PacketNumber)\n\ts.undecryptablePackets = append(s.undecryptablePackets, p)\n}\n\nfunc (s *session) tryDecryptingQueuedPackets() {\n\tfor _, p := range s.undecryptablePackets {\n\t\ts.handlePacket(p)\n\t}\n\ts.undecryptablePackets = s.undecryptablePackets[:0]\n}\n\nfunc (s *session) getWindowUpdateFrames() ([]*frames.WindowUpdateFrame, error) {\n\tupdates := s.flowControlManager.GetWindowUpdates()\n\tres := make([]*frames.WindowUpdateFrame, len(updates))\n\tfor i, u := range updates {\n\t\tres[i] = &frames.WindowUpdateFrame{StreamID: u.StreamID, ByteOffset: u.Offset}\n\t}\n\treturn res, nil\n}\n\nfunc (s *session) ackAlarmChanged(t time.Time) {\n\ts.nextAckScheduledTime = t\n\ts.maybeResetTimer()\n}\n\nfunc (s *session) LocalAddr() net.Addr {\n\treturn s.conn.LocalAddr()\n}\n\n// RemoteAddr returns the net.Addr of the client\nfunc (s *session) RemoteAddr() net.Addr {\n\treturn s.conn.RemoteAddr()\n}\n", "idx": 1, "id": 5906, "msg": "", "proj": "lucas-clemente-quic-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -29,7 +29,7 @@ module BoltSpec\n       output = run_cli(arguments, **opts)\n \n       begin\n-        result = JSON.parse(output)\n+        result = JSON.parse(output, quirks_mode: true)\n       rescue JSON::ParserError\n         expect(output.string).to eq(\"Output should be JSON\")\n       end", "y": 1, "oldf": "module BoltSpec\n  module Integration\n    def run_cli(arguments, rescue_exec: false)\n      cli = Bolt::CLI.new(arguments)\n\n      # prevent tests from reading users config\n      allow(cli.config).to receive(:default_paths).and_return([File.join('.', 'path', 'does not exist')])\n      allow(Bolt::Inventory).to receive(:default_paths).and_return([File.join('.', 'path', 'does not exist')])\n      output =  StringIO.new\n      outputter = Bolt::Outputter::JSON.new(output)\n      allow(cli).to receive(:outputter).and_return(outputter)\n\n      opts = cli.parse\n\n      if rescue_exec\n        err_kls = error_support ? Bolt::Error : StandardError\n        begin\n          cli.execute(opts)\n        # rubocop:disable HandleExceptions\n        rescue err_kls\n        end\n      else\n        cli.execute(opts)\n      end\n      output.string\n    end\n\n    def run_cli_json(arguments, **opts)\n      output = run_cli(arguments, **opts)\n\n      begin\n        result = JSON.parse(output)\n      rescue JSON::ParserError\n        expect(output.string).to eq(\"Output should be JSON\")\n      end\n      result\n    end\n\n    def run_one_node(arguments)\n      result = run_cli_json(arguments)\n      if result['_error'] ||\n         (result['items'] && result['items'][0] && result['items'][0]['status'] != 'success')\n        expect(result).to eq(\"Should have succeed on node\" => true)\n      end\n      result['items'][0]['result']\n    end\n\n    def run_failed_node(arguments)\n      result = run_cli_json(arguments)\n      expect(result['_error'] || (result['items'] && result['items'][0] && result['items'][0]['status'] != 'success'))\n      result['items'][0]['result']\n    end\n\n    def error_support\n      minor = RUBY_VERSION.split('.')[1].to_i\n      minor >= 1\n    end\n  end\nend\n", "idx": 1, "id": 7775, "msg": "This is supposed to be the default in Ruby 2.0. Did it change later?", "proj": "puppetlabs-bolt", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -11,18 +11,14 @@ from py4j.protocol import Py4JJavaError\n \n import listenbrainz_spark\n from listenbrainz_spark import config, utils, path\n-from listenbrainz_spark.recommendations.candidate_sets import get_user_id\n from listenbrainz_spark.exceptions import SQLException, SparkSessionNotInitializedException, PathNotFoundException, \\\n     FileNotFetchedException, ViewNotRegisteredException\n-from listenbrainz_spark.recommendations.utils import save_html\n \n from flask import current_app\n-from pyspark.sql.functions import lit, col\n+from pyspark.sql.functions import col\n from pyspark.sql.utils import AnalysisException\n from pyspark.mllib.recommendation import MatrixFactorizationModel\n \n-# Recommendation HTML is generated if set to true.\n-SAVE_RECOMMENDATION_HTML = True\n \n def load_model(path):\n     \"\"\" Load best model from given path in HDFS.", "y": 0, "oldf": "import os\nimport sys\nimport time\nimport json\nimport uuid\nimport logging\nfrom time import time\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom py4j.protocol import Py4JJavaError\n\nimport listenbrainz_spark\nfrom listenbrainz_spark import config, utils, path\nfrom listenbrainz_spark.recommendations.candidate_sets import get_user_id\nfrom listenbrainz_spark.exceptions import SQLException, SparkSessionNotInitializedException, PathNotFoundException, \\\n    FileNotFetchedException, ViewNotRegisteredException\nfrom listenbrainz_spark.recommendations.utils import save_html\n\nfrom flask import current_app\nfrom pyspark.sql.functions import lit, col\nfrom pyspark.sql.utils import AnalysisException\nfrom pyspark.mllib.recommendation import MatrixFactorizationModel\n\n# Recommendation HTML is generated if set to true.\nSAVE_RECOMMENDATION_HTML = True\n\ndef load_model(path):\n    \"\"\" Load best model from given path in HDFS.\n\n        Args:\n            path (str): Path where best model is stored.\n    \"\"\"\n    return MatrixFactorizationModel.load(listenbrainz_spark.context, path)\n\ndef get_recommended_recordings(candidate_set, limit, recordings_df, model, mapped_listens):\n    \"\"\" Get list of recommended recordings from the candidate set\n\n        Args:\n            candidate_set (rdd): RDD with elements as:\n                [\n                    'user_id', 'recording_id'\n                ]\n            limit (int): Number of recommendations to be generated.\n            recordings_df (dataframe): Columns can be depicted as:\n                [\n                    'mb_recording_mbid', 'mb_artist_credit_id', 'recording_id'\n                ]\n            model (parquet): Best model after training.\n            mapped_listens (dataframe): Dataframe with all the columns/fields that a typical listen has.\n\n        Returns:\n            recommended_recordings (list): [\n                    ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                    ...\n                    ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                ]\n    \"\"\"\n    recommendations = model.predictAll(candidate_set).takeOrdered(limit, lambda product: -product.rating)\n    recommended_recording_ids = [(recommendations[i].product) for i in range(len(recommendations))]\n\n    df = recordings_df.select('mb_artist_credit_id', 'mb_recording_mbid') \\\n        .where(recordings_df.recording_id.isin(recommended_recording_ids))\n\n    # get the track_name and artist_name to make the HTML redable. This step will not be required when sending recommendations\n    # to lemmy since gids are enough to recognize the track.\n    recommendations_df = df.join(mapped_listens, ['mb_artist_credit_id', 'mb_recording_mbid']) \\\n        .select('msb_artist_credit_name_matchable', 'mb_artist_credit_id', 'mb_artist_credit_mbids', 'mb_recording_mbid',\n                'mb_release_mbid', 'track_name').distinct()\n\n    recommended_recordings = []\n    for row in recommendations_df.collect():\n        rec = (row.msb_artist_credit_name_matchable, row.mb_artist_credit_id, row.mb_artist_credit_mbids, row.mb_recording_mbid,\n               row.mb_release_mbid, row.track_name)\n        recommended_recordings.append(rec)\n    return recommended_recordings\n\ndef recommend_user(user_name, model, recordings_df, users_df, top_artists_candidate_set,\n    similar_artists_candidate_set, mapped_listens):\n    \"\"\" Get recommended recordings which belong to top artists and artists similar to top\n        artists listened to by the user.\n\n        Args:\n            user_name (str): User name of the user.\n            model: Best model after training.\n            recordings_df (dataframe): Columns can be depicted as:\n                [\n                    'track_name', 'recording_msid', 'artist_name', 'artist_msid', 'release_name',\n                    'release_msid', 'recording_id'\n                ]\n            users_df (dataframe): Dataframe containing user names and user ids.\n            top_artists_candidate_set (dataframe): Dataframe containing recording ids of top artists.\n            similar_artists_candidate_set (dataframe): Dataframe containing recording ids of similar artists.\n            mapped_listens (dataframe): Dataframe with all the columns/fields that a typical listen has.\n\n        Returns:\n            user_recommendations (dict): Dictionary can be depicted as:\n                {\n                    'top_artists_recordings': [\n                        ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                        ...\n                        ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                    ]\n                    'similar_artists_recordings' : [\n                        ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                        ...\n                        ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                        ]\n                }\n    \"\"\"\n    user_recommendations = defaultdict(dict)\n    user_id = get_user_id(users_df, user_name)\n\n    top_artists_recordings_df = top_artists_candidate_set.select('user_id', 'recording_id') \\\n        .where(col('user_id') == user_id)\n    top_artists_recordings_rdd = top_artists_recordings_df.rdd.map(lambda r: (r['user_id'], r['recording_id']))\n    top_artists_recommended_recordings = get_recommended_recordings(top_artists_recordings_rdd, config \\\n        .RECOMMENDATION_TOP_ARTIST_LIMIT, recordings_df, model, mapped_listens)\n    user_recommendations['top_artists_recordings'] = top_artists_recommended_recordings\n\n    similar_artists_recordings_df = similar_artists_candidate_set.select('user_id', 'recording_id') \\\n        .where(col('user_id') == user_id)\n    similar_artists_recordings_rdd = similar_artists_recordings_df.rdd.map(lambda r : (r['user_id'], r['recording_id']))\n    similar_artists_recommended_recordings = get_recommended_recordings(similar_artists_recordings_rdd,\n        config.RECOMMENDATION_SIMILAR_ARTIST_LIMIT, recordings_df, model,mapped_listens)\n    user_recommendations['similar_artists_recordings'] = similar_artists_recommended_recordings\n    return user_recommendations\n\ndef get_recommendations(user_names, recordings_df, model, users_df, top_artists_candidate_set,\n    similar_artists_candidate_set, mapped_listens):\n    \"\"\" Generate recommendations for users.\n\n        Args:\n            user_names (list): User name of users for whom recommendations shall be generated.\n            model: Best model after training.\n            recordings_df (dataframe): Columns can be depicted as:\n                [\n                    'mb_recording_gid', 'mb_artist_credit_id', 'recording_id'\n                ]\n            users_df (dataframe): Dataframe containing user names and user ids.\n            top_artists_candidate_set (dataframe): Dataframe containing recording ids of top artists.\n            similar_artists_candidate_set (dataframe): Dataframe containing recording ids of similar artists.\n            mapped_listens (dataframe): Dataframe with all the columns/fields that a typical listen has.\n\n        Returns:\n            recommendations (dict): Dictionary can be depicted as:\n                {\n                    'user_name 1': {\n                        'time': 'xx.xx',\n                        'top_artists_recordings': [\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                            ...\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                        ]\n                        'similar_artists_recordings' : [\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                            ...\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                        ]\n                    }\n                }\n    \"\"\"\n    recommendations = defaultdict(dict)\n    for user_name in user_names:\n        try:\n            t0 = time()\n            user_recommendations = recommend_user(user_name, model, recordings_df, users_df, top_artists_candidate_set,\n                similar_artists_candidate_set, mapped_listens)\n            user_recommendations['time'] = '{:.2f}'.format((time() - t0) / 60)\n            current_app.logger.info('Recommendations for \"{}\" generated'.format(user_name))\n            recommendations[user_name] = user_recommendations\n        except IndexError:\n            current_app.logger.error('{} is new/invalid user.'.format(user_name))\n    return recommendations\n\ndef get_recommendation_html(recommendations, time_, best_model_id, ti):\n    \"\"\" Prepare and save recommendation HTML.\n\n        Args:\n            time_ (dict): Dictionary containing execution time information, can be depicted as:\n                {\n                    'load_model' : '3.09',\n                    ...\n                }\n            best_model_id (str): Id of the model used for generating recommendations\n            ti (str): Seconds since epoch when the script was run.\n            recommendations (dict): Dictionary can be depicted as:\n                {\n                    'user_name 1': {\n                        'time': 'xx.xx',\n                        'top_artists_recordings': [\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                            ...\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                        ]\n                        'similar_artists_recordings' : [\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx'),\n                            ...\n                            ('xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx', 'xxx')\n                        ]\n                    }\n                }\n    \"\"\"\n    date = datetime.utcnow().strftime('%Y-%m-%d')\n    recommendation_html = 'Recommendation-{}-{}.html'.format(uuid.uuid4(), date)\n    column = ('MSB_ARTIST_CREDIT_NAME_MATCHABLE', 'MB_ARTIST_CREDIT_ID', 'MB_ARTIST_CREDIT_MBIDS', 'MB_RECORDING_MBID',\n              'MB_RELEASE_MBID', 'TRACK_NAME')\n    context = {\n        'recommendations' : recommendations,\n        'column' : column,\n        'total_time' : '{:.2f}'.format((time() - ti) / 3600),\n        'time' : time_,\n        'best_model' : best_model_id,\n    }\n    save_html(recommendation_html, context, 'recommend.html')\n\ndef main():\n    ti = time()\n    time_ = defaultdict(dict)\n    try:\n        listenbrainz_spark.init_spark_session('Recommendations')\n    except SparkSessionNotInitializedException as err:\n        current_app.logger.error(str(err), exc_info=True)\n        sys.exit(-1)\n\n    try:\n        users_df = utils.read_files_from_HDFS(path.USERS_DATAFRAME_PATH)\n        recordings_df = utils.read_files_from_HDFS(path.RECORDINGS_DATAFRAME_PATH)\n\n        top_artists_candidate_set = utils.read_files_from_HDFS(path.TOP_ARTIST_CANDIDATE_SET)\n        similar_artists_candidate_set = utils.read_files_from_HDFS(path.SIMILAR_ARTIST_CANDIDATE_SET)\n        mapped_listens = utils.read_files_from_HDFS(path.MAPPED_LISTENS)\n    except PathNotFoundException as err:\n        current_app.logger.error(str(err), exc_info=True)\n        sys.exit(-1)\n    except FileNotFetchedException as err:\n        current_app.logger.error(str(err), exc_info=True)\n        sys.exit(-1)\n\n    metadata_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'recommendation-metadata.json')\n    with open(metadata_file_path, 'r') as f:\n        recommendation_metadata = json.load(f)\n        best_model_id = recommendation_metadata['best_model_id']\n        user_names = recommendation_metadata['user_name']\n\n    best_model_path = path.DATA_DIR + '/' + best_model_id\n\n    current_app.logger.info('Loading model...')\n    t0 = time()\n    try:\n        model = load_model(config.HDFS_CLUSTER_URI + best_model_path)\n    except Py4JJavaError as err:\n        current_app.logger.error('Unable to load model \"{}\"\\n{}\\nAborting...'.format(best_model_id, str(err.java_exception)),\n            exc_info=True)\n        sys.exit(-1)\n    time_['load_model'] = '{:.2f}'.format((time() - t0) / 60)\n\n    # an action must be called to persist data in memory\n    recordings_df.count()\n    recordings_df.persist()\n\n    t0 = time()\n    recommendations = get_recommendations(user_names, recordings_df, model, users_df, top_artists_candidate_set,\n        similar_artists_candidate_set, mapped_listens)\n    time_['total_recommendation_time'] = '{:.2f}'.format((time() - t0) / 3600)\n\n    # persisted data must be cleared from memory after usage to avoid OOM\n    recordings_df.unpersist()\n\n    if SAVE_RECOMMENDATION_HTML:\n        get_recommendation_html(recommendations, time_, best_model_id, ti)\n", "idx": 2, "id": 16134, "msg": "", "proj": "metabrainz-listenbrainz-server", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -85,10 +85,11 @@ type endpointManager struct {\n \t// hostEndpointsDirty is set to true when host endpoints are updated.\n \thostEndpointsDirty bool\n \t// activeHostIfaceToChains maps host interface name to the chains that we've programmed.\n-\tactiveHostIfaceToChains map[string][]*iptables.Chain\n-\t// activeHostDispatchChains contains the dispatch chains that we've programmed for host\n-\t// endpoints.\n-\tactiveHostDispatchChains []*iptables.Chain\n+\tactiveHostIfaceToRawChains  map[string][]*iptables.Chain\n+\tactiveHostIfaceToFiltChains map[string][]*iptables.Chain\n+\t// Dispatch chains that we've programmed for host endpoints.\n+\tactiveHostRawDispatchChains  []*iptables.Chain\n+\tactiveHostFiltDispatchChains []*iptables.Chain\n \t// activeHostEpIDToIfaceNames records which interfaces we resolved each host endpoint to.\n \tactiveHostEpIDToIfaceNames map[proto.HostEndpointID][]string\n \t// activeIfaceNameToHostEpID records which endpoint we resolved each host interface to.", "y": 0, "oldf": "// Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage intdataplane\n\nimport (\n\t\"fmt\"\n\tlog \"github.com/Sirupsen/logrus\"\n\t\"github.com/projectcalico/felix/go/felix/ifacemonitor\"\n\t\"github.com/projectcalico/felix/go/felix/ip\"\n\t\"github.com/projectcalico/felix/go/felix/iptables\"\n\t\"github.com/projectcalico/felix/go/felix/proto\"\n\t\"github.com/projectcalico/felix/go/felix/routetable\"\n\t\"github.com/projectcalico/felix/go/felix/rules\"\n\t\"github.com/projectcalico/felix/go/felix/set\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strings\"\n)\n\ntype routeTable interface {\n\tSetRoutes(ifaceName string, targets []routetable.Target)\n}\n\n// endpointManager manages the dataplane resources that belong to each endpoint as well as\n// the \"dispatch chains\" that fan out packets to the right per-endpoint chain.\n//\n// It programs the relevant iptables chains (via the iptables.Table objects) along with\n// per-endpoint routes (via the RouteTable).\n//\n// Since calculating the dispatch chains is fairly expensive, the main OnUpdate method\n// simply records the pending state of each interface and defers the actual calculation\n// to CompleteDeferredWork().  This is also the basis of our failure handling; updates\n// that fail are left in the pending state so they can be retried later.\ntype endpointManager struct {\n\t// Config.\n\tipVersion      uint8\n\twlIfacesRegexp *regexp.Regexp\n\n\t// Our dependencies.\n\tfilterTable  iptablesTable\n\truleRenderer rules.RuleRenderer\n\trouteTable   routeTable\n\twriteProcSys procSysWriter\n\n\t// Pending updates, cleared in CompleteDeferredWork as the data is copied to the activeXYZ\n\t// fields.\n\tpendingWlEpUpdates  map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint\n\tpendingIfaceUpdates map[string]ifacemonitor.State\n\n\t// Active state, updated in CompleteDeferredWork.\n\tactiveWlEndpoints     map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint\n\tactiveWlIfaceNameToID map[string]proto.WorkloadEndpointID\n\tactiveUpIfaces        set.Set\n\tactiveWlIDToChains    map[proto.WorkloadEndpointID][]*iptables.Chain\n\tactiveDispatchChains  []*iptables.Chain\n\n\t// wlIfaceNamesToReconfigure contains names of workload interfaces that need to have\n\t// their configuration (sysctls etc.) refreshed.\n\twlIfaceNamesToReconfigure set.Set\n\n\t// epIDsToUpdateStatus contains IDs of endpoints that we need to report status for.\n\t// Mix of host and workload endpoint IDs.\n\tepIDsToUpdateStatus set.Set\n\n\t// hostIfaceToAddrs maps host interface name to the set of IPs on that interface (reported\n\t// fro the dataplane).\n\thostIfaceToAddrs map[string]set.Set\n\t// rawHostEndpoints contains the raw (i.e. not resolved to interface) host endpoints.\n\trawHostEndpoints map[proto.HostEndpointID]*proto.HostEndpoint\n\t// hostEndpointsDirty is set to true when host endpoints are updated.\n\thostEndpointsDirty bool\n\t// activeHostIfaceToChains maps host interface name to the chains that we've programmed.\n\tactiveHostIfaceToChains map[string][]*iptables.Chain\n\t// activeHostDispatchChains contains the dispatch chains that we've programmed for host\n\t// endpoints.\n\tactiveHostDispatchChains []*iptables.Chain\n\t// activeHostEpIDToIfaceNames records which interfaces we resolved each host endpoint to.\n\tactiveHostEpIDToIfaceNames map[proto.HostEndpointID][]string\n\t// activeIfaceNameToHostEpID records which endpoint we resolved each host interface to.\n\tactiveIfaceNameToHostEpID map[string]proto.HostEndpointID\n\n\t// Callbacks\n\tOnEndpointStatusUpdate EndpointStatusUpdateCallback\n}\n\ntype EndpointStatusUpdateCallback func(ipVersion uint8, id interface{}, status string)\n\ntype procSysWriter func(path, value string) error\n\nfunc newEndpointManager(\n\tfilterTable iptablesTable,\n\truleRenderer rules.RuleRenderer,\n\trouteTable routeTable,\n\tipVersion uint8,\n\twlInterfacePrefixes []string,\n\tonWorkloadEndpointStatusUpdate EndpointStatusUpdateCallback,\n) *endpointManager {\n\treturn newEndpointManagerWithShims(\n\t\tfilterTable,\n\t\truleRenderer,\n\t\trouteTable,\n\t\tipVersion,\n\t\twlInterfacePrefixes,\n\t\tonWorkloadEndpointStatusUpdate,\n\t\twriteProcSys,\n\t)\n}\n\nfunc newEndpointManagerWithShims(\n\tfilterTable iptablesTable,\n\truleRenderer rules.RuleRenderer,\n\trouteTable routeTable,\n\tipVersion uint8,\n\twlInterfacePrefixes []string,\n\tonWorkloadEndpointStatusUpdate EndpointStatusUpdateCallback,\n\tprocSysWriter procSysWriter,\n) *endpointManager {\n\twlIfacesPattern := \"^(\" + strings.Join(wlInterfacePrefixes, \"|\") + \").*\"\n\twlIfacesRegexp := regexp.MustCompile(wlIfacesPattern)\n\n\treturn &endpointManager{\n\t\tipVersion:      ipVersion,\n\t\twlIfacesRegexp: wlIfacesRegexp,\n\n\t\tfilterTable:  filterTable,\n\t\truleRenderer: ruleRenderer,\n\t\trouteTable:   routeTable,\n\t\twriteProcSys: procSysWriter,\n\n\t\t// Pending updates, we store these up as OnUpdate is called, then process them\n\t\t// in CompleteDeferredWork and transfer the important data to the activeXYX fields.\n\t\tpendingWlEpUpdates:  map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint{},\n\t\tpendingIfaceUpdates: map[string]ifacemonitor.State{},\n\n\t\tactiveUpIfaces: set.New(),\n\n\t\tactiveWlEndpoints:     map[proto.WorkloadEndpointID]*proto.WorkloadEndpoint{},\n\t\tactiveWlIfaceNameToID: map[string]proto.WorkloadEndpointID{},\n\t\tactiveWlIDToChains:    map[proto.WorkloadEndpointID][]*iptables.Chain{},\n\n\t\twlIfaceNamesToReconfigure: set.New(),\n\n\t\tepIDsToUpdateStatus: set.New(),\n\n\t\thostIfaceToAddrs:   map[string]set.Set{},\n\t\trawHostEndpoints:   map[proto.HostEndpointID]*proto.HostEndpoint{},\n\t\thostEndpointsDirty: true,\n\n\t\tactiveHostIfaceToChains:  map[string][]*iptables.Chain{},\n\t\tactiveHostDispatchChains: nil,\n\n\t\tOnEndpointStatusUpdate: onWorkloadEndpointStatusUpdate,\n\t}\n}\n\nfunc (m *endpointManager) OnUpdate(protoBufMsg interface{}) {\n\tswitch msg := protoBufMsg.(type) {\n\tcase *proto.WorkloadEndpointUpdate:\n\t\tm.pendingWlEpUpdates[*msg.Id] = msg.Endpoint\n\tcase *proto.WorkloadEndpointRemove:\n\t\tm.pendingWlEpUpdates[*msg.Id] = nil\n\tcase *proto.HostEndpointUpdate:\n\t\tlog.WithField(\"msg\", msg).Debug(\"Host endpoint update\")\n\t\tm.rawHostEndpoints[*msg.Id] = msg.Endpoint\n\t\tm.hostEndpointsDirty = true\n\t\tm.epIDsToUpdateStatus.Add(*msg.Id)\n\tcase *proto.HostEndpointRemove:\n\t\tlog.WithField(\"msg\", msg).Debug(\"Host endpoint removed\")\n\t\tdelete(m.rawHostEndpoints, *msg.Id)\n\t\tm.hostEndpointsDirty = true\n\t\tm.epIDsToUpdateStatus.Add(*msg.Id)\n\tcase *ifaceUpdate:\n\t\tlog.WithField(\"update\", msg).Debug(\"Interface state changed.\")\n\t\tm.pendingIfaceUpdates[msg.Name] = msg.State\n\tcase *ifaceAddrsUpdate:\n\t\tlog.WithField(\"update\", msg).Debug(\"Interface addrs changed.\")\n\t\tif m.wlIfacesRegexp.MatchString(msg.Name) {\n\t\t\tlog.WithField(\"update\", msg).Debug(\"Workload interface, ignoring.\")\n\t\t\treturn\n\t\t}\n\t\tif msg.Addrs != nil {\n\t\t\tm.hostIfaceToAddrs[msg.Name] = msg.Addrs\n\t\t} else {\n\t\t\tdelete(m.hostIfaceToAddrs, msg.Name)\n\t\t}\n\t\tm.hostEndpointsDirty = true\n\t}\n}\n\nfunc (m *endpointManager) CompleteDeferredWork() error {\n\t// Copy the pending interface state to the active set and mark any interfaces that have\n\t// changed state for reconfiguration by resolveWorkload/HostEndpoints()\n\tfor ifaceName, state := range m.pendingIfaceUpdates {\n\t\tif state == ifacemonitor.StateUp {\n\t\t\tm.activeUpIfaces.Add(ifaceName)\n\t\t\tif m.wlIfacesRegexp.MatchString(ifaceName) {\n\t\t\t\tlog.WithField(\"ifaceName\", ifaceName).Info(\n\t\t\t\t\t\"Workload interface came up, marking for reconfiguration.\")\n\t\t\t\tm.wlIfaceNamesToReconfigure.Add(ifaceName)\n\t\t\t}\n\t\t} else {\n\t\t\tm.activeUpIfaces.Discard(ifaceName)\n\t\t}\n\t\t// If this interface is linked to any already-existing endpoints, mark the endpoint\n\t\t// status for recalculation.  If the matching endpoint changes when we do\n\t\t// resolveHostEndpoints() then that will mark old and new matching endpoints for\n\t\t// update.\n\t\tm.markEndpointStatusDirtyByIface(ifaceName)\n\t\t// Clean up as we go...\n\t\tdelete(m.pendingIfaceUpdates, ifaceName)\n\t}\n\n\tm.resolveWorkloadEndpoints()\n\n\tif m.hostEndpointsDirty {\n\t\tlog.Debug(\"Host endpoints updated, resolving them.\")\n\t\tm.resolveHostEndpoints()\n\t\tm.hostEndpointsDirty = false\n\t}\n\n\t// Now send any endpoint status updates.\n\tm.updateEndpointStatuses()\n\n\treturn nil\n}\n\nfunc (m *endpointManager) markEndpointStatusDirtyByIface(ifaceName string) {\n\tlogCxt := log.WithField(\"ifaceName\", ifaceName)\n\tif epID, ok := m.activeWlIfaceNameToID[ifaceName]; ok {\n\t\tlogCxt.Info(\"Workload interface state changed; marking for status update.\")\n\t\tm.epIDsToUpdateStatus.Add(epID)\n\t} else if epID, ok := m.activeIfaceNameToHostEpID[ifaceName]; ok {\n\t\tlogCxt.Info(\"Host interface state changed; marking for status update.\")\n\t\tm.epIDsToUpdateStatus.Add(epID)\n\t} else {\n\t\t// We don't know about this interface yet (or it's already been deleted).\n\t\t// If the endpoint gets created, we'll do the update then. If it's been\n\t\t// deleted, we've already cleaned it up.\n\t\tlogCxt.Debug(\"Ignoring interface state change for unknown interface.\")\n\t}\n}\n\nfunc (m *endpointManager) updateEndpointStatuses() {\n\tlog.WithField(\"dirtyEndpoints\", m.epIDsToUpdateStatus).Debug(\"Reporting endpoint status.\")\n\tm.epIDsToUpdateStatus.Iter(func(item interface{}) error {\n\t\tswitch id := item.(type) {\n\t\tcase proto.WorkloadEndpointID:\n\t\t\tstatus := m.calculateWorkloadEndpointStatus(id)\n\t\t\tm.OnEndpointStatusUpdate(m.ipVersion, id, status)\n\t\tcase proto.HostEndpointID:\n\t\t\tstatus := m.calculateHostEndpointStatus(id)\n\t\t\tm.OnEndpointStatusUpdate(m.ipVersion, id, status)\n\t\t}\n\n\t\treturn set.RemoveItem\n\t})\n}\n\nfunc (m *endpointManager) calculateWorkloadEndpointStatus(id proto.WorkloadEndpointID) string {\n\tlogCxt := log.WithField(\"workloadEndpointID\", id)\n\tlogCxt.Debug(\"Re-evaluating workload endpoint status\")\n\tvar operUp, adminUp, failed bool\n\tworkload, known := m.activeWlEndpoints[id]\n\tif known {\n\t\tadminUp = workload.State == \"active\"\n\t\toperUp = m.activeUpIfaces.Contains(workload.Name)\n\t\tfailed = m.wlIfaceNamesToReconfigure.Contains(workload.Name)\n\t}\n\n\t// Note: if endpoint is not known (i.e. has been deleted), status will be \"\", which signals\n\t// a deletion.\n\tvar status string\n\tif known {\n\t\tif failed {\n\t\t\tstatus = \"error\"\n\t\t} else if operUp && adminUp {\n\t\t\tstatus = \"up\"\n\t\t} else {\n\t\t\tstatus = \"down\"\n\t\t}\n\t}\n\tlogCxt = logCxt.WithFields(log.Fields{\n\t\t\"known\":   known,\n\t\t\"failed\":  failed,\n\t\t\"operUp\":  operUp,\n\t\t\"adminUp\": adminUp,\n\t\t\"status\":  status,\n\t})\n\tlogCxt.Info(\"Re-evaluated workload endpoint status\")\n\treturn status\n}\n\nfunc (m *endpointManager) calculateHostEndpointStatus(id proto.HostEndpointID) (status string) {\n\tlogCxt := log.WithField(\"hostEndpointID\", id)\n\tlogCxt.Debug(\"Re-evaluating host endpoint status\")\n\tvar resolved, operUp bool\n\t_, known := m.rawHostEndpoints[id]\n\n\t// Note: if endpoint is not known (i.e. has been deleted), status will be \"\", which signals\n\t// a deletion.\n\tif known {\n\t\tifaceNames := m.activeHostEpIDToIfaceNames[id]\n\t\tif len(ifaceNames) > 0 {\n\t\t\tresolved = true\n\t\t\toperUp = true\n\t\t\tfor _, ifaceName := range ifaceNames {\n\t\t\t\tifaceUp := m.activeUpIfaces.Contains(ifaceName)\n\t\t\t\tlogCxt.WithFields(log.Fields{\n\t\t\t\t\t\"ifaceName\": ifaceName,\n\t\t\t\t\t\"ifaceUp\":   ifaceUp,\n\t\t\t\t}).Debug(\"Status of matching interface.\")\n\t\t\t\toperUp = operUp && ifaceUp\n\t\t\t}\n\t\t}\n\n\t\tif resolved && operUp {\n\t\t\tstatus = \"up\"\n\t\t} else if resolved {\n\t\t\tstatus = \"down\"\n\t\t} else {\n\t\t\t// Known but failed to resolve, map that to error.\n\t\t\tstatus = \"error\"\n\t\t}\n\t}\n\n\tlogCxt = logCxt.WithFields(log.Fields{\n\t\t\"known\":    known,\n\t\t\"resolved\": resolved,\n\t\t\"operUp\":   operUp,\n\t\t\"status\":   status,\n\t})\n\tlogCxt.Info(\"Re-evaluated host endpoint status\")\n\treturn status\n}\n\nfunc (m *endpointManager) resolveWorkloadEndpoints() {\n\t// Optimisation, only recalculate the dispatch chains if we've never done so or if the\n\t// workloads have changed in some way.\n\tneedToCheckDispatchChains := m.activeDispatchChains == nil || len(m.pendingWlEpUpdates) > 0\n\n\t// Update any dirty endpoints.\n\tfor id, workload := range m.pendingWlEpUpdates {\n\t\tlogCxt := log.WithField(\"id\", id)\n\t\toldWorkload := m.activeWlEndpoints[id]\n\t\tif workload != nil {\n\t\t\tlogCxt.Info(\"Updating per-endpoint chains.\")\n\t\t\tif oldWorkload != nil && oldWorkload.Name != workload.Name {\n\t\t\t\tlogCxt.Debug(\"Interface name changed, cleaning up old state\")\n\t\t\t\tm.filterTable.RemoveChains(m.activeWlIDToChains[id])\n\t\t\t\tm.routeTable.SetRoutes(oldWorkload.Name, nil)\n\t\t\t\tm.wlIfaceNamesToReconfigure.Discard(oldWorkload.Name)\n\t\t\t\tdelete(m.activeWlIfaceNameToID, oldWorkload.Name)\n\t\t\t}\n\t\t\tchains := m.ruleRenderer.WorkloadEndpointToIptablesChains(&id, workload)\n\t\t\tm.filterTable.UpdateChains(chains)\n\t\t\tm.activeWlIDToChains[id] = chains\n\n\t\t\t// Collect the IP prefixes that we want to route locally to this endpoint:\n\t\t\tlogCxt.Info(\"Updating endpoint routes.\")\n\t\t\tvar (\n\t\t\t\tipStrings  []string\n\t\t\t\tnatInfos   []*proto.NatInfo\n\t\t\t\taddrSuffix string\n\t\t\t)\n\t\t\tif m.ipVersion == 4 {\n\t\t\t\tipStrings = workload.Ipv4Nets\n\t\t\t\tnatInfos = workload.Ipv4Nat\n\t\t\t\taddrSuffix = \"/32\"\n\t\t\t} else {\n\t\t\t\tipStrings = workload.Ipv6Nets\n\t\t\t\tnatInfos = workload.Ipv6Nat\n\t\t\t\taddrSuffix = \"/128\"\n\t\t\t}\n\t\t\tif len(natInfos) != 0 {\n\t\t\t\told := ipStrings\n\t\t\t\tipStrings = make([]string, len(old)+len(natInfos))\n\t\t\t\tcopy(ipStrings, old)\n\t\t\t\tfor ii, natInfo := range natInfos {\n\t\t\t\t\tipStrings[len(old)+ii] = natInfo.ExtIp + addrSuffix\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar mac net.HardwareAddr\n\t\t\tif workload.Mac != \"\" {\n\t\t\t\tvar err error\n\t\t\t\tmac, err = net.ParseMAC(workload.Mac)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogCxt.WithError(err).Error(\n\t\t\t\t\t\t\"Failed to parse endpoint's MAC address\")\n\t\t\t\t}\n\t\t\t}\n\t\t\trouteTargets := make([]routetable.Target, len(ipStrings))\n\t\t\tfor i, s := range ipStrings {\n\t\t\t\trouteTargets[i] = routetable.Target{\n\t\t\t\t\tCIDR:    ip.MustParseCIDR(s),\n\t\t\t\t\tDestMAC: mac,\n\t\t\t\t}\n\t\t\t}\n\t\t\tm.routeTable.SetRoutes(workload.Name, routeTargets)\n\t\t\tm.wlIfaceNamesToReconfigure.Add(workload.Name)\n\t\t\tm.activeWlEndpoints[id] = workload\n\t\t\tm.activeWlIfaceNameToID[workload.Name] = id\n\t\t\tdelete(m.pendingWlEpUpdates, id)\n\t\t} else {\n\t\t\tlogCxt.Info(\"Workload removed, deleting its chains.\")\n\t\t\tm.filterTable.RemoveChains(m.activeWlIDToChains[id])\n\t\t\tif oldWorkload != nil {\n\t\t\t\t// Remove any routes from the routing table.  The RouteTable will\n\t\t\t\t// remove any conntrack entries as a side-effect.\n\t\t\t\tlogCxt.Info(\"Workload removed, deleting old state.\")\n\t\t\t\tm.routeTable.SetRoutes(oldWorkload.Name, nil)\n\t\t\t\tm.wlIfaceNamesToReconfigure.Discard(oldWorkload.Name)\n\t\t\t\tdelete(m.activeWlIfaceNameToID, oldWorkload.Name)\n\t\t\t}\n\t\t\tdelete(m.activeWlEndpoints, id)\n\t\t\tdelete(m.pendingWlEpUpdates, id)\n\t\t}\n\n\t\t// Update or deletion, make sure we update the interface status.\n\t\tm.epIDsToUpdateStatus.Add(id)\n\t}\n\n\tif needToCheckDispatchChains {\n\t\t// Rewrite the dispatch chains if they've changed.\n\t\tnewDispatchChains := m.ruleRenderer.WorkloadDispatchChains(m.activeWlEndpoints)\n\t\tif !reflect.DeepEqual(newDispatchChains, m.activeDispatchChains) {\n\t\t\tlog.Info(\"Workloads changed, updating dispatch chains.\")\n\t\t\tm.filterTable.RemoveChains(m.activeDispatchChains)\n\t\t\tm.filterTable.UpdateChains(newDispatchChains)\n\t\t\tm.activeDispatchChains = newDispatchChains\n\t\t}\n\t}\n\n\tm.wlIfaceNamesToReconfigure.Iter(func(item interface{}) error {\n\t\tifaceName := item.(string)\n\t\terr := m.configureInterface(ifaceName)\n\t\tif err != nil {\n\t\t\tlog.WithError(err).Warn(\"Failed to configure interface, will retry\")\n\t\t\treturn nil\n\t\t}\n\t\treturn set.RemoveItem\n\t})\n}\n\nfunc (m *endpointManager) resolveHostEndpoints() {\n\n\t// Host endpoint resolution\n\t// ------------------------\n\t//\n\t// There is a set of non-workload interfaces on the local host, each possibly with\n\t// IP addresses, that might be controlled by HostEndpoint resources in the Calico\n\t// data model.  The data model syntactically allows multiple HostEndpoint\n\t// resources to match a given interface - for example, an interface 'eth1' might\n\t// have address 10.240.0.34 and 172.19.2.98, and the data model might include:\n\t//\n\t// - HostEndpoint A with Name 'eth1'\n\t//\n\t// - HostEndpoint B with ExpectedIpv4Addrs including '10.240.0.34'\n\t//\n\t// - HostEndpoint C with ExpectedIpv4Addrs including '172.19.2.98'.\n\t//\n\t// but at runtime, at any given time, we only allow one HostEndpoint to govern\n\t// that interface.  That HostEndpoint becomes the active one, and the others\n\t// remain inactive.  (But if, for example, the active HostEndpoint resource was\n\t// deleted, then one of the inactive ones could take over.)  Given multiple\n\t// matching HostEndpoint resources, the one that wins is the one with the\n\t// alphabetically earliest HostEndpointId\n\t//\n\t// So the process here is not about 'resolving' a particular HostEndpoint on its\n\t// own.  Rather it is looking at the set of local non-workload interfaces and\n\t// seeing which of them are matched by the current set of HostEndpoints as a\n\t// whole.\n\tnewIfaceNameToHostEpID := map[string]proto.HostEndpointID{}\n\tnewHostEpIDToIfaceNames := map[proto.HostEndpointID][]string{}\n\tfor ifaceName, ifaceAddrs := range m.hostIfaceToAddrs {\n\t\tifaceCxt := log.WithFields(log.Fields{\n\t\t\t\"ifaceName\":  ifaceName,\n\t\t\t\"ifaceAddrs\": ifaceAddrs,\n\t\t})\n\t\tbestHostEpId := proto.HostEndpointID{}\n\tHostEpLoop:\n\t\tfor id, hostEp := range m.rawHostEndpoints {\n\t\t\tlogCxt := ifaceCxt.WithField(\"id\", id)\n\t\t\tlogCxt.WithField(\"bestHostEpId\", bestHostEpId).Debug(\"See if HostEp matches interface\")\n\t\t\tif (bestHostEpId.EndpointId != \"\") && (bestHostEpId.EndpointId < id.EndpointId) {\n\t\t\t\t// We already have a HostEndpointId that is better than\n\t\t\t\t// this one, so no point looking any further.\n\t\t\t\tlogCxt.Debug(\"No better than existing match\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif hostEp.Name == ifaceName {\n\t\t\t\t// The HostEndpoint has an explicit name that matches the\n\t\t\t\t// interface.\n\t\t\t\tlogCxt.Debug(\"Match on explicit iface name\")\n\t\t\t\tbestHostEpId = id\n\t\t\t\tcontinue\n\t\t\t} else if hostEp.Name != \"\" {\n\t\t\t\t// The HostEndpoint has an explicit name that isn't this\n\t\t\t\t// interface.  Continue, so as not to allow it to match on\n\t\t\t\t// an IP address instead.\n\t\t\t\tlogCxt.Debug(\"Rejected on explicit iface name\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, wantedList := range [][]string{hostEp.ExpectedIpv4Addrs, hostEp.ExpectedIpv6Addrs} {\n\t\t\t\tfor _, wanted := range wantedList {\n\t\t\t\t\tlogCxt.WithField(\"wanted\", wanted).Debug(\"Address wanted by HostEp\")\n\t\t\t\t\tif ifaceAddrs.Contains(wanted) {\n\t\t\t\t\t\t// The HostEndpoint expects an IP address\n\t\t\t\t\t\t// that is on this interface.\n\t\t\t\t\t\tlogCxt.Debug(\"Match on address\")\n\t\t\t\t\t\tbestHostEpId = id\n\t\t\t\t\t\tcontinue HostEpLoop\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif bestHostEpId.EndpointId != \"\" {\n\t\t\tlog.WithFields(log.Fields{\n\t\t\t\t\"ifaceName\":    ifaceName,\n\t\t\t\t\"bestHostEpId\": bestHostEpId,\n\t\t\t}).Debug(\"Got HostEp for interface\")\n\t\t\tnewIfaceNameToHostEpID[ifaceName] = bestHostEpId\n\t\t\tnewHostEpIDToIfaceNames[bestHostEpId] = append(\n\t\t\t\tnewHostEpIDToIfaceNames[bestHostEpId], ifaceName)\n\t\t}\n\n\t\toldID, wasKnown := m.activeIfaceNameToHostEpID[ifaceName]\n\t\tnewID, isKnown := newIfaceNameToHostEpID[ifaceName]\n\t\tif oldID != newID {\n\t\t\tlogCxt := ifaceCxt.WithFields(log.Fields{\n\t\t\t\t\"oldID\": m.activeIfaceNameToHostEpID[ifaceName],\n\t\t\t\t\"newID\": newIfaceNameToHostEpID[ifaceName],\n\t\t\t})\n\t\t\tlogCxt.Info(\"Endpoint matching interface changed\")\n\t\t\tif wasKnown {\n\t\t\t\tlogCxt.Debug(\"Endpoint was known, updating old endpoint status\")\n\t\t\t\tm.epIDsToUpdateStatus.Add(oldID)\n\t\t\t}\n\t\t\tif isKnown {\n\t\t\t\tlogCxt.Debug(\"Endpoint is known, updating new endpoint status\")\n\t\t\t\tm.epIDsToUpdateStatus.Add(newID)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Set up programming for the host endpoints that are now to be used.\n\tnewHostIfaceChains := map[string][]*iptables.Chain{}\n\tfor ifaceName, id := range newIfaceNameToHostEpID {\n\t\tlog.WithField(\"id\", id).Info(\"Updating host endpoint chains.\")\n\t\thostEp := m.rawHostEndpoints[id]\n\t\tchains := m.ruleRenderer.HostEndpointToIptablesChains(ifaceName, hostEp)\n\t\tif !reflect.DeepEqual(chains, m.activeHostIfaceToChains[ifaceName]) {\n\t\t\tm.filterTable.UpdateChains(chains)\n\t\t}\n\t\tnewHostIfaceChains[ifaceName] = chains\n\t\tdelete(m.activeHostIfaceToChains, ifaceName)\n\t}\n\n\t// Remove programming for host endpoints that are not now in use.\n\tfor ifaceName, chains := range m.activeHostIfaceToChains {\n\t\tlog.WithField(\"ifaceName\", ifaceName).Info(\"Host interface no longer protected, deleting its chains.\")\n\t\tm.filterTable.RemoveChains(chains)\n\t}\n\n\t// Remember the host endpoints that are now in use.\n\tm.activeIfaceNameToHostEpID = newIfaceNameToHostEpID\n\tm.activeHostEpIDToIfaceNames = newHostEpIDToIfaceNames\n\tm.activeHostIfaceToChains = newHostIfaceChains\n\n\t// Rewrite the dispatch chains if they've changed.\n\tlog.WithField(\"resolvedHostEpIds\", newIfaceNameToHostEpID).Debug(\"Rewrite dispatch chains?\")\n\tnewDispatchChains := m.ruleRenderer.HostDispatchChains(newIfaceNameToHostEpID)\n\tif !reflect.DeepEqual(newDispatchChains, m.activeHostDispatchChains) {\n\t\tlog.Info(\"HostEps changed, updating dispatch chains.\")\n\t\tm.filterTable.RemoveChains(m.activeHostDispatchChains)\n\t\tm.filterTable.UpdateChains(newDispatchChains)\n\t\tm.activeHostDispatchChains = newDispatchChains\n\t}\n}\n\nfunc (m *endpointManager) configureInterface(name string) error {\n\tif !m.activeUpIfaces.Contains(name) {\n\t\tlog.WithField(\"ifaceName\", name).Info(\n\t\t\t\"Skipping configuration of interface because it is oper down.\")\n\t\treturn nil\n\t}\n\tlog.WithField(\"ifaceName\", name).Info(\n\t\t\"Applying /proc/sys configuration to interface.\")\n\tif m.ipVersion == 4 {\n\t\terr := m.writeProcSys(fmt.Sprintf(\"/proc/sys/net/ipv4/conf/%s/rp_filter\", name), \"1\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = m.writeProcSys(fmt.Sprintf(\"/proc/sys/net/ipv4/conf/%s/route_localnet\", name), \"1\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = m.writeProcSys(fmt.Sprintf(\"/proc/sys/net/ipv4/conf/%s/proxy_arp\", name), \"1\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = m.writeProcSys(fmt.Sprintf(\"/proc/sys/net/ipv4/neigh/%s/proxy_delay\", name), \"0\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\terr := m.writeProcSys(fmt.Sprintf(\"/proc/sys/net/ipv6/conf/%s/proxy_ndp\", name), \"1\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc writeProcSys(path, value string) error {\n\tf, err := os.OpenFile(path, os.O_WRONLY, 0)\n\tif err != nil {\n\t\treturn err\n\t}\n\tn, err := f.Write([]byte(value))\n\tif err == nil && n < len(value) {\n\t\terr = io.ErrShortWrite\n\t}\n\tif err1 := f.Close(); err == nil {\n\t\terr = err1\n\t}\n\treturn err\n}\n", "idx": 2, "id": 14973, "msg": "", "proj": "projectcalico-felix", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -715,11 +715,31 @@ func testUpdate(t *testing.T, coll *ds.Collection, revField string) {\n \t\t\t\t\"a\": \"X\",\n \t\t\t\t\"b\": nil,\n \t\t\t\t\"c\": \"C\",\n+\t\t\t\t\"n\": docstore.Increment(-1),\n+\t\t\t\t\"i\": nil,\n+\t\t\t\t\"m\": 3,\n+\t\t\t},\n+\t\t\twant: docmap{KeyField: \"testUpdateMap\", \"a\": \"X\", \"c\": \"C\", \"n\": 2.5, \"m\": int64(3)},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"update map overwrite only\",\n+\t\t\tdoc:  docmap{KeyField: \"testUpdateMapWrt\", \"a\": \"A\", revField: nil},\n+\t\t\tmods: ds.Mods{\n+\t\t\t\t\"a\": \"X\",\n+\t\t\t\t\"b\": nil,\n+\t\t\t\t\"m\": 3,\n+\t\t\t},\n+\t\t\twant: docmap{KeyField: \"testUpdateMapWrt\", \"a\": \"X\", \"m\": int64(3)},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"update map increment only\",\n+\t\t\tdoc:  docmap{KeyField: \"testUpdateMapInc\", \"a\": \"A\", \"n\": 3.5, \"i\": 1, revField: nil},\n+\t\t\tmods: ds.Mods{\n \t\t\t\t\"n\": docstore.Increment(-1),\n \t\t\t\t\"i\": docstore.Increment(2.5),\n \t\t\t\t\"m\": docstore.Increment(3),\n \t\t\t},\n-\t\t\twant: docmap{KeyField: \"testUpdateMap\", \"a\": \"X\", \"c\": \"C\", \"n\": 2.5, \"i\": 3.5, \"m\": int64(3)},\n+\t\t\twant: docmap{KeyField: \"testUpdateMapInc\", \"a\": \"A\", \"n\": 2.5, \"i\": 3.5, \"m\": int64(3)},\n \t\t},\n \t\t{\n \t\t\tname: \"update struct\",", "y": 0, "oldf": "// Copyright 2019 The Go Cloud Development Kit Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package drivertest provides a conformance test for implementations of\n// driver.\npackage drivertest // import \"gocloud.dev/docstore/drivertest\"\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/protobuf/proto\"\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/go-cmp/cmp/cmpopts\"\n\t\"gocloud.dev/docstore\"\n\tds \"gocloud.dev/docstore\"\n\t\"gocloud.dev/docstore/driver\"\n\t\"gocloud.dev/gcerrors\"\n)\n\n// Harness descibes the functionality test harnesses must provide to run\n// conformance tests.\ntype Harness interface {\n\t// MakeCollection makes a driver.Collection for testing.\n\t// The collection should have a single primary key field of type string named\n\t// drivertest.KeyField.\n\tMakeCollection(context.Context) (driver.Collection, error)\n\n\t// MakeTwoKeyCollection makes a driver.Collection for testing.\n\t// The collection will consist entirely of HighScore structs (see below), whose\n\t// two primary key fields are \"Game\" and \"Player\", both strings. Use\n\t// drivertest.HighScoreKey as the key function.\n\tMakeTwoKeyCollection(ctx context.Context) (driver.Collection, error)\n\n\t// MakeAlternateRevisionFieldCollection makes a driver.Collection for testing.\n\t// The collection should behave like the one returned from MakeCOllection, except\n\t// that the revision field should be drivertest.AlternateRevisionField.\n\tMakeAlternateRevisionFieldCollection(context.Context) (driver.Collection, error)\n\n\t// BeforeDoTypes should return a list of values whose types are valid for the as\n\t// function given to BeforeDo. For example, if the driver converts Get actions\n\t// to *GetRequests and write actions to *WriteRequests, then BeforeDoTypes should\n\t// return []interface{}{&GetRequest{}, &WriteRequest{}}.\n\t// TODO(jba): consider splitting these by action kind.\n\tBeforeDoTypes() []interface{}\n\n\t// BeforeQueryTypes should return a list of values whose types are valid for the as\n\t// function given to BeforeQuery.\n\tBeforeQueryTypes() []interface{}\n\n\t// RevisionsEqual reports whether two revisions are equal.\n\tRevisionsEqual(rev1, rev2 interface{}) bool\n\n\t// Close closes resources used by the harness.\n\tClose()\n}\n\n// HarnessMaker describes functions that construct a harness for running tests.\n// It is called exactly once per test; Harness.Close() will be called when the test is complete.\ntype HarnessMaker func(ctx context.Context, t *testing.T) (Harness, error)\n\n// UnsupportedType is an enum for types not supported by native codecs. We chose\n// to describe this negatively (types that aren't supported rather than types\n// that are) to make the more inclusive cases easier to write. A driver can\n// return nil for CodecTester.UnsupportedTypes, then add values from this enum\n// one by one until all tests pass.\ntype UnsupportedType int\n\n// These are known unsupported types by one or more driver. Each of them\n// corresponses to an unsupported type specific test which if the driver\n// actually supports.\nconst (\n\t// Native codec doesn't support any unsigned integer type\n\tUint UnsupportedType = iota\n\t// Native codec doesn't support arrays\n\tArrays\n\t// Native codec doesn't support full time precision\n\tNanosecondTimes\n\t// Native codec doesn't support [][]byte\n\tBinarySet\n)\n\n// CodecTester describes functions that encode and decode values using both the\n// docstore codec for a driver, and that driver's own \"native\" codec.\ntype CodecTester interface {\n\tUnsupportedTypes() []UnsupportedType\n\tNativeEncode(interface{}) (interface{}, error)\n\tNativeDecode(value, dest interface{}) error\n\tDocstoreEncode(interface{}) (interface{}, error)\n\tDocstoreDecode(value, dest interface{}) error\n}\n\n// AsTest represents a test of As functionality.\ntype AsTest interface {\n\t// Name should return a descriptive name for the test.\n\tName() string\n\t// CollectionCheck will be called to allow verification of Collection.As.\n\tCollectionCheck(coll *docstore.Collection) error\n\t// QueryCheck will be called after calling Query. It should call it.As and\n\t// verify the results.\n\tQueryCheck(it *docstore.DocumentIterator) error\n\t// ErrorCheck is called to allow verification of Collection.ErrorAs.\n\tErrorCheck(c *docstore.Collection, err error) error\n}\n\ntype verifyAsFailsOnNil struct{}\n\nfunc (verifyAsFailsOnNil) Name() string {\n\treturn \"verify As returns false when passed nil\"\n}\n\nfunc (verifyAsFailsOnNil) CollectionCheck(coll *docstore.Collection) error {\n\tif coll.As(nil) {\n\t\treturn errors.New(\"want Collection.As to return false when passed nil\")\n\t}\n\treturn nil\n}\n\nfunc (verifyAsFailsOnNil) QueryCheck(it *docstore.DocumentIterator) error {\n\tif it.As(nil) {\n\t\treturn errors.New(\"want DocumentIterator.As to return false when passed nil\")\n\t}\n\treturn nil\n}\n\nfunc (v verifyAsFailsOnNil) ErrorCheck(c *docstore.Collection, err error) (ret error) {\n\tdefer func() {\n\t\tif recover() == nil {\n\t\t\tret = errors.New(\"want ErrorAs to panic when passed nil\")\n\t\t}\n\t}()\n\tc.ErrorAs(err, nil)\n\treturn nil\n}\n\n// RunConformanceTests runs conformance tests for driver implementations of docstore.\nfunc RunConformanceTests(t *testing.T, newHarness HarnessMaker, ct CodecTester, asTests []AsTest) {\n\tt.Run(\"TypeDrivenCodec\", func(t *testing.T) { testTypeDrivenDecode(t, ct) })\n\tt.Run(\"BlindCodec\", func(t *testing.T) { testBlindDecode(t, ct) })\n\n\tt.Run(\"Create\", func(t *testing.T) { withCollection(t, newHarness, testCreate) })\n\tt.Run(\"Put\", func(t *testing.T) { withCollection(t, newHarness, testPut) })\n\tt.Run(\"Replace\", func(t *testing.T) { withCollection(t, newHarness, testReplace) })\n\tt.Run(\"Get\", func(t *testing.T) { withCollection(t, newHarness, testGet) })\n\tt.Run(\"Delete\", func(t *testing.T) { withCollection(t, newHarness, testDelete) })\n\tt.Run(\"Update\", func(t *testing.T) { withCollection(t, newHarness, testUpdate) })\n\tt.Run(\"Data\", func(t *testing.T) { withNoRevCollection(t, newHarness, testData) })\n\tt.Run(\"MultipleActions\", func(t *testing.T) { withCollection(t, newHarness, testMultipleActions) })\n\tt.Run(\"UnorderedActions\", func(t *testing.T) { withCollection(t, newHarness, testUnorderedActions) })\n\tt.Run(\"GetQueryKeyField\", func(t *testing.T) { withCollection(t, newHarness, testGetQueryKeyField) })\n\tt.Run(\"SerializeRevision\", func(t *testing.T) { withHarnessAndCollection(t, newHarness, testSerializeRevision) })\n\tt.Run(\"ActionsOnStructWithoutRevision\", func(t *testing.T) { withNoRevCollection(t, newHarness, testActionsOnStructWithoutRevision) })\n\n\tt.Run(\"GetQuery\", func(t *testing.T) { withTwoKeyCollection(t, newHarness, testGetQuery) })\n\tt.Run(\"DeleteQuery\", func(t *testing.T) { withTwoKeyCollection(t, newHarness, testDeleteQuery) })\n\tt.Run(\"UpdateQuery\", func(t *testing.T) { withTwoKeyCollection(t, newHarness, testUpdateQuery) })\n\n\tt.Run(\"BeforeDo\", func(t *testing.T) { testBeforeDo(t, newHarness) })\n\tt.Run(\"BeforeQuery\", func(t *testing.T) { testBeforeQuery(t, newHarness) })\n\n\tasTests = append(asTests, verifyAsFailsOnNil{})\n\tt.Run(\"As\", func(t *testing.T) {\n\t\tfor _, st := range asTests {\n\t\t\tif st.Name() == \"\" {\n\t\t\t\tt.Fatalf(\"AsTest.Name is required\")\n\t\t\t}\n\t\t\tt.Run(st.Name(), func(t *testing.T) {\n\t\t\t\twithTwoKeyCollection(t, newHarness, func(t *testing.T, coll *docstore.Collection) {\n\t\t\t\t\ttestAs(t, coll, st)\n\t\t\t\t})\n\t\t\t})\n\t\t}\n\t})\n}\n\nfunc withHarnessAndCollection(t *testing.T, newHarness HarnessMaker, f func(*testing.T, context.Context, Harness, *ds.Collection)) {\n\tctx := context.Background()\n\th, err := newHarness(ctx, t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer h.Close()\n\n\tdc, err := h.MakeCollection(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcoll := ds.NewCollection(dc)\n\tdefer coll.Close()\n\tclearCollection(t, coll)\n\tf(t, ctx, h, coll)\n}\n\nfunc withCollection(t *testing.T, newHarness HarnessMaker, f func(*testing.T, *ds.Collection, string)) {\n\twithHarnessAndCollection(t, newHarness, func(t *testing.T, ctx context.Context, h Harness, coll *ds.Collection) {\n\t\tt.Run(\"StdRev\", func(t *testing.T) { f(t, coll, ds.DefaultRevisionField) })\n\t\tdc, err := h.MakeAlternateRevisionFieldCollection(ctx)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcoll = ds.NewCollection(dc)\n\t\tdefer coll.Close()\n\t\tclearCollection(t, coll)\n\t\tt.Run(\"AltRev\", func(t *testing.T) { f(t, coll, AlternateRevisionField) })\n\t})\n}\n\nfunc withTwoKeyCollection(t *testing.T, newHarness HarnessMaker, f func(*testing.T, *ds.Collection)) {\n\tctx := context.Background()\n\th, err := newHarness(ctx, t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer h.Close()\n\n\tdc, err := h.MakeTwoKeyCollection(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcoll := ds.NewCollection(dc)\n\tdefer coll.Close()\n\tclearCollection(t, coll)\n\tf(t, coll)\n}\n\nfunc withNoRevCollection(t *testing.T, newHarness HarnessMaker, f func(*testing.T, *ds.Collection)) {\n\twithHarnessAndCollection(t, newHarness, func(t *testing.T, ctx context.Context, h Harness, coll *ds.Collection) {\n\t\tf(t, coll)\n\t})\n}\n\n// KeyField is the primary key field for the main test collection.\nconst KeyField = \"name\"\n\n// AlternateRevisionField is used for testing the option to provide a different\n// name for the revision field.\nconst AlternateRevisionField = \"Etag\"\n\ntype docmap = map[string]interface{}\n\nfunc newDoc(doc interface{}) interface{} {\n\tswitch v := doc.(type) {\n\tcase docmap:\n\t\treturn docmap{KeyField: v[KeyField]}\n\tcase *docstruct:\n\t\treturn &docstruct{Name: v.Name}\n\t}\n\treturn nil\n}\n\nfunc key(doc interface{}) interface{} {\n\tswitch d := doc.(type) {\n\tcase docmap:\n\t\treturn d[KeyField]\n\tcase *docstruct:\n\t\treturn d.Name\n\t}\n\treturn nil\n}\n\nfunc setKey(doc, key interface{}) {\n\tswitch d := doc.(type) {\n\tcase docmap:\n\t\td[KeyField] = key\n\tcase *docstruct:\n\t\td.Name = key\n\t}\n}\n\nfunc revision(doc interface{}, revField string) interface{} {\n\tswitch d := doc.(type) {\n\tcase docmap:\n\t\treturn d[revField]\n\tcase *docstruct:\n\t\tif revField == docstore.DefaultRevisionField {\n\t\t\treturn d.DocstoreRevision\n\t\t}\n\t\treturn d.Etag\n\t}\n\treturn nil\n}\n\nfunc setRevision(doc, rev interface{}, revField string) {\n\tswitch d := doc.(type) {\n\tcase docmap:\n\t\td[revField] = rev\n\tcase *docstruct:\n\t\tif revField == docstore.DefaultRevisionField {\n\t\t\td.DocstoreRevision = rev\n\t\t} else {\n\t\t\td.Etag = rev\n\t\t}\n\t}\n}\n\ntype docstruct struct {\n\tName             interface{} `docstore:\"name\"`\n\tDocstoreRevision interface{}\n\tEtag             interface{}\n\n\tI  int\n\tU  uint\n\tF  float64\n\tSt string\n\tB  bool\n\tM  map[string]interface{}\n}\n\nfunc nonexistentDoc() docmap { return docmap{KeyField: \"doesNotExist\"} }\n\nfunc testCreate(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\tfor _, tc := range []struct {\n\t\tname    string\n\t\tdoc     interface{}\n\t\twantErr gcerrors.ErrorCode\n\t}{\n\t\t{\n\t\t\tname: \"named map\",\n\t\t\tdoc:  docmap{KeyField: \"testCreateMap\", \"b\": true, revField: nil},\n\t\t},\n\t\t{\n\t\t\tname:    \"existing\",\n\t\t\tdoc:     docmap{KeyField: \"testCreateMap\", revField: nil},\n\t\t\twantErr: gcerrors.AlreadyExists,\n\t\t},\n\t\t{\n\t\t\tname: \"unnamed map\",\n\t\t\tdoc:  docmap{\"b\": true, revField: nil},\n\t\t},\n\t\t{\n\t\t\tname: \"named struct\",\n\t\t\tdoc:  &docstruct{Name: \"testCreateStruct\", B: true},\n\t\t},\n\t\t{\n\t\t\tname: \"unnamed struct\",\n\t\t\tdoc:  &docstruct{B: true},\n\t\t},\n\t\t{\n\t\t\tname:    \"with non-nil revision\",\n\t\t\tdoc:     docmap{KeyField: \"testCreate2\", revField: 0},\n\t\t\twantErr: gcerrors.InvalidArgument,\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tif tc.wantErr == gcerrors.OK {\n\t\t\t\tcheckNoRevisionField(t, tc.doc, revField)\n\t\t\t\tif err := coll.Create(ctx, tc.doc); err != nil {\n\t\t\t\t\tt.Fatalf(\"Create: %v\", err)\n\t\t\t\t}\n\t\t\t\tcheckHasRevisionField(t, tc.doc, revField)\n\n\t\t\t\tgot := newDoc(tc.doc)\n\t\t\t\tif err := coll.Get(ctx, got); err != nil {\n\t\t\t\t\tt.Fatalf(\"Get: %v\", err)\n\t\t\t\t}\n\t\t\t\tif diff := cmpDiff(got, tc.doc); diff != \"\" {\n\t\t\t\t\tt.Fatal(diff)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr := coll.Create(ctx, tc.doc)\n\t\t\t\tcheckCode(t, err, tc.wantErr)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc testPut(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tvar maprev, strmap interface{}\n\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tdoc  interface{}\n\t\trev  bool\n\t}{\n\t\t{\n\t\t\tname: \"create map\",\n\t\t\tdoc:  docmap{KeyField: \"testPutMap\", \"b\": true, revField: nil},\n\t\t},\n\t\t{\n\t\t\tname: \"create struct\",\n\t\t\tdoc:  &docstruct{Name: \"testPutStruct\", B: true},\n\t\t},\n\t\t{\n\t\t\tname: \"replace map\",\n\t\t\tdoc:  docmap{KeyField: \"testPutMap\", \"b\": false, revField: nil},\n\t\t\trev:  true,\n\t\t},\n\t\t{\n\t\t\tname: \"replace struct\",\n\t\t\tdoc:  &docstruct{Name: \"testPutStruct\", B: false},\n\t\t\trev:  true,\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tcheckNoRevisionField(t, tc.doc, revField)\n\t\t\tmust(coll.Put(ctx, tc.doc))\n\t\t\tcheckHasRevisionField(t, tc.doc, revField)\n\t\t\tgot := newDoc(tc.doc)\n\t\t\tmust(coll.Get(ctx, got))\n\t\t\tif diff := cmpDiff(got, tc.doc); diff != \"\" {\n\t\t\t\tt.Fatalf(diff)\n\t\t\t}\n\t\t\tif tc.rev {\n\t\t\t\tswitch v := tc.doc.(type) {\n\t\t\t\tcase docmap:\n\t\t\t\t\tmaprev = v[revField]\n\t\t\t\tcase *docstruct:\n\t\t\t\t\tif revField == docstore.DefaultRevisionField {\n\t\t\t\t\t\tstrmap = v.DocstoreRevision\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstrmap = v.Etag\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\n\t// Putting a doc with a revision field is the same as replace, meaning\n\t// it will fail if the document doesn't exist.\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tdoc  interface{}\n\t}{\n\t\t{\n\t\t\tname: \"replace map wrong key\",\n\t\t\tdoc:  docmap{KeyField: \"testPutMap2\", revField: maprev},\n\t\t},\n\t\t{\n\t\t\tname: \"replace struct wrong key\",\n\t\t\tdoc:  &docstruct{Name: \"testPutStruct2\", DocstoreRevision: strmap, Etag: strmap},\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\terr := coll.Put(ctx, tc.doc)\n\t\t\tif c := gcerrors.Code(err); c != gcerrors.NotFound && c != gcerrors.FailedPrecondition {\n\t\t\t\tt.Errorf(\"got %v, want NotFound or FailedPrecondition\", err)\n\t\t\t}\n\t\t})\n\t}\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, revField, func(doc interface{}) error {\n\t\t\treturn coll.Put(ctx, doc)\n\t\t})\n\t})\n}\n\nfunc testReplace(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tfor _, tc := range []struct {\n\t\tname       string\n\t\tdoc1, doc2 interface{}\n\t}{\n\t\t{\n\t\t\tname: \"replace map\",\n\t\t\tdoc1: docmap{KeyField: \"testReplaceMap\", \"s\": \"a\", revField: nil},\n\t\t\tdoc2: docmap{KeyField: \"testReplaceMap\", \"s\": \"b\", revField: nil},\n\t\t},\n\t\t{\n\t\t\tname: \"replace struct\",\n\t\t\tdoc1: &docstruct{Name: \"testReplaceStruct\", St: \"a\"},\n\t\t\tdoc2: &docstruct{Name: \"testReplaceStruct\", St: \"b\"},\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tmust(coll.Put(ctx, tc.doc1))\n\t\t\tcheckNoRevisionField(t, tc.doc2, revField)\n\t\t\tmust(coll.Replace(ctx, tc.doc2))\n\t\t\tcheckHasRevisionField(t, tc.doc2, revField)\n\t\t\tgot := newDoc(tc.doc2)\n\t\t\tmust(coll.Get(ctx, got))\n\t\t\tif diff := cmpDiff(got, tc.doc2); diff != \"\" {\n\t\t\t\tt.Fatalf(diff)\n\t\t\t}\n\t\t})\n\t}\n\n\t// Can't replace a nonexistent doc.\n\tcheckCode(t, coll.Replace(ctx, nonexistentDoc()), gcerrors.NotFound)\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, revField, func(doc interface{}) error {\n\t\t\treturn coll.Replace(ctx, doc)\n\t\t})\n\t})\n}\n\n// Check that doc does not have a revision field (or has a nil one).\nfunc checkNoRevisionField(t *testing.T, doc interface{}, revField string) {\n\tt.Helper()\n\tddoc, err := driver.NewDocument(doc)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif rev, _ := ddoc.GetField(revField); rev != nil {\n\t\tt.Fatal(\"doc has revision field\")\n\t}\n}\n\n// Check that doc has a non-nil revision field.\nfunc checkHasRevisionField(t *testing.T, doc interface{}, revField string) {\n\n\tt.Helper()\n\tddoc, err := driver.NewDocument(doc)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif rev, err := ddoc.GetField(revField); err != nil || rev == nil {\n\t\tt.Fatalf(\"doc missing revision field (error = %v)\", err)\n\t}\n}\n\nfunc testGet(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tdoc  interface{}\n\t\tfps  []docstore.FieldPath\n\t\twant interface{}\n\t}{\n\t\t// If Get is called with no field paths, the full document is populated.\n\t\t{\n\t\t\tname: \"get map\",\n\t\t\tdoc: docmap{\n\t\t\t\tKeyField: \"testGetMap\",\n\t\t\t\t\"s\":      \"a string\",\n\t\t\t\t\"i\":      int64(95),\n\t\t\t\t\"f\":      32.3,\n\t\t\t\t\"m\":      map[string]interface{}{\"a\": \"one\", \"b\": \"two\"},\n\t\t\t\trevField: nil,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"get struct\",\n\t\t\tdoc: &docstruct{\n\t\t\t\tName: \"testGetStruct\",\n\t\t\t\tSt:   \"a string\",\n\t\t\t\tI:    95,\n\t\t\t\tF:    32.3,\n\t\t\t\tM:    map[string]interface{}{\"a\": \"one\", \"b\": \"two\"},\n\t\t\t},\n\t\t},\n\t\t// If Get is called with field paths, the resulting document has only those fields.\n\t\t{\n\t\t\tname: \"get map with field path\",\n\t\t\tdoc: docmap{\n\t\t\t\tKeyField: \"testGetMapFP\",\n\t\t\t\t\"s\":      \"a string\",\n\t\t\t\t\"i\":      int64(95),\n\t\t\t\t\"f\":      32.3,\n\t\t\t\t\"m\":      map[string]interface{}{\"a\": \"one\", \"b\": \"two\"},\n\t\t\t\trevField: nil,\n\t\t\t},\n\t\t\tfps: []docstore.FieldPath{\"f\", \"m.b\", ds.FieldPath(revField)},\n\t\t\twant: docmap{\n\t\t\t\tKeyField: \"testGetMapFP\",\n\t\t\t\t\"f\":      32.3,\n\t\t\t\t\"m\":      map[string]interface{}{\"b\": \"two\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"get struct with field path\",\n\t\t\tdoc: &docstruct{\n\t\t\t\tName: \"testGetStructFP\",\n\t\t\t\tSt:   \"a string\",\n\t\t\t\tI:    95,\n\t\t\t\tF:    32.3,\n\t\t\t\tM:    map[string]interface{}{\"a\": \"one\", \"b\": \"two\"},\n\t\t\t},\n\t\t\tfps: []docstore.FieldPath{\"St\", \"M.a\", ds.FieldPath(revField)},\n\t\t\twant: &docstruct{\n\t\t\t\tName: \"testGetStructFP\",\n\t\t\t\tSt:   \"a string\",\n\t\t\t\tM:    map[string]interface{}{\"a\": \"one\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"get struct wrong case\",\n\t\t\tdoc: &docstruct{\n\t\t\t\tName: \"testGetStructWC\",\n\t\t\t\tSt:   \"a string\",\n\t\t\t\tI:    95,\n\t\t\t\tF:    32.3,\n\t\t\t\tM:    map[string]interface{}{\"a\": \"one\", \"b\": \"two\"},\n\t\t\t},\n\t\t\tfps: []docstore.FieldPath{\"st\", \"m.a\"},\n\t\t\twant: &docstruct{\n\t\t\t\tName: \"testGetStructWC\",\n\t\t\t},\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tmust(coll.Put(ctx, tc.doc))\n\t\t\tgot := newDoc(tc.doc)\n\t\t\tmust(coll.Get(ctx, got, tc.fps...))\n\t\t\tif tc.want == nil {\n\t\t\t\ttc.want = tc.doc\n\t\t\t}\n\t\t\tsetRevision(tc.want, revision(got, revField), revField)\n\t\t\tif diff := cmpDiff(got, tc.want); diff != \"\" {\n\t\t\t\tt.Error(\"Get with field paths:\\n\", diff)\n\t\t\t}\n\t\t})\n\t}\n\n\terr := coll.Get(ctx, nonexistentDoc())\n\tcheckCode(t, err, gcerrors.NotFound)\n}\n\nfunc testDelete(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\tvar rev interface{}\n\n\tfor _, tc := range []struct {\n\t\tname    string\n\t\tdoc     interface{}\n\t\twantErr gcerrors.ErrorCode\n\t}{\n\t\t{\n\t\t\tname: \"delete map\",\n\t\t\tdoc:  docmap{KeyField: \"testDeleteMap\", revField: nil},\n\t\t},\n\t\t{\n\t\t\tname:    \"delete map wrong rev\",\n\t\t\tdoc:     docmap{KeyField: \"testDeleteMap\", \"b\": true, revField: nil},\n\t\t\twantErr: gcerrors.FailedPrecondition,\n\t\t},\n\t\t{\n\t\t\tname: \"delete struct\",\n\t\t\tdoc:  &docstruct{Name: \"testDeleteStruct\"},\n\t\t},\n\t\t{\n\t\t\tname:    \"delete struct wrong rev\",\n\t\t\tdoc:     &docstruct{Name: \"testDeleteStruct\", B: true},\n\t\t\twantErr: gcerrors.FailedPrecondition,\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tif err := coll.Put(ctx, tc.doc); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif tc.wantErr == gcerrors.OK {\n\t\t\t\trev = revision(tc.doc, revField)\n\t\t\t\tif err := coll.Delete(ctx, tc.doc); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\t// The document should no longer exist.\n\t\t\t\tif err := coll.Get(ctx, tc.doc); err == nil {\n\t\t\t\t\tt.Error(\"want error, got nil\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsetRevision(tc.doc, rev, revField)\n\t\t\t\tcheckCode(t, coll.Delete(ctx, tc.doc), gcerrors.FailedPrecondition)\n\t\t\t}\n\t\t})\n\t}\n\t// Delete doesn't fail if the doc doesn't exist.\n\tif err := coll.Delete(ctx, nonexistentDoc()); err != nil {\n\t\tt.Errorf(\"delete nonexistent doc: want nil, got %v\", err)\n\t}\n}\n\nfunc testUpdate(t *testing.T, coll *ds.Collection, revField string) {\n\t// TODO(jba): test an increment-only update.\n\tctx := context.Background()\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tdoc  interface{}\n\t\tmods ds.Mods\n\t\twant interface{}\n\t}{\n\t\t{\n\t\t\tname: \"update map\",\n\t\t\tdoc:  docmap{KeyField: \"testUpdateMap\", \"a\": \"A\", \"b\": \"B\", \"n\": 3.5, \"i\": 1, revField: nil},\n\t\t\tmods: ds.Mods{\n\t\t\t\t\"a\": \"X\",\n\t\t\t\t\"b\": nil,\n\t\t\t\t\"c\": \"C\",\n\t\t\t\t\"n\": docstore.Increment(-1),\n\t\t\t\t\"i\": docstore.Increment(2.5),\n\t\t\t\t\"m\": docstore.Increment(3),\n\t\t\t},\n\t\t\twant: docmap{KeyField: \"testUpdateMap\", \"a\": \"X\", \"c\": \"C\", \"n\": 2.5, \"i\": 3.5, \"m\": int64(3)},\n\t\t},\n\t\t{\n\t\t\tname: \"update struct\",\n\t\t\tdoc:  &docstruct{Name: \"testUpdateStruct\", St: \"st\", I: 1, F: 3.5},\n\t\t\tmods: ds.Mods{\n\t\t\t\t\"St\": \"str\",\n\t\t\t\t\"I\":  nil,\n\t\t\t\t\"U\":  docstore.Increment(4),\n\t\t\t\t\"F\":  docstore.Increment(-3),\n\t\t\t},\n\t\t\twant: &docstruct{Name: \"testUpdateStruct\", St: \"str\", U: 4, F: 0.5},\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tif err := coll.Put(ctx, tc.doc); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tsetRevision(tc.doc, nil, revField)\n\t\t\tgot := newDoc(tc.doc)\n\t\t\tcheckNoRevisionField(t, tc.doc, revField)\n\t\t\terrs := coll.Actions().Update(tc.doc, tc.mods).Get(got).Do(ctx)\n\t\t\tif errs != nil {\n\t\t\t\tt.Fatal(errs)\n\t\t\t}\n\t\t\tcheckHasRevisionField(t, tc.doc, revField)\n\t\t\tsetRevision(tc.want, revision(got, revField), revField)\n\t\t\tif diff := cmp.Diff(got, tc.want); diff != \"\" {\n\t\t\t\tt.Error(diff)\n\t\t\t}\n\t\t})\n\t}\n\n\t// Can't update a nonexistent doc.\n\tif err := coll.Update(ctx, nonexistentDoc(), ds.Mods{\"x\": \"y\"}); err == nil {\n\t\tt.Error(\"nonexistent document: got nil, want error\")\n\t}\n\n\t// Bad increment value.\n\terr := coll.Update(ctx, docmap{KeyField: \"update invalid\"}, ds.Mods{\"x\": ds.Increment(\"3\")})\n\tcheckCode(t, err, gcerrors.InvalidArgument)\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, revField, func(doc interface{}) error {\n\t\t\treturn coll.Update(ctx, doc, ds.Mods{\"s\": \"c\"})\n\t\t})\n\t})\n}\n\n// Test that:\n// - Writing a document with a revision field succeeds if the document hasn't changed.\n// - Writing a document with a revision field fails if the document has changed.\nfunc testRevisionField(t *testing.T, coll *ds.Collection, revField string, write func(interface{}) error) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tdoc  interface{}\n\t}{\n\t\t{\n\t\t\tname: \"map revision\",\n\t\t\tdoc:  docmap{KeyField: \"testRevisionMap\", \"s\": \"a\", revField: nil},\n\t\t},\n\t\t{\n\t\t\tname: \"struct revision\",\n\t\t\tdoc:  &docstruct{Name: \"testRevisionStruct\", St: \"a\"},\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tmust(coll.Put(ctx, tc.doc))\n\t\t\tgot := newDoc(tc.doc)\n\t\t\tmust(coll.Get(ctx, got))\n\t\t\trev := revision(got, revField)\n\t\t\tif rev == nil {\n\t\t\t\tt.Fatal(\"missing revision field\")\n\t\t\t}\n\t\t\t// A write should succeed, because the document hasn't changed since it was gotten.\n\t\t\tif err := write(tc.doc); err != nil {\n\t\t\t\tt.Fatalf(\"write with revision field got %v, want nil\", err)\n\t\t\t}\n\t\t\t// This write should fail: got's revision field hasn't changed, but the stored document has.\n\t\t\terr := write(got)\n\t\t\tif c := gcerrors.Code(err); c != gcerrors.FailedPrecondition && c != gcerrors.NotFound {\n\t\t\t\tt.Errorf(\"write with old revision field: got %v, wanted FailedPrecondition or NotFound\", err)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Verify that the driver can serialize and deserialize revisions.\nfunc testSerializeRevision(t *testing.T, ctx context.Context, h Harness, coll *ds.Collection) {\n\tdoc := docmap{KeyField: \"testSerializeRevision\", \"x\": 1, docstore.DefaultRevisionField: nil}\n\tif err := coll.Create(ctx, doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\twant := doc[docstore.DefaultRevisionField]\n\tif want == nil {\n\t\tt.Fatal(\"nil revision\")\n\t}\n\ts, err := coll.RevisionToString(want)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tgot, err := coll.StringToRevision(s)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif !h.RevisionsEqual(got, want) {\n\t\tt.Fatalf(\"got %v, want %v\", got, want)\n\t}\n}\n\n// Test all Go integer types are supported, and they all come back as int64.\nfunc testData(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tfor _, test := range []struct {\n\t\tin, want interface{}\n\t}{\n\t\t{int(-1), int64(-1)},\n\t\t{int8(-8), int64(-8)},\n\t\t{int16(-16), int64(-16)},\n\t\t{int32(-32), int64(-32)},\n\t\t{int64(-64), int64(-64)},\n\t\t{uint(1), int64(1)},\n\t\t{uint8(8), int64(8)},\n\t\t{uint16(16), int64(16)},\n\t\t{uint32(32), int64(32)},\n\t\t{uint64(64), int64(64)},\n\t\t{float32(3.5), float64(3.5)},\n\t\t{[]byte{0, 1, 2}, []byte{0, 1, 2}},\n\t} {\n\t\tdoc := docmap{KeyField: \"testData\", \"val\": test.in}\n\t\tgot := docmap{KeyField: doc[KeyField]}\n\t\tif errs := coll.Actions().Put(doc).Get(got).Do(ctx); errs != nil {\n\t\t\tt.Fatal(errs)\n\t\t}\n\t\twant := docmap{\n\t\t\t\"val\":    test.want,\n\t\t\tKeyField: doc[KeyField],\n\t\t}\n\t\tif len(got) != len(want) {\n\t\t\tt.Errorf(\"%v: got %v, want %v\", test.in, got, want)\n\t\t} else if g := got[\"val\"]; !cmp.Equal(g, test.want) {\n\t\t\tt.Errorf(\"%v: got %v (%T), want %v (%T)\", test.in, g, g, test.want, test.want)\n\t\t}\n\t}\n\n\t// TODO: strings: valid vs. invalid unicode\n\n}\n\nvar (\n\t// A time with non-zero milliseconds, but zero nanoseconds.\n\tmilliTime = time.Date(2019, time.March, 27, 0, 0, 0, 5*1e6, time.UTC)\n\t// A time with non-zero nanoseconds.\n\tnanoTime = time.Date(2019, time.March, 27, 0, 0, 0, 5*1e6+7, time.UTC)\n)\n\n// Test that encoding from a struct and then decoding into the same struct works properly.\n// The decoding is \"type-driven\" because the decoder knows the expected type of the value\n// it is decoding--it is the type of a struct field.\nfunc testTypeDrivenDecode(t *testing.T, ct CodecTester) {\n\tif ct == nil {\n\t\tt.Skip(\"no CodecTester\")\n\t}\n\tcheck := func(in, dec interface{}, encode func(interface{}) (interface{}, error), decode func(interface{}, interface{}) error) {\n\t\tt.Helper()\n\t\tenc, err := encode(in)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%+v\", err)\n\t\t}\n\t\tif err := decode(enc, dec); err != nil {\n\t\t\tt.Fatalf(\"%+v\", err)\n\t\t}\n\t\tif diff := cmp.Diff(in, dec); diff != \"\" {\n\t\t\tt.Error(diff)\n\t\t}\n\t}\n\n\ts := \"bar\"\n\tdsrt := &docstoreRoundTrip{\n\t\tN:  nil,\n\t\tI:  1,\n\t\tU:  2,\n\t\tF:  2.5,\n\t\tSt: \"foo\",\n\t\tB:  true,\n\t\tL:  []int{3, 4, 5},\n\t\tA:  [2]int{6, 7},\n\t\tM:  map[string]bool{\"a\": true, \"b\": false},\n\t\tBy: []byte{6, 7, 8},\n\t\tP:  &s,\n\t\tT:  milliTime,\n\t}\n\n\tcheck(dsrt, &docstoreRoundTrip{}, ct.DocstoreEncode, ct.DocstoreDecode)\n\n\t// Test native-to-docstore and docstore-to-native round trips with a smaller set\n\t// of types.\n\tnm := &nativeMinimal{\n\t\tN:  nil,\n\t\tI:  1,\n\t\tF:  2.5,\n\t\tSt: \"foo\",\n\t\tB:  true,\n\t\tL:  []int{3, 4, 5},\n\t\tM:  map[string]bool{\"a\": true, \"b\": false},\n\t\tBy: []byte{6, 7, 8},\n\t\tP:  &s,\n\t\tT:  milliTime,\n\t\tLF: []float64{18.8, -19.9, 20},\n\t\tLS: []string{\"foo\", \"bar\"},\n\t}\n\tcheck(nm, &nativeMinimal{}, ct.DocstoreEncode, ct.NativeDecode)\n\tcheck(nm, &nativeMinimal{}, ct.NativeEncode, ct.DocstoreDecode)\n\n\t// Test various other types, unless they are unsupported.\n\tunsupported := map[UnsupportedType]bool{}\n\tfor _, u := range ct.UnsupportedTypes() {\n\t\tunsupported[u] = true\n\t}\n\n\t// Unsigned integers.\n\tif !unsupported[Uint] {\n\t\ttype Uint struct {\n\t\t\tU uint\n\t\t}\n\t\tu := &Uint{10}\n\t\tcheck(u, &Uint{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(u, &Uint{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\n\t// Arrays.\n\tif !unsupported[Arrays] {\n\t\ttype Arrays struct {\n\t\t\tA [2]int\n\t\t}\n\t\ta := &Arrays{[2]int{13, 14}}\n\t\tcheck(a, &Arrays{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(a, &Arrays{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\t// Nanosecond-precision time.\n\ttype NT struct {\n\t\tT time.Time\n\t}\n\n\tnt := &NT{nanoTime}\n\tif unsupported[NanosecondTimes] {\n\t\t// Expect rounding to the nearest millisecond.\n\t\tcheck := func(encode func(interface{}) (interface{}, error), decode func(interface{}, interface{}) error) {\n\t\t\tenc, err := encode(nt)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\tvar got NT\n\t\t\tif err := decode(enc, &got); err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\twant := nt.T.Round(time.Millisecond)\n\t\t\tif !got.T.Equal(want) {\n\t\t\t\tt.Errorf(\"got %v, want %v\", got.T, want)\n\t\t\t}\n\t\t}\n\t\tcheck(ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(ct.NativeEncode, ct.DocstoreDecode)\n\t} else {\n\t\t// Expect perfect round-tripping of nanosecond times.\n\t\tcheck(nt, &NT{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(nt, &NT{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\n\t// Binary sets.\n\tif !unsupported[BinarySet] {\n\t\ttype BinarySet struct {\n\t\t\tB [][]byte\n\t\t}\n\t\tb := &BinarySet{[][]byte{{15}, {16}, {17}}}\n\t\tcheck(b, &BinarySet{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(b, &BinarySet{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n}\n\n// Test decoding into an interface{}, where the decoder doesn't know the type of the\n// result and must return some Go type that accurately represents the value.\n// This is implemented by the AsInterface method of driver.Decoder.\n// Since it's fine for different drivers to return different types in this case,\n// each test case compares against a list of possible values.\nfunc testBlindDecode(t *testing.T, ct CodecTester) {\n\tif ct == nil {\n\t\tt.Skip(\"no CodecTester\")\n\t}\n\tt.Run(\"DocstoreEncode\", func(t *testing.T) { testBlindDecode1(t, ct.DocstoreEncode, ct.DocstoreDecode) })\n\tt.Run(\"NativeEncode\", func(t *testing.T) { testBlindDecode1(t, ct.NativeEncode, ct.DocstoreDecode) })\n}\n\nfunc testBlindDecode1(t *testing.T, encode func(interface{}) (interface{}, error), decode func(_, _ interface{}) error) {\n\t// Encode and decode expect a document, so use this struct to hold the values.\n\ttype S struct{ X interface{} }\n\n\tfor _, test := range []struct {\n\t\tin    interface{} // the value to be encoded\n\t\twant  interface{} // one possibility\n\t\twant2 interface{} // a second possibility\n\t}{\n\t\t{in: nil, want: nil},\n\t\t{in: true, want: true},\n\t\t{in: \"foo\", want: \"foo\"},\n\t\t{in: 'c', want: 'c', want2: int64('c')},\n\t\t{in: int(3), want: int32(3), want2: int64(3)},\n\t\t{in: int8(3), want: int32(3), want2: int64(3)},\n\t\t{in: int(-3), want: int32(-3), want2: int64(-3)},\n\t\t{in: int64(math.MaxInt32 + 1), want: int64(math.MaxInt32 + 1)},\n\t\t{in: float32(1.5), want: float64(1.5)},\n\t\t{in: float64(1.5), want: float64(1.5)},\n\t\t{in: []byte{1, 2}, want: []byte{1, 2}},\n\t\t{in: []int{1, 2},\n\t\t\twant:  []interface{}{int32(1), int32(2)},\n\t\t\twant2: []interface{}{int64(1), int64(2)}},\n\t\t{in: []float32{1.5, 2.5}, want: []interface{}{float64(1.5), float64(2.5)}},\n\t\t{in: []float64{1.5, 2.5}, want: []interface{}{float64(1.5), float64(2.5)}},\n\t\t{in: milliTime, want: milliTime, want2: \"2019-03-27T00:00:00.005Z\"},\n\t\t{in: []time.Time{milliTime},\n\t\t\twant:  []interface{}{milliTime},\n\t\t\twant2: []interface{}{\"2019-03-27T00:00:00.005Z\"},\n\t\t},\n\t\t{in: map[string]int{\"a\": 1},\n\t\t\twant:  map[string]interface{}{\"a\": int64(1)},\n\t\t\twant2: map[string]interface{}{\"a\": int32(1)},\n\t\t},\n\t\t{in: map[string][]byte{\"a\": {1, 2}}, want: map[string]interface{}{\"a\": []byte{1, 2}}},\n\t} {\n\t\tenc, err := encode(&S{test.in})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"encoding %T: %v\", test.in, err)\n\t\t}\n\t\tvar got S\n\t\tif err := decode(enc, &got); err != nil {\n\t\t\tt.Fatalf(\"decoding %T: %v\", test.in, err)\n\t\t}\n\t\tmatched := false\n\t\twants := []interface{}{test.want}\n\t\tif test.want2 != nil {\n\t\t\twants = append(wants, test.want2)\n\t\t}\n\t\tfor _, w := range wants {\n\t\t\tif cmp.Equal(got.X, w) {\n\t\t\t\tmatched = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !matched {\n\t\t\tt.Errorf(\"%T: got %#v (%T), not equal to %#v or %#v\", test.in, got.X, got.X, test.want, test.want2)\n\t\t}\n\t}\n}\n\n// A round trip with the docstore codec should work for all docstore-supported types,\n// regardless of native driver support.\ntype docstoreRoundTrip struct {\n\tN  *int\n\tI  int\n\tU  uint\n\tF  float64\n\tSt string\n\tB  bool\n\tBy []byte\n\tL  []int\n\tA  [2]int\n\tM  map[string]bool\n\tP  *string\n\tT  time.Time\n}\n\n// TODO(jba): add more fields: structs; embedding.\n\n// All native codecs should support these types. If one doesn't, remove it from this\n// struct and make a new single-field struct for it.\ntype nativeMinimal struct {\n\tN  *int\n\tI  int\n\tF  float64\n\tSt string\n\tB  bool\n\tBy []byte\n\tL  []int\n\tM  map[string]bool\n\tP  *string\n\tT  time.Time\n\tLF []float64\n\tLS []string\n}\n\n// The following is the schema for the collection used for query testing.\n// It is loosely borrowed from the DynamoDB documentation.\n// It is rich enough to require indexes for some drivers.\n\n// A HighScore records one user's high score in a particular game.\n// The primary key fields are Game and Player.\ntype HighScore struct {\n\tGame             string\n\tPlayer           string\n\tScore            int\n\tTime             time.Time\n\tDocstoreRevision interface{}\n}\n\nfunc newHighScore() interface{} { return &HighScore{} }\n\n// HighScoreKey constructs a single primary key from a HighScore struct\n// by concatenating the Game and Player fields.\nfunc HighScoreKey(doc docstore.Document) interface{} { return doc.(*HighScore).key() }\n\nfunc (h *HighScore) key() string { return h.Game + \"|\" + h.Player }\n\nfunc highScoreLess(h1, h2 *HighScore) bool { return h1.key() < h2.key() }\n\nfunc (h *HighScore) String() string {\n\treturn fmt.Sprintf(\"%s=%d@%s\", h.key(), h.Score, h.Time.Format(\"01/02\"))\n}\n\nfunc date(month, day int) time.Time {\n\treturn time.Date(2019, time.Month(month), day, 0, 0, 0, 0, time.UTC)\n}\n\nconst (\n\tgame1 = \"Praise All Monsters\"\n\tgame2 = \"Zombie DMV\"\n\tgame3 = \"Days Gone\"\n)\n\nvar queryDocuments = []*HighScore{\n\t{game1, \"pat\", 49, date(3, 13), nil},\n\t{game1, \"mel\", 60, date(4, 10), nil},\n\t{game1, \"andy\", 81, date(2, 1), nil},\n\t{game1, \"fran\", 33, date(3, 19), nil},\n\t{game2, \"pat\", 120, date(4, 1), nil},\n\t{game2, \"billie\", 111, date(4, 10), nil},\n\t{game2, \"mel\", 190, date(4, 18), nil},\n\t{game2, \"fran\", 33, date(3, 20), nil},\n}\n\nfunc addQueryDocuments(t *testing.T, coll *ds.Collection) {\n\talist := coll.Actions()\n\tfor _, doc := range queryDocuments {\n\t\td := *doc\n\t\talist.Put(&d)\n\t}\n\tif err := alist.Do(context.Background()); err != nil {\n\t\tt.Fatalf(\"%+v\", err)\n\t}\n}\n\nfunc testGetQueryKeyField(t *testing.T, coll *ds.Collection, revField string) {\n\t// Query the key field of a collection that has one.\n\t// (The collection used for testGetQuery uses a key function rather than a key field.)\n\tctx := context.Background()\n\tdocs := []docmap{\n\t\t{KeyField: \"qkf1\", \"a\": \"one\", revField: nil},\n\t\t{KeyField: \"qkf2\", \"a\": \"two\", revField: nil},\n\t\t{KeyField: \"qkf3\", \"a\": \"three\", revField: nil},\n\t}\n\tal := coll.Actions()\n\tfor _, d := range docs {\n\t\tal.Put(d)\n\t}\n\tif err := al.Do(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\titer := coll.Query().Where(KeyField, \"<\", \"qkf3\").Get(ctx)\n\tdefer iter.Stop()\n\tgot := mustCollect(ctx, t, iter)\n\twant := docs[:2]\n\tdiff := cmpDiff(got, want, cmpopts.SortSlices(sortByKeyField))\n\tif diff != \"\" {\n\t\tt.Error(diff)\n\t}\n\n\t// Test that queries with selected fields always return the key.\n\titer = coll.Query().Get(ctx, \"a\", ds.FieldPath(revField))\n\tdefer iter.Stop()\n\tgot = mustCollect(ctx, t, iter)\n\tfor _, d := range docs {\n\t\tcheckHasRevisionField(t, d, revField)\n\t}\n\tdiff = cmpDiff(got, docs, cmpopts.SortSlices(sortByKeyField))\n\tif diff != \"\" {\n\t\tt.Error(diff)\n\t}\n}\n\nfunc sortByKeyField(d1, d2 docmap) bool { return d1[KeyField].(string) < d2[KeyField].(string) }\n\nfunc testGetQuery(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\taddQueryDocuments(t, coll)\n\n\t// Query filters should have the same behavior when doing string and number\n\t// comparison.\n\ttests := []struct {\n\t\tname   string\n\t\tq      *ds.Query\n\t\tfields []docstore.FieldPath       // fields to get\n\t\twant   func(*HighScore) bool      // filters queryDocuments\n\t\tbefore func(x, y *HighScore) bool // if present, checks result order\n\t}{\n\t\t{\n\t\t\tname: \"All\",\n\t\t\tq:    coll.Query(),\n\t\t\twant: func(*HighScore) bool { return true },\n\t\t},\n\t\t{\n\t\t\tname: \"Game\",\n\t\t\tq:    coll.Query().Where(\"Game\", \"=\", game2),\n\t\t\twant: func(h *HighScore) bool { return h.Game == game2 },\n\t\t},\n\t\t{\n\t\t\tname: \"Score\",\n\t\t\tq:    coll.Query().Where(\"Score\", \">\", 100),\n\t\t\twant: func(h *HighScore) bool { return h.Score > 100 },\n\t\t},\n\t\t{\n\t\t\tname: \"Player\",\n\t\t\tq:    coll.Query().Where(\"Player\", \"=\", \"billie\"),\n\t\t\twant: func(h *HighScore) bool { return h.Player == \"billie\" },\n\t\t},\n\t\t{\n\t\t\tname: \"GamePlayer\",\n\t\t\tq:    coll.Query().Where(\"Player\", \"=\", \"andy\").Where(\"Game\", \"=\", game1),\n\t\t\twant: func(h *HighScore) bool { return h.Player == \"andy\" && h.Game == game1 },\n\t\t},\n\t\t{\n\t\t\tname: \"PlayerScore\",\n\t\t\tq:    coll.Query().Where(\"Player\", \"=\", \"pat\").Where(\"Score\", \"<\", 100),\n\t\t\twant: func(h *HighScore) bool { return h.Player == \"pat\" && h.Score < 100 },\n\t\t},\n\t\t{\n\t\t\tname: \"GameScore\",\n\t\t\tq:    coll.Query().Where(\"Game\", \"=\", game1).Where(\"Score\", \">=\", 50),\n\t\t\twant: func(h *HighScore) bool { return h.Game == game1 && h.Score >= 50 },\n\t\t},\n\t\t{\n\t\t\tname: \"PlayerTime\",\n\t\t\tq:    coll.Query().Where(\"Player\", \"=\", \"mel\").Where(\"Time\", \">\", date(4, 1)),\n\t\t\twant: func(h *HighScore) bool { return h.Player == \"mel\" && h.Time.After(date(4, 1)) },\n\t\t},\n\t\t{\n\t\t\tname: \"ScoreTime\",\n\t\t\tq:    coll.Query().Where(\"Score\", \">=\", 50).Where(\"Time\", \">\", date(4, 1)),\n\t\t\twant: func(h *HighScore) bool { return h.Score >= 50 && h.Time.After(date(4, 1)) },\n\t\t},\n\t\t{\n\t\t\tname:   \"AllByPlayerAsc\",\n\t\t\tq:      coll.Query().OrderBy(\"Player\", docstore.Ascending),\n\t\t\twant:   func(h *HighScore) bool { return true },\n\t\t\tbefore: func(h1, h2 *HighScore) bool { return h1.Player < h2.Player },\n\t\t},\n\t\t{\n\t\t\tname:   \"AllByPlayerDesc\",\n\t\t\tq:      coll.Query().OrderBy(\"Player\", docstore.Descending),\n\t\t\twant:   func(h *HighScore) bool { return true },\n\t\t\tbefore: func(h1, h2 *HighScore) bool { return h1.Player > h2.Player },\n\t\t},\n\t\t{\n\t\t\tname: \"GameByPlayerAsc\",\n\t\t\t// We need a filter on Player, and it can't be the empty string (DynamoDB limitation).\n\t\t\t// So pick any string that sorts less than all valid player names.\n\t\t\tq: coll.Query().Where(\"Game\", \"=\", game1).Where(\"Player\", \">\", \".\").\n\t\t\t\tOrderBy(\"Player\", docstore.Ascending),\n\t\t\twant:   func(h *HighScore) bool { return h.Game == game1 },\n\t\t\tbefore: func(h1, h2 *HighScore) bool { return h1.Player < h2.Player },\n\t\t},\n\t\t{\n\t\t\t// Same as above, but descending.\n\t\t\tname: \"GameByPlayerDesc\",\n\t\t\tq: coll.Query().Where(\"Game\", \"=\", game1).Where(\"Player\", \">\", \".\").\n\t\t\t\tOrderBy(\"Player\", docstore.Descending),\n\t\t\twant:   func(h *HighScore) bool { return h.Game == game1 },\n\t\t\tbefore: func(h1, h2 *HighScore) bool { return h1.Player > h2.Player },\n\t\t},\n\t\t// TODO(jba): add more OrderBy tests.\n\t\t{\n\t\t\tname:   \"AllWithKeyFields\",\n\t\t\tq:      coll.Query(),\n\t\t\tfields: []docstore.FieldPath{\"Game\", \"Player\", ds.FieldPath(ds.DefaultRevisionField)},\n\t\t\twant: func(h *HighScore) bool {\n\t\t\t\th.Score = 0\n\t\t\t\th.Time = time.Time{}\n\t\t\t\treturn true\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:   \"AllWithScore\",\n\t\t\tq:      coll.Query(),\n\t\t\tfields: []docstore.FieldPath{\"Game\", \"Player\", \"Score\", ds.FieldPath(ds.DefaultRevisionField)},\n\t\t\twant: func(h *HighScore) bool {\n\t\t\t\th.Time = time.Time{}\n\t\t\t\treturn true\n\t\t\t},\n\t\t},\n\t}\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tgot, err := collectHighScores(ctx, tc.q.Get(ctx, tc.fields...))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tfor _, g := range got {\n\t\t\t\tif g.DocstoreRevision == nil {\n\t\t\t\t\tt.Errorf(\"%v missing DocstoreRevision\", g)\n\t\t\t\t} else {\n\t\t\t\t\tg.DocstoreRevision = nil\n\t\t\t\t}\n\t\t\t}\n\t\t\twant := filterHighScores(queryDocuments, tc.want)\n\t\t\t_, err = tc.q.Plan()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tdiff := cmp.Diff(got, want, cmpopts.SortSlices(highScoreLess))\n\t\t\tif diff != \"\" {\n\t\t\t\tt.Fatal(diff)\n\t\t\t}\n\t\t\tif tc.before != nil {\n\t\t\t\t// Verify that the results are sorted according to tc.less.\n\t\t\t\tfor i := 1; i < len(got); i++ {\n\t\t\t\t\tif tc.before(got[i], got[i-1]) {\n\t\t\t\t\t\tt.Errorf(\"%s at %d sorts before previous %s\", got[i], i, got[i-1])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We can't assume anything about the query plan. Just verify that Plan returns\n\t\t\t// successfully.\n\t\t\tif _, err := tc.q.Plan(KeyField); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\t}\n\tt.Run(\"Limit\", func(t *testing.T) {\n\t\t// For limit, we can't be sure which documents will be returned, only their count.\n\t\tlimitQ := coll.Query().Limit(2)\n\t\tgot := mustCollectHighScores(ctx, t, limitQ.Get(ctx))\n\t\tif len(got) != 2 {\n\t\t\tt.Errorf(\"got %v, wanted two documents\", got)\n\t\t}\n\t})\n}\n\nfunc testDeleteQuery(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\n\taddQueryDocuments(t, coll)\n\n\t// Note: these tests are cumulative. If the first test deletes a document, that\n\t// change will persist for the second test.\n\ttests := []struct {\n\t\tname string\n\t\tq    *ds.Query\n\t\twant func(*HighScore) bool // filters queryDocuments\n\t}{\n\t\t{\n\t\t\tname: \"Player\",\n\t\t\tq:    coll.Query().Where(\"Player\", \"=\", \"andy\"),\n\t\t\twant: func(h *HighScore) bool { return h.Player != \"andy\" },\n\t\t},\n\t\t{\n\t\t\tname: \"Score\",\n\t\t\tq:    coll.Query().Where(\"Score\", \">\", 100),\n\t\t\twant: func(h *HighScore) bool { return h.Score <= 100 },\n\t\t},\n\t\t{\n\t\t\tname: \"All\",\n\t\t\tq:    coll.Query(),\n\t\t\twant: func(h *HighScore) bool { return false },\n\t\t},\n\t\t// TODO(jba): add a case that requires Firestore to evaluate filters on the client.\n\t}\n\tprevWant := queryDocuments\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tif err := tc.q.Delete(ctx); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tgot := mustCollectHighScores(ctx, t, coll.Query().Get(ctx))\n\t\t\tfor _, g := range got {\n\t\t\t\tg.DocstoreRevision = nil\n\t\t\t}\n\t\t\twant := filterHighScores(prevWant, tc.want)\n\t\t\tprevWant = want\n\t\t\tdiff := cmp.Diff(got, want, cmpopts.SortSlices(highScoreLess))\n\t\t\tif diff != \"\" {\n\t\t\t\tt.Error(diff)\n\t\t\t}\n\t\t})\n\t}\n\n\t// Using Limit with DeleteQuery should be an error.\n\terr := coll.Query().Where(\"Player\", \"=\", \"mel\").Limit(1).Delete(ctx)\n\tif err == nil {\n\t\tt.Fatal(\"want error for Limit, got nil\")\n\t}\n}\n\nfunc testUpdateQuery(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\taddQueryDocuments(t, coll)\n\n\tcheck := func(q *ds.Query, filter func(*HighScore) bool) {\n\t\tt.Helper()\n\t\terr := q.Update(ctx, docstore.Mods{\"Score\": 13, \"Time\": nil})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tgot := mustCollectHighScores(ctx, t, coll.Query().Get(ctx))\n\t\tfor _, g := range got {\n\t\t\tif g.DocstoreRevision == nil {\n\t\t\t\tt.Fatal(\"revision field not present\")\n\t\t\t}\n\t\t\tg.DocstoreRevision = nil\n\t\t}\n\t\twant := filterHighScores(queryDocuments, filter)\n\t\tdiff := cmp.Diff(got, want, cmpopts.SortSlices(highScoreLess))\n\t\tif diff != \"\" {\n\t\t\tt.Error(diff)\n\t\t}\n\t}\n\n\tcheck(coll.Query().Where(\"Player\", \"=\", \"fran\"), func(h *HighScore) bool {\n\t\tif h.Player == \"fran\" {\n\t\t\th.Score = 13\n\t\t\th.Time = time.Time{}\n\t\t}\n\t\treturn true\n\t})\n\n\t// Updates without a filter are blind. The revision field shouldn't be checked.\n\t// We can't really test that, but we can at least make sure it's written.\n\tcheck(coll.Query(), func(h *HighScore) bool {\n\t\th.Score = 13\n\t\th.Time = time.Time{}\n\t\treturn true\n\t})\n}\n\nfunc filterHighScores(hs []*HighScore, f func(*HighScore) bool) []*HighScore {\n\tvar res []*HighScore\n\tfor _, h := range hs {\n\t\tc := *h // Copy in case f modifies its argument.\n\t\tif f(&c) {\n\t\t\tres = append(res, &c)\n\t\t}\n\t}\n\treturn res\n}\n\n// clearCollection delete all documents from this collection after test.\nfunc clearCollection(fataler interface{ Fatalf(string, ...interface{}) }, coll *docstore.Collection) {\n\tif err := coll.Query().Delete(context.Background()); err != nil {\n\t\tfataler.Fatalf(\"%+v\", err)\n\t}\n}\n\nfunc forEach(ctx context.Context, iter *ds.DocumentIterator, create func() interface{}, handle func(interface{}) error) error {\n\tfor {\n\t\tdoc := create()\n\t\terr := iter.Next(ctx, doc)\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := handle(doc); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc mustCollect(ctx context.Context, t *testing.T, iter *ds.DocumentIterator) []docmap {\n\tvar ms []docmap\n\tnewDocmap := func() interface{} { return docmap{} }\n\tcollect := func(m interface{}) error { ms = append(ms, m.(docmap)); return nil }\n\tif err := forEach(ctx, iter, newDocmap, collect); err != nil {\n\t\tt.Fatal(err)\n\t}\n\treturn ms\n}\n\nfunc mustCollectHighScores(ctx context.Context, t *testing.T, iter *ds.DocumentIterator) []*HighScore {\n\ths, err := collectHighScores(ctx, iter)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\treturn hs\n}\n\nfunc collectHighScores(ctx context.Context, iter *ds.DocumentIterator) ([]*HighScore, error) {\n\tvar hs []*HighScore\n\tcollect := func(h interface{}) error { hs = append(hs, h.(*HighScore)); return nil }\n\tif err := forEach(ctx, iter, newHighScore, collect); err != nil {\n\t\treturn nil, err\n\t}\n\treturn hs, nil\n}\n\n// TODO(shantuo): this and the unordered action tests should be merged as now\n// they are testing the same thing. We don't have ordered actions.\nfunc testMultipleActions(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\n\tdocs := []docmap{\n\t\t{KeyField: \"testMultipleActions1\", \"s\": \"a\"},\n\t\t{KeyField: \"testMultipleActions2\", \"s\": \"b\"},\n\t\t{KeyField: \"testMultipleActions3\", \"s\": \"c\"},\n\t\t{KeyField: \"testMultipleActions4\", \"s\": \"d\"},\n\t\t{KeyField: \"testMultipleActions5\", \"s\": \"e\"},\n\t\t{KeyField: \"testMultipleActions6\", \"s\": \"f\"},\n\t\t{KeyField: \"testMultipleActions7\", \"s\": \"g\"},\n\t\t{KeyField: \"testMultipleActions8\", \"s\": \"h\"},\n\t\t{KeyField: \"testMultipleActions9\", \"s\": \"i\"},\n\t\t{KeyField: \"testMultipleActions10\", \"s\": \"j\"},\n\t\t{KeyField: \"testMultipleActions11\", \"s\": \"k\"},\n\t\t{KeyField: \"testMultipleActions12\", \"s\": \"l\"},\n\t}\n\n\tactions := coll.Actions()\n\t// Writes\n\tfor i := 0; i < 6; i++ {\n\t\tactions.Create(docs[i])\n\t}\n\tfor i := 6; i < len(docs); i++ {\n\t\tactions.Put(docs[i])\n\t}\n\n\t// Reads\n\tgots := make([]docmap, len(docs))\n\tfor i, doc := range docs {\n\t\tgots[i] = docmap{KeyField: doc[KeyField]}\n\t\tactions.Get(gots[i], docstore.FieldPath(\"s\"))\n\t}\n\tif err := actions.Do(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tfor i, got := range gots {\n\t\tif diff := cmpDiff(got, docs[i]); diff != \"\" {\n\t\t\tt.Error(diff)\n\t\t}\n\t}\n\n\t// Deletes\n\tdels := coll.Actions()\n\tfor _, got := range gots {\n\t\tdels.Delete(docmap{KeyField: got[KeyField]})\n\t}\n\tif err := dels.Do(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\nfunc testUnorderedActions(t *testing.T, coll *ds.Collection, revField string) {\n\tctx := context.Background()\n\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tvar docs []docmap\n\tfor i := 0; i < 9; i++ {\n\t\tdocs = append(docs, docmap{\n\t\t\tKeyField: fmt.Sprintf(\"testUnorderedActions%d\", i),\n\t\t\t\"s\":      fmt.Sprint(i),\n\t\t\trevField: nil,\n\t\t})\n\t}\n\n\tcompare := func(gots, wants []docmap) {\n\t\tt.Helper()\n\t\tfor i := 0; i < len(gots); i++ {\n\t\t\tgot := gots[i]\n\t\t\twant := clone(wants[i])\n\t\t\twant[revField] = got[revField]\n\t\t\tif !cmp.Equal(got, want) {\n\t\t\t\tt.Errorf(\"index #%d:\\ngot  %v\\nwant %v\", i, got, want)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Put the first three docs.\n\tactions := coll.Actions()\n\tfor i := 0; i < 6; i++ {\n\t\tactions.Create(docs[i])\n\t}\n\tmust(actions.Do(ctx))\n\n\t// Replace the first three and put six more.\n\tactions = coll.Actions()\n\tfor i := 0; i < 3; i++ {\n\t\tdocs[i][\"s\"] = fmt.Sprintf(\"%d'\", i)\n\t\tactions.Replace(docs[i])\n\t}\n\tfor i := 3; i < 9; i++ {\n\t\tactions.Put(docs[i])\n\t}\n\tmust(actions.Do(ctx))\n\n\t// Delete the first three, get the second three, and put three more.\n\tgdocs := []docmap{\n\t\t{KeyField: docs[3][KeyField]},\n\t\t{KeyField: docs[4][KeyField]},\n\t\t{KeyField: docs[5][KeyField]},\n\t}\n\tactions = coll.Actions()\n\tactions.Update(docs[6], ds.Mods{\"s\": \"6'\"})\n\tactions.Get(gdocs[0])\n\tactions.Delete(docs[0])\n\tactions.Delete(docs[1])\n\tactions.Update(docs[7], ds.Mods{\"s\": \"7'\"})\n\tactions.Get(gdocs[1])\n\tactions.Delete(docs[2])\n\tactions.Get(gdocs[2])\n\tactions.Update(docs[8], ds.Mods{\"s\": \"8'\"})\n\tmust(actions.Do(ctx))\n\tcompare(gdocs, docs[3:6])\n\n\t// At this point, the existing documents are 3 - 9.\n\n\t// Get the first four, try to create one that already exists, delete a\n\t// nonexistent doc, and put one. Only the Get of #3, the Delete and the Put\n\t// should succeed.\n\tactions = coll.Actions()\n\tfor _, doc := range []docmap{\n\t\t{KeyField: docs[0][KeyField]},\n\t\t{KeyField: docs[1][KeyField]},\n\t\t{KeyField: docs[2][KeyField]},\n\t\t{KeyField: docs[3][KeyField]},\n\t} {\n\t\tactions.Get(doc)\n\t}\n\tdocs[4][revField] = nil\n\tactions.Create(docs[4]) // create existing doc\n\tactions.Put(docs[5])\n\t// TODO(jba): Understand why the following line is necessary for dynamo but not the others.\n\tdocs[0][revField] = nil\n\tactions.Delete(docs[0]) // delete nonexistent doc\n\terr := actions.Do(ctx)\n\tif err == nil {\n\t\tt.Fatal(\"want error, got nil\")\n\t}\n\talerr, ok := err.(docstore.ActionListError)\n\tif !ok {\n\t\tt.Fatalf(\"got %v (%T), want ActionListError\", alerr, alerr)\n\t}\n\tfor _, e := range alerr {\n\t\tswitch i := e.Index; i {\n\t\tcase 3, 5, 6:\n\t\t\tt.Errorf(\"index %d: got %v, want nil\", i, e.Err)\n\n\t\tcase 4, -1: // -1 for mongodb issue, see https://jira.mongodb.org/browse/GODRIVER-1028\n\t\t\tif ec := gcerrors.Code(e.Err); ec != gcerrors.AlreadyExists &&\n\t\t\t\tec != gcerrors.FailedPrecondition { // TODO(shantuo): distinguish this case for dyanmo\n\t\t\t\tt.Errorf(\"index 4: create an existing document: got %v, want error\", e.Err)\n\t\t\t}\n\n\t\tdefault:\n\t\t\tif gcerrors.Code(e.Err) != gcerrors.NotFound {\n\t\t\t\tt.Errorf(\"index %d: got %v, want NotFound\", i, e.Err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc testActionsOnStructWithoutRevision(t *testing.T, coll *ds.Collection) {\n\ttype item struct {\n\t\tName string `docstore:\"name\"`\n\t\tI    int\n\t}\n\tdoc1 := item{Name: \"createandreplace\"}\n\tdoc2 := item{Name: \"putandupdate\"}\n\tctx := context.Background()\n\n\tgot1 := item{Name: doc1.Name}\n\tgot2 := map[string]interface{}{\"name\": doc2.Name}\n\tif err := coll.Actions().\n\t\tCreate(&doc1).Put(&doc2).\n\t\tGet(&got1).Get(got2).\n\t\tDo(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcheckNoRevisionField(t, got2, ds.DefaultRevisionField)\n\n\tgot3 := map[string]interface{}{\"name\": doc1.Name}\n\tgot4 := item{Name: doc2.Name}\n\tif err := coll.Actions().\n\t\tReplace(&doc1).Update(&item{Name: doc2.Name}, ds.Mods{\"I\": 1}).\n\t\tGet(got3, \"I\").Get(&got4, \"I\").\n\t\tDo(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcheckNoRevisionField(t, got3, ds.DefaultRevisionField)\n}\n\n// Verify that BeforeDo is invoked, and its as function behaves as expected.\nfunc testBeforeDo(t *testing.T, newHarness HarnessMaker) {\n\twithHarnessAndCollection(t, newHarness, func(t *testing.T, ctx context.Context, h Harness, coll *ds.Collection) {\n\t\tvar called bool\n\t\tbeforeDo := func(asFunc func(interface{}) bool) error {\n\t\t\tcalled = true\n\t\t\tif asFunc(nil) {\n\t\t\t\treturn errors.New(\"asFunc returned true when called with nil, want false\")\n\t\t\t}\n\t\t\t// At least one of the expected types must return true. Special case: if\n\t\t\t// there are no types, then the as function never returns true, so skip the\n\t\t\t// check.\n\t\t\tif len(h.BeforeDoTypes()) > 0 {\n\t\t\t\tfound := false\n\t\t\t\tfor _, b := range h.BeforeDoTypes() {\n\t\t\t\t\tv := reflect.New(reflect.TypeOf(b)).Interface()\n\t\t\t\t\tif asFunc(v) {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\treturn errors.New(\"none of the BeforeDoTypes works with the as function\")\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\tcheck := func(f func(*ds.ActionList)) {\n\t\t\tt.Helper()\n\t\t\t// First, verify that if a BeforeDo function returns an error, so does ActionList.Do.\n\t\t\t// We depend on that for the rest of the test.\n\t\t\tal := coll.Actions().BeforeDo(func(func(interface{}) bool) error { return errors.New(\"\") })\n\t\t\tf(al)\n\t\t\tif err := al.Do(ctx); err == nil {\n\t\t\t\tt.Error(\"beforeDo returning error: got nil from Do, want error\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcalled = false\n\t\t\tal = coll.Actions().BeforeDo(beforeDo)\n\t\t\tf(al)\n\t\t\tif err := al.Do(ctx); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif !called {\n\t\t\t\tt.Error(\"BeforeDo function never called\")\n\t\t\t}\n\t\t}\n\n\t\tdoc := docmap{KeyField: \"testBeforeDo\"}\n\t\tcheck(func(l *docstore.ActionList) { l.Create(doc) })\n\t\tcheck(func(l *docstore.ActionList) { l.Replace(doc) })\n\t\tcheck(func(l *docstore.ActionList) { l.Put(doc) })\n\t\tcheck(func(l *docstore.ActionList) { l.Update(doc, docstore.Mods{\"a\": 1}) })\n\t\tcheck(func(l *docstore.ActionList) { l.Get(doc) })\n\t\tcheck(func(l *docstore.ActionList) { l.Delete(doc) })\n\t})\n}\n\n// Verify that BeforeQuery is invoked, and its as function behaves as expected.\nfunc testBeforeQuery(t *testing.T, newHarness HarnessMaker) {\n\twithHarnessAndCollection(t, newHarness, func(t *testing.T, ctx context.Context, h Harness, coll *ds.Collection) {\n\t\tvar called bool\n\t\tbeforeQuery := func(asFunc func(interface{}) bool) error {\n\t\t\tcalled = true\n\t\t\tif asFunc(nil) {\n\t\t\t\treturn errors.New(\"asFunc returned true when called with nil, want false\")\n\t\t\t}\n\t\t\t// At least one of the expected types must return true. Special case: if\n\t\t\t// there are no types, then the as function never returns true, so skip the\n\t\t\t// check.\n\t\t\tif len(h.BeforeQueryTypes()) > 0 {\n\t\t\t\tfound := false\n\t\t\t\tfor _, b := range h.BeforeQueryTypes() {\n\t\t\t\t\tv := reflect.New(reflect.TypeOf(b)).Interface()\n\t\t\t\t\tif asFunc(v) {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !found {\n\t\t\t\t\treturn errors.New(\"none of the BeforeQueryTypes works with the as function\")\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\titer := coll.Query().BeforeQuery(beforeQuery).Get(ctx)\n\t\tif err := iter.Next(ctx, docmap{}); err != io.EOF {\n\t\t\tt.Fatalf(\"got %v, wanted io.EOF\", err)\n\t\t}\n\t\tif !called {\n\t\t\tt.Error(\"BeforeQuery function never called for Get\")\n\t\t}\n\n\t\tcalled = false\n\t\tif err := coll.Query().BeforeQuery(beforeQuery).Delete(ctx); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif !called {\n\t\t\tt.Error(\"BeforeQuery function never called for Delete\")\n\t\t}\n\n\t\tcalled = false\n\t\tif err := coll.Query().BeforeQuery(beforeQuery).Update(ctx, ds.Mods{\"a\": 1}); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif !called {\n\t\t\tt.Error(\"BeforeQuery function never called for Update\")\n\t\t}\n\t})\n}\n\nfunc testAs(t *testing.T, coll *ds.Collection, st AsTest) {\n\t// Verify Collection.As\n\tif err := st.CollectionCheck(coll); err != nil {\n\t\tt.Error(err)\n\t}\n\n\tctx := context.Background()\n\n\t// Query\n\tqs := []*docstore.Query{\n\t\tcoll.Query().Where(\"Game\", \"=\", game3),\n\t\t// Note: don't use filter on Player, the test table has Player as the\n\t\t// partition key of a Global Secondary Index, which doesn't support\n\t\t// ConsistentRead mode, which is what the As test does in its BeforeQuery\n\t\t// function.\n\t\tcoll.Query().Where(\"Score\", \">\", 50),\n\t}\n\tfor _, q := range qs {\n\t\titer := q.Get(ctx)\n\t\tif err := st.QueryCheck(iter); err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t}\n\n\t// ErrorCheck\n\tdoc := &HighScore{game3, \"steph\", 24, date(4, 25), nil}\n\tif err := coll.Create(ctx, doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdoc.DocstoreRevision = nil\n\tif err := coll.Create(ctx, doc); err == nil {\n\t\tt.Fatal(\"got nil error from creating an existing item, want an error\")\n\t} else {\n\t\tif alerr, ok := err.(docstore.ActionListError); ok {\n\t\t\tfor _, aerr := range alerr {\n\t\t\t\tif checkerr := st.ErrorCheck(coll, aerr.Err); checkerr != nil {\n\t\t\t\t\tt.Error(checkerr)\n\t\t\t\t}\n\t\t\t}\n\t\t} else if checkerr := st.ErrorCheck(coll, err); checkerr != nil {\n\t\t\tt.Error(checkerr)\n\t\t}\n\t}\n}\n\nfunc clone(m docmap) docmap {\n\tr := docmap{}\n\tfor k, v := range m {\n\t\tr[k] = v\n\t}\n\treturn r\n}\n\nfunc cmpDiff(a, b interface{}, opts ...cmp.Option) string {\n\t// Firestore revisions can be protos.\n\treturn cmp.Diff(a, b, append([]cmp.Option{cmp.Comparer(proto.Equal)}, opts...)...)\n}\n\nfunc checkCode(t *testing.T, err error, code gcerrors.ErrorCode) {\n\tt.Helper()\n\tif gcerrors.Code(err) != code {\n\t\tt.Errorf(\"got %v, want %s\", err, code)\n\t}\n}\n", "idx": 11, "id": 19494, "msg": "", "proj": "google-go-cloud", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -385,19 +385,19 @@ class ProductVisibilityRepositoryTest extends DatabaseTestCase\n         $pricingGroupFacade = $this->getContainer()->get(PricingGroupFacade::class);\n         /* @var $pricingGroupFacade \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade */\n \n-        $productEditData = $this->getDefaultProductEditData();\n-        $productEditData->productData->priceCalculationType = Product::PRICE_CALCULATION_TYPE_MANUAL;\n+        $productData = $this->getDefaultproductData();\n+        $productData->priceCalculationType = Product::PRICE_CALCULATION_TYPE_MANUAL;\n \n         $allPricingGroups = $pricingGroupFacade->getAll();\n         foreach ($allPricingGroups as $pricingGroup) {\n-            $productEditData->manualInputPricesByPricingGroupId[$pricingGroup->getId()] = 10;\n+            $productData->manualInputPricesByPricingGroupId[$pricingGroup->getId()] = 10;\n         }\n \n         $pricingGroupWithZeroPriceId = $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId();\n \n-        $productEditData->manualInputPricesByPricingGroupId[$pricingGroupWithZeroPriceId] = 0;\n+        $productData->manualInputPricesByPricingGroupId[$pricingGroupWithZeroPriceId] = 0;\n \n-        $product = $productFacade->create($productEditData);\n+        $product = $productFacade->create($productData);\n         $productPriceRecalculator->runImmediateRecalculations();\n \n         $entityManagerFacade->clear();", "y": 0, "oldf": "<?php\n\nnamespace Tests\\ShopBundle\\Database\\Model\\Product;\n\nuse DateTime;\nuse Shopsys\\FrameworkBundle\\DataFixtures\\Base\\AvailabilityDataFixture;\nuse Shopsys\\FrameworkBundle\\DataFixtures\\Base\\PricingGroupDataFixture as DemoPricingGroupDataFixture;\nuse Shopsys\\FrameworkBundle\\DataFixtures\\Base\\UnitDataFixture;\nuse Shopsys\\FrameworkBundle\\DataFixtures\\Demo\\CategoryDataFixture;\nuse Shopsys\\FrameworkBundle\\DataFixtures\\Demo\\ProductDataFixture;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\Vat;\nuse Shopsys\\FrameworkBundle\\Model\\Pricing\\Vat\\VatData;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Product;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditData;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditDataFactory;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository;\nuse Tests\\ShopBundle\\Test\\DatabaseTestCase;\n\nclass ProductVisibilityRepositoryTest extends DatabaseTestCase\n{\n    /**\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditData\n     */\n    private function getDefaultProductEditData()\n    {\n        $category = $this->getReference(CategoryDataFixture::CATEGORY_ELECTRONICS);\n\n        $em = $this->getEntityManager();\n        $vat = new Vat(new VatData('vat', 21));\n        $em->persist($vat);\n\n        $productEditData = new ProductEditData();\n        $productEditData->productData->name = ['cs' => 'Name', 'en' => 'Name'];\n        $productEditData->productData->vat = $vat;\n        $productEditData->productData->price = 100;\n        $productEditData->productData->priceCalculationType = Product::PRICE_CALCULATION_TYPE_AUTO;\n        $productEditData->productData->hidden = false;\n        $productEditData->productData->sellingDenied = false;\n        $productEditData->productData->categoriesByDomainId = [1 => [$category]];\n        $productEditData->productData->availability = $this->getReference(AvailabilityDataFixture::AVAILABILITY_IN_STOCK);\n        $productEditData->productData->unit = $this->getReference(UnitDataFixture::UNIT_PIECES);\n\n        return $productEditData;\n    }\n\n    public function testIsVisibleOnAnyDomainWhenHidden()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->hidden = true;\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $em->flush();\n        $id = $product->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productAgain = $em->getRepository(Product::class)->find($id);\n        /* @var $productAgain \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $productVisibility1 = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $productAgain,\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility1 \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertFalse($productAgain->isVisible());\n        $this->assertFalse($productVisibility1->isVisible());\n    }\n\n    public function testIsVisibleOnAnyDomainWhenNotHidden()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $em->flush();\n        $id = $product->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productAgain = $em->getRepository(Product::class)->find($id);\n        /* @var $productAgain \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $productVisibility1 = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $productAgain->getId(),\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility1 \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertTrue($productAgain->isVisible());\n        $this->assertTrue($productVisibility1->isVisible());\n    }\n\n    public function testIsVisibleOnAnyDomainWhenSellingInFuture()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $sellingFrom = new DateTime('now');\n        $sellingFrom->modify('+1 day');\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->sellingFrom = $sellingFrom;\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $em->flush();\n        $id = $product->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productAgain = $em->getRepository(Product::class)->find($id);\n        /* @var $productAgain \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $this->assertFalse($productAgain->isVisible());\n    }\n\n    public function testIsVisibleOnAnyDomainWhenSellingInPast()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $sellingTo = new DateTime('now');\n        $sellingTo->modify('-1 day');\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->sellingTo = $sellingTo;\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $em->flush();\n        $id = $product->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productAgain = $em->getRepository(Product::class)->find($id);\n        /* @var $productAgain \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $this->assertFalse($productAgain->isVisible());\n    }\n\n    public function testIsVisibleOnAnyDomainWhenSellingNow()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $sellingFrom = new DateTime('now');\n        $sellingFrom->modify('-1 day');\n        $sellingTo = new DateTime('now');\n        $sellingTo->modify('+1 day');\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->sellingFrom = $sellingFrom;\n        $productEditData->productData->sellingTo = $sellingTo;\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $em->flush();\n        $id = $product->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productAgain = $em->getRepository(Product::class)->find($id);\n        /* @var $productAgain \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $this->assertTrue($productAgain->isVisible());\n    }\n\n    public function testIsNotVisibleWhenZeroOrNullPrice()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->price = 0;\n        $product1 = $productFacade->create($productEditData);\n\n        $productEditData->productData->price = null;\n        $product2 = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $product1Id = $product1->getId();\n        $product2Id = $product2->getId();\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $product1Again = $em->getRepository(Product::class)->find($product1Id);\n        /* @var $product1Again \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $product2Again = $em->getRepository(Product::class)->find($product2Id);\n        /* @var $product2Again \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $this->assertFalse($product1Again->isVisible());\n        $this->assertFalse($product2Again->isVisible());\n    }\n\n    public function testIsVisibleWithFilledName()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->name = ['cs' => 'Name', 'en' => 'Name'];\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertTrue($productVisibility->isVisible());\n    }\n\n    public function testIsNotVisibleWithEmptyName()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->name = ['cs' => null, 'en' => null];\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertFalse($productVisibility->isVisible());\n    }\n\n    public function testIsVisibleInVisibileCategory()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $category = $this->getReference(CategoryDataFixture::CATEGORY_TOYS);\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->categoriesByDomainId = [1 => [$category]];\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertTrue($productVisibility->isVisible());\n    }\n\n    public function testIsNotVisibleInHiddenCategory()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->categoriesByDomainId = [];\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId(),\n            'domainId' => 1,\n        ]);\n\n        $this->assertFalse($productVisibility->isVisible());\n    }\n\n    public function testIsNotVisibleWhenZeroManualPrice()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n        $pricingGroupFacade = $this->getContainer()->get(PricingGroupFacade::class);\n        /* @var $pricingGroupFacade \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->priceCalculationType = Product::PRICE_CALCULATION_TYPE_MANUAL;\n\n        $allPricingGroups = $pricingGroupFacade->getAll();\n        foreach ($allPricingGroups as $pricingGroup) {\n            $productEditData->manualInputPricesByPricingGroupId[$pricingGroup->getId()] = 10;\n        }\n\n        $pricingGroupWithZeroPriceId = $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId();\n\n        $productEditData->manualInputPricesByPricingGroupId[$pricingGroupWithZeroPriceId] = 0;\n\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $pricingGroupWithZeroPriceId,\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertFalse($productVisibility->isVisible());\n    }\n\n    public function testIsNotVisibleWhenNullManualPrice()\n    {\n        $em = $this->getEntityManager();\n        $entityManagerFacade = $this->getEntityManagerFacade();\n        /* @var $entityManagerFacade \\Shopsys\\FrameworkBundle\\Component\\Doctrine\\EntityManagerFacade */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productPriceRecalculator = $this->getContainer()->get(ProductPriceRecalculator::class);\n        /* @var $productPriceRecalculator \\Shopsys\\FrameworkBundle\\Model\\Product\\Pricing\\ProductPriceRecalculator */\n        $pricingGroupFacade = $this->getContainer()->get(PricingGroupFacade::class);\n        /* @var $pricingGroupFacade \\Shopsys\\FrameworkBundle\\Model\\Pricing\\Group\\PricingGroupFacade */\n\n        $productEditData = $this->getDefaultProductEditData();\n        $productEditData->productData->priceCalculationType = Product::PRICE_CALCULATION_TYPE_MANUAL;\n\n        $allPricingGroups = $pricingGroupFacade->getAll();\n        foreach ($allPricingGroups as $pricingGroup) {\n            $productEditData->manualInputPricesByPricingGroupId[$pricingGroup->getId()] = 10;\n        }\n\n        $pricingGroupWithNullPriceId = $this->getReference(DemoPricingGroupDataFixture::PRICING_GROUP_ORDINARY_DOMAIN_1)->getId();\n        $productEditData->manualInputPricesByPricingGroupId[$pricingGroupWithNullPriceId] = null;\n\n        $product = $productFacade->create($productEditData);\n        $productPriceRecalculator->runImmediateRecalculations();\n\n        $entityManagerFacade->clear();\n\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productVisibilityRepository->refreshProductsVisibility();\n\n        $productVisibility = $em->getRepository(ProductVisibility::class)->findOneBy([\n            'product' => $product,\n            'pricingGroup' => $pricingGroupWithNullPriceId,\n            'domainId' => 1,\n        ]);\n        /* @var $productVisibility \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibility */\n\n        $this->assertFalse($productVisibility->isVisible());\n    }\n\n    public function testRefreshProductsVisibilityVisibleVariants()\n    {\n        $em = $this->getEntityManager();\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productEditDataFactory = $this->getContainer()->get(ProductEditDataFactory::class);\n        /* @var $productEditDataFactory \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditDataFactory */\n\n        $variant1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '53');\n        /* @var $variant1 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '54');\n        /* @var $variant2 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant3 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '69');\n        /* @var $variant3 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $mainVariant = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '148');\n        /* @var $mainVariant \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $variant1ProductEditData = $productEditDataFactory->createFromProduct($variant1);\n        $variant1ProductEditData->productData->hidden = true;\n        $productFacade->edit($variant1->getId(), $variant1ProductEditData);\n\n        $productVisibilityRepository->refreshProductsVisibility(true);\n\n        $em->refresh($variant1);\n        $em->refresh($variant2);\n        $em->refresh($variant3);\n        $em->refresh($mainVariant);\n\n        $this->assertFalse($variant1->isVisible());\n        $this->assertTrue($variant2->isVisible());\n        $this->assertTrue($variant3->isVisible());\n        $this->assertTrue($mainVariant->isVisible());\n    }\n\n    public function testRefreshProductsVisibilityNotVisibleVariants()\n    {\n        $em = $this->getEntityManager();\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productEditDataFactory = $this->getContainer()->get(ProductEditDataFactory::class);\n        /* @var $productEditDataFactory \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditDataFactory */\n\n        $variant1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '53');\n        /* @var $variant1 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '54');\n        /* @var $variant2 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant3 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '69');\n        /* @var $variant3 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $mainVariant = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '148');\n        /* @var $mainVariant \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $variant1ProductEditData = $productEditDataFactory->createFromProduct($variant1);\n        $variant1ProductEditData->productData->hidden = true;\n        $productFacade->edit($variant1->getId(), $variant1ProductEditData);\n\n        $variant2ProductEditData = $productEditDataFactory->createFromProduct($variant2);\n        $variant2ProductEditData->productData->hidden = true;\n        $productFacade->edit($variant2->getId(), $variant2ProductEditData);\n\n        $variant3ProductEditData = $productEditDataFactory->createFromProduct($variant3);\n        $variant3ProductEditData->productData->hidden = true;\n        $productFacade->edit($variant3->getId(), $variant3ProductEditData);\n\n        $productVisibilityRepository->refreshProductsVisibility(true);\n\n        $em->refresh($variant1);\n        $em->refresh($variant2);\n        $em->refresh($variant3);\n        $em->refresh($mainVariant);\n\n        $this->assertFalse($variant1->isVisible());\n        $this->assertFalse($variant2->isVisible());\n        $this->assertFalse($variant3->isVisible());\n        $this->assertFalse($mainVariant->isVisible());\n    }\n\n    public function testRefreshProductsVisibilityNotVisibleMainVariant()\n    {\n        $em = $this->getEntityManager();\n        $productVisibilityRepository = $this->getContainer()->get(ProductVisibilityRepository::class);\n        /* @var $productVisibilityRepository \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductVisibilityRepository */\n        $productFacade = $this->getContainer()->get(ProductFacade::class);\n        /* @var $productFacade \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade */\n        $productEditDataFactory = $this->getContainer()->get(ProductEditDataFactory::class);\n        /* @var $productEditDataFactory \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductEditDataFactory */\n\n        $variant1 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '53');\n        /* @var $variant1 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant2 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '54');\n        /* @var $variant2 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $variant3 = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '69');\n        /* @var $variant3 \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n        $mainVariant = $this->getReference(ProductDataFixture::PRODUCT_PREFIX . '148');\n        /* @var $mainVariant \\Shopsys\\FrameworkBundle\\Model\\Product\\Product */\n\n        $mainVariantProductEditData = $productEditDataFactory->createFromProduct($mainVariant);\n        $mainVariantProductEditData->productData->hidden = true;\n        $productFacade->edit($mainVariant->getId(), $mainVariantProductEditData);\n\n        $productVisibilityRepository->refreshProductsVisibility(true);\n\n        $em->refresh($variant1);\n        $em->refresh($variant2);\n        $em->refresh($variant3);\n        $em->refresh($mainVariant);\n\n        $this->assertFalse($variant1->isVisible());\n        $this->assertFalse($variant2->isVisible());\n        $this->assertFalse($variant3->isVisible());\n        $this->assertFalse($mainVariant->isVisible());\n    }\n}\n", "idx": 14, "id": 10339, "msg": "", "proj": "shopsys-shopsys", "lang": "php", "sampling_weight": 0.0739150857232327}
{"patch": "@@ -67,29 +67,33 @@ func TestActPool_validateTsf(t *testing.T) {\n \tap, ok := Ap.(*actPool)\n \trequire.True(ok)\n \t// Case I: Coinbase transfer\n-\tcoinbaseTsf := action.Transfer{IsCoinbase: true}\n-\terr = ap.validateTsf(&coinbaseTsf)\n+\tcoinbaseTsf := action.NewCoinBaseTransfer(big.NewInt(1), \"1\")\n+\terr = ap.validateTsf(coinbaseTsf)\n \trequire.Equal(ErrTransfer, errors.Cause(err))\n \t// Case II: Oversized data\n \ttmpPayload := [32769]byte{}\n \tpayload := tmpPayload[:]\n-\ttsf := action.Transfer{Payload: payload}\n-\terr = ap.validateTsf(&tsf)\n+\ttsf, err := action.NewTransfer(uint64(1), big.NewInt(1), \"1\", \"2\", payload, uint64(0), big.NewInt(0))\n+\trequire.NoError(err)\n+\terr = ap.validateTsf(tsf)\n \trequire.Equal(ErrActPool, errors.Cause(err))\n \t// Case III: Negative amount\n-\ttsf = action.Transfer{Amount: big.NewInt(-100)}\n-\terr = ap.validateTsf(&tsf)\n+\ttsf, err = action.NewTransfer(uint64(1), big.NewInt(-100), \"1\", \"2\", nil, uint64(0), big.NewInt(0))\n+\trequire.NoError(err)\n+\terr = ap.validateTsf(tsf)\n \trequire.Equal(ErrBalance, errors.Cause(err))\n \t// Case IV: Invalid address\n-\ttsf = action.Transfer{Sender: addr1.RawAddress, Recipient: \"io1qyqsyqcyq5narhapakcsrhksfajfcpl24us3xp38zwvsep\", Amount: big.NewInt(1)}\n-\terr = ap.validateTsf(&tsf)\n+\ttsf, err = action.NewTransfer(\n+\t\t1, big.NewInt(1), addr1.RawAddress, \"io1qyqsyqcyq5narhapakcsrhksfajfcpl24us3xp38zwvsep\", nil, uint64(0), big.NewInt(0))\n+\trequire.NoError(err)\n+\terr = ap.validateTsf(tsf)\n \trequire.Error(err)\n \trequire.True(strings.Contains(err.Error(), \"error when validating recipient's address\"))\n \t// Case V: Signature verification fails\n \tunsignedTsf, err := action.NewTransfer(uint64(1), big.NewInt(1), addr1.RawAddress, addr1.RawAddress, []byte{}, uint64(100000), big.NewInt(10))\n \trequire.NoError(err)\n \terr = ap.validateTsf(unsignedTsf)\n-\trequire.Equal(action.ErrTransferError, errors.Cause(err))\n+\trequire.Equal(action.ErrAction, errors.Cause(err))\n \t// Case VI: Nonce is too low\n \tprevTsf, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n \terr = ap.AddTsf(prevTsf)", "y": 1, "oldf": "// Copyright (c) 2018 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage actpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/iotexproject/iotex-core/blockchain\"\n\t\"github.com/iotexproject/iotex-core/blockchain/action\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/iotxaddress\"\n\t\"github.com/iotexproject/iotex-core/proto\"\n\t\"github.com/iotexproject/iotex-core/test/mock/mock_blockchain\"\n\t\"github.com/iotexproject/iotex-core/testutil\"\n)\n\nconst (\n\tpubkeyA = \"2c9ccbeb9ee91271f7e5c2103753be9c9edff847e1a51227df6a6b0765f31a4b424e84027b44a663950f013a88b8fd8cdc53b1eda1d4b73f9d9dc12546c8c87d68ff1435a0f8a006\"\n\tprikeyA = \"b5affb30846a00ef5aa39b57f913d70cd8cf6badd587239863cb67feacf6b9f30c34e800\"\n\tpubkeyB = \"881504d84a0659e14dcba59f24a98e71cda55b139615342668840c64678f1514941bbd053c7492fb9b719e6050cfa972efa491b79e11a1713824dda5f638fc0d9fa1b68be3c0f905\"\n\tprikeyB = \"b89c1ec0fb5b192c8bb8f6fcf9a871e4a67ef462f40d2b8ff426da1d1eaedd9696dc9d00\"\n\tpubkeyC = \"252fc7bc9a993b68dd7b13a00213c9cf4befe80da49940c52220f93c7147771ba2d783045cf0fbf2a86b32a62848befb96c0f38c0487a5ccc806ff28bb06d9faf803b93dda107003\"\n\tprikeyC = \"3e05de562a27fb6e25ac23ff8bcaa1ada0c253fa8ff7c6d15308f65d06b6990f64ee9601\"\n\tpubkeyD = \"29aa28cc21c3ee3cc658d3a322997ceb8d5d352f45d052192d3ab57cd196d3375af558067f5a2cfe5fc65d5249cc07f991bab683468382a3acaa4c8b7af35156b46aeda00620f307\"\n\tprikeyD = \"d4b7b441382751d9a1955152b46a69f3c9f9559c6205757af928f5181ff207060d0dab00\"\n\tpubkeyE = \"64dc2d5f445a78b884527252a3dba1f72f52251c97ec213dda99868882024d4d1442f100c8f1f833d0c687871a959ee97665dea24de1a627cce6c970d9db5859da9e4295bb602e04\"\n\tprikeyE = \"53a827f7c5b4b4040b22ae9b12fcaa234e8362fa022480f50b8643981806ed67c7f77a00\"\n)\n\nconst (\n\tmaxNumActsPerPool = 8192\n\tmaxNumActsPerAcct = 256\n)\n\nvar (\n\taddr1 = testutil.ConstructAddress(pubkeyA, prikeyA)\n\taddr2 = testutil.ConstructAddress(pubkeyB, prikeyB)\n\taddr3 = testutil.ConstructAddress(pubkeyC, prikeyC)\n\taddr4 = testutil.ConstructAddress(pubkeyD, prikeyD)\n\taddr5 = testutil.ConstructAddress(pubkeyE, prikeyE)\n)\n\nfunc TestActPool_validateTsf(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\t// Case I: Coinbase transfer\n\tcoinbaseTsf := action.Transfer{IsCoinbase: true}\n\terr = ap.validateTsf(&coinbaseTsf)\n\trequire.Equal(ErrTransfer, errors.Cause(err))\n\t// Case II: Oversized data\n\ttmpPayload := [32769]byte{}\n\tpayload := tmpPayload[:]\n\ttsf := action.Transfer{Payload: payload}\n\terr = ap.validateTsf(&tsf)\n\trequire.Equal(ErrActPool, errors.Cause(err))\n\t// Case III: Negative amount\n\ttsf = action.Transfer{Amount: big.NewInt(-100)}\n\terr = ap.validateTsf(&tsf)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\t// Case IV: Invalid address\n\ttsf = action.Transfer{Sender: addr1.RawAddress, Recipient: \"io1qyqsyqcyq5narhapakcsrhksfajfcpl24us3xp38zwvsep\", Amount: big.NewInt(1)}\n\terr = ap.validateTsf(&tsf)\n\trequire.Error(err)\n\trequire.True(strings.Contains(err.Error(), \"error when validating recipient's address\"))\n\t// Case V: Signature verification fails\n\tunsignedTsf, err := action.NewTransfer(uint64(1), big.NewInt(1), addr1.RawAddress, addr1.RawAddress, []byte{}, uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\terr = ap.validateTsf(unsignedTsf)\n\trequire.Equal(action.ErrTransferError, errors.Cause(err))\n\t// Case VI: Nonce is too low\n\tprevTsf, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(prevTsf)\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, []*action.Transfer{prevTsf}, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tap.Reset()\n\tnTsf, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(60), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.validateTsf(nTsf)\n\trequire.Equal(ErrNonce, errors.Cause(err))\n}\n\nfunc TestActPool_validateVote(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\t// Case I: Oversized data\n\ttmpSelfPubKey := [32769]byte{}\n\tselfPubKey := tmpSelfPubKey[:]\n\tvote := action.Vote{\n\t\tActionPb: &iproto.ActionPb{\n\t\t\tAction: &iproto.ActionPb_Vote{\n\t\t\t\tVote: &iproto.VotePb{\n\t\t\t\t\tSelfPubkey: selfPubKey},\n\t\t\t},\n\t\t},\n\t}\n\terr = ap.validateVote(&vote)\n\trequire.Equal(ErrActPool, errors.Cause(err))\n\t// Case II: Invalid voter's public key\n\tvote = action.Vote{\n\t\tActionPb: &iproto.ActionPb{\n\t\t\tAction: &iproto.ActionPb_Vote{\n\t\t\t\tVote: &iproto.VotePb{},\n\t\t\t},\n\t\t},\n\t}\n\terr = ap.validateVote(&vote)\n\trequire.Error(err)\n\trequire.True(strings.Contains(err.Error(), \"failed to get voter's public key\"))\n\t// Case III: Invalid address\n\tvote = action.Vote{\n\t\tActionPb: &iproto.ActionPb{\n\t\t\tAction: &iproto.ActionPb_Vote{\n\t\t\t\tVote: &iproto.VotePb{\n\t\t\t\t\tSelfPubkey:   addr1.PublicKey[:],\n\t\t\t\t\tVoterAddress: addr1.RawAddress,\n\t\t\t\t\tVoteeAddress: \"123\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\terr = ap.validateVote(&vote)\n\trequire.Error(err)\n\trequire.True(strings.Contains(err.Error(), \"error when validating votee's address\"))\n\t// Case IV: Signature verification fails\n\tunsignedVote, err := action.NewVote(1, addr1.RawAddress, addr2.RawAddress, uint64(100000), big.NewInt(10))\n\tunsignedVote.GetVote().SelfPubkey = addr1.PublicKey[:]\n\trequire.NoError(err)\n\terr = ap.validateVote(unsignedVote)\n\trequire.Equal(action.ErrVoteError, errors.Cause(err))\n\t// Case V: Nonce is too low\n\tprevTsf, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(prevTsf)\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, []*action.Transfer{prevTsf}, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tap.Reset()\n\tnVote, _ := signedVote(addr1, addr1, uint64(1), uint64(100000), big.NewInt(10))\n\terr = ap.validateVote(nVote)\n\trequire.Equal(ErrNonce, errors.Cause(err))\n\t// Case VI: Votee is not a candidate\n\tvote2, _ := signedVote(addr1, addr2, uint64(2), uint64(100000), big.NewInt(10))\n\terr = ap.validateVote(vote2)\n\trequire.Equal(ErrVotee, errors.Cause(err))\n}\n\nfunc TestActPool_AddActs(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(10))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\t// Test actpool status after adding a sequence of Tsfs/votes: need to check confirmed nonce, pending nonce, and pending balance\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf2, _ := signedTransfer(addr1, addr1, uint64(2), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\tvote4, _ := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\ttsf5, _ := signedTransfer(addr1, addr1, uint64(5), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf6, _ := signedTransfer(addr2, addr2, uint64(1), big.NewInt(5), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf7, _ := signedTransfer(addr2, addr2, uint64(3), big.NewInt(1), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf8, _ := signedTransfer(addr2, addr2, uint64(4), big.NewInt(5), []byte{}, uint64(100000), big.NewInt(10))\n\n\terr = ap.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf2)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf3)\n\trequire.NoError(err)\n\terr = ap.AddVote(vote4)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf5)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\terr = ap.AddTsf(tsf6)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf7)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf8)\n\trequire.NoError(err)\n\n\tpBalance1, _ := ap.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(uint64(40), pBalance1.Uint64())\n\tpNonce1, _ := ap.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(5), pNonce1)\n\n\tpBalance2, _ := ap.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(uint64(5), pBalance2.Uint64())\n\tpNonce2, _ := ap.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(2), pNonce2)\n\n\ttsf9, _ := signedTransfer(addr2, addr2, uint64(2), big.NewInt(3), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(tsf9)\n\trequire.NoError(err)\n\tpBalance2, _ = ap.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(uint64(1), pBalance2.Uint64())\n\tpNonce2, _ = ap.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(4), pNonce2)\n\t// Error Case Handling\n\t// Case I: Action already exists in pool\n\terr = ap.AddTsf(tsf1)\n\trequire.Equal(fmt.Errorf(\"existed transfer: %x\", tsf1.Hash()), err)\n\terr = ap.AddVote(vote4)\n\trequire.Equal(fmt.Errorf(\"existed vote: %x\", vote4.Hash()), err)\n\t// Case II: Pool space is full\n\tmockBC := mock_blockchain.NewMockBlockchain(ctrl)\n\tAp2, err := NewActPool(mockBC, apConfig)\n\trequire.NoError(err)\n\tap2, ok := Ap2.(*actPool)\n\trequire.True(ok)\n\tfor i := uint64(0); i < ap2.cfg.MaxNumActsPerPool; i++ {\n\t\tnTsf := action.Transfer{Amount: big.NewInt(int64(i))}\n\t\tnAction := nTsf.ConvertToActionPb()\n\t\tap2.allActions[nTsf.Hash()] = nAction\n\t}\n\tmockBC.EXPECT().Nonce(gomock.Any()).Times(2).Return(uint64(0), nil)\n\tmockBC.EXPECT().StateByAddr(gomock.Any()).Times(1).Return(nil, nil)\n\terr = ap2.AddTsf(tsf1)\n\trequire.Equal(ErrActPool, errors.Cause(err))\n\terr = ap2.AddVote(vote4)\n\trequire.Equal(ErrActPool, errors.Cause(err))\n\t// Case III: Nonce already exists\n\treplaceTsf, _ := signedTransfer(addr1, addr2, uint64(1), big.NewInt(1), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(replaceTsf)\n\trequire.Equal(ErrNonce, errors.Cause(err))\n\treplaceVote, err := action.NewVote(4, addr1.RawAddress, \"\", uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\treplaceVote, _ = replaceVote.Sign(addr1)\n\terr = ap.AddVote(replaceVote)\n\trequire.Equal(ErrNonce, errors.Cause(err))\n\t// Case IV: Nonce is too large\n\toutOfBoundsTsf, _ := signedTransfer(addr1, addr1, ap.cfg.MaxNumActsPerAcct+1, big.NewInt(1), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(outOfBoundsTsf)\n\trequire.Equal(ErrNonce, errors.Cause(err))\n\t// Case V: Insufficient balance\n\toverBalTsf, _ := signedTransfer(addr2, addr2, uint64(4), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\terr = ap.AddTsf(overBalTsf)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\t// Case VI: over gas limit\n\tcreationExecution, err := action.NewExecution(addr1.RawAddress, action.EmptyAddress, uint64(5), big.NewInt(int64(0)), blockchain.GasLimit+100, big.NewInt(10), []byte{})\n\trequire.NoError(err)\n\terr = ap.AddExecution(creationExecution)\n\trequire.Equal(ErrGasHigherThanLimit, errors.Cause(err))\n\t// Case VII: insufficient gas\n\tcreationExecution.GasLimit = 10\n\ttmpData := [1234]byte{}\n\tcreationExecution.Data = tmpData[:]\n\terr = ap.AddExecution(creationExecution)\n\trequire.Equal(ErrInsufficientGas, errors.Cause(err))\n}\n\nfunc TestActPool_PickActs(t *testing.T) {\n\tcreateActPool := func(cfg config.ActPool) (*actPool, []*action.Transfer, []*action.Vote, []*action.Execution) {\n\t\trequire := require.New(t)\n\t\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\t\trequire.NoError(bc.Start(context.Background()))\n\t\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\t\trequire.NoError(err)\n\t\t_, err = bc.CreateState(addr2.RawAddress, uint64(10))\n\t\trequire.NoError(err)\n\t\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\t\trequire.NoError(err)\n\t\trequire.Nil(bc.GetFactory().Commit())\n\t\t// Create actpool\n\t\tAp, err := NewActPool(bc, cfg)\n\t\trequire.NoError(err)\n\t\tap, ok := Ap.(*actPool)\n\t\trequire.True(ok)\n\n\t\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf2, _ := signedTransfer(addr1, addr1, uint64(2), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf4, _ := signedTransfer(addr1, addr1, uint64(4), big.NewInt(40), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf5, _ := signedTransfer(addr1, addr1, uint64(5), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\t\tvote6, _ := signedVote(addr1, addr1, uint64(6), uint64(100000), big.NewInt(10))\n\t\tvote7, _ := signedVote(addr2, addr2, uint64(1), uint64(100000), big.NewInt(10))\n\t\ttsf8, _ := signedTransfer(addr2, addr2, uint64(3), big.NewInt(5), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf9, _ := signedTransfer(addr2, addr2, uint64(4), big.NewInt(1), []byte{}, uint64(100000), big.NewInt(10))\n\t\ttsf10, _ := signedTransfer(addr2, addr2, uint64(5), big.NewInt(5), []byte{}, uint64(100000), big.NewInt(10))\n\n\t\terr = ap.AddTsf(tsf1)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf2)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf3)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf4)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf5)\n\t\trequire.Equal(ErrBalance, errors.Cause(err))\n\t\terr = ap.AddVote(vote6)\n\t\trequire.NoError(err)\n\t\terr = ap.AddVote(vote7)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf8)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf9)\n\t\trequire.NoError(err)\n\t\terr = ap.AddTsf(tsf10)\n\t\trequire.NoError(err)\n\t\treturn ap, []*action.Transfer{tsf1, tsf2, tsf3, tsf4}, []*action.Vote{vote7}, []*action.Execution{}\n\t}\n\n\tt.Run(\"no-limit\", func(t *testing.T) {\n\t\tapConfig := getActPoolCfg()\n\t\tap, transfers, votes, executions := createActPool(apConfig)\n\t\tpickedTsfs, pickedVotes, pickedExecutions := ap.PickActs()\n\t\trequire.Equal(t, transfers, pickedTsfs)\n\t\trequire.Equal(t, votes, pickedVotes)\n\t\trequire.Equal(t, executions, pickedExecutions)\n\t})\n\tt.Run(\"enough-limit\", func(t *testing.T) {\n\t\tapConfig := getActPoolCfg()\n\t\tapConfig.MaxNumActsToPick = 10\n\t\tap, transfers, votes, executions := createActPool(apConfig)\n\t\tpickedTsfs, pickedVotes, pickedExecutions := ap.PickActs()\n\t\trequire.Equal(t, transfers, pickedTsfs)\n\t\trequire.Equal(t, votes, pickedVotes)\n\t\trequire.Equal(t, executions, pickedExecutions)\n\t})\n\tt.Run(\"low-limit\", func(t *testing.T) {\n\t\tapConfig := getActPoolCfg()\n\t\tapConfig.MaxNumActsToPick = 3\n\t\tap, _, _, _ := createActPool(apConfig)\n\t\tpickedTsfs, pickedVotes, pickedExecutions := ap.PickActs()\n\t\trequire.Equal(t, 3, len(pickedTsfs)+len(pickedVotes)+len(pickedExecutions))\n\t})\n}\n\nfunc TestActPool_removeConfirmedActs(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf2, _ := signedTransfer(addr1, addr1, uint64(2), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\tvote4, _ := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\n\terr = ap.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf2)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf3)\n\trequire.NoError(err)\n\terr = ap.AddVote(vote4)\n\trequire.NoError(err)\n\n\trequire.Equal(4, len(ap.allActions))\n\trequire.NotNil(ap.accountActs[addr1.RawAddress])\n\t_, err = bc.GetFactory().RunActions(0, []*action.Transfer{tsf1, tsf2, tsf3}, []*action.Vote{vote4}, []*action.Execution{})\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tap.removeConfirmedActs()\n\trequire.Equal(0, len(ap.allActions))\n\trequire.Nil(ap.accountActs[addr1.RawAddress])\n}\n\nfunc TestActPool_Reset(t *testing.T) {\n\trequire := require.New(t)\n\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(200))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr3.RawAddress, uint64(300))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\n\tapConfig := getActPoolCfg()\n\tAp1, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap1, ok := Ap1.(*actPool)\n\trequire.True(ok)\n\tAp2, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap2, ok := Ap2.(*actPool)\n\trequire.True(ok)\n\n\t// Tsfs to be added to ap1\n\ttsf1, _ := signedTransfer(addr1, addr2, uint64(1), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf2, _ := signedTransfer(addr1, addr3, uint64(2), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf3, _ := signedTransfer(addr1, addr2, uint64(3), big.NewInt(60), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf4, _ := signedTransfer(addr2, addr1, uint64(1), big.NewInt(100), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf5, _ := signedTransfer(addr2, addr3, uint64(2), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf6, _ := signedTransfer(addr2, addr1, uint64(3), big.NewInt(60), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf7, _ := signedTransfer(addr3, addr1, uint64(1), big.NewInt(100), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf8, _ := signedTransfer(addr3, addr2, uint64(2), big.NewInt(100), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf9, _ := signedTransfer(addr3, addr1, uint64(4), big.NewInt(100), []byte{}, uint64(100000), big.NewInt(10))\n\n\terr = ap1.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf2)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf3)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\terr = ap1.AddTsf(tsf4)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf5)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf6)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\terr = ap1.AddTsf(tsf7)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf8)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf9)\n\trequire.NoError(err)\n\t// Tsfs to be added to ap2 only\n\ttsf10, _ := signedTransfer(addr1, addr2, uint64(3), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf11, _ := signedTransfer(addr1, addr3, uint64(4), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf12, _ := signedTransfer(addr2, addr3, uint64(2), big.NewInt(70), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf13, _ := signedTransfer(addr3, addr1, uint64(1), big.NewInt(200), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf14, _ := signedTransfer(addr3, addr2, uint64(2), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\n\terr = ap2.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf2)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf10)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf11)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\terr = ap2.AddTsf(tsf4)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf12)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf13)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf14)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf9)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\t// Check confirmed nonce, pending nonce, and pending balance after adding Tsfs above for each account\n\t// ap1\n\t// Addr1\n\tap1PNonce1, _ := ap1.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce1)\n\tap1PBalance1, _ := ap1.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(20).Uint64(), ap1PBalance1.Uint64())\n\t// Addr2\n\tap1PNonce2, _ := ap1.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce2)\n\tap1PBalance2, _ := ap1.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(50).Uint64(), ap1PBalance2.Uint64())\n\t// Addr3\n\tap1PNonce3, _ := ap1.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce3)\n\tap1PBalance3, _ := ap1.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(100).Uint64(), ap1PBalance3.Uint64())\n\t// ap2\n\t// Addr1\n\tap2PNonce1, _ := ap2.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(4), ap2PNonce1)\n\tap2PBalance1, _ := ap2.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(0).Uint64(), ap2PBalance1.Uint64())\n\t// Addr2\n\tap2PNonce2, _ := ap2.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce2)\n\tap2PBalance2, _ := ap2.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(30).Uint64(), ap2PBalance2.Uint64())\n\t// Addr3\n\tap2PNonce3, _ := ap2.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce3)\n\tap2PBalance3, _ := ap2.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(50).Uint64(), ap2PBalance3.Uint64())\n\t// Let ap1 be BP's actpool\n\tpickedTsfs, pickedVotes, pickedExecutions := ap1.PickActs()\n\t// ap1 commits update of accounts to trie\n\t_, err = bc.GetFactory().RunActions(0, pickedTsfs, pickedVotes, pickedExecutions)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t//Reset\n\tap1.Reset()\n\tap2.Reset()\n\t// Check confirmed nonce, pending nonce, and pending balance after resetting actpool for each account\n\t// ap1\n\t// Addr1\n\tap1PNonce1, _ = ap1.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce1)\n\tap1PBalance1, _ = ap1.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(220).Uint64(), ap1PBalance1.Uint64())\n\t// Addr2\n\tap1PNonce2, _ = ap1.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce2)\n\tap1PBalance2, _ = ap1.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(200).Uint64(), ap1PBalance2.Uint64())\n\t// Addr3\n\tap1PNonce3, _ = ap1.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce3)\n\tap1PBalance3, _ = ap1.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(180).Uint64(), ap1PBalance3.Uint64())\n\t// ap2\n\t// Addr1\n\tap2PNonce1, _ = ap2.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(4), ap2PNonce1)\n\tap2PBalance1, _ = ap2.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(200).Uint64(), ap2PBalance1.Uint64())\n\t// Addr2\n\tap2PNonce2, _ = ap2.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce2)\n\tap2PBalance2, _ = ap2.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(200).Uint64(), ap2PBalance2.Uint64())\n\t// Addr3\n\tap2PNonce3, _ = ap2.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce3)\n\tap2PBalance3, _ = ap2.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(180).Uint64(), ap2PBalance3.Uint64())\n\t// Add more Tsfs after resetting\n\t// Tsfs To be added to ap1 only\n\ttsf15, _ := signedTransfer(addr3, addr2, uint64(3), big.NewInt(80), []byte{}, uint64(100000), big.NewInt(10))\n\t// Tsfs To be added to ap2 only\n\ttsf16, _ := signedTransfer(addr1, addr2, uint64(4), big.NewInt(150), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf17, _ := signedTransfer(addr2, addr1, uint64(3), big.NewInt(90), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf18, _ := signedTransfer(addr2, addr3, uint64(4), big.NewInt(100), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf19, _ := signedTransfer(addr2, addr1, uint64(5), big.NewInt(50), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf20, _ := signedTransfer(addr3, addr2, uint64(3), big.NewInt(200), []byte{}, uint64(100000), big.NewInt(10))\n\n\terr = ap1.AddTsf(tsf15)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf16)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf17)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf18)\n\trequire.NoError(err)\n\terr = ap2.AddTsf(tsf19)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\terr = ap2.AddTsf(tsf20)\n\trequire.Equal(ErrBalance, errors.Cause(err))\n\t// Check confirmed nonce, pending nonce, and pending balance after adding Tsfs above for each account\n\t// ap1\n\t// Addr1\n\tap1PNonce1, _ = ap1.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce1)\n\tap1PBalance1, _ = ap1.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(220).Uint64(), ap1PBalance1.Uint64())\n\t// Addr2\n\tap1PNonce2, _ = ap1.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(3), ap1PNonce2)\n\tap1PBalance2, _ = ap1.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(200).Uint64(), ap1PBalance2.Uint64())\n\t// Addr3\n\tap1PNonce3, _ = ap1.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(5), ap1PNonce3)\n\tap1PBalance3, _ = ap1.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(0).Uint64(), ap1PBalance3.Uint64())\n\t// ap2\n\t// Addr1\n\tap2PNonce1, _ = ap2.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(5), ap2PNonce1)\n\tap2PBalance1, _ = ap2.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(50).Uint64(), ap2PBalance1.Uint64())\n\t// Addr2\n\tap2PNonce2, _ = ap2.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(5), ap2PNonce2)\n\tap2PBalance2, _ = ap2.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(10).Uint64(), ap2PBalance2.Uint64())\n\t// Addr3\n\tap2PNonce3, _ = ap2.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce3)\n\tap2PBalance3, _ = ap2.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(180).Uint64(), ap2PBalance3.Uint64())\n\t// Let ap2 be BP's actpool\n\tpickedTsfs, pickedVotes, pickedExecutions = ap2.PickActs()\n\t// ap2 commits update of accounts to trie\n\t_, err = bc.GetFactory().RunActions(0, pickedTsfs, pickedVotes, pickedExecutions)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t//Reset\n\tap1.Reset()\n\tap2.Reset()\n\t// Check confirmed nonce, pending nonce, and pending balance after resetting actpool for each account\n\t// ap1\n\t// Addr1\n\tap1PNonce1, _ = ap1.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(5), ap1PNonce1)\n\tap1PBalance1, _ = ap1.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(140).Uint64(), ap1PBalance1.Uint64())\n\t// Addr2\n\tap1PNonce2, _ = ap1.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(5), ap1PNonce2)\n\tap1PBalance2, _ = ap1.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(180).Uint64(), ap1PBalance2.Uint64())\n\t// Addr3\n\tap1PNonce3, _ = ap1.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(5), ap1PNonce3)\n\tap1PBalance3, _ = ap1.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(100).Uint64(), ap1PBalance3.Uint64())\n\t// ap2\n\t// Addr1\n\tap2PNonce1, _ = ap2.getPendingNonce(addr1.RawAddress)\n\trequire.Equal(uint64(5), ap2PNonce1)\n\tap2PBalance1, _ = ap2.getPendingBalance(addr1.RawAddress)\n\trequire.Equal(big.NewInt(140).Uint64(), ap2PBalance1.Uint64())\n\t// Addr2\n\tap2PNonce2, _ = ap2.getPendingNonce(addr2.RawAddress)\n\trequire.Equal(uint64(5), ap2PNonce2)\n\tap2PBalance2, _ = ap2.getPendingBalance(addr2.RawAddress)\n\trequire.Equal(big.NewInt(180).Uint64(), ap2PBalance2.Uint64())\n\t// Addr3\n\tap2PNonce3, _ = ap2.getPendingNonce(addr3.RawAddress)\n\trequire.Equal(uint64(3), ap2PNonce3)\n\tap2PBalance3, _ = ap2.getPendingBalance(addr3.RawAddress)\n\trequire.Equal(big.NewInt(280).Uint64(), ap2PBalance3.Uint64())\n\n\t// Add two more players\n\t_, err = bc.CreateState(addr4.RawAddress, uint64(10))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr5.RawAddress, uint64(20))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(1, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\ttsf21, _ := signedTransfer(addr4, addr5, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\tvote22, _ := signedVote(addr4, addr4, uint64(2), uint64(100000), big.NewInt(10))\n\tvote23, _ := action.NewVote(3, addr4.RawAddress, \"\", uint64(100000), big.NewInt(10))\n\tvote23, _ = vote23.Sign(addr4)\n\tvote24, _ := signedVote(addr5, addr5, uint64(1), uint64(100000), big.NewInt(10))\n\ttsf25, _ := signedTransfer(addr5, addr4, uint64(2), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\tvote26, _ := action.NewVote(3, addr5.RawAddress, \"\", uint64(100000), big.NewInt(10))\n\tvote26, _ = vote26.Sign(addr5)\n\n\terr = ap1.AddTsf(tsf21)\n\trequire.NoError(err)\n\terr = ap1.AddVote(vote22)\n\trequire.NoError(err)\n\terr = ap1.AddVote(vote23)\n\trequire.NoError(err)\n\terr = ap1.AddVote(vote24)\n\trequire.NoError(err)\n\terr = ap1.AddTsf(tsf25)\n\trequire.NoError(err)\n\terr = ap1.AddVote(vote26)\n\trequire.NoError(err)\n\t// Check confirmed nonce, pending nonce, and pending balance after adding actions above for account4 and account5\n\t// ap1\n\t// Addr4\n\tap1PNonce4, _ := ap1.getPendingNonce(addr4.RawAddress)\n\trequire.Equal(uint64(4), ap1PNonce4)\n\tap1PBalance4, _ := ap1.getPendingBalance(addr4.RawAddress)\n\trequire.Equal(big.NewInt(0).Uint64(), ap1PBalance4.Uint64())\n\t// Addr5\n\tap1PNonce5, _ := ap1.getPendingNonce(addr5.RawAddress)\n\trequire.Equal(uint64(4), ap1PNonce5)\n\tap1PBalance5, _ := ap1.getPendingBalance(addr5.RawAddress)\n\trequire.Equal(big.NewInt(10).Uint64(), ap1PBalance5.Uint64())\n\t// Let ap1 be BP's actpool\n\tpickedTsfs, pickedVotes, pickedExecutions = ap1.PickActs()\n\t// ap1 commits update of accounts to trie\n\t_, err = bc.GetFactory().RunActions(0, pickedTsfs, pickedVotes, pickedExecutions)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t//Reset\n\tap1.Reset()\n\t// Check confirmed nonce, pending nonce, and pending balance after resetting actpool for each account\n\t// ap1\n\t// Addr4\n\tap1PNonce4, _ = ap1.getPendingNonce(addr4.RawAddress)\n\trequire.Equal(uint64(4), ap1PNonce4)\n\tap1PBalance4, _ = ap1.getPendingBalance(addr4.RawAddress)\n\trequire.Equal(big.NewInt(10).Uint64(), ap1PBalance4.Uint64())\n\t// Addr5\n\tap1PNonce5, _ = ap1.getPendingNonce(addr5.RawAddress)\n\trequire.Equal(uint64(4), ap1PNonce5)\n\tap1PBalance5, _ = ap1.getPendingBalance(addr5.RawAddress)\n\trequire.Equal(big.NewInt(20).Uint64(), ap1PBalance5.Uint64())\n}\n\nfunc TestActPool_removeInvalidActs(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf2, _ := signedTransfer(addr1, addr1, uint64(2), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\tvote4, _ := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\n\terr = ap.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf2)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf3)\n\trequire.NoError(err)\n\terr = ap.AddVote(vote4)\n\trequire.NoError(err)\n\n\thash1 := tsf1.Hash()\n\taction1 := tsf1.ConvertToActionPb()\n\thash2 := vote4.Hash()\n\taction2 := vote4.ConvertToActionPb()\n\tacts := []*iproto.ActionPb{action1, action2}\n\trequire.NotNil(ap.allActions[hash1])\n\trequire.NotNil(ap.allActions[hash2])\n\tap.removeInvalidActs(acts)\n\trequire.Nil(ap.allActions[hash1])\n\trequire.Nil(ap.allActions[hash2])\n}\n\nfunc TestActPool_GetPendingNonce(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\tvote4, _ := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\n\terr = ap.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf3)\n\trequire.NoError(err)\n\terr = ap.AddVote(vote4)\n\trequire.NoError(err)\n\n\tnonce, err := ap.GetPendingNonce(addr2.RawAddress)\n\trequire.NoError(err)\n\trequire.Equal(uint64(1), nonce)\n\n\tnonce, err = ap.GetPendingNonce(addr1.RawAddress)\n\trequire.NoError(err)\n\trequire.Equal(uint64(2), nonce)\n}\n\nfunc TestActPool_GetUnconfirmedActs(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\tact1 := tsf1.ConvertToActionPb()\n\ttsf3, _ := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\tact3 := tsf3.ConvertToActionPb()\n\tvote4, _ := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\tact4 := vote4.ConvertToActionPb()\n\n\terr = ap.AddTsf(tsf1)\n\trequire.NoError(err)\n\terr = ap.AddTsf(tsf3)\n\trequire.NoError(err)\n\terr = ap.AddVote(vote4)\n\trequire.NoError(err)\n\n\tacts := ap.GetUnconfirmedActs(addr2.RawAddress)\n\trequire.Equal([]*iproto.ActionPb{}, acts)\n\n\tacts = ap.GetUnconfirmedActs(addr1.RawAddress)\n\trequire.Equal([]*iproto.ActionPb{act1, act3, act4}, acts)\n}\n\nfunc TestActPool_GetActionByHash(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.CreateState(addr2.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\n\ttsf1, _ := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\thash1 := tsf1.Hash()\n\tact1 := tsf1.ConvertToActionPb()\n\tvote2, _ := signedVote(addr1, addr1, uint64(2), uint64(100000), big.NewInt(10))\n\thash2 := vote2.Hash()\n\tact2 := vote2.ConvertToActionPb()\n\n\tap.allActions[hash1] = act1\n\tact, err := ap.GetActionByHash(hash1)\n\trequire.NoError(err)\n\trequire.Equal(act1, act)\n\tact, err = ap.GetActionByHash(hash2)\n\trequire.Equal(ErrHash, errors.Cause(err))\n\trequire.Nil(act)\n\n\tap.allActions[hash2] = act2\n\tact, err = ap.GetActionByHash(hash2)\n\trequire.NoError(err)\n\trequire.Equal(act2, act)\n}\n\nfunc TestActPool_GetCapacity(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\trequire.Equal(uint64(maxNumActsPerPool), ap.GetCapacity())\n}\n\nfunc TestActPool_GetSize(t *testing.T) {\n\trequire := require.New(t)\n\tbc := blockchain.NewBlockchain(&config.Default, blockchain.InMemStateFactoryOption(), blockchain.InMemDaoOption())\n\trequire.NoError(bc.Start(context.Background()))\n\t_, err := bc.CreateState(addr1.RawAddress, uint64(100))\n\trequire.NoError(err)\n\t_, err = bc.GetFactory().RunActions(0, nil, nil, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\t// Create actpool\n\tapConfig := getActPoolCfg()\n\tAp, err := NewActPool(bc, apConfig)\n\trequire.NoError(err)\n\tap, ok := Ap.(*actPool)\n\trequire.True(ok)\n\trequire.Zero(ap.GetSize())\n\n\ttsf1, err := signedTransfer(addr1, addr1, uint64(1), big.NewInt(10), []byte{}, uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\ttsf2, err := signedTransfer(addr1, addr1, uint64(2), big.NewInt(20), []byte{}, uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\ttsf3, err := signedTransfer(addr1, addr1, uint64(3), big.NewInt(30), []byte{}, uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\tvote4, err := signedVote(addr1, addr1, uint64(4), uint64(100000), big.NewInt(10))\n\trequire.NoError(err)\n\trequire.NoError(ap.AddTsf(tsf1))\n\trequire.NoError(ap.AddTsf(tsf2))\n\trequire.NoError(ap.AddTsf(tsf3))\n\trequire.NoError(ap.AddVote(vote4))\n\trequire.Equal(uint64(4), ap.GetSize())\n\t_, err = bc.GetFactory().RunActions(0, []*action.Transfer{tsf1, tsf2, tsf3}, []*action.Vote{vote4}, nil)\n\trequire.NoError(err)\n\trequire.Nil(bc.GetFactory().Commit())\n\tap.removeConfirmedActs()\n\trequire.Equal(uint64(0), ap.GetSize())\n}\n\n// Helper function to return the correct pending nonce just in case of empty queue\nfunc (ap *actPool) getPendingNonce(addr string) (uint64, error) {\n\tif queue, ok := ap.accountActs[addr]; ok {\n\t\treturn queue.PendingNonce(), nil\n\t}\n\tcommittedNonce, err := ap.bc.Nonce(addr)\n\tpendingNonce := committedNonce + 1\n\treturn pendingNonce, err\n}\n\n// Helper function to return the correct pending balance just in case of empty queue\nfunc (ap *actPool) getPendingBalance(addr string) (*big.Int, error) {\n\tif queue, ok := ap.accountActs[addr]; ok {\n\t\treturn queue.PendingBalance(), nil\n\t}\n\treturn ap.bc.Balance(addr)\n}\n\n// Helper function to return a signed transfer\nfunc signedTransfer(sender *iotxaddress.Address, recipient *iotxaddress.Address, nonce uint64, amount *big.Int, payload []byte, gasLimit uint64, gasPrice *big.Int) (*action.Transfer, error) {\n\ttransfer, err := action.NewTransfer(nonce, amount, sender.RawAddress, recipient.RawAddress, payload, gasLimit, gasPrice)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn transfer.Sign(sender)\n}\n\n// Helper function to return a signed vote\nfunc signedVote(voter *iotxaddress.Address, votee *iotxaddress.Address, nonce uint64, gasLimit uint64, gasPrice *big.Int) (*action.Vote, error) {\n\tvote, err := action.NewVote(nonce, voter.RawAddress, votee.RawAddress, gasLimit, gasPrice)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn vote.Sign(voter)\n}\n\nfunc getActPoolCfg() config.ActPool {\n\treturn config.ActPool{\n\t\tMaxNumActsPerPool: maxNumActsPerPool,\n\t\tMaxNumActsPerAcct: maxNumActsPerAcct,\n\t}\n}\n", "idx": 1, "id": 11943, "msg": "line is 121 characters", "proj": "iotexproject-iotex-core", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -10,6 +10,7 @@ import (\n \t\"github.com/aws/aws-sdk-go/aws\"\n \t\"github.com/aws/aws-sdk-go/service/cloudformation\"\n \t\"github.com/aws/copilot-cli/internal/pkg/addon\"\n+\t\"github.com/aws/copilot-cli/internal/pkg/config\"\n \t\"github.com/aws/copilot-cli/internal/pkg/manifest\"\n \t\"github.com/aws/copilot-cli/internal/pkg/template\"\n )", "y": 0, "oldf": "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n\npackage stack\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/aws/aws-sdk-go/service/cloudformation\"\n\t\"github.com/aws/copilot-cli/internal/pkg/addon\"\n\t\"github.com/aws/copilot-cli/internal/pkg/manifest\"\n\t\"github.com/aws/copilot-cli/internal/pkg/template\"\n)\n\n// Template rendering configuration.\nconst (\n\tlbWebSvcRulePriorityGeneratorPath = \"custom-resources/alb-rule-priority-generator.js\"\n\tdesiredCountGeneratorPath         = \"custom-resources/desired-count-delegation.js\"\n\tenvControllerPath                 = \"custom-resources/env-controller.js\"\n)\n\n// Parameter logical IDs for a load balanced web service.\nconst (\n\tLBWebServiceHTTPSParamKey           = \"HTTPSEnabled\"\n\tLBWebServiceContainerPortParamKey   = \"ContainerPort\"\n\tLBWebServiceRulePathParamKey        = \"RulePath\"\n\tLBWebServiceTargetContainerParamKey = \"TargetContainer\"\n\tLBWebServiceTargetPortParamKey      = \"TargetPort\"\n\tLBWebServiceStickinessParamKey      = \"Stickiness\"\n)\n\ntype loadBalancedWebSvcReadParser interface {\n\ttemplate.ReadParser\n\tParseLoadBalancedWebService(template.WorkloadOpts) (*template.Content, error)\n}\n\n// LoadBalancedWebService represents the configuration needed to create a CloudFormation stack from a load balanced web service manifest.\ntype LoadBalancedWebService struct {\n\t*ecsWkld\n\tmanifest     *manifest.LoadBalancedWebService\n\thttpsEnabled bool\n\n\tparser loadBalancedWebSvcReadParser\n}\n\n// NewLoadBalancedWebService creates a new LoadBalancedWebService stack from a manifest file.\nfunc NewLoadBalancedWebService(mft *manifest.LoadBalancedWebService, env, app string, rc RuntimeConfig) (*LoadBalancedWebService, error) {\n\tparser := template.New()\n\taddons, err := addon.New(aws.StringValue(mft.Name))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"new addons: %w\", err)\n\t}\n\tenvManifest, err := mft.ApplyEnv(env) // Apply environment overrides to the manifest values.\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"apply environment %s override: %s\", env, err)\n\t}\n\treturn &LoadBalancedWebService{\n\t\tecsWkld: &ecsWkld{\n\t\t\twkld: &wkld{\n\t\t\t\tname:   aws.StringValue(mft.Name),\n\t\t\t\tenv:    env,\n\t\t\t\tapp:    app,\n\t\t\t\trc:     rc,\n\t\t\t\timage:  envManifest.ImageConfig,\n\t\t\t\tparser: parser,\n\t\t\t\taddons: addons,\n\t\t\t},\n\t\t\ttc: envManifest.TaskConfig,\n\t\t},\n\t\tmanifest:     envManifest,\n\t\thttpsEnabled: false,\n\n\t\tparser: parser,\n\t}, nil\n}\n\n// NewHTTPSLoadBalancedWebService  creates a new LoadBalancedWebService stack from its manifest that needs to be deployed to\n// a environment within an application. It creates an HTTPS listener and assumes that the environment\n// it's being deployed into has an HTTPS configured listener.\nfunc NewHTTPSLoadBalancedWebService(mft *manifest.LoadBalancedWebService, env, app string, rc RuntimeConfig) (*LoadBalancedWebService, error) {\n\twebSvc, err := NewLoadBalancedWebService(mft, env, app, rc)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twebSvc.httpsEnabled = true\n\treturn webSvc, nil\n}\n\n// Template returns the CloudFormation template for the service parametrized for the environment.\nfunc (s *LoadBalancedWebService) Template() (string, error) {\n\trulePriorityLambda, err := s.parser.Read(lbWebSvcRulePriorityGeneratorPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"read rule priority lambda: %w\", err)\n\t}\n\tdesiredCountLambda, err := s.parser.Read(desiredCountGeneratorPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"read desired count lambda: %w\", err)\n\t}\n\tenvControllerLambda, err := s.parser.Read(envControllerPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"read env controller lambda: %w\", err)\n\t}\n\toutputs, err := s.addonsOutputs()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tsidecars, err := convertSidecar(s.manifest.Sidecars)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"convert the sidecar configuration for service %s: %w\", s.name, err)\n\t}\n\n\tadvancedCount, err := convertAdvancedCount(&s.manifest.Count.AdvancedCount)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"convert the advanced count configuration for service %s: %w\", s.name, err)\n\t}\n\n\tvar autoscaling *template.AutoscalingOpts\n\tvar desiredCountOnSpot *int\n\tvar capacityProviders []*template.CapacityProviderStrategy\n\n\tif advancedCount != nil {\n\t\tautoscaling = advancedCount.Autoscaling\n\t\tdesiredCountOnSpot = advancedCount.Spot\n\t\tcapacityProviders = advancedCount.Cps\n\t}\n\n\tstorage, err := convertStorageOpts(s.manifest.Name, s.manifest.Storage)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"convert storage options for service %s: %w\", s.name, err)\n\t}\n\n\tentrypoint, err := s.manifest.EntryPoint.ToStringSlice()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(`convert 'entrypoint' to string slice: %w`, err)\n\t}\n\tcommand, err := s.manifest.Command.ToStringSlice()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(`convert 'command' to string slice: %w`, err)\n\t}\n\tvar aliases []string\n\tif s.httpsEnabled {\n\t\talbAlias := aws.StringValue(s.manifest.Alias)\n\t\tif albAlias != \"\" {\n\t\t\taliases = append(aliases, albAlias)\n\t\t}\n\t}\n\tcontent, err := s.parser.ParseLoadBalancedWebService(template.WorkloadOpts{\n\t\tVariables:           s.manifest.Variables,\n\t\tSecrets:             s.manifest.Secrets,\n\t\tAliases:             aliases,\n\t\tNestedStack:         outputs,\n\t\tSidecars:            sidecars,\n\t\tLogConfig:           convertLogging(s.manifest.Logging),\n\t\tDockerLabels:        s.manifest.ImageConfig.DockerLabels,\n\t\tAutoscaling:         autoscaling,\n\t\tCapacityProviders:   capacityProviders,\n\t\tDesiredCountOnSpot:  desiredCountOnSpot,\n\t\tExecuteCommand:      convertExecuteCommand(&s.manifest.ExecuteCommand),\n\t\tWorkloadType:        manifest.LoadBalancedWebServiceType,\n\t\tHTTPHealthCheck:     convertHTTPHealthCheck(&s.manifest.HealthCheck),\n\t\tAllowedSourceIps:    s.manifest.AllowedSourceIps,\n\t\tRulePriorityLambda:  rulePriorityLambda.String(),\n\t\tDesiredCountLambda:  desiredCountLambda.String(),\n\t\tEnvControllerLambda: envControllerLambda.String(),\n\t\tStorage:             storage,\n\t\tNetwork:             convertNetworkConfig(s.manifest.Network),\n\t\tEntryPoint:          entrypoint,\n\t\tCommand:             command,\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn content.String(), nil\n}\n\nfunc (s *LoadBalancedWebService) loadBalancerTarget() (targetContainer *string, targetPort *string, err error) {\n\tcontainerName := s.name\n\tcontainerPort := strconv.FormatUint(uint64(aws.Uint16Value(s.manifest.ImageConfig.Port)), 10)\n\t// Route load balancer traffic to main container by default.\n\ttargetContainer = aws.String(containerName)\n\ttargetPort = aws.String(containerPort)\n\tif s.manifest.TargetContainer == nil && s.manifest.TargetContainerCamelCase != nil {\n\t\ts.manifest.TargetContainer = s.manifest.TargetContainerCamelCase\n\t}\n\tmftTargetContainer := s.manifest.TargetContainer\n\tif mftTargetContainer != nil {\n\t\tsidecar, ok := s.manifest.Sidecars[*mftTargetContainer]\n\t\tif ok {\n\t\t\ttargetContainer = mftTargetContainer\n\t\t\ttargetPort = sidecar.Port\n\t\t} else {\n\t\t\treturn nil, nil, fmt.Errorf(\"target container %s doesn't exist\", *s.manifest.TargetContainer)\n\t\t}\n\t}\n\treturn\n}\n\n// Parameters returns the list of CloudFormation parameters used by the template.\nfunc (s *LoadBalancedWebService) Parameters() ([]*cloudformation.Parameter, error) {\n\twkldParams, err := s.ecsWkld.Parameters()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttargetContainer, targetPort, err := s.loadBalancerTarget()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn append(wkldParams, []*cloudformation.Parameter{\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceContainerPortParamKey),\n\t\t\tParameterValue: aws.String(strconv.FormatUint(uint64(aws.Uint16Value(s.manifest.ImageConfig.Port)), 10)),\n\t\t},\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceRulePathParamKey),\n\t\t\tParameterValue: s.manifest.Path,\n\t\t},\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceHTTPSParamKey),\n\t\t\tParameterValue: aws.String(strconv.FormatBool(s.httpsEnabled)),\n\t\t},\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceTargetContainerParamKey),\n\t\t\tParameterValue: targetContainer,\n\t\t},\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceTargetPortParamKey),\n\t\t\tParameterValue: targetPort,\n\t\t},\n\t\t{\n\t\t\tParameterKey:   aws.String(LBWebServiceStickinessParamKey),\n\t\t\tParameterValue: aws.String(strconv.FormatBool(aws.BoolValue(s.manifest.Stickiness))),\n\t\t},\n\t}...), nil\n}\n\n// SerializedParameters returns the CloudFormation stack's parameters serialized\n// to a YAML document annotated with comments for readability to users.\nfunc (s *LoadBalancedWebService) SerializedParameters() (string, error) {\n\treturn s.templateConfiguration(s)\n}\n", "idx": 1, "id": 17827, "msg": "", "proj": "aws-copilot-cli", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -216,7 +216,13 @@ func (a *asyncInstrument) getRecorder(kvs []core.KeyValue) export.Aggregator {\n \n \tlrec, ok := a.recorders[labels.ordered]\n \tif ok {\n-\t\tlrec.modifiedEpoch = a.meter.currentEpoch\n+\t\tif lrec.modifiedEpoch == a.meter.currentEpoch {\n+\t\t\t// last value wins for Observers, so if we see the same labels\n+\t\t\t// in the current epoch, we replace the old recorder\n+\t\t\tlrec.recorder = a.meter.batcher.AggregatorFor(&a.descriptor)\n+\t\t} else {\n+\t\t\tlrec.modifiedEpoch = a.meter.currentEpoch\n+\t\t}\n \t\ta.recorders[labels.ordered] = lrec\n \t\treturn lrec.recorder\n \t}", "y": 1, "oldf": "// Copyright The OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage metric\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"go.opentelemetry.io/otel/api/core\"\n\t\"go.opentelemetry.io/otel/api/metric\"\n\tapi \"go.opentelemetry.io/otel/api/metric\"\n\texport \"go.opentelemetry.io/otel/sdk/export/metric\"\n\t\"go.opentelemetry.io/otel/sdk/export/metric/aggregator\"\n\t\"go.opentelemetry.io/otel/sdk/resource\"\n)\n\ntype (\n\t// SDK implements the OpenTelemetry Meter API.  The SDK is\n\t// bound to a single export.Batcher in `New()`.\n\t//\n\t// The SDK supports a Collect() API to gather and export\n\t// current data.  Collect() should be arranged according to\n\t// the batcher model.  Push-based batchers will setup a\n\t// timer to call Collect() periodically.  Pull-based batchers\n\t// will call Collect() when a pull request arrives.\n\tSDK struct {\n\t\t// current maps `mapkey` to *record.\n\t\tcurrent sync.Map\n\n\t\t// asyncInstruments is a set of\n\t\t// `*asyncInstrument` instances\n\t\tasyncInstruments sync.Map\n\n\t\t// currentEpoch is the current epoch number. It is\n\t\t// incremented in `Collect()`.\n\t\tcurrentEpoch int64\n\n\t\t// batcher is the configured batcher+configuration.\n\t\tbatcher export.Batcher\n\n\t\t// collectLock prevents simultaneous calls to Collect().\n\t\tcollectLock sync.Mutex\n\n\t\t// errorHandler supports delivering errors to the user.\n\t\terrorHandler ErrorHandler\n\n\t\t// resource represents the entity producing telemetry.\n\t\tresource resource.Resource\n\n\t\t// asyncSortSlice has a single purpose - as a temporary\n\t\t// place for sorting during labels creation to avoid\n\t\t// allocation.  It is cleared after use.\n\t\tasyncSortSlice sortedLabels\n\t}\n\n\tsyncInstrument struct {\n\t\tinstrument\n\t}\n\n\t// orderedLabels is a variable-size array of core.KeyValue\n\t// suitable for use as a map key.\n\torderedLabels interface{}\n\n\t// labels represents an internalized set of labels that have been\n\t// sorted and deduplicated.\n\tlabels struct {\n\t\t// cachedEncoderID needs to be aligned for atomic access\n\t\tcachedEncoderID int64\n\t\t// cachedEncoded is an encoded version of ordered\n\t\t// labels\n\t\tcachedEncoded string\n\n\t\t// ordered is the output of sorting and deduplicating\n\t\t// the labels, copied into an array of the correct\n\t\t// size for use as a map key.\n\t\tordered orderedLabels\n\n\t\t// cachedValue contains a `reflect.Value` of the `ordered`\n\t\t// member\n\t\tcachedValue reflect.Value\n\t}\n\n\t// mapkey uniquely describes a metric instrument in terms of\n\t// its InstrumentID and the encoded form of its labels.\n\tmapkey struct {\n\t\tdescriptor *metric.Descriptor\n\t\tordered    orderedLabels\n\t}\n\n\t// record maintains the state of one metric instrument.  Due\n\t// the use of lock-free algorithms, there may be more than one\n\t// `record` in existence at a time, although at most one can\n\t// be referenced from the `SDK.current` map.\n\trecord struct {\n\t\t// refMapped keeps track of refcounts and the mapping state to the\n\t\t// SDK.current map.\n\t\trefMapped refcountMapped\n\n\t\t// modified is an atomic boolean that tracks if the current record\n\t\t// was modified since the last Collect().\n\t\t//\n\t\t// modified has to be aligned for 64-bit atomic operations.\n\t\tmodified int64\n\n\t\t// labels is the processed label set for this record.\n\t\t//\n\t\t// labels has to be aligned for 64-bit atomic operations.\n\t\tlabels labels\n\n\t\t// sortSlice has a single purpose - as a temporary\n\t\t// place for sorting during labels creation to avoid\n\t\t// allocation.\n\t\tsortSlice sortedLabels\n\n\t\t// inst is a pointer to the corresponding instrument.\n\t\tinst *syncInstrument\n\n\t\t// recorder implements the actual RecordOne() API,\n\t\t// depending on the type of aggregation.  If nil, the\n\t\t// metric was disabled by the exporter.\n\t\trecorder export.Aggregator\n\t}\n\n\tinstrument struct {\n\t\tmeter      *SDK\n\t\tdescriptor metric.Descriptor\n\t}\n\n\tasyncInstrument struct {\n\t\tinstrument\n\t\t// recorders maps ordered labels to the pair of\n\t\t// labelset and recorder\n\t\trecorders map[orderedLabels]labeledRecorder\n\n\t\tcallback func(func(core.Number, []core.KeyValue))\n\t}\n\n\tlabeledRecorder struct {\n\t\tmodifiedEpoch int64\n\t\tlabels        labels\n\t\trecorder      export.Aggregator\n\t}\n\n\tErrorHandler func(error)\n)\n\nvar (\n\t_ api.MeterImpl       = &SDK{}\n\t_ api.AsyncImpl       = &asyncInstrument{}\n\t_ api.SyncImpl        = &syncInstrument{}\n\t_ api.BoundSyncImpl   = &record{}\n\t_ api.Resourcer       = &SDK{}\n\t_ export.LabelStorage = &labels{}\n\t_ export.Labels       = &labels{}\n\n\tkvType = reflect.TypeOf(core.KeyValue{})\n\n\temptyLabels = labels{\n\t\tordered:     [0]core.KeyValue{},\n\t\tcachedValue: reflect.ValueOf([0]core.KeyValue{}),\n\t}\n)\n\nfunc (inst *instrument) Descriptor() api.Descriptor {\n\treturn inst.descriptor\n}\n\nfunc (a *asyncInstrument) Implementation() interface{} {\n\treturn a\n}\n\nfunc (s *syncInstrument) Implementation() interface{} {\n\treturn s\n}\n\nfunc (a *asyncInstrument) observe(number core.Number, labels []core.KeyValue) {\n\tif err := aggregator.RangeTest(number, &a.descriptor); err != nil {\n\t\ta.meter.errorHandler(err)\n\t\treturn\n\t}\n\trecorder := a.getRecorder(labels)\n\tif recorder == nil {\n\t\t// The instrument is disabled according to the\n\t\t// AggregationSelector.\n\t\treturn\n\t}\n\tif err := recorder.Update(context.Background(), number, &a.descriptor); err != nil {\n\t\ta.meter.errorHandler(err)\n\t\treturn\n\t}\n}\n\nfunc (a *asyncInstrument) getRecorder(kvs []core.KeyValue) export.Aggregator {\n\t// We are in a single-threaded context.  Note: this assumption\n\t// could be violated if the user added concurrency within\n\t// their callback.\n\tlabels := a.meter.makeLabels(kvs, &a.meter.asyncSortSlice)\n\n\tlrec, ok := a.recorders[labels.ordered]\n\tif ok {\n\t\tlrec.modifiedEpoch = a.meter.currentEpoch\n\t\ta.recorders[labels.ordered] = lrec\n\t\treturn lrec.recorder\n\t}\n\trec := a.meter.batcher.AggregatorFor(&a.descriptor)\n\tif a.recorders == nil {\n\t\ta.recorders = make(map[orderedLabels]labeledRecorder)\n\t}\n\t// This may store nil recorder in the map, thus disabling the\n\t// asyncInstrument for the labelset for good. This is intentional,\n\t// but will be revisited later.\n\ta.recorders[labels.ordered] = labeledRecorder{\n\t\trecorder:      rec,\n\t\tlabels:        labels,\n\t\tmodifiedEpoch: a.meter.currentEpoch,\n\t}\n\treturn rec\n}\n\nfunc (m *SDK) SetErrorHandler(f ErrorHandler) {\n\tm.errorHandler = f\n}\n\n// acquireHandle gets or creates a `*record` corresponding to `kvs`,\n// the input labels.  The second argument `labels` is passed in to\n// support re-use of the orderedLabels computed by a previous\n// measurement in the same batch.   This performs two allocations\n// in the common case.\nfunc (s *syncInstrument) acquireHandle(kvs []core.KeyValue, lptr *labels) *record {\n\tvar rec *record\n\tvar labels labels\n\n\tif lptr == nil || lptr.ordered == nil {\n\t\t// This memory allocation may not be used, but it's\n\t\t// needed for the `sortSlice` field, to avoid an\n\t\t// allocation while sorting.\n\t\trec = &record{}\n\t\tlabels = s.meter.makeLabels(kvs, &rec.sortSlice)\n\t} else {\n\t\tlabels = *lptr\n\t}\n\n\t// Create lookup key for sync.Map (one allocation, as this\n\t// passes through an interface{})\n\tmk := mapkey{\n\t\tdescriptor: &s.descriptor,\n\t\tordered:    labels.ordered,\n\t}\n\n\tif actual, ok := s.meter.current.Load(mk); ok {\n\t\t// Existing record case.\n\t\texistingRec := actual.(*record)\n\t\tif existingRec.refMapped.ref() {\n\t\t\t// At this moment it is guaranteed that the entry is in\n\t\t\t// the map and will not be removed.\n\t\t\treturn existingRec\n\t\t}\n\t\t// This entry is no longer mapped, try to add a new entry.\n\t}\n\n\tif rec == nil {\n\t\trec = &record{}\n\t}\n\trec.refMapped = refcountMapped{value: 2}\n\trec.labels = labels\n\trec.inst = s\n\trec.recorder = s.meter.batcher.AggregatorFor(&s.descriptor)\n\n\tfor {\n\t\t// Load/Store: there's a memory allocation to place `mk` into\n\t\t// an interface here.\n\t\tif actual, loaded := s.meter.current.LoadOrStore(mk, rec); loaded {\n\t\t\t// Existing record case. Cannot change rec here because if fail\n\t\t\t// will try to add rec again to avoid new allocations.\n\t\t\toldRec := actual.(*record)\n\t\t\tif oldRec.refMapped.ref() {\n\t\t\t\t// At this moment it is guaranteed that the entry is in\n\t\t\t\t// the map and will not be removed.\n\t\t\t\treturn oldRec\n\t\t\t}\n\t\t\t// This loaded entry is marked as unmapped (so Collect will remove\n\t\t\t// it from the map immediately), try again - this is a busy waiting\n\t\t\t// strategy to wait until Collect() removes this entry from the map.\n\t\t\t//\n\t\t\t// This can be improved by having a list of \"Unmapped\" entries for\n\t\t\t// one time only usages, OR we can make this a blocking path and use\n\t\t\t// a Mutex that protects the delete operation (delete only if the old\n\t\t\t// record is associated with the key).\n\n\t\t\t// Let collector get work done to remove the entry from the map.\n\t\t\truntime.Gosched()\n\t\t\tcontinue\n\t\t}\n\t\t// The new entry was added to the map, good to go.\n\t\treturn rec\n\t}\n}\n\nfunc (s *syncInstrument) Bind(kvs []core.KeyValue) api.BoundSyncImpl {\n\treturn s.acquireHandle(kvs, nil)\n}\n\nfunc (s *syncInstrument) RecordOne(ctx context.Context, number core.Number, kvs []core.KeyValue) {\n\th := s.acquireHandle(kvs, nil)\n\tdefer h.Unbind()\n\th.RecordOne(ctx, number)\n}\n\n// New constructs a new SDK for the given batcher.  This SDK supports\n// only a single batcher.\n//\n// The SDK does not start any background process to collect itself\n// periodically, this responsbility lies with the batcher, typically,\n// depending on the type of export.  For example, a pull-based\n// batcher will call Collect() when it receives a request to scrape\n// current metric values.  A push-based batcher should configure its\n// own periodic collection.\nfunc New(batcher export.Batcher, opts ...Option) *SDK {\n\tc := &Config{ErrorHandler: DefaultErrorHandler}\n\tfor _, opt := range opts {\n\t\topt.Apply(c)\n\t}\n\n\treturn &SDK{\n\t\tbatcher:      batcher,\n\t\terrorHandler: c.ErrorHandler,\n\t\tresource:     c.Resource,\n\t}\n}\n\nfunc DefaultErrorHandler(err error) {\n\tfmt.Fprintln(os.Stderr, \"Metrics SDK error:\", err)\n}\n\n// makeLabels returns a `labels` corresponding to the arguments.  Labels\n// are sorted and de-duplicated, with last-value-wins semantics.  Note that\n// sorting and deduplicating happens in-place to avoid allocation, so the\n// passed slice will be modified.  The `sortSlice` argument refers to a memory\n// location used temporarily while sorting the slice, to avoid a memory\n// allocation.\nfunc (m *SDK) makeLabels(kvs []core.KeyValue, sortSlice *sortedLabels) labels {\n\t// Check for empty set.\n\tif len(kvs) == 0 {\n\t\treturn emptyLabels\n\t}\n\n\t*sortSlice = kvs\n\n\t// Sort and de-duplicate.  Note: this use of `sortSlice`\n\t// avoids an allocation because it is a pointer.\n\tsort.Stable(sortSlice)\n\n\t*sortSlice = nil\n\n\toi := 1\n\tfor i := 1; i < len(kvs); i++ {\n\t\tif kvs[i-1].Key == kvs[i].Key {\n\t\t\t// Overwrite the value for \"last-value wins\".\n\t\t\tkvs[oi-1].Value = kvs[i].Value\n\t\t\tcontinue\n\t\t}\n\t\tkvs[oi] = kvs[i]\n\t\toi++\n\t}\n\tkvs = kvs[0:oi]\n\treturn computeOrderedLabels(kvs)\n}\n\n// NumLabels is a part of an implementation of the export.LabelStorage\n// interface.\nfunc (ls *labels) NumLabels() int {\n\treturn ls.cachedValue.Len()\n}\n\n// GetLabel is a part of an implementation of the export.LabelStorage\n// interface.\nfunc (ls *labels) GetLabel(idx int) core.KeyValue {\n\treturn ls.cachedValue.Index(idx).Interface().(core.KeyValue)\n}\n\n// Iter is a part of an implementation of the export.Labels interface.\nfunc (ls *labels) Iter() export.LabelIterator {\n\treturn export.NewLabelIterator(ls)\n}\n\n// Encoded is a part of an implementation of the export.Labels\n// interface.\nfunc (ls *labels) Encoded(encoder export.LabelEncoder) string {\n\tid := encoder.ID()\n\tif id <= 0 {\n\t\t// Punish misbehaving encoders by not even trying to\n\t\t// cache them\n\t\treturn encoder.Encode(ls.Iter())\n\t}\n\tcachedID := atomic.LoadInt64(&ls.cachedEncoderID)\n\t// If cached ID is less than zero, it means that other\n\t// goroutine is currently caching the encoded labels and the\n\t// ID of the encoder. Wait until it's done - it's a\n\t// nonblocking op.\n\tfor cachedID < 0 {\n\t\t// Let other goroutine finish its work.\n\t\truntime.Gosched()\n\t\tcachedID = atomic.LoadInt64(&ls.cachedEncoderID)\n\t}\n\t// At this point, cachedID is either 0 (nothing cached) or\n\t// some other number.\n\t//\n\t// If cached ID is the same as ID of the passed encoder, we've\n\t// got the fast path.\n\tif cachedID == id {\n\t\treturn ls.cachedEncoded\n\t}\n\t// If we are here, either some other encoder cached its\n\t// encoded labels or the cache is still for the taking. Either\n\t// way, we need to compute the encoded labels anyway.\n\tencoded := encoder.Encode(ls.Iter())\n\t// If some other encoder took the cache, then we just return\n\t// our encoded labels. That's a slow path.\n\tif cachedID > 0 {\n\t\treturn encoded\n\t}\n\t// Try to take the cache for ourselves. This is the place\n\t// where other encoders may be \"blocked\".\n\tif atomic.CompareAndSwapInt64(&ls.cachedEncoderID, 0, -1) {\n\t\t// The cache is ours.\n\t\tls.cachedEncoded = encoded\n\t\tatomic.StoreInt64(&ls.cachedEncoderID, id)\n\t}\n\treturn encoded\n}\n\nfunc computeOrderedLabels(kvs []core.KeyValue) labels {\n\tvar ls labels\n\tls.ordered = computeOrderedFixed(kvs)\n\tif ls.ordered == nil {\n\t\tls.ordered = computeOrderedReflect(kvs)\n\t}\n\tls.cachedValue = reflect.ValueOf(ls.ordered)\n\treturn ls\n}\n\nfunc computeOrderedFixed(kvs []core.KeyValue) orderedLabels {\n\tswitch len(kvs) {\n\tcase 1:\n\t\tptr := new([1]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 2:\n\t\tptr := new([2]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 3:\n\t\tptr := new([3]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 4:\n\t\tptr := new([4]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 5:\n\t\tptr := new([5]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 6:\n\t\tptr := new([6]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 7:\n\t\tptr := new([7]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 8:\n\t\tptr := new([8]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 9:\n\t\tptr := new([9]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tcase 10:\n\t\tptr := new([10]core.KeyValue)\n\t\tcopy((*ptr)[:], kvs)\n\t\treturn *ptr\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc computeOrderedReflect(kvs []core.KeyValue) interface{} {\n\tat := reflect.New(reflect.ArrayOf(len(kvs), kvType)).Elem()\n\tfor i, kv := range kvs {\n\t\t*(at.Index(i).Addr().Interface().(*core.KeyValue)) = kv\n\t}\n\treturn at.Interface()\n}\n\nfunc (m *SDK) NewSyncInstrument(descriptor api.Descriptor) (api.SyncImpl, error) {\n\treturn &syncInstrument{\n\t\tinstrument: instrument{\n\t\t\tdescriptor: descriptor,\n\t\t\tmeter:      m,\n\t\t},\n\t}, nil\n}\n\nfunc (m *SDK) NewAsyncInstrument(descriptor api.Descriptor, callback func(func(core.Number, []core.KeyValue))) (api.AsyncImpl, error) {\n\ta := &asyncInstrument{\n\t\tinstrument: instrument{\n\t\t\tdescriptor: descriptor,\n\t\t\tmeter:      m,\n\t\t},\n\t\tcallback: callback,\n\t}\n\tm.asyncInstruments.Store(a, nil)\n\treturn a, nil\n}\n\n// Collect traverses the list of active records and observers and\n// exports data for each active instrument.  Collect() may not be\n// called concurrently.\n//\n// During the collection pass, the export.Batcher will receive\n// one Export() call per current aggregation.\n//\n// Returns the number of records that were checkpointed.\nfunc (m *SDK) Collect(ctx context.Context) int {\n\tm.collectLock.Lock()\n\tdefer m.collectLock.Unlock()\n\n\tcheckpointed := m.collectRecords(ctx)\n\tcheckpointed += m.collectAsync(ctx)\n\tm.currentEpoch++\n\treturn checkpointed\n}\n\nfunc (m *SDK) collectRecords(ctx context.Context) int {\n\tcheckpointed := 0\n\n\tm.current.Range(func(key interface{}, value interface{}) bool {\n\t\tinuse := value.(*record)\n\t\tunmapped := inuse.refMapped.tryUnmap()\n\t\t// If able to unmap then remove the record from the current Map.\n\t\tif unmapped {\n\t\t\t// TODO: Consider leaving the record in the map for one\n\t\t\t// collection interval? Since creating records is relatively\n\t\t\t// expensive, this would optimize common cases of ongoing use.\n\t\t\tm.current.Delete(inuse.mapkey())\n\t\t}\n\n\t\t// Always report the values if a reference to the Record is active,\n\t\t// this is to keep the previous behavior.\n\t\t// TODO: Reconsider this logic.\n\t\tif inuse.refMapped.inUse() || atomic.LoadInt64(&inuse.modified) != 0 {\n\t\t\tatomic.StoreInt64(&inuse.modified, 0)\n\t\t\tcheckpointed += m.checkpointRecord(ctx, inuse)\n\t\t}\n\n\t\t// Always continue to iterate over the entire map.\n\t\treturn true\n\t})\n\n\treturn checkpointed\n}\n\nfunc (m *SDK) collectAsync(ctx context.Context) int {\n\tcheckpointed := 0\n\n\tm.asyncInstruments.Range(func(key, value interface{}) bool {\n\t\ta := key.(*asyncInstrument)\n\t\ta.callback(a.observe)\n\t\tcheckpointed += m.checkpointAsync(ctx, a)\n\t\treturn true\n\t})\n\n\treturn checkpointed\n}\n\nfunc (m *SDK) checkpointRecord(ctx context.Context, r *record) int {\n\treturn m.checkpoint(ctx, &r.inst.descriptor, r.recorder, &r.labels)\n}\n\nfunc (m *SDK) checkpointAsync(ctx context.Context, a *asyncInstrument) int {\n\tif len(a.recorders) == 0 {\n\t\treturn 0\n\t}\n\tcheckpointed := 0\n\tfor encodedLabels, lrec := range a.recorders {\n\t\tlrec := lrec\n\t\tepochDiff := m.currentEpoch - lrec.modifiedEpoch\n\t\tif epochDiff == 0 {\n\t\t\tcheckpointed += m.checkpoint(ctx, &a.descriptor, lrec.recorder, &lrec.labels)\n\t\t} else if epochDiff > 1 {\n\t\t\t// This is second collection cycle with no\n\t\t\t// observations for this labelset. Remove the\n\t\t\t// recorder.\n\t\t\tdelete(a.recorders, encodedLabels)\n\t\t}\n\t}\n\tif len(a.recorders) == 0 {\n\t\ta.recorders = nil\n\t}\n\treturn checkpointed\n}\n\nfunc (m *SDK) checkpoint(ctx context.Context, descriptor *metric.Descriptor, recorder export.Aggregator, labels *labels) int {\n\tif recorder == nil {\n\t\treturn 0\n\t}\n\trecorder.Checkpoint(ctx, descriptor)\n\n\texportRecord := export.NewRecord(descriptor, labels, recorder)\n\terr := m.batcher.Process(ctx, exportRecord)\n\tif err != nil {\n\t\tm.errorHandler(err)\n\t}\n\treturn 1\n}\n\n// Resource returns the Resource this SDK was created with describing the\n// entity for which it creates instruments for.\n//\n// Resource means that the SDK implements the Resourcer interface and\n// therefore all metric instruments it creates will inherit its\n// Resource by default unless explicitly overwritten.\nfunc (m *SDK) Resource() resource.Resource {\n\treturn m.resource\n}\n\n// RecordBatch enters a batch of metric events.\nfunc (m *SDK) RecordBatch(ctx context.Context, kvs []core.KeyValue, measurements ...api.Measurement) {\n\t// Labels will be computed the first time acquireHandle is\n\t// called.  Subsequent calls to acquireHandle will re-use the\n\t// previously computed value instead of recomputing the\n\t// ordered labels.\n\tvar labels labels\n\tfor i, meas := range measurements {\n\t\ts := meas.SyncImpl().(*syncInstrument)\n\n\t\th := s.acquireHandle(kvs, &labels)\n\n\t\t// Re-use labels for the next measurement.\n\t\tif i == 0 {\n\t\t\tlabels = h.labels\n\t\t}\n\n\t\tdefer h.Unbind()\n\t\th.RecordOne(ctx, meas.Number())\n\t}\n}\n\nfunc (r *record) RecordOne(ctx context.Context, number core.Number) {\n\tif r.recorder == nil {\n\t\t// The instrument is disabled according to the AggregationSelector.\n\t\treturn\n\t}\n\tif err := aggregator.RangeTest(number, &r.inst.descriptor); err != nil {\n\t\tr.inst.meter.errorHandler(err)\n\t\treturn\n\t}\n\tif err := r.recorder.Update(ctx, number, &r.inst.descriptor); err != nil {\n\t\tr.inst.meter.errorHandler(err)\n\t\treturn\n\t}\n}\n\nfunc (r *record) Unbind() {\n\t// Record was modified, inform the Collect() that things need to be collected.\n\t// TODO: Reconsider if we should marked as modified when an Update happens and\n\t// collect only when updates happened even for Bounds.\n\tatomic.StoreInt64(&r.modified, 1)\n\tr.refMapped.unref()\n}\n\nfunc (r *record) mapkey() mapkey {\n\treturn mapkey{\n\t\tdescriptor: &r.inst.descriptor,\n\t\tordered:    r.labels.ordered,\n\t}\n}\n", "idx": 1, "id": 11731, "msg": "I think I like this approach because it uses no extra memory unless the caller produces multiple observations.", "proj": "open-telemetry-opentelemetry-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -46,7 +46,7 @@ RestrictionMap::RestrictionMap(const std::shared_ptr<NodeBasedDynamicGraph> &gra\n         m_restriction_start_nodes.insert(restriction.fromNode);\n         m_no_turn_via_node_set.insert(restriction.viaNode);\n \n-        std::pair<NodeID, NodeID> restriction_source = {restriction.fromNode, restriction.viaNode};\n+        RestrictionSource restriction_source(restriction.fromNode, restriction.viaNode);\n \n         unsigned index;\n         auto restriction_iter = m_restriction_map.find(restriction_source);", "y": 1, "oldf": "/*\n\nCopyright (c) 2013, Project OSRM, Dennis Luxen, others\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list\nof conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this\nlist of conditions and the following disclaimer in the documentation and/or\nother materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n*/\n\n#include \"RestrictionMap.h\"\n#include \"NodeBasedGraph.h\"\n\n#include \"../Util/SimpleLogger.h\"\n\nbool RestrictionMap::IsViaNode(const NodeID node) const\n{\n    return m_no_turn_via_node_set.find(node) != m_no_turn_via_node_set.end();\n}\n\nRestrictionMap::RestrictionMap(const std::shared_ptr<NodeBasedDynamicGraph> &graph,\n                               const std::vector<TurnRestriction> &restriction_list)\n    : m_count(0), m_graph(graph)\n{\n    // decompose restriction consisting of a start, via and end node into a\n    // a pair of starting edge and a list of all end nodes\n    for (auto &restriction : restriction_list)\n    {\n        m_restriction_start_nodes.insert(restriction.fromNode);\n        m_no_turn_via_node_set.insert(restriction.viaNode);\n\n        std::pair<NodeID, NodeID> restriction_source = {restriction.fromNode, restriction.viaNode};\n\n        unsigned index;\n        auto restriction_iter = m_restriction_map.find(restriction_source);\n        if (restriction_iter == m_restriction_map.end())\n        {\n            index = m_restriction_bucket_list.size();\n            m_restriction_bucket_list.resize(index + 1);\n            m_restriction_map.emplace(restriction_source, index);\n        }\n        else\n        {\n            index = restriction_iter->second;\n            // Map already contains an is_only_*-restriction\n            if (m_restriction_bucket_list.at(index).begin()->second)\n            {\n                continue;\n            }\n            else if (restriction.flags.isOnly)\n            {\n                // We are going to insert an is_only_*-restriction. There can be only one.\n                m_count -= m_restriction_bucket_list.at(index).size();\n                m_restriction_bucket_list.at(index).clear();\n            }\n        }\n        ++m_count;\n        m_restriction_bucket_list.at(index)\n            .emplace_back(restriction.toNode, restriction.flags.isOnly);\n    }\n}\n\n// Replace end v with w in each turn restriction containing u as via node\nvoid RestrictionMap::FixupArrivingTurnRestriction(const NodeID node_u,\n                                                  const NodeID node_v,\n                                                  const NodeID node_w)\n{\n    BOOST_ASSERT(node_u != SPECIAL_NODEID);\n    BOOST_ASSERT(node_v != SPECIAL_NODEID);\n    BOOST_ASSERT(node_w != SPECIAL_NODEID);\n\n    if (!IsViaNode(node_u))\n    {\n        return;\n    }\n\n    // find all potential start edges. It is more efficent to get a (small) list\n    // of potential start edges than iterating over all buckets\n    std::vector<NodeID> predecessors;\n    for (const EdgeID current_edge_id : m_graph->GetAdjacentEdgeRange(node_u))\n    {\n        const EdgeData &edge_data = m_graph->GetEdgeData(current_edge_id);\n        const NodeID target = m_graph->GetTarget(current_edge_id);\n        if (edge_data.backward && (node_v != target))\n        {\n            predecessors.push_back(target);\n        }\n    }\n\n    for (const NodeID node_x : predecessors)\n    {\n        const auto restriction_iterator = m_restriction_map.find({node_x, node_u});\n        if (restriction_iterator == m_restriction_map.end())\n        {\n            continue;\n        }\n\n        const unsigned index = restriction_iterator->second;\n        auto &bucket = m_restriction_bucket_list.at(index);\n        for (RestrictionTarget &restriction_target : bucket)\n        {\n            if (node_v == restriction_target.first)\n            {\n                restriction_target.first = node_w;\n            }\n        }\n    }\n}\n\n// Replaces start edge (v, w) with (u, w). Only start node changes.\nvoid RestrictionMap::FixupStartingTurnRestriction(const NodeID node_u,\n                                                  const NodeID node_v,\n                                                  const NodeID node_w)\n{\n    BOOST_ASSERT(node_u != SPECIAL_NODEID);\n    BOOST_ASSERT(node_v != SPECIAL_NODEID);\n    BOOST_ASSERT(node_w != SPECIAL_NODEID);\n\n    if (!IsSourceNode(node_v))\n    {\n        return;\n    }\n\n    const auto restriction_iterator = m_restriction_map.find({node_v, node_w});\n    if (restriction_iterator != m_restriction_map.end())\n    {\n        const unsigned index = restriction_iterator->second;\n        // remove old restriction start (v,w)\n        m_restriction_map.erase(restriction_iterator);\n        m_restriction_start_nodes.emplace(node_u);\n        // insert new restriction start (u,w) (pointing to index)\n        RestrictionSource new_source = {node_u, node_w};\n        m_restriction_map.emplace(new_source, index);\n    }\n}\n\n// Check if edge (u, v) is the start of any turn restriction.\n// If so returns id of first target node.\nNodeID RestrictionMap::CheckForEmanatingIsOnlyTurn(const NodeID node_u,\n                                                   const NodeID node_v) const\n{\n    BOOST_ASSERT(node_u != SPECIAL_NODEID);\n    BOOST_ASSERT(node_v != SPECIAL_NODEID);\n\n    if (!IsSourceNode(node_u))\n    {\n        return SPECIAL_NODEID;\n    }\n\n    auto restriction_iter = m_restriction_map.find({node_u, node_v});\n    if (restriction_iter != m_restriction_map.end())\n    {\n        const unsigned index = restriction_iter->second;\n        auto &bucket = m_restriction_bucket_list.at(index);\n        for (const RestrictionSource &restriction_target : bucket)\n        {\n            if (restriction_target.second)\n            {\n                return restriction_target.first;\n            }\n        }\n    }\n    return SPECIAL_NODEID;\n}\n\n// Checks if turn <u,v,w> is actually a turn restriction.\nbool RestrictionMap::CheckIfTurnIsRestricted(const NodeID node_u,\n                                             const NodeID node_v,\n                                             const NodeID node_w) const\n{\n    // return false;\n\n    BOOST_ASSERT(node_u != SPECIAL_NODEID);\n    BOOST_ASSERT(node_v != SPECIAL_NODEID);\n    BOOST_ASSERT(node_w != SPECIAL_NODEID);\n\n    if (!IsSourceNode(node_u))\n    {\n        return false;\n    }\n\n    auto restriction_iter = m_restriction_map.find({node_u, node_v});\n    if (restriction_iter != m_restriction_map.end())\n    {\n        const unsigned index = restriction_iter->second;\n        const auto &bucket = m_restriction_bucket_list.at(index);\n        for (const RestrictionTarget &restriction_target : bucket)\n        {\n            if ((node_w == restriction_target.first) && // target found\n                (!restriction_target.second))       // and not an only_-restr.\n            {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n\n// check of node is the start of any restriction\nbool RestrictionMap::IsSourceNode(const NodeID node) const\n{\n    if (m_restriction_start_nodes.find(node) == m_restriction_start_nodes.end())\n    {\n        return false;\n    }\n    return true;\n}\n", "idx": 1, "id": 13417, "msg": "The initialization should be done as before with curly braces.", "proj": "Project-OSRM-osrm-backend", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -26,10 +26,12 @@\n #include \"scheduler.h\"\n \n #include \"pugicast.h\"\n+#include \"events.h\"\n \n extern ConfigManager g_config;\n extern Monsters g_monsters;\n extern Game g_game;\n+extern Events* g_events;\n \n static constexpr int32_t MINSPAWN_INTERVAL = 1000;\n ", "y": 0, "oldf": "/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2019  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n */\n\n#include \"otpch.h\"\n\n#include \"spawn.h\"\n#include \"game.h\"\n#include \"monster.h\"\n#include \"configmanager.h\"\n#include \"scheduler.h\"\n\n#include \"pugicast.h\"\n\nextern ConfigManager g_config;\nextern Monsters g_monsters;\nextern Game g_game;\n\nstatic constexpr int32_t MINSPAWN_INTERVAL = 1000;\n\nbool Spawns::loadFromXml(const std::string& filename)\n{\n\tif (loaded) {\n\t\treturn true;\n\t}\n\n\tpugi::xml_document doc;\n\tpugi::xml_parse_result result = doc.load_file(filename.c_str());\n\tif (!result) {\n\t\tprintXMLError(\"Error - Spawns::loadFromXml\", filename, result);\n\t\treturn false;\n\t}\n\n\tthis->filename = filename;\n\tloaded = true;\n\n\tfor (auto spawnNode : doc.child(\"spawns\").children()) {\n\t\tPosition centerPos(\n\t\t\tpugi::cast<uint16_t>(spawnNode.attribute(\"centerx\").value()),\n\t\t\tpugi::cast<uint16_t>(spawnNode.attribute(\"centery\").value()),\n\t\t\tpugi::cast<uint16_t>(spawnNode.attribute(\"centerz\").value())\n\t\t);\n\n\t\tint32_t radius;\n\t\tpugi::xml_attribute radiusAttribute = spawnNode.attribute(\"radius\");\n\t\tif (radiusAttribute) {\n\t\t\tradius = pugi::cast<int32_t>(radiusAttribute.value());\n\t\t} else {\n\t\t\tradius = -1;\n\t\t}\n\n\t\tspawnList.emplace_front(centerPos, radius);\n\t\tSpawn& spawn = spawnList.front();\n\n\t\tfor (auto childNode : spawnNode.children()) {\n\t\t\tif (strcasecmp(childNode.name(), \"monster\") == 0) {\n\t\t\t\tpugi::xml_attribute nameAttribute = childNode.attribute(\"name\");\n\t\t\t\tif (!nameAttribute) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tDirection dir;\n\n\t\t\t\tpugi::xml_attribute directionAttribute = childNode.attribute(\"direction\");\n\t\t\t\tif (directionAttribute) {\n\t\t\t\t\tdir = static_cast<Direction>(pugi::cast<uint16_t>(directionAttribute.value()));\n\t\t\t\t} else {\n\t\t\t\t\tdir = DIRECTION_NORTH;\n\t\t\t\t}\n\n\t\t\t\tPosition pos(\n\t\t\t\t\tcenterPos.x + pugi::cast<uint16_t>(childNode.attribute(\"x\").value()),\n\t\t\t\t\tcenterPos.y + pugi::cast<uint16_t>(childNode.attribute(\"y\").value()),\n\t\t\t\t\tcenterPos.z\n\t\t\t\t);\n\t\t\t\tuint32_t interval = pugi::cast<uint32_t>(childNode.attribute(\"spawntime\").value()) * 1000;\n\t\t\t\tif (interval > MINSPAWN_INTERVAL) {\n\t\t\t\t\tspawn.addMonster(nameAttribute.as_string(), pos, dir, interval);\n\t\t\t\t} else {\n\t\t\t\t\tstd::cout << \"[Warning - Spawns::loadFromXml] \" << nameAttribute.as_string() << ' ' << pos << \" spawntime can not be less than \" << MINSPAWN_INTERVAL / 1000 << \" seconds.\" << std::endl;\n\t\t\t\t}\n\t\t\t} else if (strcasecmp(childNode.name(), \"npc\") == 0) {\n\t\t\t\tpugi::xml_attribute nameAttribute = childNode.attribute(\"name\");\n\t\t\t\tif (!nameAttribute) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tNpc* npc = Npc::createNpc(nameAttribute.as_string());\n\t\t\t\tif (!npc) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tpugi::xml_attribute directionAttribute = childNode.attribute(\"direction\");\n\t\t\t\tif (directionAttribute) {\n\t\t\t\t\tnpc->setDirection(static_cast<Direction>(pugi::cast<uint16_t>(directionAttribute.value())));\n\t\t\t\t}\n\n\t\t\t\tnpc->setMasterPos(Position(\n\t\t\t\t\tcenterPos.x + pugi::cast<uint16_t>(childNode.attribute(\"x\").value()),\n\t\t\t\t\tcenterPos.y + pugi::cast<uint16_t>(childNode.attribute(\"y\").value()),\n\t\t\t\t\tcenterPos.z\n\t\t\t\t), radius);\n\t\t\t\tnpcList.push_front(npc);\n\t\t\t}\n\t\t}\n\t}\n\treturn true;\n}\n\nvoid Spawns::startup()\n{\n\tif (!loaded || isStarted()) {\n\t\treturn;\n\t}\n\n\tfor (Npc* npc : npcList) {\n\t\tg_game.placeCreature(npc, npc->getMasterPos(), false, true);\n\t}\n\tnpcList.clear();\n\n\tfor (Spawn& spawn : spawnList) {\n\t\tspawn.startup();\n\t}\n\n\tstarted = true;\n}\n\nvoid Spawns::clear()\n{\n\tfor (Spawn& spawn : spawnList) {\n\t\tspawn.stopEvent();\n\t}\n\tspawnList.clear();\n\n\tloaded = false;\n\tstarted = false;\n\tfilename.clear();\n}\n\nbool Spawns::isInZone(const Position& centerPos, int32_t radius, const Position& pos)\n{\n\tif (radius == -1) {\n\t\treturn true;\n\t}\n\n\treturn ((pos.getX() >= centerPos.getX() - radius) && (pos.getX() <= centerPos.getX() + radius) &&\n\t        (pos.getY() >= centerPos.getY() - radius) && (pos.getY() <= centerPos.getY() + radius));\n}\n\nvoid Spawn::startSpawnCheck()\n{\n\tif (checkSpawnEvent == 0) {\n\t\tcheckSpawnEvent = g_scheduler.addEvent(createSchedulerTask(getInterval(), std::bind(&Spawn::checkSpawn, this)));\n\t}\n}\n\nSpawn::~Spawn()\n{\n\tfor (const auto& it : spawnedMap) {\n\t\tMonster* monster = it.second;\n\t\tmonster->setSpawn(nullptr);\n\t\tmonster->decrementReferenceCounter();\n\t}\n}\n\nbool Spawn::findPlayer(const Position& pos)\n{\n\tSpectatorVec spectators;\n\tg_game.map.getSpectators(spectators, pos, false, true);\n\tfor (Creature* spectator : spectators) {\n\t\tif (!spectator->getPlayer()->hasFlag(PlayerFlag_IgnoredByMonsters)) {\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nbool Spawn::isInSpawnZone(const Position& pos)\n{\n\treturn Spawns::isInZone(centerPos, radius, pos);\n}\n\nbool Spawn::spawnMonster(uint32_t spawnId, MonsterType* mType, const Position& pos, Direction dir, bool startup /*= false*/)\n{\n\tstd::unique_ptr<Monster> monster_ptr(new Monster(mType));\n\tif (startup) {\n\t\t//No need to send out events to the surrounding since there is no one out there to listen!\n\t\tif (!g_game.internalPlaceCreature(monster_ptr.get(), pos, true)) {\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tif (!g_game.placeCreature(monster_ptr.get(), pos, false, true)) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tMonster* monster = monster_ptr.release();\n\tmonster->setDirection(dir);\n\tmonster->setSpawn(this);\n\tmonster->setMasterPos(pos);\n\tmonster->incrementReferenceCounter();\n\n\tspawnedMap.insert(spawned_pair(spawnId, monster));\n\tspawnMap[spawnId].lastSpawn = OTSYS_TIME();\n\treturn true;\n}\n\nvoid Spawn::startup()\n{\n\tfor (const auto& it : spawnMap) {\n\t\tuint32_t spawnId = it.first;\n\t\tconst spawnBlock_t& sb = it.second;\n\t\tspawnMonster(spawnId, sb.mType, sb.pos, sb.direction, true);\n\t}\n}\n\nvoid Spawn::checkSpawn()\n{\n\tcheckSpawnEvent = 0;\n\n\tcleanup();\n\n\tuint32_t spawnCount = 0;\n\n\tfor (auto& it : spawnMap) {\n\t\tuint32_t spawnId = it.first;\n\t\tif (spawnedMap.find(spawnId) != spawnedMap.end()) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tspawnBlock_t& sb = it.second;\n\t\tif (OTSYS_TIME() >= sb.lastSpawn + sb.interval) {\n\t\t\tif (findPlayer(sb.pos)) {\n\t\t\t\tsb.lastSpawn = OTSYS_TIME();\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tspawnMonster(spawnId, sb.mType, sb.pos, sb.direction);\n\t\t\tif (++spawnCount >= static_cast<uint32_t>(g_config.getNumber(ConfigManager::RATE_SPAWN))) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (spawnedMap.size() < spawnMap.size()) {\n\t\tcheckSpawnEvent = g_scheduler.addEvent(createSchedulerTask(getInterval(), std::bind(&Spawn::checkSpawn, this)));\n\t}\n}\n\nvoid Spawn::cleanup()\n{\n\tauto it = spawnedMap.begin();\n\twhile (it != spawnedMap.end()) {\n\t\tuint32_t spawnId = it->first;\n\t\tMonster* monster = it->second;\n\t\tif (monster->isRemoved()) {\n\t\t\tif (spawnId != 0) {\n\t\t\t\tspawnMap[spawnId].lastSpawn = OTSYS_TIME();\n\t\t\t}\n\n\t\t\tmonster->decrementReferenceCounter();\n\t\t\tit = spawnedMap.erase(it);\n\t\t} else if (!isInSpawnZone(monster->getPosition()) && spawnId != 0) {\n\t\t\tspawnedMap.insert(spawned_pair(0, monster));\n\t\t\tit = spawnedMap.erase(it);\n\t\t} else {\n\t\t\t++it;\n\t\t}\n\t}\n}\n\nbool Spawn::addMonster(const std::string& name, const Position& pos, Direction dir, uint32_t interval)\n{\n\tMonsterType* mType = g_monsters.getMonsterType(name);\n\tif (!mType) {\n\t\tstd::cout << \"[Spawn::addMonster] Can not find \" << name << std::endl;\n\t\treturn false;\n\t}\n\n\tthis->interval = std::min(this->interval, interval);\n\n\tspawnBlock_t sb;\n\tsb.mType = mType;\n\tsb.pos = pos;\n\tsb.direction = dir;\n\tsb.interval = interval;\n\tsb.lastSpawn = 0;\n\n\tuint32_t spawnId = spawnMap.size() + 1;\n\tspawnMap[spawnId] = sb;\n\treturn true;\n}\n\nvoid Spawn::removeMonster(Monster* monster)\n{\n\tfor (auto it = spawnedMap.begin(), end = spawnedMap.end(); it != end; ++it) {\n\t\tif (it->second == monster) {\n\t\t\tmonster->decrementReferenceCounter();\n\t\t\tspawnedMap.erase(it);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid Spawn::stopEvent()\n{\n\tif (checkSpawnEvent != 0) {\n\t\tg_scheduler.stopEvent(checkSpawnEvent);\n\t\tcheckSpawnEvent = 0;\n\t}\n}\n", "idx": 1, "id": 16545, "msg": "", "proj": "otland-forgottenserver", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -1239,7 +1239,7 @@ class SpatialPooler(object):\n     below that of their neighbors are boosted depending on how infrequently they\n     have been active. The more infrequent, the more they are boosted. The exact\n     boost factor is linearly interpolated between the points (dutyCycle:0,\n-    boost:maxFiringBoost) and (dutyCycle:minDuty, boost:1.0). \n+    boost:maxFiringBoost) and (dutyCycle:minDuty, boost:1.0).\n \n             boostFactor\n                 ^", "y": 0, "oldf": "# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\nimport copy\nimport cPickle\nimport itertools\nimport numpy\n\nfrom nupic.bindings.math import (SM32 as SparseMatrix,\n                                SM_01_32_32 as SparseBinaryMatrix,\n                                GetNTAReal,\n                                Random as NupicRandom)\n\nrealDType = GetNTAReal()\nuintType = \"uint32\"\n\n\n\nclass SpatialPooler(object):\n  \"\"\"\n  This class implements a the spatial pooler. It is in charge of handling the \n  relationships between the columns of a region and the inputs bits. The \n  primary public interface to this function is the \"compute\" method, which \n  takes in an input vector and returns a list of activeColumns columns. \n  Example Usage:\n  >\n  > sp = SpatialPooler(...)\n  > for line in file:\n  >   inputVector = numpy.array(line)\n  >   sp.compute(inputVector)\n  >   ...\n  \"\"\"\n\n  def __init__(self,\n               inputDimensions=[32,32],\n               columnDimensions=[64,64],\n               potentialRadius=16,\n               potentialPct=0.5,\n               globalInhibition=False,\n               localAreaDensity=-1.0,\n               numActiveColumnsPerInhArea=10.0,\n               stimulusThreshold=0,\n               synPermInactiveDec=0.01,\n               synPermActiveInc=0.1,\n               synPermConnected=0.10,\n               minPctOverlapDutyCycle=0.001,\n               minPctActiveDutyCycle=0.001,\n               dutyCyclePeriod=1000,\n               maxBoost=10.0,\n               seed=-1,\n               spVerbosity=0\n               ):\n    \"\"\"\n    Parameters:\n    ----------------------------\n    inputDimensions:      A list representing the dimensions of the input\n                          vector. Format is [height, width, depth, ...], where\n                          each value represents the size of the dimension. For a\n                          topology of one dimesion with 100 inputs use 100, or\n                          [100]. For a two dimensional topology of 10x5 use\n                          [10,5]. \n    columnDimensions:     A list representing the dimensions of the columns in\n                          the region. Format is [height, width, depth, ...],\n                          where each value represents the size of the dimension.\n                          For a topology of one dimesion with 2000 columns use\n                          2000, or [2000]. For a three dimensional topology of\n                          32x64x16 use [32, 64, 16]. \n    potentialRadius:      This parameter deteremines the extent of the input \n                          that each column can potentially be connected to. \n                          This can be thought of as the input bits that\n                          are visible to each column, or a 'receptiveField' of \n                          the field of vision. A large enough value will result \n                          in the 'global coverage', meaning that each column \n                          can potentially be connected to every input bit. This \n                          parameter defines a square (or hyper square) area: a \n                          column will have a max square potential pool with \n                          sides of length 2 * potentialRadius + 1. \n    potentialPct:         The percent of the inputs, within a column's\n                          potential radius, that a column can be connected to. \n                          If set to 1, the column will be connected to every \n                          input within its potential radius. This parameter is \n                          used to give each column a unique potential pool when \n                          a large potentialRadius causes overlap between the \n                          columns. At initialization time we choose \n                          ((2*potentialRadius + 1)^(# inputDimensions) * \n                          potentialPct) input bits to comprise the column's\n                          potential pool.\n    globalInhibition:     If true, then during inhibition phase the winning \n                          columns are selected as the most active columns from \n                          the region as a whole. Otherwise, the winning columns \n                          are selected with resepct to their local \n                          neighborhoods. using global inhibition boosts \n                          performance x60.\n    localAreaDensity:     The desired density of active columns within a local\n                          inhibition area (the size of which is set by the\n                          internally calculated inhibitionRadius, which is in\n                          turn determined from the average size of the \n                          connected potential pools of all columns). The \n                          inhibition logic will insure that at most N columns \n                          remain ON within a local inhibition area, where N = \n                          localAreaDensity * (total number of columns in \n                          inhibition area).\n    numActivePerInhArea:  An alternate way to control the density of the active\n                          columns. If numActivePerInhArea is specified then\n                          localAreaDensity must less than 0, and vice versa. \n                          When using numActivePerInhArea, the inhibition logic \n                          will insure that at most 'numActivePerInhArea' \n                          columns remain ON within a local inhibition area (the \n                          size of which is set by the internally calculated\n                          inhibitionRadius, which is in turn determined from \n                          the average size of the connected receptive fields of \n                          all columns). When using this method, as columns \n                          learn and grow their effective receptive fields, the\n                          inhibitionRadius will grow, and hence the net density\n                          of the active columns will *decrease*. This is in\n                          contrast to the localAreaDensity method, which keeps\n                          the density of active columns the same regardless of\n                          the size of their receptive fields.\n    stimulusThreshold:    This is a number specifying the minimum number of\n                          synapses that must be on in order for a columns to\n                          turn ON. The purpose of this is to prevent noise \n                          input from activating columns. Specified as a percent \n                          of a fully grown synapse.\n    synPermInactiveDec:   The amount by which an inactive synapse is \n                          decremented in each round. Specified as a percent of \n                          a fully grown synapse.\n    synPermActiveInc:     The amount by which an active synapse is incremented \n                          in each round. Specified as a percent of a\n                          fully grown synapse.\n    synPermConnected:     The default connected threshold. Any synapse whose\n                          permanence value is above the connected threshold is\n                          a \"connected synapse\", meaning it can contribute to\n                          the cell's firing.\n    minPctOvlerapDutyCycle: A number between 0 and 1.0, used to set a floor on\n                          how often a column should have at least\n                          stimulusThreshold active inputs. Periodically, each\n                          column looks at the overlap duty cycle of\n                          all other column within its inhibition radius and \n                          sets its own internal minimal acceptable duty cycle \n                          to: minPctDutyCycleBeforeInh * max(other columns' \n                          duty cycles).\n                          On each iteration, any column whose overlap duty \n                          cycle falls below this computed value will  get\n                          all of its permanence values boosted up by\n                          synPermActiveInc. Raising all permanences in response\n                          to a sub-par duty cycle before  inhibition allows a\n                          cell to search for new inputs when either its\n                          previously learned inputs are no longer ever active,\n                          or when the vast majority of them have been \n                          \"hijacked\" by other columns.\n    minPctActiveDutyCycle: A number between 0 and 1.0, used to set a floor on\n                          how often a column should be activate.\n                          Periodically, each column looks at the activity duty \n                          cycle of all other columns within its inhibition \n                          radius and sets its own internal minimal acceptable \n                          duty cycle to:\n                            minPctDutyCycleAfterInh *\n                            max(other columns' duty cycles).\n                          On each iteration, any column whose duty cycle after\n                          inhibition falls below this computed value will get\n                          its internal boost factor increased.\n    dutyCyclePeriod:      The period used to calculate duty cycles. Higher\n                          values make it take longer to respond to changes in\n                          boost or synPerConnectedCell. Shorter values make it\n                          more unstable and likely to oscillate.\n     maxBoost:            The maximum overlap boost factor. Each column's\n                          overlap gets multiplied by a boost factor\n                          before it gets considered for inhibition.\n                          The actual boost factor for a column is number \n                          between 1.0 and maxBoost. A boost factor of 1.0 is \n                          used if the duty cycle is >= minOverlapDutyCycle, \n                          maxBoost is used if the duty cycle is 0, and any duty \n                          cycle in between is linearly extrapolated from these \n                          2 endpoints.\n    seed:                 Seed for our own pseudo-random number generator.\n    spVerbosity:          spVerbosity level: 0, 1, 2, or 3\n    \"\"\"\n    self.initialize(inputDimensions,\n                    columnDimensions,\n                    potentialRadius,\n                    potentialPct,\n                    globalInhibition,\n                    localAreaDensity,\n                    numActiveColumnsPerInhArea,\n                    stimulusThreshold,\n                    synPermInactiveDec,\n                    synPermActiveInc,\n                    synPermConnected,\n                    minPctOverlapDutyCycle,\n                    minPctActiveDutyCycle,\n                    dutyCyclePeriod,\n                    maxBoost,\n                    seed,\n                    spVerbosity)\n\n\n  def getNumColumns(self):\n    \"\"\"Returns the total number of columns\"\"\"\n    return self._numColumns\n\n\n  def getNumInputs(self):\n    \"\"\"Returns the total number of inputs\"\"\"\n    return self._numInputs\n\n\n  def getPotentialRadius(self):\n    \"\"\"Returns the potential radius\"\"\"\n    return self._potentialRadius\n\n\n  def setPotentialRadius(self, potentialRadius):\n    \"\"\"Sets the potential radius\"\"\"\n    self._potentialRadius = potentialRadius\n\n\n  def getPotentialPct(self):\n    \"\"\"Returns the potential percent\"\"\"\n    return self._potentialPct\n\n\n  def setPotentialPct(self, potentialPct):\n    \"\"\"Sets the potential percent\"\"\"\n    self._potentialPct = potentialPct\n\n\n  def getGlobalInhibition(self):\n    \"\"\"Returns whether global inhibition is enabled\"\"\"\n    return self._globalInhibition\n\n\n  def setGlobalInhibition(self, globalInhibition):\n    \"\"\"Sets global inhibition\"\"\"\n    self._globalInhibition = globalInhibition\n\n  \n  def getNumActiveColumnsPerInhArea(self):\n    \"\"\"Returns the number of active columns per inhibition area. Returns a \n    value less than 0 if parameter is unused\"\"\"\n    return self._numActiveColumnsPerInhArea\n\n\n  def setNumActiveColumnsPerInhArea(self, numActiveColumnsPerInhArea):\n    \"\"\"Sets the number of active columns per inhibition area. Invalidates the \n    'localAreaDensity' parameter\"\"\"\n    assert(numActiveColumnsPerInhArea > 0)\n    self._numActiveColumnsPerInhArea = numActiveColumnsPerInhArea\n    self._localAreaDensity = 0\n\n  \n  def getLocalAreaDensity(self):\n    \"\"\"Returns the local area density. Returns a value less than 0 if parameter\n    is unused\"\"\"\n    return self._localAreaDensity\n\n\n  def setLocalAreaDensity(self, localAreaDensity):\n    \"\"\"Sets the local area density. Invalidates the 'numActivePerInhArea' \n    parameter\"\"\"\n    assert(localAreaDensity > 0 and localAreaDensity <= 1)\n    self._localAreaDensity = localAreaDensity\n    self._numActiveColumnsPerInhArea = 0\n\n  \n  def getStimulusThreshold(self):\n    \"\"\"Returns the stimulus threshold\"\"\"\n    return self._stimulusThreshold\n\n\n  def setStimulusThreshold(self, stimulusThreshold):\n    \"\"\"Sets the stimulus threshold\"\"\"\n    self._stimulusThreshold = stimulusThreshold\n\n  \n  def getInhibitionRadius(self):\n    \"\"\"Returns the inhibition radius\"\"\"\n    return self._inhibitionRadius\n\n  \n  def setInhibitionRadius(self, inhibitionRadius):\n    \"\"\"Sets the inhibition radius\"\"\"\n    self._inhibitionRadius = inhibitionRadius\n\n  \n  def getDutyCyclePeriod(self):\n    \"\"\"Returns the duty cycle period\"\"\"\n    return self._dutyCyclePeriod\n\n  \n  def setDutyCyclePeriod(self, dutyCyclePeriod):\n    \"\"\"Sets the duty cycle period\"\"\"\n    self._dutyCyclePeriod = dutyCyclePeriod\n\n  \n  def getMaxBoost(self):\n    \"\"\"Returns the maximum boost value\"\"\"\n    return self._maxBoost\n\n\n  def setMaxBoost(self, maxBoost):\n    \"\"\"Sets the maximum boost value\"\"\"\n    self._maxBoost = maxBoost\n\n  \n  def getIterationNum(self):\n    \"\"\"Returns the iteration number\"\"\"\n    return self._iterationNum\n\n\n  def setIterationNum(self, iterationNum):\n    \"\"\"Sets the iteration number\"\"\"\n    self._iterationNum = iterationNum\n\n  \n  def getIterationLearnNum(self):\n    \"\"\"Returns the learning iteration number\"\"\"\n    return self._iterationLearnNum\n\n\n  def setIterationLearnNum(self, iterationLearnNum):\n    \"\"\"Sets the learning iteration number\"\"\"\n    self._iterationLearnNum = iterationLearnNum\n\n  \n  def getSpVerbosity(self):\n    \"\"\"Returns the verbosity level\"\"\"\n    return self._spVerbosity\n\n\n  def setSpVerbosity(self, spVerbosity):\n    \"\"\"Sets the verbosity level\"\"\"\n    self._spVerbosity = spVerbosity\n\n  \n  def getUpdatePeriod(self):\n    \"\"\"Returns the update period\"\"\"\n    return self._updatePeriod\n\n  \n  def setUpdatePeriod(self, updatePeriod):\n    \"\"\"Sets the update period\"\"\"\n    self._updatePeriod = updatePeriod\n\n  \n  def getSynPermTrimThreshold(self):\n    \"\"\"Returns the permanence trim threshold\"\"\"\n    return self._synPermTrimThreshold\n\n\n  def setSynPermTrimThreshold(self, synPermTrimThreshold):\n    \"\"\"Sets the permanence trim threshold\"\"\"\n    self._synPermTrimThreshold = synPermTrimThreshold\n\n  \n  def getSynPermActiveInc(self):\n    \"\"\"Returns the permanence increment amount for active synapses\n    inputs\"\"\"\n    return self._synPermActiveInc\n\n\n  def setSynPermActiveInc(self, synPermActiveInc):\n    \"\"\"Sets the permanence increment amount for active synapses\"\"\"\n    self._synPermActiveInc = synPermActiveInc\n\n\n  def getSynPermInactiveDec(self):\n    \"\"\"Returns the permanence decrement amount for inactive synapses\"\"\"\n    return self._synPermInactiveDec\n\n\n  def setSynPermInactiveDec(self, synPermInactiveDec):\n    \"\"\"Sets the permanence decrement amount for inactive synapses\"\"\"\n    self._synPermInactiveDec = synPermInactiveDec\n\n\n  def getSynPermBelowStimulusInc(self):\n    \"\"\"Returns the permanence increment amount for columns that have not been \n    recently active \"\"\"\n    return self._synPermBelowStimulusInc\n\n\n  def setSynPermBelowStimulusInc(self, synPermBelowStimulusInc):\n    \"\"\"Sets the permanence increment amount for columns that have not been \n    recently active \"\"\"\n    self._synPermBelowStimulusInc = synPermBelowStimulusInc\n\n\n  def getSynPermConnected(self):\n    \"\"\"Returns the permanence amount that qualifies a synapse as \n    being connected\"\"\"\n    return self._synPermConnected\n\n\n  def setSynPermConnected(self, synPermConnected):\n    \"\"\"Sets the permanence amount that qualifies a synapse as being \n    connected\"\"\"\n    self._synPermConnected = synPermConnected\n\n\n  def getMinPctOverlapDutyCycles(self):\n    \"\"\"Returns the minimum tolerated overlaps, given as percent of \n    neighbors overlap score\"\"\"\n    return self._minPctOverlapDutyCycles\n\n\n  def setMinPctOverlapDutyCycles(self, minPctOverlapDutyCycles):\n    \"\"\"Sets the minimum tolerated activity duty cycle, given as percent of \n    neighbors' activity duty cycle\"\"\"\n    self._minPctOverlapDutyCycles = minPctOverlapDutyCycles\n\n\n  def getMinPctActiveDutyCycles(self):\n    \"\"\"Returns the minimum tolerated activity duty cycle, given as percent of \n    neighbors' activity duty cycle\"\"\"\n    return self._minPctActiveDutyCycles\n\n\n  def setMinPctActiveDutyCycles(self, minPctActiveDutyCycles):\n    \"\"\"Sets the minimum tolerated activity duty, given as percent of\n    neighbors' activity duty cycle\"\"\"\n    self._minPctActiveDutyCycles = minPctActiveDutyCycles\n\n\n  def getBoostFactors(self, boostFactors):\n    \"\"\"Returns the boost factors for all columns. 'boostFactors' size must \n    match the number of columns\"\"\"\n    boostFactors[:] = self._boostFactors[:]\n\n\n  def setBoostFactors(self, boostFactors):\n    \"\"\"Sets the boost factors for all columns. 'boostFactors' size must match\n    the number of columns\"\"\"\n    self._boostFactors[:] = boostFactors[:]\n\n\n  def getOverlapDutyCycles(self, overlapDutyCycles):\n    \"\"\"Returns the overlap duty cycles for all columns. 'overlapDutyCycles' \n    size must match the number of columns\"\"\"\n    overlapDutyCycles[:] = self._overlapDutyCycles[:]\n\n\n  def setOverlapDutyCycles(self, overlapDutyCycles):\n    \"\"\"Sets the overlap duty cycles for all columns. 'overlapDutyCycles' \n    size must match the number of columns\"\"\"\n    self._overlapDutyCycles[:] = overlapDutyCycles\n\n\n  def getActiveDutyCycles(self, activeDutyCycles):\n    \"\"\"Returns the activity duty cycles for all columns. 'activeDutyCycles' \n    size must match the number of columns\"\"\"\n    activeDutyCycles[:] = self._activeDutyCycles[:]\n\n\n  def setActiveDutyCycles(self, activeDutyCycles):\n    \"\"\"Sets the activity duty cycles for all columns. 'activeDutyCycles' \n    size must match the number of columns\"\"\"\n    self._activeDutyCycles[:] = activeDutyCycles\n\n\n  def getMinOverlapDutyCycles(self, minOverlapDutyCycles):\n    \"\"\"Returns the minimum overlap duty cycles for all columns. \n    '_minOverlapDutyCycles' size must match the number of columns\"\"\"\n    minOverlapDutyCycles[:] = self._minOverlapDutyCycles[:]\n\n\n  def setMinOverlapDutyCycles(self, minOverlapDutyCycles):\n    \"\"\"Sets the minimum overlap duty cycles for all columns. \n    '_minOverlapDutyCycles' size must match the number of columns\"\"\"\n    self._minOverlapDutyCycles[:] = minOverlapDutyCycles[:]\n\n\n  def getMinActiveDutyCycles(self, minActiveDutyCycles):\n    \"\"\"Returns the minimum activity duty cycles for all columns. \n    '_minActiveDutyCycles' size must match the number of columns\"\"\"\n    minActiveDutyCycles[:] = self._minActiveDutyCycles[:]\n\n\n  def setMinActiveDutyCycles(self, minActiveDutyCycles):\n    \"\"\"Sets the minimum activity duty cycles for all columns. \n    '_minActiveDutyCycles' size must match the number of columns\"\"\"\n    self._minActiveDutyCycles = minActiveDutyCycles\n\n\n  def getPotential(self, column, potential):\n    \"\"\"Returns the potential mapping for a given column. 'potential' size\n    must match the number of inputs\"\"\"\n    assert(column < self._numColumns)\n    potential[:] = self._potentialPools.getRow(column)\n\n\n  def setPotential(self, column, potential):\n    \"\"\"Sets the potential mapping for a given column. 'potential' size\n    must match the number of inputs\"\"\"\n    assert(column < self._numColumns)\n    potentialSparse = numpy.where(potential > 0)[0]\n    self._potentialPools.replaceSparseRow(column, potentialSparse)\n\n  \n  def getPermanence(self, column, permanence):\n    \"\"\"Returns the permanence values for a given column. 'permanence' size\n    must match the number of inputs\"\"\"\n    assert(column < self._numColumns)\n    permanence[:] = self._permanences.getRow(column)\n\n\n  def setPermanence(self, column, permanence):\n    \"\"\"Sets the permanence values for a given column. 'permanence' size\n    must match the number of inputs\"\"\"\n    assert(column < self._numColumns)\n    self._updatePermanencesForColumn(permanence, column, raisePerm=False)\n\n\n  def getConnectedSynapses(self, column, connectedSynapses):\n    \"\"\"Returns the connected synapses for a given column. \n    'connectedSynapses' size must match the number of inputs\"\"\"\n    assert(column < self._numColumns)\n    connectedSynapses[:] = self._connectedSynapses.getRow(column)\n\n\n  def getConnectedCounts(self, connectedCounts):\n    \"\"\"Returns the number of connected synapses for all columns. \n    'connectedCounts' size must match the number of columns\"\"\"\n    connectedCounts[:] = self._connectedCounts[:]\n\n\n  def initialize(self,\n               inputDimensions=[32,32],\n               columnDimensions=[64,64],\n               potentialRadius=16,\n               potentialPct=0.5,\n               globalInhibition=False,\n               localAreaDensity=-1.0,\n               numActiveColumnsPerInhArea=10.0,\n               stimulusThreshold=0,\n               synPermInactiveDec=0.01,\n               synPermActiveInc=0.1,\n               synPermConnected=0.10,\n               minPctOverlapDutyCycle=0.001,\n               minPctActiveDutyCycle=0.001,\n               dutyCyclePeriod=1000,\n               maxBoost=10.0,\n               seed=-1,\n               spVerbosity=0):\n\n    # Verify input is valid\n    inputDimensions = numpy.array(inputDimensions)\n    columnDimensions = numpy.array(columnDimensions)\n    numColumns = columnDimensions.prod()\n    numInputs = inputDimensions.prod()\n\n    assert(numColumns > 0)\n    assert(numInputs > 0)\n    assert (numActiveColumnsPerInhArea > 0 or \n           (localAreaDensity > 0 and localAreaDensity <= 0.5))\n\n    self._seed(seed)\n\n    # save arguments\n    self._numInputs = int(numInputs)\n    self._numColumns = int(numColumns)\n    self._columnDimensions = columnDimensions\n    self._inputDimensions = inputDimensions\n    self._potentialRadius = int(min(potentialRadius, numInputs))\n    self._potentialPct = potentialPct\n    self._globalInhibition = globalInhibition\n    self._numActiveColumnsPerInhArea = int(numActiveColumnsPerInhArea)\n    self._localAreaDensity = localAreaDensity\n    self._stimulusThreshold = stimulusThreshold\n    self._synPermInactiveDec = synPermInactiveDec\n    self._synPermActiveInc = synPermActiveInc\n    self._synPermBelowStimulusInc = synPermConnected / 10.0\n    self._synPermConnected = synPermConnected\n    self._minPctOverlapDutyCycles = minPctOverlapDutyCycle\n    self._minPctActiveDutyCycles = minPctActiveDutyCycle\n    self._dutyCyclePeriod = dutyCyclePeriod\n    self._maxBoost = maxBoost\n    self._spVerbosity = spVerbosity\n\n    # Extra parameter settings\n    self._synPermMin = 0.0\n    self._synPermMax = 1.0\n    self._synPermTrimThreshold = synPermActiveInc / 2.0\n    assert(self._synPermTrimThreshold < self._synPermConnected)\n    self._updatePeriod = 50\n    initConnectedPct = 0.5\n\n    # Internal state\n    self._version = 1.0\n    self._iterationNum = 0\n    self._iterationLearnNum = 0\n\n    # initialize the random number generators\n    self._seed(seed)\n\n    # Store the set of all inputs that are within each column's potential pool.\n    # 'potentialPools' is a matrix, whose rows represent cortical columns, and \n    # whose columns represent the input bits. if potentialPools[i][j] == 1,\n    # then input bit 'j' is in column 'i's potential pool. A column can only be \n    # connected to inputs in its potential pool. The indices refer to a \n    # falttenned version of both the inputs and columns. Namely, irrespective \n    # of the topology of the inputs and columns, they are treated as being a \n    # one dimensional array. Since a column is typically connected to only a \n    # subset of the inputs, many of the entries in the matrix are 0. Therefore \n    # the the potentialPool matrix is stored using the SparseBinaryMatrix \n    # class, to reduce memory footprint and compuation time of algorithms that \n    # require iterating over the data strcuture.\n    self._potentialPools = SparseBinaryMatrix(numInputs)\n    self._potentialPools.resize(numColumns, numInputs)\n\n    # Initialize the permanences for each column. Similar to the \n    # 'self._potentialPools', the permances are stored in a matrix whose rows\n    # represent the cortial columns, and whose columns represent the input \n    # bits. if self._permanences[i][j] = 0.2, then the synapse connecting \n    # cortical column 'i' to input bit 'j'  has a permanence of 0.2. Here we \n    # also use the SparseMatrix class to reduce the memory footprint and \n    # computation time of algorithms that require iterating over the data \n    # structure. This permanence matrix is only allowed to have non-zero \n    # elements where the potential pool is non-zero.\n    self._permanences = SparseMatrix(numColumns, numInputs)\n    \n    # Initialize a tiny random tie breaker. This is used to determine winning\n    # columns where the overlaps are identical.\n    self._tieBreaker = 0.01*numpy.array([self._random.getReal64() for i in \n                                        xrange(self._numColumns)])\n\n\n    # 'self._connectedSynapses' is a similar matrix to 'self._permanences' \n    # (rows represent cortial columns, columns represent input bits) whose \n    # entries represent whether the cortial column is connected to the input \n    # bit, i.e. its permanence value is greater than 'synPermConnected'. While \n    # this information is readily available from the 'self._permanence' matrix, \n    # it is stored separately for efficiency purposes.\n    self._connectedSynapses = SparseBinaryMatrix(numInputs)\n    self._connectedSynapses.resize(numColumns, numInputs)\n\n    # Stores the number of connected synapses for each column. This is simply\n    # a sum of each row of 'self._connectedSynapses'. again, while this \n    # information is readily available from 'self._connectedSynapses', it is\n    # stored separately for efficiency purposes.\n    self._connectedCounts = numpy.zeros(numColumns, dtype=realDType)\n\n    # Initialize the set of permanence values for each columns. Ensure that \n    # each column is connected to enough input bits to allow it to be \n    # activated\n    for i in xrange(numColumns):\n      potential = self._mapPotential(i,wrapAround=True) \n      self._potentialPools.replaceSparseRow(i, potential.nonzero()[0])\n      perm = self._initPermanence(potential,initConnectedPct)\n      self._updatePermanencesForColumn(perm, i, raisePerm=True) \n    \n\n    self._overlapDutyCycles = numpy.zeros(numColumns, dtype=realDType)\n    self._activeDutyCycles = numpy.zeros(numColumns, dtype=realDType)\n    self._minOverlapDutyCycles = numpy.zeros(numColumns, \n                                             dtype=realDType)\n    self._minActiveDutyCycles = numpy.zeros(numColumns,\n                                            dtype=realDType)\n    self._boostFactors = numpy.ones(numColumns, dtype=realDType)\n\n    # The inhibition radius determines the size of a column's local \n    # neighborhood. of a column. A cortical column must overcome the overlap \n    # score of columns in his neighborhood in order to become actives. This \n    # radius is updated every learning round. It grows and shrinks with the \n    # average number of connected synapses per column.\n    self._inhibitionRadius = 0\n    self._updateInhibitionRadius()\n    \n    if self._spVerbosity > 0:\n      self.printParameters()\n  \n\n  def compute(self, inputVector, learn, activeArray):\n    \"\"\"\n    This is the primary public method of the SpatialPooler class. This \n    function takes a input vector and outputs the indices of the active columns.\n    If 'learn' is set to True, this method also updates the permanences of the\n    columns.\n\n    Parameters:\n    ----------------------------\n    inputVector:    a numpy array of 0's and 1's thata comprises the input to \n                    the spatial pooler. The array will be treated as a one\n                    dimensional array, therefore the dimensions of the array\n                    do not have to much the exact dimensions specified in the \n                    class constructor. In fact, even a list would suffice. \n                    The number of input bits in the vector must, however, \n                    match the number of bits specified by the call to the \n                    constructor. Therefore there must be a '0' or '1' in the\n                    array for every input bit.\n    learn:          a boolean value indicating whether learning should be \n                    performed. Learning entails updating the  permanence \n                    values of the synapses, and hence modifying the 'state' \n                    of the model. Setting learning to 'off' freezes the SP\n                    and has many uses. For example, you might want to feed in\n                    various inputs and examine the resulting SDR's.\n    activeArray:    an array whose size is equal to the number of columns. \n                    Before the function returns this array will be populated \n                    with 1's at the indices of the active columns, and 0's \n                    everywhere else.  \n    \"\"\"\n    assert (numpy.size(inputVector) == self._numInputs)\n    self._updateBookeepingVars(learn)\n    inputVector = numpy.array(inputVector, dtype=realDType)\n    inputVector.reshape(-1)\n    overlaps = self._calculateOverlap(inputVector)\n\n    # Apply boosting when learning is on\n    if learn:\n      boostedOverlaps = self._boostFactors * overlaps\n    else:\n      boostedOverlaps = overlaps\n\n    # Apply inhibition to determine the winning columns\n    activeColumns = self._inhibitColumns(boostedOverlaps)\n\n    if learn:\n      self._adaptSynapses(inputVector, activeColumns)\n      self._updateDutyCycles(overlaps, activeColumns)\n      self._bumpUpWeakColumns() \n      self._updateBoostFactors()\n      if self._isUpdateRound():\n        self._updateInhibitionRadius()\n        self._updateMinDutyCycles()\n    else:\n      activeColumns = self._stripNeverLearned(activeColumns)\n\n    activeArray.fill(0)\n    if activeColumns.size > 0:\n      activeArray[activeColumns] = 1\n\n\n\n  def _stripNeverLearned(self, activeColumns):\n    \"\"\"Removes the set of columns who have never been active from the set of \n    active columns selected in the inhibition round. Such columns cannot \n    represent learned pattern and are therefore meaningless if only inference\n    is required.\n\n    Parameters:\n    ----------------------------\n    activeColumns:  An array containing the indices of the active columns\n    \"\"\"\n    neverLearned = numpy.where(self._activeDutyCycles == 0)[0]\n    return numpy.array(list(set(activeColumns) - set(neverLearned)))\n\n\n  def _updateMinDutyCycles(self):\n    \"\"\"\n    Updates the minimum duty cycles defining normal activity for a column. A\n    column with activity duty cycle below this minimum threshold is boosted.\n    \"\"\"\n    if self._globalInhibition or self._inhibitionRadius > self._numInputs:\n      self._updateMinDutyCyclesGlobal()\n    else:\n      self._updateMinDutyCyclesLocal()\n\n\n  def _updateMinDutyCyclesGlobal(self):\n    \"\"\"\n    Updates the minimum duty cycles in a global fashion. Sets the minimum duty\n    cycles for the overlap and activation of all columns to be a percent of the \n    maximum in the region, specified by minPctOverlapDutyCycle and \n    minPctActiveDutyCycle respectively. Functionaly it is equivalent to \n    _updateMinDutyCyclesLocal, but this function exploits the globalilty of the\n    compuation to perform it in a straightforward, and more efficient manner.\n    \"\"\"\n    self._minOverlapDutyCycles.fill(\n        self._minPctOverlapDutyCycles * self._overlapDutyCycles.max()\n      )\n    self._minActiveDutyCycles.fill(\n        self._minPctActiveDutyCycles * self._activeDutyCycles.max()\n      )\n\n\n  def _updateMinDutyCyclesLocal(self):\n    \"\"\"\n    Updates the minimum duty cycles. The minimum duty cycles are determined \n    locally. Each column's minimum duty cycles are set to be a percent of the\n    maximum duty cycles in the column's neighborhood. Unlike \n    _updateMinDutyCyclesGlobal, here the values can be quite different for\n    different columns.\n    \"\"\"\n    for i in xrange(self._numColumns):\n      maskNeighbors = numpy.append(i,\n        self._getNeighborsND(i, self._columnDimensions,\n        self._inhibitionRadius)) \n      self._minOverlapDutyCycles[i] = (\n        self._overlapDutyCycles[maskNeighbors].max() * \n        self._minPctOverlapDutyCycles\n      )\n      self._minActiveDutyCycles[i] = (\n        self._activeDutyCycles[maskNeighbors].max() * \n        self._minPctActiveDutyCycles\n      )\n\n\n  def _updateDutyCycles(self, overlaps, activeColumns):\n    \"\"\"\n    Updates the duty cycles for each column. The OVERLAP duty cycle is a moving\n    average of the number of inputs which overlapped with the each column. The\n    ACTIVITY duty cycles is a moving average of the frequency of activation for \n    each column.\n\n    Parameters:\n    ----------------------------\n    overlaps:       an array containing the overlap score for each column. \n                    The overlap score for a column is defined as the number \n                    of synapses in a \"connected state\" (connected synapses) \n                    that are connected to input bits which are turned on.\n    activeColumns:  An array containing the indices of the active columns, \n                    the sprase set of columns which survived inhibition\n    \"\"\"\n    overlapArray = numpy.zeros(self._numColumns)\n    activeArray = numpy.zeros(self._numColumns)\n    overlapArray[overlaps > 0] = 1\n    if activeColumns.size > 0:\n      activeArray[activeColumns] = 1\n\n    period = self._dutyCyclePeriod\n    if (period > self._iterationNum):\n      period = self._iterationNum\n\n    self._overlapDutyCycles = self._updateDutyCyclesHelper(\n                                self._overlapDutyCycles, \n                                overlapArray, \n                                period\n                              )\n\n    self._activeDutyCycles = self._updateDutyCyclesHelper(\n                                self._activeDutyCycles, \n                                activeArray,\n                                period\n                              )\n\n\n\n  def _updateInhibitionRadius(self):\n    \"\"\"\n    Update the inhibition radius. The inhibition radius is a meausre of the\n    square (or hypersquare) of columns that each a column is \"conencted to\"\n    on average. Since columns are are not connected to each other directly, we \n    determine this quantity by first figuring out how many *inputs* a column is \n    connected to, and then multiplying it by the total number of columns that \n    exist for each input. For multiple dimension the aforementioned \n    calculations are averaged over all dimensions of inputs and columns. This \n    value is meaningless if global inhibition is enabled.\n    \"\"\"\n    if self._globalInhibition:\n      self._inhibitionRadius = self._columnDimensions.max()\n      return\n\n    avgConnectedSpan = numpy.average( \n                          [self._avgConnectedSpanForColumnND(i)\n                          for i in xrange(self._numColumns)]\n                        )\n    columnsPerInput = self._avgColumnsPerInput()\n    diameter = avgConnectedSpan * columnsPerInput\n    radius = (diameter - 1) / 2.0\n    radius = max(1.0, radius)\n    self._inhibitionRadius = int(round(radius))\n\n\n  def _avgColumnsPerInput(self):\n    \"\"\"\n    The average number of columns per input, taking into account the topology \n    of the inputs and columns. This value is used to calculate the inhibition \n    radius. This function supports an arbitrary number of dimensions. If the \n    number of column dimensions does not match the number of input dimensions, \n    we treat the missing, or phantom dimensions as 'ones'.\n    \"\"\"\n    #TODO: extend to support different number of dimensions for inputs and \n    # columns\n    numDim = max(self._columnDimensions.size, self._inputDimensions.size)\n    colDim = numpy.ones(numDim)\n    colDim[:self._columnDimensions.size] = self._columnDimensions\n\n    inputDim = numpy.ones(numDim)\n    inputDim[:self._inputDimensions.size] = self._inputDimensions\n\n    columnsPerInput = colDim.astype(realDType) / inputDim\n    return numpy.average(columnsPerInput)\n\n\n  def _avgConnectedSpanForColumn1D(self, index):\n    \"\"\"\n    The range of connected synapses for column. This is used to \n    calculate the inhibition radius. This variation of the function only \n    supports a 1 dimensional column toplogy.\n\n    Parameters:\n    ----------------------------\n    index:          The index identifying a column in the permanence, potential \n                    and connectivity matrices,\n    \"\"\"\n    assert(self._inputDimensions.size == 1)\n    connected = self._connectedSynapses.getRow(index).nonzero()[0]\n    if connected.size == 0:\n      return 0\n    else:\n      return max(connected) - min(connected) + 1\n\n\n  def _avgConnectedSpanForColumn2D(self, index):\n    \"\"\"\n    The range of connectedSynapses per column, averaged for each dimension. \n    This vaule is used to calculate the inhibition radius. This variation of \n    the  function only supports a 2 dimensional column topology.\n\n    Parameters:\n    ----------------------------\n    index:          The index identifying a column in the permanence, potential \n                    and connectivity matrices,\n    \"\"\"\n    assert(self._inputDimensions.size == 2)\n    connected = self._connectedSynapses.getRow(index)\n    (rows, cols) = connected.reshape(self._inputDimensions).nonzero()\n    if  rows.size == 0 and cols.size == 0:\n      return 0\n    rowSpan = rows.max() - rows.min() + 1\n    colSpan = cols.max() - cols.min() + 1\n    return numpy.average([rowSpan, colSpan])\n\n\n  def _avgConnectedSpanForColumnND(self, index):\n    \"\"\"\n    The range of connectedSynapses per column, averaged for each dimension. \n    This vaule is used to calculate the inhibition radius. This variation of \n    the function supports arbitrary column dimensions.\n\n    Parameters:\n    ----------------------------\n    index:          The index identifying a column in the permanence, potential \n                    and connectivity matrices.\n    \"\"\"\n    dimensions = self._inputDimensions\n    bounds = numpy.cumprod(numpy.append([1], dimensions[::-1][:-1]))[::-1]\n    def toCoords(index):\n      return (index / bounds) % dimensions\n\n    connected = self._connectedSynapses.getRow(index).nonzero()[0]\n    if connected.size == 0:\n      return 0\n    maxCoord = numpy.empty(self._inputDimensions.size)\n    minCoord = numpy.empty(self._inputDimensions.size)\n    maxCoord.fill(-1)\n    minCoord.fill(max(self._inputDimensions))\n    for i in connected:\n      maxCoord = numpy.maximum(maxCoord, toCoords(i))\n      minCoord = numpy.minimum(minCoord, toCoords(i))\n    return numpy.average(maxCoord - minCoord + 1)\n\n\n  def _adaptSynapses(self, inputVector, activeColumns):\n    \"\"\"\n    The primary method in charge of learning. Adapts the permanence values of \n    the synapses based on the input vector, and the chosen columns after \n    inhibition round. Permanence values are increased for synapses connected to \n    input bits that are turned on, and decreased for synapses connected to \n    inputs bits that are turned off. Permanence values for shared inputs, which \n    are input bits that are turned on and connected to more than one active \n    column, are decreased in order to discourage multiple columns from learning \n    the same input pattern.\n\n    Parameters:\n    ----------------------------\n    inputVector:    a numpy array of 0's and 1's thata comprises the input to \n                    the spatial pooler. There exists an entry in the array \n                    for every input bit.\n    activeColumns:  an array containing the indices of the columns that \n                    survived inhibition.\n    \"\"\"\n    inputIndices = numpy.where(inputVector > 0)[0]\n    permChanges = numpy.zeros(self._numInputs)\n    permChanges.fill(-1 * self._synPermInactiveDec)\n    permChanges[inputIndices] = self._synPermActiveInc\n    for i in activeColumns:\n      perm = self._permanences.getRow(i)\n      maskPotential = numpy.where(self._potentialPools.getRow(i) > 0)[0]\n      perm[maskPotential] += permChanges[maskPotential]\n      self._updatePermanencesForColumn(perm, i, raisePerm=True)\n\n\n  def _bumpUpWeakColumns(self):\n    \"\"\"\n    This method increases the permanence values of synapses of columns whose \n    activity level has been too low. Such columns are identified by having an\n    overlap duty cycle that drops too much below those of their peers. The\n    permanence values for such columns are increased.\n    \"\"\"\n    weakColumns = numpy.where(self._overlapDutyCycles\n                                < self._minOverlapDutyCycles)[0]   \n    for i in weakColumns:\n      perm = self._permanences.getRow(i).astype(realDType)\n      maskPotential = numpy.where(self._potentialPools.getRow(i) > 0)[0]\n      perm[maskPotential] += self._synPermBelowStimulusInc\n      self._updatePermanencesForColumn(perm, i, raisePerm=False)\n   \n\n  def _raisePermanenceToThreshold(self, perm, mask):\n    \"\"\"\n    This method ensures that each column has enough connections to input bits\n    to allow it to become active. Since a column must have at least \n    'self._stimulusThreshold' overlaps in order to be considered during the \n    inhibition phase, columns without such minimal number of connections, even\n    if all the input bits they are connected to turn on, have no chance of \n    obtaining the minimum threshold. For such columns, the permanence values\n    are increased until the minimum number of connections are formed.\n\n\n    Parameters:\n    ----------------------------\n    perm:           An array of permanence values for a column. The array is \n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    mask:           the indices of the columns whose permanences need to be \n                    raised.\n    \"\"\"\n    numpy.clip(perm, self._synPermMin, self._synPermMax, out=perm)\n    while True:\n      numConnected = numpy.nonzero(perm > self._synPermConnected)[0].size\n      if numConnected >= self._stimulusThreshold:\n        return\n      perm[mask] += self._synPermBelowStimulusInc\n\n\n  def _updatePermanencesForColumn(self, perm, index, raisePerm=True):\n    \"\"\"\n    This method updates the permanence matrix with a column's new permanence\n    values. The column is identified by its index, which reflects the row in\n    the matrix, and the permanence is given in 'dense' form, i.e. a full \n    arrray containing all the zeros as well as the non-zero values. It is in \n    charge of implementing 'clipping' - ensuring that the permanence values are\n    always between 0 and 1 - and 'trimming' - enforcing sparsity by zeroing out\n    all permanence values below '_synPermTrimThreshold'. It also maintains\n    the consistency between 'self._permanences' (the matrix storeing the \n    permanence values), 'self._connectedSynapses', (the matrix storing the bits\n    each column is connected to), and 'self._connectedCounts' (an array storing\n    the number of input bits each column is connected to). Every method wishing \n    to modify the permanence matrix should do so through this method.\n\n    Parameters:\n    ----------------------------\n    perm:           An array of permanence values for a column. The array is \n                    \"dense\", i.e. it contains an entry for each input bit, even\n                    if the permanence value is 0.\n    index:          The index identifying a column in the permanence, potential \n                    and connectivity matrices\n    raisePerm:      a boolean value indicating whether the permanence values \n                    should be raised until a minimum number are synapses are in \n                    a connected state. Should be set to 'false' when a direct \n                    assignment is required.\n    \"\"\"\n\n    maskPotential = numpy.where(self._potentialPools.getRow(index) > 0)[0]\n    if raisePerm:\n      self._raisePermanenceToThreshold(perm, maskPotential)\n    perm[perm < self._synPermTrimThreshold] = 0\n    numpy.clip(perm,self._synPermMin, self._synPermMax, out=perm)\n    newConnected = numpy.where(perm >= self._synPermConnected)[0]\n    self._permanences.setRowFromDense(index, perm)\n    self._connectedSynapses.replaceSparseRow(index, newConnected)\n    self._connectedCounts[index] = newConnected.size\n\n\n  def _initPermConnected(self):\n    \"\"\"\n    Returns a randomly generated permanence value for a synapses that is\n    initialized in a connected state. The basic idea here is to initialize\n    permanence values very close to synPermConnected so that a small number of\n    learning steps could make it disconnected or connected.\n    \n    Note: experimentation was done a long time ago on the best way to initialize\n    permanence values, but the history for this particular scheme has been lost.\n    \"\"\"\n    p =  (self._synPermConnected + self._random.getReal64() * \n      self._synPermActiveInc / 4.0)\n    \n    # Ensure we don't have too much unnecessary precision. A full 64 bits of\n    # precision causes numerical stability issues across platforms and across\n    # implementations\n    p = int(p*100000) / 100000.0\n    return p\n\n\n  def _initPermNonConnected(self):\n    \"\"\"\n    Returns a randomly generated permanence value for a synapses that is to be \n    initialized in a non-connected state.\n    \"\"\"\n    p = self._synPermConnected * self._random.getReal64()\n\n    # Ensure we don't have too much unnecessary precision. A full 64 bits of\n    # precision causes numerical stability issues across platforms and across\n    # implementations\n    p = int(p*100000) / 100000.0\n    return p\n\n  def _initPermanence(self, potential, connectedPct):\n    \"\"\"\n    Initializes the permanences of a column. The method\n    returns a 1-D array the size of the input, where each entry in the\n    array represents the initial permanence value between the input bit\n    at the particular index in the array, and the column represented by\n    the 'index' parameter.\n\n    Parameters:\n    ----------------------------\n    potential:      A numpy array specifying the potential pool of the column.\n                    Permanence values will only be generated for input bits \n                    corresponding to indices for which the mask value is 1.\n    connectedPct:   A value between 0 or 1 specifying the percent of the input \n                    bits that will start off in a connected state.\n\n    \"\"\"\n    # Determine which inputs bits will start out as connected\n    # to the inputs. Initially a subset of the input bits in a \n    # column's potential pool will be connected. This number is\n    # given by the parameter \"connectedPct\"\n    numPotential = numpy.nonzero(potential)[0].size\n    perm = numpy.zeros(self._numInputs)\n    for i in xrange(self._numInputs):\n      if (potential[i] < 1):\n        continue\n\n      if (self._random.getReal64() < connectedPct):\n        perm[i] = self._initPermConnected()\n      else:\n        perm[i] = self._initPermNonConnected()\n\n    # Clip off low values. Since we use a sparse representation\n    # to store the permanence values this helps reduce memory\n    # requirements.\n    perm[perm < self._synPermTrimThreshold] = 0\n\n    return perm\n\n\n  def _mapPotential(self, index, wrapAround=False):\n    \"\"\"\n    Maps a column to its input bits. This method encapsultes the topology of \n    the region. It takes the index of the column as an argument and determines \n    what are the indices of the input vector that are located within the \n    column's potential pool. The return value is a list containing the indices \n    of the input bits. The current implementation of the base class only \n    supports a 1 dimensional topology of columsn with a 1 dimensional topology \n    of inputs. To extend this class to support 2-D topology you will need to \n    override this method. Examples of the expected output of this method:\n    * If the potentialRadius is greater than or equal to the entire input \n      space, (global visibility), then this method returns an array filled with \n      all the indices\n    * If the topology is one dimensional, and the potentialRadius is 5, this \n      method will return an array containing 5 consecutive values centered on \n      the index of the column (wrapping around if necessary).\n    * If the topology is two dimensional (not implemented), and the \n      potentialRadius is 5, the method should return an array containing 25 \n      '1's, where the exact indices are to be determined by the mapping from \n      1-D index to 2-D position.\n\n    Parameters:\n    ----------------------------\n    index:          The index identifying a column in the permanence, potential \n                    and connectivity matrices.\n    wrapAround:     A boolean value indicating that boundaries should be \n                    region boundaries ignored.\n    \"\"\"\n    indices = numpy.array(range(2*self._potentialRadius+1))\n    indices += index\n    indices -= self._potentialRadius\n    if wrapAround:\n      indices %= self._numInputs\n    else:\n      indices = indices[\n        numpy.logical_and(indices >= 0, indices < self._numInputs)]\n    indices = numpy.array(list(set(indices)))\n\n    # Select a subset of the receptive field to serve as the\n    # the potential pool\n    sample = numpy.empty(int(round(\n      indices.size*self._potentialPct)),dtype=uintType)\n    self._random.getUInt32Sample(indices.astype(uintType),sample)\n\n    mask = numpy.zeros(self._numInputs)\n    mask[sample] = 1\n    return mask\n\n\n  @staticmethod\n  def _updateDutyCyclesHelper(dutyCycles, newInput, period):\n    \"\"\"\n    Updates a duty cycle estimate with a new value. This is a helper\n    function that is used to update several duty cycle variables in \n    the Column class, such as: overlapDutyCucle, activeDutyCycle,\n    minPctDutyCycleBeforeInh, minPctDutyCycleAfterInh, etc. returns\n    the updated duty cycle. Duty cycles are updated according to the following \n    formula:\n\n                  (period - 1)*dutyCycle + newValue\n      dutyCycle := ----------------------------------\n                              period\n\n    Parameters:\n    ----------------------------\n    dutyCycles:     An array containing one or more duty cycle values that need\n                    to be updated\n    newInput:       A new numerical value used to update the duty cycle\n    period:         The period of the duty cycle      \n    \"\"\"\n    assert(period >= 1)\n    return (dutyCycles * (period -1.0) + newInput) / period\n\n\n  def _updateBoostFactors(self):\n    \"\"\"\n    Update the boost factors for all columns. The boost factors are used to \n    increase the overlap of inactive columns to improve their chances of\n    becoming active. and hence encourage participation of more columns in the\n    learning process. This is a line defined as: y = mx + b boost =\n    (1-maxBoost)/minDuty * dutyCycle + maxFiringBoost. Intuitively this means\n    that columns that have been active enough have a boost factor of 1, meaning\n    their overlap is not boosted. Columns whose active duty cycle drops too much\n    below that of their neighbors are boosted depending on how infrequently they\n    have been active. The more infrequent, the more they are boosted. The exact\n    boost factor is linearly interpolated between the points (dutyCycle:0,\n    boost:maxFiringBoost) and (dutyCycle:minDuty, boost:1.0). \n\n            boostFactor\n                ^\n    maxBoost _  |\n                |\\\n                | \\\n          1  _  |  \\ _ _ _ _ _ _ _\n                |   \n                +--------------------> activeDutyCycle\n                   |\n            minActiveDutyCycle\n    \"\"\"\n    \n    mask = numpy.where(self._minActiveDutyCycles > 0)[0]\n    self._boostFactors[mask] = ((1 - self._maxBoost) / \n      self._minActiveDutyCycles[mask] * self._activeDutyCycles[mask]\n        ).astype(realDType) + self._maxBoost\n\n    self._boostFactors[self._activeDutyCycles > \n      self._minActiveDutyCycles] = 1.0\n\n\n  def _updateBookeepingVars(self, learn):\n    \"\"\"\n    Updates counter instance variables each round.\n\n    Parameters:\n    ----------------------------\n    learn:          a boolean value indicating whether learning should be \n                    performed. Learning entails updating the  permanence \n                    values of the synapses, and hence modifying the 'state' \n                    of the model. setting learning to 'off' might be useful\n                    for indicating separate training vs. testing sets. \n    \"\"\"\n    self._iterationNum += 1\n    if learn:\n      self._iterationLearnNum += 1\n\n\n  def _calculateOverlap(self, inputVector):\n    \"\"\"\n    This function determines each column's overlap with the current input \n    vector. The overlap of a column is the number of synapses for that column\n    that are connected (permance value is greater than '_synPermConnected') \n    to input bits which are turned on. Overlap values that are lower than\n    the 'stimulusThreshold' are ignored. The implementation takes advantage of \n    the SpraseBinaryMatrix class to perform this calculation efficiently.\n\n    Parameters:\n    ----------------------------\n    inputVector:    a numpy array of 0's and 1's that comprises the input to \n                    the spatial pooler.\n    \"\"\"\n    overlaps = numpy.zeros(self._numColumns).astype(realDType)\n    self._connectedSynapses.rightVecSumAtNZ_fast(inputVector, overlaps)\n    overlaps[overlaps < self._stimulusThreshold] = 0\n    return overlaps\n\n\n  def _calculateOverlapPct(self, overlaps):\n    return overlaps.astype(realDType) / self._connectedCounts\n\n\n  def _inhibitColumns(self, overlaps):\n    \"\"\"\n    Performs inhibition. This method calculates the necessary values needed to\n    actually perform inhibition and then delegates the task of picking the \n    active columns to helper functions.\n\n    Parameters:\n    ----------------------------\n    overlaps:       an array containing the overlap score for each  column. \n                    The overlap score for a column is defined as the number \n                    of synapses in a \"connected state\" (connected synapses) \n                    that are connected to input bits which are turned on.\n    \"\"\"\n    # determine how many columns should be selected in the inhibition phase. \n    # This can be specified by either setting the 'numActiveColumnsPerInhArea' \n    # parameter of the 'localAreaDensity' parameter when initializing the class\n    overlaps = overlaps.copy()\n    if (self._localAreaDensity > 0):\n      density = self._localAreaDensity\n    else:\n      inhibitionArea = ((2*self._inhibitionRadius + 1) \n                                    ** self._columnDimensions.size)\n      inhibitionArea = min(self._numColumns, inhibitionArea)\n      density = float(self._numActiveColumnsPerInhArea) / inhibitionArea\n      density = min(density, 0.5)\n\n    # Add our fixed little bit of random noise to the scores to help break ties.\n    overlaps += self._tieBreaker\n\n    if self._globalInhibition or \\\n      self._inhibitionRadius > max(self._columnDimensions):\n      return self._inhibitColumnsGlobal(overlaps, density)\n    else:\n      return self._inhibitColumnsLocal(overlaps, density)\n\n  \n  def _inhibitColumnsGlobal(self, overlaps, density):\n    \"\"\"\n    Perform global inhibition. Performing global inhibition entails picking the \n    top 'numActive' columns with the highest overlap score in the entire \n    region. At most half of the columns in a local neighborhood are allowed to\n    be active.\n\n    Parameters:\n    ----------------------------\n    overlaps:       an array containing the overlap score for each  column. \n                    The overlap score for a column is defined as the number \n                    of synapses in a \"connected state\" (connected synapses) \n                    that are connected to input bits which are turned on.\n    density:        The fraction of columns to survive inhibition.\n    \"\"\"\n    #calculate num active per inhibition area\n\n    numActive = int(density * self._numColumns)\n    activeColumns = numpy.zeros(self._numColumns)\n    winners = sorted(range(overlaps.size), \n                     key=lambda k: overlaps[k], \n                     reverse=True)[0:numActive]\n    activeColumns[winners] = 1\n    return numpy.where(activeColumns > 0)[0]\n\n\n  def _inhibitColumnsLocal(self, overlaps, density):\n    \"\"\"\n    Performs local inhibition. Local inhibition is performed on a column by \n    column basis. Each column observes the overlaps of its neighbors and is \n    selected if its overlap score is within the top 'numActive' in its local \n    neighborhood. At most half of the columns in a local neighborhood are \n    allowed to be active.\n\n    Parameters:\n    ----------------------------\n    overlaps:       an array containing the overlap score for each  column. \n                    The overlap score for a column is defined as the number \n                    of synapses in a \"connected state\" (connected synapses) \n                    that are connected to input bits which are turned on.\n    density:        The fraction of columns to survive inhibition. This\n                    value is only an intended target. Since the surviving\n                    columns are picked in a local fashion, the exact fraction \n                    of survining columns is likely to vary.\n    \"\"\"\n    activeColumns = numpy.zeros(self._numColumns)\n    addToWinners = max(overlaps)/1000.0   \n    overlaps = numpy.array(overlaps, dtype=realDType)\n    for i in xrange(self._numColumns):\n      maskNeighbors = self._getNeighborsND(i, self._columnDimensions,\n        self._inhibitionRadius)\n      overlapSlice = overlaps[maskNeighbors]\n      numActive = int(0.5 + density * (len(maskNeighbors) + 1))\n      numBigger = numpy.count_nonzero(overlapSlice > overlaps[i])\n      if numBigger < numActive:\n        activeColumns[i] = 1\n        overlaps[i] += addToWinners\n    return numpy.where(activeColumns > 0)[0]\n\n\n  @staticmethod\n  def _getNeighbors1D(columnIndex, dimensions, radius, wrapAround=False):\n    \"\"\"\n    Returns a list of indices corresponding to the neighbors of a given column. \n    In this variation of the method, which only supports a one dimensional \n    column toplogy, a column's neighbors are those neighbors who are 'radius'\n    indices away. This information is needed to perform inhibition. This method\n    is a subset of _getNeighborsND and is only included for illustration \n    purposes, and potentially enhanced performance for spatial pooler \n    implementations that only require a one-dimensional topology.\n\n    Parameters:\n    ----------------------------\n    columnIndex:    The index identifying a column in the permanence, potential \n                    and connectivity matrices.\n    dimensions:     An array containg a dimensions for the column space. A 2x3\n                    grid will be represented by [2,3].\n    radius:         Indicates how far away from a given column are other \n                    columns to be considered its neighbors. In the previous 2x3\n                    example, each column with coordinates:\n                    [2+/-radius, 3+/-radius] is considered a neighbor.\n    wrapAround:     A boolean value indicating whether to consider columns at \n                    the border of a dimensions to be adjacent to columns at the \n                    other end of the dimension. For example, if the columns are\n                    layed out in one deimnsion, columns 1 and 10 will be \n                    considered adjacent if wrapAround is set to true:\n                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \"\"\"\n    assert(dimensions.size == 1)\n    ncols = dimensions[0]\n\n    if wrapAround:\n      neighbors = numpy.array(\n        range(columnIndex-radius,columnIndex+radius+1)) % ncols\n    else:\n      neighbors = numpy.array(\n        range(columnIndex-radius,columnIndex+radius+1))\n      neighbors = neighbors[\n        numpy.logical_and(neighbors >= 0, neighbors < ncols)]\n\n    neighbors = list(set(neighbors) - set([columnIndex])) \n    assert(neighbors)\n    return neighbors\n\n\n  @staticmethod\n  def _getNeighbors2D(columnIndex, dimensions, radius, wrapAround=False):\n    \"\"\"\n    Returns a list of indices corresponding to the neighbors of a given column.\n    Since the permanence values are stored in such a way that information about \n    toplogy is lost, this method allows for reconstructing the toplogy of the \n    inputs, which are flattened to one array. Given a column's index, its \n    neighbors are defined as those columns that are 'radius' indices away from \n    it in each dimension. The method returns a list of the flat indices of \n    these columns. This method is a subset of _getNeighborsND and is only \n    included for illustration purposes, and potentially enhanced performance \n    for spatial pooler implementations that only require a two-dimensional \n    topology.\n\n    Parameters:\n    ----------------------------\n    columnIndex:    The index identifying a column in the permanence, potential \n                    and connectivity matrices.\n    dimensions:     An array containg a dimensions for the column space. A 2x3\n                    grid will be represented by [2,3].\n    radius:         Indicates how far away from a given column are other \n                    columns to be considered its neighbors. In the previous 2x3\n                    example, each column with coordinates:\n                    [2+/-radius, 3+/-radius] is considered a neighbor.\n    wrapAround:     A boolean value indicating whether to consider columns at \n                    the border of a dimensions to be adjacent to columns at the \n                    other end of the dimension. For example, if the columns are\n                    layed out in one deimnsion, columns 1 and 10 will be \n                    considered adjacent if wrapAround is set to true:\n                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \"\"\"\n    assert(dimensions.size == 2)\n    nrows = dimensions[0]\n    ncols = dimensions[1]\n\n    toRow = lambda index: index / ncols\n    toCol = lambda index: index % ncols\n    toIndex = lambda row, col: row * ncols + col\n\n    row = toRow(columnIndex)\n    col = toCol(columnIndex)\n\n    if wrapAround:\n      colRange = numpy.array(range(col-radius, col+radius+1)) % ncols\n      rowRange = numpy.array(range(row-radius, row+radius+1)) % nrows\n    else:\n      colRange = numpy.array(range(col-radius, col+radius+1))\n      colRange = colRange[\n        numpy.logical_and(colRange >= 0, colRange < ncols)]\n      rowRange = numpy.array(range(row-radius, row+radius+1))\n      rowRange = rowRange[\n        numpy.logical_and(rowRange >= 0, rowRange < nrows)]\n\n    neighbors = [toIndex(r, c) for (r, c) in \n      itertools.product(rowRange, colRange)]\n    neighbors = list(set(neighbors) - set([columnIndex]))\n    assert(neighbors)\n    return neighbors\n     \n\n  @staticmethod\n  def _getNeighborsND(columnIndex, dimensions, radius, wrapAround=False):\n    \"\"\"\n    Similar to _getNeighbors1D and _getNeighbors2D, this function Returns a \n    list of indices corresponding to the neighbors of a given column. Since the \n    permanence values are stored in such a way that information about toplogy \n    is lost. This method allows for reconstructing the toplogy of the inputs, \n    which are flattened to one array. Given a column's index, its neighbors are \n    defined as those columns that are 'radius' indices away from it in each \n    dimension. The method returns a list of the flat indices of these columns. \n    Parameters:\n    ----------------------------\n    columnIndex:    The index identifying a column in the permanence, potential \n                    and connectivity matrices.\n    dimensions:     An array containg a dimensions for the column space. A 2x3\n                    grid will be represented by [2,3].\n    radius:         Indicates how far away from a given column are other \n                    columns to be considered its neighbors. In the previous 2x3\n                    example, each column with coordinates:\n                    [2+/-radius, 3+/-radius] is considered a neighbor.\n    wrapAround:     A boolean value indicating whether to consider columns at \n                    the border of a dimensions to be adjacent to columns at the \n                    other end of the dimension. For example, if the columns are\n                    layed out in one deimnsion, columns 1 and 10 will be \n                    considered adjacent if wrapAround is set to true:\n                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \"\"\"\n    assert(dimensions.size > 0)\n    bounds = numpy.cumprod(numpy.append([1], dimensions[::-1][:-1]))[::-1]\n\n    def toCoords(index):\n      return (index / bounds) % dimensions\n\n    def toIndex(coords):\n      return numpy.dot(bounds, coords)\n\n    columnCoords = toCoords(columnIndex)\n    rangeND = []\n    for i in xrange(dimensions.size):\n      if wrapAround:\n        curRange = numpy.array(range(columnCoords[i]-radius, \n                                     columnCoords[i]+radius+1)) % dimensions[i]\n      else:\n        curRange = numpy.array(range(columnCoords[i]-radius,\n                                     columnCoords[i]+radius+1))\n        curRange = curRange[\n          numpy.logical_and(curRange >= 0, curRange < dimensions[i])]\n\n      rangeND.append(curRange)\n\n    neighbors = [toIndex(numpy.array(coord)) for coord in \n      itertools.product(*rangeND)]\n    neighbors = list(set(neighbors) - set([columnIndex]))\n    assert(neighbors)\n    return neighbors\n\n\n  def _isUpdateRound(self):\n    \"\"\"\n    returns true if the enough rounds have passed to warrant updates of \n    duty cycles\n    \"\"\"\n    return (self._iterationNum % self._updatePeriod) == 0\n\n\n  def _seed(self, seed=-1):\n    \"\"\"\n    Initialize the random seed\n    \"\"\"\n    if seed != -1:\n      self._random = NupicRandom(seed)\n    else:\n      self._random = NupicRandom()\n\n\n  def printParameters(self):\n    \"\"\"\n    Useful for debugging.\n    \"\"\"\n    print \"------------PY  SpatialPooler Parameters ------------------\"\n    print \"numInputs                  = \", self.getNumInputs()\n    print \"numColumns                 = \", self.getNumColumns()\n    print \"columnDimensions           = \", self._columnDimensions\n    print \"numActiveColumnsPerInhArea = \", self.getNumActiveColumnsPerInhArea()\n    print \"potentialPct               = \", self.getPotentialPct()\n    print \"globalInhibition           = \", self.getGlobalInhibition()\n    print \"localAreaDensity           = \", self.getLocalAreaDensity()\n    print \"stimulusThreshold          = \", self.getStimulusThreshold()\n    print \"synPermActiveInc           = \", self.getSynPermActiveInc()\n    print \"synPermInactiveDec         = \", self.getSynPermInactiveDec()\n    print \"synPermConnected           = \", self.getSynPermConnected()\n    print \"minPctOverlapDutyCycle     = \", self.getMinPctOverlapDutyCycles()\n    print \"minPctActiveDutyCycle      = \", self.getMinPctActiveDutyCycles()\n    print \"dutyCyclePeriod            = \", self.getDutyCyclePeriod()\n    print \"maxBoost                   = \", self.getMaxBoost()\n    print \"spVerbosity                = \", self.getSpVerbosity()\n    print \"version                    = \", self._version\n\n", "idx": 44, "id": 14019, "msg": "", "proj": "numenta-nupic", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -154,6 +154,8 @@ func (manager *connectionManager) startConnection(consumerID identity.Identity,\n \t\treturn err\n \t}\n \n+\tcancel = append(cancel, func() { session.RequestSessionDestroy(dialog, sessionID) })\n+\n \t// set the session info for future use\n \tmanager.sessionInfo = SessionInfo{\n \t\tSessionID:  sessionID,", "y": 1, "oldf": "/*\n * Copyright (C) 2017 The \"MysteriumNetwork/node\" Authors.\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\npackage connection\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"sync\"\n\n\tlog \"github.com/cihub/seelog\"\n\t\"github.com/mysteriumnetwork/node/communication\"\n\t\"github.com/mysteriumnetwork/node/consumer\"\n\t\"github.com/mysteriumnetwork/node/firewall\"\n\t\"github.com/mysteriumnetwork/node/identity\"\n\t\"github.com/mysteriumnetwork/node/service_discovery/dto\"\n\t\"github.com/mysteriumnetwork/node/session\"\n)\n\nconst managerLogPrefix = \"[connection-manager] \"\n\nvar (\n\t// ErrNoConnection error indicates that action applied to manager expects active connection (i.e. disconnect)\n\tErrNoConnection = errors.New(\"no connection exists\")\n\t// ErrAlreadyExists error indicates that action applied to manager expects no active connection (i.e. connect)\n\tErrAlreadyExists = errors.New(\"connection already exists\")\n\t// ErrConnectionCancelled indicates that connection in progress was cancelled by request of api user\n\tErrConnectionCancelled = errors.New(\"connection was cancelled\")\n\t// ErrConnectionFailed indicates that Connect method didn't reach \"Connected\" phase due to connection error\n\tErrConnectionFailed = errors.New(\"connection has failed\")\n\t// ErrUnsupportedServiceType indicates that target proposal contains unsupported service type\n\tErrUnsupportedServiceType = errors.New(\"unsupported service type in proposal\")\n)\n\n// Creator creates new connection by given options and uses state channel to report state changes\n// Given options:\n//  - session,\n//  - consumer identity\n//  - service provider identity\n//  - service proposal\ntype Creator func(ConnectOptions, StateChannel, StatisticsChannel) (Connection, error)\n\n// SessionInfo contains all the relevant info of the current session\ntype SessionInfo struct {\n\tSessionID  session.ID\n\tConsumerID identity.Identity\n\tProposal   dto.ServiceProposal\n}\n\n// Publisher is responsible for publishing given events\ntype Publisher interface {\n\tPublish(topic string, args ...interface{})\n}\n\ntype connectionManager struct {\n\t//these are passed on creation\n\tnewDialog        DialogCreator\n\tnewPromiseIssuer PromiseIssuerCreator\n\tnewConnection    Creator\n\teventPublisher   Publisher\n\n\t//these are populated by Connect at runtime\n\tctx             context.Context\n\tmutex           sync.RWMutex\n\tstatus          ConnectionStatus\n\tsessionInfo     SessionInfo\n\tcleanConnection func()\n}\n\n// NewManager creates connection manager with given dependencies\nfunc NewManager(\n\tdialogCreator DialogCreator,\n\tpromiseIssuerCreator PromiseIssuerCreator,\n\tconnectionCreator Creator,\n\teventPublisher Publisher,\n) *connectionManager {\n\treturn &connectionManager{\n\t\tnewDialog:        dialogCreator,\n\t\tnewPromiseIssuer: promiseIssuerCreator,\n\t\tnewConnection:    connectionCreator,\n\t\tstatus:           statusNotConnected(),\n\t\tcleanConnection:  warnOnClean,\n\t\teventPublisher:   eventPublisher,\n\t}\n}\n\nfunc (manager *connectionManager) Connect(consumerID identity.Identity, proposal dto.ServiceProposal, params ConnectParams) (err error) {\n\tif manager.status.State != NotConnected {\n\t\treturn ErrAlreadyExists\n\t}\n\n\tmanager.mutex.Lock()\n\tmanager.ctx, manager.cleanConnection = context.WithCancel(context.Background())\n\tmanager.status = statusConnecting()\n\tmanager.mutex.Unlock()\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tmanager.mutex.Lock()\n\t\t\tmanager.status = statusNotConnected()\n\t\t\tmanager.mutex.Unlock()\n\t\t}\n\t}()\n\n\terr = manager.startConnection(consumerID, proposal, params)\n\tif err == context.Canceled {\n\t\treturn ErrConnectionCancelled\n\t}\n\treturn err\n}\n\nfunc (manager *connectionManager) startConnection(consumerID identity.Identity, proposal dto.ServiceProposal, params ConnectParams) (err error) {\n\tmanager.mutex.Lock()\n\tcancelCtx := manager.cleanConnection\n\tmanager.mutex.Unlock()\n\n\tvar cancel []func()\n\tdefer func() {\n\t\tmanager.cleanConnection = func() {\n\t\t\tmanager.status = statusDisconnecting()\n\t\t\tcancelCtx()\n\t\t\tfor _, f := range cancel {\n\t\t\t\tf()\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tlog.Info(managerLogPrefix, \"Cancelling connection initiation\")\n\t\t\tdefer manager.cleanConnection()\n\t\t}\n\t}()\n\n\tproviderID := identity.FromAddress(proposal.ProviderID)\n\tdialog, err := manager.newDialog(consumerID, providerID, proposal.ProviderContacts[0])\n\tif err != nil {\n\t\treturn err\n\t}\n\tcancel = append(cancel, func() { dialog.Close() })\n\n\tsessionID, sessionConfig, err := session.RequestSessionCreate(dialog, proposal.ID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// set the session info for future use\n\tmanager.sessionInfo = SessionInfo{\n\t\tSessionID:  sessionID,\n\t\tConsumerID: consumerID,\n\t\tProposal:   proposal,\n\t}\n\n\tpromiseIssuer := manager.newPromiseIssuer(consumerID, dialog)\n\terr = promiseIssuer.Start(proposal)\n\tif err != nil {\n\t\treturn err\n\t}\n\tcancel = append(cancel, func() { promiseIssuer.Stop() })\n\n\tstateChannel := make(chan State, 10)\n\tstatisticsChannel := make(chan consumer.SessionStatistics, 10)\n\n\tconnectOptions := ConnectOptions{\n\t\tSessionID:     sessionID,\n\t\tSessionConfig: sessionConfig,\n\t\tConsumerID:    consumerID,\n\t\tProviderID:    providerID,\n\t\tProposal:      proposal,\n\t}\n\n\tconnection, err := manager.newConnection(connectOptions, stateChannel, statisticsChannel)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err = connection.Start(); err != nil {\n\t\treturn err\n\t}\n\tcancel = append(cancel, connection.Stop)\n\n\terr = manager.waitForConnectedState(stateChannel, sessionID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif !params.DisableKillSwitch {\n\t\t// TODO: Implement fw based kill switch for respective OS\n\t\t// we may need to wait for tun device to bet setup\n\t\tfirewall.NewKillSwitch().Enable()\n\t}\n\n\tgo manager.consumeStats(statisticsChannel)\n\tgo manager.consumeConnectionStates(stateChannel)\n\tgo connectionWaiter(connection, dialog, promiseIssuer)\n\treturn nil\n}\n\nfunc (manager *connectionManager) Status() ConnectionStatus {\n\tmanager.mutex.RLock()\n\tdefer manager.mutex.RUnlock()\n\n\treturn manager.status\n}\n\nfunc (manager *connectionManager) Disconnect() error {\n\tmanager.mutex.RLock()\n\tdefer manager.mutex.RUnlock()\n\n\tif manager.status.State == NotConnected {\n\t\treturn ErrNoConnection\n\t}\n\tmanager.cleanConnection()\n\treturn nil\n}\n\nfunc warnOnClean() {\n\tlog.Warn(managerLogPrefix, \"Trying to close when there is nothing to close. Possible bug or race condition\")\n}\n\nfunc connectionWaiter(connection Connection, dialog communication.Dialog, promiseIssuer PromiseIssuer) {\n\terr := connection.Wait()\n\tif err != nil {\n\t\tlog.Warn(managerLogPrefix, \"Connection exited with error: \", err)\n\t} else {\n\t\tlog.Info(managerLogPrefix, \"Connection exited\")\n\t}\n\n\tpromiseIssuer.Stop()\n\tdialog.Close()\n}\n\nfunc (manager *connectionManager) waitForConnectedState(stateChannel <-chan State, sessionID session.ID) error {\n\tfor {\n\t\tselect {\n\t\tcase state, more := <-stateChannel:\n\t\t\tif !more {\n\t\t\t\treturn ErrConnectionFailed\n\t\t\t}\n\n\t\t\tswitch state {\n\t\t\tcase Connected:\n\t\t\t\tmanager.onStateChanged(state)\n\t\t\t\treturn nil\n\t\t\tdefault:\n\t\t\t\tmanager.onStateChanged(state)\n\t\t\t}\n\t\tcase <-manager.ctx.Done():\n\t\t\treturn manager.ctx.Err()\n\t\t}\n\t}\n}\n\nfunc (manager *connectionManager) consumeConnectionStates(stateChannel <-chan State) {\n\tfor state := range stateChannel {\n\t\tmanager.onStateChanged(state)\n\t}\n\n\tmanager.mutex.Lock()\n\tdefer manager.mutex.Unlock()\n\n\tmanager.status = statusNotConnected()\n\tlog.Debug(managerLogPrefix, \"State updater stopCalled\")\n}\n\nfunc (manager *connectionManager) consumeStats(statisticsChannel <-chan consumer.SessionStatistics) {\n\tfor stats := range statisticsChannel {\n\t\tmanager.eventPublisher.Publish(StatisticsEventTopic, stats)\n\t}\n}\n\nfunc (manager *connectionManager) onStateChanged(state State) {\n\tmanager.mutex.Lock()\n\tdefer manager.mutex.Unlock()\n\n\tmanager.eventPublisher.Publish(StateEventTopic, StateEvent{\n\t\tState:       state,\n\t\tSessionInfo: manager.sessionInfo,\n\t})\n\n\tswitch state {\n\tcase Connected:\n\t\tmanager.status = statusConnected(manager.sessionInfo.SessionID)\n\tcase Reconnecting:\n\t\tmanager.status = statusReconnecting()\n\t}\n}\n", "idx": 1, "id": 13002, "msg": "Since we're not handling the errors here, we'd either need to add logging to the function itself, or handle errors here properly. Things might start failing and we would not know about it.", "proj": "mysteriumnetwork-node", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1957,7 +1957,7 @@ def concat(objs, axis=0, join=\"outer\", ignore_index=False, sort=False):\n             '\"{name}\"'.format(name=type(objs).__name__)\n         )\n \n-    if len(objs) == 0:\n+    if len(cast(Sized, objs)) == 0:\n         raise ValueError(\"No objects to concatenate\")\n     objs = list(filter(lambda obj: obj is not None, objs))\n     if len(objs) == 0:", "y": 0, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nWrappers around spark that correspond to common pandas functions.\n\"\"\"\nfrom typing import Any, Optional, Union, List, Tuple\nfrom collections import OrderedDict\nfrom collections.abc import Iterable\nfrom distutils.version import LooseVersion\nfrom functools import reduce\nfrom io import BytesIO\nimport json\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype, is_list_like\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyspark\nfrom pyspark import sql as spark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.types import (\n    ByteType,\n    ShortType,\n    IntegerType,\n    LongType,\n    FloatType,\n    DoubleType,\n    BooleanType,\n    TimestampType,\n    DecimalType,\n    StringType,\n    DateType,\n    StructType,\n)\n\nfrom databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.\nfrom databricks.koalas.base import IndexOpsMixin\nfrom databricks.koalas.utils import (\n    align_diff_frames,\n    default_session,\n    is_name_like_tuple,\n    name_like_string,\n    same_anchor,\n    scol_for,\n    validate_axis,\n)\nfrom databricks.koalas.frame import DataFrame, _reduce_spark_multi\nfrom databricks.koalas.internal import (\n    InternalFrame,\n    DEFAULT_SERIES_NAME,\n    SPARK_INDEX_NAME_FORMAT,\n)\nfrom databricks.koalas.series import Series, first_series\nfrom databricks.koalas.indexes import Index\n\n\n__all__ = [\n    \"from_pandas\",\n    \"range\",\n    \"read_csv\",\n    \"read_delta\",\n    \"read_table\",\n    \"read_spark_io\",\n    \"read_parquet\",\n    \"read_clipboard\",\n    \"read_excel\",\n    \"read_html\",\n    \"to_datetime\",\n    \"get_dummies\",\n    \"concat\",\n    \"melt\",\n    \"isna\",\n    \"isnull\",\n    \"notna\",\n    \"notnull\",\n    \"read_sql_table\",\n    \"read_sql_query\",\n    \"read_sql\",\n    \"read_json\",\n    \"merge\",\n    \"to_numeric\",\n    \"broadcast\",\n]\n\n\ndef from_pandas(\n    pobj: Union[\"pd.DataFrame\", \"pd.Series\", \"pd.Index\"]\n) -> Union[\"Series\", \"DataFrame\", \"Index\"]:\n    \"\"\"Create a Koalas DataFrame, Series or Index from a pandas DataFrame, Series or Index.\n\n    This is similar to Spark's `SparkSession.createDataFrame()` with pandas DataFrame,\n    but this also works with pandas Series and picks the index.\n\n    Parameters\n    ----------\n    pobj : pandas.DataFrame or pandas.Series\n        pandas DataFrame or Series to read.\n\n    Returns\n    -------\n    Series or DataFrame\n        If a pandas Series is passed in, this function returns a Koalas Series.\n        If a pandas DataFrame is passed in, this function returns a Koalas DataFrame.\n    \"\"\"\n    if isinstance(pobj, pd.Series):\n        return Series(pobj)\n    elif isinstance(pobj, pd.DataFrame):\n        return DataFrame(pobj)\n    elif isinstance(pobj, pd.Index):\n        return DataFrame(pd.DataFrame(index=pobj)).index\n    else:\n        raise ValueError(\"Unknown data type: {}\".format(type(pobj).__name__))\n\n\n_range = range  # built-in range\n\n\ndef range(\n    start: int, end: Optional[int] = None, step: int = 1, num_partitions: Optional[int] = None\n) -> DataFrame:\n    \"\"\"\n    Create a DataFrame with some range of numbers.\n\n    The resulting DataFrame has a single int64 column named `id`, containing elements in a range\n    from ``start`` to ``end`` (exclusive) with step value ``step``. If only the first parameter\n    (i.e. start) is specified, we treat it as the end value with the start value being 0.\n\n    This is similar to the range function in SparkSession and is used primarily for testing.\n\n    Parameters\n    ----------\n    start : int\n        the start value (inclusive)\n    end : int, optional\n        the end value (exclusive)\n    step : int, optional, default 1\n        the incremental step\n    num_partitions : int, optional\n        the number of partitions of the DataFrame\n\n    Returns\n    -------\n    DataFrame\n\n    Examples\n    --------\n    When the first parameter is specified, we generate a range of values up till that number.\n\n    >>> ks.range(5)\n       id\n    0   0\n    1   1\n    2   2\n    3   3\n    4   4\n\n    When start, end, and step are specified:\n\n    >>> ks.range(start = 100, end = 200, step = 20)\n        id\n    0  100\n    1  120\n    2  140\n    3  160\n    4  180\n    \"\"\"\n    sdf = default_session().range(start=start, end=end, step=step, numPartitions=num_partitions)\n    return DataFrame(sdf)\n\n\ndef read_csv(\n    path,\n    sep=\",\",\n    header=\"infer\",\n    names=None,\n    index_col=None,\n    usecols=None,\n    squeeze=False,\n    mangle_dupe_cols=True,\n    dtype=None,\n    nrows=None,\n    parse_dates=False,\n    quotechar=None,\n    escapechar=None,\n    comment=None,\n    **options\n):\n    \"\"\"Read CSV (comma-separated) file into DataFrame.\n\n    Parameters\n    ----------\n    path : str\n        The path string storing the CSV file to be read.\n    sep : str, default \u2018,\u2019\n        Delimiter to use. Must be a single character.\n    header : int, list of int, default \u2018infer\u2019\n        Whether to to use as the column names, and the start of the data.\n        Default behavior is to infer the column names: if no names are passed\n        the behavior is identical to `header=0` and column names are inferred from\n        the first line of the file, if column names are passed explicitly then\n        the behavior is identical to `header=None`. Explicitly pass `header=0` to be\n        able to replace existing names\n    names : str or array-like, optional\n        List of column names to use. If file contains no header row, then you should\n        explicitly pass `header=None`. Duplicates in this list will cause an error to be issued.\n        If a string is given, it should be a DDL-formatted string in Spark SQL, which is\n        preferred to avoid schema inference for better performance.\n    index_col: str or list of str, optional, default: None\n        Index column of table in Spark.\n    usecols : list-like or callable, optional\n        Return a subset of the columns. If list-like, all elements must either be\n        positional (i.e. integer indices into the document columns) or strings that\n        correspond to column names provided either by the user in names or inferred\n        from the document header row(s).\n        If callable, the callable function will be evaluated against the column names,\n        returning names where the callable function evaluates to `True`.\n    squeeze : bool, default False\n        If the parsed data only contains one column then return a Series.\n    mangle_dupe_cols : bool, default True\n        Duplicate columns will be specified as 'X0', 'X1', ... 'XN', rather\n        than 'X' ... 'X'. Passing in False will cause data to be overwritten if\n        there are duplicate names in the columns.\n        Currently only `True` is allowed.\n    dtype : Type name or dict of column -> type, default None\n        Data type for data or columns. E.g. {\u2018a\u2019: np.float64, \u2018b\u2019: np.int32} Use str or object\n        together with suitable na_values settings to preserve and not interpret dtype.\n    nrows : int, default None\n        Number of rows to read from the CSV file.\n    parse_dates : boolean or list of ints or names or list of lists or dict, default `False`.\n        Currently only `False` is allowed.\n    quotechar : str (length 1), optional\n        The character used to denote the start and end of a quoted item. Quoted items can include\n        the delimiter and it will be ignored.\n    escapechar : str (length 1), default None\n        One-character string used to escape delimiter\n    comment: str, optional\n        Indicates the line should not be parsed.\n    options : dict\n        All other options passed directly into Spark's data source.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n\n    Examples\n    --------\n    >>> ks.read_csv('data.csv')  # doctest: +SKIP\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    if mangle_dupe_cols is not True:\n        raise ValueError(\"mangle_dupe_cols can only be `True`: %s\" % mangle_dupe_cols)\n    if parse_dates is not False:\n        raise ValueError(\"parse_dates can only be `False`: %s\" % parse_dates)\n\n    if usecols is not None and not callable(usecols):\n        usecols = list(usecols)\n\n    if usecols is None or callable(usecols) or len(usecols) > 0:\n        reader = default_session().read\n        reader.option(\"inferSchema\", True)\n        reader.option(\"sep\", sep)\n\n        if header == \"infer\":\n            header = 0 if names is None else None\n        if header == 0:\n            reader.option(\"header\", True)\n        elif header is None:\n            reader.option(\"header\", False)\n        else:\n            raise ValueError(\"Unknown header argument {}\".format(header))\n\n        if quotechar is not None:\n            reader.option(\"quote\", quotechar)\n        if escapechar is not None:\n            reader.option(\"escape\", escapechar)\n\n        if comment is not None:\n            if not isinstance(comment, str) or len(comment) != 1:\n                raise ValueError(\"Only length-1 comment characters supported\")\n            reader.option(\"comment\", comment)\n\n        reader.options(**options)\n\n        if isinstance(names, str):\n            sdf = reader.schema(names).csv(path)\n            column_labels = OrderedDict((col, col) for col in sdf.columns)\n        else:\n            sdf = reader.csv(path)\n            if is_list_like(names):\n                names = list(names)\n                if len(set(names)) != len(names):\n                    raise ValueError(\"Found non-unique column index\")\n                if len(names) != len(sdf.columns):\n                    raise ValueError(\n                        \"The number of names [%s] does not match the number \"\n                        \"of columns [%d]. Try names by a Spark SQL DDL-formatted \"\n                        \"string.\" % (len(sdf.schema), len(names))\n                    )\n                column_labels = OrderedDict(zip(names, sdf.columns))\n            elif header is None:\n                column_labels = OrderedDict(enumerate(sdf.columns))\n            else:\n                column_labels = OrderedDict((col, col) for col in sdf.columns)\n\n        if usecols is not None:\n            if callable(usecols):\n                column_labels = OrderedDict(\n                    (label, col) for label, col in column_labels.items() if usecols(label)\n                )\n                missing = []\n            elif all(isinstance(col, int) for col in usecols):\n                new_column_labels = OrderedDict(\n                    (label, col)\n                    for i, (label, col) in enumerate(column_labels.items())\n                    if i in usecols\n                )\n                missing = [\n                    col\n                    for col in usecols\n                    if col >= len(column_labels)\n                    or list(column_labels)[col] not in new_column_labels\n                ]\n                column_labels = new_column_labels\n            elif all(isinstance(col, str) for col in usecols):\n                new_column_labels = OrderedDict(\n                    (label, col) for label, col in column_labels.items() if label in usecols\n                )\n                missing = [col for col in usecols if col not in new_column_labels]\n                column_labels = new_column_labels\n            else:\n                raise ValueError(\n                    \"'usecols' must either be list-like of all strings, \"\n                    \"all unicode, all integers or a callable.\"\n                )\n            if len(missing) > 0:\n                raise ValueError(\n                    \"Usecols do not match columns, columns expected but not \" \"found: %s\" % missing\n                )\n\n            if len(column_labels) > 0:\n                sdf = sdf.select([scol_for(sdf, col) for col in column_labels.values()])\n            else:\n                sdf = default_session().createDataFrame([], schema=StructType())\n    else:\n        sdf = default_session().createDataFrame([], schema=StructType())\n        column_labels = OrderedDict()\n\n    if nrows is not None:\n        sdf = sdf.limit(nrows)\n\n    if index_col is not None:\n        if isinstance(index_col, (str, int)):\n            index_col = [index_col]\n        for col in index_col:\n            if col not in column_labels:\n                raise KeyError(col)\n        index_spark_column_names = [column_labels[col] for col in index_col]\n        index_names = [(col,) for col in index_col]\n        column_labels = OrderedDict(\n            (label, col) for label, col in column_labels.items() if label not in index_col\n        )\n    else:\n        index_spark_column_names = None\n        index_names = None\n\n    kdf = DataFrame(\n        InternalFrame(\n            spark_frame=sdf,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n            column_labels=[\n                label if is_name_like_tuple(label) else (label,) for label in column_labels\n            ],\n            data_spark_columns=[scol_for(sdf, col) for col in column_labels.values()],\n        )\n    )\n\n    if dtype is not None:\n        if isinstance(dtype, dict):\n            for col, tpe in dtype.items():\n                kdf[col] = kdf[col].astype(tpe)\n        else:\n            for col in kdf.columns:\n                kdf[col] = kdf[col].astype(dtype)\n\n    if squeeze and len(kdf.columns) == 1:\n        return first_series(kdf)\n    return kdf\n\n\ndef read_json(path: str, index_col: Optional[Union[str, List[str]]] = None, **options):\n    \"\"\"\n    Convert a JSON string to DataFrame.\n\n    Parameters\n    ----------\n    path : string\n        File path\n    index_col : str or list of str, optional, default: None\n        Index column of table in Spark.\n    options : dict\n        All other options passed directly into Spark's data source.\n\n    Examples\n    --------\n    >>> df = ks.DataFrame([['a', 'b'], ['c', 'd']],\n    ...                   columns=['col 1', 'col 2'])\n\n    >>> df.to_json(path=r'%s/read_json/foo.json' % path, num_files=1)\n    >>> ks.read_json(\n    ...     path=r'%s/read_json/foo.json' % path\n    ... ).sort_values(by=\"col 1\")\n      col 1 col 2\n    0     a     b\n    1     c     d\n\n    >>> df.to_json(path=r'%s/read_json/foo.json' % path, num_files=1, lineSep='___')\n    >>> ks.read_json(\n    ...     path=r'%s/read_json/foo.json' % path, lineSep='___'\n    ... ).sort_values(by=\"col 1\")\n      col 1 col 2\n    0     a     b\n    1     c     d\n\n    You can preserve the index in the roundtrip as below.\n\n    >>> df.to_json(path=r'%s/read_json/bar.json' % path, num_files=1, index_col=\"index\")\n    >>> ks.read_json(\n    ...     path=r'%s/read_json/bar.json' % path, index_col=\"index\"\n    ... ).sort_values(by=\"col 1\")  # doctest: +NORMALIZE_WHITESPACE\n          col 1 col 2\n    index\n    0         a     b\n    1         c     d\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    return read_spark_io(path, format=\"json\", index_col=index_col, **options)\n\n\ndef read_delta(\n    path: str,\n    version: Optional[str] = None,\n    timestamp: Optional[str] = None,\n    index_col: Optional[Union[str, List[str]]] = None,\n    **options\n) -> DataFrame:\n    \"\"\"\n    Read a Delta Lake table on some file system and return a DataFrame.\n\n    If the Delta Lake table is already stored in the catalog (aka the metastore), use 'read_table'.\n\n    Parameters\n    ----------\n    path : string\n        Path to the Delta Lake table.\n    version : string, optional\n        Specifies the table version (based on Delta's internal transaction version) to read from,\n        using Delta's time travel feature. This sets Delta's 'versionAsOf' option.\n    timestamp : string, optional\n        Specifies the table version (based on timestamp) to read from,\n        using Delta's time travel feature. This must be a valid date or timestamp string in Spark,\n        and sets Delta's 'timestampAsOf' option.\n    index_col : str or list of str, optional, default: None\n        Index column of table in Spark.\n    options\n        Additional options that can be passed onto Delta.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    DataFrame.to_delta\n    read_table\n    read_spark_io\n    read_parquet\n\n    Examples\n    --------\n    >>> ks.range(1).to_delta('%s/read_delta/foo' % path)\n    >>> ks.read_delta('%s/read_delta/foo' % path)\n       id\n    0   0\n\n    >>> ks.range(10, 15, num_partitions=1).to_delta('%s/read_delta/foo' % path, mode='overwrite')\n    >>> ks.read_delta('%s/read_delta/foo' % path)\n       id\n    0  10\n    1  11\n    2  12\n    3  13\n    4  14\n\n    >>> ks.read_delta('%s/read_delta/foo' % path, version=0)\n       id\n    0   0\n\n    You can preserve the index in the roundtrip as below.\n\n    >>> ks.range(10, 15, num_partitions=1).to_delta(\n    ...     '%s/read_delta/bar' % path, index_col=\"index\")\n    >>> ks.read_delta('%s/read_delta/bar' % path, index_col=\"index\")\n    ... # doctest: +NORMALIZE_WHITESPACE\n           id\n    index\n    0      10\n    1      11\n    2      12\n    3      13\n    4      14\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    if version is not None:\n        options[\"versionAsOf\"] = version\n    if timestamp is not None:\n        options[\"timestampAsOf\"] = timestamp\n    return read_spark_io(path, format=\"delta\", index_col=index_col, **options)\n\n\ndef read_table(name: str, index_col: Optional[Union[str, List[str]]] = None) -> DataFrame:\n    \"\"\"\n    Read a Spark table and return a DataFrame.\n\n    Parameters\n    ----------\n    name : string\n        Table name in Spark.\n\n    index_col : str or list of str, optional, default: None\n        Index column of table in Spark.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    DataFrame.to_table\n    read_delta\n    read_parquet\n    read_spark_io\n\n    Examples\n    --------\n    >>> ks.range(1).to_table('%s.my_table' % db)\n    >>> ks.read_table('%s.my_table' % db)\n       id\n    0   0\n\n    >>> ks.range(1).to_table('%s.my_table' % db, index_col=\"index\")\n    >>> ks.read_table('%s.my_table' % db, index_col=\"index\")  # doctest: +NORMALIZE_WHITESPACE\n           id\n    index\n    0       0\n    \"\"\"\n    sdf = default_session().read.table(name)\n    index_spark_column_names, index_names = _get_index_map(sdf, index_col)\n\n    return DataFrame(\n        InternalFrame(\n            spark_frame=sdf,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n        )\n    )\n\n\ndef read_spark_io(\n    path: Optional[str] = None,\n    format: Optional[str] = None,\n    schema: Union[str, \"StructType\"] = None,\n    index_col: Optional[Union[str, List[str]]] = None,\n    **options\n) -> DataFrame:\n    \"\"\"Load a DataFrame from a Spark data source.\n\n    Parameters\n    ----------\n    path : string, optional\n        Path to the data source.\n    format : string, optional\n        Specifies the output data source format. Some common ones are:\n\n        - 'delta'\n        - 'parquet'\n        - 'orc'\n        - 'json'\n        - 'csv'\n    schema : string or StructType, optional\n        Input schema. If none, Spark tries to infer the schema automatically.\n        The schema can either be a Spark StructType, or a DDL-formatted string like\n        `col0 INT, col1 DOUBLE`.\n    index_col : str or list of str, optional, default: None\n        Index column of table in Spark.\n    options : dict\n        All other options passed directly into Spark's data source.\n\n    See Also\n    --------\n    DataFrame.to_spark_io\n    DataFrame.read_table\n    DataFrame.read_delta\n    DataFrame.read_parquet\n\n    Examples\n    --------\n    >>> ks.range(1).to_spark_io('%s/read_spark_io/data.parquet' % path)\n    >>> ks.read_spark_io(\n    ...     '%s/read_spark_io/data.parquet' % path, format='parquet', schema='id long')\n       id\n    0   0\n\n    >>> ks.range(10, 15, num_partitions=1).to_spark_io('%s/read_spark_io/data.json' % path,\n    ...                                                format='json', lineSep='__')\n    >>> ks.read_spark_io(\n    ...     '%s/read_spark_io/data.json' % path, format='json', schema='id long', lineSep='__')\n       id\n    0  10\n    1  11\n    2  12\n    3  13\n    4  14\n\n    You can preserve the index in the roundtrip as below.\n\n    >>> ks.range(10, 15, num_partitions=1).to_spark_io('%s/read_spark_io/data.orc' % path,\n    ...                                                format='orc', index_col=\"index\")\n    >>> ks.read_spark_io(\n    ...     path=r'%s/read_spark_io/data.orc' % path, format=\"orc\", index_col=\"index\")\n    ... # doctest: +NORMALIZE_WHITESPACE\n           id\n    index\n    0      10\n    1      11\n    2      12\n    3      13\n    4      14\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    sdf = default_session().read.load(path=path, format=format, schema=schema, **options)\n    index_spark_column_names, index_names = _get_index_map(sdf, index_col)\n\n    return DataFrame(\n        InternalFrame(\n            spark_frame=sdf,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n        )\n    )\n\n\ndef read_parquet(path, columns=None, index_col=None, pandas_metadata=False, **options) -> DataFrame:\n    \"\"\"Load a parquet object from the file path, returning a DataFrame.\n\n    Parameters\n    ----------\n    path : string\n        File path\n    columns : list, default=None\n        If not None, only these columns will be read from the file.\n    index_col : str or list of str, optional, default: None\n        Index column of table in Spark.\n    pandas_metadata : bool, default: False\n        If True, try to respect the metadata if the Parquet file is written from pandas.\n    options : dict\n        All other options passed directly into Spark's data source.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    DataFrame.to_parquet\n    DataFrame.read_table\n    DataFrame.read_delta\n    DataFrame.read_spark_io\n\n    Examples\n    --------\n    >>> ks.range(1).to_parquet('%s/read_spark_io/data.parquet' % path)\n    >>> ks.read_parquet('%s/read_spark_io/data.parquet' % path, columns=['id'])\n       id\n    0   0\n\n    You can preserve the index in the roundtrip as below.\n\n    >>> ks.range(1).to_parquet('%s/read_spark_io/data.parquet' % path, index_col=\"index\")\n    >>> ks.read_parquet('%s/read_spark_io/data.parquet' % path, columns=['id'], index_col=\"index\")\n    ... # doctest: +NORMALIZE_WHITESPACE\n           id\n    index\n    0       0\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    if columns is not None:\n        columns = list(columns)\n\n    index_names = None\n\n    if index_col is None and pandas_metadata:\n        if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0.0\"):\n            raise ValueError(\"pandas_metadata is not supported with Spark < 3.0.\")\n\n        # Try to read pandas metadata\n\n        @pandas_udf(\"index_col array<string>, index_names array<string>\", PandasUDFType.SCALAR)\n        def read_index_metadata(pser):\n            binary = pser.iloc[0]\n            metadata = pq.ParquetFile(pa.BufferReader(binary)).metadata.metadata\n            if b\"pandas\" in metadata:\n                pandas_metadata = json.loads(metadata[b\"pandas\"].decode(\"utf8\"))\n                if all(isinstance(col, str) for col in pandas_metadata[\"index_columns\"]):\n                    index_col = []\n                    index_names = []\n                    for col in pandas_metadata[\"index_columns\"]:\n                        index_col.append(col)\n                        for column in pandas_metadata[\"columns\"]:\n                            if column[\"field_name\"] == col:\n                                index_names.append(column[\"name\"])\n                                break\n                        else:\n                            index_names.append(None)\n                    return pd.DataFrame({\"index_col\": [index_col], \"index_names\": [index_names]})\n            return pd.DataFrame({\"index_col\": [None], \"index_names\": [None]})\n\n        index_col, index_names = (\n            default_session()\n            .read.format(\"binaryFile\")\n            .load(path)\n            .limit(1)\n            .select(read_index_metadata(\"content\").alias(\"index_metadata\"))\n            .select(\"index_metadata.*\")\n            .head()\n        )\n\n    kdf = read_spark_io(path=path, format=\"parquet\", options=options, index_col=index_col)\n\n    if columns is not None:\n        new_columns = [c for c in columns if c in kdf.columns]\n        if len(new_columns) > 0:\n            kdf = kdf[new_columns]\n        else:\n            sdf = default_session().createDataFrame([], schema=StructType())\n            index_spark_column_names, index_names = _get_index_map(sdf, index_col)\n            kdf = DataFrame(\n                InternalFrame(\n                    spark_frame=sdf,\n                    index_spark_column_names=index_spark_column_names,\n                    index_names=index_names,\n                )\n            )\n\n    if index_names is not None:\n        kdf.index.names = index_names\n\n    return kdf\n\n\ndef read_clipboard(sep=r\"\\s+\", **kwargs):\n    r\"\"\"\n    Read text from clipboard and pass to read_csv. See read_csv for the\n    full argument list\n\n    Parameters\n    ----------\n    sep : str, default '\\s+'\n        A string or regex delimiter. The default of '\\s+' denotes\n        one or more whitespace characters.\n\n    See Also\n    --------\n    DataFrame.to_clipboard : Write text out to clipboard.\n\n    Returns\n    -------\n    parsed : DataFrame\n    \"\"\"\n    return from_pandas(pd.read_clipboard(sep, **kwargs))\n\n\ndef read_excel(\n    io,\n    sheet_name=0,\n    header=0,\n    names=None,\n    index_col=None,\n    usecols=None,\n    squeeze=False,\n    dtype=None,\n    engine=None,\n    converters=None,\n    true_values=None,\n    false_values=None,\n    skiprows=None,\n    nrows=None,\n    na_values=None,\n    keep_default_na=True,\n    verbose=False,\n    parse_dates=False,\n    date_parser=None,\n    thousands=None,\n    comment=None,\n    skipfooter=0,\n    convert_float=True,\n    mangle_dupe_cols=True,\n    **kwds\n):\n    \"\"\"\n    Read an Excel file into a Koalas DataFrame.\n\n    Support both `xls` and `xlsx` file extensions from a local filesystem or URL.\n    Support an option to read a single sheet or a list of sheets.\n\n    Parameters\n    ----------\n    io : str, file descriptor, pathlib.Path, ExcelFile or xlrd.Book\n        The string could be a URL. The value URL must be available in Spark's DataFrameReader.\n\n        .. note::\n            If the underlying Spark is below 3.0, the parameter as a string is not supported.\n            You can use `ks.from_pandas(pd.read_excel(...))` as a workaround.\n\n    sheet_name : str, int, list, or None, default 0\n        Strings are used for sheet names. Integers are used in zero-indexed\n        sheet positions. Lists of strings/integers are used to request\n        multiple sheets. Specify None to get all sheets.\n\n        Available cases:\n\n        * Defaults to ``0``: 1st sheet as a `DataFrame`\n        * ``1``: 2nd sheet as a `DataFrame`\n        * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n        * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n          as a dict of `DataFrame`\n        * None: All sheets.\n\n    header : int, list of int, default 0\n        Row (0-indexed) to use for the column labels of the parsed\n        DataFrame. If a list of integers is passed those row positions will\n        be combined into a ``MultiIndex``. Use None if there is no header.\n    names : array-like, default None\n        List of column names to use. If file contains no header row,\n        then you should explicitly pass header=None.\n    index_col : int, list of int, default None\n        Column (0-indexed) to use as the row labels of the DataFrame.\n        Pass None if there is no such column.  If a list is passed,\n        those columns will be combined into a ``MultiIndex``.  If a\n        subset of data is selected with ``usecols``, index_col\n        is based on the subset.\n    usecols : int, str, list-like, or callable default None\n        Return a subset of the columns.\n\n        * If None, then parse all columns.\n        * If str, then indicates comma separated list of Excel column letters\n          and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n          both sides.\n        * If list of int, then indicates list of column numbers to be parsed.\n        * If list of string, then indicates list of column names to be parsed.\n        * If callable, then evaluate each column name against it and parse the\n          column if the callable returns ``True``.\n    squeeze : bool, default False\n        If the parsed data only contains one column then return a Series.\n    dtype : Type name or dict of column -> type, default None\n        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n        Use `object` to preserve data as stored in Excel and not interpret dtype.\n        If converters are specified, they will be applied INSTEAD\n        of dtype conversion.\n    engine : str, default None\n        If io is not a buffer or path, this must be set to identify io.\n        Acceptable values are None or xlrd.\n    converters : dict, default None\n        Dict of functions for converting values in certain columns. Keys can\n        either be integers or column labels, values are functions that take one\n        input argument, the Excel cell content, and return the transformed\n        content.\n    true_values : list, default None\n        Values to consider as True.\n    false_values : list, default None\n        Values to consider as False.\n    skiprows : list-like\n        Rows to skip at the beginning (0-indexed).\n    nrows : int, default None\n        Number of rows to parse.\n    na_values : scalar, str, list-like, or dict, default None\n        Additional strings to recognize as NA/NaN. If dict passed, specific\n        per-column NA values. By default the following values are interpreted\n        as NaN.\n    keep_default_na : bool, default True\n        If na_values are specified and keep_default_na is False the default NaN\n        values are overridden, otherwise they're appended to.\n    verbose : bool, default False\n        Indicate number of NA values placed in non-numeric columns.\n    parse_dates : bool, list-like, or dict, default False\n        The behavior is as follows:\n\n        * bool. If True -> try parsing the index.\n        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n          each as a separate date column.\n        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n          a single date column.\n        * dict, e.g. {{'foo' : [1, 3]}} -> parse columns 1, 3 as date and call\n          result 'foo'\n\n        If a column or index contains an unparseable date, the entire column or\n        index will be returned unaltered as an object data type. For non-standard\n        datetime parsing, use ``pd.to_datetime`` after ``pd.read_csv``\n\n        Note: A fast-path exists for iso8601-formatted dates.\n    date_parser : function, optional\n        Function to use for converting a sequence of string columns to an array of\n        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n        conversion. Koalas will try to call `date_parser` in three different ways,\n        advancing to the next if an exception occurs: 1) Pass one or more arrays\n        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n        string values from the columns defined by `parse_dates` into a single array\n        and pass that; and 3) call `date_parser` once for each row using one or\n        more strings (corresponding to the columns defined by `parse_dates`) as\n        arguments.\n    thousands : str, default None\n        Thousands separator for parsing string columns to numeric.  Note that\n        this parameter is only necessary for columns stored as TEXT in Excel,\n        any numeric columns will automatically be parsed, regardless of display\n        format.\n    comment : str, default None\n        Comments out remainder of line. Pass a character or characters to this\n        argument to indicate comments in the input file. Any data between the\n        comment string and the end of the current line is ignored.\n    skipfooter : int, default 0\n        Rows at the end to skip (0-indexed).\n    convert_float : bool, default True\n        Convert integral floats to int (i.e., 1.0 --> 1). If False, all numeric\n        data will be read in as floats: Excel stores all numbers as floats\n        internally.\n    mangle_dupe_cols : bool, default True\n        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n        'X'...'X'. Passing in False will cause data to be overwritten if there\n        are duplicate names in the columns.\n    **kwds : optional\n        Optional keyword arguments can be passed to ``TextFileReader``.\n\n    Returns\n    -------\n    DataFrame or dict of DataFrames\n        DataFrame from the passed in Excel file. See notes in sheet_name\n        argument for more information on when a dict of DataFrames is returned.\n\n    See Also\n    --------\n    DataFrame.to_excel : Write DataFrame to an Excel file.\n    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\n\n    Examples\n    --------\n    The file can be read using the file name as string or an open file object:\n\n    >>> ks.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n           Name  Value\n    0   string1      1\n    1   string2      2\n    2  #Comment      3\n\n    >>> ks.read_excel(open('tmp.xlsx', 'rb'),\n    ...               sheet_name='Sheet3')  # doctest: +SKIP\n       Unnamed: 0      Name  Value\n    0           0   string1      1\n    1           1   string2      2\n    2           2  #Comment      3\n\n    Index and header can be specified via the `index_col` and `header` arguments\n\n    >>> ks.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n         0         1      2\n    0  NaN      Name  Value\n    1  0.0   string1      1\n    2  1.0   string2      2\n    3  2.0  #Comment      3\n\n    Column types are inferred but can be explicitly specified\n\n    >>> ks.read_excel('tmp.xlsx', index_col=0,\n    ...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n           Name  Value\n    0   string1    1.0\n    1   string2    2.0\n    2  #Comment    3.0\n\n    True, False, and NA values, and thousands separators have defaults,\n    but can be explicitly specified, too. Supply the values you would like\n    as strings or lists of strings!\n\n    >>> ks.read_excel('tmp.xlsx', index_col=0,\n    ...               na_values=['string1', 'string2'])  # doctest: +SKIP\n           Name  Value\n    0      None      1\n    1      None      2\n    2  #Comment      3\n\n    Comment lines in the excel input file can be skipped using the `comment` kwarg\n\n    >>> ks.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n          Name  Value\n    0  string1    1.0\n    1  string2    2.0\n    2     None    NaN\n    \"\"\"\n\n    def pd_read_excel(io_or_bin, sn):\n        return pd.read_excel(\n            io=BytesIO(io_or_bin) if isinstance(io_or_bin, (bytes, bytearray)) else io_or_bin,\n            sheet_name=sn,\n            header=header,\n            names=names,\n            index_col=index_col,\n            usecols=usecols,\n            squeeze=squeeze,\n            dtype=dtype,\n            engine=engine,\n            converters=converters,\n            true_values=true_values,\n            false_values=false_values,\n            skiprows=skiprows,\n            nrows=nrows,\n            na_values=na_values,\n            keep_default_na=keep_default_na,\n            verbose=verbose,\n            parse_dates=parse_dates,\n            date_parser=date_parser,\n            thousands=thousands,\n            comment=comment,\n            skipfooter=skipfooter,\n            convert_float=convert_float,\n            mangle_dupe_cols=mangle_dupe_cols,\n            **kwds\n        )\n\n    if isinstance(io, str):\n        if LooseVersion(pyspark.__version__) < LooseVersion(\"3.0.0\"):\n            raise ValueError(\n                \"The `io` parameter as a string is not supported if the underlying Spark is \"\n                \"below 3.0. You can use `ks.from_pandas(pd.read_excel(...))` as a workaround\"\n            )\n        # 'binaryFile' format is available since Spark 3.0.0.\n        binaries = default_session().read.format(\"binaryFile\").load(io).select(\"content\").head(2)\n        io_or_bin = binaries[0][0]\n        single_file = len(binaries) == 1\n    else:\n        io_or_bin = io\n        single_file = True\n\n    pdfs = pd_read_excel(io_or_bin, sn=sheet_name)\n\n    if single_file:\n        if isinstance(pdfs, dict):\n            return OrderedDict([(key, from_pandas(value)) for key, value in pdfs.items()])\n        else:\n            return from_pandas(pdfs)\n    else:\n\n        def read_excel_on_spark(pdf, sn):\n\n            kdf = from_pandas(pdf)\n            return_schema = kdf._internal.to_internal_spark_frame.schema\n\n            def output_func(pdf):\n                pdf = pd.concat([pd_read_excel(bin, sn=sn) for bin in pdf[pdf.columns[0]]])\n\n                # TODO: deduplicate this logic with InternalFrame.from_pandas\n                new_index_columns = [\n                    SPARK_INDEX_NAME_FORMAT(i) for i in _range(len(pdf.index.names))\n                ]\n                new_data_columns = [name_like_string(col) for col in pdf.columns]\n\n                pdf.index.names = new_index_columns\n                reset_index = pdf.reset_index()\n                reset_index.columns = new_index_columns + new_data_columns\n                for name, col in reset_index.iteritems():\n                    dt = col.dtype\n                    if is_datetime64_dtype(dt) or is_datetime64tz_dtype(dt):\n                        continue\n                    reset_index[name] = col.replace({np.nan: None})\n                pdf = reset_index\n\n                # Just positionally map the column names to given schema's.\n                return pdf.rename(columns=dict(zip(pdf.columns, return_schema.fieldNames())))\n\n            sdf = (\n                default_session()\n                .read.format(\"binaryFile\")\n                .load(io)\n                .select(\"content\")\n                .mapInPandas(lambda iterator: map(output_func, iterator), schema=return_schema)\n            )\n\n            return DataFrame(kdf._internal.with_new_sdf(sdf))\n\n        if isinstance(pdfs, dict):\n            return OrderedDict([(sn, read_excel_on_spark(pdf, sn)) for sn, pdf in pdfs.items()])\n        else:\n            return read_excel_on_spark(pdfs, sheet_name)\n\n\ndef read_html(\n    io,\n    match=\".+\",\n    flavor=None,\n    header=None,\n    index_col=None,\n    skiprows=None,\n    attrs=None,\n    parse_dates=False,\n    thousands=\",\",\n    encoding=None,\n    decimal=\".\",\n    converters=None,\n    na_values=None,\n    keep_default_na=True,\n    displayed_only=True,\n):\n    r\"\"\"Read HTML tables into a ``list`` of ``DataFrame`` objects.\n\n    Parameters\n    ----------\n    io : str or file-like\n        A URL, a file-like object, or a raw string containing HTML. Note that\n        lxml only accepts the http, ftp and file url protocols. If you have a\n        URL that starts with ``'https'`` you might try removing the ``'s'``.\n\n    match : str or compiled regular expression, optional\n        The set of tables containing text matching this regex or string will be\n        returned. Unless the HTML is extremely simple you will probably need to\n        pass a non-empty string here. Defaults to '.+' (match any non-empty\n        string). The default value will return all tables contained on a page.\n        This value is converted to a regular expression so that there is\n        consistent behavior between Beautiful Soup and lxml.\n\n    flavor : str or None, container of strings\n        The parsing engine to use. 'bs4' and 'html5lib' are synonymous with\n        each other, they are both there for backwards compatibility. The\n        default of ``None`` tries to use ``lxml`` to parse and if that fails it\n        falls back on ``bs4`` + ``html5lib``.\n\n    header : int or list-like or None, optional\n        The row (or list of rows for a :class:`~ks.MultiIndex`) to use to\n        make the columns headers.\n\n    index_col : int or list-like or None, optional\n        The column (or list of columns) to use to create the index.\n\n    skiprows : int or list-like or slice or None, optional\n        0-based. Number of rows to skip after parsing the column integer. If a\n        sequence of integers or a slice is given, will skip the rows indexed by\n        that sequence.  Note that a single element sequence means 'skip the nth\n        row' whereas an integer means 'skip n rows'.\n\n    attrs : dict or None, optional\n        This is a dictionary of attributes that you can pass to use to identify\n        the table in the HTML. These are not checked for validity before being\n        passed to lxml or Beautiful Soup. However, these attributes must be\n        valid HTML table attributes to work correctly. For example, ::\n\n            attrs = {'id': 'table'}\n\n        is a valid attribute dictionary because the 'id' HTML tag attribute is\n        a valid HTML attribute for *any* HTML tag as per `this document\n        <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\n\n            attrs = {'asdf': 'table'}\n\n        is *not* a valid attribute dictionary because 'asdf' is not a valid\n        HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\n        table attributes can be found `here\n        <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\n        working draft of the HTML 5 spec can be found `here\n        <http://www.w3.org/TR/html-markup/table.html>`__. It contains the\n        latest information on table attributes for the modern web.\n\n    parse_dates : bool, optional\n        See :func:`~ks.read_csv` for more details.\n\n    thousands : str, optional\n        Separator to use to parse thousands. Defaults to ``','``.\n\n    encoding : str or None, optional\n        The encoding used to decode the web page. Defaults to ``None``.``None``\n        preserves the previous encoding behavior, which depends on the\n        underlying parser library (e.g., the parser library will try to use\n        the encoding provided by the document).\n\n    decimal : str, default '.'\n        Character to recognize as decimal point (e.g. use ',' for European\n        data).\n\n    converters : dict, default None\n        Dict of functions for converting values in certain columns. Keys can\n        either be integers or column labels, values are functions that take one\n        input argument, the cell (not column) content, and return the\n        transformed content.\n\n    na_values : iterable, default None\n        Custom NA values\n\n    keep_default_na : bool, default True\n        If na_values are specified and keep_default_na is False the default NaN\n        values are overridden, otherwise they're appended to\n\n    displayed_only : bool, default True\n        Whether elements with \"display: none\" should be parsed\n\n    Returns\n    -------\n    dfs : list of DataFrames\n\n    See Also\n    --------\n    read_csv\n    DataFrame.to_html\n    \"\"\"\n    pdfs = pd.read_html(\n        io=io,\n        match=match,\n        flavor=flavor,\n        header=header,\n        index_col=index_col,\n        skiprows=skiprows,\n        attrs=attrs,\n        parse_dates=parse_dates,\n        thousands=thousands,\n        encoding=encoding,\n        decimal=decimal,\n        converters=converters,\n        na_values=na_values,\n        keep_default_na=keep_default_na,\n        displayed_only=displayed_only,\n    )\n    return [from_pandas(pdf) for pdf in pdfs]\n\n\n# TODO: add `coerce_float` and 'parse_dates' parameters\ndef read_sql_table(table_name, con, schema=None, index_col=None, columns=None, **options):\n    \"\"\"\n    Read SQL database table into a DataFrame.\n\n    Given a table name and a JDBC URI, returns a DataFrame.\n\n    Parameters\n    ----------\n    table_name : str\n        Name of SQL table in database.\n    con : str\n        A JDBC URI could be provided as as str.\n\n        .. note:: The URI must be JDBC URI instead of Python's database URI.\n\n    schema : str, default None\n        Name of SQL schema in database to query (if database flavor\n        supports this). Uses default schema if None (default).\n    index_col : str or list of str, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    columns : list, default None\n        List of column names to select from SQL table.\n    options : dict\n        All other options passed directly into Spark's JDBC data source.\n\n    Returns\n    -------\n    DataFrame\n        A SQL table is returned as two-dimensional data structure with labeled\n        axes.\n\n    See Also\n    --------\n    read_sql_query : Read SQL query into a DataFrame.\n    read_sql : Read SQL query or database table into a DataFrame.\n\n    Examples\n    --------\n    >>> ks.read_sql_table('table_name', 'jdbc:postgresql:db_name')  # doctest: +SKIP\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    reader = default_session().read\n    reader.option(\"dbtable\", table_name)\n    reader.option(\"url\", con)\n    if schema is not None:\n        reader.schema(schema)\n    reader.options(**options)\n    sdf = reader.format(\"jdbc\").load()\n    index_spark_column_names, index_names = _get_index_map(sdf, index_col)\n    kdf = DataFrame(\n        InternalFrame(\n            spark_frame=sdf,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n        )\n    )\n    if columns is not None:\n        if isinstance(columns, str):\n            columns = [columns]\n        kdf = kdf[columns]\n    return kdf\n\n\n# TODO: add `coerce_float`, `params`, and 'parse_dates' parameters\ndef read_sql_query(sql, con, index_col=None, **options):\n    \"\"\"Read SQL query into a DataFrame.\n\n    Returns a DataFrame corresponding to the result set of the query\n    string. Optionally provide an `index_col` parameter to use one of the\n    columns as the index, otherwise default index will be used.\n\n    .. note:: Some database might hit the issue of Spark: SPARK-27596\n\n    Parameters\n    ----------\n    sql : string SQL query\n        SQL query to be executed.\n    con : str\n        A JDBC URI could be provided as as str.\n\n        .. note:: The URI must be JDBC URI instead of Python's database URI.\n\n    index_col : string or list of strings, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    options : dict\n        All other options passed directly into Spark's JDBC data source.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame.\n    read_sql\n\n    Examples\n    --------\n    >>> ks.read_sql_query('SELECT * FROM table_name', 'jdbc:postgresql:db_name')  # doctest: +SKIP\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    reader = default_session().read\n    reader.option(\"query\", sql)\n    reader.option(\"url\", con)\n    reader.options(**options)\n    sdf = reader.format(\"jdbc\").load()\n    index_spark_column_names, index_names = _get_index_map(sdf, index_col)\n    return DataFrame(\n        InternalFrame(\n            spark_frame=sdf,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n        )\n    )\n\n\n# TODO: add `coerce_float`, `params`, and 'parse_dates' parameters\ndef read_sql(sql, con, index_col=None, columns=None, **options):\n    \"\"\"\n    Read SQL query or database table into a DataFrame.\n\n    This function is a convenience wrapper around ``read_sql_table`` and\n    ``read_sql_query`` (for backward compatibility). It will delegate\n    to the specific function depending on the provided input. A SQL query\n    will be routed to ``read_sql_query``, while a database table name will\n    be routed to ``read_sql_table``. Note that the delegated function might\n    have more specific notes about their functionality not listed here.\n\n    .. note:: Some database might hit the issue of Spark: SPARK-27596\n\n    Parameters\n    ----------\n    sql : string\n        SQL query to be executed or a table name.\n    con : str\n        A JDBC URI could be provided as as str.\n\n        .. note:: The URI must be JDBC URI instead of Python's database URI.\n\n    index_col : string or list of strings, optional, default: None\n        Column(s) to set as index(MultiIndex).\n    columns : list, default: None\n        List of column names to select from SQL table (only used when reading\n        a table).\n    options : dict\n        All other options passed directly into Spark's JDBC data source.\n\n    Returns\n    -------\n    DataFrame\n\n    See Also\n    --------\n    read_sql_table : Read SQL database table into a DataFrame.\n    read_sql_query : Read SQL query into a DataFrame.\n\n    Examples\n    --------\n    >>> ks.read_sql('table_name', 'jdbc:postgresql:db_name')  # doctest: +SKIP\n    >>> ks.read_sql('SELECT * FROM table_name', 'jdbc:postgresql:db_name')  # doctest: +SKIP\n    \"\"\"\n    if \"options\" in options and isinstance(options.get(\"options\"), dict) and len(options) == 1:\n        options = options.get(\"options\")  # type: ignore\n\n    striped = sql.strip()\n    if \" \" not in striped:  # TODO: identify the table name or not more precisely.\n        return read_sql_table(sql, con, index_col=index_col, columns=columns, **options)\n    else:\n        return read_sql_query(sql, con, index_col=index_col, **options)\n\n\ndef to_datetime(\n    arg, errors=\"raise\", format=None, unit=None, infer_datetime_format=False, origin=\"unix\"\n):\n    \"\"\"\n    Convert argument to datetime.\n\n    Parameters\n    ----------\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\n           or DataFrame/dict-like\n\n    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n\n        - If 'raise', then invalid parsing will raise an exception\n        - If 'coerce', then invalid parsing will be set as NaT\n        - If 'ignore', then invalid parsing will return the input\n    format : string, default None\n        strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n        all the way up to nanoseconds.\n    unit : string, default None\n        unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n        integer or float number. This will be based off the origin.\n        Example, with unit='ms' and origin='unix' (the default), this\n        would calculate the number of milliseconds to the unix epoch start.\n    infer_datetime_format : boolean, default False\n        If True and no `format` is given, attempt to infer the format of the\n        datetime strings, and if it can be inferred, switch to a faster\n        method of parsing them. In some cases this can increase the parsing\n        speed by ~5-10x.\n    origin : scalar, default 'unix'\n        Define the reference date. The numeric values would be parsed as number\n        of units (defined by `unit`) since this reference date.\n\n        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n        - If 'julian', unit must be 'D', and origin is set to beginning of\n          Julian Calendar. Julian day number 0 is assigned to the day starting\n          at noon on January 1, 4713 BC.\n        - If Timestamp convertible, origin is set to Timestamp identified by\n          origin.\n\n    Returns\n    -------\n    ret : datetime if parsing succeeded.\n        Return type depends on input:\n\n        - list-like: DatetimeIndex\n        - Series: Series of datetime64 dtype\n        - scalar: Timestamp\n\n        In case when it is not possible to return designated types (e.g. when\n        any element of input is before Timestamp.min or after Timestamp.max)\n        return will have datetime.datetime type (or corresponding\n        array/Series).\n\n    Examples\n    --------\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\n    common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n    'ms', 'us', 'ns']) or plurals of the same\n\n    >>> df = ks.DataFrame({'year': [2015, 2016],\n    ...                    'month': [2, 3],\n    ...                    'day': [4, 5]})\n    >>> ks.to_datetime(df)\n    0   2015-02-04\n    1   2016-03-05\n    dtype: datetime64[ns]\n\n    If a date does not meet the `timestamp limitations\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html\n    #timeseries-timestamp-limits>`_, passing errors='ignore'\n    will return the original input instead of raising any exception.\n\n    Passing errors='coerce' will force an out-of-bounds date to NaT,\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\n\n    >>> ks.to_datetime('13000101', format='%Y%m%d', errors='ignore')\n    datetime.datetime(1300, 1, 1, 0, 0)\n    >>> ks.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n    NaT\n\n    Passing infer_datetime_format=True can often-times speedup a parsing\n    if its not an ISO8601 format exactly, but in a regular format.\n\n    >>> s = ks.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n    >>> s.head()\n    0    3/11/2000\n    1    3/12/2000\n    2    3/13/2000\n    3    3/11/2000\n    4    3/12/2000\n    dtype: object\n\n    >>> import timeit\n    >>> timeit.timeit(\n    ...    lambda: repr(ks.to_datetime(s, infer_datetime_format=True)),\n    ...    number = 1)  # doctest: +SKIP\n    0.35832712500000063\n\n    >>> timeit.timeit(\n    ...    lambda: repr(ks.to_datetime(s, infer_datetime_format=False)),\n    ...    number = 1)  # doctest: +SKIP\n    0.8895321660000004\n\n    Using a unix epoch time\n\n    >>> ks.to_datetime(1490195805, unit='s')\n    Timestamp('2017-03-22 15:16:45')\n    >>> ks.to_datetime(1490195805433502912, unit='ns')\n    Timestamp('2017-03-22 15:16:45.433502912')\n\n    Using a non-unix epoch origin\n\n    >>> ks.to_datetime([1, 2, 3], unit='D', origin=pd.Timestamp('1960-01-01'))\n    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], dtype='datetime64[ns]', freq=None)\n    \"\"\"\n\n    def pandas_to_datetime(pser_or_pdf) -> Series[np.datetime64]:\n        if isinstance(pser_or_pdf, pd.DataFrame):\n            pser_or_pdf = pser_or_pdf[[\"year\", \"month\", \"day\"]]\n        return pd.to_datetime(\n            pser_or_pdf,\n            errors=errors,\n            format=format,\n            unit=unit,\n            infer_datetime_format=infer_datetime_format,\n            origin=origin,\n        )\n\n    if isinstance(arg, Series):\n        return arg.koalas.transform_batch(pandas_to_datetime)\n    if isinstance(arg, DataFrame):\n        kdf = arg[[\"year\", \"month\", \"day\"]]\n        return kdf.koalas.transform_batch(pandas_to_datetime)\n    return pd.to_datetime(\n        arg,\n        errors=errors,\n        format=format,\n        unit=unit,\n        infer_datetime_format=infer_datetime_format,\n        origin=origin,\n    )\n\n\ndef get_dummies(\n    data,\n    prefix=None,\n    prefix_sep=\"_\",\n    dummy_na=False,\n    columns=None,\n    sparse=False,\n    drop_first=False,\n    dtype=None,\n):\n    \"\"\"\n    Convert categorical variable into dummy/indicator variables, also\n    known as one hot encoding.\n\n    Parameters\n    ----------\n    data : array-like, Series, or DataFrame\n    prefix : string, list of strings, or dict of strings, default None\n        String to append DataFrame column names.\n        Pass a list with length equal to the number of columns\n        when calling get_dummies on a DataFrame. Alternatively, `prefix`\n        can be a dictionary mapping column names to prefixes.\n    prefix_sep : string, default '_'\n        If appending prefix, separator/delimiter to use. Or pass a\n        list or dictionary as with `prefix.`\n    dummy_na : bool, default False\n        Add a column to indicate NaNs, if False NaNs are ignored.\n    columns : list-like, default None\n        Column names in the DataFrame to be encoded.\n        If `columns` is None then all the columns with\n        `object` or `category` dtype will be converted.\n    sparse : bool, default False\n        Whether the dummy-encoded columns should be be backed by\n        a :class:`SparseArray` (True) or a regular NumPy array (False).\n        In Koalas, this value must be \"False\".\n    drop_first : bool, default False\n        Whether to get k-1 dummies out of k categorical levels by removing the\n        first level.\n    dtype : dtype, default np.uint8\n        Data type for new columns. Only a single dtype is allowed.\n\n    Returns\n    -------\n    dummies : DataFrame\n\n    See Also\n    --------\n    Series.str.get_dummies\n\n    Examples\n    --------\n    >>> s = ks.Series(list('abca'))\n\n    >>> ks.get_dummies(s)\n       a  b  c\n    0  1  0  0\n    1  0  1  0\n    2  0  0  1\n    3  1  0  0\n\n    >>> df = ks.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n    ...                    'C': [1, 2, 3]},\n    ...                   columns=['A', 'B', 'C'])\n\n    >>> ks.get_dummies(df, prefix=['col1', 'col2'])\n       C  col1_a  col1_b  col2_a  col2_b  col2_c\n    0  1       1       0       0       1       0\n    1  2       0       1       1       0       0\n    2  3       1       0       0       0       1\n\n    >>> ks.get_dummies(ks.Series(list('abcaa')))\n       a  b  c\n    0  1  0  0\n    1  0  1  0\n    2  0  0  1\n    3  1  0  0\n    4  1  0  0\n\n    >>> ks.get_dummies(ks.Series(list('abcaa')), drop_first=True)\n       b  c\n    0  0  0\n    1  1  0\n    2  0  1\n    3  0  0\n    4  0  0\n\n    >>> ks.get_dummies(ks.Series(list('abc')), dtype=float)\n         a    b    c\n    0  1.0  0.0  0.0\n    1  0.0  1.0  0.0\n    2  0.0  0.0  1.0\n    \"\"\"\n    if sparse is not False:\n        raise NotImplementedError(\"get_dummies currently does not support sparse\")\n\n    if columns is not None:\n        if not is_list_like(columns):\n            raise TypeError(\"Input must be a list-like for parameter `columns`\")\n\n    if dtype is None:\n        dtype = \"byte\"\n\n    if isinstance(data, Series):\n        if prefix is not None:\n            prefix = [str(prefix)]\n        kdf = data.to_frame()\n        column_labels = kdf._internal.column_labels\n        remaining_columns = []\n    else:\n        if isinstance(prefix, str):\n            raise NotImplementedError(\n                \"get_dummies currently does not support prefix as string types\"\n            )\n        kdf = data.copy()\n\n        if columns is None:\n            column_labels = [\n                label\n                for label in kdf._internal.column_labels\n                if isinstance(\n                    kdf._internal.spark_type_for(label), _get_dummies_default_accept_types\n                )\n            ]\n        else:\n            if is_name_like_tuple(columns):\n                column_labels = [\n                    label\n                    for label in kdf._internal.column_labels\n                    if label[: len(columns)] == columns\n                ]\n                if len(column_labels) == 0:\n                    raise KeyError(name_like_string(columns))\n                if prefix is None:\n                    prefix = [\n                        str(label[len(columns) :])\n                        if len(label) > len(columns) + 1\n                        else label[len(columns)]\n                        if len(label) == len(columns) + 1\n                        else \"\"\n                        for label in column_labels\n                    ]\n            elif any(isinstance(col, tuple) for col in columns) and any(\n                not is_name_like_tuple(col) for col in columns\n            ):\n                raise ValueError(\n                    \"Expected tuple, got {}\".format(\n                        type(set(col for col in columns if not is_name_like_tuple(col)).pop())\n                    )\n                )\n            else:\n                column_labels = [\n                    label\n                    for key in columns\n                    for label in kdf._internal.column_labels\n                    if label == key or label[0] == key\n                ]\n        if len(column_labels) == 0:\n            if columns is None:\n                return kdf\n            raise KeyError(\"{} not in index\".format(columns))\n\n        if prefix is None:\n            prefix = [str(label) if len(label) > 1 else label[0] for label in column_labels]\n\n        column_labels_set = set(column_labels)\n        remaining_columns = [\n            (\n                kdf[label]\n                if kdf._internal.column_labels_level == 1\n                else kdf[label].rename(name_like_string(label))\n            )\n            for label in kdf._internal.column_labels\n            if label not in column_labels_set\n        ]\n\n    if any(\n        not isinstance(kdf._internal.spark_type_for(label), _get_dummies_acceptable_types)\n        for label in column_labels\n    ):\n        raise NotImplementedError(\n            \"get_dummies currently only accept {} values\".format(\n                \", \".join([t.typeName() for t in _get_dummies_acceptable_types])\n            )\n        )\n\n    if prefix is not None and len(column_labels) != len(prefix):\n        raise ValueError(\n            \"Length of 'prefix' ({}) did not match the length of \"\n            \"the columns being encoded ({}).\".format(len(prefix), len(column_labels))\n        )\n    elif isinstance(prefix, dict):\n        prefix = [prefix[column_label[0]] for column_label in column_labels]\n\n    all_values = _reduce_spark_multi(\n        kdf._internal.spark_frame,\n        [F.collect_set(kdf._internal.spark_column_for(label)) for label in column_labels],\n    )\n    for i, label in enumerate(column_labels):\n        values = sorted(all_values[i])\n        if drop_first:\n            values = values[1:]\n\n        def column_name(value):\n            if prefix is None or prefix[i] == \"\":\n                return value\n            else:\n                return \"{}{}{}\".format(prefix[i], prefix_sep, value)\n\n        for value in values:\n            remaining_columns.append(\n                (kdf[label].notnull() & (kdf[label] == value))\n                .astype(dtype)\n                .rename(column_name(value))\n            )\n        if dummy_na:\n            remaining_columns.append(kdf[label].isnull().astype(dtype).rename(column_name(np.nan)))\n\n    return kdf[remaining_columns]\n\n\n# TODO: there are many parameters to implement and support. See pandas's pd.concat.\ndef concat(objs, axis=0, join=\"outer\", ignore_index=False, sort=False):\n    \"\"\"\n    Concatenate Koalas objects along a particular axis with optional set logic\n    along the other axes.\n\n    Parameters\n    ----------\n    objs : a sequence of Series or DataFrame\n        Any None objects will be dropped silently unless\n        they are all None in which case a ValueError will be raised\n    axis : {0/'index', 1/'columns'}, default 0\n        The axis to concatenate along.\n    join : {'inner', 'outer'}, default 'outer'\n        How to handle indexes on other axis (or axes).\n    ignore_index : bool, default False\n        If True, do not use the index values along the concatenation axis. The\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n        concatenating objects where the concatenation axis does not have\n        meaningful indexing information. Note the index values on the other\n        axes are still respected in the join.\n    sort : bool, default False\n        Sort non-concatenation axis if it is not already aligned.\n\n    Returns\n    -------\n    object, type of objs\n        When concatenating all ``Series`` along the index (axis=0), a\n        ``Series`` is returned. When ``objs`` contains at least one\n        ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n        the columns (axis=1), a ``DataFrame`` is returned.\n\n    See Also\n    --------\n    Series.append : Concatenate Series.\n    DataFrame.join : Join DataFrames using indexes.\n    DataFrame.merge : Merge DataFrames by indexes or columns.\n\n    Examples\n    --------\n    Combine two ``Series``.\n\n    >>> s1 = ks.Series(['a', 'b'])\n    >>> s2 = ks.Series(['c', 'd'])\n    >>> ks.concat([s1, s2])\n    0    a\n    1    b\n    0    c\n    1    d\n    dtype: object\n\n    Clear the existing index and reset it in the result\n    by setting the ``ignore_index`` option to ``True``.\n\n    >>> ks.concat([s1, s2], ignore_index=True)\n    0    a\n    1    b\n    2    c\n    3    d\n    dtype: object\n\n    Combine two ``DataFrame`` objects with identical columns.\n\n    >>> df1 = ks.DataFrame([['a', 1], ['b', 2]],\n    ...                    columns=['letter', 'number'])\n    >>> df1\n      letter  number\n    0      a       1\n    1      b       2\n    >>> df2 = ks.DataFrame([['c', 3], ['d', 4]],\n    ...                    columns=['letter', 'number'])\n    >>> df2\n      letter  number\n    0      c       3\n    1      d       4\n\n    >>> ks.concat([df1, df2])\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    Combine ``DataFrame`` and ``Series`` objects with different columns.\n\n    >>> ks.concat([df2, s1])\n      letter  number     0\n    0      c     3.0  None\n    1      d     4.0  None\n    0   None     NaN     a\n    1   None     NaN     b\n\n    Combine ``DataFrame`` objects with overlapping columns\n    and return everything. Columns outside the intersection will\n    be filled with ``None`` values.\n\n    >>> df3 = ks.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n    ...                    columns=['letter', 'number', 'animal'])\n    >>> df3\n      letter  number animal\n    0      c       3    cat\n    1      d       4    dog\n\n    >>> ks.concat([df1, df3])\n      letter  number animal\n    0      a       1   None\n    1      b       2   None\n    0      c       3    cat\n    1      d       4    dog\n\n    Sort the columns.\n\n    >>> ks.concat([df1, df3], sort=True)\n      animal letter  number\n    0   None      a       1\n    1   None      b       2\n    0    cat      c       3\n    1    dog      d       4\n\n    Combine ``DataFrame`` objects with overlapping columns\n    and return only those that are shared by passing ``inner`` to\n    the ``join`` keyword argument.\n\n    >>> ks.concat([df1, df3], join=\"inner\")\n      letter  number\n    0      a       1\n    1      b       2\n    0      c       3\n    1      d       4\n\n    >>> df4 = ks.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n    ...                    columns=['animal', 'name'])\n\n    Combine with column axis.\n\n    >>> ks.concat([df1, df4], axis=1)\n      letter  number  animal    name\n    0      a       1    bird   polly\n    1      b       2  monkey  george\n    \"\"\"\n    if isinstance(objs, (DataFrame, IndexOpsMixin)) or not isinstance(\n        objs, Iterable\n    ):  # TODO: support dict\n        raise TypeError(\n            \"first argument must be an iterable of Koalas \"\n            \"objects, you passed an object of type \"\n            '\"{name}\"'.format(name=type(objs).__name__)\n        )\n\n    if len(objs) == 0:\n        raise ValueError(\"No objects to concatenate\")\n    objs = list(filter(lambda obj: obj is not None, objs))\n    if len(objs) == 0:\n        raise ValueError(\"All objects passed were None\")\n\n    for obj in objs:\n        if not isinstance(obj, (Series, DataFrame)):\n            raise TypeError(\n                \"cannot concatenate object of type \"\n                \"'{name}\"\n                \"; only ks.Series \"\n                \"and ks.DataFrame are valid\".format(name=type(objs).__name__)\n            )\n\n    if join not in [\"inner\", \"outer\"]:\n        raise ValueError(\"Only can inner (intersect) or outer (union) join the other axis.\")\n\n    axis = validate_axis(axis)\n    if axis == 1:\n        kdfs = [obj.to_frame() if isinstance(obj, Series) else obj for obj in objs]\n\n        level = min(kdf._internal.column_labels_level for kdf in kdfs)\n        kdfs = [\n            DataFrame._index_normalized_frame(level, kdf)\n            if kdf._internal.column_labels_level > level\n            else kdf\n            for kdf in kdfs\n        ]\n\n        concat_kdf = kdfs[0]\n        column_labels = concat_kdf._internal.column_labels.copy()\n\n        kdfs_not_same_anchor = []\n        for kdf in kdfs[1:]:\n            duplicated = [label for label in kdf._internal.column_labels if label in column_labels]\n            if len(duplicated) > 0:\n                pretty_names = [name_like_string(label) for label in duplicated]\n                raise ValueError(\n                    \"Labels have to be unique; however, got duplicated labels %s.\" % pretty_names\n                )\n            column_labels.extend(kdf._internal.column_labels)\n\n            if same_anchor(concat_kdf, kdf):\n                concat_kdf = DataFrame(\n                    concat_kdf._internal.with_new_columns(\n                        concat_kdf._internal.data_spark_columns + kdf._internal.data_spark_columns,\n                        concat_kdf._internal.column_labels + kdf._internal.column_labels,\n                    )\n                )\n            else:\n                kdfs_not_same_anchor.append(kdf)\n\n        if len(kdfs_not_same_anchor) > 0:\n            with ks.option_context(\"compute.ops_on_diff_frames\", True):\n\n                def resolve_func(kdf, this_column_labels, that_column_labels):\n                    raise AssertionError(\"This should not happen.\")\n\n                for kdf in kdfs_not_same_anchor:\n                    if join == \"inner\":\n                        concat_kdf = align_diff_frames(\n                            resolve_func, concat_kdf, kdf, fillna=False, how=\"inner\",\n                        )\n                    elif join == \"outer\":\n                        concat_kdf = align_diff_frames(\n                            resolve_func, concat_kdf, kdf, fillna=False, how=\"full\",\n                        )\n\n            concat_kdf = concat_kdf[column_labels]\n\n        if ignore_index:\n            concat_kdf.columns = list(map(str, _range(len(concat_kdf.columns))))\n\n        if sort:\n            concat_kdf = concat_kdf.sort_index()\n\n        return concat_kdf\n\n    # Series, Series ...\n    # We should return Series if objects are all Series.\n    should_return_series = all(map(lambda obj: isinstance(obj, Series), objs))\n\n    # DataFrame, Series ... & Series, Series ...\n    # In this case, we should return DataFrame.\n    new_objs = []\n    num_series = 0\n    series_names = set()\n    for obj in objs:\n        if isinstance(obj, Series):\n            num_series += 1\n            series_names.add(obj.name)\n            obj = obj.to_frame(DEFAULT_SERIES_NAME)\n        new_objs.append(obj)\n    objs = new_objs\n\n    column_labels_levels = set(obj._internal.column_labels_level for obj in objs)\n    if len(column_labels_levels) != 1:\n        raise ValueError(\"MultiIndex columns should have the same levels\")\n\n    # DataFrame, DataFrame, ...\n    # All Series are converted into DataFrame and then compute concat.\n    if not ignore_index:\n        indices_of_kdfs = [kdf.index for kdf in objs]\n        index_of_first_kdf = indices_of_kdfs[0]\n        for index_of_kdf in indices_of_kdfs:\n            if index_of_first_kdf.names != index_of_kdf.names:\n                raise ValueError(\n                    \"Index type and names should be same in the objects to concatenate. \"\n                    \"You passed different indices \"\n                    \"{index_of_first_kdf} and {index_of_kdf}\".format(\n                        index_of_first_kdf=index_of_first_kdf.names, index_of_kdf=index_of_kdf.names\n                    )\n                )\n\n    column_labels_of_kdfs = [kdf._internal.column_labels for kdf in objs]\n    if ignore_index:\n        index_names_of_kdfs = [[] for _ in objs]\n    else:\n        index_names_of_kdfs = [kdf._internal.index_names for kdf in objs]\n\n    if all(name == index_names_of_kdfs[0] for name in index_names_of_kdfs) and all(\n        idx == column_labels_of_kdfs[0] for idx in column_labels_of_kdfs\n    ):\n        # If all columns are in the same order and values, use it.\n        kdfs = objs\n    else:\n        if join == \"inner\":\n            interested_columns = set.intersection(*map(set, column_labels_of_kdfs))\n            # Keep the column order with its firsts DataFrame.\n            merged_columns = [\n                label for label in column_labels_of_kdfs[0] if label in interested_columns\n            ]\n\n            # When multi-index column, although pandas is flaky if `join=\"inner\" and sort=False`,\n            # always sort to follow the `join=\"outer\"` case behavior.\n            if (len(merged_columns) > 0 and len(merged_columns[0]) > 1) or sort:\n                # FIXME: better ordering\n                merged_columns = sorted(merged_columns, key=name_like_string)\n\n            kdfs = [kdf[merged_columns] for kdf in objs]\n        elif join == \"outer\":\n            merged_columns = []\n            for labels in column_labels_of_kdfs:\n                merged_columns.extend(label for label in labels if label not in merged_columns)\n\n            assert len(merged_columns) > 0\n\n            if LooseVersion(pd.__version__) < LooseVersion(\"0.24\"):\n                # Always sort when multi-index columns, and if there are Series, never sort.\n                sort = len(merged_columns[0]) > 1 or (num_series == 0 and sort)\n            else:\n                # Always sort when multi-index columns or there are more than two Series,\n                # and if there is only one Series, never sort.\n                sort = len(merged_columns[0]) > 1 or num_series > 1 or (num_series != 1 and sort)\n\n            if sort:\n                # FIXME: better ordering\n                merged_columns = sorted(merged_columns, key=name_like_string)\n\n            kdfs = []\n            for kdf in objs:\n                columns_to_add = list(set(merged_columns) - set(kdf._internal.column_labels))\n\n                # TODO: NaN and None difference for missing values. pandas seems filling NaN.\n                sdf = kdf._internal.resolved_copy.spark_frame\n                for label in columns_to_add:\n                    sdf = sdf.withColumn(name_like_string(label), F.lit(None))\n\n                data_columns = kdf._internal.data_spark_column_names + [\n                    name_like_string(label) for label in columns_to_add\n                ]\n                kdf = DataFrame(\n                    kdf._internal.copy(\n                        spark_frame=sdf,\n                        column_labels=(kdf._internal.column_labels + columns_to_add),\n                        data_spark_columns=[scol_for(sdf, col) for col in data_columns],\n                    )\n                )\n\n                kdfs.append(kdf[merged_columns])\n\n    if ignore_index:\n        sdfs = [kdf._internal.spark_frame.select(kdf._internal.data_spark_columns) for kdf in kdfs]\n    else:\n        sdfs = [\n            kdf._internal.spark_frame.select(\n                kdf._internal.index_spark_columns + kdf._internal.data_spark_columns\n            )\n            for kdf in kdfs\n        ]\n    concatenated = reduce(lambda x, y: x.union(y), sdfs)\n\n    if ignore_index:\n        index_spark_column_names = None\n        index_names = None\n    else:\n        index_spark_column_names = kdfs[0]._internal.index_spark_column_names\n        index_names = kdfs[0]._internal.index_names\n\n    result_kdf = DataFrame(\n        kdfs[0]._internal.copy(\n            spark_frame=concatenated,\n            index_spark_column_names=index_spark_column_names,\n            index_names=index_names,\n            data_spark_columns=[\n                scol_for(concatenated, col) for col in kdfs[0]._internal.data_spark_column_names\n            ],\n        )\n    )\n\n    if should_return_series:\n        # If all input were Series, we should return Series.\n        if len(series_names) == 1:\n            name = series_names.pop()\n        else:\n            name = None\n        return first_series(result_kdf).rename(name)\n    else:\n        return result_kdf\n\n\ndef melt(frame, id_vars=None, value_vars=None, var_name=None, value_name=\"value\"):\n    return DataFrame.melt(frame, id_vars, value_vars, var_name, value_name)\n\n\nmelt.__doc__ = DataFrame.melt.__doc__\n\n\ndef isna(obj):\n    \"\"\"\n    Detect missing values for an array-like object.\n\n    This function takes a scalar or array-like object and indicates\n    whether values are missing (``NaN`` in numeric arrays, ``None`` or ``NaN``\n    in object arrays).\n\n    Parameters\n    ----------\n    obj : scalar or array-like\n        Object to check for null or missing values.\n\n    Returns\n    -------\n    bool or array-like of bool\n        For scalar input, returns a scalar boolean.\n        For array input, returns an array of boolean indicating whether each\n        corresponding element is missing.\n\n    See Also\n    --------\n    Series.isna : Detect missing values in a Series.\n    Series.isnull : Detect missing values in a Series.\n    DataFrame.isna : Detect missing values in a DataFrame.\n    DataFrame.isnull : Detect missing values in a DataFrame.\n    Index.isna : Detect missing values in an Index.\n    Index.isnull : Detect missing values in an Index.\n\n    Examples\n    --------\n    Scalar arguments (including strings) result in a scalar boolean.\n\n    >>> ks.isna('dog')\n    False\n\n    >>> ks.isna(np.nan)\n    True\n\n    ndarrays result in an ndarray of booleans.\n\n    >>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n    >>> array\n    array([[ 1., nan,  3.],\n           [ 4.,  5., nan]])\n    >>> ks.isna(array)\n    array([[False,  True, False],\n           [False, False,  True]])\n\n    For Series and DataFrame, the same type is returned, containing booleans.\n\n    >>> df = ks.DataFrame({'a': ['ant', 'bee', 'cat'], 'b': ['dog', None, 'fly']})\n    >>> df\n         a     b\n    0  ant   dog\n    1  bee  None\n    2  cat   fly\n\n    >>> ks.isna(df)\n           a      b\n    0  False  False\n    1  False   True\n    2  False  False\n\n    >>> ks.isnull(df.b)\n    0    False\n    1     True\n    2    False\n    Name: b, dtype: bool\n    \"\"\"\n    # TODO: Add back:\n    #     notnull : Boolean inverse of pandas.isnull.\n    #   into the See Also in the docstring. It does not find the method in the latest numpydoc.\n    if isinstance(obj, (DataFrame, Series)):\n        return obj.isnull()\n    else:\n        return pd.isnull(obj)\n\n\nisnull = isna\n\n\ndef notna(obj):\n    \"\"\"\n    Detect existing (non-missing) values.\n\n    Return a boolean same-sized object indicating if the values are not NA.\n    Non-missing values get mapped to True. NA values, such as None or\n    :attr:`numpy.NaN`, get mapped to False values.\n\n    Returns\n    -------\n    bool or array-like of bool\n        Mask of bool values for each element that\n        indicates whether an element is not an NA value.\n\n    See Also\n    --------\n    isna : Detect missing values for an array-like object.\n    Series.notna : Boolean inverse of Series.isna.\n    DataFrame.notnull : Boolean inverse of DataFrame.isnull.\n    Index.notna : Boolean inverse of Index.isna.\n    Index.notnull : Boolean inverse of Index.isnull.\n\n    Examples\n    --------\n    Show which entries in a DataFrame are not NA.\n\n    >>> df = ks.DataFrame({'age': [5, 6, np.NaN],\n    ...                    'born': [pd.NaT, pd.Timestamp('1939-05-27'),\n    ...                             pd.Timestamp('1940-04-25')],\n    ...                    'name': ['Alfred', 'Batman', ''],\n    ...                    'toy': [None, 'Batmobile', 'Joker']})\n    >>> df\n       age       born    name        toy\n    0  5.0        NaT  Alfred       None\n    1  6.0 1939-05-27  Batman  Batmobile\n    2  NaN 1940-04-25              Joker\n\n    >>> df.notnull()\n         age   born  name    toy\n    0   True  False  True  False\n    1   True   True  True   True\n    2  False   True  True   True\n\n    Show which entries in a Series are not NA.\n\n    >>> ser = ks.Series([5, 6, np.NaN])\n    >>> ser\n    0    5.0\n    1    6.0\n    2    NaN\n    dtype: float64\n\n    >>> ks.notna(ser)\n    0     True\n    1     True\n    2    False\n    dtype: bool\n\n    >>> ks.notna(ser.index)\n    True\n    \"\"\"\n    # TODO: Add back:\n    #     Series.notnull :Boolean inverse of Series.isnull.\n    #     DataFrame.notna :Boolean inverse of DataFrame.isna.\n    #   into the See Also in the docstring. It does not find the method in the latest numpydoc.\n    if isinstance(obj, (DataFrame, Series)):\n        return obj.notna()\n    else:\n        return pd.notna(obj)\n\n\nnotnull = notna\n\n\ndef merge(\n    obj,\n    right: \"DataFrame\",\n    how: str = \"inner\",\n    on: Union[Any, List[Any], Tuple, List[Tuple]] = None,\n    left_on: Union[Any, List[Any], Tuple, List[Tuple]] = None,\n    right_on: Union[Any, List[Any], Tuple, List[Tuple]] = None,\n    left_index: bool = False,\n    right_index: bool = False,\n    suffixes: Tuple[str, str] = (\"_x\", \"_y\"),\n) -> \"DataFrame\":\n    \"\"\"\n    Merge DataFrame objects with a database-style join.\n\n    The index of the resulting DataFrame will be one of the following:\n        - 0...n if no index is used for merging\n        - Index of the left DataFrame if merged only on the index of the right DataFrame\n        - Index of the right DataFrame if merged only on the index of the left DataFrame\n        - All involved indices if merged using the indices of both DataFrames\n            e.g. if `left` with indices (a, x) and `right` with indices (b, x), the result will\n            be an index (x, a, b)\n\n    Parameters\n    ----------\n    right: Object to merge with.\n    how: Type of merge to be performed.\n        {'left', 'right', 'outer', 'inner'}, default 'inner'\n\n        left: use only keys from left frame, similar to a SQL left outer join; preserve key\n            order.\n        right: use only keys from right frame, similar to a SQL right outer join; preserve key\n            order.\n        outer: use union of keys from both frames, similar to a SQL full outer join; sort keys\n            lexicographically.\n        inner: use intersection of keys from both frames, similar to a SQL inner join;\n            preserve the order of the left keys.\n    on: Column or index level names to join on. These must be found in both DataFrames. If on\n        is None and not merging on indexes then this defaults to the intersection of the\n        columns in both DataFrames.\n    left_on: Column or index level names to join on in the left DataFrame. Can also\n        be an array or list of arrays of the length of the left DataFrame.\n        These arrays are treated as if they are columns.\n    right_on: Column or index level names to join on in the right DataFrame. Can also\n        be an array or list of arrays of the length of the right DataFrame.\n        These arrays are treated as if they are columns.\n    left_index: Use the index from the left DataFrame as the join key(s). If it is a\n        MultiIndex, the number of keys in the other DataFrame (either the index or a number of\n        columns) must match the number of levels.\n    right_index: Use the index from the right DataFrame as the join key. Same caveats as\n        left_index.\n    suffixes: Suffix to apply to overlapping column names in the left and right side,\n        respectively.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame of the two merged objects.\n\n    Examples\n    --------\n\n    >>> df1 = ks.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n    ...                     'value': [1, 2, 3, 5]},\n    ...                    columns=['lkey', 'value'])\n    >>> df2 = ks.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n    ...                     'value': [5, 6, 7, 8]},\n    ...                    columns=['rkey', 'value'])\n    >>> df1\n      lkey  value\n    0  foo      1\n    1  bar      2\n    2  baz      3\n    3  foo      5\n    >>> df2\n      rkey  value\n    0  foo      5\n    1  bar      6\n    2  baz      7\n    3  foo      8\n\n    Merge df1 and df2 on the lkey and rkey columns. The value columns have\n    the default suffixes, _x and _y, appended.\n\n    >>> merged = ks.merge(df1, df2, left_on='lkey', right_on='rkey')\n    >>> merged.sort_values(by=['lkey', 'value_x', 'rkey', 'value_y'])  # doctest: +ELLIPSIS\n      lkey  value_x rkey  value_y\n    ...bar        2  bar        6\n    ...baz        3  baz        7\n    ...foo        1  foo        5\n    ...foo        1  foo        8\n    ...foo        5  foo        5\n    ...foo        5  foo        8\n\n    >>> left_kdf = ks.DataFrame({'A': [1, 2]})\n    >>> right_kdf = ks.DataFrame({'B': ['x', 'y']}, index=[1, 2])\n\n    >>> ks.merge(left_kdf, right_kdf, left_index=True, right_index=True).sort_index()\n       A  B\n    1  2  x\n\n    >>> ks.merge(left_kdf, right_kdf, left_index=True, right_index=True, how='left').sort_index()\n       A     B\n    0  1  None\n    1  2     x\n\n    >>> ks.merge(left_kdf, right_kdf, left_index=True, right_index=True, how='right').sort_index()\n         A  B\n    1  2.0  x\n    2  NaN  y\n\n    >>> ks.merge(left_kdf, right_kdf, left_index=True, right_index=True, how='outer').sort_index()\n         A     B\n    0  1.0  None\n    1  2.0     x\n    2  NaN     y\n\n    Notes\n    -----\n    As described in #263, joining string columns currently returns None for missing values\n        instead of NaN.\n    \"\"\"\n    return obj.merge(\n        right,\n        how=how,\n        on=on,\n        left_on=left_on,\n        right_on=right_on,\n        left_index=left_index,\n        right_index=right_index,\n        suffixes=suffixes,\n    )\n\n\ndef to_numeric(arg):\n    \"\"\"\n    Convert argument to a numeric type.\n\n    Parameters\n    ----------\n    arg : scalar, list, tuple, 1-d array, or Series\n\n    Returns\n    -------\n    ret : numeric if parsing succeeded.\n\n    See Also\n    --------\n    DataFrame.astype : Cast argument to a specified dtype.\n    to_datetime : Convert argument to datetime.\n    to_timedelta : Convert argument to timedelta.\n    numpy.ndarray.astype : Cast a numpy array to a specified type.\n\n    Examples\n    --------\n\n    >>> kser = ks.Series(['1.0', '2', '-3'])\n    >>> kser\n    0    1.0\n    1      2\n    2     -3\n    dtype: object\n\n    >>> ks.to_numeric(kser)\n    0    1.0\n    1    2.0\n    2   -3.0\n    dtype: float32\n\n    If given Series contains invalid value to cast float, just cast it to `np.nan`\n\n    >>> kser = ks.Series(['apple', '1.0', '2', '-3'])\n    >>> kser\n    0    apple\n    1      1.0\n    2        2\n    3       -3\n    dtype: object\n\n    >>> ks.to_numeric(kser)\n    0    NaN\n    1    1.0\n    2    2.0\n    3   -3.0\n    dtype: float32\n\n    Also support for list, tuple, np.array, or a scalar\n\n    >>> ks.to_numeric(['1.0', '2', '-3'])\n    array([ 1.,  2., -3.])\n\n    >>> ks.to_numeric(('1.0', '2', '-3'))\n    array([ 1.,  2., -3.])\n\n    >>> ks.to_numeric(np.array(['1.0', '2', '-3']))\n    array([ 1.,  2., -3.])\n\n    >>> ks.to_numeric('1.0')\n    1.0\n    \"\"\"\n    if isinstance(arg, Series):\n        return arg._with_new_scol(arg.spark.column.cast(\"float\"))\n    else:\n        return pd.to_numeric(arg)\n\n\ndef broadcast(obj):\n    \"\"\"\n    Marks a DataFrame as small enough for use in broadcast joins.\n\n    Parameters\n    ----------\n    obj : DataFrame\n\n    Returns\n    -------\n    ret : DataFrame with broadcast hint.\n\n    See Also\n    --------\n    DataFrame.merge : Merge DataFrame objects with a database-style join.\n    DataFrame.join : Join columns of another DataFrame.\n    DataFrame.update : Modify in place using non-NA values from another DataFrame.\n    DataFrame.hint : Specifies some hint on the current DataFrame.\n\n    Examples\n    --------\n    >>> df1 = ks.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n    ...                     'value': [1, 2, 3, 5]},\n    ...                    columns=['lkey', 'value']).set_index('lkey')\n    >>> df2 = ks.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n    ...                     'value': [5, 6, 7, 8]},\n    ...                    columns=['rkey', 'value']).set_index('rkey')\n    >>> merged = df1.merge(ks.broadcast(df2), left_index=True, right_index=True)\n    >>> merged.spark.explain()  # doctest: +ELLIPSIS\n    == Physical Plan ==\n    ...\n    ...BroadcastHashJoin...\n    ...\n    \"\"\"\n    if not isinstance(obj, DataFrame):\n        raise ValueError(\"Invalid type : expected DataFrame got {}\".format(type(obj).__name__))\n    return DataFrame(obj._internal.with_new_sdf(F.broadcast(obj._internal.spark_frame)))\n\n\ndef _get_index_map(\n    sdf: spark.DataFrame, index_col: Optional[Union[str, List[str]]] = None\n) -> Tuple[Optional[List[str]], Optional[List[Tuple]]]:\n    if index_col is not None:\n        if isinstance(index_col, str):\n            index_col = [index_col]\n        sdf_columns = set(sdf.columns)\n        for col in index_col:\n            if col not in sdf_columns:\n                raise KeyError(col)\n        index_spark_column_names = index_col  # type: Optional[List[str]]\n        index_names = [(col,) for col in index_col]  # type: Optional[List[Tuple]]\n    else:\n        index_spark_column_names = None\n        index_names = None\n\n    return index_spark_column_names, index_names\n\n\n_get_dummies_default_accept_types = (DecimalType, StringType, DateType)\n_get_dummies_acceptable_types = _get_dummies_default_accept_types + (\n    ByteType,\n    ShortType,\n    IntegerType,\n    LongType,\n    FloatType,\n    DoubleType,\n    BooleanType,\n    TimestampType,\n)\n", "idx": 17, "id": 16925, "msg": "", "proj": "databricks-koalas", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -355,7 +355,7 @@ func (j mdJournal) getLatest(verifyBranchID bool) (\n \treturn MakeImmutableBareRootMetadata(latest, latestID, ts), nil\n }\n \n-func (j mdJournal) checkGetParams(extra ExtraMetadata) (\n+func (j mdJournal) checkGetParams() (\n \tImmutableBareRootMetadata, error) {\n \thead, err := j.getLatest(true)\n \tif err != nil {", "y": 0, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/logger\"\n\t\"github.com/keybase/kbfs/kbfscodec\"\n\n\t\"golang.org/x/net/context\"\n\n\t\"github.com/keybase/client/go/protocol/keybase1\"\n)\n\n// ImmutableBareRootMetadata is a thin wrapper around a\n// BareRootMetadata that takes ownership of it and does not ever\n// modify it again. Thus, its MdID can be calculated and\n// stored along with a local timestamp. ImmutableBareRootMetadata\n// objects can be assumed to never alias a (modifiable) BareRootMetadata.\n//\n// Note that crypto.MakeMdID() on an ImmutableBareRootMetadata will\n// compute the wrong result, since anonymous fields of interface type\n// are not encoded inline by the codec. Use\n// crypto.MakeMDID(ibrmd.BareRootMetadata) instead.\n//\n// TODO: Move this to bare_root_metadata.go if it's used in more\n// places.\ntype ImmutableBareRootMetadata struct {\n\tBareRootMetadata\n\tmdID           MdID\n\tlocalTimestamp time.Time\n}\n\n// MakeImmutableBareRootMetadata makes a new ImmutableBareRootMetadata\n// from the given BareRootMetadata and its corresponding MdID.\nfunc MakeImmutableBareRootMetadata(\n\trmd BareRootMetadata, mdID MdID,\n\tlocalTimestamp time.Time) ImmutableBareRootMetadata {\n\tif mdID == (MdID{}) {\n\t\tpanic(\"zero mdID passed to MakeImmutableBareRootMetadata\")\n\t}\n\treturn ImmutableBareRootMetadata{rmd, mdID, localTimestamp}\n}\n\n// mdJournal stores a single ordered list of metadata IDs for a (TLF,\n// user, device) tuple, along with the associated metadata objects, in\n// flat files on disk.\n//\n// The directory layout looks like:\n//\n// dir/md_journal/EARLIEST\n// dir/md_journal/LATEST\n// dir/md_journal/0...001\n// dir/md_journal/0...002\n// dir/md_journal/0...fff\n// dir/mds/0100/0...01/data\n// ...\n// dir/mds/01ff/f...ff/data\n//\n// There's a single journal subdirectory; the journal ordinals are\n// just MetadataRevisions, and the journal entries are just MdIDs.\n//\n// The Metadata objects are stored separately in dir/mds. Each MD has\n// its own subdirectory with its ID truncated to 17 bytes (34\n// characters) as a name. The MD subdirectories are splayed over (# of\n// possible hash types) * 256 subdirectories -- one byte for the hash\n// type (currently only one) plus the first byte of the hash data --\n// using the first four characters of the name to keep the number of\n// directories in dir itself to a manageable number, similar to git.\n// Each block directory has data, which is the raw MD data that should\n// hash to the MD ID. Future versions of the journal might add more\n// files to this directory; if any code is written to move MDs around,\n// it should be careful to preserve any unknown files in an MD\n// directory.\n//\n// The maximum number of characters added to the root dir by an MD\n// journal is 45:\n//\n//   /mds/01ff/f...(30 characters total)...ff/data\n//\n// This covers even the temporary files created in convertToBranch,\n// which create paths like\n//\n//   /md_journal123456789/0...(16 characters total)...001\n//\n// which have only 37 characters.\n//\n// mdJournal is not goroutine-safe, so any code that uses it must\n// guarantee that only one goroutine at a time calls its functions.\ntype mdJournal struct {\n\t// key is assumed to be the VerifyingKey of a device owned by\n\t// uid, and both uid and key are assumed constant for the\n\t// lifetime of this object.\n\tuid keybase1.UID\n\tkey VerifyingKey\n\n\tcodec  kbfscodec.Codec\n\tcrypto cryptoPure\n\tdir    string\n\n\tlog      logger.Logger\n\tdeferLog logger.Logger\n\n\tj mdIDJournal\n\n\t// This doesn't need to be persisted, even if the journal\n\t// becomes empty, since on a restart the branch ID is\n\t// retrieved from the server (via GetUnmergedForTLF).\n\tbranchID BranchID\n\n\t// Set only when the journal becomes empty due to\n\t// flushing. This doesn't need to be persisted for the same\n\t// reason as branchID.\n\tlastMdID MdID\n}\n\nfunc makeMDJournal(uid keybase1.UID, key VerifyingKey, codec kbfscodec.Codec,\n\tcrypto cryptoPure, dir string, log logger.Logger) (*mdJournal, error) {\n\tif uid == keybase1.UID(\"\") {\n\t\treturn nil, errors.New(\"Empty user\")\n\t}\n\tif key == (VerifyingKey{}) {\n\t\treturn nil, errors.New(\"Empty verifying key\")\n\t}\n\n\tjournalDir := filepath.Join(dir, \"md_journal\")\n\n\tdeferLog := log.CloneWithAddedDepth(1)\n\tjournal := mdJournal{\n\t\tuid:      uid,\n\t\tkey:      key,\n\t\tcodec:    codec,\n\t\tcrypto:   crypto,\n\t\tdir:      dir,\n\t\tlog:      log,\n\t\tdeferLog: deferLog,\n\t\tj:        makeMdIDJournal(codec, journalDir),\n\t}\n\n\tearliest, err := journal.getEarliest(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlatest, err := journal.getLatest(false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif (earliest == ImmutableBareRootMetadata{}) !=\n\t\t(latest == ImmutableBareRootMetadata{}) {\n\t\treturn nil, fmt.Errorf(\"has earliest=%t != has latest=%t\",\n\t\t\tearliest != ImmutableBareRootMetadata{},\n\t\t\tlatest != ImmutableBareRootMetadata{})\n\t}\n\n\tif earliest != (ImmutableBareRootMetadata{}) {\n\t\tif earliest.BID() != latest.BID() {\n\t\t\treturn nil, fmt.Errorf(\n\t\t\t\t\"earliest.BID=%s != latest.BID=%s\",\n\t\t\t\tearliest.BID(), latest.BID())\n\t\t}\n\t\tjournal.branchID = earliest.BID()\n\t}\n\n\treturn &journal, nil\n}\n\n// The functions below are for building various paths.\n\nfunc (j mdJournal) mdsPath() string {\n\treturn filepath.Join(j.dir, \"mds\")\n}\n\nfunc (j mdJournal) mdPath(id MdID) string {\n\t// Truncate to 34 characters, which corresponds to 16 random\n\t// bytes (since the first byte is a hash type) or 128 random\n\t// bits, which means that the expected number of MDs generated\n\t// before getting a path collision is 2^64 (see\n\t// https://en.wikipedia.org/wiki/Birthday_problem#Cast_as_a_collision_problem\n\t// ). The full ID can be recovered just by hashing the data\n\t// again with the same hash type.\n\tidStr := id.String()\n\treturn filepath.Join(j.mdsPath(), idStr[:4], idStr[4:34])\n}\n\nfunc (j mdJournal) mdDataPath(id MdID) string {\n\treturn filepath.Join(j.mdPath(id), \"data\")\n}\n\n// getMD verifies the MD data and the writer signature (but not the\n// key) for the given ID and returns it. It also returns the\n// last-modified timestamp of the file. verifyBranchID should be false\n// only when called from makeMDJournal, i.e. when figuring out what to\n// set j.branchID in the first place.\nfunc (j mdJournal) getMD(id MdID, verifyBranchID bool) (\n\tBareRootMetadata, time.Time, error) {\n\t// Read data.\n\n\tdata, err := ioutil.ReadFile(j.mdDataPath(id))\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\t// TODO: Read version info.\n\tvar rmd BareRootMetadataV2\n\terr = j.codec.Decode(data, &rmd)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\t// Check integrity.\n\n\t// TODO: MakeMdID serializes rmd -- use data instead.\n\tmdID, err := j.crypto.MakeMdID(&rmd)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tif mdID != id {\n\t\treturn nil, time.Time{}, fmt.Errorf(\n\t\t\t\"Metadata ID mismatch: expected %s, got %s\", id, mdID)\n\t}\n\n\terr = rmd.IsLastModifiedBy(j.uid, j.key)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\t// MDv3 TODO: pass key bundles when needed\n\terr = rmd.IsValidAndSigned(j.codec, j.crypto, nil)\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\tif verifyBranchID && rmd.BID() != j.branchID {\n\t\treturn nil, time.Time{}, fmt.Errorf(\n\t\t\t\"Branch ID mismatch: expected %s, got %s\",\n\t\t\tj.branchID, rmd.BID())\n\t}\n\n\tfi, err := os.Stat(j.mdPath(id))\n\tif err != nil {\n\t\treturn nil, time.Time{}, err\n\t}\n\n\treturn &rmd, fi.ModTime(), nil\n}\n\n// putMD stores the given metadata under its ID, if it's not already\n// stored.\nfunc (j mdJournal) putMD(rmd BareRootMetadata) (MdID, error) {\n\t// MDv3 TODO: pass key bundles when needed\n\terr := rmd.IsValidAndSigned(j.codec, j.crypto, nil)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\terr = rmd.IsLastModifiedBy(j.uid, j.key)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\tid, err := j.crypto.MakeMdID(rmd)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\t_, _, err = j.getMD(id, true)\n\tif os.IsNotExist(err) {\n\t\t// Continue on.\n\t} else if err != nil {\n\t\treturn MdID{}, err\n\t} else {\n\t\t// Entry exists, so nothing else to do.\n\t\treturn MdID{}, nil\n\t}\n\n\tbuf, err := j.codec.Encode(rmd)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\terr = os.MkdirAll(j.mdPath(id), 0700)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\t// TODO: Write version info.\n\n\terr = ioutil.WriteFile(j.mdDataPath(id), buf, 0600)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\treturn id, nil\n}\n\n// removeMD removes the metadata (which must exist) with the given ID.\nfunc (j *mdJournal) removeMD(id MdID) error {\n\tpath := j.mdPath(id)\n\terr := os.RemoveAll(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Remove the parent (splayed) directory (which should exist)\n\t// if it's empty.\n\terr = os.Remove(filepath.Dir(path))\n\tif isExist(err) {\n\t\terr = nil\n\t}\n\treturn err\n}\n\nfunc (j mdJournal) getEarliest(verifyBranchID bool) (\n\tImmutableBareRootMetadata, error) {\n\tentry, exists, err := j.j.getEarliestEntry()\n\tif err != nil {\n\t\treturn ImmutableBareRootMetadata{}, err\n\t}\n\tif !exists {\n\t\treturn ImmutableBareRootMetadata{}, nil\n\t}\n\tearliestID := entry.ID\n\tearliest, ts, err := j.getMD(earliestID, verifyBranchID)\n\tif err != nil {\n\t\treturn ImmutableBareRootMetadata{}, err\n\t}\n\treturn MakeImmutableBareRootMetadata(earliest, earliestID, ts), nil\n}\n\nfunc (j mdJournal) getLatest(verifyBranchID bool) (\n\tImmutableBareRootMetadata, error) {\n\tentry, exists, err := j.j.getLatestEntry()\n\tif err != nil {\n\t\treturn ImmutableBareRootMetadata{}, err\n\t}\n\tif !exists {\n\t\treturn ImmutableBareRootMetadata{}, nil\n\t}\n\tlatestID := entry.ID\n\tlatest, ts, err := j.getMD(latestID, verifyBranchID)\n\tif err != nil {\n\t\treturn ImmutableBareRootMetadata{}, err\n\t}\n\treturn MakeImmutableBareRootMetadata(latest, latestID, ts), nil\n}\n\nfunc (j mdJournal) checkGetParams(extra ExtraMetadata) (\n\tImmutableBareRootMetadata, error) {\n\thead, err := j.getLatest(true)\n\tif err != nil {\n\t\treturn ImmutableBareRootMetadata{}, err\n\t}\n\n\tif head != (ImmutableBareRootMetadata{}) {\n\t\tok, err := isReader(j.uid, head.BareRootMetadata, extra)\n\t\tif err != nil {\n\t\t\treturn ImmutableBareRootMetadata{}, err\n\t\t}\n\t\tif !ok {\n\t\t\t// TODO: Use a non-server error.\n\t\t\treturn ImmutableBareRootMetadata{},\n\t\t\t\tMDServerErrorUnauthorized{}\n\t\t}\n\t}\n\n\treturn head, nil\n}\n\nfunc (j *mdJournal) convertToBranch(\n\tctx context.Context, signer cryptoSigner,\n\ttlfID TlfID, mdcache MDCache) (bid BranchID, err error) {\n\tif j.branchID != NullBranchID {\n\t\treturn NullBranchID, fmt.Errorf(\n\t\t\t\"convertToBranch called with BID=%s\", j.branchID)\n\t}\n\n\tearliestRevision, err := j.j.readEarliestRevision()\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tlatestRevision, err := j.j.readLatestRevision()\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tj.log.CDebugf(\n\t\tctx, \"rewriting MDs %s to %s\", earliestRevision, latestRevision)\n\n\t_, allEntries, err := j.j.getEntryRange(\n\t\tearliestRevision, latestRevision)\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tbid, err = j.crypto.MakeRandomBranchID()\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tj.log.CDebugf(ctx, \"New branch ID=%s\", bid)\n\n\tjournalTempDir, err := ioutil.TempDir(j.dir, \"md_journal\")\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\tj.log.CDebugf(ctx, \"Using temp dir %s for rewriting\", journalTempDir)\n\n\tmdsToRemove := make([]MdID, 0, len(allEntries))\n\tdefer func() {\n\t\tj.log.CDebugf(ctx, \"Removing temp dir %s and %d old MDs\",\n\t\t\tjournalTempDir, len(mdsToRemove))\n\t\tremoveErr := os.RemoveAll(journalTempDir)\n\t\tif removeErr != nil {\n\t\t\tj.log.CWarningf(ctx,\n\t\t\t\t\"Error when removing temp dir %s: %v\",\n\t\t\t\tjournalTempDir, removeErr)\n\t\t}\n\t\t// Garbage-collect the unnecessary MD entries.  TODO: we'll\n\t\t// eventually need a sweeper to clean up entries left behind\n\t\t// if we crash here.\n\t\tfor _, id := range mdsToRemove {\n\t\t\tremoveErr := j.removeMD(id)\n\t\t\tif removeErr != nil {\n\t\t\t\tj.log.CWarningf(ctx, \"Error when removing old MD %s: %v\",\n\t\t\t\t\tid, removeErr)\n\t\t\t}\n\t\t}\n\t}()\n\n\ttempJournal := makeMdIDJournal(j.codec, journalTempDir)\n\n\tvar prevID MdID\n\n\tfor i, entry := range allEntries {\n\t\tibrmd, _, err := j.getMD(entry.ID, true)\n\t\tif err != nil {\n\t\t\treturn NullBranchID, err\n\t\t}\n\t\tbrmd, ok := ibrmd.(MutableBareRootMetadata)\n\t\tif !ok {\n\t\t\treturn NullBranchID, MutableBareRootMetadataNoImplError{}\n\t\t}\n\t\tbrmd.SetUnmerged()\n\t\tbrmd.SetBranchID(bid)\n\n\t\t// Delete the old \"merged\" version from the cache.\n\t\tmdcache.Delete(tlfID, ibrmd.RevisionNumber(), NullBranchID)\n\n\t\t// Re-sign the writer metadata.\n\t\tbuf, err := brmd.GetSerializedWriterMetadata(j.codec)\n\t\tif err != nil {\n\t\t\treturn NullBranchID, err\n\t\t}\n\n\t\tsigInfo, err := signer.Sign(ctx, buf)\n\t\tif err != nil {\n\t\t\treturn NullBranchID, err\n\t\t}\n\t\tbrmd.SetWriterMetadataSigInfo(sigInfo)\n\n\t\tj.log.CDebugf(ctx, \"Old prev root of rev=%s is %s\",\n\t\t\tbrmd.RevisionNumber(), brmd.GetPrevRoot())\n\n\t\tif i > 0 {\n\t\t\tj.log.CDebugf(ctx, \"Changing prev root of rev=%s to %s\",\n\t\t\t\tbrmd.RevisionNumber(), prevID)\n\t\t\tbrmd.SetPrevRoot(prevID)\n\t\t}\n\n\t\t// TODO: this rewrites the file, and so the modification time\n\t\t// no longer tracks when exactly the original operation is\n\t\t// done, so future ImmutableBareMetadatas for this MD will\n\t\t// have a slightly wrong localTimestamp.  Instead, we might\n\t\t// want to pass in the timestamp and do an explicit\n\t\t// os.Chtimes() on the file after writing it.\n\t\tnewID, err := j.putMD(brmd)\n\t\tif err != nil {\n\t\t\treturn NullBranchID, err\n\t\t}\n\t\tmdsToRemove = append(mdsToRemove, newID)\n\n\t\t// TODO: Try and preserve unknown fields from the old\n\t\t// journal.\n\t\terr = tempJournal.append(\n\t\t\tbrmd.RevisionNumber(), mdIDJournalEntry{ID: newID})\n\t\tif err != nil {\n\t\t\treturn NullBranchID, err\n\t\t}\n\n\t\tprevID = newID\n\n\t\tj.log.CDebugf(ctx, \"Changing ID for rev=%s from %s to %s\",\n\t\t\tbrmd.RevisionNumber(), entry.ID, newID)\n\t}\n\n\t// TODO: Do the below atomically on the filesystem\n\t// level. Specifically, make \"md_journal\" always be a symlink,\n\t// and then perform the swap by atomically changing the\n\t// symlink to point to the new journal directory.\n\n\toldJournalTempDir := journalTempDir + \".old\"\n\tdir, err := j.j.move(oldJournalTempDir)\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tj.log.CDebugf(ctx, \"Moved old journal from %s to %s\",\n\t\tdir, oldJournalTempDir)\n\n\tnewJournalOldDir, err := tempJournal.move(dir)\n\tif err != nil {\n\t\treturn NullBranchID, err\n\t}\n\n\tj.log.CDebugf(ctx, \"Moved new journal from %s to %s\",\n\t\tnewJournalOldDir, dir)\n\n\t// Make the defer block above remove oldJournalTempDir.\n\tjournalTempDir = oldJournalTempDir\n\n\tmdsToRemove = nil\n\tfor _, entry := range allEntries {\n\t\tmdsToRemove = append(mdsToRemove, entry.ID)\n\t}\n\n\tj.j = tempJournal\n\tj.branchID = bid\n\n\treturn bid, nil\n}\n\n// getNextEntryToFlush returns the info for the next journal entry to\n// flush, if it exists, and its revision is less than end. If there is\n// no next journal entry to flush, the returned MdID will be zero, and\n// the returned *RootMetadataSigned will be nil.\nfunc (j mdJournal) getNextEntryToFlush(\n\tctx context.Context, end MetadataRevision, signer cryptoSigner) (\n\tMdID, *RootMetadataSigned, error) {\n\trmd, err := j.getEarliest(true)\n\tif err != nil {\n\t\treturn MdID{}, nil, err\n\t}\n\tif rmd == (ImmutableBareRootMetadata{}) || rmd.RevisionNumber() >= end {\n\t\treturn MdID{}, nil, nil\n\t}\n\n\tmbrmd, ok := rmd.BareRootMetadata.(MutableBareRootMetadata)\n\tif !ok {\n\t\treturn MdID{}, nil, MutableBareRootMetadataNoImplError{}\n\t}\n\n\trmds := RootMetadataSigned{MD: mbrmd}\n\terr = signMD(ctx, j.codec, signer, &rmds)\n\tif err != nil {\n\t\treturn MdID{}, nil, err\n\t}\n\n\treturn rmd.mdID, &rmds, nil\n}\n\nfunc (j *mdJournal) removeFlushedEntry(\n\tctx context.Context, mdID MdID, rmds *RootMetadataSigned) error {\n\trmd, err := j.getEarliest(true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif rmd == (ImmutableBareRootMetadata{}) {\n\t\treturn errors.New(\"mdJournal unexpectedly empty\")\n\t}\n\n\tif mdID != rmd.mdID {\n\t\treturn fmt.Errorf(\"Expected mdID %s, got %s\", mdID, rmd.mdID)\n\t}\n\n\teq, err := kbfscodec.Equal(j.codec, rmd.BareRootMetadata, rmds.MD)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !eq {\n\t\treturn errors.New(\n\t\t\t\"Given RootMetadataSigned doesn't match earliest\")\n\t}\n\n\tempty, err := j.j.removeEarliest()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Since the journal is now empty, set lastMdID.\n\tif empty {\n\t\tj.log.CDebugf(ctx,\n\t\t\t\"Journal is now empty; saving last MdID=%s\", mdID)\n\t\tj.lastMdID = mdID\n\t}\n\n\t// Garbage-collect the old entry.  TODO: we'll eventually need a\n\t// sweeper to clean up entries left behind if we crash here.\n\treturn j.removeMD(mdID)\n}\n\nfunc getMdID(ctx context.Context, mdserver MDServer, crypto cryptoPure,\n\ttlfID TlfID, bid BranchID, mStatus MergeStatus,\n\trevision MetadataRevision) (MdID, error) {\n\trmdses, err := mdserver.GetRange(\n\t\tctx, tlfID, bid, mStatus, revision, revision)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t} else if len(rmdses) == 0 {\n\t\treturn MdID{}, nil\n\t} else if len(rmdses) > 1 {\n\t\treturn MdID{}, fmt.Errorf(\n\t\t\t\"Got more than one object when trying to get rev=%d for branch %s of TLF %s\",\n\t\t\trevision, bid, tlfID)\n\t}\n\n\treturn crypto.MakeMdID(rmdses[0].MD)\n}\n\n// All functions below are public functions.\n\nfunc (j mdJournal) readEarliestRevision() (MetadataRevision, error) {\n\treturn j.j.readEarliestRevision()\n}\n\nfunc (j mdJournal) readLatestRevision() (MetadataRevision, error) {\n\treturn j.j.readLatestRevision()\n}\n\nfunc (j mdJournal) length() (uint64, error) {\n\treturn j.j.length()\n}\n\nfunc (j mdJournal) end() (MetadataRevision, error) {\n\treturn j.j.end()\n}\n\nfunc (j mdJournal) getBranchID() BranchID {\n\treturn j.branchID\n}\n\nfunc (j mdJournal) getHead(extra ExtraMetadata) (\n\tImmutableBareRootMetadata, error) {\n\treturn j.checkGetParams(extra)\n}\n\nfunc (j mdJournal) getRange(\n\textra ExtraMetadata, start, stop MetadataRevision) (\n\t[]ImmutableBareRootMetadata, error) {\n\t_, err := j.checkGetParams(extra)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\trealStart, entries, err := j.j.getEntryRange(start, stop)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar rmds []ImmutableBareRootMetadata\n\tfor i, entry := range entries {\n\t\texpectedRevision := realStart + MetadataRevision(i)\n\t\trmd, ts, err := j.getMD(entry.ID, true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif expectedRevision != rmd.RevisionNumber() {\n\t\t\tpanic(fmt.Errorf(\"expected revision %v, got %v\",\n\t\t\t\texpectedRevision, rmd.RevisionNumber()))\n\t\t}\n\t\tirmd := MakeImmutableBareRootMetadata(rmd, entry.ID, ts)\n\t\trmds = append(rmds, irmd)\n\t}\n\n\treturn rmds, nil\n}\n\n// MDJournalConflictError is an error that is returned when a put\n// detects a rewritten journal.\ntype MDJournalConflictError struct{}\n\nfunc (e MDJournalConflictError) Error() string {\n\treturn \"MD journal conflict error\"\n}\n\n// put verifies and stores the given RootMetadata in the journal,\n// modifying it as needed. In particular, there are four cases:\n//\n// Merged\n// ------\n// rmd is merged. If the journal is empty, then rmd becomes the\n// initial entry. Otherwise, if the journal has been converted to a\n// branch, then an MDJournalConflictError error is returned, and the\n// caller is expected to set the unmerged bit and retry (see case\n// Unmerged-1). Otherwise, either rmd must be the successor to the\n// journal's head, in which case it is appended, or it must have the\n// same revision number as the journal's head, in which case it\n// replaces the journal's head. (This is necessary since if a journal\n// put is cancelled and an error is returned, it still happens, and so\n// we want the retried put (if any) to not conflict with it.)\n//\n// Unmerged-1\n// ----------\n// rmd is unmerged and has a null branch ID. This happens when case\n// Merged returns with MDJournalConflictError. In this case, the rmd's\n// branch ID is set to the journal's branch ID and its prevRoot is set\n// to the last known journal root. It doesn't matter if the journal is\n// completely drained, since the branch ID and last known root is\n// remembered in memory. However, since this cache isn't persisted to\n// disk, we need case Unmerged-3. Similarly to case Merged, this case\n// then also does append-or-replace.\n//\n// Unmerged-2\n// ----------\n// rmd is unmerged and has a non-null branch ID, and the journal was\n// non-empty at some time during this process's lifetime. Similarly to\n// case Merged, if the journal is empty, then rmd becomes the initial\n// entry, and otherwise, this case does append-or-replace.\n//\n// Unmerged-3\n// ----------\n// rmd is unmerged and has a non-null branch ID, and the journal has\n// always been empty during this process's lifetime. The branch ID is\n// assumed to be correct, i.e. retrieved from the remote MDServer, and\n// rmd becomes the initial entry.\nfunc (j *mdJournal) put(\n\tctx context.Context, signer cryptoSigner,\n\tekg encryptionKeyGetter, bsplit BlockSplitter, rmd *RootMetadata) (\n\tmdID MdID, err error) {\n\tj.log.CDebugf(ctx, \"Putting MD for TLF=%s with rev=%s bid=%s\",\n\t\trmd.TlfID(), rmd.Revision(), rmd.BID())\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tj.deferLog.CDebugf(ctx,\n\t\t\t\t\"Put MD for TLF=%s with rev=%s bid=%s failed with %v\",\n\t\t\t\trmd.TlfID(), rmd.Revision(), rmd.BID(), err)\n\t\t}\n\t}()\n\n\thead, err := j.getLatest(true)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\tmStatus := rmd.MergedStatus()\n\n\t// Make modifications for the Unmerged cases.\n\tif mStatus == Unmerged {\n\t\tvar lastMdID MdID\n\t\tif head == (ImmutableBareRootMetadata{}) {\n\t\t\tlastMdID = j.lastMdID\n\t\t} else {\n\t\t\tlastMdID = head.mdID\n\t\t}\n\n\t\tif rmd.BID() == NullBranchID && j.branchID == NullBranchID {\n\t\t\treturn MdID{}, errors.New(\n\t\t\t\t\"Unmerged put with rmd.BID() == j.branchID == NullBranchID\")\n\t\t}\n\n\t\tif head == (ImmutableBareRootMetadata{}) &&\n\t\t\tj.branchID == NullBranchID {\n\t\t\t// Case Unmerged-3.\n\t\t\tj.branchID = rmd.BID()\n\t\t\t// Revert branch ID if we encounter an error.\n\t\t\tdefer func() {\n\t\t\t\tif err != nil {\n\t\t\t\t\tj.branchID = NullBranchID\n\t\t\t\t}\n\t\t\t}()\n\t\t} else if rmd.BID() == NullBranchID {\n\t\t\t// Case Unmerged-1.\n\t\t\tj.log.CDebugf(\n\t\t\t\tctx, \"Changing branch ID to %s and prev root to %s for MD for TLF=%s with rev=%s\",\n\t\t\t\tj.branchID, lastMdID, rmd.TlfID(), rmd.Revision())\n\t\t\trmd.SetBranchID(j.branchID)\n\t\t\trmd.SetPrevRoot(lastMdID)\n\t\t} else {\n\t\t\t// Using de Morgan's laws, this branch is\n\t\t\t// taken when both rmd.BID() is non-null, and\n\t\t\t// either head is non-empty or j.branchID is\n\t\t\t// non-empty. So this is most of case\n\t\t\t// Unmerged-2, and there's nothing to do.\n\t\t\t//\n\t\t\t// The remaining part of case Unmerged-2,\n\t\t\t// where rmd.BID() is non-null, head is empty,\n\t\t\t// and j.branchID is empty, is an error case,\n\t\t\t// handled below.\n\t\t}\n\t}\n\n\t// The below is code common to all the cases.\n\n\tif (mStatus == Merged) != (rmd.BID() == NullBranchID) {\n\t\treturn MdID{}, fmt.Errorf(\n\t\t\t\"mStatus=%s doesn't match bid=%s\", mStatus, rmd.BID())\n\t}\n\n\t// If we're trying to push a merged MD onto a branch, return a\n\t// conflict error so the caller can retry with an unmerged MD.\n\tif mStatus == Merged && j.branchID != NullBranchID {\n\t\treturn MdID{}, MDJournalConflictError{}\n\t}\n\n\tif rmd.BID() != j.branchID {\n\t\treturn MdID{}, fmt.Errorf(\n\t\t\t\"Branch ID mismatch: expected %s, got %s\",\n\t\t\tj.branchID, rmd.BID())\n\t}\n\n\t// Check permissions and consistency with head, if it exists.\n\tif head != (ImmutableBareRootMetadata{}) {\n\t\tok, err := isWriterOrValidRekey(\n\t\t\tj.codec, j.uid, head.BareRootMetadata, rmd.bareMd)\n\t\tif err != nil {\n\t\t\treturn MdID{}, err\n\t\t}\n\t\tif !ok {\n\t\t\t// TODO: Use a non-server error.\n\t\t\treturn MdID{}, MDServerErrorUnauthorized{}\n\t\t}\n\n\t\t// Consistency checks\n\t\tif rmd.Revision() != head.RevisionNumber() {\n\t\t\terr = head.CheckValidSuccessorForServer(head.mdID, rmd.bareMd)\n\t\t\tif err != nil {\n\t\t\t\treturn MdID{}, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Ensure that the block changes are properly unembedded.\n\tif rmd.data.Changes.Info.BlockPointer == zeroPtr &&\n\t\t!bsplit.ShouldEmbedBlockChanges(&rmd.data.Changes) {\n\t\treturn MdID{},\n\t\t\terrors.New(\"MD has embedded block changes, but shouldn't\")\n\t}\n\n\tbrmd, err := encryptMDPrivateData(\n\t\tctx, j.codec, j.crypto, signer, ekg,\n\t\tj.uid, rmd.ReadOnly())\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\tid, err := j.putMD(brmd)\n\tif err != nil {\n\t\treturn MdID{}, err\n\t}\n\n\tif head != (ImmutableBareRootMetadata{}) &&\n\t\trmd.Revision() == head.RevisionNumber() {\n\n\t\tj.log.CDebugf(\n\t\t\tctx, \"Replacing head MD for TLF=%s with rev=%s bid=%s\",\n\t\t\trmd.TlfID(), rmd.Revision(), rmd.BID())\n\t\t// TODO: Try and preserve unknown fields from the old\n\t\t// journal.\n\t\terr = j.j.replaceHead(mdIDJournalEntry{ID: id})\n\t\tif err != nil {\n\t\t\treturn MdID{}, err\n\t\t}\n\t} else {\n\t\terr = j.j.append(\n\t\t\tbrmd.RevisionNumber(), mdIDJournalEntry{ID: id})\n\t\tif err != nil {\n\t\t\treturn MdID{}, err\n\t\t}\n\t}\n\n\t// Since the journal is now non-empty, clear lastMdID.\n\tj.lastMdID = MdID{}\n\n\treturn id, nil\n}\n\nfunc (j *mdJournal) clear(\n\tctx context.Context, bid BranchID, extra ExtraMetadata) (err error) {\n\tj.log.CDebugf(ctx, \"Clearing journal for branch %s\", bid)\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tj.deferLog.CDebugf(ctx,\n\t\t\t\t\"Clearing journal for branch %s failed with %v\",\n\t\t\t\tbid, err)\n\t\t}\n\t}()\n\n\tif bid == NullBranchID {\n\t\treturn errors.New(\"Cannot clear master branch\")\n\t}\n\n\tif j.branchID != bid {\n\t\t// Nothing to do.\n\t\tj.log.CDebugf(ctx, \"Ignoring clear for branch %s while on branch %s\",\n\t\t\tbid, j.branchID)\n\t\treturn nil\n\t}\n\n\thead, err := j.getHead(extra)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif head == (ImmutableBareRootMetadata{}) {\n\t\t// The journal has been flushed but not cleared yet.\n\t\tj.branchID = NullBranchID\n\t\treturn nil\n\t}\n\n\tif head.BID() != j.branchID {\n\t\treturn fmt.Errorf(\"Head branch ID %s doesn't match journal \"+\n\t\t\t\"branch ID %s while clearing\", head.BID(), j.branchID)\n\t}\n\n\tearliestRevision, err := j.j.readEarliestRevision()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlatestRevision, err := j.j.readLatestRevision()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, allEntries, err := j.j.getEntryRange(\n\t\tearliestRevision, latestRevision)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tj.branchID = NullBranchID\n\n\t// No need to set lastMdID in this case.\n\n\terr = j.j.clear()\n\tif err != nil {\n\t\treturn nil\n\t}\n\n\t// Garbage-collect the old branch entries.  TODO: we'll eventually\n\t// need a sweeper to clean up entries left behind if we crash\n\t// here.\n\tfor _, entry := range allEntries {\n\t\terr := j.removeMD(entry.ID)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n", "idx": 2, "id": 13500, "msg": "", "proj": "keybase-kbfs", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -2047,11 +2047,15 @@ Collection.prototype.mapReduce = function(map, reduce, options, callback) {\n  * @param {(number|string)} [options.w] The write concern.\n  * @param {number} [options.wtimeout] The write concern timeout.\n  * @param {boolean} [options.j=false] Specify a journal write concern.\n+ * @param {Boolean} [options.ignoreUndefined=false] Specify if the BSON serializer should ignore undefined fields.\n  * @param {ClientSession} [options.session] optional session to use for this operation\n  * @return {UnorderedBulkOperation}\n  */\n Collection.prototype.initializeUnorderedBulkOp = function(options) {\n   options = options || {};\n+  if (this.s.options.ignoreUndefined) {\n+    options.ignoreUndefined = this.s.options.ignoreUndefined;\n+  }\n   options.promiseLibrary = this.s.promiseLibrary;\n   return unordered(this.s.topology, this, options);\n };", "y": 1, "oldf": "'use strict';\n\nconst deprecate = require('util').deprecate;\nconst deprecateOptions = require('./utils').deprecateOptions;\nconst checkCollectionName = require('./utils').checkCollectionName;\nconst ObjectID = require('mongodb-core').BSON.ObjectID;\nconst AggregationCursor = require('./aggregation_cursor');\nconst MongoError = require('mongodb-core').MongoError;\nconst toError = require('./utils').toError;\nconst normalizeHintField = require('./utils').normalizeHintField;\nconst handleCallback = require('./utils').handleCallback;\nconst decorateCommand = require('./utils').decorateCommand;\nconst decorateWithCollation = require('./utils').decorateWithCollation;\nconst decorateWithReadConcern = require('./utils').decorateWithReadConcern;\nconst formattedOrderClause = require('./utils').formattedOrderClause;\nconst ReadPreference = require('mongodb-core').ReadPreference;\nconst CommandCursor = require('./command_cursor');\nconst unordered = require('./bulk/unordered');\nconst ordered = require('./bulk/ordered');\nconst ChangeStream = require('./change_stream');\nconst executeOperation = require('./utils').executeOperation;\nconst applyWriteConcern = require('./utils').applyWriteConcern;\nconst resolveReadPreference = require('./utils').resolveReadPreference;\n\n// Operations\nconst bulkWrite = require('./operations/collection_ops').bulkWrite;\nconst checkForAtomicOperators = require('./operations/collection_ops').checkForAtomicOperators;\nconst count = require('./operations/collection_ops').count;\nconst countDocuments = require('./operations/collection_ops').countDocuments;\nconst createIndex = require('./operations/collection_ops').createIndex;\nconst createIndexes = require('./operations/collection_ops').createIndexes;\nconst deleteMany = require('./operations/collection_ops').deleteMany;\nconst deleteOne = require('./operations/collection_ops').deleteOne;\nconst distinct = require('./operations/collection_ops').distinct;\nconst dropIndex = require('./operations/collection_ops').dropIndex;\nconst dropIndexes = require('./operations/collection_ops').dropIndexes;\nconst ensureIndex = require('./operations/collection_ops').ensureIndex;\nconst findAndModify = require('./operations/collection_ops').findAndModify;\nconst findAndRemove = require('./operations/collection_ops').findAndRemove;\nconst findOne = require('./operations/collection_ops').findOne;\nconst findOneAndDelete = require('./operations/collection_ops').findOneAndDelete;\nconst findOneAndReplace = require('./operations/collection_ops').findOneAndReplace;\nconst findOneAndUpdate = require('./operations/collection_ops').findOneAndUpdate;\nconst geoHaystackSearch = require('./operations/collection_ops').geoHaystackSearch;\nconst group = require('./operations/collection_ops').group;\nconst indexes = require('./operations/collection_ops').indexes;\nconst indexExists = require('./operations/collection_ops').indexExists;\nconst indexInformation = require('./operations/collection_ops').indexInformation;\nconst insertOne = require('./operations/collection_ops').insertOne;\nconst isCapped = require('./operations/collection_ops').isCapped;\nconst mapReduce = require('./operations/collection_ops').mapReduce;\nconst optionsOp = require('./operations/collection_ops').optionsOp;\nconst parallelCollectionScan = require('./operations/collection_ops').parallelCollectionScan;\nconst prepareDocs = require('./operations/collection_ops').prepareDocs;\nconst reIndex = require('./operations/collection_ops').reIndex;\nconst removeDocuments = require('./operations/collection_ops').removeDocuments;\nconst rename = require('./operations/collection_ops').rename;\nconst replaceOne = require('./operations/collection_ops').replaceOne;\nconst save = require('./operations/collection_ops').save;\nconst stats = require('./operations/collection_ops').stats;\nconst updateDocuments = require('./operations/collection_ops').updateDocuments;\nconst updateMany = require('./operations/collection_ops').updateMany;\nconst updateOne = require('./operations/collection_ops').updateOne;\n\n/**\n * @fileOverview The **Collection** class is an internal class that embodies a MongoDB collection\n * allowing for insert/update/remove/find and other command operation on that MongoDB collection.\n *\n * **COLLECTION Cannot directly be instantiated**\n * @example\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   // Create a collection we want to drop later\n *   const col = client.db(dbName).collection('createIndexExample1');\n *   // Show that duplicate records got dropped\n *   col.find({}).toArray(function(err, items) {\n *     test.equal(null, err);\n *     test.equal(4, items.length);\n *     client.close();\n *   });\n * });\n */\n\nconst mergeKeys = ['ignoreUndefined'];\n\n/**\n * Create a new Collection instance (INTERNAL TYPE, do not instantiate directly)\n * @class\n * @property {string} collectionName Get the collection name.\n * @property {string} namespace Get the full collection namespace.\n * @property {object} writeConcern The current write concern values.\n * @property {object} readConcern The current read concern values.\n * @property {object} hint Get current index hint for collection.\n * @return {Collection} a Collection instance.\n */\nfunction Collection(db, topology, dbName, name, pkFactory, options) {\n  checkCollectionName(name);\n\n  // Unpack variables\n  const internalHint = null;\n  const slaveOk = options == null || options.slaveOk == null ? db.slaveOk : options.slaveOk;\n  const serializeFunctions =\n    options == null || options.serializeFunctions == null\n      ? db.s.options.serializeFunctions\n      : options.serializeFunctions;\n  const raw = options == null || options.raw == null ? db.s.options.raw : options.raw;\n  const promoteLongs =\n    options == null || options.promoteLongs == null\n      ? db.s.options.promoteLongs\n      : options.promoteLongs;\n  const promoteValues =\n    options == null || options.promoteValues == null\n      ? db.s.options.promoteValues\n      : options.promoteValues;\n  const promoteBuffers =\n    options == null || options.promoteBuffers == null\n      ? db.s.options.promoteBuffers\n      : options.promoteBuffers;\n  let readPreference = null;\n  const collectionHint = null;\n  const namespace = `${dbName}.${name}`;\n\n  // Get the promiseLibrary\n  const promiseLibrary = options.promiseLibrary || Promise;\n\n  // Assign the right collection level readPreference\n  if (options && options.readPreference) {\n    readPreference = options.readPreference;\n  } else if (db.options.readPreference) {\n    readPreference = db.options.readPreference;\n  }\n\n  // Set custom primary key factory if provided\n  pkFactory = pkFactory == null ? ObjectID : pkFactory;\n\n  // Internal state\n  this.s = {\n    // Set custom primary key factory if provided\n    pkFactory: pkFactory,\n    // Db\n    db: db,\n    // Topology\n    topology: topology,\n    // dbName\n    dbName: dbName,\n    // Options\n    options: options,\n    // Namespace\n    namespace: namespace,\n    // Read preference\n    readPreference: readPreference,\n    // SlaveOK\n    slaveOk: slaveOk,\n    // Serialize functions\n    serializeFunctions: serializeFunctions,\n    // Raw\n    raw: raw,\n    // promoteLongs\n    promoteLongs: promoteLongs,\n    // promoteValues\n    promoteValues: promoteValues,\n    // promoteBuffers\n    promoteBuffers: promoteBuffers,\n    // internalHint\n    internalHint: internalHint,\n    // collectionHint\n    collectionHint: collectionHint,\n    // Name\n    name: name,\n    // Promise library\n    promiseLibrary: promiseLibrary,\n    // Read Concern\n    readConcern: options.readConcern,\n    // Write Concern\n    writeConcern: options.writeConcern\n  };\n}\n\nObject.defineProperty(Collection.prototype, 'dbName', {\n  enumerable: true,\n  get: function() {\n    return this.s.dbName;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'collectionName', {\n  enumerable: true,\n  get: function() {\n    return this.s.name;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'namespace', {\n  enumerable: true,\n  get: function() {\n    return this.s.namespace;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'readConcern', {\n  enumerable: true,\n  get: function() {\n    return this.s.readConcern || { level: 'local' };\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'writeConcern', {\n  enumerable: true,\n  get: function() {\n    let ops = {};\n    if (this.s.writeConcern) {\n      return this.s.writeConcern;\n    }\n\n    if (this.s.options.w != null) ops.w = this.s.options.w;\n    if (this.s.options.j != null) ops.j = this.s.options.j;\n    if (this.s.options.fsync != null) ops.fsync = this.s.options.fsync;\n    if (this.s.options.wtimeout != null) ops.wtimeout = this.s.options.wtimeout;\n    return ops;\n  }\n});\n\n/**\n * @ignore\n */\nObject.defineProperty(Collection.prototype, 'hint', {\n  enumerable: true,\n  get: function() {\n    return this.s.collectionHint;\n  },\n  set: function(v) {\n    this.s.collectionHint = normalizeHintField(v);\n  }\n});\n\nconst DEPRECATED_FIND_OPTIONS = ['maxScan', 'fields', 'snapshot'];\n\n/**\n * Creates a cursor for a query that can be used to iterate over results from MongoDB\n * @method\n * @param {object} [query={}] The cursor query object.\n * @param {object} [options] Optional settings.\n * @param {number} [options.limit=0] Sets the limit of documents returned in the query.\n * @param {(array|object)} [options.sort] Set to sort the documents coming back from the query. Array of indexes, [['a', 1]] etc.\n * @param {object} [options.projection] The fields to return in the query. Object of fields to include or exclude (not both), {'a':1}\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {number} [options.skip=0] Set to skip N documents ahead in your query (useful for pagination).\n * @param {Object} [options.hint] Tell the query to use specific indexes in the query. Object of indexes to use, {'_id':1}\n * @param {boolean} [options.explain=false] Explain the query instead of returning the data.\n * @param {boolean} [options.snapshot=false] DEPRECATED: Snapshot query.\n * @param {boolean} [options.timeout=false] Specify if the cursor can timeout.\n * @param {boolean} [options.tailable=false] Specify if the cursor is tailable.\n * @param {number} [options.batchSize=0] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {boolean} [options.returnKey=false] Only return the index key.\n * @param {number} [options.maxScan] DEPRECATED: Limit the number of items to scan.\n * @param {number} [options.min] Set index bounds.\n * @param {number} [options.max] Set index bounds.\n * @param {boolean} [options.showDiskLoc=false] Show disk location of results.\n * @param {string} [options.comment] You can put a $comment field on a query to make looking in the profiler logs simpler.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {boolean} [options.partial=false] Specify if the cursor should return partial results when querying against a sharded system\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @throws {MongoError}\n * @return {Cursor}\n */\nCollection.prototype.find = deprecateOptions(\n  {\n    name: 'collection.find',\n    deprecatedOptions: DEPRECATED_FIND_OPTIONS,\n    optionsIndex: 1\n  },\n  function(query, options, callback) {\n    if (typeof callback === 'object') {\n      // TODO(MAJOR): throw in the future\n      console.warn('Third parameter to `find()` must be a callback or undefined');\n    }\n\n    let selector = query;\n    // figuring out arguments\n    if (typeof callback !== 'function') {\n      if (typeof options === 'function') {\n        callback = options;\n        options = undefined;\n      } else if (options == null) {\n        callback = typeof selector === 'function' ? selector : undefined;\n        selector = typeof selector === 'object' ? selector : undefined;\n      }\n    }\n\n    // Ensure selector is not null\n    selector = selector == null ? {} : selector;\n    // Validate correctness off the selector\n    const object = selector;\n    if (Buffer.isBuffer(object)) {\n      const object_size = object[0] | (object[1] << 8) | (object[2] << 16) | (object[3] << 24);\n      if (object_size !== object.length) {\n        const error = new Error(\n          'query selector raw message size does not match message header size [' +\n            object.length +\n            '] != [' +\n            object_size +\n            ']'\n        );\n        error.name = 'MongoError';\n        throw error;\n      }\n    }\n\n    // Check special case where we are using an objectId\n    if (selector != null && selector._bsontype === 'ObjectID') {\n      selector = { _id: selector };\n    }\n\n    if (!options) options = {};\n\n    let projection = options.projection || options.fields;\n\n    if (projection && !Buffer.isBuffer(projection) && Array.isArray(projection)) {\n      projection = projection.length\n        ? projection.reduce((result, field) => {\n            result[field] = 1;\n            return result;\n          }, {})\n        : { _id: 1 };\n    }\n\n    // Make a shallow copy of options\n    let newOptions = Object.assign({}, options);\n\n    // Make a shallow copy of the collection options\n    for (let key in this.s.options) {\n      if (mergeKeys.indexOf(key) !== -1) {\n        newOptions[key] = this.s.options[key];\n      }\n    }\n\n    // Unpack options\n    newOptions.skip = options.skip ? options.skip : 0;\n    newOptions.limit = options.limit ? options.limit : 0;\n    newOptions.raw = typeof options.raw === 'boolean' ? options.raw : this.s.raw;\n    newOptions.hint =\n      options.hint != null ? normalizeHintField(options.hint) : this.s.collectionHint;\n    newOptions.timeout = typeof options.timeout === 'undefined' ? undefined : options.timeout;\n    // // If we have overridden slaveOk otherwise use the default db setting\n    newOptions.slaveOk = options.slaveOk != null ? options.slaveOk : this.s.db.slaveOk;\n\n    // Add read preference if needed\n    newOptions.readPreference = resolveReadPreference(newOptions, {\n      db: this.s.db,\n      collection: this\n    });\n\n    // Set slave ok to true if read preference different from primary\n    if (\n      newOptions.readPreference != null &&\n      (newOptions.readPreference !== 'primary' || newOptions.readPreference.mode !== 'primary')\n    ) {\n      newOptions.slaveOk = true;\n    }\n\n    // Ensure the query is an object\n    if (selector != null && typeof selector !== 'object') {\n      throw MongoError.create({ message: 'query selector must be an object', driver: true });\n    }\n\n    // Build the find command\n    const findCommand = {\n      find: this.s.namespace,\n      limit: newOptions.limit,\n      skip: newOptions.skip,\n      query: selector\n    };\n\n    // Ensure we use the right await data option\n    if (typeof newOptions.awaitdata === 'boolean') {\n      newOptions.awaitData = newOptions.awaitdata;\n    }\n\n    // Translate to new command option noCursorTimeout\n    if (typeof newOptions.timeout === 'boolean') newOptions.noCursorTimeout = newOptions.timeout;\n\n    decorateCommand(findCommand, newOptions, ['session', 'collation']);\n\n    if (projection) findCommand.fields = projection;\n\n    // Add db object to the new options\n    newOptions.db = this.s.db;\n\n    // Add the promise library\n    newOptions.promiseLibrary = this.s.promiseLibrary;\n\n    // Set raw if available at collection level\n    if (newOptions.raw == null && typeof this.s.raw === 'boolean') newOptions.raw = this.s.raw;\n    // Set promoteLongs if available at collection level\n    if (newOptions.promoteLongs == null && typeof this.s.promoteLongs === 'boolean')\n      newOptions.promoteLongs = this.s.promoteLongs;\n    if (newOptions.promoteValues == null && typeof this.s.promoteValues === 'boolean')\n      newOptions.promoteValues = this.s.promoteValues;\n    if (newOptions.promoteBuffers == null && typeof this.s.promoteBuffers === 'boolean')\n      newOptions.promoteBuffers = this.s.promoteBuffers;\n\n    // Sort options\n    if (findCommand.sort) {\n      findCommand.sort = formattedOrderClause(findCommand.sort);\n    }\n\n    // Set the readConcern\n    decorateWithReadConcern(findCommand, this, options);\n\n    // Decorate find command with collation options\n    try {\n      decorateWithCollation(findCommand, this, options);\n    } catch (err) {\n      if (typeof callback === 'function') return callback(err, null);\n      throw err;\n    }\n\n    const cursor = this.s.topology.cursor(this.s.namespace, findCommand, newOptions);\n\n    return typeof callback === 'function' ? handleCallback(callback, null, cursor) : cursor;\n  }\n);\n\n/**\n * Inserts a single document into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object} doc Document to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertOneWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.insertOne = function(doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, insertOne, [this, doc, options, callback]);\n};\n\nfunction mapInsertManyResults(docs, r) {\n  const finalResult = {\n    result: { ok: 1, n: r.insertedCount },\n    ops: docs,\n    insertedCount: r.insertedCount,\n    insertedIds: r.insertedIds\n  };\n\n  if (r.getLastOp()) {\n    finalResult.result.opTime = r.getLastOp();\n  }\n\n  return finalResult;\n}\n\n/**\n * Inserts an array of documents into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object[]} docs Documents to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {boolean} [options.ordered=true] If true, when an insert fails, don't execute the remaining writes. If false, continue with remaining inserts when one fails.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.insertMany = function(docs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options ? Object.assign({}, options) : { ordered: true };\n\n  if (!Array.isArray(docs) && typeof callback === 'function') {\n    return callback(\n      MongoError.create({ message: 'docs parameter must be an array of documents', driver: true })\n    );\n  } else if (!Array.isArray(docs)) {\n    return new this.s.promiseLibrary((resolve, reject) => {\n      reject(\n        MongoError.create({ message: 'docs parameter must be an array of documents', driver: true })\n      );\n    });\n  }\n\n  // If keep going set unordered\n  options['serializeFunctions'] = options['serializeFunctions'] || this.s.serializeFunctions;\n\n  docs = prepareDocs(this, docs, options);\n\n  // Generate the bulk write operations\n  const operations = [\n    {\n      insertMany: docs\n    }\n  ];\n\n  return executeOperation(this.s.topology, bulkWrite, [this, operations, options, callback], {\n    resultMutator: result => mapInsertManyResults(docs, result)\n  });\n};\n\n/**\n * @typedef {Object} Collection~BulkWriteOpResult\n * @property {number} insertedCount Number of documents inserted.\n * @property {number} matchedCount Number of documents matched for update.\n * @property {number} modifiedCount Number of documents modified.\n * @property {number} deletedCount Number of documents deleted.\n * @property {number} upsertedCount Number of documents upserted.\n * @property {object} insertedIds Inserted document generated Id's, hash key is the index of the originating operation\n * @property {object} upsertedIds Upserted document generated Id's, hash key is the index of the originating operation\n * @property {object} result The command result object.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~bulkWriteOpCallback\n * @param {BulkWriteError} error An error instance representing the error during the execution.\n * @param {Collection~BulkWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Perform a bulkWrite operation without a fluent API\n *\n * Legal operation types are\n *\n *  { insertOne: { document: { a: 1 } } }\n *\n *  { updateOne: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n *\n *  { updateMany: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n *\n *  { deleteOne: { filter: {c:1} } }\n *\n *  { deleteMany: { filter: {c:1} } }\n *\n *  { replaceOne: { filter: {c:3}, replacement: {c:4}, upsert:true}}\n *\n * If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object[]} operations Bulk operations to perform.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.ordered=true] Execute write operation in ordered or unordered fashion.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~bulkWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.bulkWrite = function(operations, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || { ordered: true };\n\n  if (!Array.isArray(operations)) {\n    throw MongoError.create({ message: 'operations must be an array of documents', driver: true });\n  }\n\n  return executeOperation(this.s.topology, bulkWrite, [this, operations, options, callback]);\n};\n\n/**\n * @typedef {Object} Collection~WriteOpResult\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The command result object.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~writeOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~WriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * @typedef {Object} Collection~insertWriteOpResult\n * @property {Number} insertedCount The total amount of documents inserted.\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {Object.<Number, ObjectId>} insertedIds Map of the index of the inserted document to the id of the inserted document.\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The raw command result object returned from MongoDB (content might vary by server version).\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents inserted.\n */\n\n/**\n * @typedef {Object} Collection~insertOneWriteOpResult\n * @property {Number} insertedCount The total amount of documents inserted.\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {ObjectId} insertedId The driver generated ObjectId for the insert operation.\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The raw command result object returned from MongoDB (content might vary by server version).\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents inserted.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~insertWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~insertWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~insertOneWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~insertOneWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Inserts a single document or a an array of documents into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {(object|object[])} docs Documents to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated Use insertOne, insertMany or bulkWrite\n */\nCollection.prototype.insert = deprecate(function(docs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || { ordered: false };\n  docs = !Array.isArray(docs) ? [docs] : docs;\n\n  if (options.keepGoing === true) {\n    options.ordered = false;\n  }\n\n  return this.insertMany(docs, options, callback);\n}, 'collection.insert is deprecated. Use insertOne, insertMany or bulkWrite instead.');\n\n/**\n * @typedef {Object} Collection~updateWriteOpResult\n * @property {Object} result The raw result returned from MongoDB. Will vary depending on server version.\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents scanned.\n * @property {Number} result.nModified The total count of documents modified.\n * @property {Object} connection The connection object used for the operation.\n * @property {Number} matchedCount The number of documents that matched the filter.\n * @property {Number} modifiedCount The number of documents that were modified.\n * @property {Number} upsertedCount The number of documents upserted.\n * @property {Object} upsertedId The upserted id.\n * @property {ObjectId} upsertedId._id The upserted _id returned from the server.\n * @property {Object} message\n * @property {Array} ops\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~updateWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~updateWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Update a single document in a collection\n * @method\n * @param {object} filter The Filter used to select the document to update\n * @param {object} update The update operations to be applied to the document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert=false] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.updateOne = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  options = Object.assign({}, options);\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, updateOne, [this, filter, update, options, callback]);\n};\n\n/**\n * Replace a document in a collection with another document\n * @method\n * @param {object} filter The Filter used to select the document to replace\n * @param {object} doc The Document that replaces the matching document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert=false] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise<Collection~updatewriteOpResultObject>} returns Promise if no callback passed\n */\nCollection.prototype.replaceOne = function(filter, doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = Object.assign({}, options);\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, replaceOne, [this, filter, doc, options, callback]);\n};\n\n/**\n * Update multiple documents in a collection\n * @method\n * @param {object} filter The Filter used to select the documents to update\n * @param {object} update The update operations to be applied to the documents\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert=false] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise<Collection~updateWriteOpResultObject>} returns Promise if no callback passed\n */\nCollection.prototype.updateMany = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  options = Object.assign({}, options);\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, updateMany, [this, filter, update, options, callback]);\n};\n\n/**\n * Updates documents.\n * @method\n * @param {object} selector The selector for the update operation.\n * @param {object} document The update document.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.upsert=false] Update operation is an upsert.\n * @param {boolean} [options.multi=false] Update one/all documents with operation.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @throws {MongoError}\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use updateOne, updateMany or bulkWrite\n */\nCollection.prototype.update = deprecate(function(selector, document, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, updateDocuments, [\n    this,\n    selector,\n    document,\n    options,\n    callback\n  ]);\n}, 'collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.');\n\n/**\n * @typedef {Object} Collection~deleteWriteOpResult\n * @property {Object} result The raw result returned from MongoDB. Will vary depending on server version.\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents deleted.\n * @property {Object} connection The connection object used for the operation.\n * @property {Number} deletedCount The number of documents deleted.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~deleteWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~deleteWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Delete a document from a collection\n * @method\n * @param {object} filter The Filter used to select the document to remove\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~deleteWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.deleteOne = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = Object.assign({}, options);\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, deleteOne, [this, filter, options, callback]);\n};\n\nCollection.prototype.removeOne = Collection.prototype.deleteOne;\n\n/**\n * Delete multiple documents from a collection\n * @method\n * @param {object} filter The Filter used to select the documents to remove\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~deleteWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.deleteMany = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = Object.assign({}, options);\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, deleteMany, [this, filter, options, callback]);\n};\n\nCollection.prototype.removeMany = Collection.prototype.deleteMany;\n\n/**\n * Remove documents.\n * @method\n * @param {object} selector The selector for the update operation.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.single=false] Removes the first document found.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use deleteOne, deleteMany or bulkWrite\n */\nCollection.prototype.remove = deprecate(function(selector, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, removeDocuments, [this, selector, options, callback]);\n}, 'collection.remove is deprecated. Use deleteOne, deleteMany, or bulkWrite instead.');\n\n/**\n * Save a document. Simple full document replacement function. Not recommended for efficiency, use atomic\n * operators and update instead for more efficient operations.\n * @method\n * @param {object} doc Document to save\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use insertOne, insertMany, updateOne or updateMany\n */\nCollection.prototype.save = deprecate(function(doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, save, [this, doc, options, callback]);\n}, 'collection.save is deprecated. Use insertOne, insertMany, updateOne, or updateMany instead.');\n\n/**\n * The callback format for results\n * @callback Collection~resultCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {object} result The result object if the command was executed successfully.\n */\n\n/**\n * The callback format for an aggregation call\n * @callback Collection~aggregationCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {AggregationCursor} cursor The cursor if the aggregation command was executed successfully.\n */\n\n/**\n * Fetches the first document that matches the query\n * @method\n * @param {object} query Query for find Operation\n * @param {object} [options] Optional settings.\n * @param {number} [options.limit=0] Sets the limit of documents returned in the query.\n * @param {(array|object)} [options.sort] Set to sort the documents coming back from the query. Array of indexes, [['a', 1]] etc.\n * @param {object} [options.projection] The fields to return in the query. Object of fields to include or exclude (not both), {'a':1}\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {number} [options.skip=0] Set to skip N documents ahead in your query (useful for pagination).\n * @param {Object} [options.hint] Tell the query to use specific indexes in the query. Object of indexes to use, {'_id':1}\n * @param {boolean} [options.explain=false] Explain the query instead of returning the data.\n * @param {boolean} [options.snapshot=false] DEPRECATED: Snapshot query.\n * @param {boolean} [options.timeout=false] Specify if the cursor can timeout.\n * @param {boolean} [options.tailable=false] Specify if the cursor is tailable.\n * @param {number} [options.batchSize=0] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {boolean} [options.returnKey=false] Only return the index key.\n * @param {number} [options.maxScan] DEPRECATED: Limit the number of items to scan.\n * @param {number} [options.min] Set index bounds.\n * @param {number} [options.max] Set index bounds.\n * @param {boolean} [options.showDiskLoc=false] Show disk location of results.\n * @param {string} [options.comment] You can put a $comment field on a query to make looking in the profiler logs simpler.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {boolean} [options.partial=false] Specify if the cursor should return partial results when querying against a sharded system\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.findOne = deprecateOptions(\n  {\n    name: 'collection.find',\n    deprecatedOptions: DEPRECATED_FIND_OPTIONS,\n    optionsIndex: 1\n  },\n  function(query, options, callback) {\n    if (typeof callback === 'object') {\n      // TODO(MAJOR): throw in the future\n      console.warn('Third parameter to `findOne()` must be a callback or undefined');\n    }\n\n    if (typeof query === 'function') (callback = query), (query = {}), (options = {});\n    if (typeof options === 'function') (callback = options), (options = {});\n    query = query || {};\n    options = options || {};\n\n    return executeOperation(this.s.topology, findOne, [this, query, options, callback]);\n  }\n);\n\n/**\n * The callback format for the collection method, must be used if strict is specified\n * @callback Collection~collectionResultCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection} collection The collection instance.\n */\n\n/**\n * Rename the collection.\n *\n * @method\n * @param {string} newName New name of of the collection.\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.dropTarget=false] Drop the target name collection if it previously exists.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~collectionResultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.rename = function(newName, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = Object.assign({}, options, { readPreference: ReadPreference.PRIMARY });\n\n  return executeOperation(this.s.topology, rename, [this, newName, options, callback]);\n};\n\n/**\n * Drop the collection from the database, removing it permanently. New accesses will create a new collection.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.drop = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, this.s.db.dropCollection.bind(this.s.db), [\n    this.s.name,\n    options,\n    callback\n  ]);\n};\n\n/**\n * Returns the options of the collection.\n *\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.options = function(opts, callback) {\n  if (typeof opts === 'function') (callback = opts), (opts = {});\n  opts = opts || {};\n\n  return executeOperation(this.s.topology, optionsOp, [this, opts, callback]);\n};\n\n/**\n * Returns if the collection is a capped collection\n *\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.isCapped = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, isCapped, [this, options, callback]);\n};\n\n/**\n * Creates an index on the db and collection collection.\n * @method\n * @param {(string|object)} fieldOrSpec Defines the index.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.unique=false] Creates an unique index.\n * @param {boolean} [options.sparse=false] Creates a sparse index.\n * @param {boolean} [options.background=false] Creates the index in the background, yielding whenever possible.\n * @param {boolean} [options.dropDups=false] A unique index cannot be created on a key that has pre-existing duplicate values. If you would like to create the index anyway, keeping the first document the database indexes and deleting all subsequent documents that have duplicate value\n * @param {number} [options.min] For geospatial indexes set the lower bound for the co-ordinates.\n * @param {number} [options.max] For geospatial indexes set the high bound for the co-ordinates.\n * @param {number} [options.v] Specify the format version of the indexes.\n * @param {number} [options.expireAfterSeconds] Allows you to expire data on indexes applied to a data (MongoDB 2.2 or higher)\n * @param {string} [options.name] Override the autogenerated index name (useful if the resulting name is larger than 128 bytes)\n * @param {object} [options.partialFilterExpression] Creates a partial index based on the given filter object (MongoDB 3.2 or higher)\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.createIndex = function(fieldOrSpec, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, createIndex, [this, fieldOrSpec, options, callback]);\n};\n\n/**\n * Creates multiple indexes in the collection, this method is only supported for\n * MongoDB 2.6 or higher. Earlier version of MongoDB will throw a command not supported\n * error. Index specifications are defined at http://docs.mongodb.org/manual/reference/command/createIndexes/.\n * @method\n * @param {array} indexSpecs An array of index specifications to be created\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.createIndexes = function(indexSpecs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = options ? Object.assign({}, options) : {};\n  if (typeof options.maxTimeMS !== 'number') delete options.maxTimeMS;\n\n  return executeOperation(this.s.topology, createIndexes, [this, indexSpecs, options, callback]);\n};\n\n/**\n * Drops an index from this collection.\n * @method\n * @param {string} indexName Name of the index to drop.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.dropIndex = function(indexName, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n\n  options = args.length ? args.shift() || {} : {};\n  // Run only against primary\n  options.readPreference = ReadPreference.PRIMARY;\n\n  return executeOperation(this.s.topology, dropIndex, [this, indexName, options, callback]);\n};\n\n/**\n * Drops all indexes from this collection.\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.dropIndexes = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options ? Object.assign({}, options) : {};\n\n  if (typeof options.maxTimeMS !== 'number') delete options.maxTimeMS;\n\n  return executeOperation(this.s.topology, dropIndexes, [this, options, callback]);\n};\n\n/**\n * Drops all indexes from this collection.\n * @method\n * @deprecated use dropIndexes\n * @param {Collection~resultCallback} callback The command result callback\n * @return {Promise} returns Promise if no [callback] passed\n */\nCollection.prototype.dropAllIndexes = deprecate(\n  Collection.prototype.dropIndexes,\n  'collection.dropAllIndexes is deprecated. Use dropIndexes instead.'\n);\n\n/**\n * Reindex all indexes on the collection\n * Warning: reIndex is a blocking operation (indexes are rebuilt in the foreground) and will be slow for large collections.\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.reIndex = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, reIndex, [this, options, callback]);\n};\n\n/**\n * Get the list of all indexes information for the collection.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {number} [options.batchSize] The batchSize for the returned command cursor or if pre 2.8 the systems batch collection\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {CommandCursor}\n */\nCollection.prototype.listIndexes = function(options) {\n  options = options || {};\n  // Clone the options\n  options = Object.assign({}, options);\n  // Determine the read preference in the options.\n  options.readPreference = resolveReadPreference(options, { db: this.s.db, collection: this });\n  // Set the CommandCursor constructor\n  options.cursorFactory = CommandCursor;\n  // Set the promiseLibrary\n  options.promiseLibrary = this.s.promiseLibrary;\n\n  if (!this.s.topology.capabilities()) {\n    throw new MongoError('cannot connect to server');\n  }\n\n  // Cursor options\n  let cursor = options.batchSize ? { batchSize: options.batchSize } : {};\n\n  // We have a list collections command\n  if (this.s.topology.capabilities().hasListIndexesCommand) {\n    // Build the command\n    const command = { listIndexes: this.s.name, cursor: cursor };\n    // Execute the cursor\n    cursor = this.s.topology.cursor(`${this.s.dbName}.$cmd`, command, options);\n    // Do we have a readPreference, apply it\n    if (options.readPreference) cursor.setReadPreference(options.readPreference);\n    // Return the cursor\n    return cursor;\n  }\n\n  // Get the namespace\n  const ns = `${this.s.dbName}.system.indexes`;\n  // Get the query\n  cursor = this.s.topology.cursor(ns, { find: ns, query: { ns: this.s.namespace } }, options);\n  // Do we have a readPreference, apply it\n  if (options.readPreference) cursor.setReadPreference(options.readPreference);\n  // Set the passed in batch size if one was provided\n  if (options.batchSize) cursor = cursor.batchSize(options.batchSize);\n  // Return the cursor\n  return cursor;\n};\n\n/**\n * Ensures that an index exists, if it does not it creates it\n * @method\n * @deprecated use createIndexes instead\n * @param {(string|object)} fieldOrSpec Defines the index.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.unique=false] Creates an unique index.\n * @param {boolean} [options.sparse=false] Creates a sparse index.\n * @param {boolean} [options.background=false] Creates the index in the background, yielding whenever possible.\n * @param {boolean} [options.dropDups=false] A unique index cannot be created on a key that has pre-existing duplicate values. If you would like to create the index anyway, keeping the first document the database indexes and deleting all subsequent documents that have duplicate value\n * @param {number} [options.min] For geospatial indexes set the lower bound for the co-ordinates.\n * @param {number} [options.max] For geospatial indexes set the high bound for the co-ordinates.\n * @param {number} [options.v] Specify the format version of the indexes.\n * @param {number} [options.expireAfterSeconds] Allows you to expire data on indexes applied to a data (MongoDB 2.2 or higher)\n * @param {number} [options.name] Override the autogenerated index name (useful if the resulting name is larger than 128 bytes)\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.ensureIndex = deprecate(function(fieldOrSpec, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, ensureIndex, [this, fieldOrSpec, options, callback]);\n}, 'collection.ensureIndex is deprecated. Use createIndexes instead.');\n\n/**\n * Checks if one or more indexes exist on the collection, fails on first non-existing index\n * @method\n * @param {(string|array)} indexes One or more index names to check.\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.indexExists = function(indexes, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, indexExists, [this, indexes, options, callback]);\n};\n\n/**\n * Retrieves this collections index info.\n * @method\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.full=false] Returns the full raw index information.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.indexInformation = function(options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, indexInformation, [this, options, callback]);\n};\n\n/**\n * The callback format for results\n * @callback Collection~countCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {number} result The count of documents that matched the query.\n */\n\n/**\n * Count number of matching documents in the db to a query.\n * @method\n * @param {object} [query={}] The query for the count.\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.limit] The limit of documents to count.\n * @param {boolean} [options.skip] The number of documents to skip for the count.\n * @param {string} [options.hint] An index name hint for the query.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~countCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use {@link Collection#countDocuments countDocuments} or {@link Collection#estimatedDocumentCount estimatedDocumentCount} instead\n */\nCollection.prototype.count = deprecate(function(query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  query = args.length ? args.shift() || {} : {};\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, count, [this, query, options, callback]);\n}, 'collection.count is deprecated, and will be removed in a future version.' +\n  ' Use collection.countDocuments or collection.estimatedDocumentCount instead');\n\n/**\n * Gets an estimate of the count of documents in a collection using collection metadata.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the operation to run.\n * @param {Collection~countCallback} [callback] The command result callback.\n * @return {Promise} returns Promise if no callback passed.\n */\nCollection.prototype.estimatedDocumentCount = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, count, [this, null, options, callback]);\n};\n\n/**\n * Gets the number of documents matching the filter.\n *\n * **Note**: When migrating from {@link Collection#count count} to {@link Collection#countDocuments countDocuments}\n * the following query operators must be replaced:\n *\n * | Operator | Replacement |\n * | -------- | ----------- |\n * | `$where`   | [`$expr`][1] |\n * | `$near`    | [`$geoWithin`][2] with [`$center`][3] |\n * | `$nearSphere` | [`$geoWithin`][2] with [`$centerSphere`][4] |\n *\n * [1]: https://docs.mongodb.com/manual/reference/operator/query/expr/\n * [2]: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n * [3]: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n * [4]: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n *\n * @param {object} [query] the query for the count\n * @param {object} [options] Optional settings.\n * @param {object} [options.collation] Specifies a collation.\n * @param {string|object} [options.hint] The index to use.\n * @param {number} [options.limit] The maximum number of document to count.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the operation to run.\n * @param {number} [options.skip] The number of documents to skip before counting.\n * @param {Collection~countCallback} [callback] The command result callback.\n * @return {Promise} returns Promise if no callback passed.\n * @see https://docs.mongodb.com/manual/reference/operator/query/expr/\n * @see https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n * @see https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n * @see https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n */\n\nCollection.prototype.countDocuments = function(query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  query = args.length ? args.shift() || {} : {};\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, countDocuments, [this, query, options, callback]);\n};\n\n/**\n * The distinct command returns returns a list of distinct values for the given key across a collection.\n * @method\n * @param {string} key Field of the document to find distinct values for.\n * @param {object} query The query for filtering the set of documents to which we apply the distinct filter.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.distinct = function(key, query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  const queryOption = args.length ? args.shift() || {} : {};\n  const optionsOption = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, distinct, [\n    this,\n    key,\n    queryOption,\n    optionsOption,\n    callback\n  ]);\n};\n\n/**\n * Retrieve all the indexes on the collection.\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.indexes = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, indexes, [this, options, callback]);\n};\n\n/**\n * Get all the collection statistics.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {number} [options.scale] Divide the returned sizes by scale value.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The collection result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.stats = function(options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, stats, [this, options, callback]);\n};\n\n/**\n * @typedef {Object} Collection~findAndModifyWriteOpResult\n * @property {object} value Document returned from findAndModify command.\n * @property {object} lastErrorObject The raw lastErrorObject returned from the command.\n * @property {Number} ok Is 1 if the command executed correctly.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~findAndModifyCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~findAndModifyWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Find a document and delete it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to remove\n * @param {object} [options] Optional settings.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nCollection.prototype.findOneAndDelete = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n\n  return executeOperation(this.s.topology, findOneAndDelete, [this, filter, options, callback]);\n};\n\n/**\n * Find a document and replace it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to replace\n * @param {object} replacement The Document that replaces the matching document\n * @param {object} [options] Optional settings.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {boolean} [options.upsert=false] Upsert the document if it does not exist.\n * @param {boolean} [options.returnOriginal=true] When false, returns the updated document rather than the original. The default is true.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nCollection.prototype.findOneAndReplace = function(filter, replacement, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n  if (replacement == null || typeof replacement !== 'object')\n    throw toError('replacement parameter must be an object');\n\n  return executeOperation(this.s.topology, findOneAndReplace, [\n    this,\n    filter,\n    replacement,\n    options,\n    callback\n  ]);\n};\n\n/**\n * Find a document and update it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to update\n * @param {object} update Update operations to be performed on the document\n * @param {object} [options] Optional settings.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {boolean} [options.upsert=false] Upsert the document if it does not exist.\n * @param {boolean} [options.returnOriginal=true] When false, returns the updated document rather than the original. The default is true.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nCollection.prototype.findOneAndUpdate = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n  if (update == null || typeof update !== 'object')\n    throw toError('update parameter must be an object');\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  return executeOperation(this.s.topology, findOneAndUpdate, [\n    this,\n    filter,\n    update,\n    options,\n    callback\n  ]);\n};\n\n/**\n * Find and update a document.\n * @method\n * @param {object} query Query object to locate the object to modify.\n * @param {array} sort If multiple docs match, choose the first one in the specified sort order as the object to manipulate.\n * @param {object} doc The fields/vals to be updated.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.remove=false] Set to true to remove the object before returning.\n * @param {boolean} [options.upsert=false] Perform an upsert operation.\n * @param {boolean} [options.new=false] Set to true if you want to return the modified object rather than the original. Ignored for remove.\n * @param {object} [options.projection] Object containing the field projection for the result returned from the operation.\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {Collection~findAndModifyCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use findOneAndUpdate, findOneAndReplace or findOneAndDelete instead\n */\nCollection.prototype.findAndModify = deprecate(function(query, sort, doc, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  sort = args.length ? args.shift() || [] : [];\n  doc = args.length ? args.shift() : null;\n  options = args.length ? args.shift() || {} : {};\n\n  // Clone options\n  options = Object.assign({}, options);\n  // Force read preference primary\n  options.readPreference = ReadPreference.PRIMARY;\n\n  return executeOperation(this.s.topology, findAndModify, [\n    this,\n    query,\n    sort,\n    doc,\n    options,\n    callback\n  ]);\n}, 'collection.findAndModify is deprecated. Use findOneAndUpdate, findOneAndReplace or findOneAndDelete instead.');\n\n/**\n * Find and remove a document.\n * @method\n * @param {object} query Query object to locate the object to modify.\n * @param {array} sort If multiple docs match, choose the first one in the specified sort order as the object to manipulate.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use findOneAndDelete instead\n */\nCollection.prototype.findAndRemove = deprecate(function(query, sort, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  sort = args.length ? args.shift() || [] : [];\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, findAndRemove, [this, query, sort, options, callback]);\n}, 'collection.findAndRemove is deprecated. Use findOneAndDelete instead.');\n\n/**\n * Execute an aggregation framework pipeline against the collection, needs MongoDB >= 2.2\n * @method\n * @param {object} [pipeline=[]] Array containing all the aggregation framework commands for the execution.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {object} [options.cursor] Return the query as cursor, on 2.6 > it returns as a real cursor on pre 2.6 it returns as an emulated cursor.\n * @param {number} [options.cursor.batchSize] The batchSize for the cursor\n * @param {boolean} [options.explain=false] Explain returns the aggregation execution plan (requires mongodb 2.6 >).\n * @param {boolean} [options.allowDiskUse=false] allowDiskUse lets the server know if it can use disk to store temporary results for the aggregation (requires mongodb 2.6 >).\n * @param {number} [options.maxTimeMS] maxTimeMS specifies a cumulative time limit in milliseconds for processing operations on the cursor. MongoDB interrupts the operation at the earliest following interrupt point.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {string} [options.comment] Add a comment to an aggregation command\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~aggregationCallback} callback The command result callback\n * @return {(null|AggregationCursor)}\n */\nCollection.prototype.aggregate = function(pipeline, options, callback) {\n  if (Array.isArray(pipeline)) {\n    // Set up callback if one is provided\n    if (typeof options === 'function') {\n      callback = options;\n      options = {};\n    }\n\n    // If we have no options or callback we are doing\n    // a cursor based aggregation\n    if (options == null && callback == null) {\n      options = {};\n    }\n  } else {\n    // Aggregation pipeline passed as arguments on the method\n    const args = Array.prototype.slice.call(arguments, 0);\n    // Get the callback\n    callback = args.pop();\n    // Get the possible options object\n    const opts = args[args.length - 1];\n    // If it contains any of the admissible options pop it of the args\n    options =\n      opts &&\n      (opts.readPreference ||\n        opts.explain ||\n        opts.cursor ||\n        opts.out ||\n        opts.maxTimeMS ||\n        opts.hint ||\n        opts.allowDiskUse)\n        ? args.pop()\n        : {};\n    // Left over arguments is the pipeline\n    pipeline = args;\n  }\n\n  // Ignore readConcern option\n  let ignoreReadConcern = false;\n\n  // Build the command\n  const command = { aggregate: this.s.name, pipeline: pipeline };\n\n  // If out was specified\n  if (typeof options.out === 'string') {\n    pipeline.push({ $out: options.out });\n    // Ignore read concern\n    ignoreReadConcern = true;\n  } else if (pipeline.length > 0 && pipeline[pipeline.length - 1]['$out']) {\n    ignoreReadConcern = true;\n  }\n\n  // Decorate command with writeConcern if out has been specified\n  if (\n    pipeline.length > 0 &&\n    pipeline[pipeline.length - 1]['$out'] &&\n    this.s.topology.capabilities().commandsTakeWriteConcern\n  ) {\n    applyWriteConcern(command, { db: this.s.db, collection: this }, options);\n  }\n\n  // Have we specified collation\n  try {\n    decorateWithCollation(command, this, options);\n  } catch (err) {\n    if (typeof callback === 'function') return callback(err, null);\n    throw err;\n  }\n\n  // If we have bypassDocumentValidation set\n  if (options.bypassDocumentValidation === true) {\n    command.bypassDocumentValidation = options.bypassDocumentValidation;\n  }\n\n  // Do we have a readConcern specified\n  if (!ignoreReadConcern) {\n    decorateWithReadConcern(command, this, options);\n  }\n\n  // If we have allowDiskUse defined\n  if (options.allowDiskUse) command.allowDiskUse = options.allowDiskUse;\n  if (typeof options.maxTimeMS === 'number') command.maxTimeMS = options.maxTimeMS;\n\n  // If we are giving a hint\n  if (options.hint) command.hint = options.hint;\n\n  options = Object.assign({}, options);\n  // Ensure we have the right read preference inheritance\n  options.readPreference = resolveReadPreference(options, { db: this.s.db, collection: this });\n\n  // If explain has been specified add it\n  if (options.explain) {\n    if (command.readConcern || command.writeConcern) {\n      throw toError('\"explain\" cannot be used on an aggregate call with readConcern/writeConcern');\n    }\n    command.explain = options.explain;\n  }\n\n  if (typeof options.comment === 'string') command.comment = options.comment;\n\n  // Validate that cursor options is valid\n  if (options.cursor != null && typeof options.cursor !== 'object') {\n    throw toError('cursor options must be an object');\n  }\n\n  options.cursor = options.cursor || {};\n  if (options.batchSize) options.cursor.batchSize = options.batchSize;\n  command.cursor = options.cursor;\n\n  // promiseLibrary\n  options.promiseLibrary = this.s.promiseLibrary;\n\n  // Set the AggregationCursor constructor\n  options.cursorFactory = AggregationCursor;\n  if (typeof callback !== 'function') {\n    if (!this.s.topology.capabilities()) {\n      throw new MongoError('cannot connect to server');\n    }\n\n    // Allow disk usage command\n    if (typeof options.allowDiskUse === 'boolean') command.allowDiskUse = options.allowDiskUse;\n    if (typeof options.maxTimeMS === 'number') command.maxTimeMS = options.maxTimeMS;\n\n    // Execute the cursor\n    return this.s.topology.cursor(this.s.namespace, command, options);\n  }\n\n  return handleCallback(callback, null, this.s.topology.cursor(this.s.namespace, command, options));\n};\n\n/**\n * Create a new Change Stream, watching for new changes (insertions, updates, replacements, deletions, and invalidations) in this collection.\n * @method\n * @since 3.0.0\n * @param {Array} [pipeline] An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.\n * @param {object} [options] Optional settings\n * @param {string} [options.fullDocument='default'] Allowed values: \u2018default\u2019, \u2018updateLookup\u2019. When set to \u2018updateLookup\u2019, the change stream will include both a delta describing the changes to the document, as well as a copy of the entire document that was changed from some time after the change occurred.\n * @param {object} [options.resumeAfter] Specifies the logical starting point for the new change stream. This should be the _id field from a previously returned change stream document.\n * @param {number} [options.maxAwaitTimeMS] The maximum amount of time for the server to wait on new documents to satisfy a change stream query\n * @param {number} [options.batchSize] The number of documents to return per batch. See {@link https://docs.mongodb.com/manual/reference/command/aggregate|aggregation documentation}.\n * @param {object} [options.collation] Specify collation settings for operation. See {@link https://docs.mongodb.com/manual/reference/command/aggregate|aggregation documentation}.\n * @param {ReadPreference} [options.readPreference] The read preference. Defaults to the read preference of the database or collection. See {@link https://docs.mongodb.com/manual/reference/read-preference|read preference documentation}.\n * @param {Timestamp} [options.startAtClusterTime] receive change events that occur after the specified timestamp\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {ChangeStream} a ChangeStream instance.\n */\nCollection.prototype.watch = function(pipeline, options) {\n  pipeline = pipeline || [];\n  options = options || {};\n\n  // Allow optionally not specifying a pipeline\n  if (!Array.isArray(pipeline)) {\n    options = pipeline;\n    pipeline = [];\n  }\n\n  return new ChangeStream(this, pipeline, options);\n};\n\n/**\n * The callback format for results\n * @callback Collection~parallelCollectionScanCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Cursor[]} cursors A list of cursors returned allowing for parallel reading of collection.\n */\n\n/**\n * Return N number of parallel cursors for a collection allowing parallel reading of entire collection. There are\n * no ordering guarantees for returned results.\n * @method\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.batchSize] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {number} [options.numCursors=1] The maximum number of parallel command cursors to return (the number of returned cursors will be in the range 1:numCursors)\n * @param {boolean} [options.raw=false] Return all BSON documents as Raw Buffer documents.\n * @param {Collection~parallelCollectionScanCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.parallelCollectionScan = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = { numCursors: 1 });\n  // Set number of cursors to 1\n  options.numCursors = options.numCursors || 1;\n  options.batchSize = options.batchSize || 1000;\n\n  options = Object.assign({}, options);\n  // Ensure we have the right read preference inheritance\n  options.readPreference = resolveReadPreference(options, { db: this.s.db, collection: this });\n\n  // Add a promiseLibrary\n  options.promiseLibrary = this.s.promiseLibrary;\n\n  if (options.session) {\n    options.session = undefined;\n  }\n\n  return executeOperation(this.s.topology, parallelCollectionScan, [this, options, callback], {\n    skipSessions: true\n  });\n};\n\n/**\n * Execute a geo search using a geo haystack index on a collection.\n *\n * @method\n * @param {number} x Point to search on the x axis, ensure the indexes are ordered in the same order.\n * @param {number} y Point to search on the y axis, ensure the indexes are ordered in the same order.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxDistance] Include results up to maxDistance from the point.\n * @param {object} [options.search] Filter the results by a query.\n * @param {number} [options.limit=false] Max number of results to return.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.geoHaystackSearch = function(x, y, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 2);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, geoHaystackSearch, [this, x, y, options, callback]);\n};\n\n/**\n * Run a group command across a collection\n *\n * @method\n * @param {(object|array|function|code)} keys An object, array or function expressing the keys to group by.\n * @param {object} condition An optional condition that must be true for a row to be considered.\n * @param {object} initial Initial value of the aggregation counter object.\n * @param {(function|Code)} reduce The reduce function aggregates (reduces) the objects iterated\n * @param {(function|Code)} finalize An optional function to be run on each item in the result set just before the item is returned.\n * @param {boolean} command Specify if you wish to run using the internal group command or using eval, default is true.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated MongoDB 3.6 or higher no longer supports the group command. We recommend rewriting using the aggregation framework.\n */\nCollection.prototype.group = deprecate(function(\n  keys,\n  condition,\n  initial,\n  reduce,\n  finalize,\n  command,\n  options,\n  callback\n) {\n  const args = Array.prototype.slice.call(arguments, 3);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  reduce = args.length ? args.shift() : null;\n  finalize = args.length ? args.shift() : null;\n  command = args.length ? args.shift() : null;\n  options = args.length ? args.shift() || {} : {};\n\n  // Make sure we are backward compatible\n  if (!(typeof finalize === 'function')) {\n    command = finalize;\n    finalize = null;\n  }\n\n  if (\n    !Array.isArray(keys) &&\n    keys instanceof Object &&\n    typeof keys !== 'function' &&\n    !(keys._bsontype === 'Code')\n  ) {\n    keys = Object.keys(keys);\n  }\n\n  if (typeof reduce === 'function') {\n    reduce = reduce.toString();\n  }\n\n  if (typeof finalize === 'function') {\n    finalize = finalize.toString();\n  }\n\n  // Set up the command as default\n  command = command == null ? true : command;\n\n  return executeOperation(this.s.topology, group, [\n    this,\n    keys,\n    condition,\n    initial,\n    reduce,\n    finalize,\n    command,\n    options,\n    callback\n  ]);\n},\n'MongoDB 3.6 or higher no longer supports the group command. We recommend rewriting using the aggregation framework.');\n\n/**\n * Run Map Reduce across a collection. Be aware that the inline option for out will return an array of results not a collection.\n *\n * @method\n * @param {(function|string)} map The mapping function.\n * @param {(function|string)} reduce The reduce function.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {object} [options.out] Sets the output target for the map reduce job. *{inline:1} | {replace:'collectionName'} | {merge:'collectionName'} | {reduce:'collectionName'}*\n * @param {object} [options.query] Query filter object.\n * @param {object} [options.sort] Sorts the input objects using this key. Useful for optimization, like sorting by the emit key for fewer reduces.\n * @param {number} [options.limit] Number of objects to return from collection.\n * @param {boolean} [options.keeptemp=false] Keep temporary data.\n * @param {(function|string)} [options.finalize] Finalize function.\n * @param {object} [options.scope] Can pass in variables that can be access from map/reduce/finalize.\n * @param {boolean} [options.jsMode=false] It is possible to make the execution stay in JS. Provided in MongoDB > 2.0.X.\n * @param {boolean} [options.verbose=false] Provide statistics on job execution time.\n * @param {boolean} [options.bypassDocumentValidation=false] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @throws {MongoError}\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.mapReduce = function(map, reduce, options, callback) {\n  if ('function' === typeof options) (callback = options), (options = {});\n  // Out must allways be defined (make sure we don't break weirdly on pre 1.8+ servers)\n  if (null == options.out) {\n    throw new Error(\n      'the out option parameter must be defined, see mongodb docs for possible values'\n    );\n  }\n\n  if ('function' === typeof map) {\n    map = map.toString();\n  }\n\n  if ('function' === typeof reduce) {\n    reduce = reduce.toString();\n  }\n\n  if ('function' === typeof options.finalize) {\n    options.finalize = options.finalize.toString();\n  }\n\n  return executeOperation(this.s.topology, mapReduce, [this, map, reduce, options, callback]);\n};\n\n/**\n * Initiate an Out of order batch write operation. All operations will be buffered into insert/update/remove commands executed out of order.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {UnorderedBulkOperation}\n */\nCollection.prototype.initializeUnorderedBulkOp = function(options) {\n  options = options || {};\n  options.promiseLibrary = this.s.promiseLibrary;\n  return unordered(this.s.topology, this, options);\n};\n\n/**\n * Initiate an In order bulk write operation. Operations will be serially executed in the order they are added, creating a new operation for each switch in types.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {OrderedBulkOperation} callback The command result callback\n * @return {null}\n */\nCollection.prototype.initializeOrderedBulkOp = function(options) {\n  options = options || {};\n  options.promiseLibrary = this.s.promiseLibrary;\n  return ordered(this.s.topology, this, options);\n};\n\n/**\n * Return the db logger\n * @method\n * @return {Logger} return the db logger\n * @ignore\n */\nCollection.prototype.getLogger = function() {\n  return this.s.db.s.logger;\n};\n\nmodule.exports = Collection;\n", "idx": 1, "id": 15090, "msg": "This should be `boolean` instead of `Boolean`.", "proj": "mongodb-node-mongodb-native", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -1065,12 +1065,19 @@ namespace pwiz.Skyline.Model.AuditLog\n             if (!string.IsNullOrEmpty(ExtraInfo))\n                 writer.WriteElementString(EL.extra_info, ExtraInfo.EscapeNonPrintableChars());\n \n-            writer.WriteElement(EL.message, UndoRedo);\n-            writer.WriteElement(EL.message, Summary);\n+            writer.WriteElement(EL.undo_redo, UndoRedo);\n+            writer.WriteElement(EL.summary, Summary);\n \n-            var startIndex = InsertUndoRedoIntoAllInfo ? 1 : 0;\n-            for (var i = startIndex; i < _allInfo.Count; ++i)\n-                writer.WriteElement(EL.message, _allInfo[i]);\n+            foreach (var allInfo in _allInfoNoUndoRedo)\n+                writer.WriteElement(EL.all_info, allInfo);\n+\n+            if (!string.IsNullOrEmpty(ExtraInfo) && LogMessage.ExpansionToken.EnumerateTokens(ExtraInfo).Any())\n+            {\n+                EnExtraInfo = null;\n+                writer.WriteElementString(EL.en_extra_info, EnExtraInfo);\n+            }\n+\n+            writer.WriteElementString(EL.en_hash, EnHash.ActualHash);\n         }\n \n         public void ReadXml(XmlReader reader)", "y": 0, "oldf": "\ufeff/*\n * Original author: Tobias Rohde <tobiasr .at. uw.edu>,\n *                  MacCoss Lab, Department of Genome Sciences, UW\n *\n * Copyright 2018 University of Washington - Seattle, WA\n * \n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nusing System;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.IO;\nusing System.Linq;\nusing System.Security.Cryptography;\nusing System.Security.Principal;\nusing System.Text;\nusing System.Threading;\nusing System.Xml;\nusing System.Xml.Schema;\nusing System.Xml.Serialization;\nusing pwiz.Common.Collections;\nusing pwiz.Common.DataBinding;\nusing pwiz.Common.SystemUtil;\nusing pwiz.Skyline.Model.DocSettings;\nusing pwiz.Skyline.Model.Serialization;\nusing pwiz.Skyline.Properties;\nusing pwiz.Skyline.Util;\nusing pwiz.Skyline.Util.Extensions;\n\nnamespace pwiz.Skyline.Model.AuditLog\n{\n    [XmlRoot(XML_ROOT)]\n    public class AuditLogList : Immutable, IXmlSerializable\n    {\n        public const string XML_ROOT = \"audit_log\"; // Not L10N\n        public const string EXT = \".skyl\"; // Not L10N\n\n        public static bool IgnoreTestChecks { get; set; }\n\n        public AuditLogList(AuditLogEntry entries)\n        {\n            AuditLogEntries = entries;\n        }\n\n        public AuditLogList() : this(AuditLogEntry.ROOT)\n        {\n        }\n\n\n        public AuditLogEntry AuditLogEntries { get; private set; }\n\n        public static SrmDocument ToggleAuditLogging(SrmDocument doc, bool enable)\n        {\n            var newDoc = doc.ChangeSettings(\n                doc.Settings.ChangeDataSettings(doc.Settings.DataSettings.ChangeAuditLogging(enable)));\n\n            return newDoc;\n        }\n\n        public XmlSchema GetSchema()\n        {\n            return null;\n        }\n\n        public static AuditLogList Deserialize(XmlReader reader)\n        {\n            return reader.Deserialize(new AuditLogList());\n        }\n\n        private AuditLogEntry ReadEntries(XmlReader reader)\n        {\n            if (!reader.IsStartElement(AuditLogEntry.XML_ROOT))\n                return AuditLogEntry.ROOT;\n\n            return reader.DeserializeElement<AuditLogEntry>()\n                .ChangeParent(ReadEntries(reader));\n        }\n\n        public void ReadXml(XmlReader reader)\n        {\n            var isEmpty = reader.IsEmptyElement;\n            reader.ReadStartElement();\n\n            AuditLogEntries = ReadEntries(reader);\n\n            if (!isEmpty)\n                reader.ReadEndElement();\n\n            Validate();\n        }\n\n        public void Validate()\n        {\n            if (ReferenceEquals(AuditLogEntries, AuditLogEntry.ROOT))\n                return;\n\n            var time = DateTime.MaxValue;\n            var logIndex = int.MinValue;\n\n            foreach (var entry in AuditLogEntries.Enumerate())\n            {\n                Assume.IsTrue(entry.TimeStamp <= time && entry.LogIndex > logIndex,\n                    AuditLogStrings.AuditLogList_Validate_Audit_log_is_corrupted__Audit_log_entry_time_stamps_and_indices_should_be_decreasing);\n\n                time = entry.TimeStamp;\n                logIndex = entry.LogIndex;\n            }\n        }\n\n        public void WriteXml(XmlWriter writer)\n        {\n            AuditLogEntries.Enumerate().ForEach(writer.WriteElement);\n        }\n\n        private enum EL\n        {\n            document_hash\n        }\n\n        public string GetHash()\n        {\n            // Surprisingly, the XmlTextWriter disposes the stream\n            using (var stream = new MemoryStream())\n            {\n                using (var writer = new XmlTextWriter(stream, Encoding.UTF8) { Formatting = Formatting.Indented })\n                {\n                    WriteToXmlWriter(writer);\n\n                    stream.Seek(0, SeekOrigin.Begin);\n\n                    // Leave stream open, otherwise XmlTextWriter will try to close it which causes an exception\n                    using (var reader = new StreamReader(stream, Encoding.UTF8, true, 1024, true))\n                    {\n                        return AuditLogEntry.Hash(reader.ReadToEnd());\n                    }\n                }\n            }\n        }\n\n        public void WriteToFile(string fileName, string documentHash)\n        {\n            using (var writer = new XmlTextWriter(fileName, Encoding.UTF8) {Formatting = Formatting.Indented})\n            {\n                WriteToXmlWriter(writer, documentHash);\n            }\n        }\n\n        private void WriteToXmlWriter(XmlWriter writer, string documentHash = null)\n        {\n            writer.WriteStartDocument();\n            writer.WriteStartElement(\"audit_log_root\"); // Not L10N\n            if (!string.IsNullOrEmpty(documentHash))\n                writer.WriteElementString(EL.document_hash, documentHash);\n            writer.WriteElement(this);\n            writer.WriteEndElement();\n            writer.WriteEndDocument();\n        }\n\n        public static AuditLogList ReadFromFile(string fileName, out string documentHash)\n        {\n            using (var reader = new XmlTextReader(fileName))\n            {\n                reader.ReadStartElement();\n                documentHash = reader.ReadElementString(EL.document_hash.ToString());\n                var result = reader.DeserializeElement<AuditLogList>();\n                reader.ReadEndElement();\n                return result;\n            }\n        }\n    }\n\n    public class DocumentNodeCounts : AuditLogOperationSettings<DocumentNodeCounts>\n    {\n        public DocumentNodeCounts(SrmDocument doc)\n        {\n            IsPeptideOnly = doc.DocumentType == SrmDocument.DOCUMENT_TYPE.proteomic;\n            MoleculeGroupCount = doc.GetCount((int) SrmDocument.Level.MoleculeGroups);\n            MoleculeCount = doc.GetCount((int)SrmDocument.Level.Molecules);\n            PrecursorCount = doc.GetCount((int)SrmDocument.Level.TransitionGroups);\n            TransitionCount = doc.GetCount((int)SrmDocument.Level.Transitions);\n        }\n\n        public bool IsPeptideOnly { get; private set; }\n\n        [Track(customLocalizer:typeof(MoleculeGroupCountLocalizer))]\n        public int MoleculeGroupCount { get; private set; }\n        [Track(customLocalizer: typeof(MoleculeCountLocalizer))]\n        public int MoleculeCount { get; private set; }\n        [Track]\n        public int PrecursorCount { get; private set; }\n        [Track]\n        public int TransitionCount { get; private set; }\n\n        private class PeptideSmallMoleculeLocalizer : CustomPropertyLocalizer\n        {\n            private string _propertyName;\n\n            protected PeptideSmallMoleculeLocalizer(string propertyName) : base(PropertyPath.Parse(nameof(IsPeptideOnly)), true)\n            {\n                _propertyName = propertyName;\n            }\n\n            private string _smallMoleculeName\n            {\n                get { return _propertyName + \"_smallmol\"; } // Not L10N\n            }\n\n            protected override string Localize(ObjectPair<object> objectPair)\n            {\n                return (bool)objectPair.NewObject ? _propertyName : _smallMoleculeName;\n            }\n\n            public override string[] PossibleResourceNames\n            {\n                get { return new[] { _propertyName, _smallMoleculeName }; }\n            }\n        }\n\n        private class MoleculeGroupCountLocalizer : PeptideSmallMoleculeLocalizer\n        {\n            public MoleculeGroupCountLocalizer() : base(nameof(MoleculeGroupCount)) { }\n        }\n\n        private class MoleculeCountLocalizer : PeptideSmallMoleculeLocalizer\n        {\n            public MoleculeCountLocalizer() : base(nameof(MoleculeCount)) { }\n        }\n    }\n\n    public class AuditLogEntryCreator\n    {\n        public AuditLogEntryCreator(Func<SrmDocumentPair, AuditLogEntry> create)\n        {\n            Create = create;\n        }\n\n        public Func<SrmDocumentPair, AuditLogEntry> Create { get; private set; }\n    }\n\n    public class AuditLogEntryCreatorList\n    {\n        public AuditLogEntryCreatorList()\n        {\n            EntryCreators = new List<AuditLogEntryCreator>();\n        }\n\n        public void Add(Func<SrmDocumentPair, AuditLogEntry> fn)\n        {\n            Add(new AuditLogEntryCreator(fn));\n        }\n\n        public void Add(params MessageInfo[] allInfoMessages)\n        {\n            Add(docPair => AuditLogEntry.CreateEmptyEntry(docPair.OldDoc).ChangeAllInfo(allInfoMessages));\n        }\n\n        public void Add(AuditLogEntryCreator entryCreator)\n        {\n            EntryCreators.Add(entryCreator);\n        }\n\n        public IEnumerable<AuditLogEntry> CreateEntries(SrmDocumentPair docPair)\n        {\n            foreach (var entryCreator in EntryCreators)\n            {\n                var entry = entryCreator.Create(docPair);\n                if (entry != null)\n                    yield return entry;\n            }\n        }\n\n        public List<AuditLogEntryCreator> EntryCreators { get; private set; }\n    }\n\n    public class LogException : Exception\n    {\n        public LogException(Exception innerException) : base(null, innerException)\n        {\n        }\n\n        public override string Message\n        {\n            get\n            {\n                if (OldUndoRedoMessage == null)\n                {\n                    return AuditLogStrings.LogException_Message_An_error_occured_while_creating_a_log_entry__The_document_was_still_successfully_modified;\n                }\n                else\n                {\n                    return string.Format(\n                        AuditLogStrings.LogException_Message_An_error_occured_while_creating_a_log_entry__Action___0___was_still_successfull,\n                        OldUndoRedoMessage);\n                }\n            }\n        }\n\n        public string OldUndoRedoMessage { get; set; }\n    }\n\n    public class MessageArgs\n    {\n        public MessageArgs(params object[] args)\n        {\n            Args = args;\n        }\n\n        public static MessageArgs Create(params object[] args)\n        {\n            return new MessageArgs(args);\n        }\n\n        public static MessageArgs DefaultSingular(object obj)\n        {\n            return Create(obj);\n        }\n\n        public object[] Args { get; set; }\n    }\n\n    [XmlRoot(XML_ROOT)]\n    public class AuditLogEntry : Immutable, IXmlSerializable\n    {\n        public const string XML_ROOT = \"audit_log_entry\"; // Not L10N\n\n        private ImmutableList<LogMessage> _allInfo;\n        private Action<AuditLogEntry> _undoAction;\n        private static int _logIndexCounter;\n\n        public static AuditLogEntry ROOT = new AuditLogEntry { Count = 0, LogIndex = int.MaxValue };\n\n        public bool IsRoot\n        {\n            get { return ReferenceEquals(this, ROOT); }\n        }\n\n        private AuditLogEntry(SrmDocument document, DateTime timeStamp, string reason, bool insertIntoUndoRedo = false,\n            string extraInfo = null) : this()\n        {\n            LogIndex = Interlocked.Increment(ref _logIndexCounter);\n\n            SkylineVersion = Install.Version;\n            if (Install.Is64Bit)\n                SkylineVersion += \" (64-Bit)\"; // Not L10N\n\n            FormatVersion = document.FormatVersion;\n            TimeStamp = timeStamp;\n            ExtraInfo = extraInfo;\n\n            using (var identity = WindowsIdentity.GetCurrent())\n            {\n                // ReSharper disable once PossibleNullReferenceException\n                User = identity.Name;\n            }\n\n            Reason = reason ?? string.Empty;\n            //InsertUndoRedoIntoAllInfo = insertIntoUndoRedo;\n        }\n\n        /// Parent node, topmost node will be <see cref=\"ROOT\" />\n        public AuditLogEntry Parent { get; private set; }\n        // The number of nodes in this linked list, including this node itself\n        public int Count { get; private set; }\n\n        public string SkylineVersion { get; private set; }\n        public DocumentFormat FormatVersion { get; private set; }\n        public DateTime TimeStamp { get; private set; }\n        public string User { get; private set; }\n        public string Reason { get; private set; }\n        public string ExtraInfo { get; private set; }\n        public LogMessage UndoRedo { get; private set; }\n        public LogMessage Summary { get; private set; }\n\n        public bool InsertUndoRedoIntoAllInfo\n        {\n            get { return UndoRedo != null; }\n        }\n\n        public MessageType? CountEntryType { get; private set; }\n\n        public Action UndoAction\n        {\n            get\n            {\n                if (_undoAction == null)\n                    return null;\n\n                return () => _undoAction(this);\n            }\n        }\n\n        private IEnumerable<LogMessage> _mergeAllInfo\n        {\n            get\n            {\n                return !InsertUndoRedoIntoAllInfo\n                    ? _allInfo\n                    : (_allInfo.Count == 1 ? _allInfo : _allInfoNoUndoRedo);\n            }\n        }\n\n        private IEnumerable<LogMessage> _allInfoNoUndoRedo\n        {\n            get { return InsertUndoRedoIntoAllInfo ? _allInfo.Skip(1) : _allInfo; }\n        }\n\n        public IList<LogMessage> AllInfo\n        {\n            get { return _allInfo; }\n            private set\n            {\n                _allInfo = ImmutableList.ValueOf(InsertUndoRedoIntoAllInfo\n                    ? ImmutableList.Singleton(UndoRedo).Concat(value)\n                    : value);\n            }\n        }\n\n        public bool HasSingleAllInfoRow\n        {\n            get { return _allInfo != null && _allInfo.Count == (InsertUndoRedoIntoAllInfo ? 2 : 1); }\n        }\n\n        public int LogIndex { get; private set; }\n\n        public static string Hash(byte[] bytes)\n        {\n            using (var sha1 = new SHA1CryptoServiceProvider())\n            {\n                var hash = sha1.ComputeHash(bytes);\n                return string.Join(string.Empty, hash.Select(b => b.ToString(\"X2\"))); // Not L10N\n            }\n        }\n\n        public static string Hash(string s)\n        {\n            return Hash(Encoding.UTF8.GetBytes(s));\n        }\n\n        public AuditLogEntry this[int i]\n        {\n            get { return Enumerate().ElementAt(i); }\n        }\n\n        public IEnumerable<AuditLogEntry> Enumerate()\n        {\n            if (IsRoot)\n                yield break;\n\n            yield return this;\n            foreach (var entry in Parent.Enumerate())\n                yield return entry;\n        }\n\n        #region Property change functions\n\n        public AuditLogEntry ChangeParent(AuditLogEntry parent)\n        {\n            return ChangeProp(ImClone(this), im =>\n            {\n                im.Parent = parent;\n                im.Count = parent.Count + 1;\n            });\n        }\n        public AuditLogEntry ChangeReason(string reason)\n        {\n            return ChangeProp(ImClone(this), im => im.Reason = reason);\n        }\n\n        public AuditLogEntry ChangeUndoRedo(MessageInfo undoRedo)\n        {\n            return ChangeProp(ImClone(this), im =>\n            {\n                im.UndoRedo = undoRedo.ToMessage(LogLevel.undo_redo);\n                // Since the all info list might contain the undo redo message,\n                // changing it requires updating the all info\n                if (InsertUndoRedoIntoAllInfo)\n                    im.AllInfo = im._allInfoNoUndoRedo.ToList();\n            });\n        }\n\n        public AuditLogEntry ChangeSummary(MessageInfo summary)\n        {\n            return ChangeSummary(summary.ToMessage(LogLevel.summary));\n        }\n\n        public AuditLogEntry ChangeSummary(LogMessage summary)\n        {\n            return ChangeProp(ImClone(this), im => im.Summary = summary);\n        }\n\n        public AuditLogEntry ChangeAllInfo(IList<MessageInfo> allInfo)\n        {\n            return ChangeAllInfo(allInfo.Select(info => info.ToMessage(LogLevel.all_info)).ToList());\n        }\n\n        public AuditLogEntry ChangeAllInfo(IList<LogMessage> allInfo)\n        {\n            return ChangeProp(ImClone(this), im => im.AllInfo = allInfo);\n        }\n\n        public AuditLogEntry AppendAllInfo(IEnumerable<LogMessage> allInfo)\n        {\n            return ChangeProp(ImClone(this), im => im.AllInfo = _allInfoNoUndoRedo.Concat(allInfo).ToList());\n        }\n\n        public AuditLogEntry AppendAllInfo(IEnumerable<MessageInfo> allInfo)\n        {\n            return ChangeProp(ImClone(this),\n                im => im.AllInfo = _allInfoNoUndoRedo\n                    .Concat(allInfo.Select(msgInfo => msgInfo.ToMessage(LogLevel.all_info))).ToList());\n        }\n\n        public AuditLogEntry ClearAllInfo()\n        {\n            return ChangeAllInfo(new LogMessage[0]);\n        }\n\n        public AuditLogEntry ChangeUndoAction(Action<AuditLogEntry> undoAction)\n        {\n            return ChangeProp(ImClone(this), im => im._undoAction = undoAction);\n        }\n\n        public AuditLogEntry ChangeExtraInfo(string extraInfo)\n        {\n            return ChangeProp(ImClone(this), im => im.ExtraInfo = extraInfo);\n        }\n\n        #endregion\n\n        #region Functions to create log entries\n\n        private static MessageInfo GetLogClearedInfo(int clearedCount)\n        {\n            return new MessageInfo(clearedCount > 1 ? MessageType.log_cleared : MessageType.log_cleared_single,\n                clearedCount);\n        }\n\n        private static MessageInfo GetUnloggedMessages(int unloggedCount)\n        {\n            return new MessageInfo(\n                unloggedCount == 1 ? MessageType.log_unlogged_change : MessageType.log_unlogged_changes,\n                unloggedCount);\n        }\n\n        private AuditLogEntry CreateUnloggedEntry(SrmDocument doc, out bool replace)\n        {\n            var countEntry = doc.AuditLog.AuditLogEntries;\n\n            replace = countEntry != null && countEntry.CountEntryType == MessageType.log_unlogged_changes;\n            if (!replace)\n            {\n                countEntry = CreateSimpleEntry(doc, MessageType.log_unlogged_changes, 0)\n                    // This entry needs a non-undoredo all info message to work properly\n                    .ChangeAllInfo(ImmutableList.Singleton(new MessageInfo(MessageType.log_unlogged_changes, 0)));\n                countEntry.CountEntryType = MessageType.log_unlogged_changes;\n            }\n            return countEntry.ChangeUndoRedo(GetUnloggedMessages(int.Parse(countEntry.UndoRedo.Names[0]) + 1))\n                .ChangeSummary(GetUnloggedMessages(int.Parse(countEntry.Summary.Names[0]) + 1))\n                .ChangeAllInfo(ImmutableList.Singleton(GetUnloggedMessages(\n                    int.Parse(countEntry._allInfoNoUndoRedo.First().Names[0]) + _allInfoNoUndoRedo.Count())));\n        }\n\n        public static AuditLogEntry GetAuditLoggingStartExistingDocEntry(SrmDocument doc)\n        {\n            // Don't want to have these entries in tests (except for the AuditLogSaving test which actually tests this type of entry)\n            if (Program.FunctionalTest && !AuditLogList.IgnoreTestChecks)\n                return null;\n\n            var defaultDoc = new SrmDocument(SrmSettingsList.GetDefault());\n            var docPair = SrmDocumentPair.Create(defaultDoc, doc);\n\n            var changeFromDefaultSettings = SettingsLogFunction(docPair);\n            var initialNodeCounts = doc.Children.Count > 0 ? new DocumentNodeCounts(doc).EntryCreator.Create(docPair) : null;\n\n            var entry = CreateSimpleEntry(doc, MessageType.start_log_existing_doc)\n                .Merge(initialNodeCounts).Merge(changeFromDefaultSettings);\n\n            if (changeFromDefaultSettings != null || initialNodeCounts != null)\n                return entry;\n\n            return null;\n        }\n\n        public static AuditLogEntry GetUndocumentedChangeEntry(SrmDocument doc)\n        {\n            return CreateSimpleEntry(doc, MessageType.undocumented_change);\n        }\n\n        /// <summary>\n        /// Creates a log entry that indicated the log was cleared and how many changes were cleared\n        /// </summary>\n        public static AuditLogEntry ClearLogEntry(SrmDocument doc)\n        {\n            var entries = doc.AuditLog.AuditLogEntries;\n            var undoRedoCount = entries.Count;\n            var allInfoCount = 0;\n\n            foreach (var e in entries.Enumerate())\n            {\n                if (e.CountEntryType == MessageType.log_cleared ||\n                    e.CountEntryType == MessageType.log_unlogged_changes)\n                {\n                    undoRedoCount += int.Parse(e.UndoRedo.Names[0]) - 1;\n                    allInfoCount += int.Parse(e._allInfoNoUndoRedo.First().Names[0]) - 1;\n                }\n\n                allInfoCount += e._allInfoNoUndoRedo.Count();\n            }\n\n            var entry = CreateEmptyEntry(doc);\n            entry.CountEntryType = MessageType.log_cleared;\n\n            var msgInfoUndoRedo = GetLogClearedInfo(undoRedoCount);\n\n            return entry.ChangeUndoRedo(msgInfoUndoRedo)\n                .ChangeSummary(msgInfoUndoRedo)\n                .ChangeAllInfo(ImmutableList.Singleton(GetLogClearedInfo(allInfoCount)));\n        }\n\n        /// <summary>\n        /// Creates an empty entry that can be useful when making entries\n        /// that will get merged into another entry\n        /// </summary>\n        public static AuditLogEntry CreateEmptyEntry(SrmDocument document)\n        {\n            return new AuditLogEntry(document, DateTime.Now, string.Empty);\n        }\n\n        /// <summary>\n        /// Creates a simple entry only containing one message in each category with the given type and names\n        /// extra info\n        /// </summary>\n        public static AuditLogEntry CreateSimpleEntry(SrmDocument document, MessageType type, params object[] args)\n        {\n            return CreateSingleMessageEntry(document, new MessageInfo(type, args));\n        }\n\n        /// <summary>\n        /// Creates an entry that depends on whether there are 1 or multiple elements\n        /// in a collection.\n        /// </summary>\n        /// <param name=\"document\">Document change was made to</param>\n        /// <param name=\"singular\">Message to show if there's 1 element in the collection. Only element gets passed as argument to the message</param>\n        /// <param name=\"plural\">Message to show if there are multiple elements. The count gets passed to the message</param>\n        /// <param name=\"items\">Items to consider</param>\n        /// <param name=\"singularArgsFunc\">Converts the element to MessageArgs that get passed to the singular message</param>\n        /// <param name=\"pluralArgs\">Args to be passed to plural. If null, count is passed as single arg</param>\n        public static AuditLogEntry CreateCountChangeEntry<T>(SrmDocument document, MessageType singular,\n            MessageType plural, ICollection<T> items, Func<T, MessageArgs> singularArgsFunc, MessageArgs pluralArgs)\n        {\n            var singularArgs = items.Count == 1 ? singularArgsFunc(items.FirstOrDefault()) : null;\n            return CreateCountChangeEntry(document, singular, plural, items.Count, singularArgs,\n                pluralArgs);\n        }\n\n        /// <summary>\n        /// Creates an entry that depends on whether there are 1 or multiple elements\n        /// in a collection.\n        /// </summary>\n        /// <param name=\"document\">Document change was made to</param>\n        /// <param name=\"singular\">Message to show if there's 1 element in the collection. Only element gets passed as argument to the message</param>\n        /// <param name=\"plural\">Message to show if there are multiple elements. The count gets passed to the message</param>\n        /// <param name=\"items\">Items to consider</param>\n        /// <param name=\"count\">Number of elements in IEnumerable. If null, all items are enumerated</param>\n        /// <param name=\"singularArgsFunc\">Converts the element to MessageArgs that get passed to the singular message</param>\n        /// <param name=\"pluralArgs\">Args to be passed to plural. If null, count is passed as single arg</param>\n        public static AuditLogEntry CreateCountChangeEntry<T>(SrmDocument document, MessageType singular,\n            MessageType plural, IEnumerable<T> items, int? count, Func<T, MessageArgs> singularArgsFunc, MessageArgs pluralArgs)\n        {\n            if (!count.HasValue)\n            {\n                var collection = items as ICollection<T> ?? items.ToArray();\n                return CreateCountChangeEntry(document, singular, plural, collection, singularArgsFunc, pluralArgs);\n            }\n\n            var singularArgs = count.Value == 1 ? singularArgsFunc(items.FirstOrDefault()) : null;\n            return CreateCountChangeEntry(document, singular, plural, count.Value, singularArgs, pluralArgs);\n        }\n\n        // Overload for common case\n        public static AuditLogEntry CreateCountChangeEntry(SrmDocument document, MessageType singular,\n            MessageType plural, ICollection<string> items)\n        {\n            return CreateCountChangeEntry(document, singular, plural, items, MessageArgs.DefaultSingular, null);\n        }\n\n        // Overload for common case\n        public static AuditLogEntry CreateCountChangeEntry(SrmDocument document, MessageType singular,\n            MessageType plural, IEnumerable<string> items, int? count)\n        {\n            if (!count.HasValue)\n            {\n                var collection = items as ICollection<string> ?? items.ToArray();\n                return CreateCountChangeEntry(document, singular, plural, collection, MessageArgs.DefaultSingular, null);\n            }\n\n            return CreateCountChangeEntry(document, singular, plural, items, count, MessageArgs.DefaultSingular, null);\n        }\n\n        private static AuditLogEntry CreateCountChangeEntry(SrmDocument document, MessageType singular,\n            MessageType plural, int count, MessageArgs singularArgs, MessageArgs pluralArgs)\n        {\n            switch (count)\n            {\n                case 1:\n                    return CreateSimpleEntry(document, singular, singularArgs.Args);\n                default:\n                    return CreateSimpleEntry(document, plural,\n                        pluralArgs != null ? pluralArgs.Args : MessageArgs.Create(count).Args);\n            }\n        }\n\n        /// <summary>\n        /// Creates a simple entry only containing one message in each category with the given type and names and\n        /// extra info\n        /// </summary>\n        public static AuditLogEntry CreateSingleMessageEntry(SrmDocument document,\n            MessageInfo info, string extraInfo = null)\n        {\n            var result = new AuditLogEntry(document, DateTime.Now, string.Empty, true, extraInfo)\n            {\n                UndoRedo = info.ToMessage(LogLevel.undo_redo),\n                Summary = info.ToMessage(LogLevel.summary),\n                AllInfo = new LogMessage[0]//new[] { info.ToMessage(LogLevel.all_info) }\n            };\n\n            return result;\n        }\n\n        // Creates a new PropertyName with top most node removed\n        private static PropertyName RemoveTopmostParent(PropertyName name)\n        {\n            if (name == PropertyName.ROOT || name.Parent == PropertyName.ROOT)\n                return name;\n\n            if (name.Parent.Parent == PropertyName.ROOT)\n                return PropertyName.ROOT.SubProperty(name);\n\n            return RemoveTopmostParent(name.Parent).SubProperty(name);\n        }\n\n        /// <summary>\n        /// Creates a log entry representing the changes in the diff tree\n        /// </summary>\n        /// <param name=\"document\">Document changes were made to</param>\n        /// <param name=\"tree\">Tree that should be logged</param>\n        /// <param name=\"extraInfo\">Text that should be displayed when clicking the magnifying glass in the audit log form</param>\n        /// <returns></returns>\n        public static AuditLogEntry CreateSettingsChangeEntry(SrmDocument document, DiffTree tree, string extraInfo = null)\n        {\n            if (tree?.Root == null)\n                return null;\n\n            var result = new AuditLogEntry(document, tree.TimeStamp, string.Empty, true, extraInfo);\n\n            var nodeNamePair = tree.Root.FindFirstMultiChildParent(tree, PropertyName.ROOT, true, false);\n            // Remove \"Settings\" from property name if possible\n            if (nodeNamePair.Name != null && nodeNamePair.Name.Parent != PropertyName.ROOT)\n            {\n                var name = nodeNamePair.Name;\n                while (name.Parent.Parent != PropertyName.ROOT)\n                    name = name.Parent;\n\n                if (name.Parent.Name == \"{0:Settings}\") // Not L10N\n                {\n                    name = RemoveTopmostParent(nodeNamePair.Name);\n                    nodeNamePair = nodeNamePair.ChangeName(name);\n                }\n            }\n\n            result.UndoRedo = nodeNamePair.ToMessage(LogLevel.undo_redo);\n            result.Summary = tree.Root.FindFirstMultiChildParent(tree, PropertyName.ROOT, false, false)\n                .ToMessage(LogLevel.summary);\n            result.AllInfo = tree.Root.FindAllLeafNodes(tree, PropertyName.ROOT, true)\n                .Select(n => n.ToMessage(LogLevel.all_info)).ToArray();\n            \n            return result;\n        }\n\n        private static bool IsTransitionDiff(Type type)\n        {\n            if (type == typeof(TransitionDocNode) || type == typeof(TransitionGroupDocNode))\n                return true;\n\n            if (type.GenericTypeArguments.Length == 1 &&\n                typeof(IEnumerable<>).MakeGenericType(type.GenericTypeArguments[0]).IsAssignableFrom(type))\n                type = type.GenericTypeArguments[0];\n            else\n                return false;\n            return IsTransitionDiff(type);\n        }\n\n        public static AuditLogEntry DiffDocNodes(MessageType action, SrmDocumentPair documentPair,\n            bool ignoreTransitions, params object[] actionParameters)\n        {\n            var property = RootProperty.Create(typeof(Targets));\n            var objInfo = new ObjectInfo<object>(documentPair.OldDoc.Targets, documentPair.NewDoc.Targets,\n                documentPair.OldDoc, documentPair.NewDoc, documentPair.OldDoc, documentPair.NewDoc);\n\n            var diffTree = DiffTree.FromEnumerator(\n                Reflector<Targets>.EnumerateDiffNodes(objInfo, property, false,\n                    ignoreTransitions\n                        ? (Func<DiffNode, bool>) (node => !IsTransitionDiff(node.Property.PropertyType))\n                        : null),\n                DateTime.Now);\n\n            if (diffTree.Root != null)\n            {\n                var message = new MessageInfo(action, actionParameters);\n                var entry = CreateSettingsChangeEntry(documentPair.OldDoc, diffTree)\n                    .ChangeUndoRedo(message);\n                return entry;\n            }\n\n            return null;\n        }\n\n        public static AuditLogEntry DiffDocNodes(MessageType action, SrmDocumentPair documentPair, params object[] actionParameters)\n        {\n            return DiffDocNodes(action, documentPair, false, actionParameters);\n        }\n\n        /// <summary>\n        /// Creates a log entry indicating that logging was enabled or disabled\n        /// </summary>\n        public static AuditLogEntry CreateLogEnabledDisabledEntry(SrmDocument document)\n        {\n            var result = new AuditLogEntry(document, DateTime.Now, string.Empty);\n\n            var type = document.Settings.DataSettings.AuditLogging ? MessageType.log_enabled : MessageType.log_disabled;\n\n            result.UndoRedo = new LogMessage(LogLevel.undo_redo, type, string.Empty, false);\n            result.Summary = new LogMessage(LogLevel.summary, type, string.Empty, false);\n            result.AllInfo = new List<LogMessage> { new LogMessage(LogLevel.all_info, type, string.Empty, false) };\n\n            return result;\n        }\n\n        public static AuditLogEntry CreateExceptionEntry(SrmDocument doc, LogException ex)\n        {\n            // ReSharper disable PossibleNullReferenceException\n            if (ex.OldUndoRedoMessage == null)\n            {\n                return CreateSingleMessageEntry(doc,\n                    new MessageInfo(MessageType.log_error, ex.InnerException.GetType().Name), ex.InnerException.StackTrace);\n            }\n            else\n            {\n                var entry = CreateSingleMessageEntry(doc,\n                    new MessageInfo(MessageType.empty_single_arg, ex.OldUndoRedoMessage), ex.InnerException.StackTrace);\n                return entry.AppendAllInfo(ImmutableList.Singleton(new MessageInfo(MessageType.log_error_old_msg,\n                    ex.InnerException.GetType().Name, ex.OldUndoRedoMessage)));\n            }\n            // ReSharper enable PossibleNullReferenceException\n        }\n\n        #endregion Functions to create log entries\n\n        /// <summary>\n        /// Merges two audit log entries, by adding the other entries\n        /// all info messages and extra info to the current entry\n        /// </summary>\n        /// <param name=\"other\">Entry to merge into the current one</param>\n        /// <param name=\"append\">true if all info and extra info should be appended, if false they are replaced</param>\n        /// <returns>A new, merged entry</returns>\n        public AuditLogEntry Merge(AuditLogEntry other, bool append = true)\n        {\n            if (other == null)\n                return this;\n\n            var entry = append\n                ? AppendAllInfo(other._mergeAllInfo)\n                : ChangeAllInfo(other._mergeAllInfo.ToList());\n\n            if (!string.IsNullOrEmpty(other.ExtraInfo))\n            {\n                entry = entry.ChangeExtraInfo(string.IsNullOrEmpty(ExtraInfo) || !append\n                    ? other.ExtraInfo\n                    : TextUtil.LineSeparate(ExtraInfo, other.ExtraInfo));\n            }\n            return entry;\n        }\n\n        /// <summary>\n        /// Merges the entries created by the given creator list into the current entry\n        /// </summary>\n        /// <param name=\"docPair\">Documents used to construct new entries</param>\n        /// <param name=\"creatorList\">Entries to be constructed</param>\n        /// <param name=\"append\">see <see cref=\"Merge(AuditLogEntry,bool)\"/></param>\n        /// <returns>A new, merged entry</returns>\n        public AuditLogEntry Merge(SrmDocumentPair docPair, AuditLogEntryCreatorList creatorList, bool append = true)\n        {\n            return creatorList.EntryCreators.Aggregate(this, (e, c) => e.Merge(c.Create(docPair), append));\n        }\n\n        /// <summary>\n        /// Updates the document with the given AuditLogEntry. If the entry is non-null and logging is enabled,\n        /// it is added to the document. If audit logging changed from being disabled to enabled and the document is different\n        /// from the default document(no children, default settings) a \"start from existing document\" entry is added. If audit logging\n        /// changes from being enabled to disabled, the log is cleared.\n        /// </summary>\n        /// <param name=\"entry\">The entry to add</param>\n        /// <param name=\"docPair\">Document pair containing the new document the entry should get added to</param>\n        /// <returns>A new document with this entry added</returns>\n        public static SrmDocument UpdateDocument(AuditLogEntry entry, SrmDocumentPair docPair)\n        {\n            var newDoc = docPair.NewDoc;\n            /*if (Settings.Default.AuditLogging || CountEntryType == MessageType.log_cleared)\n            {\n                newDoc = newDoc.ChangeAuditLog(ChangeParent(docPair.NewDoc.AuditLog.AuditLogEntries));\n            }\n            else\n            {\n                var entry = CreateUnloggedEntry(document, out var replace);\n\n                // This is the only property we have to copy over, since we don't care about the content of the log message\n                // but still want the ability to undo unlogged entries. We only change the undo action for the first\n                // unlogged message entry, otherwise clicking the undo button in the grid would undo the unlogged changes one-by-one\n                // instead of in a single \"batch undo.\" TODO: Is this how it should be?\n                // (This one-by-one behavior can still be achieved by using the undo redo buffer)\n                if (!replace)\n                    entry = entry.ChangeUndoAction(_undoAction);\n\n                var oldEntries = document.AuditLog.AuditLogEntries;\n                var newEntries = replace\n                    ? entry.ChangeParent(oldEntries?.Parent)\n                    : entry.ChangeParent(oldEntries);\n\n                newDoc = document.ChangeAuditLog(newEntries);\n            }*/\n\n            var oldLogging = docPair.OldDoc.Settings.DataSettings.AuditLogging;\n            var newLogging = docPair.NewDoc.Settings.DataSettings.AuditLogging;\n\n            if (oldLogging && !newLogging)\n                return newDoc.ChangeAuditLog(ROOT);\n            else if (!oldLogging && newLogging)\n            {\n                var startEntry = GetAuditLoggingStartExistingDocEntry(newDoc);\n                if (startEntry != null && entry != null)\n                    startEntry = startEntry.ChangeParent(entry);\n                entry = startEntry ?? entry;\n            }\n\n            if (newLogging)\n                newDoc = entry?.AppendEntryToDocument(newDoc) ?? newDoc;\n\n            return newDoc;\n        }\n\n        public SrmDocument AppendEntryToDocument(SrmDocument doc)\n        {\n            return doc.ChangeAuditLog(ChangeParent(doc.AuditLog.AuditLogEntries));\n        }\n\n        public static bool ConvertPathsToFileNames { get; set; }\n\n        /// <summary>\n        /// Compares the settings objects of the given documents and creates an entry\n        /// for the differences\n        /// </summary>\n        /// <param name=\"documentPair\">The pair of documents to compare</param>\n        /// <returns>A log entry containing the changes</returns>\n        public static AuditLogEntry SettingsLogFunction(SrmDocumentPair documentPair)\n        {\n            var property = RootProperty.Create(typeof(SrmSettings), \"Settings\"); // Not L10N\n            var objInfo = new ObjectInfo<object>(documentPair.OldDoc.Settings, documentPair.NewDoc.Settings,\n                documentPair.OldDoc, documentPair.NewDoc, documentPair.OldDoc, documentPair.NewDoc);\n\n            var tree = DiffTree.FromEnumerator(Reflector<SrmSettings>.EnumerateDiffNodes(objInfo, property, false));\n            return tree.Root != null ? CreateSettingsChangeEntry(documentPair.OldDoc, tree) : null;\n        }\n\n        public static PropertyName GetNodeName(SrmDocument doc, DocNode docNode)\n        {\n            DocNode nextNode = null;\n            if (docNode is TransitionDocNode)\n            {\n                nextNode = doc.MoleculeTransitionGroups.FirstOrDefault(group =>\n                    group.Transitions.Any(t => ReferenceEquals(t.Id, docNode.Id)));\n            }\n            else if (docNode is TransitionGroupDocNode)\n            {\n                nextNode = doc.Molecules.FirstOrDefault(group =>\n                    group.TransitionGroups.Any(t => ReferenceEquals(t.Id, docNode.Id)));\n            }\n            else if (docNode is PeptideDocNode)\n            {\n                nextNode = doc.MoleculeGroups.FirstOrDefault(group =>\n                    group.Molecules.Any(m => ReferenceEquals(m.Id, docNode.Id)));\n            }\n\n            // TODO: add other interface to these doc nodes?\n            var auditLogObj = docNode as IAuditLogObject;\n            \n            if (auditLogObj == null)\n                return null;\n\n            var text = auditLogObj.AuditLogText;\n\n            return nextNode == null\n                ? PropertyName.ROOT.SubProperty(text)\n                : GetNodeName(doc, nextNode).SubProperty(text);\n        }\n\n        #region Implementation of IXmlSerializable\n\n        private AuditLogEntry()\n        {\n            Parent = ROOT;\n            Count = 1;\n        }\n\n        public XmlSchema GetSchema()\n        {\n            return null;\n        }\n\n        private enum ATTR\n        {\n            format_version,\n            time_stamp,\n            user,\n            count_type,\n            insert_undo_redo\n        }\n\n        private enum EL\n        {\n            message,\n            reason,\n            extra_info\n        }\n\n        public static AuditLogEntry Deserialize(XmlReader reader)\n        {\n            return reader.Deserialize(new AuditLogEntry());\n        }\n\n        public void WriteXml(XmlWriter writer)\n        {\n            writer.WriteAttribute(ATTR.format_version, FormatVersion.AsDouble());\n            writer.WriteAttribute(ATTR.time_stamp, TimeStamp.ToUniversalTime().ToString(CultureInfo.InvariantCulture));\n            writer.WriteAttribute(ATTR.user, User);\n\n            //writer.WriteAttribute(ATTR.insert_undo_redo, InsertUndoRedoIntoAllInfo);\n\n            if (CountEntryType.HasValue)\n                writer.WriteAttribute(ATTR.count_type, CountEntryType);\n\n            if (!string.IsNullOrEmpty(Reason))\n                writer.WriteElementString(EL.reason, Reason);\n\n            if (!string.IsNullOrEmpty(ExtraInfo))\n                writer.WriteElementString(EL.extra_info, ExtraInfo.EscapeNonPrintableChars());\n\n            writer.WriteElement(EL.message, UndoRedo);\n            writer.WriteElement(EL.message, Summary);\n\n            var startIndex = InsertUndoRedoIntoAllInfo ? 1 : 0;\n            for (var i = startIndex; i < _allInfo.Count; ++i)\n                writer.WriteElement(EL.message, _allInfo[i]);\n        }\n\n        public void ReadXml(XmlReader reader)\n        {\n            LogIndex = Interlocked.Increment(ref _logIndexCounter);\n            FormatVersion = new DocumentFormat(reader.GetDoubleAttribute(ATTR.format_version));\n            var time = DateTime.Parse(reader.GetAttribute(ATTR.time_stamp), CultureInfo.InvariantCulture);\n            TimeStamp = DateTime.SpecifyKind(time, DateTimeKind.Utc).ToLocalTime();\n            User = reader.GetAttribute(ATTR.user);\n\n            //InsertUndoRedoIntoAllInfo = reader.GetBoolAttribute(ATTR.insert_undo_redo);\n\n            var countType = reader.GetAttribute(ATTR.count_type);\n            if (countType == null)\n                CountEntryType = null;\n            else\n                CountEntryType = (MessageType) Enum.Parse(typeof(MessageType), countType);\n\n            reader.ReadStartElement();\n\n            Reason = reader.IsStartElement(EL.reason) ? reader.ReadElementString() : string.Empty;\n            ExtraInfo = reader.IsStartElement(EL.extra_info) ? reader.ReadElementString().UnescapeNonPrintableChars() : string.Empty;\n\n            UndoRedo = reader.DeserializeElement<LogMessage>().ChangeLevel(LogLevel.undo_redo);\n            Summary = reader.DeserializeElement<LogMessage>().ChangeLevel(LogLevel.summary);\n\n            var list = new List<LogMessage>();\n            while (reader.IsStartElement(EL.message))\n                list.Add(reader.DeserializeElement<LogMessage>().ChangeLevel(LogLevel.all_info));\n\n            AllInfo = list;\n\n            reader.ReadEndElement();\n        }\n        #endregion\n    }\n\n    /// <summary>\n    /// Base class for objects that represent settings for an operation\n    /// that creates log messages.\n    /// </summary>\n    /// <typeparam name=\"T\">Type of the derived settings type</typeparam>\n    public class AuditLogOperationSettings<T> : Immutable where T : AuditLogOperationSettings<T>\n    {\n        /// <summary>\n        /// Info used in constructing an entry from this settings object.\n        /// Should be overriden, unless entry gets merged into another entry\n        /// </summary>\n        public virtual MessageInfo MessageInfo\n        {\n            get { return new MessageInfo(MessageType.none); }\n        }\n        \n        /// <summary>\n        /// Returns an object that can create an audit log entry from this settings object.\n        /// This is useful when a form gets disposed, but the entry should be constructed\n        /// later without accessing the disposed form.\n        /// </summary>\n        public AuditLogEntryCreator EntryCreator\n        {\n            get { return new AuditLogEntryCreator(ImClone(this).CreateEntry); }\n        }\n\n        protected virtual AuditLogEntry CreateBaseEntry(SrmDocumentPair docPair)\n        {\n            return MessageInfo.Type == MessageType.none\n                ? AuditLogEntry.CreateEmptyEntry(docPair.OldDoc).ChangeAllInfo(new LogMessage[0])\n                : AuditLogEntry.CreateSingleMessageEntry(docPair.OldDoc, MessageInfo);\n        }\n\n        protected virtual AuditLogEntry CreateEntry(SrmDocumentPair docPair)\n        {\n            var baseEntry = CreateBaseEntry(docPair);\n            var rootProp = RootProperty.Create(typeof(T));\n\n            var objectInfo = new ObjectInfo<object>()\n                    .ChangeObjectPair(ObjectPair<object>.Create(null, this))\n                    .ChangeRootObjectPair(docPair.ToObjectType());\n\n            var diffTree =\n                DiffTree.FromEnumerator(Reflector<T>.EnumerateDiffNodes(docPair.ToObjectType(), rootProp, (T)this), DateTime.Now);\n            if (diffTree.Root == null)\n                return baseEntry;\n\n            var settingsString = Reflector<T>.ToString(objectInfo.RootObjectPair, diffTree.Root,\n                ToStringState.DEFAULT.ChangeFormatWhitespace(true));\n            var entry = AuditLogEntry.CreateSettingsChangeEntry(docPair.OldDoc, diffTree, settingsString);\n            return baseEntry.Merge(entry);\n        }\n    }\n    \n    /// <summary>\n    /// An optional interface for forms that have a settings object representing\n    /// them\n    /// </summary>\n    /// <typeparam name=\"T\">Type of the settings object</typeparam>\n    public interface IAuditLogModifier<out T> where T : AuditLogOperationSettings<T>\n    {\n        T FormSettings { get; }\n    }     \n}", "idx": 20, "id": 12539, "msg": "", "proj": "ProteoWizard-pwiz", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -69,10 +69,11 @@ define(['loading', 'listView', 'cardBuilder', 'libraryMenu', 'libraryBrowser', '\n             showLoadingMessage();\n             var query = getQuery(view);\n             var promise1 = ApiClient.getItems(Dashboard.getCurrentUserId(), query);\n+            // TODO: promise2 is unused, check if necessary.\n             var promise2 = Dashboard.getCurrentUser();\n             Promise.all([promise1, promise2]).then(function (responses) {\n                 var result = responses[0];\n-                responses[1];\n+                // TODO: Is the scroll necessary?\n                 window.scrollTo(0, 0);\n                 var html = '';\n                 var viewStyle = getPageData(view).view;", "y": 1, "oldf": "define(['loading', 'listView', 'cardBuilder', 'libraryMenu', 'libraryBrowser', 'apphost', 'imageLoader', 'userSettings', 'emby-itemscontainer'], function (loading, listView, cardBuilder, libraryMenu, libraryBrowser, appHost, imageLoader, userSettings) {\n    'use strict';\n\n    return function (view, params) {\n        function getPageData(context) {\n            var key = getSavedQueryKey(context);\n            var pageData = data[key];\n\n            if (!pageData) {\n                pageData = data[key] = {\n                    query: {\n                        SortBy: 'SortName',\n                        SortOrder: 'Ascending',\n                        IncludeItemTypes: 'Playlist',\n                        Recursive: true,\n                        Fields: 'PrimaryImageAspectRatio,SortName,CumulativeRunTimeTicks,CanDelete',\n                        StartIndex: 0\n                    },\n                    view: libraryBrowser.getSavedView(key) || 'Poster'\n                };\n\n                if (userSettings.libraryPageSize() > 0) {\n                    pageData.query['Limit'] = userSettings.libraryPageSize();\n                }\n\n                pageData.query.ParentId = libraryMenu.getTopParentId();\n                libraryBrowser.loadSavedQueryValues(key, pageData.query);\n            }\n\n            return pageData;\n        }\n\n        function getQuery(context) {\n            return getPageData(context).query;\n        }\n\n        function getSavedQueryKey(context) {\n            if (!context.savedQueryKey) {\n                context.savedQueryKey = libraryBrowser.getSavedQueryKey();\n            }\n\n            return context.savedQueryKey;\n        }\n\n        function showLoadingMessage() {\n            loading.show();\n        }\n\n        function hideLoadingMessage() {\n            loading.hide();\n        }\n\n        function onViewStyleChange() {\n            var viewStyle = getPageData(view).view;\n            var itemsContainer = view.querySelector('.itemsContainer');\n\n            if ('List' == viewStyle) {\n                itemsContainer.classList.add('vertical-list');\n                itemsContainer.classList.remove('vertical-wrap');\n            } else {\n                itemsContainer.classList.remove('vertical-list');\n                itemsContainer.classList.add('vertical-wrap');\n            }\n\n            itemsContainer.innerHTML = '';\n        }\n\n        function reloadItems() {\n            showLoadingMessage();\n            var query = getQuery(view);\n            var promise1 = ApiClient.getItems(Dashboard.getCurrentUserId(), query);\n            var promise2 = Dashboard.getCurrentUser();\n            Promise.all([promise1, promise2]).then(function (responses) {\n                var result = responses[0];\n                responses[1];\n                window.scrollTo(0, 0);\n                var html = '';\n                var viewStyle = getPageData(view).view;\n                view.querySelector('.listTopPaging').innerHTML = libraryBrowser.getQueryPagingHtml({\n                    startIndex: query.StartIndex,\n                    limit: query.Limit,\n                    totalRecordCount: result.TotalRecordCount,\n                    viewButton: false,\n                    showLimit: false,\n                    updatePageSizeSetting: false,\n                    addLayoutButton: true,\n                    layouts: 'List,Poster,PosterCard,Thumb,ThumbCard',\n                    currentLayout: viewStyle\n                });\n\n                if (result.TotalRecordCount) {\n                    if (viewStyle == 'List') {\n                        html = listView.getListViewHtml({\n                            items: result.Items,\n                            sortBy: query.SortBy\n                        });\n                    } else if (viewStyle == 'PosterCard') {\n                        html = cardBuilder.getCardsHtml({\n                            items: result.Items,\n                            shape: 'square',\n                            coverImage: true,\n                            showTitle: true,\n                            cardLayout: true\n                        });\n                    } else if (viewStyle == 'Thumb') {\n                        html = cardBuilder.getCardsHtml({\n                            items: result.Items,\n                            shape: 'backdrop',\n                            showTitle: true,\n                            centerText: true,\n                            preferThumb: true,\n                            overlayPlayButton: true\n                        });\n                    } else if (viewStyle == 'ThumbCard') {\n                        html = cardBuilder.getCardsHtml({\n                            items: result.Items,\n                            shape: 'backdrop',\n                            showTitle: true,\n                            preferThumb: true,\n                            cardLayout: true\n                        });\n                    } else {\n                        html = cardBuilder.getCardsHtml({\n                            items: result.Items,\n                            shape: 'square',\n                            showTitle: true,\n                            coverImage: true,\n                            centerText: true,\n                            overlayPlayButton: true\n                        });\n                    }\n                    view.querySelector('.noItemsMessage').classList.add('hide');\n                } else {\n                    view.querySelector('.noItemsMessage').classList.remove('hide');\n                }\n\n                var elem = view.querySelector('.itemsContainer');\n                elem.innerHTML = html;\n                imageLoader.lazyChildren(elem);\n                var btnNextPage = view.querySelector('.btnNextPage');\n\n                if (btnNextPage) {\n                    btnNextPage.addEventListener('click', function () {\n                        if (userSettings.libraryPageSize() > 0) {\n                            query.StartIndex += query.Limit;\n                        }\n                        reloadItems();\n                    });\n                }\n\n                var btnPreviousPage = view.querySelector('.btnPreviousPage');\n\n                if (btnPreviousPage) {\n                    btnPreviousPage.addEventListener('click', function () {\n                        if (userSettings.libraryPageSize() > 0) {\n                            query.StartIndex = Math.max(0, query.StartIndex - query.Limit);\n                        }\n                        reloadItems();\n                    });\n                }\n\n                var btnChangeLayout = view.querySelector('.btnChangeLayout');\n\n                if (btnChangeLayout) {\n                    btnChangeLayout.addEventListener('layoutchange', function (e) {\n                        var layout = e.detail.viewStyle;\n                        getPageData(view).view = layout;\n                        libraryBrowser.saveViewSetting(getSavedQueryKey(view), layout);\n                        onViewStyleChange();\n                        reloadItems();\n                    });\n                }\n\n                libraryBrowser.saveQueryValues(getSavedQueryKey(view), query);\n                hideLoadingMessage();\n            });\n        }\n\n        var data = {};\n        view.addEventListener('viewbeforeshow', function () {\n            reloadItems();\n        });\n        view.querySelector('.btnNewPlaylist').addEventListener('click', function () {\n            require(['playlistEditor'], function (playlistEditor) {\n                var serverId = ApiClient.serverInfo().Id;\n                new playlistEditor().show({\n                    items: [],\n                    serverId: serverId\n                });\n            });\n        });\n        onViewStyleChange();\n    };\n});\n", "idx": 1, "id": 15533, "msg": "In case of paging, we should go to the top. But in the same time (on some pages), this is unacceptable when we change (or not) metadata - no individual updating. (#901)", "proj": "jellyfin-jellyfin-web", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -1288,7 +1288,7 @@ public interface List<T> extends Kind1<List<?>, T>, LinearSeq<T>, Stack<T> {\n \n     @Override\n     default <U> List<U> unit(Iterable<? extends U> iterable) {\n-        return List.ofAll(iterable);\n+        return ofAll(iterable);\n     }\n \n     @Override", "y": 0, "oldf": "/*     / \\____  _    _  ____   ______  / \\ ____  __    _______\n *    /  /    \\/ \\  / \\/    \\ /  /\\__\\/  //    \\/  \\  //  /\\__\\   J\u039bV\u039bSL\u039bNG\n *  _/  /  /\\  \\  \\/  /  /\\  \\\\__\\\\  \\  //  /\\  \\ /\\\\/ \\ /__\\ \\   Copyright 2014-2016 Javaslang, http://javaslang.io\n * /___/\\_/  \\_/\\____/\\_/  \\_/\\__\\/__/\\__\\_/  \\_//  \\__/\\_____/   Licensed under the Apache License, Version 2.0\n */\npackage javaslang.collection;\n\nimport javaslang.*;\nimport javaslang.collection.List.Nil;\nimport javaslang.collection.ListModule.*;\nimport javaslang.control.Option;\n\nimport java.io.*;\nimport java.util.*;\nimport java.util.function.*;\nimport java.util.stream.Collector;\n\n/**\n * An immutable {@code List} is an eager sequence of elements. Its immutability makes it suitable for concurrent programming.\n * <p>\n * A {@code List} is composed of a {@code head} element and a {@code tail} {@code List}.\n * <p>\n * There are two implementations of the {@code List} interface:\n * <ul>\n * <li>{@link Nil}, which represents the empty {@code List}.</li>\n * <li>{@link Cons}, which represents a {@code List} containing one or more elements.</li>\n * </ul>\n * Methods to obtain a {@code List}:\n * <pre>\n * <code>\n * // factory methods\n * List.empty()                        // = List.of() = Nil.instance()\n * List.of(x)                          // = new Cons&lt;&gt;(x, Nil.instance())\n * List.of(Object...)                  // e.g. List.of(1, 2, 3)\n * List.ofAll(Iterable)                // e.g. List.ofAll(Stream.of(1, 2, 3)) = 1, 2, 3\n * List.ofAll(&lt;primitive array&gt;) // e.g. List.of(new int[] {1, 2, 3}) = 1, 2, 3\n *\n * // int sequences\n * List.range(0, 3)              // = 0, 1, 2\n * List.rangeClosed(0, 3)        // = 0, 1, 2, 3\n * </code>\n * </pre>\n *\n * Note: A {@code List} is primary a {@code Seq} and extends {@code Stack} for technical reasons (so {@code Stack} does not need to wrap {@code List}).\n * <p>\n * If operating on a {@code List}, please prefer\n * <ul>\n * <li>{@link #prepend(Object)} over {@link #push(Object)}</li>\n * <li>{@link #prependAll(Iterable)} over {@link #pushAll(Iterable)}</li>\n * <li>{@link #tail()} over {@link #pop()}</li>\n * <li>{@link #tailOption()} over {@link #popOption()}</li>\n * </ul>\n *\n * Factory method applications:\n *\n * <pre>\n * <code>\n * List&lt;Integer&gt;       s1 = List.of(1);\n * List&lt;Integer&gt;       s2 = List.of(1, 2, 3);\n *                           // = List.of(new Integer[] {1, 2, 3});\n *\n * List&lt;int[]&gt;         s3 = List.ofAll(new int[] {1, 2, 3});\n * List&lt;List&lt;Integer&gt;&gt; s4 = List.ofAll(List.of(1, 2, 3));\n *\n * List&lt;Integer&gt;       s5 = List.ofAll(new int[] {1, 2, 3});\n * List&lt;Integer&gt;       s6 = List.ofAll(List.of(1, 2, 3));\n *\n * // cuckoo's egg\n * List&lt;Integer[]&gt;     s7 = List.&lt;Integer[]&gt; of(new Integer[] {1, 2, 3});\n * </code>\n * </pre>\n *\n * Example: Converting a String to digits\n *\n * <pre>\n * <code>\n * // = List(1, 2, 3)\n * List.of(\"123\".toCharArray()).map(c -&gt; Character.digit(c, 10))\n * </code>\n * </pre>\n *\n * See Okasaki, Chris: <em>Purely Functional Data Structures</em> (p. 7 ff.). Cambridge, 2003.\n *\n * @param <T> Component type of the List\n * @author Daniel Dietrich\n * @since 1.1.0\n */\npublic interface List<T> extends Kind1<List<?>, T>, LinearSeq<T>, Stack<T> {\n\n    long serialVersionUID = 1L;\n\n    /**\n     * Returns a {@link java.util.stream.Collector} which may be used in conjunction with\n     * {@link java.util.stream.Stream#collect(java.util.stream.Collector)} to obtain a {@link javaslang.collection.List}.\n     *\n     * @param <T> Component type of the List.\n     * @return A javaslang.collection.List Collector.\n     */\n    static <T> Collector<T, ArrayList<T>, List<T>> collector() {\n        final Supplier<ArrayList<T>> supplier = ArrayList::new;\n        final BiConsumer<ArrayList<T>, T> accumulator = ArrayList::add;\n        final BinaryOperator<ArrayList<T>> combiner = (left, right) -> {\n            left.addAll(right);\n            return left;\n        };\n        final Function<ArrayList<T>, List<T>> finisher = List::ofAll;\n        return Collector.of(supplier, accumulator, combiner, finisher);\n    }\n\n    /**\n     * Returns the single instance of Nil. Convenience method for {@code Nil.instance()} .\n     * <p>\n     * Note: this method intentionally returns type {@code List} and not {@code Nil}. This comes handy when folding.\n     * If you explicitly need type {@code Nil} use {@linkplain Nil#instance()}.\n     *\n     * @param <T> Component type of Nil, determined by type inference in the particular context.\n     * @return The empty list.\n     */\n    static <T> List<T> empty() {\n        return Nil.instance();\n    }\n\n    @Override\n    boolean isEmpty();\n\n    /**\n     * Narrows a widened {@code List<? extends T>} to {@code List<T>}\n     * by performing a type safe-cast. This is eligible because immutable/read-only\n     * collections are covariant.\n     *\n     * @param list A {@code List}.\n     * @param <T>  Component type of the {@code List}.\n     * @return the given {@code list} instance as narrowed type {@code List<T>}.\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> List<T> narrow(List<? extends T> list) {\n        return (List<T>) list;\n    }\n\n    /**\n     * Returns a singleton {@code List}, i.e. a {@code List} of one element.\n     *\n     * @param element An element.\n     * @param <T>     The component type\n     * @return A new List instance containing the given element\n     */\n    static <T> List<T> of(T element) {\n        return new Cons<>(element, Nil.instance());\n    }\n\n    /**\n     * Creates a List of the given elements.\n     * <pre>\n     * <code>\n     *   List.of(1, 2, 3, 4)\n     * = Nil.instance().prepend(4).prepend(3).prepend(2).prepend(1)\n     * = new Cons(1, new Cons(2, new Cons(3, new Cons(4, Nil.instance()))))\n     * </code>\n     * </pre>\n     *\n     * @param <T>      Component type of the List.\n     * @param elements Zero or more elements.\n     * @return A list containing the given elements in the same order.\n     * @throws NullPointerException if {@code elements} is null\n     */\n    @SafeVarargs\n    static <T> List<T> of(T... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        List<T> result = Nil.instance();\n        for (int i = elements.length - 1; i >= 0; i--) {\n            result = result.prepend(elements[i]);\n        }\n        return result;\n    }\n\n    /**\n     * Creates a List of the given elements.\n     * <p>\n     * The resulting list has the same iteration order as the given iterable of elements\n     * if the iteration order of the elements is stable.\n     *\n     * @param <T>      Component type of the List.\n     * @param elements An Iterable of elements.\n     * @return A list containing the given elements in the same order.\n     * @throws NullPointerException if {@code elements} is null\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> List<T> ofAll(Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        if (elements instanceof List) {\n            return (List<T>) elements;\n        } else if (elements instanceof java.util.List) {\n            List<T> result = Nil.instance();\n            final java.util.List<T> list = (java.util.List<T>) elements;\n            final ListIterator<T> iterator = list.listIterator(list.size());\n            while (iterator.hasPrevious()) {\n                result = result.prepend(iterator.previous());\n            }\n            return result;\n        } else if (elements instanceof NavigableSet) {\n            List<T> result = Nil.instance();\n            final java.util.Iterator<T> iterator = ((NavigableSet<T>) elements).descendingIterator();\n            while (iterator.hasNext()) {\n                result = result.prepend(iterator.next());\n            }\n            return result;\n        } else {\n            List<T> result = Nil.instance();\n            for (T element : elements) {\n                result = result.prepend(element);\n            }\n            return result.reverse();\n        }\n    }\n\n    /**\n     * Creates a List based on the elements of a boolean array.\n     *\n     * @param array a boolean array\n     * @return A new List of Boolean values\n     */\n    static List<Boolean> ofAll(boolean[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a byte array.\n     *\n     * @param array a byte array\n     * @return A new List of Byte values\n     */\n    static List<Byte> ofAll(byte[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a char array.\n     *\n     * @param array a char array\n     * @return A new List of Character values\n     */\n    static List<Character> ofAll(char[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a double array.\n     *\n     * @param array a double array\n     * @return A new List of Double values\n     */\n    static List<Double> ofAll(double[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a float array.\n     *\n     * @param array a float array\n     * @return A new List of Float values\n     */\n    static List<Float> ofAll(float[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of an int array.\n     *\n     * @param array an int array\n     * @return A new List of Integer values\n     */\n    static List<Integer> ofAll(int[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a long array.\n     *\n     * @param array a long array\n     * @return A new List of Long values\n     */\n    static List<Long> ofAll(long[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Creates a List based on the elements of a short array.\n     *\n     * @param array a short array\n     * @return A new List of Short values\n     */\n    static List<Short> ofAll(short[] array) {\n        Objects.requireNonNull(array, \"array is null\");\n        return List.ofAll(Iterator.ofAll(array));\n    }\n\n    /**\n     * Returns a List containing {@code n} values of a given Function {@code f}\n     * over a range of integer values from 0 to {@code n - 1}.\n     *\n     * @param <T> Component type of the List\n     * @param n   The number of elements in the List\n     * @param f   The Function computing element values\n     * @return A List consisting of elements {@code f(0),f(1), ..., f(n - 1)}\n     * @throws NullPointerException if {@code f} is null\n     */\n    static <T> List<T> tabulate(int n, Function<? super Integer, ? extends T> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return Collections.tabulate(n, f, List.empty(), List::of);\n    }\n\n    /**\n     * Returns a List containing {@code n} values supplied by a given Supplier {@code s}.\n     *\n     * @param <T> Component type of the List\n     * @param n   The number of elements in the List\n     * @param s   The Supplier computing element values\n     * @return A List of size {@code n}, where each element contains the result supplied by {@code s}.\n     * @throws NullPointerException if {@code s} is null\n     */\n    static <T> List<T> fill(int n, Supplier<? extends T> s) {\n        Objects.requireNonNull(s, \"s is null\");\n        return Collections.fill(n, s, List.empty(), List::of);\n    }\n\n    static List<Character> range(char from, char toExclusive) {\n        return List.ofAll(Iterator.range(from, toExclusive));\n    }\n\n    static List<Character> rangeBy(char from, char toExclusive, int step) {\n        return List.ofAll(Iterator.rangeBy(from, toExclusive, step));\n    }\n\n    static List<Double> rangeBy(double from, double toExclusive, double step) {\n        return List.ofAll(Iterator.rangeBy(from, toExclusive, step));\n    }\n\n    /**\n     * Creates a List of int numbers starting from {@code from}, extending to {@code toExclusive - 1}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.range(0, 0)  // = List()\n     * List.range(2, 0)  // = List()\n     * List.range(-2, 2) // = List(-2, -1, 0, 1)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @return a range of int values as specified or the empty range if {@code from >= toExclusive}\n     */\n    static List<Integer> range(int from, int toExclusive) {\n        return List.ofAll(Iterator.range(from, toExclusive));\n    }\n\n    /**\n     * Creates a List of int numbers starting from {@code from}, extending to {@code toExclusive - 1},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeBy(1, 3, 1)  // = List(1, 2)\n     * List.rangeBy(1, 4, 2)  // = List(1, 3)\n     * List.rangeBy(4, 1, -2) // = List(4, 2)\n     * List.rangeBy(4, 1, 2)  // = List()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @param step        the step\n     * @return a range of long values as specified or the empty range if<br>\n     * {@code from >= toInclusive} and {@code step > 0} or<br>\n     * {@code from <= toInclusive} and {@code step < 0}\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static List<Integer> rangeBy(int from, int toExclusive, int step) {\n        return List.ofAll(Iterator.rangeBy(from, toExclusive, step));\n    }\n\n    /**\n     * Creates a List of long numbers starting from {@code from}, extending to {@code toExclusive - 1}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.range(0L, 0L)  // = List()\n     * List.range(2L, 0L)  // = List()\n     * List.range(-2L, 2L) // = List(-2L, -1L, 0L, 1L)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @return a range of long values as specified or the empty range if {@code from >= toExclusive}\n     */\n    static List<Long> range(long from, long toExclusive) {\n        return List.ofAll(Iterator.range(from, toExclusive));\n    }\n\n    /**\n     * Creates a List of long numbers starting from {@code from}, extending to {@code toExclusive - 1},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeBy(1L, 3L, 1L)  // = List(1L, 2L)\n     * List.rangeBy(1L, 4L, 2L)  // = List(1L, 3L)\n     * List.rangeBy(4L, 1L, -2L) // = List(4L, 2L)\n     * List.rangeBy(4L, 1L, 2L)  // = List()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @param step        the step\n     * @return a range of long values as specified or the empty range if<br>\n     * {@code from >= toInclusive} and {@code step > 0} or<br>\n     * {@code from <= toInclusive} and {@code step < 0}\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static List<Long> rangeBy(long from, long toExclusive, long step) {\n        return List.ofAll(Iterator.rangeBy(from, toExclusive, step));\n    }\n\n    static List<Character> rangeClosed(char from, char toInclusive) {\n        return List.ofAll(Iterator.rangeClosed(from, toInclusive));\n    }\n\n    static List<Character> rangeClosedBy(char from, char toInclusive, int step) {\n        return List.ofAll(Iterator.rangeClosedBy(from, toInclusive, step));\n    }\n\n    static List<Double> rangeClosedBy(double from, double toInclusive, double step) {\n        return List.ofAll(Iterator.rangeClosedBy(from, toInclusive, step));\n    }\n\n    /**\n     * Creates a List of int numbers starting from {@code from}, extending to {@code toInclusive}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeClosed(0, 0)  // = List(0)\n     * List.rangeClosed(2, 0)  // = List()\n     * List.rangeClosed(-2, 2) // = List(-2, -1, 0, 1, 2)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @return a range of int values as specified or the empty range if {@code from > toInclusive}\n     */\n    static List<Integer> rangeClosed(int from, int toInclusive) {\n        return List.ofAll(Iterator.rangeClosed(from, toInclusive));\n    }\n\n    /**\n     * Creates a List of int numbers starting from {@code from}, extending to {@code toInclusive},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeClosedBy(1, 3, 1)  // = List(1, 2, 3)\n     * List.rangeClosedBy(1, 4, 2)  // = List(1, 3)\n     * List.rangeClosedBy(4, 1, -2) // = List(4, 2)\n     * List.rangeClosedBy(4, 1, 2)  // = List()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @param step        the step\n     * @return a range of int values as specified or the empty range if<br>\n     * {@code from > toInclusive} and {@code step > 0} or<br>\n     * {@code from < toInclusive} and {@code step < 0}\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static List<Integer> rangeClosedBy(int from, int toInclusive, int step) {\n        return List.ofAll(Iterator.rangeClosedBy(from, toInclusive, step));\n    }\n\n    /**\n     * Creates a List of long numbers starting from {@code from}, extending to {@code toInclusive}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeClosed(0L, 0L)  // = List(0L)\n     * List.rangeClosed(2L, 0L)  // = List()\n     * List.rangeClosed(-2L, 2L) // = List(-2L, -1L, 0L, 1L, 2L)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @return a range of long values as specified or the empty range if {@code from > toInclusive}\n     */\n    static List<Long> rangeClosed(long from, long toInclusive) {\n        return List.ofAll(Iterator.rangeClosed(from, toInclusive));\n    }\n\n    /**\n     * Creates a List of long numbers starting from {@code from}, extending to {@code toInclusive},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * List.rangeClosedBy(1L, 3L, 1L)  // = List(1L, 2L, 3L)\n     * List.rangeClosedBy(1L, 4L, 2L)  // = List(1L, 3L)\n     * List.rangeClosedBy(4L, 1L, -2L) // = List(4L, 2L)\n     * List.rangeClosedBy(4L, 1L, 2L)  // = List()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @param step        the step\n     * @return a range of int values as specified or the empty range if<br>\n     * {@code from > toInclusive} and {@code step > 0} or<br>\n     * {@code from < toInclusive} and {@code step < 0}\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static List<Long> rangeClosedBy(long from, long toInclusive, long step) {\n        return List.ofAll(Iterator.rangeClosedBy(from, toInclusive, step));\n    }\n\n    @Override\n    default List<T> append(T element) {\n        return foldRight(List.of(element), (x, xs) -> xs.prepend(x));\n    }\n\n    @Override\n    default List<T> appendAll(Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return foldRight(List.ofAll(elements), (x, xs) -> xs.prepend(x));\n    }\n\n    @Override\n    default List<List<T>> combinations() {\n        return List.rangeClosed(0, length()).map(this::combinations).flatMap(Function.identity());\n    }\n\n    @Override\n    default List<List<T>> combinations(int k) {\n        return Combinations.apply(this, Math.max(k, 0));\n    }\n\n    @Override\n    default Iterator<List<T>> crossProduct(int power) {\n        return Collections.crossProduct(List.empty(), this, power);\n    }\n\n    @Override\n    default List<T> distinct() {\n        return distinctBy(Function.identity());\n    }\n\n    @Override\n    default List<T> distinctBy(Comparator<? super T> comparator) {\n        Objects.requireNonNull(comparator, \"comparator is null\");\n        final java.util.Set<T> seen = new java.util.TreeSet<>(comparator);\n        return filter(seen::add);\n    }\n\n    @Override\n    default <U> List<T> distinctBy(Function<? super T, ? extends U> keyExtractor) {\n        Objects.requireNonNull(keyExtractor, \"keyExtractor is null\");\n        final java.util.Set<U> seen = new java.util.HashSet<>();\n        return filter(t -> seen.add(keyExtractor.apply(t)));\n    }\n\n    @Override\n    default List<T> drop(long n) {\n        List<T> list = this;\n        for (long i = n; i > 0 && !list.isEmpty(); i--) {\n            list = list.tail();\n        }\n        return list;\n    }\n\n    @Override\n    default List<T> dropRight(long n) {\n        if (n <= 0) {\n            return this;\n        }\n        if (n >= length()) {\n            return empty();\n        }\n        return List.ofAll(iterator().dropRight(n));\n    }\n\n    @Override\n    default List<T> dropUntil(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return dropWhile(predicate.negate());\n    }\n\n    @Override\n    default List<T> dropWhile(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        List<T> list = this;\n        while (!list.isEmpty() && predicate.test(list.head())) {\n            list = list.tail();\n        }\n        return list;\n    }\n\n    @Override\n    default List<T> filter(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        final List<T> filtered = foldLeft(List.<T> empty(), (xs, x) -> predicate.test(x) ? xs.prepend(x) : xs);\n\n        if (filtered.isEmpty()) {\n            return List.empty();\n        } else if (filtered.length() == length()) {\n            return this;\n        } else {\n            return filtered.reverse();\n        }\n    }\n\n    @Override\n    default <U> List<U> flatMap(Function<? super T, ? extends Iterable<? extends U>> mapper) {\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        if (isEmpty()) {\n            return empty();\n        } else {\n            List<U> list = empty();\n            for (T t : this) {\n                for (U u : mapper.apply(t)) {\n                    list = list.prepend(u);\n                }\n            }\n            return list.reverse();\n        }\n    }\n\n    @Override\n    default T get(int index) {\n        if (isEmpty()) {\n            throw new IndexOutOfBoundsException(\"get(\" + index + \") on Nil\");\n        }\n        if (index < 0) {\n            throw new IndexOutOfBoundsException(\"get(\" + index + \")\");\n        }\n        List<T> list = this;\n        for (int i = index - 1; i >= 0; i--) {\n            list = list.tail();\n            if (list.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"get(\" + index + \") on List of length \" + (index - i));\n            }\n        }\n        return list.head();\n    }\n\n    @Override\n    default <C> Map<C, List<T>> groupBy(Function<? super T, ? extends C> classifier) {\n        Objects.requireNonNull(classifier, \"classifier is null\");\n        return iterator().groupBy(classifier).map((c, it) -> Tuple.of(c, List.ofAll(it)));\n    }\n\n    @Override\n    default Iterator<List<T>> grouped(long size) {\n        return sliding(size, size);\n    }\n\n    @Override\n    default boolean hasDefiniteSize() {\n        return true;\n    }\n\n    @Override\n    default int indexOf(T element, int from) {\n        int index = 0;\n        for (List<T> list = this; !list.isEmpty(); list = list.tail(), index++) {\n            if (index >= from && Objects.equals(list.head(), element)) {\n                return index;\n            }\n        }\n        return -1;\n    }\n\n    @Override\n    default List<T> init() {\n        if (isEmpty()) {\n            throw new UnsupportedOperationException(\"init of empty list\");\n        } else {\n            return dropRight(1);\n        }\n    }\n\n    @Override\n    default Option<List<T>> initOption() {\n        return isEmpty() ? Option.none() : Option.some(init());\n    }\n\n    @Override\n    int length();\n\n    @Override\n    default List<T> insert(int index, T element) {\n        if (index < 0) {\n            throw new IndexOutOfBoundsException(\"insert(\" + index + \", e)\");\n        }\n        List<T> preceding = Nil.instance();\n        List<T> tail = this;\n        for (int i = index; i > 0; i--, tail = tail.tail()) {\n            if (tail.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"insert(\" + index + \", e) on List of length \" + length());\n            }\n            preceding = preceding.prepend(tail.head());\n        }\n        List<T> result = tail.prepend(element);\n        for (T next : preceding) {\n            result = result.prepend(next);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> insertAll(int index, Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        if (index < 0) {\n            throw new IndexOutOfBoundsException(\"insertAll(\" + index + \", elements)\");\n        }\n        List<T> preceding = Nil.instance();\n        List<T> tail = this;\n        for (int i = index; i > 0; i--, tail = tail.tail()) {\n            if (tail.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"insertAll(\" + index + \", elements) on List of length \" + length());\n            }\n            preceding = preceding.prepend(tail.head());\n        }\n        List<T> result = tail.prependAll(elements);\n        for (T next : preceding) {\n            result = result.prepend(next);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> intersperse(T element) {\n        return ofAll(iterator().intersperse(element));\n    }\n\n    @Override\n    default boolean isTraversableAgain() {\n        return true;\n    }\n\n    @Override\n    default int lastIndexOf(T element, int end) {\n        int result = -1, index = 0;\n        for (List<T> list = this; index <= end && !list.isEmpty(); list = list.tail(), index++) {\n            if (Objects.equals(list.head(), element)) {\n                result = index;\n            }\n        }\n        return result;\n    }\n\n    @Override\n    default <U> List<U> map(Function<? super T, ? extends U> mapper) {\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        List<U> list = empty();\n        for (T t : this) {\n            list = list.prepend(mapper.apply(t));\n        }\n        return list.reverse();\n    }\n\n    @Override\n    default List<T> padTo(int length, T element) {\n        final int actualLength = length();\n        if (length <= actualLength) {\n            return this;\n        } else {\n            return appendAll(Iterator.continually(element).take(length - actualLength));\n        }\n    }\n\n    @Override\n    default List<T> leftPadTo(int length, T element) {\n        final int actualLength = length();\n        if (length <= actualLength) {\n            return this;\n        } else {\n            return prependAll(Iterator.continually(element).take(length - actualLength));\n        }\n    }\n\n    @Override\n    default List<T> patch(int from, Iterable<? extends T> that, int replaced) {\n        from = from < 0 ? 0 : from;\n        replaced = replaced < 0 ? 0 : replaced;\n        List<T> result = take(from).appendAll(that);\n        from += replaced;\n        result = result.appendAll(drop(from));\n        return result;\n    }\n\n    @Override\n    default Tuple2<List<T>, List<T>> partition(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        List<T> left = empty(), right = empty();\n        for (T t : this) {\n            if (predicate.test(t)) {\n                left = left.prepend(t);\n            } else {\n                right = right.prepend(t);\n            }\n        }\n        return Tuple.of(left.reverse(), right.reverse());\n    }\n\n    @Override\n    default T peek() {\n        if (isEmpty()) {\n            throw new NoSuchElementException(\"peek of empty list\");\n        }\n        return head();\n    }\n\n    @Override\n    default Option<T> peekOption() {\n        return isEmpty() ? Option.none() : Option.some(head());\n    }\n\n    /**\n     * Performs an action on the head element of this {@code List}.\n     *\n     * @param action A {@code Consumer}\n     * @return this {@code List}\n     */\n    @Override\n    default List<T> peek(Consumer<? super T> action) {\n        Objects.requireNonNull(action, \"action is null\");\n        if (!isEmpty()) {\n            action.accept(head());\n        }\n        return this;\n    }\n\n    @Override\n    default List<List<T>> permutations() {\n        if (isEmpty()) {\n            return Nil.instance();\n        } else {\n            final List<T> tail = tail();\n            if (tail.isEmpty()) {\n                return List.of(this);\n            } else {\n                final List<List<T>> zero = Nil.instance();\n                return distinct().foldLeft(zero, (xs, x) -> {\n                    final Function<List<T>, List<T>> prepend = l -> l.prepend(x);\n                    return xs.appendAll(remove(x).permutations().map(prepend));\n                });\n            }\n        }\n    }\n\n    @Override\n    default List<T> pop() {\n        if (isEmpty()) {\n            throw new NoSuchElementException(\"pop of empty list\");\n        }\n        return tail();\n    }\n\n    @Override\n    default Option<List<T>> popOption() {\n        return isEmpty() ? Option.none() : Option.some(pop());\n    }\n\n    @Override\n    default Tuple2<T, List<T>> pop2() {\n        if (isEmpty()) {\n            throw new NoSuchElementException(\"pop2 of empty list\");\n        }\n        return Tuple.of(head(), tail());\n    }\n\n    @Override\n    default Option<Tuple2<T, List<T>>> pop2Option() {\n        return isEmpty() ? Option.none() : Option.some(Tuple.of(head(), pop()));\n    }\n\n    @Override\n    default List<T> prepend(T element) {\n        return new Cons<>(element, this);\n    }\n\n    @Override\n    default List<T> prependAll(Iterable<? extends T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return isEmpty() ? List.ofAll(elements) : List.ofAll(elements).reverse().foldLeft(this, List::prepend);\n    }\n\n    @Override\n    default List<T> push(T element) {\n        return new Cons<>(element, this);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @Override\n    default List<T> push(T... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        List<T> result = this;\n        for (T element : elements) {\n            result = result.prepend(element);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> pushAll(Iterable<T> elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        List<T> result = this;\n        for (T element : elements) {\n            result = result.prepend(element);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> remove(T element) {\n        List<T> preceding = Nil.instance();\n        List<T> tail = this;\n        boolean found = false;\n        while (!found && !tail.isEmpty()) {\n            final T head = tail.head();\n            if (head.equals(element)) {\n                found = true;\n            } else {\n                preceding = preceding.prepend(head);\n            }\n            tail = tail.tail();\n        }\n        if (!found) {\n            return this;\n        }\n        List<T> result = tail;\n        for (T next : preceding) {\n            result = result.prepend(next);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> removeFirst(Predicate<T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        List<T> init = List.empty();\n        List<T> tail = this;\n        while (!tail.isEmpty() && !predicate.test(tail.head())) {\n            init = init.prepend(tail.head());\n            tail = tail.tail();\n        }\n        if (tail.isEmpty()) {\n            return this;\n        } else {\n            return init.foldLeft(tail.tail(), List::prepend);\n        }\n    }\n\n    @Override\n    default List<T> removeLast(Predicate<T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        final List<T> removedAndReversed = reverse().removeFirst(predicate);\n        return removedAndReversed.length() == length() ? this : removedAndReversed.reverse();\n    }\n\n    @Override\n    default List<T> removeAt(int index) {\n        if (index < 0) {\n            throw new IndexOutOfBoundsException(\"removeAt(\" + index + \")\");\n        }\n        if (isEmpty()) {\n            throw new IndexOutOfBoundsException(\"removeAt(\" + index + \") on Nil\");\n        }\n        List<T> init = Nil.instance();\n        List<T> tail = this;\n        while (index > 0 && !tail.isEmpty()) {\n            init = init.prepend(tail.head());\n            tail = tail.tail();\n            index--;\n        }\n        if (index > 0) {\n            throw new IndexOutOfBoundsException(\"removeAt() on Nil\");\n        }\n        return init.reverse().appendAll(tail.tail());\n    }\n\n    @Override\n    default List<T> removeAll(T element) {\n        return Collections.removeAll(this, element);\n    }\n\n    @Override\n    default List<T> removeAll(Iterable<? extends T> elements) {\n        return Collections.removeAll(this, elements);\n    }\n\n    @Override\n    default List<T> replace(T currentElement, T newElement) {\n        List<T> preceding = Nil.instance();\n        List<T> tail = this;\n        while (!tail.isEmpty() && !Objects.equals(tail.head(), currentElement)) {\n            preceding = preceding.prepend(tail.head());\n            tail = tail.tail();\n        }\n        if (tail.isEmpty()) {\n            return this;\n        }\n        // skip the current head element because it is replaced\n        List<T> result = tail.tail().prepend(newElement);\n        for (T next : preceding) {\n            result = result.prepend(next);\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> replaceAll(T currentElement, T newElement) {\n        List<T> result = Nil.instance();\n        boolean changed = false;\n        for (List<T> list = this; !list.isEmpty(); list = list.tail()) {\n            final T head = list.head();\n            if (Objects.equals(head, currentElement)) {\n                result = result.prepend(newElement);\n                changed = true;\n            } else {\n                result = result.prepend(head);\n            }\n        }\n        return changed ? result.reverse() : this;\n    }\n\n    @Override\n    default List<T> retainAll(Iterable<? extends T> elements) {\n        return Collections.retainAll(this, elements);\n    }\n\n    @Override\n    default List<T> reverse() {\n        return isEmpty() ? this : foldLeft(empty(), List::prepend);\n    }\n\n    @Override\n    default List<T> scan(T zero, BiFunction<? super T, ? super T, ? extends T> operation) {\n        return scanLeft(zero, operation);\n    }\n\n    @Override\n    default <U> List<U> scanLeft(U zero, BiFunction<? super U, ? super T, ? extends U> operation) {\n        Objects.requireNonNull(operation, \"operation is null\");\n        return Collections.scanLeft(this, zero, operation, List.empty(), List::prepend, List::reverse);\n    }\n\n    @Override\n    default <U> List<U> scanRight(U zero, BiFunction<? super T, ? super U, ? extends U> operation) {\n        Objects.requireNonNull(operation, \"operation is null\");\n        return Collections.scanRight(this, zero, operation, List.empty(), List::prepend, Function.identity());\n    }\n\n    @Override\n    default List<T> slice(long beginIndex, long endIndex) {\n        if (beginIndex >= endIndex || beginIndex >= length() || isEmpty()) {\n            return empty();\n        } else {\n            List<T> result = Nil.instance();\n            List<T> list = this;\n            final long lowerBound = Math.max(beginIndex, 0);\n            final long upperBound = Math.min(endIndex, length());\n            for (int i = 0; i < upperBound; i++) {\n                if (i >= lowerBound) {\n                    result = result.prepend(list.head());\n                }\n                list = list.tail();\n            }\n            return result.reverse();\n        }\n    }\n\n    @Override\n    default Iterator<List<T>> sliding(long size) {\n        return sliding(size, 1);\n    }\n\n    @Override\n    default Iterator<List<T>> sliding(long size, long step) {\n        return iterator().sliding(size, step).map(List::ofAll);\n    }\n\n    @Override\n    default List<T> sorted() {\n        return isEmpty() ? this : toJavaStream().sorted().collect(List.collector());\n    }\n\n    @Override\n    default List<T> sorted(Comparator<? super T> comparator) {\n        Objects.requireNonNull(comparator, \"comparator is null\");\n        return isEmpty() ? this : toJavaStream().sorted(comparator).collect(List.collector());\n    }\n\n    @Override\n    default <U extends Comparable<? super U>> List<T> sortBy(Function<? super T, ? extends U> mapper) {\n        return sortBy(U::compareTo, mapper);\n    }\n\n    @Override\n    default <U> List<T> sortBy(Comparator<? super U> comparator, Function<? super T, ? extends U> mapper) {\n        final Function<? super T, ? extends U> domain = Function1.of(mapper::apply).memoized();\n        return toJavaStream()\n                .sorted((e1, e2) -> comparator.compare(domain.apply(e1), domain.apply(e2)))\n                .collect(collector());\n    }\n\n    @Override\n    default Tuple2<List<T>, List<T>> span(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        final Tuple2<Iterator<T>, Iterator<T>> itt = iterator().span(predicate);\n        return Tuple.of(List.ofAll(itt._1), List.ofAll(itt._2));\n    }\n\n    @Override\n    default Tuple2<List<T>, List<T>> splitAt(long n) {\n        if (isEmpty()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            List<T> init = Nil.instance();\n            List<T> tail = this;\n            while (n > 0 && !tail.isEmpty()) {\n                init = init.prepend(tail.head());\n                tail = tail.tail();\n                n--;\n            }\n            return Tuple.of(init.reverse(), tail);\n        }\n    }\n\n    @Override\n    default Tuple2<List<T>, List<T>> splitAt(Predicate<? super T> predicate) {\n        if (isEmpty()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            final Tuple2<List<T>, List<T>> t = SplitAt.splitByPredicateReversed(this, predicate);\n            if (t._2.isEmpty()) {\n                return Tuple.of(this, empty());\n            } else {\n                return Tuple.of(t._1.reverse(), t._2);\n            }\n        }\n    }\n\n    @Override\n    default Tuple2<List<T>, List<T>> splitAtInclusive(Predicate<? super T> predicate) {\n        if (isEmpty()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            final Tuple2<List<T>, List<T>> t = SplitAt.splitByPredicateReversed(this, predicate);\n            if (t._2.isEmpty() || t._2.tail().isEmpty()) {\n                return Tuple.of(this, empty());\n            } else {\n                return Tuple.of(t._1.prepend(t._2.head()).reverse(), t._2.tail());\n            }\n        }\n    }\n\n    @Override\n    default Spliterator<T> spliterator() {\n        return Spliterators.spliterator(iterator(), length(), Spliterator.ORDERED | Spliterator.IMMUTABLE);\n    }\n\n    @Override\n    default String stringPrefix() {\n        return \"List\";\n    }\n\n    @Override\n    default List<T> subSequence(int beginIndex) {\n        if (beginIndex < 0) {\n            throw new IndexOutOfBoundsException(\"subSequence(\" + beginIndex + \")\");\n        }\n        List<T> result = this;\n        for (int i = 0; i < beginIndex; i++, result = result.tail()) {\n            if (result.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"subSequence(\" + beginIndex + \") on List of length \" + i);\n            }\n        }\n        return result;\n    }\n\n    @Override\n    default List<T> subSequence(int beginIndex, int endIndex) {\n        if (beginIndex < 0 || beginIndex > endIndex) {\n            throw new IndexOutOfBoundsException(\"subSequence(\" + beginIndex + \", \" + endIndex + \") on List of length \" + length());\n        }\n        List<T> result = Nil.instance();\n        List<T> list = this;\n        for (int i = 0; i < endIndex; i++, list = list.tail()) {\n            if (list.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"subSequence(\" + beginIndex + \", \" + endIndex + \") on List of length \" + i);\n            }\n            if (i >= beginIndex) {\n                result = result.prepend(list.head());\n            }\n        }\n        return result.reverse();\n    }\n\n    @Override\n    List<T> tail();\n\n    @Override\n    default Option<List<T>> tailOption() {\n        return isEmpty() ? Option.none() : Option.some(tail());\n    }\n\n    @Override\n    default List<T> take(long n) {\n        if (n >= length()) {\n            return this;\n        }\n        if (n <= 0) {\n            return empty();\n        }\n        List<T> result = Nil.instance();\n        List<T> list = this;\n        for (int i = 0; i < n; i++, list = list.tail()) {\n            result = result.prepend(list.head());\n        }\n        return result.reverse();\n    }\n\n    @Override\n    default List<T> takeRight(long n) {\n        if (n >= length()) {\n            return this;\n        }\n        if (n <= 0) {\n            return empty();\n        }\n        return reverse().take(n).reverse();\n    }\n\n    @Override\n    default List<T> takeUntil(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return takeWhile(predicate.negate());\n    }\n\n    @Override\n    default List<T> takeWhile(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        List<T> result = Nil.instance();\n        for (List<T> list = this; !list.isEmpty() && predicate.test(list.head()); list = list.tail()) {\n            result = result.prepend(list.head());\n        }\n        return result.length() == length() ? this : result.reverse();\n    }\n\n    /**\n     * Transforms this {@code List}.\n     *\n     * @param f   A transformation\n     * @param <U> Type of transformation result\n     * @return An instance of type {@code U}\n     * @throws NullPointerException if {@code f} is null\n     */\n    default <U> U transform(Function<? super List<T>, ? extends U> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return f.apply(this);\n    }\n\n    @Override\n    default <U> List<U> unit(Iterable<? extends U> iterable) {\n        return List.ofAll(iterable);\n    }\n\n    @Override\n    default <T1, T2> Tuple2<List<T1>, List<T2>> unzip(\n            Function<? super T, Tuple2<? extends T1, ? extends T2>> unzipper) {\n        Objects.requireNonNull(unzipper, \"unzipper is null\");\n        List<T1> xs = Nil.instance();\n        List<T2> ys = Nil.instance();\n        for (T element : this) {\n            final Tuple2<? extends T1, ? extends T2> t = unzipper.apply(element);\n            xs = xs.prepend(t._1);\n            ys = ys.prepend(t._2);\n        }\n        return Tuple.of(xs.reverse(), ys.reverse());\n    }\n\n    @Override\n    default <T1, T2, T3> Tuple3<List<T1>, List<T2>, List<T3>> unzip3(\n            Function<? super T, Tuple3<? extends T1, ? extends T2, ? extends T3>> unzipper) {\n        Objects.requireNonNull(unzipper, \"unzipper is null\");\n        List<T1> xs = Nil.instance();\n        List<T2> ys = Nil.instance();\n        List<T3> zs = Nil.instance();\n        for (T element : this) {\n            final Tuple3<? extends T1, ? extends T2, ? extends T3> t = unzipper.apply(element);\n            xs = xs.prepend(t._1);\n            ys = ys.prepend(t._2);\n            zs = zs.prepend(t._3);\n        }\n        return Tuple.of(xs.reverse(), ys.reverse(), zs.reverse());\n    }\n\n    @Override\n    default List<T> update(int index, T element) {\n        if (isEmpty()) {\n            throw new IndexOutOfBoundsException(\"update(\" + index + \", e) on Nil\");\n        }\n        if (index < 0) {\n            throw new IndexOutOfBoundsException(\"update(\" + index + \", e)\");\n        }\n        List<T> preceding = Nil.instance();\n        List<T> tail = this;\n        for (int i = index; i > 0; i--, tail = tail.tail()) {\n            if (tail.isEmpty()) {\n                throw new IndexOutOfBoundsException(\"update(\" + index + \", e) on List of length \" + length());\n            }\n            preceding = preceding.prepend(tail.head());\n        }\n        if (tail.isEmpty()) {\n            throw new IndexOutOfBoundsException(\"update(\" + index + \", e) on List of length \" + length());\n        }\n        // skip the current head element because it is replaced\n        List<T> result = tail.tail().prepend(element);\n        for (T next : preceding) {\n            result = result.prepend(next);\n        }\n        return result;\n    }\n\n    @Override\n    default <U> List<Tuple2<T, U>> zip(Iterable<? extends U> that) {\n        Objects.requireNonNull(that, \"that is null\");\n        return List.ofAll(iterator().zip(that));\n    }\n\n    @Override\n    default <U> List<Tuple2<T, U>> zipAll(Iterable<? extends U> that, T thisElem, U thatElem) {\n        Objects.requireNonNull(that, \"that is null\");\n        return List.ofAll(iterator().zipAll(that, thisElem, thatElem));\n    }\n\n    @Override\n    default List<Tuple2<T, Long>> zipWithIndex() {\n        return List.ofAll(iterator().zipWithIndex());\n    }\n\n    /**\n     * Representation of the singleton empty {@code List}.\n     *\n     * @param <T> Component type of the List.\n     * @since 1.1.0\n     */\n    final class Nil<T> implements List<T>, Serializable {\n\n        private static final long serialVersionUID = 1L;\n\n        private static final Nil<?> INSTANCE = new Nil<>();\n\n        // hidden\n        private Nil() {\n        }\n\n        /**\n         * Returns the singleton instance of the liked list.\n         *\n         * @param <T> Component type of the List\n         * @return the singleton instance of the linked list.\n         */\n        @SuppressWarnings(\"unchecked\")\n        public static <T> Nil<T> instance() {\n            return (Nil<T>) INSTANCE;\n        }\n\n        @Override\n        public T head() {\n            throw new NoSuchElementException(\"head of empty list\");\n        }\n\n        @Override\n        public int length() {\n            return 0;\n        }\n\n        @Override\n        public List<T> tail() {\n            throw new UnsupportedOperationException(\"tail of empty list\");\n        }\n\n        @Override\n        public boolean isEmpty() {\n            return true;\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            return o == this;\n        }\n\n        @Override\n        public int hashCode() {\n            return 1;\n        }\n\n        @Override\n        public String toString() {\n            return stringPrefix() + \"()\";\n        }\n\n        /**\n         * Instance control for object serialization.\n         *\n         * @return The singleton instance of Nil.\n         * @see java.io.Serializable\n         */\n        private Object readResolve() {\n            return INSTANCE;\n        }\n    }\n\n    /**\n     * Non-empty {@code List}, consisting of a {@code head} and a {@code tail}.\n     *\n     * @param <T> Component type of the List.\n     * @since 1.1.0\n     */\n    // DEV NOTE: class declared final because of serialization proxy pattern (see Effective Java, 2nd ed., p. 315)\n    final class Cons<T> implements List<T>, Serializable {\n\n        private static final long serialVersionUID = 1L;\n\n        private final T head;\n        private final List<T> tail;\n        private final int length;\n\n        /**\n         * Creates a List consisting of a head value and a trailing List.\n         *\n         * @param head The head\n         * @param tail The tail\n         */\n        private Cons(T head, List<T> tail) {\n            this.head = head;\n            this.tail = tail;\n            this.length = 1 + tail.length();\n        }\n\n        @Override\n        public T head() {\n            return head;\n        }\n\n        @Override\n        public int length() {\n            return length;\n        }\n\n        @Override\n        public List<T> tail() {\n            return tail;\n        }\n\n        @Override\n        public boolean isEmpty() {\n            return false;\n        }\n\n        @Override\n        public boolean equals(Object o) {\n            if (o == this) {\n                return true;\n            } else if (o instanceof List) {\n                List<?> list1 = this;\n                List<?> list2 = (List<?>) o;\n                while (!list1.isEmpty() && !list2.isEmpty()) {\n                    final boolean isEqual = Objects.equals(list1.head(), list2.head());\n                    if (!isEqual) {\n                        return false;\n                    }\n                    list1 = list1.tail();\n                    list2 = list2.tail();\n                }\n                return list1.isEmpty() && list2.isEmpty();\n            } else {\n                return false;\n            }\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(head, tail);\n        }\n\n        @Override\n        public String toString() {\n            return mkString(stringPrefix() + \"(\", \", \", \")\");\n        }\n\n        /**\n         * {@code writeReplace} method for the serialization proxy pattern.\n         * <p>\n         * The presence of this method causes the serialization system to emit a SerializationProxy instance instead of\n         * an instance of the enclosing class.\n         *\n         * @return A SerialiationProxy for this enclosing class.\n         */\n        private Object writeReplace() {\n            return new SerializationProxy<>(this);\n        }\n\n        /**\n         * {@code readObject} method for the serialization proxy pattern.\n         * <p>\n         * Guarantees that the serialization system will never generate a serialized instance of the enclosing class.\n         *\n         * @param stream An object serialization stream.\n         * @throws java.io.InvalidObjectException This method will throw with the message \"Proxy required\".\n         */\n        private void readObject(ObjectInputStream stream) throws InvalidObjectException {\n            throw new InvalidObjectException(\"Proxy required\");\n        }\n\n        /**\n         * A serialization proxy which, in this context, is used to deserialize immutable, linked Lists with final\n         * instance fields.\n         *\n         * @param <T> The component type of the underlying list.\n         */\n        // DEV NOTE: The serialization proxy pattern is not compatible with non-final, i.e. extendable,\n        // classes. Also, it may not be compatible with circular object graphs.\n        private static final class SerializationProxy<T> implements Serializable {\n\n            private static final long serialVersionUID = 1L;\n\n            // the instance to be serialized/deserialized\n            private transient Cons<T> list;\n\n            /**\n             * Constructor for the case of serialization, called by {@link Cons#writeReplace()}.\n             * <p/>\n             * The constructor of a SerializationProxy takes an argument that concisely represents the logical state of\n             * an instance of the enclosing class.\n             *\n             * @param list a Cons\n             */\n            SerializationProxy(Cons<T> list) {\n                this.list = list;\n            }\n\n            /**\n             * Write an object to a serialization stream.\n             *\n             * @param s An object serialization stream.\n             * @throws java.io.IOException If an error occurs writing to the stream.\n             */\n            private void writeObject(ObjectOutputStream s) throws IOException {\n                s.defaultWriteObject();\n                s.writeInt(list.length());\n                for (List<T> l = list; !l.isEmpty(); l = l.tail()) {\n                    s.writeObject(l.head());\n                }\n            }\n\n            /**\n             * Read an object from a deserialization stream.\n             *\n             * @param s An object deserialization stream.\n             * @throws ClassNotFoundException If the object's class read from the stream cannot be found.\n             * @throws InvalidObjectException If the stream contains no list elements.\n             * @throws IOException            If an error occurs reading from the stream.\n             */\n            private void readObject(ObjectInputStream s) throws ClassNotFoundException, IOException {\n                s.defaultReadObject();\n                final int size = s.readInt();\n                if (size <= 0) {\n                    throw new InvalidObjectException(\"No elements\");\n                }\n                List<T> temp = Nil.instance();\n                for (int i = 0; i < size; i++) {\n                    @SuppressWarnings(\"unchecked\")\n                    final T element = (T) s.readObject();\n                    temp = temp.prepend(element);\n                }\n                list = (Cons<T>) temp.reverse();\n            }\n\n            /**\n             * {@code readResolve} method for the serialization proxy pattern.\n             * <p>\n             * Returns a logically equivalent instance of the enclosing class. The presence of this method causes the\n             * serialization system to translate the serialization proxy back into an instance of the enclosing class\n             * upon deserialization.\n             *\n             * @return A deserialized instance of the enclosing class.\n             */\n            private Object readResolve() {\n                return list;\n            }\n        }\n    }\n}\n\ninterface ListModule {\n\n    interface Combinations {\n\n        static <T> List<List<T>> apply(List<T> elements, int k) {\n            if (k == 0) {\n                return List.of(List.empty());\n            } else {\n                return elements.zipWithIndex().flatMap(\n                        t -> apply(elements.drop(t._2 + 1), (k - 1)).map(c -> c.prepend(t._1))\n                );\n            }\n        }\n    }\n\n    interface SplitAt {\n\n        static <T> Tuple2<List<T>, List<T>> splitByPredicateReversed(List<T> source, Predicate<? super T> predicate) {\n            Objects.requireNonNull(predicate, \"predicate is null\");\n            List<T> init = Nil.instance();\n            List<T> tail = source;\n            while (!tail.isEmpty() && !predicate.test(tail.head())) {\n                init = init.prepend(tail.head());\n                tail = tail.tail();\n            }\n            return Tuple.of(init, tail);\n        }\n    }\n}\n", "idx": 33, "id": 7982, "msg": "", "proj": "vavr-io-vavr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -2734,9 +2734,9 @@ public class DatasetPage implements java.io.Serializable {\n     }\n     \n     public boolean isLockedFromDownload(){\n-        if(null == lockedFromDownloadVar) {\n+        if(null == lockedFromDownloadVar || stateChanged) {\n             try {\n-                permissionService.checkDownloadFileLock(dataset, dvRequestService.getDataverseRequest(), new CreateDatasetCommand(dataset, dvRequestService.getDataverseRequest()));\n+                permissionService.checkDownloadFileLock(dataset, dvRequestService.getDataverseRequest(), new CreateNewDatasetCommand(dataset, dvRequestService.getDataverseRequest()));\n                 lockedFromDownloadVar = false;\n             } catch (IllegalCommandException ex) {\n                 lockedFromDownloadVar = true;", "y": 0, "oldf": "package edu.harvard.iq.dataverse;\n\nimport edu.harvard.iq.dataverse.authorization.AuthenticationServiceBean;\nimport edu.harvard.iq.dataverse.authorization.Permission;\nimport edu.harvard.iq.dataverse.authorization.providers.builtin.BuiltinUserServiceBean;\nimport edu.harvard.iq.dataverse.authorization.users.AuthenticatedUser;\nimport edu.harvard.iq.dataverse.authorization.users.PrivateUrlUser;\nimport edu.harvard.iq.dataverse.dataaccess.StorageIO;\nimport edu.harvard.iq.dataverse.dataaccess.ImageThumbConverter;\nimport edu.harvard.iq.dataverse.dataaccess.SwiftAccessIO;\nimport edu.harvard.iq.dataverse.datacapturemodule.DataCaptureModuleUtil;\nimport edu.harvard.iq.dataverse.datacapturemodule.ScriptRequestResponse;\nimport edu.harvard.iq.dataverse.dataset.DatasetThumbnail;\nimport edu.harvard.iq.dataverse.dataset.DatasetUtil;\nimport edu.harvard.iq.dataverse.datavariable.VariableServiceBean;\nimport edu.harvard.iq.dataverse.engine.command.Command;\nimport edu.harvard.iq.dataverse.engine.command.exception.CommandException;\nimport edu.harvard.iq.dataverse.engine.command.impl.CreateDatasetCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.CreatePrivateUrlCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.DeaccessionDatasetVersionCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.DeleteDatasetVersionCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.DeletePrivateUrlCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.DestroyDatasetCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.GetPrivateUrlCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.LinkDatasetCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.PublishDatasetCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.PublishDataverseCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.UpdateDatasetCommand;\nimport edu.harvard.iq.dataverse.export.ExportException;\nimport edu.harvard.iq.dataverse.export.ExportService;\nimport edu.harvard.iq.dataverse.export.spi.Exporter;\nimport edu.harvard.iq.dataverse.ingest.IngestRequest;\nimport edu.harvard.iq.dataverse.ingest.IngestServiceBean;\nimport edu.harvard.iq.dataverse.metadataimport.ForeignMetadataImportServiceBean;\nimport edu.harvard.iq.dataverse.privateurl.PrivateUrl;\nimport edu.harvard.iq.dataverse.privateurl.PrivateUrlServiceBean;\nimport edu.harvard.iq.dataverse.privateurl.PrivateUrlUtil;\nimport edu.harvard.iq.dataverse.search.SearchFilesServiceBean;\nimport edu.harvard.iq.dataverse.search.SortBy;\nimport edu.harvard.iq.dataverse.settings.SettingsServiceBean;\nimport edu.harvard.iq.dataverse.util.BundleUtil;\nimport edu.harvard.iq.dataverse.util.FileSortFieldAndOrder;\nimport edu.harvard.iq.dataverse.util.FileUtil;\nimport edu.harvard.iq.dataverse.util.JsfHelper;\nimport static edu.harvard.iq.dataverse.util.JsfHelper.JH;\nimport edu.harvard.iq.dataverse.util.StringUtil;\nimport edu.harvard.iq.dataverse.util.SystemConfig;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.sql.Timestamp;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.logging.Logger;\nimport javax.ejb.EJB;\nimport javax.ejb.EJBException;\nimport javax.faces.application.FacesMessage;\nimport javax.faces.context.FacesContext;\nimport javax.faces.event.ActionEvent;\nimport javax.faces.event.ValueChangeEvent;\nimport javax.faces.view.ViewScoped;\nimport javax.inject.Inject;\nimport javax.inject.Named;\nimport org.primefaces.event.FileUploadEvent;\nimport org.primefaces.model.UploadedFile;\nimport javax.validation.ConstraintViolation;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.primefaces.context.RequestContext;\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport javax.faces.model.SelectItem;\nimport java.util.logging.Level;\nimport edu.harvard.iq.dataverse.datasetutility.WorldMapPermissionHelper;\nimport edu.harvard.iq.dataverse.engine.command.exception.IllegalCommandException;\nimport edu.harvard.iq.dataverse.engine.command.impl.GetLatestPublishedDatasetVersionCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.RequestRsyncScriptCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.PublishDatasetResult;\nimport edu.harvard.iq.dataverse.engine.command.impl.RestrictFileCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.ReturnDatasetToAuthorCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.SubmitDatasetForReviewCommand;\nimport edu.harvard.iq.dataverse.externaltools.ExternalTool;\nimport edu.harvard.iq.dataverse.externaltools.ExternalToolServiceBean;\nimport edu.harvard.iq.dataverse.export.SchemaDotOrgExporter;\nimport java.util.Collections;\n\nimport javax.faces.event.AjaxBehaviorEvent;\nimport javax.servlet.ServletOutputStream;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.apache.commons.lang.StringEscapeUtils;\n\nimport org.primefaces.component.tabview.TabView;\nimport org.primefaces.event.CloseEvent;\nimport org.primefaces.event.TabChangeEvent;\n\n/**\n *\n * @author gdurand\n */\n@ViewScoped\n@Named(\"DatasetPage\")\npublic class DatasetPage implements java.io.Serializable {\n\n    private static final Logger logger = Logger.getLogger(DatasetPage.class.getCanonicalName());\n\n    public enum EditMode {\n\n        CREATE, INFO, FILE, METADATA, LICENSE\n    };\n\n    public enum DisplayMode {\n\n        INIT, SAVE\n    };\n    \n\n    @EJB\n    DatasetServiceBean datasetService;\n    @EJB\n    DatasetVersionServiceBean datasetVersionService;\n    @EJB\n    DataFileServiceBean datafileService;\n    @EJB\n    PermissionServiceBean permissionService;\n    @EJB\n    DataverseServiceBean dataverseService;\n    @EJB\n    DatasetFieldServiceBean fieldService;\n    @EJB\n    VariableServiceBean variableService;\n    @EJB\n    IngestServiceBean ingestService;\n    @EJB\n    ForeignMetadataImportServiceBean metadataImportService;\n    @EJB\n    EjbDataverseEngine commandEngine;\n    @Inject\n    DataverseSession session;\n    @EJB\n    UserNotificationServiceBean userNotificationService;\n    @EJB\n    MapLayerMetadataServiceBean mapLayerMetadataService;\n    @EJB\n    BuiltinUserServiceBean builtinUserService;\n    @EJB\n    DataverseFieldTypeInputLevelServiceBean dataverseFieldTypeInputLevelService;\n    @EJB\n    SettingsServiceBean settingsService;\n    @EJB\n    AuthenticationServiceBean authService;\n    @EJB\n    SystemConfig systemConfig;\n    @EJB\n    GuestbookResponseServiceBean guestbookResponseService;\n    @EJB\n    FileDownloadServiceBean fileDownloadService;\n    @EJB\n    DataverseLinkingServiceBean dvLinkingService;\n    @EJB\n    DatasetLinkingServiceBean dsLinkingService;\n    @EJB\n    SearchFilesServiceBean searchFilesService;\n    @EJB\n    DataverseRoleServiceBean dataverseRoleService;\n    @EJB\n    PrivateUrlServiceBean privateUrlService;\n    @EJB\n    ExternalToolServiceBean externalToolService;\n\n    @Inject\n    DataverseRequestServiceBean dvRequestService;\n    @Inject\n    DatasetVersionUI datasetVersionUI;\n    @Inject\n    PermissionsWrapper permissionsWrapper;\n    @Inject\n    FileDownloadHelper fileDownloadHelper;\n    @Inject\n    WorldMapPermissionHelper worldMapPermissionHelper;\n    @Inject\n    ThumbnailServiceWrapper thumbnailServiceWrapper;\n    @Inject\n    SettingsWrapper settingsWrapper; \n    \n\n\n    private Dataset dataset = new Dataset();\n    private EditMode editMode;\n    private boolean bulkFileDeleteInProgress = false;\n\n    private Long ownerId;\n    private Long versionId;\n    private int selectedTabIndex;\n    private List<DataFile> newFiles = new ArrayList<>();\n    private DatasetVersion workingVersion;\n    private int releaseRadio = 1;\n    private int deaccessionRadio = 0;\n    private int deaccessionReasonRadio = 0;\n    private String datasetNextMajorVersion = \"1.0\";\n    private String datasetNextMinorVersion = \"\";\n    private String dropBoxSelection = \"\";\n    private String deaccessionReasonText = \"\";\n    private String displayCitation;\n    private String deaccessionForwardURLFor = \"\";\n    private String showVersionList = \"false\";\n    private List<Template> dataverseTemplates = new ArrayList<>();\n    private Template defaultTemplate;\n    private Template selectedTemplate;\n    private String persistentId;\n    private String version;\n    private String protocol = \"\";\n    private String authority = \"\";\n    private String separator = \"\";\n    private String customFields=\"\";\n\n    private boolean noDVsAtAll = false;\n\n    private boolean noDVsRemaining = false;\n    \n    private boolean stateChanged = false;\n\n    private List<Dataverse> dataversesForLinking = new ArrayList<>();\n    private Long linkingDataverseId;\n    private List<SelectItem> linkingDVSelectItems;\n    private Dataverse linkingDataverse;\n    \n    // Version tab lists\n    private List<DatasetVersion> versionTabList = new ArrayList<>();\n    private List<DatasetVersion> versionTabListForPostLoad = new ArrayList<>();\n\n    \n    // Used to store results of permissions checks\n    private final Map<String, Boolean> datasetPermissionMap = new HashMap<>(); // { Permission human_name : Boolean }\n    \n\n    \n    private DataFile selectedDownloadFile;\n\n    private Long maxFileUploadSizeInBytes = null;\n    \n    private String dataverseSiteUrl = \"\"; \n    \n    private boolean removeUnusedTags;\n    \n    private Boolean hasRsyncScript = false;\n    \n    List<ExternalTool> configureTools = new ArrayList<>();\n    List<ExternalTool> exploreTools = new ArrayList<>();\n    Map<Long, List<ExternalTool>> configureToolsByFileId = new HashMap<>();\n    Map<Long, List<ExternalTool>> exploreToolsByFileId = new HashMap<>();\n    \n    public Boolean isHasRsyncScript() {\n        return hasRsyncScript;\n    }\n\n    public void setHasRsyncScript(Boolean hasRsyncScript) {\n        this.hasRsyncScript = hasRsyncScript;\n    }\n    \n    /**\n     * The contents of the script.\n     */\n    private String rsyncScript = \"\";\n\n    public String getRsyncScript() {\n        return rsyncScript;\n    }\n\n    public void setRsyncScript(String rsyncScript) {\n        this.rsyncScript = rsyncScript;\n    }\n\n    private String rsyncScriptFilename;\n\n    public String getRsyncScriptFilename() {\n        return rsyncScriptFilename;\n    }\n\n    private String thumbnailString = null; \n\n    // This is the Dataset-level thumbnail; \n    // it's either the thumbnail of the designated datafile, \n    // or scaled down uploaded \"logo\" file, or randomly selected\n    // image datafile from this dataset. \n    public String getThumbnailString() {\n        // This method gets called 30 (!) times, just to load the page!\n        // - so let's cache that string the first time it's called. \n            \n        if (thumbnailString != null) {\n            if (\"\".equals(thumbnailString)) {\n                return null;\n            } \n            return thumbnailString;\n        }\n\n        if (!readOnly) {\n        DatasetThumbnail datasetThumbnail = dataset.getDatasetThumbnail();\n        if (datasetThumbnail == null) {\n            thumbnailString = \"\";\n            return null; \n        } \n        \n        if (datasetThumbnail.isFromDataFile()) {\n            if (!datasetThumbnail.getDataFile().equals(dataset.getThumbnailFile())) {\n                datasetService.assignDatasetThumbnailByNativeQuery(dataset, datasetThumbnail.getDataFile());\n                dataset = datasetService.find(dataset.getId());\n            }\n        }\n           \n        thumbnailString = datasetThumbnail.getBase64image();\n        } else {\n            thumbnailString = thumbnailServiceWrapper.getDatasetCardImageAsBase64Url(dataset, workingVersion.getId());\n            if (thumbnailString == null) {\n                thumbnailString = \"\";\n                return null;\n            }\n            \n            \n        }\n        return thumbnailString;\n    }\n\n    public void setThumbnailString(String thumbnailString) {\n        //Dummy method\n    }\n\n    public boolean isRemoveUnusedTags() {\n        return removeUnusedTags;\n    }\n\n    public void setRemoveUnusedTags(boolean removeUnusedTags) {\n        this.removeUnusedTags = removeUnusedTags;\n    }\n\n    private List<FileMetadata> fileMetadatas;\n    private String fileSortField;\n    private String fileSortOrder;\n\n    private LazyFileMetadataDataModel lazyModel;\n\n    public LazyFileMetadataDataModel getLazyModel() {\n        return lazyModel;\n    }\n\n    public void setLazyModel(LazyFileMetadataDataModel lazyModel) {\n        this.lazyModel = lazyModel;\n    }\n    \n    private String fileLabelSearchTerm;\n\n    public String getFileLabelSearchTerm() {\n        return fileLabelSearchTerm;\n    }\n\n    public void setFileLabelSearchTerm(String fileLabelSearchTerm) {\n        if (fileLabelSearchTerm != null) {\n            this.fileLabelSearchTerm = fileLabelSearchTerm.trim();\n        }\n    }\n    \n    private List<FileMetadata> fileMetadatasSearch;\n    \n    public List<FileMetadata> getFileMetadatasSearch() {\n        return fileMetadatasSearch;\n    }\n\n    public void setFileMetadatasSearch(List<FileMetadata> fileMetadatasSearch) {\n        this.fileMetadatasSearch = fileMetadatasSearch;\n    }\n    \n    public void updateFileSearch(){  \n        logger.info(\"updating file search list\");\n        if (readOnly) {\n            this.fileMetadatasSearch = selectFileMetadatasForDisplay(this.fileLabelSearchTerm); \n        } else {\n            this.fileMetadatasSearch = datafileService.findFileMetadataByDatasetVersionIdLabelSearchTerm(workingVersion.getId(), this.fileLabelSearchTerm, \"\", \"\");\n        }\n    }\n    \n        private Long numberOfFilesToShow = (long) 25;\n\n    public Long getNumberOfFilesToShow() {\n        return numberOfFilesToShow;\n    }\n\n    public void setNumberOfFilesToShow(Long numberOfFilesToShow) {\n        this.numberOfFilesToShow = numberOfFilesToShow;\n    }\n    \n    public void showAll(){\n        setNumberOfFilesToShow(new Long(fileMetadatasSearch.size()));\n    }\n    \n    private List<FileMetadata> selectFileMetadatasForDisplay(String searchTerm) {\n        Set<Long> searchResultsIdSet = null; \n        \n        if (searchTerm != null && !searchTerm.equals(\"\")) {\n            List<Integer> searchResultsIdList = datafileService.findFileMetadataIdsByDatasetVersionIdLabelSearchTerm(workingVersion.getId(), searchTerm, \"\", \"\");\n            searchResultsIdSet = new HashSet<>();\n            for (Integer id : searchResultsIdList) {\n                searchResultsIdSet.add(id.longValue());\n            }\n        }\n        \n        List<FileMetadata> retList = new ArrayList<>(); \n        \n        for (FileMetadata fileMetadata : workingVersion.getFileMetadatasSorted()) {\n            if (searchResultsIdSet == null || searchResultsIdSet.contains(fileMetadata.getId())) {\n                retList.add(fileMetadata);\n            }\n        }\n        \n        return retList;\n    }\n    \n    /*\n        Save the setting locally so db isn't hit repeatedly\n    \n        This may be \"null\", signifying unlimited download size\n    */\n    public Long getMaxFileUploadSizeInBytes(){\n        return this.maxFileUploadSizeInBytes;\n    }\n    \n    public boolean isUnlimitedUploadFileSize(){\n        \n        if (this.maxFileUploadSizeInBytes == null){\n            return true;\n        }\n        return false;\n    }\n\n    public String getDataverseSiteUrl() {\n        return this.dataverseSiteUrl;\n    }\n    \n    public void setDataverseSiteUrl(String dataverseSiteUrl) {\n        this.dataverseSiteUrl = dataverseSiteUrl;\n    }\n    \n    public DataFile getInitialDataFile() {\n        if (workingVersion.getFileMetadatas() != null && workingVersion.getFileMetadatas().size() > 0) {\n            return workingVersion.getFileMetadatas().get(0).getDataFile();\n        }\n        return null;\n    }\n    \n    public SwiftAccessIO getSwiftObject() {\n        try {\n            StorageIO<DataFile> storageIO = getInitialDataFile() == null ? null : getInitialDataFile().getStorageIO();\n            if (storageIO != null && storageIO instanceof SwiftAccessIO) {\n                return (SwiftAccessIO)storageIO;\n            } else {\n                logger.fine(\"DatasetPage: Failed to cast storageIO as SwiftAccessIO (most likely because storageIO is a FileAccessIO)\");\n            } \n        } catch (IOException e) {\n            logger.fine(\"DatasetPage: Failed to get storageIO\");\n\n        }\n        return null;\n    }\n\n    public String getSwiftContainerName() throws IOException {\n        SwiftAccessIO swiftObject = getSwiftObject();\n        try {\n            swiftObject.open();\n            return swiftObject.getSwiftContainerName();\n        } catch (Exception e){\n            logger.info(\"DatasetPage: Failed to open swift object\");\n        }\n        \n        return \"\";\n        \n    }\n    \n    public void setSwiftContainerName(String name){\n        \n    }\n    //This function applies to an entire dataset\n    private boolean isSwiftStorage() {\n        //containers without datafiles will not be stored in swift storage\n        if (getInitialDataFile() != null){\n            for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n                //if any of the datafiles are stored in swift\n                if (fmd.getDataFile().getStorageIdentifier().startsWith(\"swift://\")) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n    \n    //This function applies to a single datafile\n    private boolean isSwiftStorage(FileMetadata metadata){\n        if (metadata.getDataFile().getStorageIdentifier().startsWith(\"swift://\")) {\n            return true;\n        }\n        return false;\n    }\n    \n    \n    private Boolean showComputeButtonForDataset = null;\n    //This function applies to an entire dataset\n    public boolean showComputeButton() {\n        if (showComputeButtonForDataset != null) {\n            return showComputeButtonForDataset;\n        }\n        \n        if (isSwiftStorage() && (settingsService.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) != null)) {\n            showComputeButtonForDataset = true;\n        } else {\n            showComputeButtonForDataset = false;\n        }\n        return showComputeButtonForDataset;\n    }\n    \n    private Map<Long, Boolean> showComputeButtonForFile = new HashMap<>();\n    //this function applies to a single datafile\n    public boolean showComputeButton(FileMetadata metadata) {\n        Long fileId = metadata.getDataFile().getId();\n        \n        if (fileId == null) {\n            return false;\n        }\n        \n        if (showComputeButtonForFile.containsKey(fileId)) {\n            return showComputeButtonForFile.get(fileId);\n        }\n        \n        boolean result = isSwiftStorage(metadata) \n                && settingsService.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) != null;\n        \n        showComputeButtonForFile.put(fileId, result);\n        return result;\n    }\n\n    public boolean canDownloadAllFiles(){\n       for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (!fileDownloadHelper.canDownloadFile(fmd)) {\n                return false;\n            }\n        }\n       return true;\n    }\n    /*\n    in getComputeUrl(), we are sending the container/dataset name and the exipiry and signature \n    for the temporary url of only ONE datafile within the dataset. This is because in the \n    ceph version of swift, we are only able to generate the temporary url for a single object \n    within a container.\n    Ideally, we want a temporary url for an entire container/dataset, so perhaps this could instead\n    be handled on the compute environment end. \n    Additionally, we have to think about the implications this could have with dataset versioning, \n    since we currently store all files (even from old versions) in the same container.\n    --SF\n    */\n    public String getComputeUrl() throws IOException {\n        SwiftAccessIO swiftObject = getSwiftObject();\n        if (swiftObject != null) {\n            swiftObject.open();\n            if (settingsWrapper.isTrueForKey(SettingsServiceBean.Key.PublicInstall, false)) {\n                return settingsWrapper.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) + \"?containerName=\" + swiftObject.getSwiftContainerName();\n            }\n            //assuming we are able to get a temp url for a dataset\n            return settingsWrapper.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) + \"?containerName=\" + swiftObject.getSwiftContainerName() + \"&temp_url_sig=\" + swiftObject.getTempUrlSignature() + \"&temp_url_expires=\" + swiftObject.getTempUrlExpiry();\n        }\n        return \"\";\n        \n    }\n    \n    //For a single file\n    public String getComputeUrl(FileMetadata metadata) {\n        SwiftAccessIO swiftObject = null;\n        try {\n            StorageIO<DataFile> storageIO = metadata.getDataFile().getStorageIO();\n            if (storageIO != null && storageIO instanceof SwiftAccessIO) {\n                swiftObject = (SwiftAccessIO)storageIO;\n                swiftObject.open();\n            }\n\n        } catch (IOException e) {\n            logger.info(\"DatasetPage: Failed to get storageIO\");\n        }\n        if (settingsWrapper.isTrueForKey(SettingsServiceBean.Key.PublicInstall, false)) {\n            return settingsWrapper.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) + \"?containerName=\" + swiftObject.getSwiftContainerName() + \"&objectName=\" + swiftObject.getSwiftFileName();\n        }\n        \n        return settingsWrapper.getValueForKey(SettingsServiceBean.Key.ComputeBaseUrl) + \"?containerName=\" + swiftObject.getSwiftContainerName() + \"&objectName=\" + swiftObject.getSwiftFileName() + \"&temp_url_sig=\" + swiftObject.getTempUrlSignature() + \"&temp_url_expires=\" + swiftObject.getTempUrlExpiry();\n\n    }\n    \n    public String getCloudEnvironmentName() {\n        return settingsWrapper.getValueForKey(SettingsServiceBean.Key.CloudEnvironmentName);\n    }\n    \n    public DataFile getSelectedDownloadFile() {\n        return selectedDownloadFile;\n    }\n\n    public void setSelectedDownloadFile(DataFile selectedDownloadFile) {\n        this.selectedDownloadFile = selectedDownloadFile;\n    }\n    \n    public List<DataFile> getNewFiles() {\n        return newFiles;\n    }\n    \n    public void setNewFiles(List<DataFile> newFiles) {\n        this.newFiles = newFiles;\n    }\n    \n    public Dataverse getLinkingDataverse() {\n        return linkingDataverse;\n    }\n\n    public void setLinkingDataverse(Dataverse linkingDataverse) {\n        this.linkingDataverse = linkingDataverse;\n    }\n\n    public List<SelectItem> getLinkingDVSelectItems() {\n        return linkingDVSelectItems;\n    }\n\n    public void setLinkingDVSelectItems(List<SelectItem> linkingDVSelectItems) {\n        this.linkingDVSelectItems = linkingDVSelectItems;\n    }\n\n    public Long getLinkingDataverseId() {\n        return linkingDataverseId;\n    }\n\n    public void setLinkingDataverseId(Long linkingDataverseId) {\n        this.linkingDataverseId = linkingDataverseId;\n    }\n\n    public List<Dataverse> getDataversesForLinking() {\n        return dataversesForLinking;\n    }\n\n    public void setDataversesForLinking(List<Dataverse> dataversesForLinking) {\n        this.dataversesForLinking = dataversesForLinking;\n    }\n    \n    public void updateReleasedVersions(){\n        \n        setReleasedVersionTabList(resetReleasedVersionTabList());\n        \n    }\n    \n    public void updateLinkableDataverses() {\n        dataversesForLinking = new ArrayList<>();\n        linkingDVSelectItems = new ArrayList<>();\n        \n        //Since this is a super user we are getting all dataverses\n        dataversesForLinking = dataverseService.findAll();\n        if (dataversesForLinking.isEmpty()) {\n            setNoDVsAtAll(true);\n            return;\n        }\n        \n        dataversesForLinking.remove(dataset.getOwner());\n        Dataverse testDV = dataset.getOwner();\n        while(testDV.getOwner() != null){\n            dataversesForLinking.remove(testDV.getOwner());\n            testDV = testDV.getOwner();\n        }                      \n        \n        for (Dataverse removeLinked : dsLinkingService.findLinkingDataverses(dataset.getId())) {\n            dataversesForLinking.remove(removeLinked);\n        }\n        for (Dataverse removeLinked : dvLinkingService.findLinkingDataverses(dataset.getOwner().getId())) {\n            dataversesForLinking.remove(removeLinked);\n        }\n\n        if (dataversesForLinking.isEmpty()) {\n            setNoDVsRemaining(true);            \n            return;\n        }\n\n        for (Dataverse selectDV : dataversesForLinking) {\n            linkingDVSelectItems.add(new SelectItem(selectDV.getId(), selectDV.getDisplayName()));\n        }\n\n        if (!dataversesForLinking.isEmpty() && dataversesForLinking.size() == 1 && dataversesForLinking.get(0) != null) {\n            linkingDataverse = dataversesForLinking.get(0);\n            linkingDataverseId = linkingDataverse.getId();\n        }\n    }\n\n    public void updateSelectedLinkingDV(ValueChangeEvent event) {\n        linkingDataverseId = (Long) event.getNewValue();\n    }\n\n    public boolean isNoDVsAtAll() {\n        return noDVsAtAll;\n    }\n\n    public void setNoDVsAtAll(boolean noDVsAtAll) {\n        this.noDVsAtAll = noDVsAtAll;\n    }\n\n    public boolean isNoDVsRemaining() {\n        return noDVsRemaining;\n    }\n    \n\n    private Map<Long, String> datafileThumbnailsMap = new HashMap<>();\n\n    public boolean isThumbnailAvailable(FileMetadata fileMetadata) {\n        \n        // new and optimized logic: \n        // - check download permission here (should be cached - so it's free!)\n        // - only then check if the thumbnail is available/exists.\n        // then cache the results!\n        \n        Long dataFileId = fileMetadata.getDataFile().getId();\n        \n        if (datafileThumbnailsMap.containsKey(dataFileId)) {\n            return !\"\".equals(datafileThumbnailsMap.get(dataFileId));\n        }\n        \n        if (!FileUtil.isThumbnailSupported(fileMetadata.getDataFile())) {\n            datafileThumbnailsMap.put(dataFileId, \"\");\n            return false;\n        }\n        \n        if (!this.fileDownloadHelper.canDownloadFile(fileMetadata)) {\n            datafileThumbnailsMap.put(dataFileId, \"\");\n            return false;\n        }\n     \n        \n        \n        String thumbnailAsBase64 = ImageThumbConverter.getImageThumbnailAsBase64(fileMetadata.getDataFile(), ImageThumbConverter.DEFAULT_THUMBNAIL_SIZE);\n        \n        \n        //if (datafileService.isThumbnailAvailable(fileMetadata.getDataFile())) {\n        if (!StringUtil.isEmpty(thumbnailAsBase64)) {\n            datafileThumbnailsMap.put(dataFileId, thumbnailAsBase64);\n            return true;\n        } \n        \n        datafileThumbnailsMap.put(dataFileId, \"\");\n        return false;\n        \n    }\n    \n    public String getDataFileThumbnailAsBase64(FileMetadata fileMetadata) {\n        return datafileThumbnailsMap.get(fileMetadata.getDataFile().getId());\n    }\n    \n    // Another convenience method - to cache Update Permission on the dataset: \n    public boolean canUpdateDataset() {\n        return permissionsWrapper.canUpdateDataset(dvRequestService.getDataverseRequest(), this.dataset);\n    }\n    public boolean canPublishDataverse() {\n        return permissionsWrapper.canIssuePublishDataverseCommand(dataset.getOwner());\n    }\n\n    public boolean canViewUnpublishedDataset() {\n        return permissionsWrapper.canViewUnpublishedDataset( dvRequestService.getDataverseRequest(), dataset);\n    }\n        \n    /* \n     * 4.2.1 optimization. \n     * HOWEVER, this doesn't appear to be saving us anything! \n     * i.e., it's just as cheap to use session.getUser().isAuthenticated() \n     * every time; it doesn't do any new db lookups. \n    */\n    public boolean isSessionUserAuthenticated() {\n        return session.getUser().isAuthenticated();\n    }\n    \n    /**\n     * For use in the Dataset page\n     * @return \n     */\n    public boolean isSuperUser(){\n        \n        if (!this.isSessionUserAuthenticated()){\n            return false;\n        }\n        \n        if (this.session.getUser().isSuperuser()){\n            return true;\n        }\n        return false;\n    }\n    /* \n       TODO/OPTIMIZATION: This is still costing us N SELECT FROM GuestbookResponse queries, \n       where N is the number of files. This could of course be replaced by a query that'll \n       look up all N at once... Not sure if it's worth it; especially now that N\n       will always be 10, for the initial page load. -- L.A. 4.2.1\n     */\n    public Long getGuestbookResponseCount(FileMetadata fileMetadata) {\n        return guestbookResponseService.getCountGuestbookResponsesByDataFileId(fileMetadata.getDataFile().getId());\n    }\n    /**\n     * Check Dataset related permissions\n     * \n     * @param permissionToCheck\n     * @return \n     */\n    public boolean doesSessionUserHaveDataSetPermission(Permission permissionToCheck){\n        if (permissionToCheck == null){\n            return false;\n        }\n               \n        String permName = permissionToCheck.getHumanName();\n       \n        // Has this check already been done? \n        // \n        if (this.datasetPermissionMap.containsKey(permName)){\n            // Yes, return previous answer\n            return this.datasetPermissionMap.get(permName);\n        }\n        \n        // Check the permission\n        //\n        boolean hasPermission = this.permissionService.userOn(this.session.getUser(), this.dataset).has(permissionToCheck);\n\n        // Save the permission\n        this.datasetPermissionMap.put(permName, hasPermission);\n        \n        // return true/false\n        return hasPermission;\n    }\n    \n    public void setNoDVsRemaining(boolean noDVsRemaining) {\n        this.noDVsRemaining = noDVsRemaining;\n    }\n\n    private final Map<Long, MapLayerMetadata> mapLayerMetadataLookup = new HashMap<>();\n\n    private GuestbookResponse guestbookResponse;\n    private Guestbook selectedGuestbook;\n\n    public GuestbookResponse getGuestbookResponse() {\n        return guestbookResponse;\n    }\n\n    public void setGuestbookResponse(GuestbookResponse guestbookResponse) {\n        this.guestbookResponse = guestbookResponse;\n    }\n\n    public Guestbook getSelectedGuestbook() {\n        return selectedGuestbook;\n    }\n\n    public void setSelectedGuestbook(Guestbook selectedGuestbook) {\n        this.selectedGuestbook = selectedGuestbook;\n    }\n\n    public void viewSelectedGuestbook(Guestbook selectedGuestbook) {\n        this.selectedGuestbook = selectedGuestbook;\n    }\n\n    public void reset() {\n        dataset.setGuestbook(null);\n    }\n    \n\n\n    public String getGlobalId() {\n        return persistentId;\n    }\n        \n    public String getPersistentId() {\n        return persistentId;\n    }\n\n    public void setPersistentId(String persistentId) {\n        this.persistentId = persistentId;\n    }\n    public String getVersion() {\n        return version;\n    }\n\n    public void setVersion(String version) {\n        this.version = version;\n    }    \n\n    public String getShowVersionList() {\n        return showVersionList;\n    }\n\n    public void setShowVersionList(String showVersionList) {\n        this.showVersionList = showVersionList;\n    }\n\n    public String getShowOtherText() {\n        return showOtherText;\n    }\n\n    public void setShowOtherText(String showOtherText) {\n        this.showOtherText = showOtherText;\n    }\n    private String showOtherText = \"false\";\n\n    public String getDeaccessionForwardURLFor() {\n        return deaccessionForwardURLFor;\n    }\n\n    public void setDeaccessionForwardURLFor(String deaccessionForwardURLFor) {\n        this.deaccessionForwardURLFor = deaccessionForwardURLFor;\n    }\n    private DatasetVersionDifference datasetVersionDifference;\n\n    public String getDeaccessionReasonText() {\n        return deaccessionReasonText;\n    }\n\n    public void setDeaccessionReasonText(String deaccessionReasonText) {\n        this.deaccessionReasonText = deaccessionReasonText;\n    }\n\n    public String getDisplayCitation() {\n        //displayCitation = dataset.getCitation(false, workingVersion);\n        return displayCitation;\n    }\n\n    public void setDisplayCitation(String displayCitation) {\n        this.displayCitation = displayCitation;\n    }\n\n    public String getDropBoxSelection() {\n        return dropBoxSelection;\n    }\n\n    public String getDropBoxKey() {\n        // Site-specific DropBox application registration key is configured \n        // via a JVM option under glassfish.\n        //if (true)return \"some-test-key\";  // for debugging\n\n        String configuredDropBoxKey = System.getProperty(\"dataverse.dropbox.key\");\n        if (configuredDropBoxKey != null) {\n            return configuredDropBoxKey;\n        }\n        return \"\";\n    }\n\n    public void setDropBoxSelection(String dropBoxSelection) {\n        this.dropBoxSelection = dropBoxSelection;\n    }\n\n    public Dataset getDataset() {\n        return dataset;\n    }\n\n    public void setDataset(Dataset dataset) {\n        this.dataset = dataset;\n    }\n\n    public DatasetVersion getWorkingVersion() {\n        return workingVersion;\n    }\n\n    public EditMode getEditMode() {\n        return editMode;\n    }\n\n    public void setEditMode(EditMode editMode) {\n        this.editMode = editMode;\n    }\n\n    public Long getOwnerId() {\n        return ownerId;\n    }\n\n    public void setOwnerId(Long ownerId) {\n        this.ownerId = ownerId;\n    }\n\n    public Long getVersionId() {\n        return versionId;\n    }\n\n    public void setVersionId(Long versionId) {\n        this.versionId = versionId;\n    }\n\n    public int getSelectedTabIndex() {\n        return selectedTabIndex;\n    }\n\n    public void setSelectedTabIndex(int selectedTabIndex) {\n        this.selectedTabIndex = selectedTabIndex;\n    }\n\n    public int getReleaseRadio() {\n        return releaseRadio;\n    }\n\n    public void setReleaseRadio(int releaseRadio) {\n        this.releaseRadio = releaseRadio;\n    }\n\n    public String getDatasetNextMajorVersion() {\n        return datasetNextMajorVersion;\n    }\n\n    public void setDatasetNextMajorVersion(String datasetNextMajorVersion) {\n        this.datasetNextMajorVersion = datasetNextMajorVersion;\n    }\n\n    public String getDatasetNextMinorVersion() {\n        return datasetNextMinorVersion;\n    }\n\n    public void setDatasetNextMinorVersion(String datasetNextMinorVersion) {\n        this.datasetNextMinorVersion = datasetNextMinorVersion;\n    }\n\n    public int getDeaccessionReasonRadio() {\n        return deaccessionReasonRadio;\n    }\n\n    public void setDeaccessionReasonRadio(int deaccessionReasonRadio) {\n        this.deaccessionReasonRadio = deaccessionReasonRadio;\n    }\n\n    public int getDeaccessionRadio() {\n        return deaccessionRadio;\n    }\n\n    public void setDeaccessionRadio(int deaccessionRadio) {\n        this.deaccessionRadio = deaccessionRadio;\n    }\n\n    public List<Template> getDataverseTemplates() {\n        return dataverseTemplates;\n    }\n\n    public void setDataverseTemplates(List<Template> dataverseTemplates) {\n        this.dataverseTemplates = dataverseTemplates;\n    }\n\n    public Template getDefaultTemplate() {\n        return defaultTemplate;\n    }\n\n    public void setDefaultTemplate(Template defaultTemplate) {\n        this.defaultTemplate = defaultTemplate;\n    }\n\n    public Template getSelectedTemplate() {\n        return selectedTemplate;\n    }\n\n    public void setSelectedTemplate(Template selectedTemplate) {\n        this.selectedTemplate = selectedTemplate;\n    }\n\n    public void updateSelectedTemplate(ValueChangeEvent event) {\n        \n        selectedTemplate = (Template) event.getNewValue();\n        if (selectedTemplate != null) {\n            //then create new working version from the selected template\n            workingVersion.updateDefaultValuesFromTemplate(selectedTemplate); \n            updateDatasetFieldInputLevels();\n        } else { \n            workingVersion.initDefaultValues();\n            updateDatasetFieldInputLevels();\n        }\n        resetVersionUI();\n    }\n\n    \n    \n    /*\n    // Original\n    private void updateDatasetFieldInputLevels() {\n        Long dvIdForInputLevel = ownerId;\n\n        if (!dataverseService.find(ownerId).isMetadataBlockRoot()) {\n            dvIdForInputLevel = dataverseService.find(ownerId).getMetadataRootId();\n        }\n        for (DatasetField dsf : workingVersion.getFlatDatasetFields()) {\n            DataverseFieldTypeInputLevel dsfIl = dataverseFieldTypeInputLevelService.findByDataverseIdDatasetFieldTypeId(dvIdForInputLevel, dsf.getDatasetFieldType().getId());\n            if (dsfIl != null) {\n                dsf.setInclude(dsfIl.isInclude());\n            } else {\n                dsf.setInclude(true);\n            }\n        }\n    }*/\n\n    /***\n     *\n     * Note: Updated to retrieve DataverseFieldTypeInputLevel objects in single query\n     *\n     */    \n     private void updateDatasetFieldInputLevels() {\n         Long dvIdForInputLevel = ownerId;\n        \n         // OPTIMIZATION (?): replaced \"dataverseService.find(ownerId)\" with \n         // simply dataset.getOwner()... saves us a few lookups.\n         // TODO: could there possibly be any reason we want to look this \n         // dataverse up by the id here?? -- L.A. 4.2.1\n         if (!dataset.getOwner().isMetadataBlockRoot()) {\n             dvIdForInputLevel = dataset.getOwner().getMetadataRootId();\n         }        \n        \n        /* ---------------------------------------------------------\n            Map to hold DatasetFields  \n            Format:  {  DatasetFieldType.id : DatasetField }\n         --------------------------------------------------------- */\n        // Initialize Map\n        Map<Long, DatasetField> mapDatasetFields = new HashMap<>();   \n\n        // Populate Map\n         for (DatasetField dsf : workingVersion.getFlatDatasetFields()) {\n            if (dsf.getDatasetFieldType().getId() != null){\n                mapDatasetFields.put(dsf.getDatasetFieldType().getId(), dsf);\n            }\n        }      \n        \n        /* ---------------------------------------------------------\n            Retrieve List of DataverseFieldTypeInputLevel objects\n            Use the DatasetFieldType id's which are the Map's keys\n         --------------------------------------------------------- */\n        List<Long> idList = new ArrayList<>(mapDatasetFields.keySet());\n        List<DataverseFieldTypeInputLevel> dsFieldTypeInputLevels = dataverseFieldTypeInputLevelService.findByDataverseIdAndDatasetFieldTypeIdList(dvIdForInputLevel, idList);\n        \n        /* ---------------------------------------------------------\n            Iterate through List of DataverseFieldTypeInputLevel objects\n            Call \"setInclude\" on its related DatasetField object\n         --------------------------------------------------------- */\n        for (DataverseFieldTypeInputLevel oneDSFieldTypeInputLevel : dsFieldTypeInputLevels){\n            \n            if (oneDSFieldTypeInputLevel != null) {\n                // Is the DatasetField in the hash?    hash format: {  DatasetFieldType.id : DatasetField }\n                DatasetField dsf = mapDatasetFields.get(oneDSFieldTypeInputLevel.getDatasetFieldType().getId());  \n                if (dsf != null){\n                    // Yes, call \"setInclude\"\n                    dsf.setInclude(oneDSFieldTypeInputLevel.isInclude());\n                    // remove from hash                \n                    mapDatasetFields.remove(oneDSFieldTypeInputLevel.getDatasetFieldType().getId());    \n                }\n            }\n        }  // end: updateDatasetFieldInputLevels\n        \n        /* ---------------------------------------------------------\n            Iterate through any DatasetField objects remaining in the hash\n            Call \"setInclude(true) on each one\n         --------------------------------------------------------- */\n        for ( DatasetField dsf  : mapDatasetFields.values()) {\n               if (dsf != null){\n                   dsf.setInclude(true);\n               }\n        }\n     }\n\n    public void handleChange() {\n        logger.fine(\"handle change\");\n        logger.fine(\"new value \" + selectedTemplate.getId());\n    }\n\n    public void handleChangeButton() {\n\n    }\n\n    public boolean isShapefileType(FileMetadata fm) {\n        if (fm == null) {\n            return false;\n        }\n        if (fm.getDataFile() == null) {\n            return false;\n        }\n\n        return fm.getDataFile().isShapefileType();\n    }\n\n    /*\n     Check if the FileMetadata.dataFile has an associated MapLayerMetadata object\n    \n     The MapLayerMetadata objects have been fetched at page inception by \"loadMapLayerMetadataLookup()\" \n     */\n    public boolean hasMapLayerMetadata(FileMetadata fm) {\n        if (fm == null) {\n            return false;\n        }\n        if (fm.getDataFile() == null) {\n            return false;\n        }\n        return doesDataFileHaveMapLayerMetadata(fm.getDataFile());\n    }\n\n    /**\n     * Check if a DataFile has an associated MapLayerMetadata object\n     *\n     * The MapLayerMetadata objects have been fetched at page inception by\n     * \"loadMapLayerMetadataLookup()\"\n     */\n    private boolean doesDataFileHaveMapLayerMetadata(DataFile df) {\n        if (df == null) {\n            return false;\n        }\n        if (df.getId() == null) {\n            return false;\n        }\n        return this.mapLayerMetadataLookup.containsKey(df.getId());\n    }\n\n    /**\n     * Using a DataFile id, retrieve an associated MapLayerMetadata object\n     *\n     * The MapLayerMetadata objects have been fetched at page inception by\n     * \"loadMapLayerMetadataLookup()\"\n     */\n    public MapLayerMetadata getMapLayerMetadata(DataFile df) {\n        if (df == null) {\n            return null;\n        }\n        return this.mapLayerMetadataLookup.get(df.getId());\n    }\n\n    private void msg(String s){\n        // System.out.println(s);\n    }\n    \n    /**\n     * For development\n     * \n     * Flag for whether to show sample insert statements for Geoconnect Debug\n     * \n     * Conditions to meet: Person is superuser and GeoconnectDebug active \n     * \n     * @return \n     */\n    public boolean isGeoconnectDebugAvailable(){\n\n        if (!this.isSuperUser()){\n            return false;\n        }\n\n        if (settingsWrapper.isTrueForKey(SettingsServiceBean.Key.GeoconnectDebug, false)){\n            return true;\n        }    \n        return false;\n      \n    }\n\n    /**\n     * Create a hashmap consisting of { DataFile.id : MapLayerMetadata object}\n     *\n     * Very few DataFiles will have associated MapLayerMetadata objects so only\n     * use 1 query to get them\n     */\n    private void loadMapLayerMetadataLookup() {\n        if (this.dataset == null) {\n        }\n        if (this.dataset.getId() == null) {\n            return;\n        }\n        List<MapLayerMetadata> mapLayerMetadataList = mapLayerMetadataService.getMapLayerMetadataForDataset(this.dataset);\n        if (mapLayerMetadataList == null) {\n            return;\n        }\n        for (MapLayerMetadata layer_metadata : mapLayerMetadataList) {\n            mapLayerMetadataLookup.put(layer_metadata.getDataFile().getId(), layer_metadata);\n        }\n\n    }// A DataFile may have a related MapLayerMetadata object\n\n    \n    \n    private List<FileMetadata> displayFileMetadata;\n\n    public List<FileMetadata> getDisplayFileMetadata() {\n        return displayFileMetadata;\n    }\n\n    public void setDisplayFileMetadata(List<FileMetadata> displayFileMetadata) {\n        this.displayFileMetadata = displayFileMetadata;\n    }\n    \n    private boolean readOnly = true; \n    private String originalSourceUrl = null;\n\n    public String getOriginalSourceUrl() {\n        return originalSourceUrl; \n    }\n    \n    public void setOriginalSourceUrl(String originalSourceUrl) {\n        this.originalSourceUrl = originalSourceUrl;\n    }\n    \n    public String init() {\n        return init(true);\n    }\n    \n    public String initCitation() {\n        return init(false);\n    }     \n    \n    private String init(boolean initFull) {\n        //System.out.println(\"_YE_OLDE_QUERY_COUNTER_\");  // for debug purposes\n               \n        this.maxFileUploadSizeInBytes = systemConfig.getMaxFileUploadSize();\n        setDataverseSiteUrl(systemConfig.getDataverseSiteUrl());\n\n        guestbookResponse = new GuestbookResponse();\n        \n        String nonNullDefaultIfKeyNotFound = \"\";\n        protocol = settingsWrapper.getValueForKey(SettingsServiceBean.Key.Protocol, nonNullDefaultIfKeyNotFound);\n        authority = settingsWrapper.getValueForKey(SettingsServiceBean.Key.Authority, nonNullDefaultIfKeyNotFound);\n        separator = settingsWrapper.getValueForKey(SettingsServiceBean.Key.DoiSeparator, nonNullDefaultIfKeyNotFound);\n        \n        if (dataset.getId() != null || versionId != null || persistentId != null) { // view mode for a dataset     \n\n            DatasetVersionServiceBean.RetrieveDatasetVersionResponse retrieveDatasetVersionResponse = null;\n\n            // ---------------------------------------\n            // Set the workingVersion and Dataset\n            // ---------------------------------------           \n            if (persistentId != null) {\n                logger.fine(\"initializing DatasetPage with persistent ID \" + persistentId);\n                // Set Working Version and Dataset by PersistentID\n                dataset = datasetService.findByGlobalId(persistentId);\n                if (dataset == null) {\n                    logger.warning(\"No such dataset: \"+persistentId);\n                    return permissionsWrapper.notFound();\n                }\n                logger.fine(\"retrieved dataset, id=\"+dataset.getId());\n                \n                retrieveDatasetVersionResponse = datasetVersionService.selectRequestedVersion(dataset.getVersions(), version);\n                //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionByPersistentId(persistentId, version);\n                this.workingVersion = retrieveDatasetVersionResponse.getDatasetVersion();\n                logger.fine(\"retrieved version: id: \" + workingVersion.getId() + \", state: \" + this.workingVersion.getVersionState());\n\n            } else if (dataset.getId() != null) {\n                // Set Working Version and Dataset by Datasaet Id and Version\n                dataset = datasetService.find(dataset.getId());\n                if (dataset == null) {\n                    logger.warning(\"No such dataset: \"+dataset);\n                    return permissionsWrapper.notFound();\n                }\n                //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionById(dataset.getId(), version);\n                retrieveDatasetVersionResponse = datasetVersionService.selectRequestedVersion(dataset.getVersions(), version);\n                this.workingVersion = retrieveDatasetVersionResponse.getDatasetVersion();\n                logger.info(\"retreived version: id: \" + workingVersion.getId() + \", state: \" + this.workingVersion.getVersionState());\n\n            } else if (versionId != null) {\n                // TODO: 4.2.1 - this method is broken as of now!\n                // Set Working Version and Dataset by DatasaetVersion Id\n                //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionByVersionId(versionId);\n\n            } \n\n            if (retrieveDatasetVersionResponse == null) {\n                return permissionsWrapper.notFound();\n            }\n\n            \n            //this.dataset = this.workingVersion.getDataset();\n\n            // end: Set the workingVersion and Dataset\n            // ---------------------------------------\n            // Is the DatasetVersion or Dataset null?\n            //\n            if (workingVersion == null || this.dataset == null) {\n                return permissionsWrapper.notFound();\n            }\n\n            // Is the Dataset harvested?\n            if (dataset.isHarvested()) {\n                // if so, we'll simply forward to the remote URL for the original\n                // source of this harvested dataset:\n                String originalSourceURL = dataset.getRemoteArchiveURL();\n                if (originalSourceURL != null && !originalSourceURL.equals(\"\")) {\n                    logger.fine(\"redirecting to \"+originalSourceURL);\n                    try {\n                        FacesContext.getCurrentInstance().getExternalContext().redirect(originalSourceURL);\n                    } catch (IOException ioex) {\n                        // must be a bad URL...\n                        // we don't need to do anything special here - we'll redirect\n                        // to the local 404 page, below.\n                        logger.warning(\"failed to issue a redirect to \"+originalSourceURL);\n                    }\n                    return originalSourceURL;\n                }\n\n                return permissionsWrapper.notFound();\n            }\n\n            // Check permisisons           \n            if (!(workingVersion.isReleased() || workingVersion.isDeaccessioned()) && !this.canViewUnpublishedDataset()) {\n                return permissionsWrapper.notAuthorized();\n            }\n\n            if (!retrieveDatasetVersionResponse.wasRequestedVersionRetrieved()) {\n                //msg(\"checkit \" + retrieveDatasetVersionResponse.getDifferentVersionMessage());\n                JsfHelper.addWarningMessage(retrieveDatasetVersionResponse.getDifferentVersionMessage());//JH.localize(\"dataset.message.metadataSuccess\"));\n            }\n            \n            // init the citation\n            displayCitation = dataset.getCitation(true, workingVersion);\n            \n\n            if (initFull) {\n                // init the list of FileMetadatas\n                if (workingVersion.isDraft() && canUpdateDataset()) {\n                    readOnly = false;\n                } else {\n                    // an attempt to retreive both the filemetadatas and datafiles early on, so that \n                    // we don't have to do so later (possibly, many more times than necessary):\n                    datafileService.findFileMetadataOptimizedExperimental(dataset);\n                }\n                fileMetadatasSearch = workingVersion.getFileMetadatasSorted();\n\n                ownerId = dataset.getOwner().getId();\n                datasetNextMajorVersion = this.dataset.getNextMajorVersionString();\n                datasetNextMinorVersion = this.dataset.getNextMinorVersionString();\n                datasetVersionUI = datasetVersionUI.initDatasetVersionUI(workingVersion, false);\n                updateDatasetFieldInputLevels();\n                \n                setExistReleasedVersion(resetExistRealeaseVersion());\n                //moving setVersionTabList to tab change event\n                //setVersionTabList(resetVersionTabList());\n                //setReleasedVersionTabList(resetReleasedVersionTabList());\n                //SEK - lazymodel may be needed for datascroller in future release\n                // lazyModel = new LazyFileMetadataDataModel(workingVersion.getId(), datafileService );\n                // populate MapLayerMetadata\n                this.loadMapLayerMetadataLookup();  // A DataFile may have a related MapLayerMetadata object\n                this.guestbookResponse = guestbookResponseService.initGuestbookResponseForFragment(dataset, null, session);\n                this.getFileDownloadHelper().setGuestbookResponse(guestbookResponse);\n                logger.fine(\"Checking if rsync support is enabled.\");\n                if (DataCaptureModuleUtil.rsyncSupportEnabled(settingsWrapper.getValueForKey(SettingsServiceBean.Key.UploadMethods))) {\n                    try {\n                        ScriptRequestResponse scriptRequestResponse = commandEngine.submit(new RequestRsyncScriptCommand(dvRequestService.getDataverseRequest(), dataset));\n                        logger.fine(\"script: \" + scriptRequestResponse.getScript());\n                        if(scriptRequestResponse.getScript()!=null && !scriptRequestResponse.getScript().isEmpty()){\n                            setHasRsyncScript(true);\n                            setRsyncScript(scriptRequestResponse.getScript());\n                            rsyncScriptFilename = \"upload-\"+ workingVersion.getDataset().getIdentifier() + \".bash\";\n                        }\n                        else{\n                            setHasRsyncScript(false);\n                        }\n                    } catch (RuntimeException ex) {\n                        logger.warning(\"Problem getting rsync script: \" + ex.getLocalizedMessage());\n                    } catch (CommandException cex) {\n                        logger.warning(\"Problem getting rsync script (Command Exception): \" + cex.getLocalizedMessage());\n                    }  \n                }\n                \n            }\n        } else if (ownerId != null) {\n            // create mode for a new child dataset\n            readOnly = false; \n            editMode = EditMode.CREATE;\n            dataset.setOwner(dataverseService.find(ownerId));\n            dataset.setProtocol(protocol);\n            dataset.setAuthority(authority);\n            dataset.setDoiSeparator(separator);\n            //Wait until the create command before actually getting an identifier  \n            //dataset.setIdentifier(datasetService.generateDatasetIdentifier(protocol, authority, separator));\n\n            if (dataset.getOwner() == null) {\n                return permissionsWrapper.notFound();\n            } else if (!permissionService.on(dataset.getOwner()).has(Permission.AddDataset)) {\n                return permissionsWrapper.notAuthorized(); \n            }\n\n            dataverseTemplates = dataverseService.find(ownerId).getTemplates();\n            if (!dataverseService.find(ownerId).isTemplateRoot()) {\n                dataverseTemplates.addAll(dataverseService.find(ownerId).getParentTemplates());\n            }\n            defaultTemplate = dataverseService.find(ownerId).getDefaultTemplate();\n            if (defaultTemplate != null) {\n                selectedTemplate = defaultTemplate;\n                for (Template testT : dataverseTemplates) {\n                    if (defaultTemplate.getId().equals(testT.getId())) {\n                        selectedTemplate = testT;\n                    }\n                }\n                workingVersion = dataset.getEditVersion(selectedTemplate);\n                updateDatasetFieldInputLevels();\n            } else {\n                workingVersion = dataset.getCreateVersion();\n                updateDatasetFieldInputLevels();\n            }\n            \n            if (settingsWrapper.isTrueForKey(SettingsServiceBean.Key.PublicInstall, false)){\n                JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.message.publicInstall\"));\n            }\n\n            resetVersionUI();\n\n            // FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_INFO, \"Add New Dataset\", \" - Enter metadata to create the dataset's citation. You can add more metadata about this dataset after it's created.\"));\n        } else {\n            return permissionsWrapper.notFound();\n        }\n        try {\n            privateUrl = commandEngine.submit(new GetPrivateUrlCommand(dvRequestService.getDataverseRequest(), dataset));\n            if (privateUrl != null) {\n                JH.addMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.privateurl.infoMessageAuthor\", Arrays.asList(getPrivateUrlLink(privateUrl))));\n            }\n        } catch (CommandException ex) {\n            // No big deal. The user simply doesn't have access to create or delete a Private URL.\n        }\n        if (session.getUser() instanceof PrivateUrlUser) {\n            PrivateUrlUser privateUrlUser = (PrivateUrlUser) session.getUser();\n            if (dataset != null && dataset.getId().equals(privateUrlUser.getDatasetId())) {\n                JH.addMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.privateurl.infoMessageReviewer\"));\n            }\n        }\n                \n        // Various info messages, when the dataset is locked (for various reasons):\n        if (dataset.isLocked()) {\n            if (dataset.isLockedFor(DatasetLock.Reason.Workflow)) {\n                JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.locked.message\"),\n                        BundleUtil.getStringFromBundle(\"dataset.publish.workflow.inprogress\"));\n            }\n            if (dataset.isLockedFor(DatasetLock.Reason.InReview)) {\n                JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.locked.inReview.message\"),\n                        BundleUtil.getStringFromBundle(\"dataset.inreview.infoMessage\"));\n            }\n            if (dataset.isLockedFor(DatasetLock.Reason.DcmUpload)) {\n                JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"file.rsyncUpload.inProgressMessage.summary\"),\n                        BundleUtil.getStringFromBundle(\"file.rsyncUpload.inProgressMessage.details\"));\n            }\n        }\n\n        configureTools = externalToolService.findByType(ExternalTool.Type.CONFIGURE);\n        exploreTools = externalToolService.findByType(ExternalTool.Type.EXPLORE);\n\n        return null;\n    }\n    \n    public boolean isReadOnly() {\n        return readOnly; \n    }\n\n    private void resetVersionUI() {\n        \n        datasetVersionUI = datasetVersionUI.initDatasetVersionUI(workingVersion, true);\n        if (isSessionUserAuthenticated()) {\n            AuthenticatedUser au = (AuthenticatedUser) session.getUser();\n\n            //On create set pre-populated fields\n            for (DatasetField dsf : dataset.getEditVersion().getDatasetFields()) {\n                if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.depositor) && dsf.isEmpty()) {\n                    dsf.getDatasetFieldValues().get(0).setValue(au.getLastName() + \", \" + au.getFirstName());\n                }\n                if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.dateOfDeposit) && dsf.isEmpty()) {\n                    dsf.getDatasetFieldValues().get(0).setValue(new SimpleDateFormat(\"yyyy-MM-dd\").format(new Timestamp(new Date().getTime())));\n                }\n\n                if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.datasetContact) && dsf.isEmpty()) {\n                    for (DatasetFieldCompoundValue contactValue : dsf.getDatasetFieldCompoundValues()) {\n                        for (DatasetField subField : contactValue.getChildDatasetFields()) {\n                            if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.datasetContactName)) {\n                                subField.getDatasetFieldValues().get(0).setValue(au.getLastName() + \", \" + au.getFirstName());\n                            }\n                            if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.datasetContactAffiliation)) {\n                                subField.getDatasetFieldValues().get(0).setValue(au.getAffiliation());\n                            }\n                            if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.datasetContactEmail)) {\n                                subField.getDatasetFieldValues().get(0).setValue(au.getEmail());\n                            }\n                        }\n                    }\n                }\n\n                String creatorOrcidId = au.getOrcidId();\n                if (dsf.getDatasetFieldType().getName().equals(DatasetFieldConstant.author) && dsf.isEmpty()) {\n                    for (DatasetFieldCompoundValue authorValue : dsf.getDatasetFieldCompoundValues()) {\n                        for (DatasetField subField : authorValue.getChildDatasetFields()) {\n                            if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.authorName)) {\n                                subField.getDatasetFieldValues().get(0).setValue(au.getLastName() + \", \" + au.getFirstName());\n                            }\n                            if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.authorAffiliation)) {\n                                subField.getDatasetFieldValues().get(0).setValue(au.getAffiliation());\n                            }\n                            if (creatorOrcidId != null) {\n                                if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.authorIdValue)) {\n                                    subField.getDatasetFieldValues().get(0).setValue(creatorOrcidId);\n                                }\n                                if (subField.getDatasetFieldType().getName().equals(DatasetFieldConstant.authorIdType)) {  \n                                   DatasetFieldType authorIdTypeDatasetField = fieldService.findByName(DatasetFieldConstant.authorIdType);\n                                   subField.setSingleControlledVocabularyValue(fieldService.findControlledVocabularyValueByDatasetFieldTypeAndStrValue(authorIdTypeDatasetField, \"ORCID\", true));\n                                }                                \n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \n    private boolean bulkUpdateCheckVersion(){\n        return workingVersion.isReleased();\n    }\n    \n    private void refreshSelectedFiles(){\n        if (readOnly) {\n            dataset = datasetService.find(dataset.getId());\n        }\n        String termsOfAccess = workingVersion.getTermsOfUseAndAccess().getTermsOfAccess();\n        boolean requestAccess = workingVersion.getTermsOfUseAndAccess().isFileAccessRequest();\n        workingVersion = dataset.getEditVersion();\n        workingVersion.getTermsOfUseAndAccess().setTermsOfAccess(termsOfAccess);\n        workingVersion.getTermsOfUseAndAccess().setFileAccessRequest(requestAccess);\n        List <FileMetadata> newSelectedFiles = new ArrayList<>();\n        for (FileMetadata fmd : selectedFiles){\n            for (FileMetadata fmdn: workingVersion.getFileMetadatas()){\n                if (fmd.getDataFile().equals(fmdn.getDataFile())){\n                    newSelectedFiles.add(fmdn);\n                }\n            }\n        }\n        \n        selectedFiles.clear();\n        for (FileMetadata fmdn : newSelectedFiles ){\n            selectedFiles.add(fmdn);\n        }\n        readOnly = false;\n    }\n    \n    public void testSelectedFilesForMapData(){\n        setSelectedFilesHasMapLayer(false); \n        for (FileMetadata fmd : selectedFiles){\n            if(worldMapPermissionHelper.hasMapLayerMetadata(fmd)){\n                setSelectedFilesHasMapLayer(true);\n                return; //only need one for warning message\n            }\n        }\n    }\n    \n    private boolean selectedFilesHasMapLayer;\n\n    public boolean isSelectedFilesHasMapLayer() {\n        return selectedFilesHasMapLayer;\n    }\n\n    public void setSelectedFilesHasMapLayer(boolean selectedFilesHasMapLayer) {\n        this.selectedFilesHasMapLayer = selectedFilesHasMapLayer;\n    }\n    \n    private Integer chunkSize = 25;\n\n    public Integer getChunkSize() {\n        return chunkSize;\n    }\n\n    public void setChunkSize(Integer chunkSize) {\n        this.chunkSize = chunkSize;\n    }\n    \n    public void viewAllButtonPress(){\n        setChunkSize(fileMetadatasSearch.size());\n    }\n    \n     private int activeTabIndex;\n\n    public int getActiveTabIndex() {\n        return activeTabIndex;\n    }\n\n    public void setActiveTabIndex(int activeTabIndex) {\n        this.activeTabIndex = activeTabIndex;\n    }\n    \n    public void tabChanged(TabChangeEvent event) {\n        TabView tv = (TabView) event.getComponent();\n        this.activeTabIndex = tv.getActiveIndex();\n        if (this.activeTabIndex == 3) {\n            setVersionTabList(resetVersionTabList());\n            setReleasedVersionTabList(resetReleasedVersionTabList());\n        } else {\n            releasedVersionTabList = new ArrayList<>();\n            versionTabList = new ArrayList<>();\n            if(this.activeTabIndex == 0) {\n                 init();\n            }          \n        }\n    }\n\n    public void edit(EditMode editMode) {\n        this.editMode = editMode;\n        if (this.readOnly) {\n            dataset = datasetService.find(dataset.getId());\n        }\n        workingVersion = dataset.getEditVersion();\n\n        if (editMode == EditMode.INFO) {\n            // ?\n        } else if (editMode == EditMode.FILE) {\n            // JH.addMessage(FacesMessage.SEVERITY_INFO, JH.localize(\"dataset.message.editFiles\"));\n            // FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_INFO, \"Upload + Edit Dataset Files\", \" - You can drag and drop your files from your desktop, directly into the upload widget.\"));\n        } else if (editMode.equals(EditMode.METADATA)) {\n            datasetVersionUI = datasetVersionUI.initDatasetVersionUI(workingVersion, true);\n            updateDatasetFieldInputLevels();\n            JH.addMessage(FacesMessage.SEVERITY_INFO, JH.localize(\"dataset.message.editMetadata\"));\n            //FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_INFO, \"Edit Dataset Metadata\", \" - Add more metadata about your dataset to help others easily find it.\"));\n        } else if (editMode.equals(EditMode.LICENSE)){\n            JH.addMessage(FacesMessage.SEVERITY_INFO, JH.localize(\"dataset.message.editTerms\"));\n            //FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_INFO, \"Edit Dataset License and Terms\", \" - Update your dataset's license and terms of use.\"));\n        }\n        this.readOnly = false;\n    }\n\n    public String releaseDraft() {\n        if (releaseRadio == 1) {\n            return releaseDataset(true);\n        } else {\n            return releaseDataset(false);\n        }\n    }\n\n    public String releaseMajor() {\n        return releaseDataset(false);\n    }\n\n    public String sendBackToContributor() {\n        try {\n            //FIXME - Get Return Comment from sendBackToContributor popup\n            Command<Dataset> cmd = new ReturnDatasetToAuthorCommand(dvRequestService.getDataverseRequest(), dataset, \"\");\n            dataset = commandEngine.submit(cmd);\n            JsfHelper.addSuccessMessage(BundleUtil.getStringFromBundle(\"dataset.reject.success\"));\n        } catch (CommandException ex) {\n            String message = ex.getMessage();\n            logger.log(Level.SEVERE, \"sendBackToContributor: {0}\", message);\n            JsfHelper.addErrorMessage(BundleUtil.getStringFromBundle(\"dataset.reject.failure\", Collections.singletonList(message)));\n        }\n        \n        /* \n         The notifications below are redundant, since the ReturnDatasetToAuthorCommand\n         sends them already. - L.A. Sep. 7 2017\n         \n        List<AuthenticatedUser> authUsers = permissionService.getUsersWithPermissionOn(Permission.PublishDataset, dataset);\n        List<AuthenticatedUser> editUsers = permissionService.getUsersWithPermissionOn(Permission.EditDataset, dataset);\n\n        editUsers.removeAll(authUsers);\n        new HashSet<>(editUsers).forEach( au -> \n            userNotificationService.sendNotification(au, new Timestamp(new Date().getTime()), \n                                                     UserNotification.Type.RETURNEDDS, dataset.getLatestVersion().getId())\n        );\n        */\n\n        //FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_INFO, \"DatasetSubmitted\", \"This dataset has been sent back to the contributor.\");\n        //FacesContext.getCurrentInstance().addMessage(null, message);\n        return  returnToLatestVersion();\n    }\n\n    public String submitDataset() {\n        try {\n            Command<Dataset> cmd = new SubmitDatasetForReviewCommand( dvRequestService.getDataverseRequest(), dataset);\n            dataset = commandEngine.submit(cmd);\n            //JsfHelper.addSuccessMessage(BundleUtil.getStringFromBundle(\"dataset.submit.success\"));\n        } catch (CommandException ex) {\n            String message = ex.getMessage();\n            logger.log(Level.SEVERE, \"submitDataset: {0}\", message);\n            JsfHelper.addErrorMessage(BundleUtil.getStringFromBundle(\"dataset.submit.failure\", Collections.singletonList(message)));\n        }\n        return returnToLatestVersion();\n    }\n    \n    public String releaseParentDVAndDataset(){\n        releaseParentDV();\n        return releaseDataset(false);\n    }\n\n    public String releaseDataset() {\n        return releaseDataset(false);\n    }\n    \n    private void releaseParentDV(){\n        if (session.getUser() instanceof AuthenticatedUser) {\n            PublishDataverseCommand cmd = new PublishDataverseCommand(dvRequestService.getDataverseRequest(), dataset.getOwner());\n            try {\n                commandEngine.submit(cmd);\n                JsfHelper.addSuccessMessage(JH.localize(\"dataverse.publish.success\"));\n\n            } catch (CommandException ex) {\n                logger.log(Level.SEVERE, \"Unexpected Exception calling  publish dataverse command\", ex);\n                JsfHelper.addErrorMessage(JH.localize(\"dataverse.publish.failure\"));\n\n            }\n        } else {\n            FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_INFO, \"DataverseNotReleased\", \"Only authenticated users can release a dataverse.\");\n            FacesContext.getCurrentInstance().addMessage(null, message);\n        }\n        \n    }\n\n    public String deaccessionVersions() {\n        Command<DatasetVersion> cmd;\n        try {\n            if (selectedDeaccessionVersions == null) {\n                for (DatasetVersion dv : this.dataset.getVersions()) {\n                    if (dv.isReleased()) {\n                        DatasetVersion deaccession = datasetVersionService.find(dv.getId());\n                        cmd = new DeaccessionDatasetVersionCommand(dvRequestService.getDataverseRequest(), setDatasetVersionDeaccessionReasonAndURL(deaccession), true);\n                        DatasetVersion datasetv = commandEngine.submit(cmd);\n                    }\n                }\n            } else {\n                for (DatasetVersion dv : selectedDeaccessionVersions) {\n                    DatasetVersion deaccession = datasetVersionService.find(dv.getId());\n                    cmd = new DeaccessionDatasetVersionCommand(dvRequestService.getDataverseRequest(), setDatasetVersionDeaccessionReasonAndURL(deaccession), false);\n                    DatasetVersion datasetv = commandEngine.submit(cmd);\n                }\n            }\n        } catch (CommandException ex) {\n            logger.severe(ex.getMessage());\n            JH.addMessage(FacesMessage.SEVERITY_FATAL, JH.localize(\"dataset.message.deaccessionFailure\"));\n        }\n        JsfHelper.addSuccessMessage(JH.localize(\"datasetVersion.message.deaccessionSuccess\"));\n        return returnToDatasetOnly();\n    }\n\n    private DatasetVersion setDatasetVersionDeaccessionReasonAndURL(DatasetVersion dvIn) {\n        int deaccessionReasonCode = getDeaccessionReasonRadio();\n        String deacessionReasonDetail = getDeaccessionReasonText() != null ? ( getDeaccessionReasonText()).trim() : \"\";\n        switch (deaccessionReasonCode) {\n            case 1:\n                dvIn.setVersionNote(\"There is identifiable data in one or more files.\");\n                break;\n            case 2:\n                dvIn.setVersionNote(\"The research article has been retracted.\");\n                break;\n            case 3:\n                dvIn.setVersionNote(\"The dataset has been transferred to another repository.\");\n                break;\n            case 4:\n                dvIn.setVersionNote(\"IRB request.\");\n                break;\n            case 5:\n                dvIn.setVersionNote(\"Legal issue or Data Usage Agreement.\");\n                break;\n            case 6:\n                dvIn.setVersionNote(\"Not a valid dataset.\");\n                break;\n            case 7:\n                break;\n        }\n        if (!deacessionReasonDetail.isEmpty()){\n            if (!StringUtil.isEmpty(dvIn.getVersionNote())){\n                dvIn.setVersionNote(dvIn.getVersionNote() + \" \" + deacessionReasonDetail);\n            } else {\n                dvIn.setVersionNote(deacessionReasonDetail);\n            }\n        }\n        \n        dvIn.setArchiveNote(getDeaccessionForwardURLFor());\n        return dvIn;\n    }\n    \n    public boolean isMapLayerToBeDeletedOnPublish(){\n\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()){\n             if (worldMapPermissionHelper.hasMapLayerMetadata(fmd)){\n                 if (fmd.isRestricted() || fmd.isRestrictedUI()){\n                        return true;\n                 }\n             }\n        }      \n        return false;\n    }\n\n    private String releaseDataset(boolean minor) {\n        Command<PublishDatasetResult> cmd;\n\n        if (session.getUser() instanceof AuthenticatedUser) {\n            try {\n                if (editMode == EditMode.CREATE) { //FIXME: Why are we using the same commands?\n                    cmd = new PublishDatasetCommand(dataset, dvRequestService.getDataverseRequest(), minor); \n                } else {\n                    cmd = new PublishDatasetCommand(dataset, dvRequestService.getDataverseRequest(), minor); \n                }\n                dataset = commandEngine.submit(cmd).getDataset();\n                // Sucessfully executing PublishDatasetCommand does not guarantee that the dataset \n                // has been published. If a publishing workflow is configured, this may have sent the \n                // dataset into a workflow limbo, potentially waiting for a third party system to complete \n                // the process. So it may be premature to show the \"success\" message at this point. \n                if (dataset.isLockedFor(DatasetLock.Reason.Workflow)) {\n                    JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.locked.message\"), BundleUtil.getStringFromBundle(\"dataset.publish.workflow.inprogress\"));\n                } else {\n                    JsfHelper.addSuccessMessage(BundleUtil.getStringFromBundle(\"dataset.message.publishSuccess\"));\n                }\n            } catch (CommandException ex) {\n                \n                JsfHelper.addErrorMessage(ex.getLocalizedMessage());\n                logger.severe(ex.getMessage());\n            }\n        } else {\n            \n            JsfHelper.addErrorMessage(BundleUtil.getStringFromBundle(\"dataset.message.only.authenticatedUsers\"));\n        }\n        return returnToDatasetOnly();\n    }\n\n    public String registerDataset() {\n        UpdateDatasetCommand cmd;\n        try {\n            cmd = new UpdateDatasetCommand(dataset, dvRequestService.getDataverseRequest());\n            cmd.setValidateLenient(true); \n            dataset = commandEngine.submit(cmd);\n        } catch (CommandException ex) {\n            FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_WARN, \"Dataset Registration Failed\", \" - \" + ex.toString()));\n            logger.severe(ex.getMessage());\n        }\n        FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_INFO, \"DatasetRegistered\", \"Your dataset is now registered.\");\n        FacesContext.getCurrentInstance().addMessage(null, message);\n        return returnToDatasetOnly();\n    }\n\n    public void refresh(ActionEvent e) {\n        refresh();\n    }\n    \n    public void refresh() {\n        logger.fine(\"refreshing\");\n\n        //dataset = datasetService.find(dataset.getId());\n        dataset = null;\n\n        logger.fine(\"refreshing working version\");\n\n        DatasetVersionServiceBean.RetrieveDatasetVersionResponse retrieveDatasetVersionResponse = null;\n\n        if (persistentId != null) {\n            //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionByPersistentId(persistentId, version);\n            dataset = datasetService.findByGlobalId(persistentId);\n            retrieveDatasetVersionResponse = datasetVersionService.selectRequestedVersion(dataset.getVersions(), version);\n        } else if (versionId != null) {\n            retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionByVersionId(versionId);\n        } else if (dataset.getId() != null) {\n            //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionById(dataset.getId(), version);\n            dataset = datasetService.find(dataset.getId());\n            retrieveDatasetVersionResponse = datasetVersionService.selectRequestedVersion(dataset.getVersions(), version);\n        }\n\n        if (retrieveDatasetVersionResponse == null) {\n            // TODO: \n            // should probably redirect to the 404 page, if we can't find \n            // this version anymore. \n            // -- L.A. 4.2.3 \n            return;\n        }\n        this.workingVersion = retrieveDatasetVersionResponse.getDatasetVersion();\n\n        if (this.workingVersion == null) {\n            // TODO: \n            // same as the above\n\n            return;\n        }\n\n\n        if (dataset == null) {\n            // this would be the case if we were retrieving the version by \n            // the versionId, above.\n            this.dataset = this.workingVersion.getDataset();\n        }\n\n        \n\n        if (readOnly) {\n            datafileService.findFileMetadataOptimizedExperimental(dataset);\n        } \n        fileMetadatasSearch = workingVersion.getFileMetadatasSorted();\n\n        displayCitation = dataset.getCitation(true, workingVersion);\n        stateChanged = false;\n    }\n    \n    public String deleteDataset() {\n\n        DestroyDatasetCommand cmd;\n        try {\n            cmd = new DestroyDatasetCommand(dataset, dvRequestService.getDataverseRequest());\n            commandEngine.submit(cmd);\n            /* - need to figure out what to do \n             Update notification in Delete Dataset Method\n             for (UserNotification und : userNotificationService.findByDvObject(dataset.getId())){\n             userNotificationService.delete(und);\n             } */\n        } catch (CommandException ex) {\n            JH.addMessage(FacesMessage.SEVERITY_FATAL, JH.localize(\"dataset.message.deleteFailure\"));\n            logger.severe(ex.getMessage());\n        }\n            JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.deleteSuccess\"));\n        return \"/dataverse.xhtml?alias=\" + dataset.getOwner().getAlias() + \"&faces-redirect=true\";\n    }\n    \n    public String editFileMetadata(){\n        // If there are no files selected, return an empty string - which \n        // means, do nothing, don't redirect anywhere, stay on this page. \n        // The dialogue telling the user to select at least one file will \n        // be shown to them by an onclick javascript method attached to the \n        // filemetadata edit button on the page.\n        // -- L.A. 4.2.1\n        if (this.selectedFiles == null || this.selectedFiles.size() < 1) {\n            return \"\";\n        } \n        return \"/editdatafiles.xhtml?selectedFileIds=\" + getSelectedFilesIdsString() + \"&datasetId=\" + dataset.getId() +\"&faces-redirect=true\";\n    }\n\n    public String deleteDatasetVersion() {\n        DeleteDatasetVersionCommand cmd;\n        try {\n            cmd = new DeleteDatasetVersionCommand(dvRequestService.getDataverseRequest(), dataset);\n            commandEngine.submit(cmd);\n            JsfHelper.addSuccessMessage(JH.localize(\"datasetVersion.message.deleteSuccess\"));\n        } catch (CommandException ex) {\n            JH.addMessage(FacesMessage.SEVERITY_FATAL, JH.localize(\"dataset.message.deleteFailure\"));\n            logger.severe(ex.getMessage());\n        }\n\n        return returnToDatasetOnly();\n    }\n\n    private List<FileMetadata> selectedFiles = new ArrayList<>();\n\n    public List<FileMetadata> getSelectedFiles() {\n        return selectedFiles;\n    }\n\n    public void setSelectedFiles(List<FileMetadata> selectedFiles) {\n        this.selectedFiles = selectedFiles;\n    }\n    \n    private List<FileMetadata> selectedRestrictedFiles; // = new ArrayList<>();\n\n    public List<FileMetadata> getSelectedRestrictedFiles() {\n        return selectedRestrictedFiles;\n    }\n\n    public void setSelectedRestrictedFiles(List<FileMetadata> selectedRestrictedFiles) {\n        this.selectedRestrictedFiles = selectedRestrictedFiles;\n    }\n    \n    private List<FileMetadata> selectedUnrestrictedFiles; // = new ArrayList<>();\n\n    public List<FileMetadata> getSelectedUnrestrictedFiles() {\n        return selectedUnrestrictedFiles;\n    }\n\n    public void setSelectedUnrestrictedFiles(List<FileMetadata> selectedUnrestrictedFiles) {\n        this.selectedUnrestrictedFiles = selectedUnrestrictedFiles;\n    }\n    \n    private List<FileMetadata> selectedDownloadableFiles;\n\n    public List<FileMetadata> getSelectedDownloadableFiles() {\n        return selectedDownloadableFiles;\n    }\n\n    public void setSelectedDownloadableFiles(List<FileMetadata> selectedDownloadableFiles) {\n        this.selectedDownloadableFiles = selectedDownloadableFiles;\n    }\n    \n    private List<FileMetadata> selectedNonDownloadableFiles;\n\n    public List<FileMetadata> getSelectedNonDownloadableFiles() {\n        return selectedNonDownloadableFiles;\n    }\n\n    public void setSelectedNonDownloadableFiles(List<FileMetadata> selectedNonDownloadableFiles) {\n        this.selectedNonDownloadableFiles = selectedNonDownloadableFiles;\n    }\n    \n            \n    public void validateFilesForDownload(boolean guestbookRequired){\n        setSelectedDownloadableFiles(new ArrayList<>());\n        setSelectedNonDownloadableFiles(new ArrayList<>());\n        \n        if (this.selectedFiles.isEmpty()) {\n            RequestContext requestContext = RequestContext.getCurrentInstance();\n            requestContext.execute(\"PF('selectFilesForDownload').show()\");\n            return;\n        }\n\n        List<FileMetadata> allFiles = new ArrayList<>();\n        \n        if (isSelectAllFiles()){\n            for (FileMetadata fm: workingVersion.getFileMetadatas()){\n                allFiles.add(fm);\n            }\n            this.selectedFiles = allFiles;\n        }\n \n        for (FileMetadata fmd : this.selectedFiles){\n            if(this.fileDownloadHelper.canDownloadFile(fmd)){\n                getSelectedDownloadableFiles().add(fmd);\n            } else {\n                getSelectedNonDownloadableFiles().add(fmd);\n            }\n        }\n        \n        if(!getSelectedDownloadableFiles().isEmpty() && getSelectedNonDownloadableFiles().isEmpty()){\n            if (guestbookRequired){\n                modifyGuestbookMultipleResponse();\n            } else{\n                startMultipleFileDownload(false);\n            }        \n        }\n\n        if(getSelectedDownloadableFiles().isEmpty() && !getSelectedNonDownloadableFiles().isEmpty()){\n            RequestContext requestContext = RequestContext.getCurrentInstance();\n            requestContext.execute(\"PF('downloadInvalid').show()\");\n            return;\n        } \n        \n        if(!getSelectedDownloadableFiles().isEmpty() && !getSelectedNonDownloadableFiles().isEmpty()){\n            RequestContext requestContext = RequestContext.getCurrentInstance();\n            requestContext.execute(\"PF('downloadMixed').show()\");\n        }       \n\n    }\n\n    private boolean selectAllFiles;\n\n    public boolean isSelectAllFiles() {\n        return selectAllFiles;\n    }\n\n    public void setSelectAllFiles(boolean selectAllFiles) {\n        this.selectAllFiles = selectAllFiles;\n    }\n\n    public void toggleAllSelected(){\n        //This is here so that if the user selects all on the dataset page\n        // s/he will get all files on download\n        this.selectAllFiles = !this.selectAllFiles;\n    }\n    \n\n    // helper Method\n    public String getSelectedFilesIdsString() {        \n        String downloadIdString = \"\";\n        for (FileMetadata fmd : this.selectedFiles){\n            if (!StringUtil.isEmpty(downloadIdString)) {\n                downloadIdString += \",\";\n            }\n            downloadIdString += fmd.getDataFile().getId();\n        }\n        return downloadIdString;     \n    }\n    \n        // helper Method\n    public String getSelectedDownloadableFilesIdsString() {        \n        String downloadIdString = \"\";\n        for (FileMetadata fmd : this.selectedDownloadableFiles){\n            if (!StringUtil.isEmpty(downloadIdString)) {\n                downloadIdString += \",\";\n            }\n            downloadIdString += fmd.getDataFile().getId();\n        }\n        return downloadIdString;     \n    }\n    \n        // helper Method\n    public String getSelectedFilesIdsStringForDownload() {        \n        String downloadIdString = \"\";\n        for (FileMetadata fmd : this.selectedFiles){\n            if (!StringUtil.isEmpty(downloadIdString)) {\n                downloadIdString += \",\";\n            }\n            downloadIdString += fmd.getDataFile().getId();\n        }\n        return downloadIdString;     \n    }\n    \n    public String getDownloadableFilesIdsString() {        \n        String downloadIdString = \"\";\n        for (FileMetadata fmd : this.selectedDownloadableFiles){\n            if (!StringUtil.isEmpty(downloadIdString)) {\n                downloadIdString += \",\";\n            }\n            downloadIdString += fmd.getDataFile().getId();\n        }\n        return downloadIdString;\n      \n    }\n    \n    public void updateFileCounts(){\n        setSelectedUnrestrictedFiles(new ArrayList<>());\n        setSelectedRestrictedFiles(new ArrayList<>());\n        setTabularDataSelected(false);\n        for (FileMetadata fmd : this.selectedFiles){\n            if(fmd.isRestricted()){\n                getSelectedRestrictedFiles().add(fmd);\n            } else {\n                getSelectedUnrestrictedFiles().add(fmd);\n            }\n            if(fmd.getDataFile().isTabularData()){\n                setTabularDataSelected(true);\n            }\n        }\n    }\n    \n\n        private List<String> getSuccessMessageArguments() {\n        List<String> arguments = new ArrayList<>();\n        arguments.add(StringEscapeUtils.escapeHtml(dataset.getDisplayName()));\n        String linkString = \"<a href=\\\"/dataverse/\" + linkingDataverse.getAlias() + \"\\\">\" + StringEscapeUtils.escapeHtml(linkingDataverse.getDisplayName()) + \"</a>\";\n        arguments.add(linkString);\n        return arguments;\n    }\n    \n    public String saveLinkedDataset() {\n        if (linkingDataverseId == null) {\n            JsfHelper.addFlashMessage(\"You must select a linking dataverse.\");\n            return \"\";\n        }\n        linkingDataverse = dataverseService.find(linkingDataverseId);\n        if (readOnly) {\n            // Pass a \"real\", non-readonly dataset the the LinkDatasetCommand: \n            dataset = datasetService.find(dataset.getId());\n        }\n        LinkDatasetCommand cmd = new LinkDatasetCommand(dvRequestService.getDataverseRequest(), linkingDataverse, dataset);\n        try {\n            commandEngine.submit(cmd);\n            JsfHelper.addSuccessMessage(BundleUtil.getStringFromBundle(\"dataset.message.linkSuccess\", getSuccessMessageArguments()));\n        } catch (CommandException ex) {\n            String msg = \"There was a problem linking this dataset to yours: \" + ex;\n            logger.severe(msg);\n            /**\n             * @todo how do we get this message to show up in the GUI?\n             */\n            FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_INFO, \"DatasetNotLinked\", msg);\n            FacesContext.getCurrentInstance().addMessage(null, message);\n        }\n        return returnToLatestVersion();\n    }\n\n    List<FileMetadata> previouslyRestrictedFiles = null;\n    \n    public boolean isShowAccessPopup() {\n        \n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n\n            if (fmd.isRestricted()) {\n            \n                if (editMode == EditMode.CREATE) {\n                    // if this is a brand new file, it's definitely not \n                    // of a previously restricted kind!\n                    return true; \n                }\n            \n                if (previouslyRestrictedFiles != null) {\n                    // We've already checked whether we are in the CREATE mode, \n                    // above; and that means we can safely assume this filemetadata\n                    // has an existing db id. So it is safe to use the .contains()\n                    // method below:\n                    if (!previouslyRestrictedFiles.contains(fmd)) {\n                        return true;\n                    }\n                }\n            }\n        }\n        \n        return false;\n    }\n    \n    public void setShowAccessPopup(boolean showAccessPopup) {} // dummy set method\n    \n    public String testSelectedFilesForRestrict(){\n        RequestContext requestContext = RequestContext.getCurrentInstance();\n        if (selectedFiles.isEmpty()) {\n                requestContext.execute(\"PF('selectFilesForRestrict').show()\");           \n            return \"\";\n        } else {           \n            boolean validSelection = false;\n            for (FileMetadata fmd : selectedFiles) {\n                if (!fmd.isRestricted() ){\n                    validSelection = true;\n                }\n            }\n            if (!validSelection) {\n                requestContext.execute(\"PF('selectFilesForRestrict').show()\");\n                return \"\";\n            }                       \n            testSelectedFilesForMapData();\n            requestContext.execute(\"PF('accessPopup').show()\");\n            return \"\";\n        }        \n    }\n    \n        \n    public String restrictSelectedFiles(boolean restricted) throws CommandException{\n        \n        RequestContext requestContext = RequestContext.getCurrentInstance();\n        if (selectedFiles.isEmpty()) {\n            if (restricted) {\n                requestContext.execute(\"PF('selectFilesForRestrict').show()\");\n            } else {\n                requestContext.execute(\"PF('selectFilesForUnRestrict').show()\");\n            }\n            return \"\";\n        } else {\n            boolean validSelection = false;\n            for (FileMetadata fmd : selectedFiles) {\n                if ((fmd.isRestricted() && !restricted) || (!fmd.isRestricted() && restricted)) {\n                    validSelection = true;\n                }\n            }\n            if (!validSelection) {\n                if (restricted) {\n                    requestContext.execute(\"PF('selectFilesForRestrict').show()\");\n                }\n                if (!restricted) {\n                    requestContext.execute(\"PF('selectFilesForUnRestrict').show()\");\n                }\n                return \"\";\n            }\n        }\n        \n        if (editMode != EditMode.CREATE) {\n            if (bulkUpdateCheckVersion()) {\n                refreshSelectedFiles();\n            }\n            restrictFiles(restricted);\n        }\n        \n        save();\n        \n        return  returnToDraftVersion();\n    }\n\n    public void restrictFiles(boolean restricted) throws CommandException {\n   \n        //if (previouslyRestrictedFiles == null) {\n        // we don't need to buther with this \"previously restricted\" business \n        // when in Create mode... because all the files are new, so none could \n        // have been restricted previously;\n        // (well, it looks like the code below should never be called in the \n        // CREATE mode in the first place... the edit files fragment uses\n        // its own restrictFiles() method there; also, the fmd.getDataFile().equals(fmw.getDataFile()))\n        // line is not going to work on a new file... so be mindful of all this\n        // when the code between the 2 beans is merged in 4.3.\n        if (editMode != EditMode.CREATE) {\n            previouslyRestrictedFiles = new ArrayList<>();\n            for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n                if (fmd.isRestricted()) {\n                    previouslyRestrictedFiles.add(fmd);\n                }\n            }\n            \n            Command cmd;\n            String fileNames = null;\n            for (FileMetadata fmw : workingVersion.getFileMetadatas()) {\n                for (FileMetadata fmd : this.getSelectedFiles()) {\n                    if (restricted && !fmw.isRestricted()) {\n                    // collect the names of the newly-restrticted files, \n                        // to show in the success message:\n                        // I don't think this does the right thing: \n                        // (adds too many files to the message; good thing this \n                        // message isn't used, normally)\n                        if (fileNames == null) {\n                            fileNames = fmd.getLabel();\n                        } else {\n                            fileNames = fileNames.concat(fmd.getLabel());\n                        }\n                    }\n                    if (fmd.getDataFile().equals(fmw.getDataFile())) {\n                        cmd = new RestrictFileCommand(fmw.getDataFile(), dvRequestService.getDataverseRequest(), restricted);\n                        commandEngine.submit(cmd);\n                        \n                        \n//                        fmw.setRestricted(restricted);\n//                        if (workingVersion.isDraft() && !fmw.getDataFile().isReleased()) {\n//                            // We do not really need to check that the working version is \n//                            // a draft here - it must be a draft, if we've gotten this\n//                            // far. But just in case. -- L.A. 4.2.1\n//                            fmw.getDataFile().setRestricted(restricted);\n//                        }\n                    }\n                }\n            }\n            if (fileNames != null) {\n                String successMessage = JH.localize(\"file.restricted.success\");\n                logger.fine(successMessage);\n                successMessage = successMessage.replace(\"{0}\", fileNames);\n                JsfHelper.addFlashMessage(successMessage);\n            }\n        }\n    }\n\n    public int getRestrictedFileCount() {\n        int restrictedFileCount = 0;\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (fmd.isRestricted()) {\n                restrictedFileCount++;\n            }\n        }\n\n        return restrictedFileCount;\n    }\n\n    private List<FileMetadata> filesToBeDeleted = new ArrayList<>();\n    \n    public String  deleteFilesAndSave(){\n        bulkFileDeleteInProgress = true;\n        if (bulkUpdateCheckVersion()){\n           refreshSelectedFiles(); \n        }\n        deleteFiles();\n        return save();       \n    }\n    \n    public void deleteFiles() {\n\n        for (FileMetadata markedForDelete : selectedFiles) {\n            \n            if (markedForDelete.getId() != null) {\n                // This FileMetadata has an id, i.e., it exists in the database. \n                // We are going to remove this filemetadata from the version: \n                dataset.getEditVersion().getFileMetadatas().remove(markedForDelete);\n                // But the actual delete will be handled inside the UpdateDatasetCommand\n                // (called later on). The list \"filesToBeDeleted\" is passed to the \n                // command as a parameter:\n                filesToBeDeleted.add(markedForDelete);\n            } else {\n                // This FileMetadata does not have an id, meaning it has just been \n                // created, and not yet saved in the database. This in turn means this is \n                // a freshly created DRAFT version; specifically created because \n                // the user is trying to delete a file from an existing published \n                // version. This means we are not really *deleting* the file - \n                // we are going to keep it in the published version; we are simply \n                // going to save a new DRAFT version that does not contain this file. \n                // So below we are deleting the metadata from the version; we are \n                // NOT adding the file to the filesToBeDeleted list that will be \n                // passed to the UpdateDatasetCommand. -- L.A. Aug 2017\n                \n                Iterator<FileMetadata> fmit = dataset.getEditVersion().getFileMetadatas().iterator();\n                while (fmit.hasNext()) {\n                    FileMetadata fmd = fmit.next();\n                    if (markedForDelete.getDataFile().getStorageIdentifier().equals(fmd.getDataFile().getStorageIdentifier())) {\n                        // And if this is an image file that happens to be assigned \n                        // as the dataset thumbnail, let's null the assignment here:\n                        \n                        if (fmd.getDataFile().equals(dataset.getThumbnailFile())) {\n                            dataset.setThumbnailFile(null);\n                        }\n                        fmit.remove();\n                        break;\n                    }\n                }\n            }\n        }\n\n        /* \n           Do note that if we are deleting any files that have UNFs (i.e., \n           tabular files), we DO NEED TO RECALCULATE the UNF of the version!\n           - but we will do this inside the UpdateDatasetCommand.\n        */\n    }\n\n    public String save() {\n        // Validate\n        Set<ConstraintViolation> constraintViolations = workingVersion.validate();\n        if (!constraintViolations.isEmpty()) {\n             //JsfHelper.addFlashMessage(JH.localize(\"dataset.message.validationError\"));\n             JH.addMessage(FacesMessage.SEVERITY_ERROR, JH.localize(\"dataset.message.validationError\"));\n            //FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_ERROR, \"Validation Error\", \"See below for details.\"));\n            return \"\";\n        }\n\n        // Use the API to save the dataset: \n        Command<Dataset> cmd;\n        try {\n            if (editMode == EditMode.CREATE) {\n                if ( selectedTemplate != null ) {\n                    if ( isSessionUserAuthenticated() ) {\n                        cmd = new CreateDatasetCommand(dataset, dvRequestService.getDataverseRequest(), false, null, selectedTemplate); \n                    } else {\n                        JH.addMessage(FacesMessage.SEVERITY_FATAL, JH.localize(\"dataset.create.authenticatedUsersOnly\"));\n                        return null;\n                    }\n                } else {\n                   cmd = new CreateDatasetCommand(dataset, dvRequestService.getDataverseRequest());\n                }\n                \n            } else {\n                cmd = new UpdateDatasetCommand(dataset, dvRequestService.getDataverseRequest(), filesToBeDeleted);\n                ((UpdateDatasetCommand) cmd).setValidateLenient(true);  \n            }\n            dataset = commandEngine.submit(cmd);\n            if (editMode == EditMode.CREATE) {\n                if (session.getUser() instanceof AuthenticatedUser) {\n                    userNotificationService.sendNotification((AuthenticatedUser) session.getUser(), dataset.getCreateDate(), UserNotification.Type.CREATEDS, dataset.getLatestVersion().getId());\n                }\n            }\n            logger.fine(\"Successfully executed SaveDatasetCommand.\");\n        } catch (EJBException ex) {\n            StringBuilder error = new StringBuilder();\n            error.append(ex).append(\" \");\n            error.append(ex.getMessage()).append(\" \");\n            Throwable cause = ex;\n            while (cause.getCause()!= null) {\n                cause = cause.getCause();\n                error.append(cause).append(\" \");\n                error.append(cause.getMessage()).append(\" \");\n            }\n            logger.log(Level.FINE, \"Couldn''t save dataset: {0}\", error.toString());\n            populateDatasetUpdateFailureMessage();\n            return returnToDraftVersion();\n        } catch (CommandException ex) {\n            //FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_ERROR, \"Dataset Save Failed\", \" - \" + ex.toString()));\n            logger.severe(\"CommandException, when attempting to update the dataset: \" + ex.getMessage());\n            populateDatasetUpdateFailureMessage();\n            return returnToDraftVersion();\n        }\n        \n        newFiles.clear();\n        if (editMode != null) {\n            if (editMode.equals(EditMode.CREATE)) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.createSuccess\"));\n            }\n            if (editMode.equals(EditMode.METADATA)) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.metadataSuccess\"));\n            }\n            if (editMode.equals(EditMode.LICENSE)) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.termsSuccess\"));\n            }\n            if (editMode.equals(EditMode.FILE)) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.filesSuccess\"));\n            }\n\n        } else {\n            // must have been a bulk file update or delete:\n            if (bulkFileDeleteInProgress) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.bulkFileDeleteSuccess\"));\n            } else {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.bulkFileUpdateSuccess\"));\n            }\n        }\n\n        editMode = null;\n        bulkFileDeleteInProgress = false;\n\n        // Call Ingest Service one more time, to \n        // queue the data ingest jobs for asynchronous execution: \n        ingestService.startIngestJobs(dataset, (AuthenticatedUser) session.getUser());\n\n        logger.fine(\"Redirecting to the Dataset page.\");\n        \n        return returnToDraftVersion();\n    }\n    \n    private void populateDatasetUpdateFailureMessage(){\n        if (editMode == null) {\n            // that must have been a bulk file update or delete:\n            if (bulkFileDeleteInProgress) {\n                JsfHelper.addSuccessMessage(JH.localize(\"dataset.message.bulkFileDeleteFailure\"));\n\n            } else {\n                JsfHelper.addErrorMessage(JH.localize(\"dataset.message.filesFailure\"));\n            }\n        } else {\n\n            if (editMode.equals(EditMode.CREATE)) {\n                JsfHelper.addErrorMessage(JH.localize(\"dataset.message.createFailure\"));\n            }\n            if (editMode.equals(EditMode.METADATA)) {\n                JsfHelper.addErrorMessage(JH.localize(\"dataset.message.metadataFailure\"));\n            }\n            if (editMode.equals(EditMode.LICENSE)) {\n                JsfHelper.addErrorMessage(JH.localize(\"dataset.message.termsFailure\"));\n            }\n            if (editMode.equals(EditMode.FILE)) {\n                JsfHelper.addErrorMessage(JH.localize(\"dataset.message.filesFailure\"));\n            }\n        }\n        \n        bulkFileDeleteInProgress = false;\n    }\n    \n    private String returnToLatestVersion(){\n         dataset = datasetService.find(dataset.getId());\n         workingVersion = dataset.getLatestVersion();\n         if (workingVersion.isDeaccessioned() && dataset.getReleasedVersion() != null) {\n         workingVersion = dataset.getReleasedVersion();\n         }\n         setVersionTabList(resetVersionTabList()); \n         setReleasedVersionTabList(resetReleasedVersionTabList());\n         newFiles.clear();\n         editMode = null;         \n         return \"/dataset.xhtml?persistentId=\" + dataset.getGlobalId() + \"&version=\"+ workingVersion.getFriendlyVersionNumber() +  \"&faces-redirect=true\";       \n    }\n    \n    private String returnToDatasetOnly(){\n         dataset = datasetService.find(dataset.getId());\n         editMode = null;         \n         return \"/dataset.xhtml?persistentId=\" + dataset.getGlobalId()  +  \"&faces-redirect=true\";       \n    }\n    \n    private String returnToDraftVersion(){      \n         return \"/dataset.xhtml?persistentId=\" + dataset.getGlobalId() + \"&version=DRAFT\" + \"&faces-redirect=true\";    \n    }\n\n    public String cancel() {\n        return  returnToLatestVersion();\n    }\n\n    private HttpClient getClient() {\n        // TODO: \n        // cache the http client? -- L.A. 4.0 alpha\n        return new HttpClient();\n    }\n\n    public void refreshLock() {\n        //RequestContext requestContext = RequestContext.getCurrentInstance();\n        logger.fine(\"checking lock\");\n        if (isStillLocked()) {\n            logger.fine(\"(still locked)\");\n        } else {\n            // OK, the dataset is no longer locked. \n            // let's tell the page to refresh:\n            logger.fine(\"no longer locked!\");\n            stateChanged = true;\n            //requestContext.execute(\"refreshPage();\");\n        }\n    }\n    \n        public void refreshIngestLock() {\n        //RequestContext requestContext = RequestContext.getCurrentInstance();\n        logger.fine(\"checking ingest lock\");\n        if (isStillLockedForIngest()) {\n            logger.fine(\"(still locked)\");\n        } else {\n            // OK, the dataset is no longer locked. \n            // let's tell the page to refresh:\n            logger.fine(\"no longer locked!\");\n            stateChanged = true;\n            //requestContext.execute(\"refreshPage();\");\n        }\n    }\n\n    /* \n\n    public boolean isLockedInProgress() {\n        if (dataset != null) {\n            logger.log(Level.FINE, \"checking lock status of dataset {0}\", dataset.getId());\n            if (dataset.isLocked()) {\n                return true;\n            }\n        }\n        return false;\n    }*/\n    \n    public boolean isDatasetLockedInWorkflow() {\n        return (dataset != null) \n                ? dataset.isLockedFor(DatasetLock.Reason.Workflow) \n                : false;\n    }\n    \n    public boolean isStillLocked() {\n        \n        if (dataset != null && dataset.getId() != null) {\n            logger.log(Level.FINE, \"checking lock status of dataset {0}\", dataset.getId());\n            if(dataset.getLocks().size() == 1 && dataset.getLockFor(DatasetLock.Reason.InReview) != null){\n                return false;\n            }\n            if (datasetService.checkDatasetLock(dataset.getId())) {\n                return true;\n            }\n        }\n        return false;\n    }\n    \n    \n    public boolean isStillLockedForIngest() {\n        if (dataset.getId() != null) {\n            Dataset testDataset = datasetService.find(dataset.getId());\n            if (testDataset != null && testDataset.getId() != null) {\n                logger.log(Level.FINE, \"checking lock status of dataset {0}\", dataset.getId());\n\n                if (testDataset.getLockFor(DatasetLock.Reason.Ingest) != null) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n    \n    public boolean isLocked() {\n        if (stateChanged) {\n            return false; \n        }\n        \n        if (dataset != null) {\n            if (dataset.isLocked()) {\n                return true;\n            }\n        }\n        return false;\n    }\n    \n    public boolean isLockedForIngest() {\n        if (dataset.getId() != null) {\n            Dataset testDataset = datasetService.find(dataset.getId());\n            if (stateChanged) {\n                return false;\n            }\n\n            if (testDataset != null) {\n                if (testDataset.getLockFor(DatasetLock.Reason.Ingest) != null) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    private Boolean lockedFromEditsVar;\n    private Boolean lockedFromDownloadVar;    \n    /**\n     * Authors are not allowed to edit but curators are allowed - when Dataset is inReview\n     * For all other locks edit should be locked for all editors.\n     */\n    public boolean isLockedFromEdits() {\n        if(null == lockedFromEditsVar) {\n            try {\n                permissionService.checkEditDatasetLock(dataset, dvRequestService.getDataverseRequest(), new UpdateDatasetCommand(dataset, dvRequestService.getDataverseRequest()));\n                lockedFromEditsVar = false;\n            } catch (IllegalCommandException ex) {\n                lockedFromEditsVar = true;\n            }\n        }\n        return lockedFromEditsVar;\n    }\n    \n    public boolean isLockedFromDownload(){\n        if(null == lockedFromDownloadVar) {\n            try {\n                permissionService.checkDownloadFileLock(dataset, dvRequestService.getDataverseRequest(), new CreateDatasetCommand(dataset, dvRequestService.getDataverseRequest()));\n                lockedFromDownloadVar = false;\n            } catch (IllegalCommandException ex) {\n                lockedFromDownloadVar = true;\n                return true;\n            }\n        }\n        return lockedFromDownloadVar;\n    }\n\n    public void setLocked(boolean locked) {\n        // empty method, so that we can use DatasetPage.locked in a hidden \n        // input on the page. \n    }\n    \n    public void setLockedForIngest(boolean locked) {\n        // empty method, so that we can use DatasetPage.locked in a hidden \n        // input on the page. \n    }\n    \n    public boolean isStateChanged() {\n        return stateChanged;\n    }\n    \n    public void setStateChanged(boolean stateChanged) {\n        // empty method, so that we can use DatasetPage.stateChanged in a hidden \n        // input on the page. \n    }\n\n    public DatasetVersionUI getDatasetVersionUI() {\n        return datasetVersionUI;\n    }\n\n    \n    public List<DatasetVersion> getVersionTabList() {\n        return versionTabList;\n    }\n    \n    public List<DatasetVersion> getVersionTabListForPostLoad(){\n        return this.versionTabListForPostLoad;\n    }\n\n    public void setVersionTabListForPostLoad(List<DatasetVersion> versionTabListForPostLoad) {\n\n        this.versionTabListForPostLoad = versionTabListForPostLoad;\n    }\n    \n    public Integer getCompareVersionsCount() {\n        Integer retVal = 0;\n        for (DatasetVersion dvTest : dataset.getVersions()) {\n            if (!dvTest.isDeaccessioned()) {\n                retVal++;\n            }\n        }\n        return retVal;\n    }\n    \n    /**\n     * To improve performance, Version Differences\n     * are retrieved/calculated after the page load\n     * \n     * See: dataset-versions.xhtml, remoteCommand 'postLoadVersionTablList'\n    */\n    public void postLoadSetVersionTabList(){\n\n        if (this.getVersionTabList().isEmpty() && workingVersion.isDeaccessioned()){\n            setVersionTabList(resetVersionTabList());\n        }\n        \n        this.setVersionTabListForPostLoad(this.getVersionTabList());\n        \n\n        //this.versionTabList = this.resetVersionTabList();        \n    }\n\n    /**\n     * \n     * \n     * @param versionTabList \n     */\n    public void setVersionTabList(List<DatasetVersion> versionTabList) {\n        \n        this.versionTabList = versionTabList;\n    }\n\n    private List<DatasetVersion> releasedVersionTabList = new ArrayList<>();\n\n    public List<DatasetVersion> getReleasedVersionTabList() {\n        return releasedVersionTabList;\n    }\n\n    public void setReleasedVersionTabList(List<DatasetVersion> releasedVersionTabList) {\n        this.releasedVersionTabList = releasedVersionTabList;\n    }\n\n    private List<DatasetVersion> selectedVersions;\n\n    public List<DatasetVersion> getSelectedVersions() {\n        return selectedVersions;\n    }\n\n    public void setSelectedVersions(List<DatasetVersion> selectedVersions) {\n        this.selectedVersions = selectedVersions;\n    }\n\n    private List<DatasetVersion> selectedDeaccessionVersions;\n\n    public List<DatasetVersion> getSelectedDeaccessionVersions() {\n        return selectedDeaccessionVersions;\n    }\n\n    public void setSelectedDeaccessionVersions(List<DatasetVersion> selectedDeaccessionVersions) {\n        this.selectedDeaccessionVersions = selectedDeaccessionVersions;\n    }\n\n    public DatasetVersionDifference getDatasetVersionDifference() {\n        return datasetVersionDifference;\n    }\n\n    public void setDatasetVersionDifference(DatasetVersionDifference datasetVersionDifference) {\n        this.datasetVersionDifference = datasetVersionDifference;\n    }\n        \n    public void startMultipleFileDownload (Boolean writeGuestbook){\n\n        fileDownloadService.callDownloadServlet(getDownloadableFilesIdsString(), writeGuestbook);\n\n    }\n \n    private String downloadType = \"\";\n\n    public String getDownloadType() {\n        return downloadType;\n    }\n\n    public void setDownloadType(String downloadType) {\n        this.downloadType = downloadType;\n    }\n    \n    \n    public void modifyGuestbookMultipleResponse(){\n        if (this.selectedFiles.isEmpty()) {\n            RequestContext requestContext = RequestContext.getCurrentInstance();\n            requestContext.execute(\"PF('selectFilesForDownload').show()\");\n            return;\n        }\n        \n         this.guestbookResponse = this.guestbookResponseService.modifySelectedFileIds(guestbookResponse, getSelectedDownloadableFilesIdsString());\n         this.guestbookResponse.setDownloadtype(\"Download\");\n         this.guestbookResponse.setFileFormat(\"Download\");\n        RequestContext requestContext = RequestContext.getCurrentInstance();\n        requestContext.execute(\"PF('downloadPopup').show();handleResizeDialog('downloadPopup');\");\n    }\n    \n    public void initGuestbookMultipleResponse(String selectedFileIds){\n         initGuestbookResponse(null, \"download\", selectedFileIds);\n    }\n\n    public void initGuestbookResponse(FileMetadata fileMetadata, String downloadFormat, String selectedFileIds) {\n        \n        this.guestbookResponse = guestbookResponseService.initGuestbookResponse(fileMetadata, downloadFormat, selectedFileIds, session);\n    }\n\n\n\n    public void compareVersionDifferences() {\n        RequestContext requestContext = RequestContext.getCurrentInstance();\n        if (this.selectedVersions.size() != 2) {\n            requestContext.execute(\"openCompareTwo();\");\n        } else {\n            //order depends on order of selection - needs to be chronological order\n            if (this.selectedVersions.get(0).getId().intValue() > this.selectedVersions.get(1).getId().intValue()) {\n                updateVersionDifferences(this.selectedVersions.get(0), this.selectedVersions.get(1));\n            } else {\n                updateVersionDifferences(this.selectedVersions.get(1), this.selectedVersions.get(0));\n            }\n        }\n    }\n\n    public void updateVersionDifferences(DatasetVersion newVersion, DatasetVersion originalVersion) {\n        if (originalVersion == null) {\n            setDatasetVersionDifference(newVersion.getDefaultVersionDifference());\n        } else {\n            setDatasetVersionDifference(new DatasetVersionDifference(newVersion, originalVersion));\n        }\n    }\n    \n    \n    \n\n    private List<DatasetVersion> resetVersionTabList() {\n        //if (true)return null;\n        List<DatasetVersion> retList = new ArrayList<>();\n\n        if (permissionService.on(dataset).has(Permission.ViewUnpublishedDataset)) {\n            for (DatasetVersion version : dataset.getVersions()) {\n                version.setContributorNames(datasetVersionService.getContributorsNames(version));\n                retList.add(version);\n            }\n\n        } else {\n            for (DatasetVersion version : dataset.getVersions()) {\n                if (version.isReleased() || version.isDeaccessioned()) {\n                    version.setContributorNames(datasetVersionService.getContributorsNames(version));\n                    retList.add(version);\n                }\n            }\n        }\n        return retList;\n    }\n\n\n    \n    private boolean existReleasedVersion;\n\n    public boolean isExistReleasedVersion() {\n        return existReleasedVersion;\n    }\n\n    public void setExistReleasedVersion(boolean existReleasedVersion) {\n        this.existReleasedVersion = existReleasedVersion;\n    }\n    \n    private boolean resetExistRealeaseVersion(){\n\n        for (DatasetVersion version : dataset.getVersions()) {\n            if (version.isReleased() || version.isArchived()) {\n                return true;\n            }\n        }\n        return false;\n        \n    }\n\n    private List<DatasetVersion> resetReleasedVersionTabList() {\n        List<DatasetVersion> retList = new ArrayList<>();\n        for (DatasetVersion version : dataset.getVersions()) {\n            if (version.isReleased() || version.isArchived()) {\n                retList.add(version);\n            }\n        }\n        return retList;\n    }\n\n    public String getDatasetPublishCustomText(){\n        String datasetPublishCustomText = settingsWrapper.getValueForKey(SettingsServiceBean.Key.DatasetPublishPopupCustomText);\n        if( datasetPublishCustomText!= null && !datasetPublishCustomText.isEmpty()){\n            return datasetPublishCustomText;\n            \n        }\n        return \"\";\n    }\n    \n    public Boolean isDatasetPublishPopupCustomTextOnAllVersions(){\n        return  settingsWrapper.isTrueForKey(SettingsServiceBean.Key.DatasetPublishPopupCustomTextOnAllVersions, false);\n    }\n\n    public String getVariableMetadataURL(Long fileid) {\n        String myHostURL = getDataverseSiteUrl();\n        String metaURL = myHostURL + \"/api/meta/datafile/\" + fileid;\n\n        return metaURL;\n    }\n\n    public String getTabularDataFileURL(Long fileid) {\n        String myHostURL = getDataverseSiteUrl();\n        String dataURL = myHostURL + \"/api/access/datafile/\" + fileid;\n\n        return dataURL;\n    }\n\n    public List< String[]> getExporters(){\n        List<String[]> retList = new ArrayList<>();\n        String myHostURL = getDataverseSiteUrl();\n        for (String [] provider : ExportService.getInstance(settingsService).getExportersLabels() ){\n            String formatName = provider[1];\n            String formatDisplayName = provider[0];\n            \n            Exporter exporter = null; \n            try {\n                exporter = ExportService.getInstance(settingsService).getExporter(formatName);\n            } catch (ExportException ex) {\n                exporter = null;\n            }\n            if (exporter != null && exporter.isAvailableToUsers()) {\n                // Not all metadata exports should be presented to the web users!\n                // Some are only for harvesting clients.\n                \n                String[] temp = new String[2];            \n                temp[0] = formatDisplayName;\n                temp[1] = myHostURL + \"/api/datasets/export?exporter=\" + formatName + \"&persistentId=\" + dataset.getGlobalId();\n                retList.add(temp);\n            }\n        }\n        return retList;  \n    }\n    \n\n    private FileMetadata fileMetadataSelected = null;\n\n    public void  setFileMetadataSelected(FileMetadata fm){\n       setFileMetadataSelected(fm, null); \n    }\n    \n    public void setFileMetadataSelected(FileMetadata fm, String guestbook) {\n        if (guestbook != null) {\n            if (guestbook.equals(\"create\")) {\n                //\n                /*\n                FIX ME guestbook entry for subsetting\n                */\n                \n                \n                \n                \n               // guestbookResponseService.createSilentGuestbookEntry(fm, \"Subset\");\n            } else {\n                initGuestbookResponse(fm, \"Subset\", null);\n            }\n        }\n\n        fileMetadataSelected = fm;\n        logger.fine(\"set the file for the advanced options popup (\" + fileMetadataSelected.getLabel() + \")\");\n    }\n\n    public FileMetadata getFileMetadataSelected() {\n        if (fileMetadataSelected != null) {\n            logger.fine(\"returning file metadata for the advanced options popup (\" + fileMetadataSelected.getLabel() + \")\");\n        } else {\n            logger.fine(\"file metadata for the advanced options popup is null.\");\n        }\n        return fileMetadataSelected;\n    }\n\n    public void clearFileMetadataSelected() {\n        fileMetadataSelected = null;\n    }\n    \n    public boolean isDesignatedDatasetThumbnail (FileMetadata fileMetadata) {\n        if (fileMetadata != null) {\n            if (fileMetadata.getDataFile() != null) {\n                if (fileMetadata.getDataFile().getId() != null) {\n                    if (fileMetadata.getDataFile().getOwner() != null) {\n                        if (fileMetadata.getDataFile().equals(fileMetadata.getDataFile().getOwner().getThumbnailFile())) {\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n        return false;\n    }\n    \n    /* \n     * Items for the \"Designated this image as the Dataset thumbnail: \n     */\n    \n    private FileMetadata fileMetadataSelectedForThumbnailPopup = null; \n\n    public void  setFileMetadataSelectedForThumbnailPopup(FileMetadata fm){\n       fileMetadataSelectedForThumbnailPopup = fm; \n       alreadyDesignatedAsDatasetThumbnail = getUseAsDatasetThumbnail();\n\n    }\n    \n    public FileMetadata getFileMetadataSelectedForThumbnailPopup() {\n        return fileMetadataSelectedForThumbnailPopup;\n    }\n    \n    public void clearFileMetadataSelectedForThumbnailPopup() {\n        fileMetadataSelectedForThumbnailPopup = null;\n    }\n    \n    private boolean alreadyDesignatedAsDatasetThumbnail = false; \n    \n    public boolean getUseAsDatasetThumbnail() {\n\n        if (fileMetadataSelectedForThumbnailPopup != null) {\n            if (fileMetadataSelectedForThumbnailPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForThumbnailPopup.getDataFile().getId() != null) {\n                    if (fileMetadataSelectedForThumbnailPopup.getDataFile().getOwner() != null) {\n                        if (fileMetadataSelectedForThumbnailPopup.getDataFile().equals(fileMetadataSelectedForThumbnailPopup.getDataFile().getOwner().getThumbnailFile())) {\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n        return false;\n    }\n\n    \n    \n    public void setUseAsDatasetThumbnail(boolean useAsThumbnail) {\n        if (fileMetadataSelectedForThumbnailPopup != null) {\n            if (fileMetadataSelectedForThumbnailPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForThumbnailPopup.getDataFile().getId() != null) { // ?\n                    if (fileMetadataSelectedForThumbnailPopup.getDataFile().getOwner() != null) {\n                        if (useAsThumbnail) {\n                            fileMetadataSelectedForThumbnailPopup.getDataFile().getOwner().setThumbnailFile(fileMetadataSelectedForThumbnailPopup.getDataFile());\n                        } else if (getUseAsDatasetThumbnail()) {\n                            fileMetadataSelectedForThumbnailPopup.getDataFile().getOwner().setThumbnailFile(null);\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    public void saveAsDesignatedThumbnail() {\n        // We don't need to do anything specific to save this setting, because\n        // the setUseAsDatasetThumbnail() method, above, has already updated the\n        // file object appropriately. \n        // However, once the \"save\" button is pressed, we want to show a success message, if this is \n        // a new image has been designated as such:\n        if (getUseAsDatasetThumbnail() && !alreadyDesignatedAsDatasetThumbnail) {\n            String successMessage = JH.localize(\"file.assignedDataverseImage.success\");\n            logger.fine(successMessage);\n            successMessage = successMessage.replace(\"{0}\", fileMetadataSelectedForThumbnailPopup.getLabel());\n            JsfHelper.addFlashMessage(successMessage);\n        }\n        \n        // And reset the selected fileMetadata:\n        \n        fileMetadataSelectedForThumbnailPopup = null;\n    }\n    \n    /* \n     * Items for the \"Tags (Categories)\" popup.\n     *\n     */\n    private FileMetadata fileMetadataSelectedForTagsPopup = null; \n    \n    public void  setFileMetadataSelectedForTagsPopup(){\n\n    }\n\n    public void  setFileMetadataSelectedForTagsPopup(FileMetadata fm){\n       fileMetadataSelectedForTagsPopup = fm; \n    }\n    \n    public FileMetadata getFileMetadataSelectedForTagsPopup() {\n        return fileMetadataSelectedForTagsPopup;\n    }\n    \n    public void clearFileMetadataSelectedForTagsPopup() {\n        fileMetadataSelectedForTagsPopup = null;\n    }\n    \n    public List <FileMetadata> getListFileMetadataSelectedForTagsPopup(){\n        List<FileMetadata> retList = new ArrayList<>();\n        for (FileMetadata fm : selectedFiles){\n            retList.add(fm);\n        }\n        return retList;       \n    }\n    \n    private List<String> categoriesByName;\n    \n    public void setCategoriesByName(List<String>  dummy){\n        categoriesByName = dummy;\n    }\n    \n    public void refreshTagsPopUp(){\n        if (bulkUpdateCheckVersion()){\n           refreshSelectedFiles();           \n        }  \n        updateFileCounts();\n        refreshCategoriesByName();\n        refreshTabFileTagsByName();\n    }\n    \n    private List<String> tabFileTagsByName;\n\n    public List<String> getTabFileTagsByName() {\n        return tabFileTagsByName;\n    }\n\n    public void setTabFileTagsByName(List<String> tabFileTagsByName) {\n        this.tabFileTagsByName = tabFileTagsByName;\n    }\n    \n    private void refreshCategoriesByName(){\n        categoriesByName= new ArrayList<>();\n        for (String category: dataset.getCategoriesByName() ){\n            categoriesByName.add(category);\n        }\n        refreshSelectedTags();\n    }\n    \n    \n    \n    \n    public List<String> getCategoriesByName() {\n            return categoriesByName;\n    }\n    \n    /*\n     * 1. Tabular File Tags: \n     */\n    \n    private List<String> tabFileTags = null;\n\n    public List<String> getTabFileTags() {\n        if (tabFileTags == null) {\n            tabFileTags = DataFileTag.listTags();\n        }\n        return tabFileTags;\n    }\n\n    public void setTabFileTags(List<String> tabFileTags) {\n        this.tabFileTags = tabFileTags;\n    }\n    \n    private String[] selectedTabFileTags = {};\n\n    public String[] getSelectedTabFileTags() {\n        return selectedTabFileTags;\n    }\n\n    public void setSelectedTabFileTags(String[] selectedTabFileTags) {\n        this.selectedTabFileTags = selectedTabFileTags;\n    }\n\n    private String[] selectedTags = {};\n    \n    public void handleSelection(final AjaxBehaviorEvent event) {\n        if (selectedTags != null) {\n            selectedTags = selectedTags.clone();\n        }\n    }\n        \n\n    \n    private void refreshTabFileTagsByName(){\n        \n        tabFileTagsByName= new ArrayList<>();\n        for (FileMetadata fm : selectedFiles) {\n            if (fm.getDataFile().getTags() != null) {\n                for (int i = 0; i < fm.getDataFile().getTags().size(); i++) {\n                    if (!tabFileTagsByName.contains(fm.getDataFile().getTags().get(i).getTypeLabel())) {\n                        tabFileTagsByName.add(fm.getDataFile().getTags().get(i).getTypeLabel());\n                    }\n                }\n            }\n        }\n        refreshSelectedTabFileTags();\n    }\n    \n    private void refreshSelectedTabFileTags() {\n        selectedTabFileTags = null;\n        selectedTabFileTags = new String[0];\n        if (tabFileTagsByName.size() > 0) {\n            selectedTabFileTags = new String[tabFileTagsByName.size()];\n            for (int i = 0; i < tabFileTagsByName.size(); i++) {\n                selectedTabFileTags[i] = tabFileTagsByName.get(i);\n            }\n        }\n        Arrays.sort(selectedTabFileTags);\n    }\n    \n    private boolean tabularDataSelected = false;\n\n    public boolean isTabularDataSelected() {\n        return tabularDataSelected;\n    }\n\n    public void setTabularDataSelected(boolean tabularDataSelected) {\n        this.tabularDataSelected = tabularDataSelected;\n    }\n\n    public String[] getSelectedTags() {    \n\n        return selectedTags;\n    }\n\n    public void setSelectedTags(String[] selectedTags) {\n        this.selectedTags = selectedTags;\n    }\n\n    /*\n     * \"File Tags\" (aka \"File Categories\"): \n    */\n    \n    private String newCategoryName = null;\n\n    public String getNewCategoryName() {\n        return newCategoryName;\n    }\n\n    public void setNewCategoryName(String newCategoryName) {\n        this.newCategoryName = newCategoryName;\n    }\n\n    public String saveNewCategory() {\n        if (newCategoryName != null && !newCategoryName.isEmpty()) {\n            categoriesByName.add(newCategoryName);\n        }\n        //Now increase size of selectedTags and add new category\n        String[] temp = new String[selectedTags.length + 1];\n        System.arraycopy(selectedTags, 0, temp, 0, selectedTags.length);\n        selectedTags = temp;\n        selectedTags[selectedTags.length - 1] = newCategoryName;\n        //Blank out added category\n        newCategoryName = \"\";\n        return \"\";\n    }\n    \n    private void refreshSelectedTags() {\n        selectedTags = null;\n        selectedTags = new String[0];\n        \n        List<String> selectedCategoriesByName= new ArrayList<>();\n        for (FileMetadata fm : selectedFiles) {\n            if (fm.getCategories() != null) {\n                for (int i = 0; i < fm.getCategories().size(); i++) {\n                    if (!selectedCategoriesByName.contains(fm.getCategories().get(i).getName())) {\n                    selectedCategoriesByName.add(fm.getCategories().get(i).getName());\n                    }\n\n                }\n\n            }\n        }\n\n        if (selectedCategoriesByName.size() > 0) {\n            selectedTags = new String[selectedCategoriesByName.size()];\n            for (int i = 0; i < selectedCategoriesByName.size(); i++) {\n                selectedTags[i] = selectedCategoriesByName.get(i);\n            }\n        }\n        Arrays.sort(selectedTags);\n    }\n        \n    /* This method handles saving both \"tabular file tags\" and \n     * \"file categories\" (which are also considered \"tags\" in 4.0)\n    */\n    public String saveFileTagsAndCategories() {\n        // 1. New Category name:\n        // With we don't need to do anything for the file categories that the user\n        // selected from the pull down list; that was done directly from the \n        // page with the FileMetadata.setCategoriesByName() method. \n        // So here we only need to take care of the new, custom category\n        // name, if entered: \n        if (bulkUpdateCheckVersion()) {\n            refreshSelectedFiles();\n        }\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (selectedFiles != null && selectedFiles.size() > 0) {\n                for (FileMetadata fm : selectedFiles) {\n                    if (fm.getDataFile().equals(fmd.getDataFile())) {\n                        fmd.setCategories(new ArrayList<>());\n                        if (newCategoryName != null) {\n                            fmd.addCategoryByName(newCategoryName);\n                        }\n                        // 2. Tabular DataFile Tags: \n                        if (selectedTags != null) {\n                            for (String selectedTag : selectedTags) {\n                                fmd.addCategoryByName(selectedTag);\n                            }\n                        }\n                        if (fmd.getDataFile().isTabularData()) {\n                            fmd.getDataFile().setTags(null);\n                            for (String selectedTabFileTag : selectedTabFileTags) {\n                                DataFileTag tag = new DataFileTag();\n                                try {\n                                    tag.setTypeByLabel(selectedTabFileTag);\n                                    tag.setDataFile(fmd.getDataFile());\n                                    fmd.getDataFile().addTag(tag);\n                                }catch (IllegalArgumentException iax) {\n                                    // ignore \n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n               // success message: \n                String successMessage = JH.localize(\"file.assignedTabFileTags.success\");\n                logger.fine(successMessage);\n                successMessage = successMessage.replace(\"{0}\", \"Selected Files\");\n                JsfHelper.addFlashMessage(successMessage);\n         selectedTags = null;\n\n        logger.fine(\"New category name: \" + newCategoryName);\n\n        newCategoryName = null;\n        \n        if (removeUnusedTags){\n            removeUnusedFileTagsFromDataset();\n        }\n        save();\n        return  returnToDraftVersion();\n    }\n    \n    /*\n    Remove unused file tags\n    When updating datafile tags see if any custom tags are not in use.\n    Remove them\n    \n    */\n    private void removeUnusedFileTagsFromDataset() {\n        categoriesByName = new ArrayList<>();\n        for (FileMetadata fm : workingVersion.getFileMetadatas()) {\n            if (fm.getCategories() != null) {\n                for (int i = 0; i < fm.getCategories().size(); i++) {\n                    if (!categoriesByName.contains(fm.getCategories().get(i).getName())) {\n                        categoriesByName.add(fm.getCategories().get(i).getName());\n                    }\n                }\n            }\n        }\n        List<DataFileCategory> datasetFileCategoriesToRemove = new ArrayList<>();\n\n        for (DataFileCategory test : dataset.getCategories()) {\n            boolean remove = true;\n            for (String catByName : categoriesByName) {\n                if (catByName.equals(test.getName())) {\n                    remove = false;\n                    break;\n                }\n            }\n            if (remove) {\n                datasetFileCategoriesToRemove.add(test);\n            }\n        }\n\n        if (!datasetFileCategoriesToRemove.isEmpty()) {\n            for (DataFileCategory remove : datasetFileCategoriesToRemove) {\n                dataset.getCategories().remove(remove);\n            }\n\n        }\n\n    }\n\n    \n    /* \n     * Items for the \"Advanced (Ingest) Options\" popup. \n     * \n     */\n    private FileMetadata fileMetadataSelectedForIngestOptionsPopup = null; \n\n    public void  setFileMetadataSelectedForIngestOptionsPopup(FileMetadata fm){\n       fileMetadataSelectedForIngestOptionsPopup = fm; \n    }\n    \n    public FileMetadata getFileMetadataSelectedForIngestOptionsPopup() {\n        return fileMetadataSelectedForIngestOptionsPopup;\n    }\n    \n    public void clearFileMetadataSelectedForIngestOptionsPopup() {\n        fileMetadataSelectedForIngestOptionsPopup = null;\n    }\n    \n    private String ingestLanguageEncoding = null;\n\n    public String getIngestLanguageEncoding() {\n        if (ingestLanguageEncoding == null) {\n            return \"UTF8 (default)\";\n        }\n        return ingestLanguageEncoding;\n    }\n\n    public void setIngestLanguageEncoding(String ingestLanguageEncoding) {\n        this.ingestLanguageEncoding = ingestLanguageEncoding;\n    }\n\n    public void setIngestEncoding(String ingestEncoding) {\n        ingestLanguageEncoding = ingestEncoding;\n    }\n\n    private String savedLabelsTempFile = null;\n\n    public void handleLabelsFileUpload(FileUploadEvent event) {\n        logger.fine(\"entering handleUpload method.\");\n        UploadedFile file = event.getFile();\n\n        if (file != null) {\n\n            InputStream uploadStream = null;\n            try {\n                uploadStream = file.getInputstream();\n            } catch (IOException ioex) {\n                logger.warning(\"the file \" + file.getFileName() + \" failed to upload!\");\n                FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_WARN, \"upload failure\", \"the file \" + file.getFileName() + \" failed to upload!\");\n                FacesContext.getCurrentInstance().addMessage(null, message);\n                return;\n            }\n\n            savedLabelsTempFile = saveTempFile(uploadStream);\n\n            logger.fine(file.getFileName() + \" is successfully uploaded.\");\n            FacesMessage message = new FacesMessage(\"Succesful\", file.getFileName() + \" is uploaded.\");\n            FacesContext.getCurrentInstance().addMessage(null, message);\n        }\n\n        // process file (i.e., just save it in a temp location; for now):\n    }\n\n    private String saveTempFile(InputStream input) {\n        if (input == null) {\n            return null;\n        }\n        byte[] buffer = new byte[8192];\n        int bytesRead = 0;\n        File labelsFile = null;\n        FileOutputStream output = null;\n        try {\n            labelsFile = File.createTempFile(\"tempIngestLabels.\", \".txt\");\n            output = new FileOutputStream(labelsFile);\n            while ((bytesRead = input.read(buffer)) > -1) {\n                output.write(buffer, 0, bytesRead);\n            }\n        } catch (IOException ioex) {\n            if (input != null) {\n                try {\n                    input.close();\n                } catch (IOException e) {\n                }\n            }\n            if (output != null) {\n                try {\n                    output.close();\n                } catch (IOException e) {\n                }\n            }\n            return null;\n        }\n\n        if (labelsFile != null) {\n            return labelsFile.getAbsolutePath();\n        }\n        return null;\n    }\n\n    public void saveAdvancedOptions() {\n        \n        // Language encoding for SPSS SAV (and, possibly, other tabular ingests:) \n        if (ingestLanguageEncoding != null) {\n            if (fileMetadataSelectedForIngestOptionsPopup != null && fileMetadataSelectedForIngestOptionsPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest() == null) {\n                    IngestRequest ingestRequest = new IngestRequest();\n                    ingestRequest.setDataFile(fileMetadataSelectedForIngestOptionsPopup.getDataFile());\n                    fileMetadataSelectedForIngestOptionsPopup.getDataFile().setIngestRequest(ingestRequest);\n\n                }\n                fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest().setTextEncoding(ingestLanguageEncoding);\n            }\n        }\n        ingestLanguageEncoding = null;\n\n        // Extra labels for SPSS POR (and, possibly, other tabular ingests:)\n        // (we are adding this parameter to the IngestRequest now, instead of back\n        // when it was uploaded. This is because we want the user to be able to \n        // hit cancel and bail out, until they actually click 'save' in the \n        // \"advanced options\" popup) -- L.A. 4.0 beta 11\n        if (savedLabelsTempFile != null) {\n            if (fileMetadataSelectedForIngestOptionsPopup != null && fileMetadataSelectedForIngestOptionsPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest() == null) {\n                    IngestRequest ingestRequest = new IngestRequest();\n                    ingestRequest.setDataFile(fileMetadataSelectedForIngestOptionsPopup.getDataFile());\n                    fileMetadataSelectedForIngestOptionsPopup.getDataFile().setIngestRequest(ingestRequest);\n                }\n                fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest().setLabelsFile(savedLabelsTempFile);\n            }\n        }\n        savedLabelsTempFile = null;\n\n        fileMetadataSelectedForIngestOptionsPopup = null;\n    }\n\n    private Boolean downloadButtonAvailable = null; \n    \n    public boolean isDownloadButtonAvailable(){\n        \n        if (downloadButtonAvailable != null) {\n            return downloadButtonAvailable;\n        }\n\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (this.fileDownloadHelper.canDownloadFile(fmd)) {\n                downloadButtonAvailable = true;\n                return true;\n            }\n        }\n        downloadButtonAvailable = false;\n        return false;\n    }\n    \n    public boolean isFileAccessRequestMultiButtonRequired(){\n        if (!isSessionUserAuthenticated() || !dataset.isFileAccessRequest()){\n            return false;\n        }\n        if (workingVersion == null) {\n            return false;\n        }\n        if (!workingVersion.getTermsOfUseAndAccess().isFileAccessRequest()){\n           // return false;\n        }\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()){\n            if (!this.fileDownloadHelper.canDownloadFile(fmd)){\n                return true;               \n            }\n        }\n        return false;\n    }\n\n    public boolean isFileAccessRequestMultiButtonEnabled(){\n        if (!isSessionUserAuthenticated() || !dataset.isFileAccessRequest()){\n            return false;\n        }\n        if( this.selectedRestrictedFiles == null || this.selectedRestrictedFiles.isEmpty() ){\n            return false;\n        }\n        for (FileMetadata fmd : this.selectedRestrictedFiles){\n            if (!this.fileDownloadHelper.canDownloadFile(fmd)){\n                return true;               \n            }\n        }\n        return false;\n    } \n    \n    private Boolean downloadButtonAllEnabled = null;\n\n    public boolean isDownloadAllButtonEnabled() {\n\n        if (downloadButtonAllEnabled == null) {\n            for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n                if (!this.fileDownloadHelper.canDownloadFile(fmd)) {\n                    downloadButtonAllEnabled = false;\n                    break;\n                }\n            }\n            downloadButtonAllEnabled = true;\n        }\n        return downloadButtonAllEnabled;\n    }\n\n    public boolean isDownloadSelectedButtonEnabled(){\n\n        if( this.selectedFiles == null || this.selectedFiles.isEmpty() ){\n            return false;\n        }\n        for (FileMetadata fmd : this.selectedFiles){\n            if (this.fileDownloadHelper.canDownloadFile(fmd)){\n                return true;               \n            }\n        }\n        return false;\n    } \n    \n    public boolean isFileAccessRequestMultiSignUpButtonRequired(){\n        if (isSessionUserAuthenticated()){\n            return false;\n        }\n        // only show button if dataset allows an access request\n        if (!dataset.isFileAccessRequest()){\n            return false;\n        }\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()){\n            if (!this.fileDownloadHelper.canDownloadFile(fmd)){\n                return true;               \n            }\n        }\n        return false;\n    }\n\n    public boolean isFileAccessRequestMultiSignUpButtonEnabled(){\n        if (isSessionUserAuthenticated()){\n            return false;\n        }\n        if( this.selectedRestrictedFiles == null || this.selectedRestrictedFiles.isEmpty() ){\n            return false;\n        }\n        // only show button if dataset allows an access request\n        if (!dataset.isFileAccessRequest()){\n            return false;\n        }\n        for (FileMetadata fmd : this.selectedRestrictedFiles){\n            if (!this.fileDownloadHelper.canDownloadFile(fmd)){\n                return true;               \n            }\n        }\n        return false;\n    }\n\n    public boolean isDownloadPopupRequired() {\n        return FileUtil.isDownloadPopupRequired(workingVersion);\n    }\n    \n    public boolean isRequestAccessPopupRequired() {  \n        return FileUtil.isRequestAccessPopupRequired(workingVersion);\n    }\n    \n    public String requestAccessMultipleFiles() {\n\n        if (selectedFiles.isEmpty()) {\n            RequestContext requestContext = RequestContext.getCurrentInstance();\n            requestContext.execute(\"PF('selectFilesForRequestAccess').show()\");\n            return \"\";\n        } else {\n            fileDownloadHelper.clearRequestAccessFiles();\n            for (FileMetadata fmd : selectedFiles){\n                 fileDownloadHelper.addMultipleFilesForRequestAccess(fmd.getDataFile());\n            }\n            if (isRequestAccessPopupRequired()) {\n                RequestContext requestContext = RequestContext.getCurrentInstance();                \n                requestContext.execute(\"PF('requestAccessPopup').show()\");               \n                return \"\";\n            } else {\n                //No popup required\n                fileDownloadHelper.requestAccessIndirect();\n                return \"\";\n            }\n        }\n    }\n\n    public boolean isSortButtonEnabled() {\n        /**\n         * @todo The \"Sort\" Button seems to stop responding to mouse clicks\n         * after a while so it can't be shipped in 4.2 and will be deferred, to\n         * be picked up in https://github.com/IQSS/dataverse/issues/2506\n         */\n        return false;\n    }\n\n    public void updateFileListing(String fileSortField, String fileSortOrder) {\n        this.fileSortField = fileSortField;\n        this.fileSortOrder = fileSortOrder;\n        fileMetadatas = populateFileMetadatas();\n    }\n\n    private List<FileMetadata> populateFileMetadatas() {\n        if (isSortButtonEnabled()) {\n            List<FileMetadata> fileMetadatasToSet = new ArrayList<>();\n            Long datasetVersion = workingVersion.getId();\n            if (datasetVersion != null) {\n                int unlimited = 0;\n                int maxResults = unlimited;\n                List<FileMetadata> dataFilesNew = datafileService.findFileMetadataByDatasetVersionId(datasetVersion, maxResults, fileSortField, fileSortOrder);\n                fileMetadatasToSet.addAll(dataFilesNew);\n            }\n            return fileMetadatasToSet;\n        } else {\n            return new ArrayList<>();\n        }\n    }\n\n    public String getFileSortField() {\n        return fileSortField;\n    }\n\n    public void setFileSortField(String fileSortField) {\n        this.fileSortField = fileSortField;\n    }\n\n    public String getFileSortOrder() {\n        return fileSortOrder;\n    }\n\n    public void setFileSortOrder(String fileSortOrder) {\n        this.fileSortOrder = fileSortOrder;\n    }\n\n    public List<FileMetadata> getFileMetadatas() {\n        if (isSortButtonEnabled()) {\n            return fileMetadatas;\n        } else {\n            return new ArrayList<>();\n        }\n    }\n\n    public String getFileSortFieldName() {\n        return FileSortFieldAndOrder.label;\n    }\n\n    public String getFileSortFieldDate() {\n        return FileSortFieldAndOrder.createDate;\n    }\n\n    public String getFileSortFieldSize() {\n        return FileSortFieldAndOrder.size;\n    }\n\n    public String getFileSortFieldType() {\n        return FileSortFieldAndOrder.type;\n    }\n\n    public String getSortByAscending() {\n        return SortBy.ASCENDING;\n    }\n\n    public String getSortByDescending() {\n        return SortBy.DESCENDING;\n    }\n\n    PrivateUrl privateUrl;\n\n    public PrivateUrl getPrivateUrl() {\n        return privateUrl;\n    }\n\n    public void setPrivateUrl(PrivateUrl privateUrl) {\n        this.privateUrl = privateUrl;\n    }\n\n    public void initPrivateUrlPopUp() {\n        if (privateUrl != null) {\n            setPrivateUrlJustCreatedToFalse();\n        }\n    }\n\n    boolean privateUrlWasJustCreated;\n\n    public boolean isPrivateUrlWasJustCreated() {\n        return privateUrlWasJustCreated;\n    }\n\n    public void setPrivateUrlJustCreatedToFalse() {\n        privateUrlWasJustCreated = false;\n    }\n\n    public void createPrivateUrl() {\n        try {\n            PrivateUrl createdPrivateUrl = commandEngine.submit(new CreatePrivateUrlCommand(dvRequestService.getDataverseRequest(), dataset));\n            privateUrl = createdPrivateUrl;\n            JH.addMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.privateurl.infoMessageAuthor\", Arrays.asList(getPrivateUrlLink(privateUrl))));\n            privateUrlWasJustCreated = true;\n        } catch (CommandException ex) {\n            String msg = BundleUtil.getStringFromBundle(\"dataset.privateurl.noPermToCreate\", PrivateUrlUtil.getRequiredPermissions(ex));\n            logger.info(\"Unable to create a Private URL for dataset id \" + dataset.getId() + \". Message to user: \" + msg + \" Exception: \" + ex);\n            JH.addErrorMessage(msg);\n        }\n    }\n\n    public void disablePrivateUrl() {\n        try {\n            commandEngine.submit(new DeletePrivateUrlCommand(dvRequestService.getDataverseRequest(), dataset));\n            privateUrl = null;\n            JH.addSuccessMessage(BundleUtil.getStringFromBundle(\"dataset.privateurl.disabledSuccess\"));\n        } catch (CommandException ex) {\n            logger.info(\"CommandException caught calling DeletePrivateUrlCommand: \" + ex);\n        }\n    }\n\n    public boolean isUserCanCreatePrivateURL() {\n        return dataset.getLatestVersion().isDraft();\n    }\n\n    public String getPrivateUrlLink(PrivateUrl privateUrl) {\n        return privateUrl.getLink();\n    }\n    \n    \n    public FileDownloadHelper getFileDownloadHelper() {\n        return fileDownloadHelper;\n    }\n\n    public void setFileDownloadHelper(FileDownloadHelper fileDownloadHelper) {\n        this.fileDownloadHelper = fileDownloadHelper;\n    }\n    \n    \n    public FileDownloadServiceBean getFileDownloadService() {\n        return fileDownloadService;\n    }\n\n    public void setFileDownloadService(FileDownloadServiceBean fileDownloadService) {\n        this.fileDownloadService = fileDownloadService;\n    }\n    \n    \n    public GuestbookResponseServiceBean getGuestbookResponseService() {\n        return guestbookResponseService;\n    }\n\n    public void setGuestbookResponseService(GuestbookResponseServiceBean guestbookResponseService) {\n        this.guestbookResponseService = guestbookResponseService;\n    }\n    \n       \n    public WorldMapPermissionHelper getWorldMapPermissionHelper() {\n        return worldMapPermissionHelper;\n    }\n\n    public void setWorldMapPermissionHelper(WorldMapPermissionHelper worldMapPermissionHelper) {\n        this.worldMapPermissionHelper = worldMapPermissionHelper;\n    }\n\n    /**\n     * dataset title\n     * @return title of workingVersion\n     */\n    public String getTitle() {\n        assert (null != workingVersion);\n        return workingVersion.getTitle();\n    }\n\n    /**\n     * dataset description\n     *\n     * @return description of workingVersion\n     */\n    public String getDescription() {\n        return workingVersion.getDescriptionPlainText();\n    }\n\n    /**\n     * dataset authors\n     *\n     * @return list of author names\n     */\n    public List<String> getDatasetAuthors() {\n        assert (workingVersion != null);\n        return workingVersion.getDatasetAuthorNames();\n    }\n\n    /**\n     * publisher (aka - name of root dataverse)\n     *\n     * @return the publisher of the version\n     */\n    public String getPublisher() {\n        assert (null != workingVersion);\n        return workingVersion.getRootDataverseNameforCitation();\n    }\n    \n    /*\n    public String getThumbnail() {\n        DatasetThumbnail datasetThumbnail = dataset.getDatasetThumbnail();\n        if (datasetThumbnail != null) {\n            return datasetThumbnail.getBase64image();\n        } else {\n            return null;\n        }\n    }*/\n    \n    \n    public void downloadRsyncScript() {\n\n        String bibFormatDowload = new BibtexCitation(workingVersion).toString();\n        FacesContext ctx = FacesContext.getCurrentInstance();\n        HttpServletResponse response = (HttpServletResponse) ctx.getExternalContext().getResponse();\n        response.setContentType(\"application/download\");\n\n        String contentDispositionString;\n\n        contentDispositionString = \"attachment;filename=\" + rsyncScriptFilename;\n        response.setHeader(\"Content-Disposition\", contentDispositionString);\n\n        try {\n            ServletOutputStream out = response.getOutputStream();\n            out.write(getRsyncScript().getBytes());\n            out.flush();\n            ctx.responseComplete();\n        } catch (IOException e) {\n            String error = \"Problem getting bytes from rsync script: \" + e;\n            logger.warning(error);\n            return; \n        }\n        \n        // If the script has been successfully downloaded, lock the dataset:\n        String lockInfoMessage = \"script downloaded\";\n        DatasetLock lock = datasetService.addDatasetLock(dataset.getId(), DatasetLock.Reason.DcmUpload, session.getUser() != null ? ((AuthenticatedUser)session.getUser()).getId() : null, lockInfoMessage);\n        if (lock != null) {\n            dataset.addLock(lock);\n        } else {\n            logger.log(Level.WARNING, \"Failed to lock the dataset (dataset id={0})\", dataset.getId());\n        }\n        \n    }\n    \n    public void closeRsyncScriptPopup(CloseEvent event) {\n        finishRsyncScriptAction();\n    }\n    \n    public String finishRsyncScriptAction() { \n        // This method is called when the user clicks on \"Close\" in the \"Rsync Upload\" \n        // popup. If they have successfully downloaded the rsync script, the \n        // dataset should now be locked; which means we should put up the \n        // \"dcm upload in progress\" message - that will be shown on the page \n        // until the rsync upload is completed and the dataset is unlocked. \n        if (isLocked()) {\n            JH.addMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"file.rsyncUpload.inProgressMessage.summary\"), BundleUtil.getStringFromBundle(\"file.rsyncUpload.inProgressMessage.details\"));\n        } \n        return \"\";\n    }\n\n    /**\n     * this method returns the dataset fields to be shown in the dataset summary \n     * on the dataset page.\n     * It returns the default summary fields( subject, description, keywords, related publications and notes)\n     * if the custom summary datafields has not been set, otherwise will set the custom fields set by the sysadmins\n     * \n     * @return the dataset fields to be shown in the dataset summary\n     */\n    public List<DatasetField> getDatasetSummaryFields() {\n       customFields  = settingsWrapper.getValueForKey(SettingsServiceBean.Key.CustomDatasetSummaryFields);\n       \n        return DatasetUtil.getDatasetSummaryFields(workingVersion, customFields);\n    }\n\n    public List<ExternalTool> getConfigureToolsForDataFile(Long fileId) {\n        return getCachedToolsForDataFile(fileId, ExternalTool.Type.CONFIGURE);\n    }\n\n    public List<ExternalTool> getExploreToolsForDataFile(Long fileId) {\n        return getCachedToolsForDataFile(fileId, ExternalTool.Type.EXPLORE);\n    }\n\n    public List<ExternalTool> getCachedToolsForDataFile(Long fileId, ExternalTool.Type type) {\n        Map<Long, List<ExternalTool>> cachedToolsByFileId = new HashMap<>();\n        List<ExternalTool> externalTools = new ArrayList<>();\n        switch (type) {\n            case EXPLORE:\n                cachedToolsByFileId = exploreToolsByFileId;\n                externalTools = exploreTools;\n                break;\n            case CONFIGURE:\n                cachedToolsByFileId = configureToolsByFileId;\n                externalTools = configureTools;\n                break;\n            default:\n                break;\n        }\n        List<ExternalTool> cachedTools = cachedToolsByFileId.get(fileId);\n        if (cachedTools != null) { //if already queried before and added to list\n            return cachedTools;\n        }\n        DataFile dataFile = datafileService.find(fileId);\n        cachedTools = ExternalToolServiceBean.findExternalToolsByFile(externalTools, dataFile);\n        cachedToolsByFileId.put(fileId, cachedTools); //add to map so we don't have to do the lifting again\n        return cachedTools;\n    }\n\n    Boolean thisLatestReleasedVersion = null;\n    \n    public boolean isThisLatestReleasedVersion() {\n        if (thisLatestReleasedVersion != null) {\n            return thisLatestReleasedVersion;\n        }\n        \n        if (!workingVersion.isPublished()) {\n            thisLatestReleasedVersion = false;\n            return false;\n        }\n        \n        DatasetVersion latestPublishedVersion = null;\n        Command<DatasetVersion> cmd = new GetLatestPublishedDatasetVersionCommand(dvRequestService.getDataverseRequest(), dataset);\n        try {\n            latestPublishedVersion = commandEngine.submit(cmd);\n        } catch (Exception ex) {\n            // whatever...\n        }\n        \n        thisLatestReleasedVersion = workingVersion.equals(latestPublishedVersion);\n        \n        return thisLatestReleasedVersion;\n        \n    }\n    \n    public String getJsonLd() {\n        if (isThisLatestReleasedVersion()) {\n            ExportService instance = ExportService.getInstance(settingsService);\n            String jsonLd = instance.getExportAsString(dataset, SchemaDotOrgExporter.NAME);\n            if (jsonLd != null) {\n                logger.fine(\"Returning cached schema.org JSON-LD.\");\n                return jsonLd;\n            } else {\n                logger.fine(\"No cached schema.org JSON-LD available. Going to the database.\");\n                return workingVersion.getJsonLd();\n            }\n        }\n        return \"\";\n    }\n}\n", "idx": 40, "id": 37660, "msg": "", "proj": "IQSS-dataverse", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -43,6 +43,7 @@ from nupic.database.ClientJobsDAO import (\n # nupic.data.dictutils module -- we will eventually want to change this\n # TODO: 'ValidationError', 'validate', 'loadJSONValueFromFile' duplicated in\n # nupic.data.jsonhelpers -- will want to remove later\n+# TODO: 'matchPatterns' duplicated from nupic.frameworks.opf.opfutils\n \n class JobFailException(Exception):\n   \"\"\" If a model raises this exception, then the runModelXXX code will", "y": 0, "oldf": "# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\nimport copy\nimport json\nimport os\nimport sys\nimport tempfile\nimport logging\nimport re\nimport traceback\nimport StringIO\nfrom collections import namedtuple\nimport pprint\nimport shutil\nimport types\nimport signal\nimport uuid\nimport validictory\n\nfrom nupic.database.ClientJobsDAO import (\n    ClientJobsDAO, InvalidConnectionException)\n\n# TODO: Note the function 'rUpdate' is also duplicated in the\n# nupic.data.dictutils module -- we will eventually want to change this\n# TODO: 'ValidationError', 'validate', 'loadJSONValueFromFile' duplicated in\n# nupic.data.jsonhelpers -- will want to remove later\n\nclass JobFailException(Exception):\n  \"\"\" If a model raises this exception, then the runModelXXX code will\n  mark the job as canceled so that all other workers exit immediately, and mark\n  the job as failed.\n  \"\"\"\n  pass\n\n\n\ndef getCopyrightHead():\n  return \"\"\"# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\"\"\"\n\n\n\ndef _paramsFileHead():\n  \"\"\"\n  This is the first portion of every sub-experiment params file we generate. Between\n  the head and the tail are the experiment specific options.\n  \"\"\"\n\n  str = getCopyrightHead() + \\\n\"\"\"\n\n## This file defines parameters for a prediction experiment.\n\n###############################################################################\n#                                IMPORTANT!!!\n# This params file is dynamically generated by the RunExperimentPermutations\n# script. Any changes made manually will be over-written the next time\n# RunExperimentPermutations is run!!!\n###############################################################################\n\n\nfrom nupic.frameworks.opf.expdescriptionhelpers import importBaseDescription\n\n# the sub-experiment configuration\nconfig ={\n\"\"\"\n\n  return str\n\n\ndef _paramsFileTail():\n  \"\"\"\n  This is the tail of every params file we generate. Between the head and the tail\n  are the experiment specific options.\n  \"\"\"\n\n  str = \\\n\"\"\"\n}\n\nmod = importBaseDescription('base.py', config)\nlocals().update(mod.__dict__)\n\"\"\"\n  return str\n\n\n\ndef _appendReportKeys(keys, prefix, results):\n  \"\"\"\n  Generate a set of possible report keys for an experiment's results.\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n  This routine is called recursively to build keys that are multiple levels\n  deep from the results dict.\n\n  Parameters:\n  -----------------------------------------------------------\n  keys:         Set of report keys accumulated so far\n  prefix:       prefix formed so far, this is the colon separated list of key\n                  names that led up to the dict passed in results\n  results:      dictionary of results at this level.\n  \"\"\"\n\n  allKeys = results.keys()\n  allKeys.sort()\n  for key in allKeys:\n    if hasattr(results[key], 'keys'):\n      _appendReportKeys(keys, \"%s%s:\" % (prefix, key), results[key])\n    else:\n      keys.add(\"%s%s\" % (prefix, key))\n\n\n\nclass _BadKeyError(Exception):\n  \"\"\" If a model raises this exception, then the runModelXXX code will\n  mark the job as canceled so that all other workers exit immediately, and mark\n  the job as failed.\n  \"\"\"\n  pass\n\n\n\ndef _matchReportKeys(reportKeyREs=[], allReportKeys=[]):\n  \"\"\"\n  Extract all items from the 'allKeys' list whose key matches one of the regular\n  expressions passed in 'reportKeys'.\n\n  Parameters:\n  ----------------------------------------------------------------------------\n  reportKeyREs:     List of regular expressions\n  allReportKeys:    List of all keys\n\n  retval:         list of keys from allReportKeys that match the regular expressions\n                    in 'reportKeyREs'\n                  If an invalid regular expression was included in 'reportKeys',\n                    then BadKeyError() is raised\n  \"\"\"\n\n  matchingReportKeys = []\n\n  # Extract the report items of interest\n  for keyRE in reportKeyREs:\n    # Find all keys that match this regular expression\n    matchObj = re.compile(keyRE)\n    found = False\n    for keyName in allReportKeys:\n      match = matchObj.match(keyName)\n      if match and match.end() == len(keyName):\n        matchingReportKeys.append(keyName)\n        found = True\n    if not found:\n      raise _BadKeyError(keyRE)\n\n  return matchingReportKeys\n\n\n\ndef _getReportItem(itemName, results):\n  \"\"\"\n  Get a specific item by name out of the results dict.\n\n  The format of itemName is a string of dictionary keys separated by colons,\n  each key being one level deeper into the results dict. For example,\n  'key1:key2' would fetch results['key1']['key2'].\n\n  If itemName is not found in results, then None is returned\n\n  \"\"\"\n\n  subKeys = itemName.split(':')\n  subResults = results\n  for subKey in subKeys:\n    subResults = subResults[subKey]\n\n  return subResults\n\n\n\ndef filterResults(allResults, reportKeys, optimizeKey=None):\n  \"\"\" Given the complete set of results generated by an experiment (passed in\n  'results'), filter out and return only the ones the caller wants, as\n  specified through 'reportKeys' and 'optimizeKey'.\n\n  A report key is a string of key names separated by colons, each key being one\n  level deeper into the experiment results dict. For example, 'key1:key2'.\n\n\n  Parameters:\n  -------------------------------------------------------------------------\n  results:             dict of all results generated by an experiment\n  reportKeys:          list of items from the results dict to include in\n                       the report. These can be regular expressions.\n  optimizeKey:         Which report item, if any, we will be optimizing for. This can\n                       also be a regular expression, but is an error if it matches\n                       more than one key from the experiment's results.\n  retval:  (reportDict, optimizeDict)\n              reportDict: a dictionary of the metrics named by desiredReportKeys\n              optimizeDict: A dictionary containing 1 item: the full name and\n                    value of the metric identified by the optimizeKey\n\n  \"\"\"\n\n  # Init return values\n  optimizeDict = dict()\n\n  # Get all available report key names for this experiment\n  allReportKeys = set()\n  _appendReportKeys(keys=allReportKeys, prefix='', results=allResults)\n\n  #----------------------------------------------------------------------------\n  # Extract the report items that match the regular expressions passed in reportKeys\n  matchingKeys = _matchReportKeys(reportKeys, allReportKeys)\n\n  # Extract the values of the desired items\n  reportDict = dict()\n  for keyName in matchingKeys:\n    value = _getReportItem(keyName, allResults)\n    reportDict[keyName] = value\n\n\n  # -------------------------------------------------------------------------\n  # Extract the report item that matches the regular expression passed in\n  #   optimizeKey\n  if optimizeKey is not None:\n    matchingKeys = _matchReportKeys([optimizeKey], allReportKeys)\n    if len(matchingKeys) == 0:\n      raise _BadKeyError(optimizeKey)\n    elif len(matchingKeys) > 1:\n      raise _BadOptimizeKeyError(optimizeKey, matchingKeys)\n    optimizeKeyFullName = matchingKeys[0]\n\n    # Get the value of the optimize metric\n    value = _getReportItem(optimizeKeyFullName, allResults)\n    optimizeDict[optimizeKeyFullName] = value\n    reportDict[optimizeKeyFullName] = value\n\n  # Return info\n  return(reportDict, optimizeDict)\n\n\n\ndef _quoteAndEscape(string):\n  \"\"\"\n  string:   input string (ascii or unicode)\n\n  Returns:  a quoted string with characters that are represented in python via\n            escape sequences converted to those escape sequences\n  \"\"\"\n  assert type(string) in types.StringTypes\n  return pprint.pformat(string)\n\n\n\ndef _handleModelRunnerException(jobID, modelID, jobsDAO, experimentDir, logger,\n                                e):\n  \"\"\" Perform standard handling of an exception that occurs while running\n  a model.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  jobID:                ID for this hypersearch job in the jobs table\n  modelID:              model ID\n  jobsDAO:              ClientJobsDAO instance\n  experimentDir:        directory containing the experiment\n  logger:               the logger to use\n  e:                    the exception that occurred\n  retval:               (completionReason, completionMsg)\n  \"\"\"\n\n  msg = StringIO.StringIO()\n  print >>msg, \"Exception occurred while running model %s: %r (%s)\" % (\n    modelID, e, type(e))\n  traceback.print_exc(None, msg)\n\n  completionReason = jobsDAO.CMPL_REASON_ERROR\n  completionMsg = msg.getvalue()\n  logger.error(completionMsg)\n\n  # Write results to the model database for the error case. Ignore\n  # InvalidConnectionException, as this is usually caused by orphaned models\n  #\n  # TODO: do we really want to set numRecords to 0? Last updated value might\n  #       be useful for debugging\n  if type(e) is not InvalidConnectionException:\n    jobsDAO.modelUpdateResults(modelID,  results=None, numRecords=0)\n\n  # TODO: Make sure this wasn't the best model in job. If so, set the best\n  # appropriately\n\n  # If this was an exception that should mark the job as failed, do that\n  # now.\n  if type(e) == JobFailException:\n    workerCmpReason = jobsDAO.jobGetFields(jobID,\n        ['workerCompletionReason'])[0]\n    if workerCmpReason == ClientJobsDAO.CMPL_REASON_SUCCESS:\n      jobsDAO.jobSetFields(jobID, fields=dict(\n          cancel=True,\n          workerCompletionReason = ClientJobsDAO.CMPL_REASON_ERROR,\n          workerCompletionMsg = \": \".join(str(i) for i in e.args)),\n          useConnectionID=False,\n          ignoreUnchanged=True)\n\n  return (completionReason, completionMsg)\n\n\n\ndef runModelGivenBaseAndParams(modelID, jobID, baseDescription, params,\n            predictedField, reportKeys, optimizeKey, jobsDAO,\n            modelCheckpointGUID, logLevel=None, predictionCacheMaxRecords=None):\n  \"\"\" This creates an experiment directory with a base.py description file\n  created from 'baseDescription' and a description.py generated from the\n  given params dict and then runs the experiment.\n\n  Parameters:\n  -------------------------------------------------------------------------\n  modelID:              ID for this model in the models table\n  jobID:                ID for this hypersearch job in the jobs table\n  baseDescription:      Contents of a description.py with the base experiment\n                                          description\n  params:               Dictionary of specific parameters to override within\n                                  the baseDescriptionFile.\n  predictedField:       Name of the input field for which this model is being\n                                    optimized\n  reportKeys:           Which metrics of the experiment to store into the\n                                    results dict of the model's database entry\n  optimizeKey:          Which metric we are optimizing for\n  jobsDAO               Jobs data access object - the interface to the\n                                  jobs database which has the model's table.\n  modelCheckpointGUID:  A persistent, globally-unique identifier for\n                                  constructing the model checkpoint key\n  logLevel:             override logging level to this value, if not None\n\n  retval:               (completionReason, completionMsg)\n  \"\"\"\n  from nupic.swarming.ModelRunner import OPFModelRunner\n\n  # The logger for this method\n  logger = logging.getLogger('com.numenta.nupic.hypersearch.utils')\n\n\n  # --------------------------------------------------------------------------\n  # Create a temp directory for the experiment and the description files\n  experimentDir = tempfile.mkdtemp()\n  try:\n    logger.info(\"Using experiment directory: %s\" % (experimentDir))\n\n    # Create the decription.py from the overrides in params\n    paramsFilePath = os.path.join(experimentDir, 'description.py')\n    paramsFile = open(paramsFilePath, 'wb')\n    paramsFile.write(_paramsFileHead())\n\n    items = params.items()\n    items.sort()\n    for (key,value) in items:\n      quotedKey = _quoteAndEscape(key)\n      if isinstance(value, basestring):\n\n        paramsFile.write(\"  %s : '%s',\\n\" % (quotedKey , value))\n      else:\n        paramsFile.write(\"  %s : %s,\\n\" % (quotedKey , value))\n\n    paramsFile.write(_paramsFileTail())\n    paramsFile.close()\n\n\n    # Write out the base description\n    baseParamsFile = open(os.path.join(experimentDir, 'base.py'), 'wb')\n    baseParamsFile.write(baseDescription)\n    baseParamsFile.close()\n\n\n    # Store the experiment's sub-description file into the model table\n    #  for reference\n    fd = open(paramsFilePath)\n    expDescription = fd.read()\n    fd.close()\n    jobsDAO.modelSetFields(modelID, {'genDescription': expDescription})\n\n\n    # Run the experiment now\n    try:\n      runner = OPFModelRunner(\n        modelID=modelID,\n        jobID=jobID,\n        predictedField=predictedField,\n        experimentDir=experimentDir,\n        reportKeyPatterns=reportKeys,\n        optimizeKeyPattern=optimizeKey,\n        jobsDAO=jobsDAO,\n        modelCheckpointGUID=modelCheckpointGUID,\n        logLevel=logLevel,\n        predictionCacheMaxRecords=predictionCacheMaxRecords)\n\n      signal.signal(signal.SIGINT, runner.handleWarningSignal)\n\n      (completionReason, completionMsg) = runner.run()\n\n    except InvalidConnectionException:\n      raise\n    except Exception, e:\n\n      (completionReason, completionMsg) = _handleModelRunnerException(jobID,\n                                     modelID, jobsDAO, experimentDir, logger, e)\n\n  finally:\n    # delete our temporary directory tree\n    shutil.rmtree(experimentDir)\n    signal.signal(signal.SIGINT, signal.default_int_handler)\n\n  # Return completion reason and msg\n  return (completionReason, completionMsg)\n\n\n\ndef runDummyModel(modelID, jobID, params, predictedField, reportKeys,\n                  optimizeKey, jobsDAO, modelCheckpointGUID, logLevel=None, predictionCacheMaxRecords=None):\n  from nupic.swarming.DummyModelRunner import OPFDummyModelRunner\n\n  # The logger for this method\n  logger = logging.getLogger('com.numenta.nupic.hypersearch.utils')\n\n\n  # Run the experiment now\n  try:\n    if type(params) is bool:\n      params = {}\n\n    runner = OPFDummyModelRunner(modelID=modelID,\n                                 jobID=jobID,\n                                 params=params,\n                                 predictedField=predictedField,\n                                 reportKeyPatterns=reportKeys,\n                                 optimizeKeyPattern=optimizeKey,\n                                 jobsDAO=jobsDAO,\n                                 modelCheckpointGUID=modelCheckpointGUID,\n                                 logLevel=logLevel,\n                                 predictionCacheMaxRecords=predictionCacheMaxRecords)\n\n    (completionReason, completionMsg) = runner.run()\n\n  # The dummy model runner will call sys.exit(1) if\n  #  NTA_TEST_sysExitFirstNModels is set and the number of models in the\n  #  models table is <= NTA_TEST_sysExitFirstNModels\n  except SystemExit:\n    sys.exit(1)\n  except InvalidConnectionException:\n    raise\n  except Exception, e:\n    (completionReason, completionMsg) = _handleModelRunnerException(jobID,\n                                   modelID, jobsDAO, \"NA\",\n                                   logger, e)\n\n  # Return completion reason and msg\n  return (completionReason, completionMsg)\n\n\n\n# Passed as parameter to ActivityMgr\n#\n# repeating: True if the activity is a repeating activite, False if one-shot\n# period: period of activity's execution (number of \"ticks\")\n# cb: a callable to call upon expiration of period; will be called\n#     as cb()\nPeriodicActivityRequest = namedtuple(\"PeriodicActivityRequest\",\n                                     (\"repeating\", \"period\", \"cb\"))\n\n\n\nclass PeriodicActivityMgr(object):\n  \"\"\"\n  TODO: move to shared script so that we can share it with run_opf_experiment\n  \"\"\"\n\n  # iteratorHolder: a list holding one iterator; we use a list so that we can\n  #           replace the iterator for repeating activities (a tuple would not\n  #           allow it if the field was an imutable value)\n  Activity = namedtuple(\"Activity\", (\"repeating\",\n                                     \"period\",\n                                     \"cb\",\n                                     \"iteratorHolder\"))\n\n  def __init__(self, requestedActivities):\n    \"\"\"\n    requestedActivities: a sequence of PeriodicActivityRequest elements\n    \"\"\"\n\n    self.__activities = []\n    for req in requestedActivities:\n      act =   self.Activity(repeating=req.repeating,\n                            period=req.period,\n                            cb=req.cb,\n                            iteratorHolder=[iter(xrange(req.period))])\n      self.__activities.append(act)\n    return\n\n  def tick(self):\n    \"\"\" Activity tick handler; services all activities\n\n    Returns:      True if controlling iterator says it's okay to keep going;\n                  False to stop\n    \"\"\"\n\n    # Run activities whose time has come\n    for act in self.__activities:\n      if not act.iteratorHolder[0]:\n        continue\n\n      try:\n        next(act.iteratorHolder[0])\n      except StopIteration:\n        act.cb()\n        if act.repeating:\n          act.iteratorHolder[0] = iter(xrange(act.period))\n        else:\n          act.iteratorHolder[0] = None\n\n    return True\n\n\n\ndef generatePersistentJobGUID():\n  \"\"\"Generates a \"persistentJobGUID\" value.\n\n  Parameters:\n  ----------------------------------------------------------------------\n  retval:          A persistentJobGUID value\n\n  \"\"\"\n  return \"JOB_UUID1-\" + str(uuid.uuid1())\n\n\n\ndef identityConversion(value, _keys):\n  return value\n\n\n\ndef rCopy(d, f=identityConversion, discardNoneKeys=True, deepCopy=True):\n  \"\"\"Recursively copies a dict and returns the result.\n\n  Args:\n    d: The dict to copy.\n    f: A function to apply to values when copying that takes the value and the\n        list of keys from the root of the dict to the value and returns a value\n        for the new dict.\n    discardNoneKeys: If True, discard key-value pairs when f returns None for\n        the value.\n    deepCopy: If True, all values in returned dict are true copies (not the\n        same object).\n  Returns:\n    A new dict with keys and values from d replaced with the result of f.\n  \"\"\"\n  # Optionally deep copy the dict.\n  if deepCopy:\n    d = copy.deepcopy(d)\n\n  newDict = {}\n  toCopy = [(k, v, newDict, ()) for k, v in d.iteritems()]\n  while len(toCopy) > 0:\n    k, v, d, prevKeys = toCopy.pop()\n    prevKeys = prevKeys + (k,)\n    if isinstance(v, dict):\n      d[k] = dict()\n      toCopy[0:0] = [(innerK, innerV, d[k], prevKeys)\n                     for innerK, innerV in v.iteritems()]\n    else:\n      #print k, v, prevKeys\n      newV = f(v, prevKeys)\n      if not discardNoneKeys or newV is not None:\n        d[k] = newV\n  return newDict\n\n\n\ndef rApply(d, f):\n  \"\"\"Recursively applies f to the values in dict d.\n\n  Args:\n    d: The dict to recurse over.\n    f: A function to apply to values in d that takes the value and a list of\n        keys from the root of the dict to the value.\n  \"\"\"\n  remainingDicts = [(d, ())]\n  while len(remainingDicts) > 0:\n    current, prevKeys = remainingDicts.pop()\n    for k, v in current.iteritems():\n      keys = prevKeys + (k,)\n      if isinstance(v, dict):\n        remainingDicts.insert(0, (v, keys))\n      else:\n        f(v, keys)\n\n\n\ndef clippedObj(obj, maxElementSize=64):\n  \"\"\"\n  Return a clipped version of obj suitable for printing, This\n  is useful when generating log messages by printing data structures, but\n  don't want the message to be too long.\n\n  If passed in a dict, list, or namedtuple, each element of the structure's\n  string representation will be limited to 'maxElementSize' characters. This\n  will return a new object where the string representation of each element\n  has been truncated to fit within maxElementSize.\n  \"\"\"\n\n  # Is it a named tuple?\n  if hasattr(obj, '_asdict'):\n    obj = obj._asdict()\n\n\n  # Printing a dict?\n  if isinstance(obj, dict):\n    objOut = dict()\n    for key,val in obj.iteritems():\n      objOut[key] = clippedObj(val)\n\n  # Printing a list?\n  elif hasattr(obj, '__iter__'):\n    objOut = []\n    for val in obj:\n      objOut.append(clippedObj(val))\n\n  # Some other object\n  else:\n    objOut = str(obj)\n    if len(objOut) > maxElementSize:\n      objOut = objOut[0:maxElementSize] + '...'\n\n  return objOut\n\n\n\nclass ValidationError(validictory.ValidationError):\n  pass\n\n\n\ndef validate(value, **kwds):\n  \"\"\" Validate a python value against json schema:\n  validate(value, schemaPath)\n  validate(value, schemaDict)\n\n  value:          python object to validate against the schema\n\n  The json schema may be specified either as a path of the file containing\n  the json schema or as a python dictionary using one of the\n  following keywords as arguments:\n    schemaPath:     Path of file containing the json schema object.\n    schemaDict:     Python dictionary containing the json schema object\n\n  Returns: nothing\n\n  Raises:\n          ValidationError when value fails json validation\n  \"\"\"\n\n  assert len(kwds.keys()) >= 1\n  assert 'schemaPath' in kwds or 'schemaDict' in kwds\n\n  schemaDict = None\n  if 'schemaPath' in kwds:\n    schemaPath = kwds.pop('schemaPath')\n    schemaDict = loadJsonValueFromFile(schemaPath)\n  elif 'schemaDict' in kwds:\n    schemaDict = kwds.pop('schemaDict')\n\n  try:\n    validictory.validate(value, schemaDict, **kwds)\n  except validictory.ValidationError as e:\n    raise ValidationError(e)\n\n\n\ndef loadJsonValueFromFile(inputFilePath):\n  \"\"\" Loads a json value from a file and converts it to the corresponding python\n  object.\n\n  inputFilePath:\n                  Path of the json file;\n\n  Returns:\n                  python value that represents the loaded json value\n\n  \"\"\"\n  with open(inputFilePath) as fileObj:\n    value = json.load(fileObj)\n\n  return value\n\n\n\ndef sortedJSONDumpS(obj):\n  \"\"\"\n  Return a JSON representation of obj with sorted keys on any embedded dicts.\n  This insures that the same object will always be represented by the same\n  string even if it contains dicts (where the sort order of the keys is\n  normally undefined).\n  \"\"\"\n\n  itemStrs = []\n\n  if isinstance(obj, dict):\n    items = obj.items()\n    items.sort()\n    for key, value in items:\n      itemStrs.append('%s: %s' % (json.dumps(key), sortedJSONDumpS(value)))\n    return '{%s}' % (', '.join(itemStrs))\n\n  elif hasattr(obj, '__iter__'):\n    for val in obj:\n      itemStrs.append(sortedJSONDumpS(val))\n    return '[%s]' % (', '.join(itemStrs))\n\n  else:\n    return json.dumps(obj)\n", "idx": 1, "id": 19290, "msg": "", "proj": "numenta-nupic", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -23,4 +23,14 @@ module RSpecHelpers\n     end\n   end\n \n+  def expect_deprecation_with_call_site(file, line)\n+    expect(RSpec.configuration.reporter).to receive(:deprecation) do |options|\n+      expect(options[:call_site]).to include([file, line].join(':'))\n+    end\n+  end\n+\n+  def allow_deprecation\n+    allow(RSpec.configuration.reporter).to receive(:deprecation)\n+  end\n+\n end", "y": 1, "oldf": "module RSpecHelpers\n  def relative_path(path)\n    RSpec::Core::Metadata.relative_path(path)\n  end\n\n  def ignoring_warnings\n    original = $VERBOSE\n    $VERBOSE = nil\n    result = yield\n    $VERBOSE = original\n    result\n  end\n\n  def safely\n    Thread.new do\n      ignoring_warnings { $SAFE = 3 }\n      yield\n    end.join\n\n    # $SAFE is not supported on Rubinius\n    unless defined?(Rubinius)\n      expect($SAFE).to eql 0 # $SAFE should not have changed in this thread.\n    end\n  end\n\nend\n", "idx": 1, "id": 9432, "msg": "I can't get this helper to fail. I've passed incorrect line numbers and it always passes. I see the block get run, but it doesn't seem this inner expectation is properly raised.", "proj": "rspec-rspec-core", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -19,7 +19,7 @@ import (\n var actionDeployCmd = &cobra.Command{\n \tUse:   \"deploy [AMOUNT_IOTX] [-s SIGNER] -b BYTE_CODE [-l GAS_LIMIT] [-p GAS_PRICE] [-P PASSWORD] [-y]\",\n \tShort: \"Deploy smart contract on IoTeX blockchain\",\n-\tArgs:  cobra.MaximumNArgs(0),\n+\tArgs:  cobra.MaximumNArgs(1),\n \tRunE: func(cmd *cobra.Command, args []string) error {\n \t\tcmd.SilenceUsage = true\n \t\terr := deploy(args)", "y": 1, "oldf": "// Copyright (c) 2019 IoTeX Foundation\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage action\n\nimport (\n\t\"math/big\"\n\n\t\"github.com/spf13/cobra\"\n\n\t\"github.com/iotexproject/iotex-core/ioctl/output\"\n\t\"github.com/iotexproject/iotex-core/ioctl/util\"\n)\n\n// actionDeployCmd represents the action deploy command\nvar actionDeployCmd = &cobra.Command{\n\tUse:   \"deploy [AMOUNT_IOTX] [-s SIGNER] -b BYTE_CODE [-l GAS_LIMIT] [-p GAS_PRICE] [-P PASSWORD] [-y]\",\n\tShort: \"Deploy smart contract on IoTeX blockchain\",\n\tArgs:  cobra.MaximumNArgs(0),\n\tRunE: func(cmd *cobra.Command, args []string) error {\n\t\tcmd.SilenceUsage = true\n\t\terr := deploy(args)\n\t\treturn output.PrintError(err)\n\t},\n}\n\nfunc init() {\n\tregisterWriteCommand(actionDeployCmd)\n\tbytecodeFlag.RegisterCommand(actionDeployCmd)\n\tbytecodeFlag.MarkFlagRequired(actionDeployCmd)\n}\n\nfunc deploy(args []string) error {\n\tbytecode, err := decodeBytecode()\n\tif err != nil {\n\t\treturn output.NewError(output.FlagError, \"invalid bytecode flag\", err)\n\t}\n\tamount := big.NewInt(0)\n\tif len(args) == 1 {\n\t\tamount, err = util.StringToRau(args[0], util.IotxDecimalNum)\n\t\tif err != nil {\n\t\t\treturn output.NewError(output.ConvertError, \"invalid amount\", err)\n\t\t}\n\t}\n\treturn Execute(\"\", amount, bytecode)\n}\n", "idx": 1, "id": 18709, "msg": "if args = 0, how come the deploy can work now?", "proj": "iotexproject-iotex-core", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -249,7 +249,7 @@ namespace Nethermind.Blockchain.Test.Synchronization\n         [TestCase(65L)]\n         public async Task Peer_sends_just_one_item_when_advertising_more_blocks(long headNumber)\n         {\n-            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n+            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, new InMemoryReceiptStorage(), LimboLogs.Instance);\n \n             ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n             syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())", "y": 0, "oldf": "/*\n * Copyright (c) 2018 Demerzel Solutions Limited\n * This file is part of the Nethermind library.\n *\n * The Nethermind library is free software: you can redistribute it and/or modify\n * it under the terms of the GNU Lesser General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * The Nethermind library is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU Lesser General Public License for more details.\n *\n * You should have received a copy of the GNU Lesser General Public License\n * along with the Nethermind. If not, see <http://www.gnu.org/licenses/>.\n */\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Nethermind.Blockchain.Synchronization;\nusing Nethermind.Blockchain.TxPools;\nusing Nethermind.Blockchain.Validators;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Core.Encoding;\nusing Nethermind.Core.Specs;\nusing Nethermind.Core.Test.Builders;\nusing Nethermind.Dirichlet.Numerics;\nusing Nethermind.Logging;\nusing Nethermind.Mining;\nusing Nethermind.Network.P2P.Subprotocols.Eth;\nusing Nethermind.Stats.Model;\nusing Nethermind.Store;\nusing Nethermind.Store.Repositories;\nusing NSubstitute;\nusing NSubstitute.Core;\nusing NSubstitute.ExceptionExtensions;\nusing NUnit.Framework;\n\nnamespace Nethermind.Blockchain.Test.Synchronization\n{\n    [TestFixture]\n    public class BlockDownloaderTests\n    {\n        private IBlockTree _blockTree;\n        private ResponseBuilder _responseBuilder;\n        private Dictionary<long, Keccak> _testHeaderMapping;\n\n        private class ResponseBuilder\n        {\n            private IBlockTree _blockTree;\n            private readonly Dictionary<long, Keccak> _testHeaderMapping;\n\n            public ResponseBuilder(IBlockTree blockTree, Dictionary<long, Keccak> testHeaderMapping)\n            {\n                _blockTree = blockTree;\n                _testHeaderMapping = testHeaderMapping;\n            }\n\n            public async Task<BlockHeader[]> BuildHeaderResponse(long startNumber, int number, Response flags)\n            {\n                bool consistent = flags.HasFlag(Response.Consistent);\n                bool validSeals = flags.HasFlag(Response.ValidSeals);\n                bool noEmptySpaces = flags.HasFlag(Response.NoEmptySpace);\n                bool justFirst = flags.HasFlag(Response.JustFirst);\n                bool allKnown = flags.HasFlag(Response.AllKnown);\n                bool timeoutOnFullBatch = flags.HasFlag(Response.TimeoutOnFullBatch);\n                bool noBody = flags.HasFlag(Response.NoBody);\n\n                if (timeoutOnFullBatch && number == SyncBatchSize.Max)\n                {\n                    throw new TimeoutException();\n                }\n\n                BlockHeader startBlock = _blockTree.FindHeader(_testHeaderMapping[startNumber], BlockTreeLookupOptions.None);\n                BlockHeader[] headers = new BlockHeader[number];\n                headers[0] = startBlock;\n                if (!justFirst)\n                {\n                    for (int i = 1; i < number; i++)\n                    {\n                        \n                        headers[i] = consistent\n                            ? Build.A.BlockHeader.WithParent(headers[i - 1]).WithOmmersHash(noBody ? Keccak.OfAnEmptySequenceRlp : Keccak.Zero).TestObject\n                            : Build.A.BlockHeader.WithNumber(headers[i - 1].Number + 1).TestObject;\n\n                        if (allKnown)\n                        {\n                            _blockTree.SuggestHeader(headers[i]);\n                        }\n\n                        _testHeaderMapping[startNumber + i] = headers[i].Hash;\n                    }\n                }\n\n                BlockHeadersMessage message = new BlockHeadersMessage(headers);\n                byte[] messageSerialized = _headersSerializer.Serialize(message);\n                return await Task.FromResult(_headersSerializer.Deserialize(messageSerialized).BlockHeaders);\n            }\n\n            private BlockHeadersMessageSerializer _headersSerializer = new BlockHeadersMessageSerializer();\n            private BlockBodiesMessageSerializer _bodiesSerializer = new BlockBodiesMessageSerializer();\n\n            public async Task<BlockBody[]> BuildBlocksResponse(Keccak[] blockHashes, Response flags)\n            {\n                bool consistent = flags.HasFlag(Response.Consistent);\n                bool validSeals = flags.HasFlag(Response.ValidSeals);\n                bool noEmptySpaces = flags.HasFlag(Response.NoEmptySpace);\n                bool justFirst = flags.HasFlag(Response.JustFirst);\n                bool allKnown = flags.HasFlag(Response.AllKnown);\n                bool timeoutOnFullBatch = flags.HasFlag(Response.TimeoutOnFullBatch);\n\n                if (timeoutOnFullBatch && blockHashes.Length == SyncBatchSize.Max)\n                {\n                    throw new TimeoutException();\n                }\n\n                BlockHeader startHeader = _blockTree.FindHeader(blockHashes[0], BlockTreeLookupOptions.None);\n                if (startHeader == null) startHeader = Build.A.BlockHeader.WithHash(blockHashes[0]).TestObject;\n\n                BlockHeader[] blockHeaders = new BlockHeader[blockHashes.Length];\n                BlockBody[] blockBodies = new BlockBody[blockHashes.Length];\n                blockBodies[0] = new BlockBody(new Transaction[0], new BlockHeader[0]);\n                blockHeaders[0] = startHeader;\n                if (!justFirst)\n                {\n                    for (int i = 1; i < blockHashes.Length; i++)\n                    {\n                        blockHeaders[i] = consistent\n                            ? Build.A.BlockHeader.WithParent(blockHeaders[i - 1]).TestObject\n                            : Build.A.BlockHeader.WithNumber(blockHeaders[i - 1].Number + 1).TestObject;\n\n                        _testHeaderMapping[startHeader.Number + i] = blockHeaders[i].Hash;\n\n                        Block block = consistent\n                            ? Build.A.Block.WithHeader(blockHeaders[i]).TestObject\n                            : Build.A.Block.WithHeader(blockHeaders[i - 1]).TestObject;\n                        blockBodies[i] = new BlockBody(block.Transactions, block.Ommers);\n\n                        if (allKnown)\n                        {\n                            _blockTree.SuggestBlock(block);\n                        }\n                    }\n                }\n\n                BlockBodiesMessage message = new BlockBodiesMessage(blockBodies);\n                byte[] messageSerialized = _bodiesSerializer.Serialize(message);\n                return await Task.FromResult(_bodiesSerializer.Deserialize(messageSerialized).Bodies);\n            }\n        }\n\n        [SetUp]\n        public void Setup()\n        {\n            Block genesis = Build.A.Block.Genesis.TestObject;\n            var blockInfoDb = new MemDb();\n            _blockTree = new BlockTree(new MemDb(), new MemDb(), blockInfoDb, new ChainLevelInfoRepository(blockInfoDb), MainNetSpecProvider.Instance, NullTxPool.Instance, LimboLogs.Instance);\n            _blockTree.SuggestBlock(genesis);\n\n            _testHeaderMapping = new Dictionary<long, Keccak>();\n            _testHeaderMapping.Add(0, genesis.Hash);\n\n            _responseBuilder = new ResponseBuilder(_blockTree, _testHeaderMapping);\n        }\n\n        [TestCase(0L)]\n        [TestCase(32L)]\n        [TestCase(33L)]\n        [TestCase(SyncBatchSize.Max * 8)]\n        public async Task Happy_path(long headNumber)\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = headNumber;\n\n            await blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            Assert.AreEqual(Math.Max(0, headNumber - SyncModeSelector.FullSyncThreshold), _blockTree.BestSuggestedHeader.Number, \"headers\");\n\n            peerInfo.HeadNumber *= 2;\n            await blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None);\n            Assert.AreEqual(Math.Max(0, headNumber * 2), _blockTree.BestSuggestedHeader.Number);\n        }\n\n        [Test]\n        public async Task Can_sync_with_peer_when_it_times_out_on_full_batch()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(async ci => await _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect | Response.TimeoutOnFullBatch));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect | Response.TimeoutOnFullBatch));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = SyncBatchSize.Max * 2 + 32;\n\n            await blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None).ContinueWith(t => { });\n            await blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            Assert.AreEqual(Math.Max(0, peerInfo.HeadNumber - SyncModeSelector.FullSyncThreshold), _blockTree.BestSuggestedHeader.Number);\n\n            peerInfo.HeadNumber *= 2;\n            await blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None).ContinueWith(t => { });\n            await blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None);\n            Assert.AreEqual(Math.Max(0, peerInfo.HeadNumber), _blockTree.BestSuggestedHeader.Number);\n        }\n\n        [Test]\n        public async Task Headers_already_known()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect | Response.AllKnown));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect | Response.AllKnown));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 64;\n\n            await blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None)\n                .ContinueWith(t => Assert.True(t.IsCompletedSuccessfully));\n\n            peerInfo.HeadNumber = 128;\n            await blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None)\n                .ContinueWith(t => Assert.True(t.IsCompletedSuccessfully));\n        }\n\n        [TestCase(33L)]\n        [TestCase(65L)]\n        public async Task Peer_sends_just_one_item_when_advertising_more_blocks(long headNumber)\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect | Response.JustFirst));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = headNumber;\n\n            Task task = blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None);\n            await task.ContinueWith(t => Assert.True(t.IsFaulted));\n\n            Assert.AreEqual(0, _blockTree.BestSuggestedHeader.Number);\n        }\n        \n        [TestCase(33L)]\n        [TestCase(65L)]\n        public async Task Peer_sends_just_one_item_when_advertising_more_blocks_but_no_bodies(long headNumber)\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect | Response.NoBody));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect | Response.JustFirst));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = headNumber;\n\n            Task task = blockDownloader.DownloadBlocks(peerInfo, 0, CancellationToken.None);\n            await task.ContinueWith(t => Assert.False(t.IsFaulted));\n\n            Assert.AreEqual(headNumber, _blockTree.BestSuggestedHeader.Number);\n        }\n\n        [Test]\n        public async Task Throws_on_null_best_peer()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n            Task task1 = blockDownloader.DownloadHeaders(null, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            await task1.ContinueWith(t => Assert.True(t.IsFaulted));\n\n            Task task2 = blockDownloader.DownloadBlocks(null, 0, CancellationToken.None);\n            await task2.ContinueWith(t => Assert.True(t.IsFaulted));\n        }\n\n        [Test]\n        public async Task Throws_on_inconsistent_batch()\n        {\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect ^ Response.Consistent));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1024;\n\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n            Task task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            await task.ContinueWith(t => Assert.True(t.IsFaulted));\n        }\n\n        [Test]\n        public async Task Throws_on_invalid_seal()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.NeverValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1000;\n\n            Task task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            await task.ContinueWith(t => Assert.True(t.IsFaulted));\n        }\n\n        [Test]\n        public async Task Throws_on_invalid_header()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.NeverValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1000;\n\n            Task task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None);\n            await task.ContinueWith(t => Assert.True(t.IsFaulted));\n        }\n\n        private class SlowSealValidator : ISealValidator\n        {\n            public bool ValidateParams(BlockHeader parent, BlockHeader header)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n\n            public bool ValidateSeal(BlockHeader header)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n        }\n\n        private class SlowHeaderValidator : IBlockValidator\n        {\n            public bool ValidateHash(BlockHeader header)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n\n            public bool ValidateHeader(BlockHeader header, BlockHeader parent, bool isOmmer)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n\n            public bool ValidateHeader(BlockHeader header, bool isOmmer)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n\n            public bool ValidateSuggestedBlock(Block block)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n\n            public bool ValidateProcessedBlock(Block processedBlock, TxReceipt[] receipts, Block suggestedBlock)\n            {\n                Thread.Sleep(1000);\n                return true;\n            }\n        }\n\n        [Test, MaxTime(7000)]\n        [Ignore(\"Fails OneLoggerLogManager Travis only\")]\n        public async Task Can_cancel_seal_validation()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, new SlowSealValidator(), NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1000;\n\n            CancellationTokenSource cancellation = new CancellationTokenSource();\n            cancellation.CancelAfter(1000);\n            Task task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, cancellation.Token);\n            await task.ContinueWith(t => Assert.True(t.IsCanceled, $\"headers {t.Status}\"));\n\n            peerInfo.HeadNumber = 2000;\n            cancellation = new CancellationTokenSource();\n            cancellation.CancelAfter(1000);\n            task = blockDownloader.DownloadBlocks(peerInfo, 0, cancellation.Token);\n            await task.ContinueWith(t => Assert.True(t.IsCanceled, $\"blocks {t.Status}\"));\n        }\n\n        [Test, MaxTime(7000)]\n        public async Task Can_cancel_adding_headers()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, new SlowHeaderValidator(), TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = Substitute.For<ISyncPeer>();\n            syncPeer.GetBlockHeaders(Arg.Any<long>(), Arg.Any<int>(), Arg.Any<int>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildHeaderResponse(ci.ArgAt<long>(0), ci.ArgAt<int>(1), Response.AllCorrect));\n\n            syncPeer.GetBlocks(Arg.Any<Keccak[]>(), Arg.Any<CancellationToken>())\n                .Returns(ci => _responseBuilder.BuildBlocksResponse(ci.ArgAt<Keccak[]>(0), Response.AllCorrect));\n\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1000;\n\n            CancellationTokenSource cancellation = new CancellationTokenSource();\n            cancellation.CancelAfter(1000);\n            Task task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, cancellation.Token);\n            await task.ContinueWith(t => Assert.True(t.IsCanceled, \"headers\"));\n\n            peerInfo.HeadNumber *= 2;\n            cancellation = new CancellationTokenSource();\n            cancellation.CancelAfter(1000);\n            task = blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, cancellation.Token);\n            await task.ContinueWith(t => Assert.True(t.IsCanceled, \"blocks\"));\n        }\n\n        private class ThrowingPeer : ISyncPeer\n        {\n            public Guid SessionId { get; }\n            public bool IsFastSyncSupported { get; }\n            public Node Node { get; }\n            public string ClientId => \"EX peer\";\n            public UInt256 TotalDifficultyOnSessionStart => UInt256.MaxValue;\n\n            public void Disconnect(DisconnectReason reason, string details)\n            {\n                throw new NotImplementedException();\n            }\n\n            public Task<BlockBody[]> GetBlocks(Keccak[] blockHashes, CancellationToken token)\n            {\n                throw new NotImplementedException();\n            }\n\n            public Task<BlockHeader[]> GetBlockHeaders(Keccak blockHash, int maxBlocks, int skip, CancellationToken token)\n            {\n                throw new NotImplementedException();\n            }\n\n            public Task<BlockHeader[]> GetBlockHeaders(long number, int maxBlocks, int skip, CancellationToken token)\n            {\n                throw new Exception();\n            }\n\n            public Task<BlockHeader> GetHeadBlockHeader(Keccak hash, CancellationToken token)\n            {\n                throw new NotImplementedException();\n            }\n\n            public void SendNewBlock(Block block)\n            {\n                throw new NotImplementedException();\n            }\n\n            public void SendNewTransaction(Transaction transaction)\n            {\n                throw new NotImplementedException();\n            }\n\n            public Task<TxReceipt[][]> GetReceipts(Keccak[] blockHash, CancellationToken token)\n            {\n                throw new NotImplementedException();\n            }\n\n            public Task<byte[][]> GetNodeData(Keccak[] hashes, CancellationToken token)\n            {\n                throw new NotImplementedException();\n            }\n        }\n\n        [Test]\n        public async Task Faults_on_get_headers_faulting()\n        {\n            BlockDownloader blockDownloader = new BlockDownloader(_blockTree, TestBlockValidator.AlwaysValid, TestSealValidator.AlwaysValid, NullSyncReport.Instance, LimboLogs.Instance);\n\n            ISyncPeer syncPeer = new ThrowingPeer();\n            PeerInfo peerInfo = new PeerInfo(syncPeer);\n            peerInfo.TotalDifficulty = UInt256.MaxValue;\n            peerInfo.HeadNumber = 1000;\n\n            await blockDownloader.DownloadHeaders(peerInfo, SyncModeSelector.FullSyncThreshold, CancellationToken.None)\n                .ContinueWith(t => Assert.True(t.IsFaulted));\n        }\n\n        private Task<BlockHeader[]> Throw(CallInfo arg)\n        {\n            throw new Exception();\n        }\n\n        [Flags]\n        private enum Response\n        {\n            Consistent = 1,\n            ValidSeals = 2,\n            NoEmptySpace = 4,\n            AllCorrect = 7,\n            JustFirst = 8,\n            AllKnown = 16,\n            TimeoutOnFullBatch = 32,\n            NoBody = 64,\n        }\n    }\n}", "idx": 5, "id": 22776, "msg": "", "proj": "NethermindEth-nethermind", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -2231,6 +2231,51 @@ void Game::playerRotateItem(uint32_t playerId, const Position& pos, uint8_t stac\n \t}\n }\n \n+void Game::playerWrapItem(uint32_t playerId, const Position& pos, uint8_t stackPos, const uint16_t spriteId)\n+{\n+\tPlayer* player = getPlayerByID(playerId);\n+\tif (!player) {\n+\t\treturn;\n+\t}\n+\n+\tThing* thing = internalGetThing(player, pos, stackPos, 0, STACKPOS_TOPDOWN_ITEM);\n+\tif (!thing) {\n+\t\treturn;\n+\t}\n+\n+\tItem* item = thing->getItem();\n+\tif (!item || item->getClientID() != spriteId || !item->isWrappable() || item->hasAttribute(ITEM_ATTRIBUTE_UNIQUEID)) {\n+\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n+\t\treturn;\n+\t}\n+\n+\tif (pos.x != 0xFFFF && !Position::areInRange<1, 1, 0>(pos, player->getPosition())) {\n+\t\tstd::forward_list<Direction> listDir;\n+\t\tif (player->getPathTo(pos, listDir, 0, 1, true, true)) {\n+\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n+\t\t\t\tthis, player->getID(), listDir)));\n+\n+\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerWrapItem, this,\n+\t\t\t\tplayerId, pos, stackPos, spriteId));\n+\t\t\tplayer->setNextWalkActionTask(task);\n+\t\t}\n+\t\telse {\n+\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n+\t\t}\n+\t\treturn;\n+\t}\n+\n+\tif (!g_events->eventPlayerOnWrapItem(player, item)) {\n+\t\treturn;\n+\t}\n+\n+\tuint16_t newId = Item::items[item->getID()].wrapTo;\n+\n+\tif (newId != 0) {\n+\t\ttransformItem(item, newId);\n+\t}\n+}\n+\n void Game::playerWriteItem(uint32_t playerId, uint32_t windowTextId, const std::string& text)\n {\n \tPlayer* player = getPlayerByID(playerId);", "y": 1, "oldf": "/**\n * The Forgotten Server - a free and open-source MMORPG server emulator\n * Copyright (C) 2016  Mark Samman <mark.samman@gmail.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n */\n\n#include \"otpch.h\"\n\n#include \"pugicast.h\"\n\n#include \"items.h\"\n#include \"commands.h\"\n#include \"creature.h\"\n#include \"monster.h\"\n#include \"game.h\"\n#include \"actions.h\"\n#include \"iologindata.h\"\n#include \"iomarket.h\"\n#include \"talkaction.h\"\n#include \"spells.h\"\n#include \"configmanager.h\"\n#include \"server.h\"\n#include \"globalevent.h\"\n#include \"bed.h\"\n#include \"scheduler.h\"\n#include \"events.h\"\n#include \"databasetasks.h\"\n\nextern ConfigManager g_config;\nextern Actions* g_actions;\nextern Chat* g_chat;\nextern TalkActions* g_talkActions;\nextern Spells* g_spells;\nextern Vocations g_vocations;\nextern GlobalEvents* g_globalEvents;\nextern Events* g_events;\n\nGame::Game()\n{\n\tofflineTrainingWindow.choices.emplace_back(\"Sword Fighting and Shielding\", SKILL_SWORD);\n\tofflineTrainingWindow.choices.emplace_back(\"Axe Fighting and Shielding\", SKILL_AXE);\n\tofflineTrainingWindow.choices.emplace_back(\"Club Fighting and Shielding\", SKILL_CLUB);\n\tofflineTrainingWindow.choices.emplace_back(\"Distance Fighting and Shielding\", SKILL_DISTANCE);\n\tofflineTrainingWindow.choices.emplace_back(\"Magic Level and Shielding\", SKILL_MAGLEVEL);\n\tofflineTrainingWindow.buttons.emplace_back(\"Okay\", 1);\n\tofflineTrainingWindow.buttons.emplace_back(\"Cancel\", 0);\n\tofflineTrainingWindow.defaultEnterButton = 1;\n\tofflineTrainingWindow.defaultEscapeButton = 0;\n\tofflineTrainingWindow.priority = true;\n}\n\nGame::~Game()\n{\n\tfor (const auto& it : guilds) {\n\t\tdelete it.second;\n\t}\n}\n\nvoid Game::start(ServiceManager* manager)\n{\n\tserviceManager = manager;\n\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_LIGHTINTERVAL, std::bind(&Game::checkLight, this)));\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_CREATURE_THINK_INTERVAL, std::bind(&Game::checkCreatures, this, 0)));\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_DECAYINTERVAL, std::bind(&Game::checkDecay, this)));\n}\n\nGameState_t Game::getGameState() const\n{\n\treturn gameState;\n}\n\nvoid Game::setWorldType(WorldType_t type)\n{\n\tworldType = type;\n}\n\nvoid Game::setGameState(GameState_t newState)\n{\n\tif (gameState == GAME_STATE_SHUTDOWN) {\n\t\treturn;    //this cannot be stopped\n\t}\n\n\tif (gameState == newState) {\n\t\treturn;\n\t}\n\n\tgameState = newState;\n\tswitch (newState) {\n\t\tcase GAME_STATE_INIT: {\n\t\t\tcommands.loadFromXml();\n\n\t\t\tloadExperienceStages();\n\n\t\t\tgroups.load();\n\t\t\tg_chat->load();\n\n\t\t\tmap.spawns.startup();\n\n\t\t\traids.loadFromXml();\n\t\t\traids.startup();\n\n\t\t\tquests.loadFromXml();\n\t\t\tmounts.loadFromXml();\n\n\t\t\tloadMotdNum();\n\t\t\tloadPlayersRecord();\n\n\t\t\tg_globalEvents->startup();\n\t\t\tbreak;\n\t\t}\n\n\t\tcase GAME_STATE_SHUTDOWN: {\n\t\t\tg_globalEvents->execute(GLOBALEVENT_SHUTDOWN);\n\n\t\t\t//kick all players that are still online\n\t\t\tauto it = players.begin();\n\t\t\twhile (it != players.end()) {\n\t\t\t\tit->second->kickPlayer(true);\n\t\t\t\tit = players.begin();\n\t\t\t}\n\n\t\t\tsaveMotdNum();\n\t\t\tsaveGameState();\n\n\t\t\tg_dispatcher.addTask(\n\t\t\t\tcreateTask(std::bind(&Game::shutdown, this)));\n\n\t\t\tg_scheduler.stop();\n\t\t\tg_databaseTasks.stop();\n\t\t\tg_dispatcher.stop();\n\t\t\tbreak;\n\t\t}\n\n\t\tcase GAME_STATE_CLOSED: {\n\t\t\t/* kick all players without the CanAlwaysLogin flag */\n\t\t\tauto it = players.begin();\n\t\t\twhile (it != players.end()) {\n\t\t\t\tif (!it->second->hasFlag(PlayerFlag_CanAlwaysLogin)) {\n\t\t\t\t\tit->second->kickPlayer(true);\n\t\t\t\t\tit = players.begin();\n\t\t\t\t} else {\n\t\t\t\t\t++it;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsaveGameState();\n\t\t\tbreak;\n\t\t}\n\n\t\tdefault:\n\t\t\tbreak;\n\t}\n}\n\nvoid Game::saveGameState()\n{\n\tif (gameState == GAME_STATE_NORMAL) {\n\t\tsetGameState(GAME_STATE_MAINTAIN);\n\t}\n\n\tstd::cout << \"Saving server...\" << std::endl;\n\n\tfor (const auto& it : players) {\n\t\tit.second->loginPosition = it.second->getPosition();\n\t\tIOLoginData::savePlayer(it.second);\n\t}\n\n\tMap::save();\n\n\tif (gameState == GAME_STATE_MAINTAIN) {\n\t\tsetGameState(GAME_STATE_NORMAL);\n\t}\n}\n\nbool Game::loadMainMap(const std::string& filename)\n{\n\tMonster::despawnRange = g_config.getNumber(ConfigManager::DEFAULT_DESPAWNRANGE);\n\tMonster::despawnRadius = g_config.getNumber(ConfigManager::DEFAULT_DESPAWNRADIUS);\n\treturn map.loadMap(\"data/world/\" + filename + \".otbm\", true);\n}\n\nvoid Game::loadMap(const std::string& path)\n{\n\tmap.loadMap(path, false);\n}\n\nCylinder* Game::internalGetCylinder(Player* player, const Position& pos) const\n{\n\tif (pos.x != 0xFFFF) {\n\t\treturn map.getTile(pos);\n\t}\n\n\t//container\n\tif (pos.y & 0x40) {\n\t\tuint8_t from_cid = pos.y & 0x0F;\n\t\treturn player->getContainerByID(from_cid);\n\t}\n\n\t//inventory\n\treturn player;\n}\n\nThing* Game::internalGetThing(Player* player, const Position& pos, int32_t index, uint32_t spriteId, stackPosType_t type) const\n{\n\tif (pos.x != 0xFFFF) {\n\t\tTile* tile = map.getTile(pos);\n\t\tif (!tile) {\n\t\t\treturn nullptr;\n\t\t}\n\n\t\tThing* thing;\n\t\tswitch (type) {\n\t\t\tcase STACKPOS_LOOK: {\n\t\t\t\treturn tile->getTopVisibleThing(player);\n\t\t\t}\n\n\t\t\tcase STACKPOS_MOVE: {\n\t\t\t\tItem* item = tile->getTopDownItem();\n\t\t\t\tif (item && item->isMoveable()) {\n\t\t\t\t\tthing = item;\n\t\t\t\t} else {\n\t\t\t\t\tthing = tile->getTopVisibleCreature(player);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase STACKPOS_USEITEM: {\n\t\t\t\tthing = tile->getUseItem();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase STACKPOS_TOPDOWN_ITEM: {\n\t\t\t\tthing = tile->getTopDownItem();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcase STACKPOS_USETARGET: {\n\t\t\t\tthing = tile->getTopVisibleCreature(player);\n\t\t\t\tif (!thing) {\n\t\t\t\t\tthing = tile->getUseItem();\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdefault: {\n\t\t\t\tthing = nullptr;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (player && tile->hasFlag(TILESTATE_SUPPORTS_HANGABLE)) {\n\t\t\t//do extra checks here if the thing is accessable\n\t\t\tif (thing && thing->getItem()) {\n\t\t\t\tif (tile->hasProperty(CONST_PROP_ISVERTICAL)) {\n\t\t\t\t\tif (player->getPosition().x + 1 == tile->getPosition().x) {\n\t\t\t\t\t\tthing = nullptr;\n\t\t\t\t\t}\n\t\t\t\t} else { // horizontal\n\t\t\t\t\tif (player->getPosition().y + 1 == tile->getPosition().y) {\n\t\t\t\t\t\tthing = nullptr;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn thing;\n\t}\n\n\t//container\n\tif (pos.y & 0x40) {\n\t\tuint8_t fromCid = pos.y & 0x0F;\n\n\t\tContainer* parentContainer = player->getContainerByID(fromCid);\n\t\tif (!parentContainer) {\n\t\t\treturn nullptr;\n\t\t}\n\n\t\tif (parentContainer->getID() == ITEM_BROWSEFIELD) {\n\t\t\tTile* tile = parentContainer->getTile();\n\t\t\tif (tile && tile->hasFlag(TILESTATE_SUPPORTS_HANGABLE)) {\n\t\t\t\tif (tile->hasProperty(CONST_PROP_ISVERTICAL)) {\n\t\t\t\t\tif (player->getPosition().x + 1 == tile->getPosition().x) {\n\t\t\t\t\t\treturn nullptr;\n\t\t\t\t\t}\n\t\t\t\t} else { // horizontal\n\t\t\t\t\tif (player->getPosition().y + 1 == tile->getPosition().y) {\n\t\t\t\t\t\treturn nullptr;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tuint8_t slot = pos.z;\n\t\treturn parentContainer->getItemByIndex(player->getContainerIndex(fromCid) + slot);\n\t} else if (pos.y == 0 && pos.z == 0) {\n\t\tconst ItemType& it = Item::items.getItemIdByClientId(spriteId);\n\t\tif (it.id == 0) {\n\t\t\treturn nullptr;\n\t\t}\n\n\t\tint32_t subType;\n\t\tif (it.isFluidContainer() && index < static_cast<int32_t>(sizeof(reverseFluidMap) / sizeof(uint8_t))) {\n\t\t\tsubType = reverseFluidMap[index];\n\t\t} else {\n\t\t\tsubType = -1;\n\t\t}\n\n\t\treturn findItemOfType(player, it.id, true, subType);\n\t}\n\n\t//inventory\n\tslots_t slot = static_cast<slots_t>(pos.y);\n\treturn player->getInventoryItem(slot);\n}\n\nvoid Game::internalGetPosition(Item* item, Position& pos, uint8_t& stackpos)\n{\n\tpos.x = 0;\n\tpos.y = 0;\n\tpos.z = 0;\n\tstackpos = 0;\n\n\tCylinder* topParent = item->getTopParent();\n\tif (topParent) {\n\t\tif (Player* player = dynamic_cast<Player*>(topParent)) {\n\t\t\tpos.x = 0xFFFF;\n\n\t\t\tContainer* container = dynamic_cast<Container*>(item->getParent());\n\t\t\tif (container) {\n\t\t\t\tpos.y = static_cast<uint16_t>(0x40) | static_cast<uint16_t>(player->getContainerID(container));\n\t\t\t\tpos.z = container->getThingIndex(item);\n\t\t\t\tstackpos = pos.z;\n\t\t\t} else {\n\t\t\t\tpos.y = player->getThingIndex(item);\n\t\t\t\tstackpos = pos.y;\n\t\t\t}\n\t\t} else if (Tile* tile = topParent->getTile()) {\n\t\t\tpos = tile->getPosition();\n\t\t\tstackpos = tile->getThingIndex(item);\n\t\t}\n\t}\n}\n\nCreature* Game::getCreatureByID(uint32_t id)\n{\n\tif (id <= Player::playerAutoID) {\n\t\treturn getPlayerByID(id);\n\t} else if (id <= Monster::monsterAutoID) {\n\t\treturn getMonsterByID(id);\n\t} else if (id <= Npc::npcAutoID) {\n\t\treturn getNpcByID(id);\n\t}\n\treturn nullptr;\n}\n\nMonster* Game::getMonsterByID(uint32_t id)\n{\n\tif (id == 0) {\n\t\treturn nullptr;\n\t}\n\n\tauto it = monsters.find(id);\n\tif (it == monsters.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nNpc* Game::getNpcByID(uint32_t id)\n{\n\tif (id == 0) {\n\t\treturn nullptr;\n\t}\n\n\tauto it = npcs.find(id);\n\tif (it == npcs.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nPlayer* Game::getPlayerByID(uint32_t id)\n{\n\tif (id == 0) {\n\t\treturn nullptr;\n\t}\n\n\tauto it = players.find(id);\n\tif (it == players.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nCreature* Game::getCreatureByName(const std::string& s)\n{\n\tif (s.empty()) {\n\t\treturn nullptr;\n\t}\n\n\tconst std::string& lowerCaseName = asLowerCaseString(s);\n\n\tauto m_it = mappedPlayerNames.find(lowerCaseName);\n\tif (m_it != mappedPlayerNames.end()) {\n\t\treturn m_it->second;\n\t}\n\n\tfor (const auto& it : npcs) {\n\t\tif (lowerCaseName == asLowerCaseString(it.second->getName())) {\n\t\t\treturn it.second;\n\t\t}\n\t}\n\n\tfor (const auto& it : monsters) {\n\t\tif (lowerCaseName == asLowerCaseString(it.second->getName())) {\n\t\t\treturn it.second;\n\t\t}\n\t}\n\treturn nullptr;\n}\n\nNpc* Game::getNpcByName(const std::string& s)\n{\n\tif (s.empty()) {\n\t\treturn nullptr;\n\t}\n\n\tconst char* npcName = s.c_str();\n\tfor (const auto& it : npcs) {\n\t\tif (strcasecmp(npcName, it.second->getName().c_str()) == 0) {\n\t\t\treturn it.second;\n\t\t}\n\t}\n\treturn nullptr;\n}\n\nPlayer* Game::getPlayerByName(const std::string& s)\n{\n\tif (s.empty()) {\n\t\treturn nullptr;\n\t}\n\n\tauto it = mappedPlayerNames.find(asLowerCaseString(s));\n\tif (it == mappedPlayerNames.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nPlayer* Game::getPlayerByGUID(const uint32_t& guid)\n{\n\tif (guid == 0) {\n\t\treturn nullptr;\n\t}\n\n\tfor (const auto& it : players) {\n\t\tif (guid == it.second->getGUID()) {\n\t\t\treturn it.second;\n\t\t}\n\t}\n\treturn nullptr;\n}\n\nReturnValue Game::getPlayerByNameWildcard(const std::string& s, Player*& player)\n{\n\tsize_t strlen = s.length();\n\tif (strlen == 0 || strlen > 20) {\n\t\treturn RETURNVALUE_PLAYERWITHTHISNAMEISNOTONLINE;\n\t}\n\n\tif (s.back() == '~') {\n\t\tconst std::string& query = asLowerCaseString(s.substr(0, strlen - 1));\n\t\tstd::string result;\n\t\tReturnValue ret = wildcardTree.findOne(query, result);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\treturn ret;\n\t\t}\n\n\t\tplayer = getPlayerByName(result);\n\t} else {\n\t\tplayer = getPlayerByName(s);\n\t}\n\n\tif (!player) {\n\t\treturn RETURNVALUE_PLAYERWITHTHISNAMEISNOTONLINE;\n\t}\n\n\treturn RETURNVALUE_NOERROR;\n}\n\nPlayer* Game::getPlayerByAccount(uint32_t acc)\n{\n\tfor (const auto& it : players) {\n\t\tif (it.second->getAccount() == acc) {\n\t\t\treturn it.second;\n\t\t}\n\t}\n\treturn nullptr;\n}\n\nbool Game::internalPlaceCreature(Creature* creature, const Position& pos, bool extendedPos /*=false*/, bool forced /*= false*/)\n{\n\tif (creature->getParent() != nullptr) {\n\t\treturn false;\n\t}\n\n\tif (!map.placeCreature(pos, creature, extendedPos, forced)) {\n\t\treturn false;\n\t}\n\n\tcreature->incrementReferenceCounter();\n\tcreature->setID();\n\tcreature->addList();\n\treturn true;\n}\n\nbool Game::placeCreature(Creature* creature, const Position& pos, bool extendedPos /*=false*/, bool forced /*= false*/)\n{\n\tif (!internalPlaceCreature(creature, pos, extendedPos, forced)) {\n\t\treturn false;\n\t}\n\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true);\n\tfor (Creature* spectator : list) {\n\t\tif (Player* tmpPlayer = spectator->getPlayer()) {\n\t\t\ttmpPlayer->sendCreatureAppear(creature, creature->getPosition(), true);\n\t\t}\n\t}\n\n\tfor (Creature* spectator : list) {\n\t\tspectator->onCreatureAppear(creature, true);\n\t}\n\n\tcreature->getParent()->postAddNotification(creature, nullptr, 0);\n\n\taddCreatureCheck(creature);\n\tcreature->onPlacedCreature();\n\treturn true;\n}\n\nbool Game::removeCreature(Creature* creature, bool isLogout/* = true*/)\n{\n\tif (creature->isRemoved()) {\n\t\treturn false;\n\t}\n\n\tTile* tile = creature->getTile();\n\n\tstd::vector<int32_t> oldStackPosVector;\n\n\tSpectatorVec list;\n\tmap.getSpectators(list, tile->getPosition(), true);\n\tfor (Creature* spectator : list) {\n\t\tif (Player* player = spectator->getPlayer()) {\n\t\t\toldStackPosVector.push_back(player->canSeeCreature(creature) ? tile->getStackposOfCreature(player, creature) : -1);\n\t\t}\n\t}\n\n\ttile->removeCreature(creature);\n\n\tconst Position& tilePosition = tile->getPosition();\n\n\t//send to client\n\tsize_t i = 0;\n\tfor (Creature* spectator : list) {\n\t\tif (Player* player = spectator->getPlayer()) {\n\t\t\tplayer->sendRemoveTileThing(tilePosition, oldStackPosVector[i++]);\n\t\t}\n\t}\n\n\t//event method\n\tfor (Creature* spectator : list) {\n\t\tspectator->onRemoveCreature(creature, isLogout);\n\t}\n\n\tcreature->getParent()->postRemoveNotification(creature, nullptr, 0);\n\n\tcreature->removeList();\n\tcreature->setRemoved();\n\tReleaseCreature(creature);\n\n\tremoveCreatureCheck(creature);\n\n\tfor (Creature* summon : creature->summons) {\n\t\tsummon->setLossSkill(false);\n\t\tremoveCreature(summon);\n\t}\n\treturn true;\n}\n\nvoid Game::playerMoveThing(uint32_t playerId, const Position& fromPos,\n                           uint16_t spriteId, uint8_t fromStackPos, const Position& toPos, uint8_t count)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tuint8_t fromIndex = 0;\n\tif (fromPos.x == 0xFFFF) {\n\t\tif (fromPos.y & 0x40) {\n\t\t\tfromIndex = fromPos.z;\n\t\t} else {\n\t\t\tfromIndex = static_cast<uint8_t>(fromPos.y);\n\t\t}\n\t} else {\n\t\tfromIndex = fromStackPos;\n\t}\n\n\tThing* thing = internalGetThing(player, fromPos, fromIndex, 0, STACKPOS_MOVE);\n\tif (!thing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tif (Creature* movingCreature = thing->getCreature()) {\n\t\tTile* tile = map.getTile(toPos);\n\t\tif (!tile) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\treturn;\n\t\t}\n\n\t\tif (Position::areInRange<1, 1, 0>(movingCreature->getPosition(), player->getPosition())) {\n\t\t\tSchedulerTask* task = createSchedulerTask(1000,\n\t\t\t                      std::bind(&Game::playerMoveCreatureByID, this, player->getID(),\n\t\t\t                                  movingCreature->getID(), movingCreature->getPosition(), tile->getPosition()));\n\t\t\tplayer->setNextActionTask(task);\n\t\t} else {\n\t\t\tplayerMoveCreature(player, movingCreature, movingCreature->getPosition(), tile);\n\t\t}\n\t} else if (thing->getItem()) {\n\t\tCylinder* toCylinder = internalGetCylinder(player, toPos);\n\t\tif (!toCylinder) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\treturn;\n\t\t}\n\n\t\tplayerMoveItem(player, fromPos, spriteId, fromStackPos, toPos, count, thing->getItem(), toCylinder);\n\t}\n}\n\nvoid Game::playerMoveCreatureByID(uint32_t playerId, uint32_t movingCreatureId, const Position& movingCreatureOrigPos, const Position& toPos)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tCreature* movingCreature = getCreatureByID(movingCreatureId);\n\tif (!movingCreature) {\n\t\treturn;\n\t}\n\n\tTile* toTile = map.getTile(toPos);\n\tif (!toTile) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tplayerMoveCreature(player, movingCreature, movingCreatureOrigPos, toTile);\n}\n\nvoid Game::playerMoveCreature(Player* player, Creature* movingCreature, const Position& movingCreatureOrigPos, Tile* toTile)\n{\n\tif (!player->canDoAction()) {\n\t\tuint32_t delay = player->getNextActionTime();\n\t\tSchedulerTask* task = createSchedulerTask(delay, std::bind(&Game::playerMoveCreatureByID,\n\t\t\tthis, player->getID(), movingCreature->getID(), movingCreatureOrigPos, toTile->getPosition()));\n\t\tplayer->setNextActionTask(task);\n\t\treturn;\n\t}\n\n\tplayer->setNextActionTask(nullptr);\n\n\tif (!Position::areInRange<1, 1, 0>(movingCreatureOrigPos, player->getPosition())) {\n\t\t//need to walk to the creature first before moving it\n\t\tstd::forward_list<Direction> listDir;\n\t\tif (player->getPathTo(movingCreatureOrigPos, listDir, 0, 1, true, true)) {\n\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t                                this, player->getID(), listDir)));\n\t\t\tSchedulerTask* task = createSchedulerTask(1500, std::bind(&Game::playerMoveCreatureByID, this,\n\t\t\t\tplayer->getID(), movingCreature->getID(), movingCreatureOrigPos, toTile->getPosition()));\n\t\t\tplayer->setNextWalkActionTask(task);\n\t\t} else {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t}\n\t\treturn;\n\t}\n\n\tif ((!movingCreature->isPushable() && !player->hasFlag(PlayerFlag_CanPushAllCreatures)) ||\n\t        (movingCreature->isInGhostMode() && !player->isAccessPlayer())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTMOVEABLE);\n\t\treturn;\n\t}\n\n\t//check throw distance\n\tconst Position& movingCreaturePos = movingCreature->getPosition();\n\tconst Position& toPos = toTile->getPosition();\n\tif ((Position::getDistanceX(movingCreaturePos, toPos) > movingCreature->getThrowRange()) || (Position::getDistanceY(movingCreaturePos, toPos) > movingCreature->getThrowRange()) || (Position::getDistanceZ(movingCreaturePos, toPos) * 4 > movingCreature->getThrowRange())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_DESTINATIONOUTOFREACH);\n\t\treturn;\n\t}\n\n\tif (player != movingCreature) {\n\t\tif (toTile->hasFlag(TILESTATE_BLOCKPATH)) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTENOUGHROOM);\n\t\t\treturn;\n\t\t} else if ((movingCreature->getZone() == ZONE_PROTECTION && !toTile->hasFlag(TILESTATE_PROTECTIONZONE)) || (movingCreature->getZone() == ZONE_NOPVP && !toTile->hasFlag(TILESTATE_NOPVPZONE))) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\treturn;\n\t\t} else {\n\t\t\tif (CreatureVector* tileCreatures = toTile->getCreatures()) {\n\t\t\t\tfor (Creature* tileCreature : *tileCreatures) {\n\t\t\t\t\tif (!tileCreature->isInGhostMode()) {\n\t\t\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTENOUGHROOM);\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tNpc* movingNpc = movingCreature->getNpc();\n\t\t\tif (movingNpc && !Spawns::isInZone(movingNpc->getMasterPos(), movingNpc->getMasterRadius(), toPos)) {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTENOUGHROOM);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!g_events->eventPlayerOnMoveCreature(player, movingCreature, movingCreaturePos, toPos)) {\n\t\treturn;\n\t}\n\n\tReturnValue ret = internalMoveCreature(*movingCreature, *toTile);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tplayer->sendCancelMessage(ret);\n\t}\n}\n\nReturnValue Game::internalMoveCreature(Creature* creature, Direction direction, uint32_t flags /*= 0*/)\n{\n\tcreature->setLastPosition(creature->getPosition());\n\tconst Position& currentPos = creature->getPosition();\n\tPosition destPos = getNextPosition(direction, currentPos);\n\n\tbool diagonalMovement = (direction & DIRECTION_DIAGONAL_MASK) != 0;\n\tif (creature->getPlayer() && !diagonalMovement) {\n\t\t//try go up\n\t\tif (currentPos.z != 8 && creature->getTile()->hasHeight(3)) {\n\t\t\tTile* tmpTile = map.getTile(currentPos.x, currentPos.y, currentPos.getZ() - 1);\n\t\t\tif (tmpTile == nullptr || (tmpTile->getGround() == nullptr && !tmpTile->hasFlag(TILESTATE_BLOCKSOLID))) {\n\t\t\t\ttmpTile = map.getTile(destPos.x, destPos.y, destPos.getZ() - 1);\n\t\t\t\tif (tmpTile && tmpTile->getGround() && !tmpTile->hasFlag(TILESTATE_BLOCKSOLID)) {\n\t\t\t\t\tflags |= FLAG_IGNOREBLOCKITEM | FLAG_IGNOREBLOCKCREATURE;\n\n\t\t\t\t\tif (!tmpTile->hasFlag(TILESTATE_FLOORCHANGE)) {\n\t\t\t\t\t\tdestPos.z--;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t//try go down\n\t\t\tTile* tmpTile = map.getTile(destPos.x, destPos.y, destPos.z);\n\t\t\tif (currentPos.z != 7 && (tmpTile == nullptr || (tmpTile->getGround() == nullptr && !tmpTile->hasFlag(TILESTATE_BLOCKSOLID)))) {\n\t\t\t\ttmpTile = map.getTile(destPos.x, destPos.y, destPos.z + 1);\n\t\t\t\tif (tmpTile && tmpTile->hasHeight(3)) {\n\t\t\t\t\tflags |= FLAG_IGNOREBLOCKITEM | FLAG_IGNOREBLOCKCREATURE;\n\t\t\t\t\tdestPos.z++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tTile* toTile = map.getTile(destPos);\n\tif (!toTile) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\treturn internalMoveCreature(*creature, *toTile, flags);\n}\n\nReturnValue Game::internalMoveCreature(Creature& creature, Tile& toTile, uint32_t flags /*= 0*/)\n{\n\t//check if we can move the creature to the destination\n\tReturnValue ret = toTile.queryAdd(0, creature, 1, flags);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\tmap.moveCreature(creature, toTile);\n\tif (creature.getParent() != &toTile) {\n\t\treturn RETURNVALUE_NOERROR;\n\t}\n\n\tint32_t index = 0;\n\tItem* toItem = nullptr;\n\tTile* subCylinder = nullptr;\n\tTile* toCylinder = &toTile;\n\tTile* fromCylinder = nullptr;\n\tuint32_t n = 0;\n\n\twhile ((subCylinder = toCylinder->queryDestination(index, creature, &toItem, flags)) != toCylinder) {\n\t\tmap.moveCreature(creature, *subCylinder);\n\n\t\tif (creature.getParent() != subCylinder) {\n\t\t\t//could happen if a script move the creature\n\t\t\tfromCylinder = nullptr;\n\t\t\tbreak;\n\t\t}\n\n\t\tfromCylinder = toCylinder;\n\t\ttoCylinder = subCylinder;\n\t\tflags = 0;\n\n\t\t//to prevent infinite loop\n\t\tif (++n >= MAP_MAX_LAYERS) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (fromCylinder) {\n\t\tconst Position& fromPosition = fromCylinder->getPosition();\n\t\tconst Position& toPosition = toCylinder->getPosition();\n\t\tif (fromPosition.z != toPosition.z && (fromPosition.x != toPosition.x || fromPosition.y != toPosition.y)) {\n\t\t\tDirection dir = getDirectionTo(fromPosition, toPosition);\n\t\t\tif ((dir & DIRECTION_DIAGONAL_MASK) == 0) {\n\t\t\t\tinternalCreatureTurn(&creature, dir);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn RETURNVALUE_NOERROR;\n}\n\nvoid Game::playerMoveItemByPlayerID(uint32_t playerId, const Position& fromPos, uint16_t spriteId, uint8_t fromStackPos, const Position& toPos, uint8_t count)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\tplayerMoveItem(player, fromPos, spriteId, fromStackPos, toPos, count, nullptr, nullptr);\n}\n\nvoid Game::playerMoveItem(Player* player, const Position& fromPos,\n                          uint16_t spriteId, uint8_t fromStackPos, const Position& toPos, uint8_t count, Item* item, Cylinder* toCylinder)\n{\n\tif (!player->canDoAction()) {\n\t\tuint32_t delay = player->getNextActionTime();\n\t\tSchedulerTask* task = createSchedulerTask(delay, std::bind(&Game::playerMoveItemByPlayerID, this,\n\t\t                      player->getID(), fromPos, spriteId, fromStackPos, toPos, count));\n\t\tplayer->setNextActionTask(task);\n\t\treturn;\n\t}\n\n\tplayer->setNextActionTask(nullptr);\n\n\tif (item == nullptr) {\n\t\tuint8_t fromIndex = 0;\n\t\tif (fromPos.x == 0xFFFF) {\n\t\t\tif (fromPos.y & 0x40) {\n\t\t\t\tfromIndex = fromPos.z;\n\t\t\t} else {\n\t\t\t\tfromIndex = static_cast<uint8_t>(fromPos.y);\n\t\t\t}\n\t\t} else {\n\t\t\tfromIndex = fromStackPos;\n\t\t}\n\n\t\tThing* thing = internalGetThing(player, fromPos, fromIndex, 0, STACKPOS_MOVE);\n\t\tif (!thing || !thing->getItem()) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\treturn;\n\t\t}\n\n\t\titem = thing->getItem();\n\t}\n\n\tif (item->getClientID() != spriteId) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tCylinder* fromCylinder = internalGetCylinder(player, fromPos);\n\tif (fromCylinder == nullptr) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tif (toCylinder == nullptr) {\n\t\ttoCylinder = internalGetCylinder(player, toPos);\n\t\tif (toCylinder == nullptr) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!item->isPushable() || item->hasAttribute(ITEM_ATTRIBUTE_UNIQUEID)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTMOVEABLE);\n\t\treturn;\n\t}\n\n\tconst Position& playerPos = player->getPosition();\n\tconst Position& mapFromPos = fromCylinder->getTile()->getPosition();\n\tif (playerPos.z != mapFromPos.z) {\n\t\tplayer->sendCancelMessage(playerPos.z > mapFromPos.z ? RETURNVALUE_FIRSTGOUPSTAIRS : RETURNVALUE_FIRSTGODOWNSTAIRS);\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<1, 1>(playerPos, mapFromPos)) {\n\t\t//need to walk to the item first before using it\n\t\tstd::forward_list<Direction> listDir;\n\t\tif (player->getPathTo(item->getPosition(), listDir, 0, 1, true, true)) {\n\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerMoveItemByPlayerID, this,\n\t\t\t                      player->getID(), fromPos, spriteId, fromStackPos, toPos, count));\n\t\t\tplayer->setNextWalkActionTask(task);\n\t\t} else {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t}\n\t\treturn;\n\t}\n\n\tconst Tile* toCylinderTile = toCylinder->getTile();\n\tconst Position& mapToPos = toCylinderTile->getPosition();\n\n\t//hangable item specific code\n\tif (item->isHangable() && toCylinderTile->hasFlag(TILESTATE_SUPPORTS_HANGABLE)) {\n\t\t//destination supports hangable objects so need to move there first\n\t\tbool vertical = toCylinderTile->hasProperty(CONST_PROP_ISVERTICAL);\n\t\tif (vertical) {\n\t\t\tif (playerPos.x + 1 == mapToPos.x) {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else { // horizontal\n\t\t\tif (playerPos.y + 1 == mapToPos.y) {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tif (!Position::areInRange<1, 1, 0>(playerPos, mapToPos)) {\n\t\t\tPosition walkPos = mapToPos;\n\t\t\tif (vertical) {\n\t\t\t\twalkPos.x++;\n\t\t\t} else {\n\t\t\t\twalkPos.y++;\n\t\t\t}\n\n\t\t\tPosition itemPos = fromPos;\n\t\t\tuint8_t itemStackPos = fromStackPos;\n\n\t\t\tif (fromPos.x != 0xFFFF && Position::areInRange<1, 1>(mapFromPos, playerPos)\n\t\t\t        && !Position::areInRange<1, 1, 0>(mapFromPos, walkPos)) {\n\t\t\t\t//need to pickup the item first\n\t\t\t\tItem* moveItem = nullptr;\n\n\t\t\t\tReturnValue ret = internalMoveItem(fromCylinder, player, INDEX_WHEREEVER, item, count, &moveItem);\n\t\t\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\t\t\tplayer->sendCancelMessage(ret);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\t//changing the position since its now in the inventory of the player\n\t\t\t\tinternalGetPosition(moveItem, itemPos, itemStackPos);\n\t\t\t}\n\n\t\t\tstd::forward_list<Direction> listDir;\n\t\t\tif (player->getPathTo(walkPos, listDir, 0, 0, true, true)) {\n\t\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerMoveItemByPlayerID, this,\n\t\t\t\t                      player->getID(), itemPos, spriteId, itemStackPos, toPos, count));\n\t\t\t\tplayer->setNextWalkActionTask(task);\n\t\t\t} else {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif ((Position::getDistanceX(playerPos, mapToPos) > item->getThrowRange()) ||\n\t        (Position::getDistanceY(playerPos, mapToPos) > item->getThrowRange()) ||\n\t        (Position::getDistanceZ(mapFromPos, mapToPos) * 4 > item->getThrowRange())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_DESTINATIONOUTOFREACH);\n\t\treturn;\n\t}\n\n\tif (!canThrowObjectTo(mapFromPos, mapToPos)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CANNOTTHROW);\n\t\treturn;\n\t}\n\n\tif (!g_events->eventPlayerOnMoveItem(player, item, count, fromPos, toPos, fromCylinder, toCylinder)) {\n\t\treturn;\n\t}\n\n\tuint8_t toIndex = 0;\n\tif (toPos.x == 0xFFFF) {\n\t\tif (toPos.y & 0x40) {\n\t\t\ttoIndex = toPos.z;\n\t\t} else {\n\t\t\ttoIndex = static_cast<uint8_t>(toPos.y);\n\t\t}\n\t}\n\n\tReturnValue ret = internalMoveItem(fromCylinder, toCylinder, toIndex, item, count, nullptr, 0, player);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tplayer->sendCancelMessage(ret);\n\t}\n}\n\nReturnValue Game::internalMoveItem(Cylinder* fromCylinder, Cylinder* toCylinder, int32_t index,\n                                   Item* item, uint32_t count, Item** _moveItem, uint32_t flags /*= 0*/, Creature* actor/* = nullptr*/, Item* tradeItem/* = nullptr*/)\n{\n\tTile* fromTile = fromCylinder->getTile();\n\tif (fromTile) {\n\t\tauto it = browseFields.find(fromTile);\n\t\tif (it != browseFields.end() && it->second == fromCylinder) {\n\t\t\tfromCylinder = fromTile;\n\t\t}\n\t}\n\n\tItem* toItem = nullptr;\n\n\tCylinder* subCylinder;\n\tint floorN = 0;\n\n\twhile ((subCylinder = toCylinder->queryDestination(index, *item, &toItem, flags)) != toCylinder) {\n\t\ttoCylinder = subCylinder;\n\t\tflags = 0;\n\n\t\t//to prevent infinite loop\n\t\tif (++floorN >= MAP_MAX_LAYERS) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t//destination is the same as the source?\n\tif (item == toItem) {\n\t\treturn RETURNVALUE_NOERROR;    //silently ignore move\n\t}\n\n\t//check if we can add this item\n\tReturnValue ret = toCylinder->queryAdd(index, *item, count, flags, actor);\n\tif (ret == RETURNVALUE_NEEDEXCHANGE) {\n\t\t//check if we can add it to source cylinder\n\t\tret = fromCylinder->queryAdd(fromCylinder->getThingIndex(item), *toItem, toItem->getItemCount(), 0);\n\t\tif (ret == RETURNVALUE_NOERROR) {\n\t\t\t//check how much we can move\n\t\t\tuint32_t maxExchangeQueryCount = 0;\n\t\t\tReturnValue retExchangeMaxCount = fromCylinder->queryMaxCount(INDEX_WHEREEVER, *toItem, toItem->getItemCount(), maxExchangeQueryCount, 0);\n\n\t\t\tif (retExchangeMaxCount != RETURNVALUE_NOERROR && maxExchangeQueryCount == 0) {\n\t\t\t\treturn retExchangeMaxCount;\n\t\t\t}\n\n\t\t\tif (toCylinder->queryRemove(*toItem, toItem->getItemCount(), flags) == RETURNVALUE_NOERROR) {\n\t\t\t\tint32_t oldToItemIndex = toCylinder->getThingIndex(toItem);\n\t\t\t\ttoCylinder->removeThing(toItem, toItem->getItemCount());\n\t\t\t\tfromCylinder->addThing(toItem);\n\n\t\t\t\tif (oldToItemIndex != -1) {\n\t\t\t\t\ttoCylinder->postRemoveNotification(toItem, fromCylinder, oldToItemIndex);\n\t\t\t\t}\n\n\t\t\t\tint32_t newToItemIndex = fromCylinder->getThingIndex(toItem);\n\t\t\t\tif (newToItemIndex != -1) {\n\t\t\t\t\tfromCylinder->postAddNotification(toItem, toCylinder, newToItemIndex);\n\t\t\t\t}\n\n\t\t\t\tret = toCylinder->queryAdd(index, *item, count, flags);\n\t\t\t\ttoItem = nullptr;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\t//check how much we can move\n\tuint32_t maxQueryCount = 0;\n\tReturnValue retMaxCount = toCylinder->queryMaxCount(index, *item, count, maxQueryCount, flags);\n\tif (retMaxCount != RETURNVALUE_NOERROR && maxQueryCount == 0) {\n\t\treturn retMaxCount;\n\t}\n\n\tuint32_t m;\n\tif (item->isStackable()) {\n\t\tm = std::min<uint32_t>(count, maxQueryCount);\n\t} else {\n\t\tm = maxQueryCount;\n\t}\n\n\tItem* moveItem = item;\n\n\t//check if we can remove this item\n\tret = fromCylinder->queryRemove(*item, m, flags);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\tif (tradeItem) {\n\t\tif (toCylinder->getItem() == tradeItem) {\n\t\t\treturn RETURNVALUE_NOTENOUGHROOM;\n\t\t}\n\n\t\tCylinder* tmpCylinder = toCylinder->getParent();\n\t\twhile (tmpCylinder) {\n\t\t\tif (tmpCylinder->getItem() == tradeItem) {\n\t\t\t\treturn RETURNVALUE_NOTENOUGHROOM;\n\t\t\t}\n\n\t\t\ttmpCylinder = tmpCylinder->getParent();\n\t\t}\n\t}\n\n\t//remove the item\n\tint32_t itemIndex = fromCylinder->getThingIndex(item);\n\tItem* updateItem = nullptr;\n\tfromCylinder->removeThing(item, m);\n\n\t//update item(s)\n\tif (item->isStackable()) {\n\t\tuint32_t n;\n\n\t\tif (item->equals(toItem)) {\n\t\t\tn = std::min<uint32_t>(100 - toItem->getItemCount(), m);\n\t\t\ttoCylinder->updateThing(toItem, toItem->getID(), toItem->getItemCount() + n);\n\t\t\tupdateItem = toItem;\n\t\t} else {\n\t\t\tn = 0;\n\t\t}\n\n\t\tint32_t newCount = m - n;\n\t\tif (newCount > 0) {\n\t\t\tmoveItem = item->clone();\n\t\t\tmoveItem->setItemCount(newCount);\n\t\t} else {\n\t\t\tmoveItem = nullptr;\n\t\t}\n\n\t\tif (item->isRemoved()) {\n\t\t\tReleaseItem(item);\n\t\t}\n\t}\n\n\t//add item\n\tif (moveItem /*m - n > 0*/) {\n\t\ttoCylinder->addThing(index, moveItem);\n\t}\n\n\tif (itemIndex != -1) {\n\t\tfromCylinder->postRemoveNotification(item, toCylinder, itemIndex);\n\t}\n\n\tif (moveItem) {\n\t\tint32_t moveItemIndex = toCylinder->getThingIndex(moveItem);\n\t\tif (moveItemIndex != -1) {\n\t\t\ttoCylinder->postAddNotification(moveItem, fromCylinder, moveItemIndex);\n\t\t}\n\t}\n\n\tif (updateItem) {\n\t\tint32_t updateItemIndex = toCylinder->getThingIndex(updateItem);\n\t\tif (updateItemIndex != -1) {\n\t\t\ttoCylinder->postAddNotification(updateItem, fromCylinder, updateItemIndex);\n\t\t}\n\t}\n\n\tif (_moveItem) {\n\t\tif (moveItem) {\n\t\t\t*_moveItem = moveItem;\n\t\t} else {\n\t\t\t*_moveItem = item;\n\t\t}\n\t}\n\n\t//we could not move all, inform the player\n\tif (item->isStackable() && maxQueryCount < count) {\n\t\treturn retMaxCount;\n\t}\n\n\treturn ret;\n}\n\nReturnValue Game::internalAddItem(Cylinder* toCylinder, Item* item, int32_t index /*= INDEX_WHEREEVER*/,\n                                  uint32_t flags/* = 0*/, bool test/* = false*/)\n{\n\tuint32_t remainderCount = 0;\n\treturn internalAddItem(toCylinder, item, index, flags, test, remainderCount);\n}\n\nReturnValue Game::internalAddItem(Cylinder* toCylinder, Item* item, int32_t index,\n                                  uint32_t flags, bool test, uint32_t& remainderCount)\n{\n\tif (toCylinder == nullptr || item == nullptr) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tCylinder* destCylinder = toCylinder;\n\tItem* toItem = nullptr;\n\ttoCylinder = toCylinder->queryDestination(index, *item, &toItem, flags);\n\n\t//check if we can add this item\n\tReturnValue ret = toCylinder->queryAdd(index, *item, item->getItemCount(), flags);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\t/*\n\tCheck if we can move add the whole amount, we do this by checking against the original cylinder,\n\tsince the queryDestination can return a cylinder that might only hold a part of the full amount.\n\t*/\n\tuint32_t maxQueryCount = 0;\n\tret = destCylinder->queryMaxCount(INDEX_WHEREEVER, *item, item->getItemCount(), maxQueryCount, flags);\n\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\tif (test) {\n\t\treturn RETURNVALUE_NOERROR;\n\t}\n\n\tif (item->isStackable() && item->equals(toItem)) {\n\t\tuint32_t m = std::min<uint32_t>(item->getItemCount(), maxQueryCount);\n\t\tuint32_t n = std::min<uint32_t>(100 - toItem->getItemCount(), m);\n\n\t\ttoCylinder->updateThing(toItem, toItem->getID(), toItem->getItemCount() + n);\n\n\t\tint32_t count = m - n;\n\t\tif (count > 0) {\n\t\t\tif (item->getItemCount() != count) {\n\t\t\t\tItem* remainderItem = item->clone();\n\t\t\t\tremainderItem->setItemCount(count);\n\t\t\t\tif (internalAddItem(destCylinder, remainderItem, INDEX_WHEREEVER, flags, false) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tReleaseItem(remainderItem);\n\t\t\t\t\tremainderCount = count;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ttoCylinder->addThing(index, item);\n\n\t\t\t\tint32_t itemIndex = toCylinder->getThingIndex(item);\n\t\t\t\tif (itemIndex != -1) {\n\t\t\t\t\ttoCylinder->postAddNotification(item, nullptr, itemIndex);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t//fully merged with toItem, item will be destroyed\n\t\t\titem->onRemoved();\n\t\t\tReleaseItem(item);\n\n\t\t\tint32_t itemIndex = toCylinder->getThingIndex(toItem);\n\t\t\tif (itemIndex != -1) {\n\t\t\t\ttoCylinder->postAddNotification(toItem, nullptr, itemIndex);\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttoCylinder->addThing(index, item);\n\n\t\tint32_t itemIndex = toCylinder->getThingIndex(item);\n\t\tif (itemIndex != -1) {\n\t\t\ttoCylinder->postAddNotification(item, nullptr, itemIndex);\n\t\t}\n\t}\n\n\treturn RETURNVALUE_NOERROR;\n}\n\nReturnValue Game::internalRemoveItem(Item* item, int32_t count /*= -1*/, bool test /*= false*/, uint32_t flags /*= 0*/)\n{\n\tCylinder* cylinder = item->getParent();\n\tif (cylinder == nullptr) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tTile* fromTile = cylinder->getTile();\n\tif (fromTile) {\n\t\tauto it = browseFields.find(fromTile);\n\t\tif (it != browseFields.end() && it->second == cylinder) {\n\t\t\tcylinder = fromTile;\n\t\t}\n\t}\n\n\tif (count == -1) {\n\t\tcount = item->getItemCount();\n\t}\n\n\t//check if we can remove this item\n\tReturnValue ret = cylinder->queryRemove(*item, count, flags | FLAG_IGNORENOTMOVEABLE);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\treturn ret;\n\t}\n\n\tif (!item->canRemove()) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tif (!test) {\n\t\tint32_t index = cylinder->getThingIndex(item);\n\n\t\t//remove the item\n\t\tcylinder->removeThing(item, count);\n\n\t\tif (item->isRemoved()) {\n\t\t\tReleaseItem(item);\n\t\t}\n\n\t\tcylinder->postRemoveNotification(item, nullptr, index);\n\t}\n\n\titem->onRemoved();\n\treturn RETURNVALUE_NOERROR;\n}\n\nReturnValue Game::internalPlayerAddItem(Player* player, Item* item, bool dropOnMap /*= true*/, slots_t slot /*= CONST_SLOT_WHEREEVER*/)\n{\n\tuint32_t remainderCount = 0;\n\tReturnValue ret = internalAddItem(player, item, static_cast<int32_t>(slot), 0, false, remainderCount);\n\tif (remainderCount != 0) {\n\t\tItem* remainderItem = Item::CreateItem(item->getID(), remainderCount);\n\t\tReturnValue remaindRet = internalAddItem(player->getTile(), remainderItem, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t\tif (remaindRet != RETURNVALUE_NOERROR) {\n\t\t\tReleaseItem(remainderItem);\n\t\t}\n\t}\n\n\tif (ret != RETURNVALUE_NOERROR && dropOnMap) {\n\t\tret = internalAddItem(player->getTile(), item, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t}\n\n\treturn ret;\n}\n\nItem* Game::findItemOfType(Cylinder* cylinder, uint16_t itemId,\n                           bool depthSearch /*= true*/, int32_t subType /*= -1*/) const\n{\n\tif (cylinder == nullptr) {\n\t\treturn nullptr;\n\t}\n\n\tstd::vector<Container*> containers;\n\tfor (size_t i = cylinder->getFirstIndex(), j = cylinder->getLastIndex(); i < j; ++i) {\n\t\tThing* thing = cylinder->getThing(i);\n\t\tif (!thing) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tItem* item = thing->getItem();\n\t\tif (!item) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (item->getID() == itemId && (subType == -1 || subType == item->getSubType())) {\n\t\t\treturn item;\n\t\t}\n\n\t\tif (depthSearch) {\n\t\t\tContainer* container = item->getContainer();\n\t\t\tif (container) {\n\t\t\t\tcontainers.push_back(container);\n\t\t\t}\n\t\t}\n\t}\n\n\tsize_t i = 0;\n\twhile (i < containers.size()) {\n\t\tContainer* container = containers[i++];\n\t\tfor (Item* item : container->getItemList()) {\n\t\t\tif (item->getID() == itemId && (subType == -1 || subType == item->getSubType())) {\n\t\t\t\treturn item;\n\t\t\t}\n\n\t\t\tContainer* subContainer = item->getContainer();\n\t\t\tif (subContainer) {\n\t\t\t\tcontainers.push_back(subContainer);\n\t\t\t}\n\t\t}\n\t}\n\treturn nullptr;\n}\n\nbool Game::removeMoney(Cylinder* cylinder, uint64_t money, uint32_t flags /*= 0*/)\n{\n\tif (cylinder == nullptr) {\n\t\treturn false;\n\t}\n\n\tif (money == 0) {\n\t\treturn true;\n\t}\n\n\tstd::vector<Container*> containers;\n\n\tstd::multimap<uint32_t, Item*> moneyMap;\n\tuint64_t moneyCount = 0;\n\n\tfor (size_t i = cylinder->getFirstIndex(), j = cylinder->getLastIndex(); i < j; ++i) {\n\t\tThing* thing = cylinder->getThing(i);\n\t\tif (!thing) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tItem* item = thing->getItem();\n\t\tif (!item) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tContainer* container = item->getContainer();\n\t\tif (container) {\n\t\t\tcontainers.push_back(container);\n\t\t} else {\n\t\t\tconst uint32_t worth = item->getWorth();\n\t\t\tif (worth != 0) {\n\t\t\t\tmoneyCount += worth;\n\t\t\t\tmoneyMap.emplace(worth, item);\n\t\t\t}\n\t\t}\n\t}\n\n\tsize_t i = 0;\n\twhile (i < containers.size()) {\n\t\tContainer* container = containers[i++];\n\t\tfor (Item* item : container->getItemList()) {\n\t\t\tContainer* tmpContainer = item->getContainer();\n\t\t\tif (tmpContainer) {\n\t\t\t\tcontainers.push_back(tmpContainer);\n\t\t\t} else {\n\t\t\t\tconst uint32_t worth = item->getWorth();\n\t\t\t\tif (worth != 0) {\n\t\t\t\t\tmoneyCount += worth;\n\t\t\t\t\tmoneyMap.emplace(worth, item);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (moneyCount < money) {\n\t\treturn false;\n\t}\n\n\tfor (const auto& moneyEntry : moneyMap) {\n\t\tItem* item = moneyEntry.second;\n\t\tif (moneyEntry.first < money) {\n\t\t\tinternalRemoveItem(item);\n\t\t\tmoney -= moneyEntry.first;\n\t\t} else if (moneyEntry.first > money) {\n\t\t\tconst uint32_t worth = moneyEntry.first / item->getItemCount();\n\t\t\tconst uint32_t removeCount = (money / worth) + 1;\n\n\t\t\taddMoney(cylinder, (worth * removeCount) - money, flags);\n\t\t\tinternalRemoveItem(item, removeCount);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tinternalRemoveItem(item);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn true;\n}\n\nvoid Game::addMoney(Cylinder* cylinder, uint64_t money, uint32_t flags /*= 0*/)\n{\n\tif (money == 0) {\n\t\treturn;\n\t}\n\n\tuint32_t crystalCoins = money / 10000;\n\tmoney -= crystalCoins * 10000;\n\twhile (crystalCoins > 0) {\n\t\tconst uint16_t count = std::min<uint32_t>(100, crystalCoins);\n\n\t\tItem* remaindItem = Item::CreateItem(ITEM_CRYSTAL_COIN, count);\n\n\t\tReturnValue ret = internalAddItem(cylinder, remaindItem, INDEX_WHEREEVER, flags);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\tinternalAddItem(cylinder->getTile(), remaindItem, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t\t}\n\n\t\tcrystalCoins -= count;\n\t}\n\n\tuint16_t platinumCoins = money / 100;\n\tif (platinumCoins != 0) {\n\t\tItem* remaindItem = Item::CreateItem(ITEM_PLATINUM_COIN, platinumCoins);\n\n\t\tReturnValue ret = internalAddItem(cylinder, remaindItem, INDEX_WHEREEVER, flags);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\tinternalAddItem(cylinder->getTile(), remaindItem, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t\t}\n\n\t\tmoney -= platinumCoins * 100;\n\t}\n\n\tif (money != 0) {\n\t\tItem* remaindItem = Item::CreateItem(ITEM_GOLD_COIN, money);\n\n\t\tReturnValue ret = internalAddItem(cylinder, remaindItem, INDEX_WHEREEVER, flags);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\tinternalAddItem(cylinder->getTile(), remaindItem, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t\t}\n\t}\n}\n\nItem* Game::transformItem(Item* item, uint16_t newId, int32_t newCount /*= -1*/)\n{\n\tif (item->getID() == newId && (newCount == -1 || (newCount == item->getSubType() && newCount != 0))) { //chargeless item placed on map = infinite\n\t\treturn item;\n\t}\n\n\tCylinder* cylinder = item->getParent();\n\tif (cylinder == nullptr) {\n\t\treturn nullptr;\n\t}\n\n\tTile* fromTile = cylinder->getTile();\n\tif (fromTile) {\n\t\tauto it = browseFields.find(fromTile);\n\t\tif (it != browseFields.end() && it->second == cylinder) {\n\t\t\tcylinder = fromTile;\n\t\t}\n\t}\n\n\tint32_t itemIndex = cylinder->getThingIndex(item);\n\tif (itemIndex == -1) {\n\t\treturn item;\n\t}\n\n\tif (!item->canTransform()) {\n\t\treturn item;\n\t}\n\n\tconst ItemType& newType = Item::items[newId];\n\tif (newType.id == 0) {\n\t\treturn item;\n\t}\n\n\tconst ItemType& curType = Item::items[item->getID()];\n\tif (curType.alwaysOnTop != newType.alwaysOnTop) {\n\t\t//This only occurs when you transform items on tiles from a downItem to a topItem (or vice versa)\n\t\t//Remove the old, and add the new\n\t\tcylinder->removeThing(item, item->getItemCount());\n\t\tcylinder->postRemoveNotification(item, cylinder, itemIndex);\n\n\t\titem->setID(newId);\n\t\tif (newCount != -1) {\n\t\t\titem->setSubType(newCount);\n\t\t}\n\t\tcylinder->addThing(item);\n\n\t\tCylinder* newParent = item->getParent();\n\t\tif (newParent == nullptr) {\n\t\t\tReleaseItem(item);\n\t\t\treturn nullptr;\n\t\t}\n\n\t\tnewParent->postAddNotification(item, cylinder, newParent->getThingIndex(item));\n\t\treturn item;\n\t}\n\n\tif (curType.type == newType.type) {\n\t\t//Both items has the same type so we can safely change id/subtype\n\t\tif (newCount == 0 && (item->isStackable() || item->hasAttribute(ITEM_ATTRIBUTE_CHARGES))) {\n\t\t\tif (item->isStackable()) {\n\t\t\t\tinternalRemoveItem(item);\n\t\t\t\treturn nullptr;\n\t\t\t} else {\n\t\t\t\tint32_t newItemId = newId;\n\t\t\t\tif (curType.id == newType.id) {\n\t\t\t\t\tnewItemId = curType.decayTo;\n\t\t\t\t}\n\n\t\t\t\tif (newItemId < 0) {\n\t\t\t\t\tinternalRemoveItem(item);\n\t\t\t\t\treturn nullptr;\n\t\t\t\t} else if (newItemId != newId) {\n\t\t\t\t\t//Replacing the the old item with the new while maintaining the old position\n\t\t\t\t\tItem* newItem = Item::CreateItem(newItemId, 1);\n\t\t\t\t\tif (newItem == nullptr) {\n\t\t\t\t\t\treturn nullptr;\n\t\t\t\t\t}\n\n\t\t\t\t\tcylinder->replaceThing(itemIndex, newItem);\n\t\t\t\t\tcylinder->postAddNotification(newItem, cylinder, itemIndex);\n\n\t\t\t\t\titem->setParent(nullptr);\n\t\t\t\t\tcylinder->postRemoveNotification(item, cylinder, itemIndex);\n\t\t\t\t\tReleaseItem(item);\n\t\t\t\t\treturn newItem;\n\t\t\t\t} else {\n\t\t\t\t\treturn transformItem(item, newItemId);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tcylinder->postRemoveNotification(item, cylinder, itemIndex);\n\t\t\tuint16_t itemId = item->getID();\n\t\t\tint32_t count = item->getSubType();\n\n\t\t\tif (curType.id != newType.id) {\n\t\t\t\tif (newType.group != curType.group) {\n\t\t\t\t\titem->setDefaultSubtype();\n\t\t\t\t}\n\n\t\t\t\titemId = newId;\n\t\t\t}\n\n\t\t\tif (newCount != -1 && newType.hasSubType()) {\n\t\t\t\tcount = newCount;\n\t\t\t}\n\n\t\t\tcylinder->updateThing(item, itemId, count);\n\t\t\tcylinder->postAddNotification(item, cylinder, itemIndex);\n\t\t\treturn item;\n\t\t}\n\t}\n\n\t//Replacing the the old item with the new while maintaining the old position\n\tItem* newItem;\n\tif (newCount == -1) {\n\t\tnewItem = Item::CreateItem(newId);\n\t} else {\n\t\tnewItem = Item::CreateItem(newId, newCount);\n\t}\n\n\tif (newItem == nullptr) {\n\t\treturn nullptr;\n\t}\n\n\tcylinder->replaceThing(itemIndex, newItem);\n\tcylinder->postAddNotification(newItem, cylinder, itemIndex);\n\n\titem->setParent(nullptr);\n\tcylinder->postRemoveNotification(item, cylinder, itemIndex);\n\tReleaseItem(item);\n\n\treturn newItem;\n}\n\nReturnValue Game::internalTeleport(Thing* thing, const Position& newPos, bool pushMove/* = true*/, uint32_t flags /*= 0*/)\n{\n\tif (newPos == thing->getPosition()) {\n\t\treturn RETURNVALUE_NOERROR;\n\t} else if (thing->isRemoved()) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tTile* toTile = map.getTile(newPos);\n\tif (!toTile) {\n\t\treturn RETURNVALUE_NOTPOSSIBLE;\n\t}\n\n\tif (Creature* creature = thing->getCreature()) {\n\t\tReturnValue ret = toTile->queryAdd(0, *creature, 1, FLAG_NOLIMIT);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\treturn ret;\n\t\t}\n\n\t\tmap.moveCreature(*creature, *toTile, !pushMove);\n\t\treturn RETURNVALUE_NOERROR;\n\t} else if (Item* item = thing->getItem()) {\n\t\treturn internalMoveItem(item->getParent(), toTile, INDEX_WHEREEVER, item, item->getItemCount(), nullptr, flags);\n\t}\n\treturn RETURNVALUE_NOTPOSSIBLE;\n}\n\n//Implementation of player invoked events\nvoid Game::playerMove(uint32_t playerId, Direction direction)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tplayer->setNextWalkActionTask(nullptr);\n\n\tplayer->startAutoWalk(std::forward_list<Direction> { direction });\n}\n\nbool Game::playerBroadcastMessage(Player* player, const std::string& text) const\n{\n\tif (!player->hasFlag(PlayerFlag_CanBroadcast)) {\n\t\treturn false;\n\t}\n\n\tstd::cout << \"> \" << player->getName() << \" broadcasted: \\\"\" << text << \"\\\".\" << std::endl;\n\n\tfor (const auto& it : players) {\n\t\tit.second->sendPrivateMessage(player, TALKTYPE_BROADCAST, text);\n\t}\n\n\treturn true;\n}\n\nvoid Game::playerCreatePrivateChannel(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player || !player->isPremium()) {\n\t\treturn;\n\t}\n\n\tChatChannel* channel = g_chat->createChannel(*player, CHANNEL_PRIVATE);\n\tif (!channel || !channel->addUser(*player)) {\n\t\treturn;\n\t}\n\n\tplayer->sendCreatePrivateChannel(channel->getId(), channel->getName());\n}\n\nvoid Game::playerChannelInvite(uint32_t playerId, const std::string& name)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPrivateChatChannel* channel = g_chat->getPrivateChannel(*player);\n\tif (!channel) {\n\t\treturn;\n\t}\n\n\tPlayer* invitePlayer = getPlayerByName(name);\n\tif (!invitePlayer) {\n\t\treturn;\n\t}\n\n\tif (player == invitePlayer) {\n\t\treturn;\n\t}\n\n\tchannel->invitePlayer(*player, *invitePlayer);\n}\n\nvoid Game::playerChannelExclude(uint32_t playerId, const std::string& name)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPrivateChatChannel* channel = g_chat->getPrivateChannel(*player);\n\tif (!channel) {\n\t\treturn;\n\t}\n\n\tPlayer* excludePlayer = getPlayerByName(name);\n\tif (!excludePlayer) {\n\t\treturn;\n\t}\n\n\tif (player == excludePlayer) {\n\t\treturn;\n\t}\n\n\tchannel->excludePlayer(*player, *excludePlayer);\n}\n\nvoid Game::playerRequestChannels(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->sendChannelsDialog();\n}\n\nvoid Game::playerOpenChannel(uint32_t playerId, uint16_t channelId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tChatChannel* channel = g_chat->addUserToChannel(*player, channelId);\n\tif (!channel) {\n\t\treturn;\n\t}\n\n\tconst InvitedMap* invitedUsers = channel->getInvitedUsers();\n\tconst UsersMap* users;\n\tif (!channel->isPublicChannel()) {\n\t\tusers = &channel->getUsers();\n\t} else {\n\t\tusers = nullptr;\n\t}\n\n\tplayer->sendChannel(channel->getId(), channel->getName(), users, invitedUsers);\n}\n\nvoid Game::playerCloseChannel(uint32_t playerId, uint16_t channelId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tg_chat->removeUserFromChannel(*player, channelId);\n}\n\nvoid Game::playerOpenPrivateChannel(uint32_t playerId, std::string& receiver)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!IOLoginData::formatPlayerName(receiver)) {\n\t\tplayer->sendCancelMessage(\"A player with this name does not exist.\");\n\t\treturn;\n\t}\n\n\tif (player->getName() == receiver) {\n\t\tplayer->sendCancelMessage(\"You cannot set up a private message channel with yourself.\");\n\t\treturn;\n\t}\n\n\tplayer->sendOpenPrivateChannel(receiver);\n}\n\nvoid Game::playerCloseNpcChannel(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tSpectatorVec list;\n\tmap.getSpectators(list, player->getPosition());\n\tfor (Creature* spectator : list) {\n\t\tif (Npc* npc = spectator->getNpc()) {\n\t\t\tnpc->onPlayerCloseChannel(player);\n\t\t}\n\t}\n}\n\nvoid Game::playerReceivePing(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->receivePing();\n}\n\nvoid Game::playerReceivePingBack(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->sendPingBack();\n}\n\nvoid Game::playerAutoWalk(uint32_t playerId, const std::forward_list<Direction>& listDir)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tplayer->setNextWalkTask(nullptr);\n\tplayer->startAutoWalk(listDir);\n}\n\nvoid Game::playerStopAutoWalk(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->stopWalk();\n}\n\nvoid Game::playerUseItemEx(uint32_t playerId, const Position& fromPos, uint8_t fromStackPos, uint16_t fromSpriteId,\n                           const Position& toPos, uint8_t toStackPos, uint16_t toSpriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tbool isHotkey = (fromPos.x == 0xFFFF && fromPos.y == 0 && fromPos.z == 0);\n\tif (isHotkey && !g_config.getBoolean(ConfigManager::AIMBOT_HOTKEY_ENABLED)) {\n\t\treturn;\n\t}\n\n\tThing* thing = internalGetThing(player, fromPos, fromStackPos, fromSpriteId, STACKPOS_USEITEM);\n\tif (!thing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tItem* item = thing->getItem();\n\tif (!item || !item->isUseable() || item->getClientID() != fromSpriteId) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CANNOTUSETHISOBJECT);\n\t\treturn;\n\t}\n\n\tPosition walkToPos = fromPos;\n\tReturnValue ret = g_actions->canUse(player, fromPos);\n\tif (ret == RETURNVALUE_NOERROR) {\n\t\tret = g_actions->canUse(player, toPos, item);\n\t\tif (ret == RETURNVALUE_TOOFARAWAY) {\n\t\t\twalkToPos = toPos;\n\t\t}\n\t}\n\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tif (ret == RETURNVALUE_TOOFARAWAY) {\n\t\t\tPosition itemPos = fromPos;\n\t\t\tuint8_t itemStackPos = fromStackPos;\n\n\t\t\tif (fromPos.x != 0xFFFF && toPos.x != 0xFFFF && Position::areInRange<1, 1, 0>(fromPos, player->getPosition()) &&\n\t\t\t        !Position::areInRange<1, 1, 0>(fromPos, toPos)) {\n\t\t\t\tItem* moveItem = nullptr;\n\n\t\t\t\tret = internalMoveItem(item->getParent(), player, INDEX_WHEREEVER, item, item->getItemCount(), &moveItem);\n\t\t\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\t\t\tplayer->sendCancelMessage(ret);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\t//changing the position since its now in the inventory of the player\n\t\t\t\tinternalGetPosition(moveItem, itemPos, itemStackPos);\n\t\t\t}\n\n\t\t\tstd::forward_list<Direction> listDir;\n\t\t\tif (player->getPathTo(walkToPos, listDir, 0, 1, true, true)) {\n\t\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk, this, player->getID(), listDir)));\n\n\t\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerUseItemEx, this,\n\t\t\t\t                      playerId, itemPos, itemStackPos, fromSpriteId, toPos, toStackPos, toSpriteId));\n\t\t\t\tplayer->setNextWalkActionTask(task);\n\t\t\t} else {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->sendCancelMessage(ret);\n\t\treturn;\n\t}\n\n\tif (!player->canDoAction()) {\n\t\tuint32_t delay = player->getNextActionTime();\n\t\tSchedulerTask* task = createSchedulerTask(delay, std::bind(&Game::playerUseItemEx, this,\n\t\t                      playerId, fromPos, fromStackPos, fromSpriteId, toPos, toStackPos, toSpriteId));\n\t\tplayer->setNextActionTask(task);\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tplayer->setNextActionTask(nullptr);\n\n\tg_actions->useItemEx(player, fromPos, toPos, toStackPos, item, isHotkey);\n}\n\nvoid Game::playerUseItem(uint32_t playerId, const Position& pos, uint8_t stackPos,\n                         uint8_t index, uint16_t spriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tbool isHotkey = (pos.x == 0xFFFF && pos.y == 0 && pos.z == 0);\n\tif (isHotkey && !g_config.getBoolean(ConfigManager::AIMBOT_HOTKEY_ENABLED)) {\n\t\treturn;\n\t}\n\n\tThing* thing = internalGetThing(player, pos, stackPos, spriteId, STACKPOS_USEITEM);\n\tif (!thing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tItem* item = thing->getItem();\n\tif (!item || item->isUseable() || item->getClientID() != spriteId) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CANNOTUSETHISOBJECT);\n\t\treturn;\n\t}\n\n\tReturnValue ret = g_actions->canUse(player, pos);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tif (ret == RETURNVALUE_TOOFARAWAY) {\n\t\t\tstd::forward_list<Direction> listDir;\n\t\t\tif (player->getPathTo(pos, listDir, 0, 1, true, true)) {\n\t\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerUseItem, this,\n\t\t\t\t                      playerId, pos, stackPos, index, spriteId));\n\t\t\t\tplayer->setNextWalkActionTask(task);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tret = RETURNVALUE_THEREISNOWAY;\n\t\t}\n\n\t\tplayer->sendCancelMessage(ret);\n\t\treturn;\n\t}\n\n\tif (!player->canDoAction()) {\n\t\tuint32_t delay = player->getNextActionTime();\n\t\tSchedulerTask* task = createSchedulerTask(delay, std::bind(&Game::playerUseItem, this,\n\t\t                      playerId, pos, stackPos, index, spriteId));\n\t\tplayer->setNextActionTask(task);\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tplayer->setNextActionTask(nullptr);\n\n\tg_actions->useItem(player, pos, index, item, isHotkey);\n}\n\nvoid Game::playerUseWithCreature(uint32_t playerId, const Position& fromPos, uint8_t fromStackPos, uint32_t creatureId, uint16_t spriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (!creature) {\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<7, 5, 0>(creature->getPosition(), player->getPosition())) {\n\t\treturn;\n\t}\n\n\tbool isHotkey = (fromPos.x == 0xFFFF && fromPos.y == 0 && fromPos.z == 0);\n\tif (!g_config.getBoolean(ConfigManager::AIMBOT_HOTKEY_ENABLED)) {\n\t\tif (creature->getPlayer() || isHotkey) {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_DIRECTPLAYERSHOOT);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tThing* thing = internalGetThing(player, fromPos, fromStackPos, spriteId, STACKPOS_USEITEM);\n\tif (!thing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tItem* item = thing->getItem();\n\tif (!item || !item->isUseable() || item->getClientID() != spriteId) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CANNOTUSETHISOBJECT);\n\t\treturn;\n\t}\n\n\tPosition toPos = creature->getPosition();\n\tPosition walkToPos = fromPos;\n\tReturnValue ret = g_actions->canUse(player, fromPos);\n\tif (ret == RETURNVALUE_NOERROR) {\n\t\tret = g_actions->canUse(player, toPos, item);\n\t\tif (ret == RETURNVALUE_TOOFARAWAY) {\n\t\t\twalkToPos = toPos;\n\t\t}\n\t}\n\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tif (ret == RETURNVALUE_TOOFARAWAY) {\n\t\t\tPosition itemPos = fromPos;\n\t\t\tuint8_t itemStackPos = fromStackPos;\n\n\t\t\tif (fromPos.x != 0xFFFF && Position::areInRange<1, 1, 0>(fromPos, player->getPosition()) && !Position::areInRange<1, 1, 0>(fromPos, toPos)) {\n\t\t\t\tItem* moveItem = nullptr;\n\t\t\t\tret = internalMoveItem(item->getParent(), player, INDEX_WHEREEVER, item, item->getItemCount(), &moveItem);\n\t\t\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\t\t\tplayer->sendCancelMessage(ret);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\t//changing the position since its now in the inventory of the player\n\t\t\t\tinternalGetPosition(moveItem, itemPos, itemStackPos);\n\t\t\t}\n\n\t\t\tstd::forward_list<Direction> listDir;\n\t\t\tif (player->getPathTo(walkToPos, listDir, 0, 1, true, true)) {\n\t\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerUseWithCreature, this,\n\t\t\t\t                      playerId, itemPos, itemStackPos, creatureId, spriteId));\n\t\t\t\tplayer->setNextWalkActionTask(task);\n\t\t\t} else {\n\t\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->sendCancelMessage(ret);\n\t\treturn;\n\t}\n\n\tif (!player->canDoAction()) {\n\t\tuint32_t delay = player->getNextActionTime();\n\t\tSchedulerTask* task = createSchedulerTask(delay, std::bind(&Game::playerUseWithCreature, this,\n\t\t                      playerId, fromPos, fromStackPos, creatureId, spriteId));\n\t\tplayer->setNextActionTask(task);\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tplayer->setNextActionTask(nullptr);\n\n\tg_actions->useItemEx(player, fromPos, creature->getPosition(), creature->getParent()->getThingIndex(creature), item, isHotkey, creature);\n}\n\nvoid Game::playerCloseContainer(uint32_t playerId, uint8_t cid)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->closeContainer(cid);\n\tplayer->sendCloseContainer(cid);\n}\n\nvoid Game::playerMoveUpContainer(uint32_t playerId, uint8_t cid)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tContainer* container = player->getContainerByID(cid);\n\tif (!container) {\n\t\treturn;\n\t}\n\n\tContainer* parentContainer = dynamic_cast<Container*>(container->getRealParent());\n\tif (!parentContainer) {\n\t\tTile* tile = container->getTile();\n\t\tif (!tile) {\n\t\t\treturn;\n\t\t}\n\n\t\tauto it = browseFields.find(tile);\n\t\tif (it == browseFields.end()) {\n\t\t\tparentContainer = new Container(tile);\n\t\t\tparentContainer->incrementReferenceCounter();\n\t\t\tbrowseFields[tile] = parentContainer;\n\t\t\tg_scheduler.addEvent(createSchedulerTask(30000, std::bind(&Game::decreaseBrowseFieldRef, this, tile->getPosition())));\n\t\t} else {\n\t\t\tparentContainer = it->second;\n\t\t}\n\t}\n\n\tplayer->addContainer(cid, parentContainer);\n\tplayer->sendContainer(cid, parentContainer, parentContainer->hasParent(), player->getContainerIndex(cid));\n}\n\nvoid Game::playerUpdateContainer(uint32_t playerId, uint8_t cid)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tContainer* container = player->getContainerByID(cid);\n\tif (!container) {\n\t\treturn;\n\t}\n\n\tplayer->sendContainer(cid, container, container->hasParent(), player->getContainerIndex(cid));\n}\n\nvoid Game::playerRotateItem(uint32_t playerId, const Position& pos, uint8_t stackPos, const uint16_t spriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tThing* thing = internalGetThing(player, pos, stackPos, 0, STACKPOS_TOPDOWN_ITEM);\n\tif (!thing) {\n\t\treturn;\n\t}\n\n\tItem* item = thing->getItem();\n\tif (!item || item->getClientID() != spriteId || !item->isRotatable() || item->hasAttribute(ITEM_ATTRIBUTE_UNIQUEID)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tif (pos.x != 0xFFFF && !Position::areInRange<1, 1, 0>(pos, player->getPosition())) {\n\t\tstd::forward_list<Direction> listDir;\n\t\tif (player->getPathTo(pos, listDir, 0, 1, true, true)) {\n\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerRotateItem, this,\n\t\t\t                      playerId, pos, stackPos, spriteId));\n\t\t\tplayer->setNextWalkActionTask(task);\n\t\t} else {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t}\n\t\treturn;\n\t}\n\n\tuint16_t newId = Item::items[item->getID()].rotateTo;\n\tif (newId != 0) {\n\t\ttransformItem(item, newId);\n\t}\n}\n\nvoid Game::playerWriteItem(uint32_t playerId, uint32_t windowTextId, const std::string& text)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tuint16_t maxTextLength = 0;\n\tuint32_t internalWindowTextId = 0;\n\n\tItem* writeItem = player->getWriteItem(internalWindowTextId, maxTextLength);\n\tif (text.length() > maxTextLength || windowTextId != internalWindowTextId) {\n\t\treturn;\n\t}\n\n\tif (!writeItem || writeItem->isRemoved()) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tCylinder* topParent = writeItem->getTopParent();\n\n\tPlayer* owner = dynamic_cast<Player*>(topParent);\n\tif (owner && owner != player) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<1, 1, 0>(writeItem->getPosition(), player->getPosition())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tfor (auto creatureEvent : player->getCreatureEvents(CREATURE_EVENT_TEXTEDIT)) {\n\t\tif (!creatureEvent->executeTextEdit(player, writeItem, text)) {\n\t\t\tplayer->setWriteItem(nullptr);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (!text.empty()) {\n\t\tif (writeItem->getText() != text) {\n\t\t\twriteItem->setText(text);\n\t\t\twriteItem->setWriter(player->getName());\n\t\t\twriteItem->setDate(time(nullptr));\n\t\t}\n\t} else {\n\t\twriteItem->resetText();\n\t\twriteItem->resetWriter();\n\t\twriteItem->resetDate();\n\t}\n\n\tuint16_t newId = Item::items[writeItem->getID()].writeOnceItemId;\n\tif (newId != 0) {\n\t\ttransformItem(writeItem, newId);\n\t}\n\n\tplayer->setWriteItem(nullptr);\n}\n\nvoid Game::playerBrowseField(uint32_t playerId, const Position& pos)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tconst Position& playerPos = player->getPosition();\n\tif (playerPos.z != pos.z) {\n\t\tplayer->sendCancelMessage(playerPos.z > pos.z ? RETURNVALUE_FIRSTGOUPSTAIRS : RETURNVALUE_FIRSTGODOWNSTAIRS);\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<1, 1>(playerPos, pos)) {\n\t\tstd::forward_list<Direction> listDir;\n\t\tif (player->getPathTo(pos, listDir, 0, 1, true, true)) {\n\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t                                this, player->getID(), listDir)));\n\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(\n\t\t\t                          &Game::playerBrowseField, this, playerId, pos\n\t\t\t                      ));\n\t\t\tplayer->setNextWalkActionTask(task);\n\t\t} else {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t}\n\t\treturn;\n\t}\n\n\tTile* tile = map.getTile(pos);\n\tif (!tile) {\n\t\treturn;\n\t}\n\n\tif (!g_events->eventPlayerOnBrowseField(player, pos)) {\n\t\treturn;\n\t}\n\n\tContainer* container;\n\n\tauto it = browseFields.find(tile);\n\tif (it == browseFields.end()) {\n\t\tcontainer = new Container(tile);\n\t\tcontainer->incrementReferenceCounter();\n\t\tbrowseFields[tile] = container;\n\t\tg_scheduler.addEvent(createSchedulerTask(30000, std::bind(&Game::decreaseBrowseFieldRef, this, tile->getPosition())));\n\t} else {\n\t\tcontainer = it->second;\n\t}\n\n\tuint8_t dummyContainerId = 0xF - ((pos.x % 3) * 3 + (pos.y % 3));\n\tContainer* openContainer = player->getContainerByID(dummyContainerId);\n\tif (openContainer) {\n\t\tplayer->onCloseContainer(openContainer);\n\t\tplayer->closeContainer(dummyContainerId);\n\t} else {\n\t\tplayer->addContainer(dummyContainerId, container);\n\t\tplayer->sendContainer(dummyContainerId, container, false, 0);\n\t}\n}\n\nvoid Game::playerSeekInContainer(uint32_t playerId, uint8_t containerId, uint16_t index)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tContainer* container = player->getContainerByID(containerId);\n\tif (!container || !container->hasPagination()) {\n\t\treturn;\n\t}\n\n\tif ((index % container->capacity()) != 0 || index >= container->size()) {\n\t\treturn;\n\t}\n\n\tplayer->setContainerIndex(containerId, index);\n\tplayer->sendContainer(containerId, container, container->hasParent(), index);\n}\n\nvoid Game::playerUpdateHouseWindow(uint32_t playerId, uint8_t listId, uint32_t windowTextId, const std::string& text)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tuint32_t internalWindowTextId;\n\tuint32_t internalListId;\n\n\tHouse* house = player->getEditHouse(internalWindowTextId, internalListId);\n\tif (house && house->canEditAccessList(internalListId, player) && internalWindowTextId == windowTextId && listId == 0) {\n\t\thouse->setAccessList(internalListId, text);\n\t}\n\n\tplayer->setEditHouse(nullptr);\n}\n\nvoid Game::playerRequestTrade(uint32_t playerId, const Position& pos, uint8_t stackPos,\n                              uint32_t tradePlayerId, uint16_t spriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPlayer* tradePartner = getPlayerByID(tradePlayerId);\n\tif (!tradePartner || tradePartner == player) {\n\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"Sorry, not possible.\");\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<2, 2, 0>(tradePartner->getPosition(), player->getPosition())) {\n\t\tstd::ostringstream ss;\n\t\tss << tradePartner->getName() << \" tells you to move closer.\";\n\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, ss.str());\n\t\treturn;\n\t}\n\n\tif (!canThrowObjectTo(tradePartner->getPosition(), player->getPosition())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CREATUREISNOTREACHABLE);\n\t\treturn;\n\t}\n\n\tThing* tradeThing = internalGetThing(player, pos, stackPos, 0, STACKPOS_TOPDOWN_ITEM);\n\tif (!tradeThing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tItem* tradeItem = tradeThing->getItem();\n\tif (tradeItem->getClientID() != spriteId || !tradeItem->isPickupable() || tradeItem->hasAttribute(ITEM_ATTRIBUTE_UNIQUEID)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tconst Position& playerPosition = player->getPosition();\n\tconst Position& tradeItemPosition = tradeItem->getPosition();\n\tif (playerPosition.z != tradeItemPosition.z) {\n\t\tplayer->sendCancelMessage(playerPosition.z > tradeItemPosition.z ? RETURNVALUE_FIRSTGOUPSTAIRS : RETURNVALUE_FIRSTGODOWNSTAIRS);\n\t\treturn;\n\t}\n\n\tif (!Position::areInRange<1, 1>(tradeItemPosition, playerPosition)) {\n\t\tstd::forward_list<Direction> listDir;\n\t\tif (player->getPathTo(pos, listDir, 0, 1, true, true)) {\n\t\t\tg_dispatcher.addTask(createTask(std::bind(&Game::playerAutoWalk,\n\t\t\t                                this, player->getID(), listDir)));\n\n\t\t\tSchedulerTask* task = createSchedulerTask(400, std::bind(&Game::playerRequestTrade, this,\n\t\t\t                      playerId, pos, stackPos, tradePlayerId, spriteId));\n\t\t\tplayer->setNextWalkActionTask(task);\n\t\t} else {\n\t\t\tplayer->sendCancelMessage(RETURNVALUE_THEREISNOWAY);\n\t\t}\n\t\treturn;\n\t}\n\n\tContainer* tradeItemContainer = tradeItem->getContainer();\n\tif (tradeItemContainer) {\n\t\tfor (const auto& it : tradeItems) {\n\t\t\tItem* item = it.first;\n\t\t\tif (tradeItem == item) {\n\t\t\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"This item is already being traded.\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (tradeItemContainer->isHoldingItem(item)) {\n\t\t\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"This item is already being traded.\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tContainer* container = item->getContainer();\n\t\t\tif (container && container->isHoldingItem(tradeItem)) {\n\t\t\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"This item is already being traded.\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (const auto& it : tradeItems) {\n\t\t\tItem* item = it.first;\n\t\t\tif (tradeItem == item) {\n\t\t\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"This item is already being traded.\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tContainer* container = item->getContainer();\n\t\t\tif (container && container->isHoldingItem(tradeItem)) {\n\t\t\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"This item is already being traded.\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tContainer* tradeContainer = tradeItem->getContainer();\n\tif (tradeContainer && tradeContainer->getItemHoldingCount() + 1 > 100) {\n\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"You can not trade more than 100 items.\");\n\t\treturn;\n\t}\n\n\tif (!g_events->eventPlayerOnTradeRequest(player, tradePartner, tradeItem)) {\n\t\treturn;\n\t}\n\n\tinternalStartTrade(player, tradePartner, tradeItem);\n}\n\nbool Game::internalStartTrade(Player* player, Player* tradePartner, Item* tradeItem)\n{\n\tif (player->tradeState != TRADE_NONE && !(player->tradeState == TRADE_ACKNOWLEDGE && player->tradePartner == tradePartner)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_YOUAREALREADYTRADING);\n\t\treturn false;\n\t} else if (tradePartner->tradeState != TRADE_NONE && tradePartner->tradePartner != player) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_THISPLAYERISALREADYTRADING);\n\t\treturn false;\n\t}\n\n\tplayer->tradePartner = tradePartner;\n\tplayer->tradeItem = tradeItem;\n\tplayer->tradeState = TRADE_INITIATED;\n\ttradeItem->incrementReferenceCounter();\n\ttradeItems[tradeItem] = player->getID();\n\n\tplayer->sendTradeItemRequest(player->getName(), tradeItem, true);\n\n\tif (tradePartner->tradeState == TRADE_NONE) {\n\t\tstd::ostringstream ss;\n\t\tss << player->getName() << \" wants to trade with you.\";\n\t\ttradePartner->sendTextMessage(MESSAGE_EVENT_ADVANCE, ss.str());\n\t\ttradePartner->tradeState = TRADE_ACKNOWLEDGE;\n\t\ttradePartner->tradePartner = player;\n\t} else {\n\t\tItem* counterOfferItem = tradePartner->tradeItem;\n\t\tplayer->sendTradeItemRequest(tradePartner->getName(), counterOfferItem, false);\n\t\ttradePartner->sendTradeItemRequest(player->getName(), tradeItem, false);\n\t}\n\n\treturn true;\n}\n\nvoid Game::playerAcceptTrade(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!(player->getTradeState() == TRADE_ACKNOWLEDGE || player->getTradeState() == TRADE_INITIATED)) {\n\t\treturn;\n\t}\n\n\tPlayer* tradePartner = player->tradePartner;\n\tif (!tradePartner) {\n\t\treturn;\n\t}\n\n\tif (!canThrowObjectTo(tradePartner->getPosition(), player->getPosition())) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_CREATUREISNOTREACHABLE);\n\t\treturn;\n\t}\n\n\tplayer->setTradeState(TRADE_ACCEPT);\n\n\tif (tradePartner->getTradeState() == TRADE_ACCEPT) {\n\t\tItem* tradeItem1 = player->tradeItem;\n\t\tItem* tradeItem2 = tradePartner->tradeItem;\n\n\t\tif (!g_events->eventPlayerOnTradeAccept(player, tradePartner, tradeItem1, tradeItem2)) {\n\t\t\tinternalCloseTrade(player);\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->setTradeState(TRADE_TRANSFER);\n\t\ttradePartner->setTradeState(TRADE_TRANSFER);\n\n\t\tauto it = tradeItems.find(tradeItem1);\n\t\tif (it != tradeItems.end()) {\n\t\t\tReleaseItem(it->first);\n\t\t\ttradeItems.erase(it);\n\t\t}\n\n\t\tit = tradeItems.find(tradeItem2);\n\t\tif (it != tradeItems.end()) {\n\t\t\tReleaseItem(it->first);\n\t\t\ttradeItems.erase(it);\n\t\t}\n\n\t\tbool isSuccess = false;\n\n\t\tReturnValue ret1 = internalAddItem(tradePartner, tradeItem1, INDEX_WHEREEVER, 0, true);\n\t\tReturnValue ret2 = internalAddItem(player, tradeItem2, INDEX_WHEREEVER, 0, true);\n\t\tif (ret1 == RETURNVALUE_NOERROR && ret2 == RETURNVALUE_NOERROR) {\n\t\t\tret1 = internalRemoveItem(tradeItem1, tradeItem1->getItemCount(), true);\n\t\t\tret2 = internalRemoveItem(tradeItem2, tradeItem2->getItemCount(), true);\n\t\t\tif (ret1 == RETURNVALUE_NOERROR && ret2 == RETURNVALUE_NOERROR) {\n\t\t\t\tCylinder* cylinder1 = tradeItem1->getParent();\n\t\t\t\tCylinder* cylinder2 = tradeItem2->getParent();\n\n\t\t\t\tuint32_t count1 = tradeItem1->getItemCount();\n\t\t\t\tuint32_t count2 = tradeItem2->getItemCount();\n\n\t\t\t\tret1 = internalMoveItem(cylinder1, tradePartner, INDEX_WHEREEVER, tradeItem1, count1, nullptr, FLAG_IGNOREAUTOSTACK, nullptr, tradeItem2);\n\t\t\t\tif (ret1 == RETURNVALUE_NOERROR) {\n\t\t\t\t\tinternalMoveItem(cylinder2, player, INDEX_WHEREEVER, tradeItem2, count2, nullptr, FLAG_IGNOREAUTOSTACK);\n\n\t\t\t\t\ttradeItem1->onTradeEvent(ON_TRADE_TRANSFER, tradePartner);\n\t\t\t\t\ttradeItem2->onTradeEvent(ON_TRADE_TRANSFER, player);\n\n\t\t\t\t\tisSuccess = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (!isSuccess) {\n\t\t\tstd::string errorDescription;\n\n\t\t\tif (tradePartner->tradeItem) {\n\t\t\t\terrorDescription = getTradeErrorDescription(ret1, tradeItem1);\n\t\t\t\ttradePartner->sendTextMessage(MESSAGE_EVENT_ADVANCE, errorDescription);\n\t\t\t\ttradePartner->tradeItem->onTradeEvent(ON_TRADE_CANCEL, tradePartner);\n\t\t\t}\n\n\t\t\tif (player->tradeItem) {\n\t\t\t\terrorDescription = getTradeErrorDescription(ret2, tradeItem2);\n\t\t\t\tplayer->sendTextMessage(MESSAGE_EVENT_ADVANCE, errorDescription);\n\t\t\t\tplayer->tradeItem->onTradeEvent(ON_TRADE_CANCEL, player);\n\t\t\t}\n\t\t}\n\n\t\tplayer->setTradeState(TRADE_NONE);\n\t\tplayer->tradeItem = nullptr;\n\t\tplayer->tradePartner = nullptr;\n\t\tplayer->sendTradeClose();\n\n\t\ttradePartner->setTradeState(TRADE_NONE);\n\t\ttradePartner->tradeItem = nullptr;\n\t\ttradePartner->tradePartner = nullptr;\n\t\ttradePartner->sendTradeClose();\n\t}\n}\n\nstd::string Game::getTradeErrorDescription(ReturnValue ret, Item* item)\n{\n\tif (item) {\n\t\tif (ret == RETURNVALUE_NOTENOUGHCAPACITY) {\n\t\t\tstd::ostringstream ss;\n\t\t\tss << \"You do not have enough capacity to carry\";\n\n\t\t\tif (item->isStackable() && item->getItemCount() > 1) {\n\t\t\t\tss << \" these objects.\";\n\t\t\t} else {\n\t\t\t\tss << \" this object.\";\n\t\t\t}\n\n\t\t\tss << std::endl << ' ' << item->getWeightDescription();\n\t\t\treturn ss.str();\n\t\t} else if (ret == RETURNVALUE_NOTENOUGHROOM || ret == RETURNVALUE_CONTAINERNOTENOUGHROOM) {\n\t\t\tstd::ostringstream ss;\n\t\t\tss << \"You do not have enough room to carry\";\n\n\t\t\tif (item->isStackable() && item->getItemCount() > 1) {\n\t\t\t\tss << \" these objects.\";\n\t\t\t} else {\n\t\t\t\tss << \" this object.\";\n\t\t\t}\n\n\t\t\treturn ss.str();\n\t\t}\n\t}\n\treturn \"Trade could not be completed.\";\n}\n\nvoid Game::playerLookInTrade(uint32_t playerId, bool lookAtCounterOffer, uint8_t index)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPlayer* tradePartner = player->tradePartner;\n\tif (!tradePartner) {\n\t\treturn;\n\t}\n\n\tItem* tradeItem;\n\tif (lookAtCounterOffer) {\n\t\ttradeItem = tradePartner->getTradeItem();\n\t} else {\n\t\ttradeItem = player->getTradeItem();\n\t}\n\n\tif (!tradeItem) {\n\t\treturn;\n\t}\n\n\tconst Position& playerPosition = player->getPosition();\n\tconst Position& tradeItemPosition = tradeItem->getPosition();\n\n\tint32_t lookDistance = std::max<int32_t>(Position::getDistanceX(playerPosition, tradeItemPosition),\n\t                                         Position::getDistanceY(playerPosition, tradeItemPosition));\n\tif (index == 0) {\n\t\tg_events->eventPlayerOnLookInTrade(player, tradePartner, tradeItem, lookDistance);\n\t\treturn;\n\t}\n\n\tContainer* tradeContainer = tradeItem->getContainer();\n\tif (!tradeContainer) {\n\t\treturn;\n\t}\n\n\tstd::vector<const Container*> containers {tradeContainer};\n\tsize_t i = 0;\n\twhile (i < containers.size()) {\n\t\tconst Container* container = containers[i++];\n\t\tfor (Item* item : container->getItemList()) {\n\t\t\tContainer* tmpContainer = item->getContainer();\n\t\t\tif (tmpContainer) {\n\t\t\t\tcontainers.push_back(tmpContainer);\n\t\t\t}\n\n\t\t\tif (--index == 0) {\n\t\t\t\tg_events->eventPlayerOnLookInTrade(player, tradePartner, item, lookDistance);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid Game::playerCloseTrade(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tinternalCloseTrade(player);\n}\n\nvoid Game::internalCloseTrade(Player* player)\n{\n\tPlayer* tradePartner = player->tradePartner;\n\tif ((tradePartner && tradePartner->getTradeState() == TRADE_TRANSFER) || player->getTradeState() == TRADE_TRANSFER) {\n\t\treturn;\n\t}\n\n\tif (player->getTradeItem()) {\n\t\tauto it = tradeItems.find(player->getTradeItem());\n\t\tif (it != tradeItems.end()) {\n\t\t\tReleaseItem(it->first);\n\t\t\ttradeItems.erase(it);\n\t\t}\n\n\t\tplayer->tradeItem->onTradeEvent(ON_TRADE_CANCEL, player);\n\t\tplayer->tradeItem = nullptr;\n\t}\n\n\tplayer->setTradeState(TRADE_NONE);\n\tplayer->tradePartner = nullptr;\n\n\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"Trade cancelled.\");\n\tplayer->sendTradeClose();\n\n\tif (tradePartner) {\n\t\tif (tradePartner->getTradeItem()) {\n\t\t\tauto it = tradeItems.find(tradePartner->getTradeItem());\n\t\t\tif (it != tradeItems.end()) {\n\t\t\t\tReleaseItem(it->first);\n\t\t\t\ttradeItems.erase(it);\n\t\t\t}\n\n\t\t\ttradePartner->tradeItem->onTradeEvent(ON_TRADE_CANCEL, tradePartner);\n\t\t\ttradePartner->tradeItem = nullptr;\n\t\t}\n\n\t\ttradePartner->setTradeState(TRADE_NONE);\n\t\ttradePartner->tradePartner = nullptr;\n\n\t\ttradePartner->sendTextMessage(MESSAGE_STATUS_SMALL, \"Trade cancelled.\");\n\t\ttradePartner->sendTradeClose();\n\t}\n}\n\nvoid Game::playerPurchaseItem(uint32_t playerId, uint16_t spriteId, uint8_t count, uint8_t amount,\n                              bool ignoreCap/* = false*/, bool inBackpacks/* = false*/)\n{\n\tif (amount == 0 || amount > 100) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tint32_t onBuy, onSell;\n\n\tNpc* merchant = player->getShopOwner(onBuy, onSell);\n\tif (!merchant) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items.getItemIdByClientId(spriteId);\n\tif (it.id == 0) {\n\t\treturn;\n\t}\n\n\tuint8_t subType;\n\tif (it.isSplash() || it.isFluidContainer()) {\n\t\tsubType = clientFluidToServer(count);\n\t} else {\n\t\tsubType = count;\n\t}\n\n\tif (!player->hasShopItemForSale(it.id, subType)) {\n\t\treturn;\n\t}\n\n\tmerchant->onPlayerTrade(player, onBuy, it.id, subType, amount, ignoreCap, inBackpacks);\n}\n\nvoid Game::playerSellItem(uint32_t playerId, uint16_t spriteId, uint8_t count, uint8_t amount, bool ignoreEquipped)\n{\n\tif (amount == 0 || amount > 100) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tint32_t onBuy, onSell;\n\n\tNpc* merchant = player->getShopOwner(onBuy, onSell);\n\tif (!merchant) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items.getItemIdByClientId(spriteId);\n\tif (it.id == 0) {\n\t\treturn;\n\t}\n\n\tuint8_t subType;\n\tif (it.isSplash() || it.isFluidContainer()) {\n\t\tsubType = clientFluidToServer(count);\n\t} else {\n\t\tsubType = count;\n\t}\n\n\tmerchant->onPlayerTrade(player, onSell, it.id, subType, amount, ignoreEquipped);\n}\n\nvoid Game::playerCloseShop(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->closeShopWindow();\n}\n\nvoid Game::playerLookInShop(uint32_t playerId, uint16_t spriteId, uint8_t count)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tint32_t onBuy, onSell;\n\n\tNpc* merchant = player->getShopOwner(onBuy, onSell);\n\tif (!merchant) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items.getItemIdByClientId(spriteId);\n\tif (it.id == 0) {\n\t\treturn;\n\t}\n\n\tint32_t subType;\n\tif (it.isFluidContainer() || it.isSplash()) {\n\t\tsubType = clientFluidToServer(count);\n\t} else {\n\t\tsubType = count;\n\t}\n\n\tif (!player->hasShopItemForSale(it.id, subType)) {\n\t\treturn;\n\t}\n\n\tif (!g_events->eventPlayerOnLookInShop(player, &it, subType)) {\n\t\treturn;\n\t}\n\n\tstd::ostringstream ss;\n\tss << \"You see \" << Item::getDescription(it, 1, nullptr, subType);\n\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, ss.str());\n}\n\nvoid Game::playerLookAt(uint32_t playerId, const Position& pos, uint8_t stackPos)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tThing* thing = internalGetThing(player, pos, stackPos, 0, STACKPOS_LOOK);\n\tif (!thing) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tPosition thingPos = thing->getPosition();\n\tif (!player->canSee(thingPos)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_NOTPOSSIBLE);\n\t\treturn;\n\t}\n\n\tPosition playerPos = player->getPosition();\n\n\tint32_t lookDistance;\n\tif (thing != player) {\n\t\tlookDistance = std::max<int32_t>(Position::getDistanceX(playerPos, thingPos), Position::getDistanceY(playerPos, thingPos));\n\t\tif (playerPos.z != thingPos.z) {\n\t\t\tlookDistance += 15;\n\t\t}\n\t} else {\n\t\tlookDistance = -1;\n\t}\n\n\tg_events->eventPlayerOnLook(player, pos, thing, stackPos, lookDistance);\n}\n\nvoid Game::playerLookInBattleList(uint32_t playerId, uint32_t creatureId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (!creature) {\n\t\treturn;\n\t}\n\n\tif (!player->canSeeCreature(creature)) {\n\t\treturn;\n\t}\n\n\tconst Position& creaturePos = creature->getPosition();\n\tif (!player->canSee(creaturePos)) {\n\t\treturn;\n\t}\n\n\tint32_t lookDistance;\n\tif (creature != player) {\n\t\tconst Position& playerPos = player->getPosition();\n\t\tlookDistance = std::max<int32_t>(Position::getDistanceX(playerPos, creaturePos), Position::getDistanceY(playerPos, creaturePos));\n\t\tif (playerPos.z != creaturePos.z) {\n\t\t\tlookDistance += 15;\n\t\t}\n\t} else {\n\t\tlookDistance = -1;\n\t}\n\n\tg_events->eventPlayerOnLookInBattleList(player, creature, lookDistance);\n}\n\nvoid Game::playerCancelAttackAndFollow(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayerSetAttackedCreature(playerId, 0);\n\tplayerFollowCreature(playerId, 0);\n\tplayer->stopWalk();\n}\n\nvoid Game::playerSetAttackedCreature(uint32_t playerId, uint32_t creatureId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (player->getAttackedCreature() && creatureId == 0) {\n\t\tplayer->setAttackedCreature(nullptr);\n\t\tplayer->sendCancelTarget();\n\t\treturn;\n\t}\n\n\tCreature* attackCreature = getCreatureByID(creatureId);\n\tif (!attackCreature) {\n\t\tplayer->setAttackedCreature(nullptr);\n\t\tplayer->sendCancelTarget();\n\t\treturn;\n\t}\n\n\tReturnValue ret = Combat::canTargetCreature(player, attackCreature);\n\tif (ret != RETURNVALUE_NOERROR) {\n\t\tplayer->sendCancelMessage(ret);\n\t\tplayer->sendCancelTarget();\n\t\tplayer->setAttackedCreature(nullptr);\n\t\treturn;\n\t}\n\n\tplayer->setAttackedCreature(attackCreature);\n\tg_dispatcher.addTask(createTask(std::bind(&Game::updateCreatureWalk, this, player->getID())));\n}\n\nvoid Game::playerFollowCreature(uint32_t playerId, uint32_t creatureId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->setAttackedCreature(nullptr);\n\tg_dispatcher.addTask(createTask(std::bind(&Game::updateCreatureWalk, this, player->getID())));\n\tplayer->setFollowCreature(getCreatureByID(creatureId));\n}\n\nvoid Game::playerSetFightModes(uint32_t playerId, fightMode_t fightMode, bool chaseMode, bool secureMode)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->setFightMode(fightMode);\n\tplayer->setChaseMode(chaseMode);\n\tplayer->setSecureMode(secureMode);\n}\n\nvoid Game::playerRequestAddVip(uint32_t playerId, const std::string& name)\n{\n\tif (name.length() > 20) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPlayer* vipPlayer = getPlayerByName(name);\n\tif (!vipPlayer) {\n\t\tuint32_t guid;\n\t\tbool specialVip;\n\t\tstd::string formattedName = name;\n\t\tif (!IOLoginData::getGuidByNameEx(guid, specialVip, formattedName)) {\n\t\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"A player with this name does not exist.\");\n\t\t\treturn;\n\t\t}\n\n\t\tif (specialVip && !player->hasFlag(PlayerFlag_SpecialVIP)) {\n\t\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"You can not add this player.\");\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->addVIP(guid, formattedName, VIPSTATUS_OFFLINE);\n\t} else {\n\t\tif (vipPlayer->hasFlag(PlayerFlag_SpecialVIP) && !player->hasFlag(PlayerFlag_SpecialVIP)) {\n\t\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"You can not add this player.\");\n\t\t\treturn;\n\t\t}\n\n\t\tif (!vipPlayer->isInGhostMode() || player->isAccessPlayer()) {\n\t\t\tplayer->addVIP(vipPlayer->getGUID(), vipPlayer->getName(), VIPSTATUS_ONLINE);\n\t\t} else {\n\t\t\tplayer->addVIP(vipPlayer->getGUID(), vipPlayer->getName(), VIPSTATUS_OFFLINE);\n\t\t}\n\t}\n}\n\nvoid Game::playerRequestRemoveVip(uint32_t playerId, uint32_t guid)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->removeVIP(guid);\n}\n\nvoid Game::playerRequestEditVip(uint32_t playerId, uint32_t guid, const std::string& description, uint32_t icon, bool notify)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->editVIP(guid, description, icon, notify);\n}\n\nvoid Game::playerTurn(uint32_t playerId, Direction dir)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!g_events->eventPlayerOnTurn(player, dir)) {\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\tinternalCreatureTurn(player, dir);\n}\n\nvoid Game::playerRequestOutfit(uint32_t playerId)\n{\n\tif (!g_config.getBoolean(ConfigManager::ALLOW_CHANGEOUTFIT)) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->sendOutfitWindow();\n}\n\nvoid Game::playerToggleMount(uint32_t playerId, bool mount)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->toggleMount(mount);\n}\n\nvoid Game::playerChangeOutfit(uint32_t playerId, Outfit_t outfit)\n{\n\tif (!g_config.getBoolean(ConfigManager::ALLOW_CHANGEOUTFIT)) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (outfit.lookMount != 0) {\n\t\tMount* mount = mounts.getMountByClientID(outfit.lookMount);\n\t\tif (!mount) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (!player->hasMount(mount)) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (player->isMounted()) {\n\t\t\tMount* prevMount = mounts.getMountByID(player->getCurrentMount());\n\t\t\tif (prevMount) {\n\t\t\t\tchangeSpeed(player, mount->speed - prevMount->speed);\n\t\t\t}\n\n\t\t\tplayer->setCurrentMount(mount->id);\n\t\t} else {\n\t\t\tplayer->setCurrentMount(mount->id);\n\t\t\toutfit.lookMount = 0;\n\t\t}\n\t} else if (player->isMounted()) {\n\t\tplayer->dismount();\n\t}\n\n\tif (player->canWear(outfit.lookType, outfit.lookAddons)) {\n\t\tplayer->defaultOutfit = outfit;\n\n\t\tif (player->hasCondition(CONDITION_OUTFIT)) {\n\t\t\treturn;\n\t\t}\n\n\t\tinternalCreatureChangeOutfit(player, outfit);\n\t}\n}\n\nvoid Game::playerShowQuestLog(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->sendQuestLog();\n}\n\nvoid Game::playerShowQuestLine(uint32_t playerId, uint16_t questId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tQuest* quest = quests.getQuestByID(questId);\n\tif (!quest) {\n\t\treturn;\n\t}\n\n\tplayer->sendQuestLine(quest);\n}\n\nvoid Game::playerSay(uint32_t playerId, uint16_t channelId, SpeakClasses type,\n                     const std::string& receiver, const std::string& text)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->resetIdleTime();\n\n\tuint32_t muteTime = player->isMuted();\n\tif (muteTime > 0) {\n\t\tstd::ostringstream ss;\n\t\tss << \"You are still muted for \" << muteTime << \" seconds.\";\n\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, ss.str());\n\t\treturn;\n\t}\n\n\tif (playerSayCommand(player, text)) {\n\t\treturn;\n\t}\n\n\tif (playerSaySpell(player, type, text)) {\n\t\treturn;\n\t}\n\n\tif (!text.empty() && text.front() == '/' && player->isAccessPlayer()) {\n\t\treturn;\n\t}\n\n\tif (type != TALKTYPE_PRIVATE_PN) {\n\t\tplayer->removeMessageBuffer();\n\t}\n\n\tswitch (type) {\n\t\tcase TALKTYPE_SAY:\n\t\t\tinternalCreatureSay(player, TALKTYPE_SAY, text, false);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_WHISPER:\n\t\t\tplayerWhisper(player, text);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_YELL:\n\t\t\tplayerYell(player, text);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_PRIVATE_TO:\n\t\tcase TALKTYPE_PRIVATE_RED_TO:\n\t\t\tplayerSpeakTo(player, type, receiver, text);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_CHANNEL_O:\n\t\tcase TALKTYPE_CHANNEL_Y:\n\t\tcase TALKTYPE_CHANNEL_R1:\n\t\t\tg_chat->talkToChannel(*player, type, text, channelId);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_PRIVATE_PN:\n\t\t\tplayerSpeakToNpc(player, text);\n\t\t\tbreak;\n\n\t\tcase TALKTYPE_BROADCAST:\n\t\t\tplayerBroadcastMessage(player, text);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t}\n}\n\nbool Game::playerSayCommand(Player* player, const std::string& text)\n{\n\tif (text.empty()) {\n\t\treturn false;\n\t}\n\n\tchar firstCharacter = text.front();\n\tfor (char commandTag : commandTags) {\n\t\tif (commandTag == firstCharacter) {\n\t\t\tif (commands.exeCommand(*player, text)) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\treturn false;\n}\n\nbool Game::playerSaySpell(Player* player, SpeakClasses type, const std::string& text)\n{\n\tstd::string words = text;\n\n\tTalkActionResult_t result = g_talkActions->playerSaySpell(player, type, words);\n\tif (result == TALKACTION_BREAK) {\n\t\treturn true;\n\t}\n\n\tresult = g_spells->playerSaySpell(player, words);\n\tif (result == TALKACTION_BREAK) {\n\t\tif (!g_config.getBoolean(ConfigManager::EMOTE_SPELLS)) {\n\t\t\treturn internalCreatureSay(player, TALKTYPE_SAY, words, false);\n\t\t} else {\n\t\t\treturn internalCreatureSay(player, TALKTYPE_MONSTER_SAY, words, false);\n\t\t}\n\n\t} else if (result == TALKACTION_FAILED) {\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nvoid Game::playerWhisper(Player* player, const std::string& text)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, player->getPosition(), false, false,\n\t              Map::maxClientViewportX, Map::maxClientViewportX,\n\t              Map::maxClientViewportY, Map::maxClientViewportY);\n\n\t//send to client\n\tfor (Creature* spectator : list) {\n\t\tif (Player* spectatorPlayer = spectator->getPlayer()) {\n\t\t\tif (!Position::areInRange<1, 1>(player->getPosition(), spectatorPlayer->getPosition())) {\n\t\t\t\tspectatorPlayer->sendCreatureSay(player, TALKTYPE_WHISPER, \"pspsps\");\n\t\t\t} else {\n\t\t\t\tspectatorPlayer->sendCreatureSay(player, TALKTYPE_WHISPER, text);\n\t\t\t}\n\t\t}\n\t}\n\n\t//event method\n\tfor (Creature* spectator : list) {\n\t\tspectator->onCreatureSay(player, TALKTYPE_WHISPER, text);\n\t}\n}\n\nbool Game::playerYell(Player* player, const std::string& text)\n{\n\tif (player->getLevel() == 1) {\n\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"You may not yell as long as you are on level 1.\");\n\t\treturn false;\n\t}\n\n\tif (player->hasCondition(CONDITION_YELLTICKS)) {\n\t\tplayer->sendCancelMessage(RETURNVALUE_YOUAREEXHAUSTED);\n\t\treturn false;\n\t}\n\n\tif (player->getAccountType() < ACCOUNT_TYPE_GAMEMASTER) {\n\t\tCondition* condition = Condition::createCondition(CONDITIONID_DEFAULT, CONDITION_YELLTICKS, 30000, 0);\n\t\tplayer->addCondition(condition);\n\t}\n\n\tinternalCreatureSay(player, TALKTYPE_YELL, asUpperCaseString(text), false);\n\treturn true;\n}\n\nbool Game::playerSpeakTo(Player* player, SpeakClasses type, const std::string& receiver,\n                         const std::string& text)\n{\n\tPlayer* toPlayer = getPlayerByName(receiver);\n\tif (!toPlayer) {\n\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"A player with this name is not online.\");\n\t\treturn false;\n\t}\n\n\tif (type == TALKTYPE_PRIVATE_RED_TO && (player->hasFlag(PlayerFlag_CanTalkRedPrivate) || player->getAccountType() >= ACCOUNT_TYPE_GAMEMASTER)) {\n\t\ttype = TALKTYPE_PRIVATE_RED_FROM;\n\t} else {\n\t\ttype = TALKTYPE_PRIVATE_FROM;\n\t}\n\n\ttoPlayer->sendPrivateMessage(player, type, text);\n\ttoPlayer->onCreatureSay(player, type, text);\n\n\tif (toPlayer->isInGhostMode() && !player->isAccessPlayer()) {\n\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, \"A player with this name is not online.\");\n\t} else {\n\t\tstd::ostringstream ss;\n\t\tss << \"Message sent to \" << toPlayer->getName() << '.';\n\t\tplayer->sendTextMessage(MESSAGE_STATUS_SMALL, ss.str());\n\t}\n\treturn true;\n}\n\nvoid Game::playerSpeakToNpc(Player* player, const std::string& text)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, player->getPosition());\n\tfor (Creature* spectator : list) {\n\t\tif (spectator->getNpc()) {\n\t\t\tspectator->onCreatureSay(player, TALKTYPE_PRIVATE_PN, text);\n\t\t}\n\t}\n}\n\n//--\nbool Game::canThrowObjectTo(const Position& fromPos, const Position& toPos, bool checkLineOfSight /*= true*/,\n                            int32_t rangex /*= Map::maxClientViewportX*/, int32_t rangey /*= Map::maxClientViewportY*/) const\n{\n\treturn map.canThrowObjectTo(fromPos, toPos, checkLineOfSight, rangex, rangey);\n}\n\nbool Game::isSightClear(const Position& fromPos, const Position& toPos, bool floorCheck) const\n{\n\treturn map.isSightClear(fromPos, toPos, floorCheck);\n}\n\nbool Game::internalCreatureTurn(Creature* creature, Direction dir)\n{\n\tif (creature->getDirection() == dir) {\n\t\treturn false;\n\t}\n\n\tcreature->setDirection(dir);\n\n\t//send to client\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureTurn(creature);\n\t}\n\treturn true;\n}\n\nbool Game::internalCreatureSay(Creature* creature, SpeakClasses type, const std::string& text,\n                               bool ghostMode, SpectatorVec* listPtr/* = nullptr*/, const Position* pos/* = nullptr*/)\n{\n\tif (text.empty()) {\n\t\treturn false;\n\t}\n\n\tif (!pos) {\n\t\tpos = &creature->getPosition();\n\t}\n\n\tSpectatorVec list;\n\n\tif (!listPtr || listPtr->empty()) {\n\t\t// This somewhat complex construct ensures that the cached SpectatorVec\n\t\t// is used if available and if it can be used, else a local vector is\n\t\t// used (hopefully the compiler will optimize away the construction of\n\t\t// the temporary when it's not used).\n\t\tif (type != TALKTYPE_YELL && type != TALKTYPE_MONSTER_YELL) {\n\t\t\tmap.getSpectators(list, *pos, false, false,\n\t\t\t              Map::maxClientViewportX, Map::maxClientViewportX,\n\t\t\t              Map::maxClientViewportY, Map::maxClientViewportY);\n\t\t} else {\n\t\t\tmap.getSpectators(list, *pos, true, false, 18, 18, 14, 14);\n\t\t}\n\t} else {\n\t\tlist = (*listPtr);\n\t}\n\n\t//send to client\n\tfor (Creature* spectator : list) {\n\t\tif (Player* tmpPlayer = spectator->getPlayer()) {\n\t\t\tif (!ghostMode || tmpPlayer->canSeeCreature(creature)) {\n\t\t\t\ttmpPlayer->sendCreatureSay(creature, type, text, pos);\n\t\t\t}\n\t\t}\n\t}\n\n\t//event method\n\tfor (Creature* spectator : list) {\n\t\tspectator->onCreatureSay(creature, type, text);\n\t}\n\treturn true;\n}\n\nvoid Game::checkCreatureWalk(uint32_t creatureId)\n{\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (creature && creature->getHealth() > 0) {\n\t\tcreature->onWalk();\n\t\tcleanup();\n\t}\n}\n\nvoid Game::updateCreatureWalk(uint32_t creatureId)\n{\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (creature && creature->getHealth() > 0) {\n\t\tcreature->goToFollowCreature();\n\t}\n}\n\nvoid Game::checkCreatureAttack(uint32_t creatureId)\n{\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (creature && creature->getHealth() > 0) {\n\t\tcreature->onAttacking(0);\n\t}\n}\n\nvoid Game::addCreatureCheck(Creature* creature)\n{\n\tcreature->creatureCheck = true;\n\n\tif (creature->inCheckCreaturesVector) {\n\t\t// already in a vector\n\t\treturn;\n\t}\n\n\tcreature->inCheckCreaturesVector = true;\n\tcheckCreatureLists[uniform_random(0, EVENT_CREATURECOUNT - 1)].push_back(creature);\n\tcreature->incrementReferenceCounter();\n}\n\nvoid Game::removeCreatureCheck(Creature* creature)\n{\n\tif (creature->inCheckCreaturesVector) {\n\t\tcreature->creatureCheck = false;\n\t}\n}\n\nvoid Game::checkCreatures(size_t index)\n{\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_CHECK_CREATURE_INTERVAL, std::bind(&Game::checkCreatures, this, (index + 1) % EVENT_CREATURECOUNT)));\n\n\tauto& checkCreatureList = checkCreatureLists[index];\n\tauto it = checkCreatureList.begin(), end = checkCreatureList.end();\n\twhile (it != end) {\n\t\tCreature* creature = *it;\n\t\tif (creature->creatureCheck) {\n\t\t\tif (creature->getHealth() > 0) {\n\t\t\t\tcreature->onThink(EVENT_CREATURE_THINK_INTERVAL);\n\t\t\t\tcreature->onAttacking(EVENT_CREATURE_THINK_INTERVAL);\n\t\t\t\tcreature->executeConditions(EVENT_CREATURE_THINK_INTERVAL);\n\t\t\t} else {\n\t\t\t\tcreature->onDeath();\n\t\t\t}\n\t\t\t++it;\n\t\t} else {\n\t\t\tcreature->inCheckCreaturesVector = false;\n\t\t\tit = checkCreatureList.erase(it);\n\t\t\tReleaseCreature(creature);\n\t\t}\n\t}\n\n\tcleanup();\n}\n\nvoid Game::changeSpeed(Creature* creature, int32_t varSpeedDelta)\n{\n\tint32_t varSpeed = creature->getSpeed() - creature->getBaseSpeed();\n\tvarSpeed += varSpeedDelta;\n\n\tcreature->setSpeed(varSpeed);\n\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), false, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendChangeSpeed(creature, creature->getStepSpeed());\n\t}\n}\n\nvoid Game::internalCreatureChangeOutfit(Creature* creature, const Outfit_t& outfit)\n{\n\tif (!g_events->eventCreatureOnChangeOutfit(creature, outfit)) {\n\t\treturn;\n\t}\n\n\tcreature->setCurrentOutfit(outfit);\n\n\tif (creature->isInvisible()) {\n\t\treturn;\n\t}\n\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureChangeOutfit(creature, outfit);\n\t}\n}\n\nvoid Game::internalCreatureChangeVisible(Creature* creature, bool visible)\n{\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureChangeVisible(creature, visible);\n\t}\n}\n\nvoid Game::changeLight(const Creature* creature)\n{\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureLight(creature);\n\t}\n}\n\nbool Game::combatBlockHit(CombatDamage& damage, Creature* attacker, Creature* target, bool checkDefense, bool checkArmor, bool field)\n{\n\tif (damage.primary.type == COMBAT_NONE && damage.secondary.type == COMBAT_NONE) {\n\t\treturn true;\n\t}\n\n\tif (target->getPlayer() && target->isInGhostMode()) {\n\t\treturn true;\n\t}\n\n\tif (damage.primary.value > 0) {\n\t\treturn false;\n\t}\n\n\tstatic const auto sendBlockEffect = [this](BlockType_t blockType, CombatType_t combatType, const Position& targetPos) {\n\t\tif (blockType == BLOCK_DEFENSE) {\n\t\t\taddMagicEffect(targetPos, CONST_ME_POFF);\n\t\t} else if (blockType == BLOCK_ARMOR) {\n\t\t\taddMagicEffect(targetPos, CONST_ME_BLOCKHIT);\n\t\t} else if (blockType == BLOCK_IMMUNITY) {\n\t\t\tuint8_t hitEffect = 0;\n\t\t\tswitch (combatType) {\n\t\t\t\tcase COMBAT_UNDEFINEDDAMAGE: {\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tcase COMBAT_ENERGYDAMAGE:\n\t\t\t\tcase COMBAT_FIREDAMAGE:\n\t\t\t\tcase COMBAT_PHYSICALDAMAGE:\n\t\t\t\tcase COMBAT_ICEDAMAGE:\n\t\t\t\tcase COMBAT_DEATHDAMAGE: {\n\t\t\t\t\thitEffect = CONST_ME_BLOCKHIT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcase COMBAT_EARTHDAMAGE: {\n\t\t\t\t\thitEffect = CONST_ME_GREEN_RINGS;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcase COMBAT_HOLYDAMAGE: {\n\t\t\t\t\thitEffect = CONST_ME_HOLYDAMAGE;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdefault: {\n\t\t\t\t\thitEffect = CONST_ME_POFF;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\taddMagicEffect(targetPos, hitEffect);\n\t\t}\n\t};\n\n\tBlockType_t primaryBlockType, secondaryBlockType;\n\tif (damage.primary.type != COMBAT_NONE) {\n\t\tdamage.primary.value = -damage.primary.value;\n\t\tprimaryBlockType = target->blockHit(attacker, damage.primary.type, damage.primary.value, checkDefense, checkArmor, field);\n\n\t\tdamage.primary.value = -damage.primary.value;\n\t\tsendBlockEffect(primaryBlockType, damage.primary.type, target->getPosition());\n\t} else {\n\t\tprimaryBlockType = BLOCK_NONE;\n\t}\n\n\tif (damage.secondary.type != COMBAT_NONE) {\n\t\tdamage.secondary.value = -damage.secondary.value;\n\t\tsecondaryBlockType = target->blockHit(attacker, damage.secondary.type, damage.secondary.value, false, false, field);\n\n\t\tdamage.secondary.value = -damage.secondary.value;\n\t\tsendBlockEffect(secondaryBlockType, damage.secondary.type, target->getPosition());\n\t} else {\n\t\tsecondaryBlockType = BLOCK_NONE;\n\t}\n\treturn (primaryBlockType != BLOCK_NONE) && (secondaryBlockType != BLOCK_NONE);\n}\n\nvoid Game::combatGetTypeInfo(CombatType_t combatType, Creature* target, TextColor_t& color, uint8_t& effect)\n{\n\tswitch (combatType) {\n\t\tcase COMBAT_PHYSICALDAMAGE: {\n\t\t\tItem* splash = nullptr;\n\t\t\tswitch (target->getRace()) {\n\t\t\t\tcase RACE_VENOM:\n\t\t\t\t\tcolor = TEXTCOLOR_LIGHTGREEN;\n\t\t\t\t\teffect = CONST_ME_HITBYPOISON;\n\t\t\t\t\tsplash = Item::CreateItem(ITEM_SMALLSPLASH, FLUID_GREEN);\n\t\t\t\t\tbreak;\n\t\t\t\tcase RACE_BLOOD:\n\t\t\t\t\tcolor = TEXTCOLOR_RED;\n\t\t\t\t\teffect = CONST_ME_DRAWBLOOD;\n\t\t\t\t\tif (const Tile* tile = target->getTile()) {\n\t\t\t\t\t\tif (!tile->hasFlag(TILESTATE_PROTECTIONZONE)) {\n\t\t\t\t\t\t\tsplash = Item::CreateItem(ITEM_SMALLSPLASH, FLUID_BLOOD);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tcase RACE_UNDEAD:\n\t\t\t\t\tcolor = TEXTCOLOR_LIGHTGREY;\n\t\t\t\t\teffect = CONST_ME_HITAREA;\n\t\t\t\t\tbreak;\n\t\t\t\tcase RACE_FIRE:\n\t\t\t\t\tcolor = TEXTCOLOR_ORANGE;\n\t\t\t\t\teffect = CONST_ME_DRAWBLOOD;\n\t\t\t\t\tbreak;\n\t\t\t\tcase RACE_ENERGY:\n\t\t\t\t\tcolor = TEXTCOLOR_PURPLE;\n\t\t\t\t\teffect = CONST_ME_ENERGYHIT;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tcolor = TEXTCOLOR_NONE;\n\t\t\t\t\teffect = CONST_ME_NONE;\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (splash) {\n\t\t\t\tinternalAddItem(target->getTile(), splash, INDEX_WHEREEVER, FLAG_NOLIMIT);\n\t\t\t\tstartDecay(splash);\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\n\t\tcase COMBAT_ENERGYDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_PURPLE;\n\t\t\teffect = CONST_ME_ENERGYHIT;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase COMBAT_EARTHDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_LIGHTGREEN;\n\t\t\teffect = CONST_ME_GREEN_RINGS;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase COMBAT_DROWNDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_LIGHTBLUE;\n\t\t\teffect = CONST_ME_LOSEENERGY;\n\t\t\tbreak;\n\t\t}\n\t\tcase COMBAT_FIREDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_ORANGE;\n\t\t\teffect = CONST_ME_HITBYFIRE;\n\t\t\tbreak;\n\t\t}\n\t\tcase COMBAT_ICEDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_SKYBLUE;\n\t\t\teffect = CONST_ME_ICEATTACK;\n\t\t\tbreak;\n\t\t}\n\t\tcase COMBAT_HOLYDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_YELLOW;\n\t\t\teffect = CONST_ME_HOLYDAMAGE;\n\t\t\tbreak;\n\t\t}\n\t\tcase COMBAT_DEATHDAMAGE: {\n\t\t\tcolor = TEXTCOLOR_DARKRED;\n\t\t\teffect = CONST_ME_SMALLCLOUDS;\n\t\t\tbreak;\n\t\t}\n\t\tcase COMBAT_LIFEDRAIN: {\n\t\t\tcolor = TEXTCOLOR_RED;\n\t\t\teffect = CONST_ME_MAGIC_RED;\n\t\t\tbreak;\n\t\t}\n\t\tdefault: {\n\t\t\tcolor = TEXTCOLOR_NONE;\n\t\t\teffect = CONST_ME_NONE;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nbool Game::combatChangeHealth(Creature* attacker, Creature* target, CombatDamage& damage)\n{\n\tconst Position& targetPos = target->getPosition();\n\tif (damage.primary.value > 0) {\n\t\tif (target->getHealth() <= 0) {\n\t\t\treturn false;\n\t\t}\n\n\t\tPlayer* attackerPlayer;\n\t\tif (attacker) {\n\t\t\tattackerPlayer = attacker->getPlayer();\n\t\t} else {\n\t\t\tattackerPlayer = nullptr;\n\t\t}\n\n\t\tPlayer* targetPlayer = target->getPlayer();\n\t\tif (attackerPlayer && targetPlayer && attackerPlayer->getSkull() == SKULL_BLACK && attackerPlayer->getSkullClient(targetPlayer) == SKULL_NONE) {\n\t\t\treturn false;\n\t\t}\n\n\t\tif (damage.origin != ORIGIN_NONE) {\n\t\t\tconst auto& events = target->getCreatureEvents(CREATURE_EVENT_HEALTHCHANGE);\n\t\t\tif (!events.empty()) {\n\t\t\t\tfor (CreatureEvent* creatureEvent : events) {\n\t\t\t\t\tcreatureEvent->executeHealthChange(target, attacker, damage);\n\t\t\t\t}\n\t\t\t\tdamage.origin = ORIGIN_NONE;\n\t\t\t\treturn combatChangeHealth(attacker, target, damage);\n\t\t\t}\n\t\t}\n\n\t\tint32_t realHealthChange = target->getHealth();\n\t\ttarget->gainHealth(attacker, damage.primary.value);\n\t\trealHealthChange = target->getHealth() - realHealthChange;\n\n\t\tif (realHealthChange > 0 && !target->isInGhostMode()) {\n\t\t\tstd::string damageString = std::to_string(realHealthChange) + (realHealthChange != 1 ? \" hitpoints.\" : \" hitpoint.\");\n\n\t\t\tstd::string spectatorMessage;\n\t\t\tif (!attacker) {\n\t\t\t\tspectatorMessage += ucfirst(target->getNameDescription());\n\t\t\t\tspectatorMessage += \" was healed for \" + damageString;\n\t\t\t} else {\n\t\t\t\tspectatorMessage += ucfirst(attacker->getNameDescription());\n\t\t\t\tspectatorMessage += \" healed \";\n\t\t\t\tif (attacker == target) {\n\t\t\t\t\tspectatorMessage += (targetPlayer ? (targetPlayer->getSex() == PLAYERSEX_FEMALE ? \"herself\" : \"himself\") : \"itself\");\n\t\t\t\t} else {\n\t\t\t\t\tspectatorMessage += target->getNameDescription();\n\t\t\t\t}\n\t\t\t\tspectatorMessage += \" for \" + damageString;\n\t\t\t}\n\n\t\t\tTextMessage message;\n\t\t\tmessage.position = targetPos;\n\t\t\tmessage.primary.value = realHealthChange;\n\t\t\tmessage.primary.color = TEXTCOLOR_MAYABLUE;\n\n\t\t\tSpectatorVec list;\n\t\t\tmap.getSpectators(list, targetPos, false, true);\n\t\t\tfor (Creature* spectator : list) {\n\t\t\t\tPlayer* tmpPlayer = spectator->getPlayer();\n\t\t\t\tif (tmpPlayer == attackerPlayer && attackerPlayer != targetPlayer) {\n\t\t\t\t\tmessage.type = MESSAGE_HEALED;\n\t\t\t\t\tmessage.text = \"You heal \" + target->getNameDescription() + \" for \" + damageString;\n\t\t\t\t} else if (tmpPlayer == targetPlayer) {\n\t\t\t\t\tmessage.type = MESSAGE_HEALED;\n\t\t\t\t\tif (!attacker) {\n\t\t\t\t\t\tmessage.text = \"You were healed for \" + damageString;\n\t\t\t\t\t} else if (targetPlayer == attackerPlayer) {\n\t\t\t\t\t\tmessage.text = \"You heal yourself for \" + damageString;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmessage.text = \"You were healed by \" + attacker->getNameDescription() + \" for \" + damageString;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tmessage.type = MESSAGE_HEALED_OTHERS;\n\t\t\t\t\tmessage.text = spectatorMessage;\n\t\t\t\t}\n\t\t\t\ttmpPlayer->sendTextMessage(message);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!target->isAttackable()) {\n\t\t\tif (!target->isInGhostMode()) {\n\t\t\t\taddMagicEffect(targetPos, CONST_ME_POFF);\n\t\t\t}\n\t\t\treturn true;\n\t\t}\n\n\t\tPlayer* attackerPlayer;\n\t\tif (attacker) {\n\t\t\tattackerPlayer = attacker->getPlayer();\n\t\t} else {\n\t\t\tattackerPlayer = nullptr;\n\t\t}\n\n\t\tPlayer* targetPlayer = target->getPlayer();\n\t\tif (attackerPlayer && targetPlayer && attackerPlayer->getSkull() == SKULL_BLACK && attackerPlayer->getSkullClient(targetPlayer) == SKULL_NONE) {\n\t\t\treturn false;\n\t\t}\n\n\t\tdamage.primary.value = std::abs(damage.primary.value);\n\t\tdamage.secondary.value = std::abs(damage.secondary.value);\n\n\t\tint32_t healthChange = damage.primary.value + damage.secondary.value;\n\t\tif (healthChange == 0) {\n\t\t\treturn true;\n\t\t}\n\n\t\tTextMessage message;\n\t\tmessage.position = targetPos;\n\n\t\tSpectatorVec list;\n\t\tif (target->hasCondition(CONDITION_MANASHIELD) && damage.primary.type != COMBAT_UNDEFINEDDAMAGE) {\n\t\t\tint32_t manaDamage = std::min<int32_t>(target->getMana(), healthChange);\n\t\t\tif (manaDamage != 0) {\n\t\t\t\tif (damage.origin != ORIGIN_NONE) {\n\t\t\t\t\tconst auto& events = target->getCreatureEvents(CREATURE_EVENT_MANACHANGE);\n\t\t\t\t\tif (!events.empty()) {\n\t\t\t\t\t\tfor (CreatureEvent* creatureEvent : events) {\n\t\t\t\t\t\t\tcreatureEvent->executeManaChange(target, attacker, healthChange, damage.origin);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (healthChange == 0) {\n\t\t\t\t\t\t\treturn true;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmanaDamage = std::min<int32_t>(target->getMana(), healthChange);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttarget->drainMana(attacker, manaDamage);\n\t\t\t\tmap.getSpectators(list, targetPos, true, true);\n\t\t\t\taddMagicEffect(list, targetPos, CONST_ME_LOSEENERGY);\n\n\t\t\t\tstd::string damageString = std::to_string(manaDamage);\n\t\t\t\tstd::string spectatorMessage = ucfirst(target->getNameDescription()) + \" loses \" + damageString + \" mana\";\n\t\t\t\tif (attacker) {\n\t\t\t\t\tspectatorMessage += \" due to \";\n\t\t\t\t\tif (attacker == target) {\n\t\t\t\t\t\tspectatorMessage += (targetPlayer ? (targetPlayer->getSex() == PLAYERSEX_FEMALE ? \"her own attack\" : \"his own attack\") : \"its own attack\");\n\t\t\t\t\t} else {\n\t\t\t\t\t\tspectatorMessage += \"an attack by \" + attacker->getNameDescription();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tspectatorMessage += '.';\n\n\t\t\t\tmessage.primary.value = manaDamage;\n\t\t\t\tmessage.primary.color = TEXTCOLOR_BLUE;\n\n\t\t\t\tfor (Creature* spectator : list) {\n\t\t\t\t\tPlayer* tmpPlayer = spectator->getPlayer();\n\t\t\t\t\tif (tmpPlayer->getPosition().z != targetPos.z) {\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\tif (tmpPlayer == attackerPlayer && attackerPlayer != targetPlayer) {\n\t\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_DEALT;\n\t\t\t\t\t\tmessage.text = ucfirst(target->getNameDescription()) + \" loses \" + damageString + \" mana due to your attack.\";\n\t\t\t\t\t} else if (tmpPlayer == targetPlayer) {\n\t\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_RECEIVED;\n\t\t\t\t\t\tif (!attacker) {\n\t\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana.\";\n\t\t\t\t\t\t} else if (targetPlayer == attackerPlayer) {\n\t\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana due to your own attack.\";\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana due to an attack by \" + attacker->getNameDescription() + '.';\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_OTHERS;\n\t\t\t\t\t\tmessage.text = spectatorMessage;\n\t\t\t\t\t}\n\t\t\t\t\ttmpPlayer->sendTextMessage(message);\n\t\t\t\t}\n\n\t\t\t\tdamage.primary.value -= manaDamage;\n\t\t\t\tif (damage.primary.value < 0) {\n\t\t\t\t\tdamage.secondary.value = std::max<int32_t>(0, damage.secondary.value + damage.primary.value);\n\t\t\t\t\tdamage.primary.value = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tint32_t realDamage = damage.primary.value + damage.secondary.value;\n\t\tif (realDamage == 0) {\n\t\t\treturn true;\n\t\t}\n\n\t\tif (damage.origin != ORIGIN_NONE) {\n\t\t\tconst auto& events = target->getCreatureEvents(CREATURE_EVENT_HEALTHCHANGE);\n\t\t\tif (!events.empty()) {\n\t\t\t\tfor (CreatureEvent* creatureEvent : events) {\n\t\t\t\t\tcreatureEvent->executeHealthChange(target, attacker, damage);\n\t\t\t\t}\n\t\t\t\tdamage.origin = ORIGIN_NONE;\n\t\t\t\treturn combatChangeHealth(attacker, target, damage);\n\t\t\t}\n\t\t}\n\n\t\tint32_t targetHealth = target->getHealth();\n\t\tif (damage.primary.value >= targetHealth) {\n\t\t\tdamage.primary.value = targetHealth;\n\t\t\tdamage.secondary.value = 0;\n\t\t} else if (damage.secondary.value) {\n\t\t\tdamage.secondary.value = std::min<int32_t>(damage.secondary.value, targetHealth - damage.primary.value);\n\t\t}\n\n\t\trealDamage = damage.primary.value + damage.secondary.value;\n\t\tif (realDamage == 0) {\n\t\t\treturn true;\n\t\t} else if (realDamage >= targetHealth) {\n\t\t\tfor (CreatureEvent* creatureEvent : target->getCreatureEvents(CREATURE_EVENT_PREPAREDEATH)) {\n\t\t\t\tif (!creatureEvent->executeOnPrepareDeath(target, attacker)) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ttarget->drainHealth(attacker, realDamage);\n\t\tif (list.empty()) {\n\t\t\tmap.getSpectators(list, targetPos, true, true);\n\t\t}\n\t\taddCreatureHealth(list, target);\n\n\t\tmessage.primary.value = damage.primary.value;\n\t\tmessage.secondary.value = damage.secondary.value;\n\n\t\tuint8_t hitEffect;\n\t\tif (message.primary.value) {\n\t\t\tcombatGetTypeInfo(damage.primary.type, target, message.primary.color, hitEffect);\n\t\t\tif (hitEffect != CONST_ME_NONE) {\n\t\t\t\taddMagicEffect(list, targetPos, hitEffect);\n\t\t\t}\n\t\t}\n\n\t\tif (message.secondary.value) {\n\t\t\tcombatGetTypeInfo(damage.secondary.type, target, message.secondary.color, hitEffect);\n\t\t\tif (hitEffect != CONST_ME_NONE) {\n\t\t\t\taddMagicEffect(list, targetPos, hitEffect);\n\t\t\t}\n\t\t}\n\n\t\tif (message.primary.color != TEXTCOLOR_NONE || message.secondary.color != TEXTCOLOR_NONE) {\n\t\t\tstd::string damageString = std::to_string(realDamage) + (realDamage != 1 ? \" hitpoints\" : \" hitpoint\");\n\t\t\tstd::string spectatorMessage = ucfirst(target->getNameDescription()) + \" loses \" + damageString;\n\t\t\tif (attacker) {\n\t\t\t\tspectatorMessage += \" due to \";\n\t\t\t\tif (attacker == target) {\n\t\t\t\t\tspectatorMessage += (targetPlayer ? (targetPlayer->getSex() == PLAYERSEX_FEMALE ? \"her own attack\" : \"his own attack\") : \"its own attack\");\n\t\t\t\t} else {\n\t\t\t\t\tspectatorMessage += \"an attack by \" + attacker->getNameDescription();\n\t\t\t\t}\n\t\t\t}\n\t\t\tspectatorMessage += '.';\n\n\t\t\tfor (Creature* spectator : list) {\n\t\t\t\tPlayer* tmpPlayer = spectator->getPlayer();\n\t\t\t\tif (tmpPlayer->getPosition().z != targetPos.z) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tif (tmpPlayer == attackerPlayer && attackerPlayer != targetPlayer) {\n\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_DEALT;\n\t\t\t\t\tmessage.text = ucfirst(target->getNameDescription()) + \" loses \" + damageString + \" due to your attack.\";\n\t\t\t\t} else if (tmpPlayer == targetPlayer) {\n\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_RECEIVED;\n\t\t\t\t\tif (!attacker) {\n\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + '.';\n\t\t\t\t\t} else if (targetPlayer == attackerPlayer) {\n\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" due to your own attack.\";\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" due to an attack by \" + attacker->getNameDescription() + '.';\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tmessage.type = MESSAGE_DAMAGE_OTHERS;\n\t\t\t\t\t// TODO: Avoid copying spectatorMessage everytime we send to a spectator\n\t\t\t\t\tmessage.text = spectatorMessage;\n\t\t\t\t}\n\t\t\t\ttmpPlayer->sendTextMessage(message);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn true;\n}\n\nbool Game::combatChangeMana(Creature* attacker, Creature* target, int32_t manaChange, CombatOrigin origin)\n{\n\tif (manaChange > 0) {\n\t\tif (attacker) {\n\t\t\tconst Player* attackerPlayer = attacker->getPlayer();\n\t\t\tif (attackerPlayer && attackerPlayer->getSkull() == SKULL_BLACK && target->getPlayer() && attackerPlayer->getSkullClient(target) == SKULL_NONE) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\n\t\tif (origin != ORIGIN_NONE) {\n\t\t\tconst auto& events = target->getCreatureEvents(CREATURE_EVENT_MANACHANGE);\n\t\t\tif (!events.empty()) {\n\t\t\t\tfor (CreatureEvent* creatureEvent : events) {\n\t\t\t\t\tcreatureEvent->executeManaChange(target, attacker, manaChange, origin);\n\t\t\t\t}\n\t\t\t\treturn combatChangeMana(attacker, target, manaChange, ORIGIN_NONE);\n\t\t\t}\n\t\t}\n\n\t\ttarget->changeMana(manaChange);\n\t} else {\n\t\tconst Position& targetPos = target->getPosition();\n\t\tif (!target->isAttackable()) {\n\t\t\tif (!target->isInGhostMode()) {\n\t\t\t\taddMagicEffect(targetPos, CONST_ME_POFF);\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\tPlayer* attackerPlayer;\n\t\tif (attacker) {\n\t\t\tattackerPlayer = attacker->getPlayer();\n\t\t} else {\n\t\t\tattackerPlayer = nullptr;\n\t\t}\n\n\t\tPlayer* targetPlayer = target->getPlayer();\n\t\tif (attackerPlayer && targetPlayer && attackerPlayer->getSkull() == SKULL_BLACK && attackerPlayer->getSkullClient(targetPlayer) == SKULL_NONE) {\n\t\t\treturn false;\n\t\t}\n\n\t\tint32_t manaLoss = std::min<int32_t>(target->getMana(), -manaChange);\n\t\tBlockType_t blockType = target->blockHit(attacker, COMBAT_MANADRAIN, manaLoss);\n\t\tif (blockType != BLOCK_NONE) {\n\t\t\taddMagicEffect(targetPos, CONST_ME_POFF);\n\t\t\treturn false;\n\t\t}\n\n\t\tif (manaLoss <= 0) {\n\t\t\treturn true;\n\t\t}\n\n\t\tif (origin != ORIGIN_NONE) {\n\t\t\tconst auto& events = target->getCreatureEvents(CREATURE_EVENT_MANACHANGE);\n\t\t\tif (!events.empty()) {\n\t\t\t\tfor (CreatureEvent* creatureEvent : events) {\n\t\t\t\t\tcreatureEvent->executeManaChange(target, attacker, manaChange, origin);\n\t\t\t\t}\n\t\t\t\treturn combatChangeMana(attacker, target, manaChange, ORIGIN_NONE);\n\t\t\t}\n\t\t}\n\n\t\ttarget->drainMana(attacker, manaLoss);\n\n\t\tstd::string damageString = std::to_string(manaLoss);\n\t\tstd::string spectatorMessage = ucfirst(target->getNameDescription()) + \" loses \" + damageString + \" mana\";\n\t\tif (attacker) {\n\t\t\tspectatorMessage += \" due to \";\n\t\t\tif (attacker == target) {\n\t\t\t\tspectatorMessage += (targetPlayer ? (targetPlayer->getSex() == PLAYERSEX_FEMALE ? \"her own attack\" : \"his own attack\") : \"its own attack\");\n\t\t\t} else {\n\t\t\t\tspectatorMessage += \"an attack by \" + attacker->getNameDescription();\n\t\t\t}\n\t\t}\n\t\tspectatorMessage += '.';\n\n\t\tTextMessage message;\n\t\tmessage.position = targetPos;\n\t\tmessage.primary.value = manaLoss;\n\t\tmessage.primary.color = TEXTCOLOR_BLUE;\n\n\t\tSpectatorVec list;\n\t\tmap.getSpectators(list, targetPos, false, true);\n\t\tfor (Creature* spectator : list) {\n\t\t\tPlayer* tmpPlayer = spectator->getPlayer();\n\t\t\tif (tmpPlayer == attackerPlayer && attackerPlayer != targetPlayer) {\n\t\t\t\tmessage.type = MESSAGE_DAMAGE_DEALT;\n\t\t\t\tmessage.text = ucfirst(target->getNameDescription()) + \" loses \" + damageString + \" mana due to your attack.\";\n\t\t\t} else if (tmpPlayer == targetPlayer) {\n\t\t\t\tmessage.type = MESSAGE_DAMAGE_RECEIVED;\n\t\t\t\tif (!attacker) {\n\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana.\";\n\t\t\t\t} else if (targetPlayer == attackerPlayer) {\n\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana due to your own attack.\";\n\t\t\t\t} else {\n\t\t\t\t\tmessage.text = \"You lose \" + damageString + \" mana due to an attack by \" + attacker->getNameDescription() + '.';\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tmessage.type = MESSAGE_DAMAGE_OTHERS;\n\t\t\t\tmessage.text = spectatorMessage;\n\t\t\t}\n\t\t\ttmpPlayer->sendTextMessage(message);\n\t\t}\n\t}\n\n\treturn true;\n}\n\nvoid Game::addCreatureHealth(const Creature* target)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, target->getPosition(), true, true);\n\taddCreatureHealth(list, target);\n}\n\nvoid Game::addCreatureHealth(const SpectatorVec& list, const Creature* target)\n{\n\tfor (Creature* spectator : list) {\n\t\tif (Player* tmpPlayer = spectator->getPlayer()) {\n\t\t\ttmpPlayer->sendCreatureHealth(target);\n\t\t}\n\t}\n}\n\nvoid Game::addMagicEffect(const Position& pos, uint8_t effect)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, pos, true, true);\n\taddMagicEffect(list, pos, effect);\n}\n\nvoid Game::addMagicEffect(const SpectatorVec& list, const Position& pos, uint8_t effect)\n{\n\tfor (Creature* spectator : list) {\n\t\tif (Player* tmpPlayer = spectator->getPlayer()) {\n\t\t\ttmpPlayer->sendMagicEffect(pos, effect);\n\t\t}\n\t}\n}\n\nvoid Game::addDistanceEffect(const Position& fromPos, const Position& toPos, uint8_t effect)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, fromPos, false, true);\n\tmap.getSpectators(list, toPos, false, true);\n\taddDistanceEffect(list, fromPos, toPos, effect);\n}\n\nvoid Game::addDistanceEffect(const SpectatorVec& list, const Position& fromPos, const Position& toPos, uint8_t effect)\n{\n\tfor (Creature* spectator : list) {\n\t\tif (Player* tmpPlayer = spectator->getPlayer()) {\n\t\t\ttmpPlayer->sendDistanceShoot(fromPos, toPos, effect);\n\t\t}\n\t}\n}\n\nvoid Game::startDecay(Item* item)\n{\n\tif (!item || !item->canDecay()) {\n\t\treturn;\n\t}\n\n\tItemDecayState_t decayState = item->getDecaying();\n\tif (decayState == DECAYING_TRUE) {\n\t\treturn;\n\t}\n\n\tif (item->getDuration() > 0) {\n\t\titem->incrementReferenceCounter();\n\t\titem->setDecaying(DECAYING_TRUE);\n\t\ttoDecayItems.push_front(item);\n\t} else {\n\t\tinternalDecayItem(item);\n\t}\n}\n\nvoid Game::internalDecayItem(Item* item)\n{\n\tconst ItemType& it = Item::items[item->getID()];\n\tif (it.decayTo != 0) {\n\t\tItem* newItem = transformItem(item, it.decayTo);\n\t\tstartDecay(newItem);\n\t} else {\n\t\tReturnValue ret = internalRemoveItem(item);\n\t\tif (ret != RETURNVALUE_NOERROR) {\n\t\t\tstd::cout << \"[Debug - Game::internalDecayItem] internalDecayItem failed, error code: \" << static_cast<uint32_t>(ret) << \", item id: \" << item->getID() << std::endl;\n\t\t}\n\t}\n}\n\nvoid Game::checkDecay()\n{\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_DECAYINTERVAL, std::bind(&Game::checkDecay, this)));\n\n\tsize_t bucket = (lastBucket + 1) % EVENT_DECAY_BUCKETS;\n\n\tauto it = decayItems[bucket].begin(), end = decayItems[bucket].end();\n\twhile (it != end) {\n\t\tItem* item = *it;\n\t\tif (!item->canDecay()) {\n\t\t\titem->setDecaying(DECAYING_FALSE);\n\t\t\tReleaseItem(item);\n\t\t\tit = decayItems[bucket].erase(it);\n\t\t\tcontinue;\n\t\t}\n\n\t\tint32_t duration = item->getDuration();\n\t\tint32_t decreaseTime = std::min<int32_t>(EVENT_DECAYINTERVAL * EVENT_DECAY_BUCKETS, duration);\n\n\t\tduration -= decreaseTime;\n\t\titem->decreaseDuration(decreaseTime);\n\n\t\tif (duration <= 0) {\n\t\t\tit = decayItems[bucket].erase(it);\n\t\t\tinternalDecayItem(item);\n\t\t\tReleaseItem(item);\n\t\t} else if (duration < EVENT_DECAYINTERVAL * EVENT_DECAY_BUCKETS) {\n\t\t\tit = decayItems[bucket].erase(it);\n\t\t\tsize_t newBucket = (bucket + ((duration + EVENT_DECAYINTERVAL / 2) / 1000)) % EVENT_DECAY_BUCKETS;\n\t\t\tif (newBucket == bucket) {\n\t\t\t\tinternalDecayItem(item);\n\t\t\t\tReleaseItem(item);\n\t\t\t} else {\n\t\t\t\tdecayItems[newBucket].push_back(item);\n\t\t\t}\n\t\t} else {\n\t\t\t++it;\n\t\t}\n\t}\n\n\tlastBucket = bucket;\n\tcleanup();\n}\n\nvoid Game::checkLight()\n{\n\tg_scheduler.addEvent(createSchedulerTask(EVENT_LIGHTINTERVAL, std::bind(&Game::checkLight, this)));\n\n\tlightHour += lightHourDelta;\n\n\tif (lightHour > 1440) {\n\t\tlightHour -= 1440;\n\t}\n\n\tif (std::abs(lightHour - SUNRISE) < 2 * lightHourDelta) {\n\t\tlightState = LIGHT_STATE_SUNRISE;\n\t} else if (std::abs(lightHour - SUNSET) < 2 * lightHourDelta) {\n\t\tlightState = LIGHT_STATE_SUNSET;\n\t}\n\n\tint32_t newLightLevel = lightLevel;\n\tbool lightChange = false;\n\n\tswitch (lightState) {\n\t\tcase LIGHT_STATE_SUNRISE: {\n\t\t\tnewLightLevel += (LIGHT_LEVEL_DAY - LIGHT_LEVEL_NIGHT) / 30;\n\t\t\tlightChange = true;\n\t\t\tbreak;\n\t\t}\n\t\tcase LIGHT_STATE_SUNSET: {\n\t\t\tnewLightLevel -= (LIGHT_LEVEL_DAY - LIGHT_LEVEL_NIGHT) / 30;\n\t\t\tlightChange = true;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t}\n\n\tif (newLightLevel <= LIGHT_LEVEL_NIGHT) {\n\t\tlightLevel = LIGHT_LEVEL_NIGHT;\n\t\tlightState = LIGHT_STATE_NIGHT;\n\t} else if (newLightLevel >= LIGHT_LEVEL_DAY) {\n\t\tlightLevel = LIGHT_LEVEL_DAY;\n\t\tlightState = LIGHT_STATE_DAY;\n\t} else {\n\t\tlightLevel = newLightLevel;\n\t}\n\n\tif (lightChange) {\n\t\tLightInfo lightInfo;\n\t\tgetWorldLightInfo(lightInfo);\n\n\t\tfor (const auto& it : players) {\n\t\t\tit.second->sendWorldLight(lightInfo);\n\t\t}\n\t}\n}\n\nvoid Game::getWorldLightInfo(LightInfo& lightInfo) const\n{\n\tlightInfo.level = lightLevel;\n\tlightInfo.color = 0xD7;\n}\n\nvoid Game::addCommandTag(char tag)\n{\n\tfor (char commandTag : commandTags) {\n\t\tif (commandTag == tag) {\n\t\t\treturn;\n\t\t}\n\t}\n\tcommandTags.push_back(tag);\n}\n\nvoid Game::resetCommandTag()\n{\n\tcommandTags.clear();\n}\n\nvoid Game::shutdown()\n{\n\tstd::cout << \"Shutting down...\" << std::flush;\n\n\tg_scheduler.shutdown();\n\tg_databaseTasks.shutdown();\n\tg_dispatcher.shutdown();\n\tmap.spawns.clear();\n\traids.clear();\n\n\tcleanup();\n\n\tif (serviceManager) {\n\t\tserviceManager->stop();\n\t}\n\n\tConnectionManager::getInstance().closeAll();\n\n\tstd::cout << \" done!\" << std::endl;\n}\n\nvoid Game::cleanup()\n{\n\t//free memory\n\tfor (auto creature : ToReleaseCreatures) {\n\t\tcreature->decrementReferenceCounter();\n\t}\n\tToReleaseCreatures.clear();\n\n\tfor (auto item : ToReleaseItems) {\n\t\titem->decrementReferenceCounter();\n\t}\n\tToReleaseItems.clear();\n\n\tfor (Item* item : toDecayItems) {\n\t\tconst uint32_t dur = item->getDuration();\n\t\tif (dur >= EVENT_DECAYINTERVAL * EVENT_DECAY_BUCKETS) {\n\t\t\tdecayItems[lastBucket].push_back(item);\n\t\t} else {\n\t\t\tdecayItems[(lastBucket + 1 + dur / 1000) % EVENT_DECAY_BUCKETS].push_back(item);\n\t\t}\n\t}\n\ttoDecayItems.clear();\n}\n\nvoid Game::ReleaseCreature(Creature* creature)\n{\n\tToReleaseCreatures.push_back(creature);\n}\n\nvoid Game::ReleaseItem(Item* item)\n{\n\tToReleaseItems.push_back(item);\n}\n\nvoid Game::broadcastMessage(const std::string& text, MessageClasses type) const\n{\n\tstd::cout << \"> Broadcasted message: \\\"\" << text << \"\\\".\" << std::endl;\n\tfor (const auto& it : players) {\n\t\tit.second->sendTextMessage(type, text);\n\t}\n}\n\nvoid Game::updateCreatureWalkthrough(const Creature* creature)\n{\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tPlayer* tmpPlayer = spectator->getPlayer();\n\t\ttmpPlayer->sendCreatureWalkthrough(creature, tmpPlayer->canWalkthroughEx(creature));\n\t}\n}\n\nvoid Game::updateCreatureSkull(const Creature* creature)\n{\n\tif (getWorldType() != WORLD_TYPE_PVP) {\n\t\treturn;\n\t}\n\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureSkull(creature);\n\t}\n}\n\nvoid Game::updatePlayerShield(Player* player)\n{\n\tSpectatorVec list;\n\tmap.getSpectators(list, player->getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureShield(player);\n\t}\n}\n\nvoid Game::updatePlayerHelpers(const Player& player)\n{\n\tuint32_t creatureId = player.getID();\n\tuint16_t helpers = player.getHelpers();\n\n\tSpectatorVec list;\n\tmap.getSpectators(list, player.getPosition(), true, true);\n\tfor (Creature* spectator : list) {\n\t\tspectator->getPlayer()->sendCreatureHelpers(creatureId, helpers);\n\t}\n}\n\nvoid Game::updateCreatureType(Creature* creature)\n{\n\tconst Player* masterPlayer = nullptr;\n\n\tuint32_t creatureId = creature->getID();\n\tCreatureType_t creatureType = creature->getType();\n\tif (creatureType == CREATURETYPE_MONSTER) {\n\t\tconst Creature* master = creature->getMaster();\n\t\tif (master) {\n\t\t\tmasterPlayer = master->getPlayer();\n\t\t\tif (masterPlayer) {\n\t\t\t\tcreatureType = CREATURETYPE_SUMMON_OTHERS;\n\t\t\t}\n\t\t}\n\t}\n\n\t//send to clients\n\tSpectatorVec list;\n\tmap.getSpectators(list, creature->getPosition(), true, true);\n\n\tif (creatureType == CREATURETYPE_SUMMON_OTHERS) {\n\t\tfor (Creature* spectator : list) {\n\t\t\tPlayer* player = spectator->getPlayer();\n\t\t\tif (masterPlayer == player) {\n\t\t\t\tplayer->sendCreatureType(creatureId, CREATURETYPE_SUMMON_OWN);\n\t\t\t} else {\n\t\t\t\tplayer->sendCreatureType(creatureId, creatureType);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (Creature* spectator : list) {\n\t\t\tspectator->getPlayer()->sendCreatureType(creatureId, creatureType);\n\t\t}\n\t}\n}\n\nvoid Game::updatePremium(Account& account)\n{\n\tbool save = false;\n\ttime_t timeNow = time(nullptr);\n\n\tif (account.premiumDays != 0 && account.premiumDays != std::numeric_limits<uint16_t>::max()) {\n\t\tif (account.lastDay == 0) {\n\t\t\taccount.lastDay = timeNow;\n\t\t\tsave = true;\n\t\t} else {\n\t\t\tuint32_t days = (timeNow - account.lastDay) / 86400;\n\t\t\tif (days > 0) {\n\t\t\t\tif (days >= account.premiumDays) {\n\t\t\t\t\taccount.premiumDays = 0;\n\t\t\t\t\taccount.lastDay = 0;\n\t\t\t\t} else {\n\t\t\t\t\taccount.premiumDays -= days;\n\t\t\t\t\ttime_t remainder = (timeNow - account.lastDay) % 86400;\n\t\t\t\t\taccount.lastDay = timeNow - remainder;\n\t\t\t\t}\n\n\t\t\t\tsave = true;\n\t\t\t}\n\t\t}\n\t} else if (account.lastDay != 0) {\n\t\taccount.lastDay = 0;\n\t\tsave = true;\n\t}\n\n\tif (save && !IOLoginData::saveAccount(account)) {\n\t\tstd::cout << \"> ERROR: Failed to save account: \" << account.name << \"!\" << std::endl;\n\t}\n}\n\nvoid Game::loadMotdNum()\n{\n\tDatabase* db = Database::getInstance();\n\n\tDBResult_ptr result = db->storeQuery(\"SELECT `value` FROM `server_config` WHERE `config` = 'motd_num'\");\n\tif (result) {\n\t\tmotdNum = result->getNumber<uint32_t>(\"value\");\n\t} else {\n\t\tdb->executeQuery(\"INSERT INTO `server_config` (`config`, `value`) VALUES ('motd_num', '0')\");\n\t}\n\n\tresult = db->storeQuery(\"SELECT `value` FROM `server_config` WHERE `config` = 'motd_hash'\");\n\tif (result) {\n\t\tmotdHash = result->getString(\"value\");\n\t\tif (motdHash != transformToSHA1(g_config.getString(ConfigManager::MOTD))) {\n\t\t\t++motdNum;\n\t\t}\n\t} else {\n\t\tdb->executeQuery(\"INSERT INTO `server_config` (`config`, `value`) VALUES ('motd_hash', '')\");\n\t}\n}\n\nvoid Game::saveMotdNum() const\n{\n\tDatabase* db = Database::getInstance();\n\n\tstd::ostringstream query;\n\tquery << \"UPDATE `server_config` SET `value` = '\" << motdNum << \"' WHERE `config` = 'motd_num'\";\n\tdb->executeQuery(query.str());\n\n\tquery.str(std::string());\n\tquery << \"UPDATE `server_config` SET `value` = '\" << transformToSHA1(g_config.getString(ConfigManager::MOTD)) << \"' WHERE `config` = 'motd_hash'\";\n\tdb->executeQuery(query.str());\n}\n\nvoid Game::checkPlayersRecord()\n{\n\tconst size_t playersOnline = getPlayersOnline();\n\tif (playersOnline > playersRecord) {\n\t\tuint32_t previousRecord = playersRecord;\n\t\tplayersRecord = playersOnline;\n\n\t\tfor (const auto& it : g_globalEvents->getEventMap(GLOBALEVENT_RECORD)) {\n\t\t\tit.second->executeRecord(playersRecord, previousRecord);\n\t\t}\n\t\tupdatePlayersRecord();\n\t}\n}\n\nvoid Game::updatePlayersRecord() const\n{\n\tDatabase* db = Database::getInstance();\n\n\tstd::ostringstream query;\n\tquery << \"UPDATE `server_config` SET `value` = '\" << playersRecord << \"' WHERE `config` = 'players_record'\";\n\tdb->executeQuery(query.str());\n}\n\nvoid Game::loadPlayersRecord()\n{\n\tDatabase* db = Database::getInstance();\n\n\tDBResult_ptr result = db->storeQuery(\"SELECT `value` FROM `server_config` WHERE `config` = 'players_record'\");\n\tif (result) {\n\t\tplayersRecord = result->getNumber<uint32_t>(\"value\");\n\t} else {\n\t\tdb->executeQuery(\"INSERT INTO `server_config` (`config`, `value`) VALUES ('players_record', '0')\");\n\t}\n}\n\nuint64_t Game::getExperienceStage(uint32_t level)\n{\n\tif (!stagesEnabled) {\n\t\treturn g_config.getNumber(ConfigManager::RATE_EXPERIENCE);\n\t}\n\n\tif (useLastStageLevel && level >= lastStageLevel) {\n\t\treturn stages[lastStageLevel];\n\t}\n\n\treturn stages[level];\n}\n\nbool Game::loadExperienceStages()\n{\n\tpugi::xml_document doc;\n\tpugi::xml_parse_result result = doc.load_file(\"data/XML/stages.xml\");\n\tif (!result) {\n\t\tprintXMLError(\"Error - Game::loadExperienceStages\", \"data/XML/stages.xml\", result);\n\t\treturn false;\n\t}\n\n\tfor (auto stageNode : doc.child(\"stages\").children()) {\n\t\tif (strcasecmp(stageNode.name(), \"config\") == 0) {\n\t\t\tstagesEnabled = stageNode.attribute(\"enabled\").as_bool();\n\t\t} else {\n\t\t\tuint32_t minLevel, maxLevel, multiplier;\n\n\t\t\tpugi::xml_attribute minLevelAttribute = stageNode.attribute(\"minlevel\");\n\t\t\tif (minLevelAttribute) {\n\t\t\t\tminLevel = pugi::cast<uint32_t>(minLevelAttribute.value());\n\t\t\t} else {\n\t\t\t\tminLevel = 1;\n\t\t\t}\n\n\t\t\tpugi::xml_attribute maxLevelAttribute = stageNode.attribute(\"maxlevel\");\n\t\t\tif (maxLevelAttribute) {\n\t\t\t\tmaxLevel = pugi::cast<uint32_t>(maxLevelAttribute.value());\n\t\t\t} else {\n\t\t\t\tmaxLevel = 0;\n\t\t\t\tlastStageLevel = minLevel;\n\t\t\t\tuseLastStageLevel = true;\n\t\t\t}\n\n\t\t\tpugi::xml_attribute multiplierAttribute = stageNode.attribute(\"multiplier\");\n\t\t\tif (multiplierAttribute) {\n\t\t\t\tmultiplier = pugi::cast<uint32_t>(multiplierAttribute.value());\n\t\t\t} else {\n\t\t\t\tmultiplier = 1;\n\t\t\t}\n\n\t\t\tif (useLastStageLevel) {\n\t\t\t\tstages[lastStageLevel] = multiplier;\n\t\t\t} else {\n\t\t\t\tfor (uint32_t i = minLevel; i <= maxLevel; ++i) {\n\t\t\t\t\tstages[i] = multiplier;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn true;\n}\n\nvoid Game::playerInviteToParty(uint32_t playerId, uint32_t invitedId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPlayer* invitedPlayer = getPlayerByID(invitedId);\n\tif (!invitedPlayer || invitedPlayer->isInviting(player)) {\n\t\treturn;\n\t}\n\n\tif (invitedPlayer->getParty()) {\n\t\tstd::ostringstream ss;\n\t\tss << invitedPlayer->getName() << \" is already in a party.\";\n\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, ss.str());\n\t\treturn;\n\t}\n\n\tParty* party = player->getParty();\n\tif (!party) {\n\t\tparty = new Party(player);\n\t} else if (party->getLeader() != player) {\n\t\treturn;\n\t}\n\n\tparty->invitePlayer(*invitedPlayer);\n}\n\nvoid Game::playerJoinParty(uint32_t playerId, uint32_t leaderId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tPlayer* leader = getPlayerByID(leaderId);\n\tif (!leader || !leader->isInviting(player)) {\n\t\treturn;\n\t}\n\n\tParty* party = leader->getParty();\n\tif (!party || party->getLeader() != leader) {\n\t\treturn;\n\t}\n\n\tif (player->getParty()) {\n\t\tplayer->sendTextMessage(MESSAGE_INFO_DESCR, \"You are already in a party.\");\n\t\treturn;\n\t}\n\n\tparty->joinParty(*player);\n}\n\nvoid Game::playerRevokePartyInvitation(uint32_t playerId, uint32_t invitedId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tParty* party = player->getParty();\n\tif (!party || party->getLeader() != player) {\n\t\treturn;\n\t}\n\n\tPlayer* invitedPlayer = getPlayerByID(invitedId);\n\tif (!invitedPlayer || !player->isInviting(invitedPlayer)) {\n\t\treturn;\n\t}\n\n\tparty->revokeInvitation(*invitedPlayer);\n}\n\nvoid Game::playerPassPartyLeadership(uint32_t playerId, uint32_t newLeaderId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tParty* party = player->getParty();\n\tif (!party || party->getLeader() != player) {\n\t\treturn;\n\t}\n\n\tPlayer* newLeader = getPlayerByID(newLeaderId);\n\tif (!newLeader || !player->isPartner(newLeader)) {\n\t\treturn;\n\t}\n\n\tparty->passPartyLeadership(newLeader);\n}\n\nvoid Game::playerLeaveParty(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tParty* party = player->getParty();\n\tif (!party || player->hasCondition(CONDITION_INFIGHT)) {\n\t\treturn;\n\t}\n\n\tparty->leaveParty(player);\n}\n\nvoid Game::playerEnableSharedPartyExperience(uint32_t playerId, bool sharedExpActive)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tParty* party = player->getParty();\n\tif (!party || player->hasCondition(CONDITION_INFIGHT)) {\n\t\treturn;\n\t}\n\n\tparty->setSharedExperience(player, sharedExpActive);\n}\n\nvoid Game::sendGuildMotd(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tGuild* guild = player->getGuild();\n\tif (guild) {\n\t\tplayer->sendChannelMessage(\"Message of the Day\", guild->getMotd(), TALKTYPE_CHANNEL_R1, CHANNEL_GUILD);\n\t}\n}\n\nvoid Game::kickPlayer(uint32_t playerId, bool displayEffect)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->kickPlayer(displayEffect);\n}\n\nvoid Game::playerReportBug(uint32_t playerId, const std::string& message, const Position& position, uint8_t category)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (player->getAccountType() == ACCOUNT_TYPE_NORMAL) {\n\t\treturn;\n\t}\n\n\tstd::string fileName = \"data/reports/\" + player->getName() + \" report.txt\";\n\tFILE* file = fopen(fileName.c_str(), \"a\");\n\tif (!file) {\n\t\tplayer->sendTextMessage(MESSAGE_EVENT_DEFAULT, \"There was an error when processing your report, please contact a gamemaster.\");\n\t\treturn;\n\t}\n\n\tconst Position& playerPosition = player->getPosition();\n\tif (category == BUG_CATEGORY_MAP) {\n\t\tfprintf(file, \"------------------------------\\nName: %s [Map Position: %u, %u, %u] [Player Position: %u, %u, %u]\\nComment: %s\\n\", player->getName().c_str(), position.x, position.y, position.z, playerPosition.x, playerPosition.y, playerPosition.z, message.c_str());\n\t} else {\n\t\tfprintf(file, \"------------------------------\\nName: %s [Player Position: %u, %u, %u]\\nComment: %s\\n\", player->getName().c_str(), playerPosition.x, playerPosition.y, playerPosition.z, message.c_str());\n\t}\n\tfclose(file);\n\n\tplayer->sendTextMessage(MESSAGE_EVENT_DEFAULT, \"Your report has been sent to \" + g_config.getString(ConfigManager::SERVER_NAME) + \".\");\n}\n\nvoid Game::playerDebugAssert(uint32_t playerId, const std::string& assertLine, const std::string& date, const std::string& description, const std::string& comment)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\t// TODO: move debug assertions to database\n\tFILE* file = fopen(\"client_assertions.txt\", \"a\");\n\tif (file) {\n\t\tfprintf(file, \"----- %s - %s (%s) -----\\n\", formatDate(time(nullptr)).c_str(), player->getName().c_str(), convertIPToString(player->getIP()).c_str());\n\t\tfprintf(file, \"%s\\n%s\\n%s\\n%s\\n\", assertLine.c_str(), date.c_str(), description.c_str(), comment.c_str());\n\t\tfclose(file);\n\t}\n}\n\nvoid Game::playerLeaveMarket(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tplayer->setInMarket(false);\n}\n\nvoid Game::playerBrowseMarket(uint32_t playerId, uint16_t spriteId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items.getItemIdByClientId(spriteId);\n\tif (it.id == 0) {\n\t\treturn;\n\t}\n\n\tif (it.wareId == 0) {\n\t\treturn;\n\t}\n\n\tconst MarketOfferList& buyOffers = IOMarket::getActiveOffers(MARKETACTION_BUY, it.id);\n\tconst MarketOfferList& sellOffers = IOMarket::getActiveOffers(MARKETACTION_SELL, it.id);\n\tplayer->sendMarketBrowseItem(it.id, buyOffers, sellOffers);\n\tplayer->sendMarketDetail(it.id);\n}\n\nvoid Game::playerBrowseMarketOwnOffers(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tconst MarketOfferList& buyOffers = IOMarket::getOwnOffers(MARKETACTION_BUY, player->getGUID());\n\tconst MarketOfferList& sellOffers = IOMarket::getOwnOffers(MARKETACTION_SELL, player->getGUID());\n\tplayer->sendMarketBrowseOwnOffers(buyOffers, sellOffers);\n}\n\nvoid Game::playerBrowseMarketOwnHistory(uint32_t playerId)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tconst HistoryMarketOfferList& buyOffers = IOMarket::getOwnHistory(MARKETACTION_BUY, player->getGUID());\n\tconst HistoryMarketOfferList& sellOffers = IOMarket::getOwnHistory(MARKETACTION_SELL, player->getGUID());\n\tplayer->sendMarketBrowseOwnHistory(buyOffers, sellOffers);\n}\n\nvoid Game::playerCreateMarketOffer(uint32_t playerId, uint8_t type, uint16_t spriteId, uint16_t amount, uint32_t price, bool anonymous)\n{\n\tif (amount == 0 || amount > 64000) {\n\t\treturn;\n\t}\n\n\tif (price == 0 || price > 999999999) {\n\t\treturn;\n\t}\n\n\tif (type != MARKETACTION_BUY && type != MARKETACTION_SELL) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tif (g_config.getBoolean(ConfigManager::MARKET_PREMIUM) && !player->isPremium()) {\n\t\tplayer->sendMarketLeave();\n\t\treturn;\n\t}\n\n\tconst ItemType& itt = Item::items.getItemIdByClientId(spriteId);\n\tif (itt.id == 0 || itt.wareId == 0) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items.getItemIdByClientId(itt.wareId);\n\tif (it.id == 0 || it.wareId == 0) {\n\t\treturn;\n\t}\n\n\tif (!it.stackable && amount > 2000) {\n\t\treturn;\n\t}\n\n\tconst uint32_t maxOfferCount = g_config.getNumber(ConfigManager::MAX_MARKET_OFFERS_AT_A_TIME_PER_PLAYER);\n\tif (maxOfferCount != 0 && IOMarket::getPlayerOfferCount(player->getGUID()) >= maxOfferCount) {\n\t\treturn;\n\t}\n\n\tuint64_t fee = (price / 100.) * amount;\n\tif (fee < 20) {\n\t\tfee = 20;\n\t} else if (fee > 1000) {\n\t\tfee = 1000;\n\t}\n\n\tif (type == MARKETACTION_SELL) {\n\t\tif (fee > player->bankBalance) {\n\t\t\treturn;\n\t\t}\n\n\t\tDepotChest* depotChest = player->getDepotChest(player->getLastDepotId(), false);\n\t\tif (!depotChest) {\n\t\t\treturn;\n\t\t}\n\n\t\tstd::forward_list<Item*> itemList = getMarketItemList(it.wareId, amount, depotChest, player->getInbox());\n\t\tif (itemList.empty()) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (it.stackable) {\n\t\t\tuint16_t tmpAmount = amount;\n\t\t\tfor (Item* item : itemList) {\n\t\t\t\tuint16_t removeCount = std::min<uint16_t>(tmpAmount, item->getItemCount());\n\t\t\t\ttmpAmount -= removeCount;\n\t\t\t\tinternalRemoveItem(item, removeCount);\n\n\t\t\t\tif (tmpAmount == 0) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor (Item* item : itemList) {\n\t\t\t\tinternalRemoveItem(item);\n\t\t\t}\n\t\t}\n\n\t\tplayer->bankBalance -= fee;\n\t} else {\n\t\tuint64_t totalPrice = static_cast<uint64_t>(price) * amount;\n\t\ttotalPrice += fee;\n\t\tif (totalPrice > player->bankBalance) {\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->bankBalance -= totalPrice;\n\t}\n\n\tIOMarket::createOffer(player->getGUID(), static_cast<MarketAction_t>(type), it.id, amount, price, anonymous);\n\n\tplayer->sendMarketEnter(player->getLastDepotId());\n\tconst MarketOfferList& buyOffers = IOMarket::getActiveOffers(MARKETACTION_BUY, it.id);\n\tconst MarketOfferList& sellOffers = IOMarket::getActiveOffers(MARKETACTION_SELL, it.id);\n\tplayer->sendMarketBrowseItem(it.id, buyOffers, sellOffers);\n}\n\nvoid Game::playerCancelMarketOffer(uint32_t playerId, uint32_t timestamp, uint16_t counter)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tMarketOfferEx offer = IOMarket::getOfferByCounter(timestamp, counter);\n\tif (offer.id == 0 || offer.playerId != player->getGUID()) {\n\t\treturn;\n\t}\n\n\tif (offer.type == MARKETACTION_BUY) {\n\t\tplayer->bankBalance += static_cast<uint64_t>(offer.price) * offer.amount;\n\t\tplayer->sendMarketEnter(player->getLastDepotId());\n\t} else {\n\t\tconst ItemType& it = Item::items[offer.itemId];\n\t\tif (it.id == 0) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (it.stackable) {\n\t\t\tuint16_t tmpAmount = offer.amount;\n\t\t\twhile (tmpAmount > 0) {\n\t\t\t\tint32_t stackCount = std::min<int32_t>(100, tmpAmount);\n\t\t\t\tItem* item = Item::CreateItem(it.id, stackCount);\n\t\t\t\tif (internalAddItem(player->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\ttmpAmount -= stackCount;\n\t\t\t}\n\t\t} else {\n\t\t\tint32_t subType;\n\t\t\tif (it.charges != 0) {\n\t\t\t\tsubType = it.charges;\n\t\t\t} else {\n\t\t\t\tsubType = -1;\n\t\t\t}\n\n\t\t\tfor (uint16_t i = 0; i < offer.amount; ++i) {\n\t\t\t\tItem* item = Item::CreateItem(it.id, subType);\n\t\t\t\tif (internalAddItem(player->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tIOMarket::moveOfferToHistory(offer.id, OFFERSTATE_CANCELLED);\n\toffer.amount = 0;\n\toffer.timestamp += g_config.getNumber(ConfigManager::MARKET_OFFER_DURATION);\n\tplayer->sendMarketCancelOffer(offer);\n}\n\nvoid Game::playerAcceptMarketOffer(uint32_t playerId, uint32_t timestamp, uint16_t counter, uint16_t amount)\n{\n\tif (amount == 0 || amount > 64000) {\n\t\treturn;\n\t}\n\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->isInMarket()) {\n\t\treturn;\n\t}\n\n\tMarketOfferEx offer = IOMarket::getOfferByCounter(timestamp, counter);\n\tif (offer.id == 0) {\n\t\treturn;\n\t}\n\n\tif (amount > offer.amount) {\n\t\treturn;\n\t}\n\n\tconst ItemType& it = Item::items[offer.itemId];\n\tif (it.id == 0) {\n\t\treturn;\n\t}\n\n\tuint64_t totalPrice = static_cast<uint64_t>(offer.price) * amount;\n\n\tif (offer.type == MARKETACTION_BUY) {\n\t\tDepotChest* depotChest = player->getDepotChest(player->getLastDepotId(), false);\n\t\tif (!depotChest) {\n\t\t\treturn;\n\t\t}\n\n\t\tstd::forward_list<Item*> itemList = getMarketItemList(it.wareId, amount, depotChest, player->getInbox());\n\t\tif (itemList.empty()) {\n\t\t\treturn;\n\t\t}\n\n\t\tPlayer* buyerPlayer = getPlayerByGUID(offer.playerId);\n\t\tif (!buyerPlayer) {\n\t\t\tbuyerPlayer = new Player(nullptr);\n\t\t\tif (!IOLoginData::loadPlayerById(buyerPlayer, offer.playerId)) {\n\t\t\t\tdelete buyerPlayer;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tif (it.stackable) {\n\t\t\tuint16_t tmpAmount = amount;\n\t\t\tfor (Item* item : itemList) {\n\t\t\t\tuint16_t removeCount = std::min<uint16_t>(tmpAmount, item->getItemCount());\n\t\t\t\ttmpAmount -= removeCount;\n\t\t\t\tinternalRemoveItem(item, removeCount);\n\n\t\t\t\tif (tmpAmount == 0) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor (Item* item : itemList) {\n\t\t\t\tinternalRemoveItem(item);\n\t\t\t}\n\t\t}\n\n\t\tplayer->bankBalance += totalPrice;\n\n\t\tif (it.stackable) {\n\t\t\tuint16_t tmpAmount = amount;\n\t\t\twhile (tmpAmount > 0) {\n\t\t\t\tuint16_t stackCount = std::min<uint16_t>(100, tmpAmount);\n\t\t\t\tItem* item = Item::CreateItem(it.id, stackCount);\n\t\t\t\tif (internalAddItem(buyerPlayer->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\ttmpAmount -= stackCount;\n\t\t\t}\n\t\t} else {\n\t\t\tint32_t subType;\n\t\t\tif (it.charges != 0) {\n\t\t\t\tsubType = it.charges;\n\t\t\t} else {\n\t\t\t\tsubType = -1;\n\t\t\t}\n\n\t\t\tfor (uint16_t i = 0; i < amount; ++i) {\n\t\t\t\tItem* item = Item::CreateItem(it.id, subType);\n\t\t\t\tif (internalAddItem(buyerPlayer->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (buyerPlayer->isOffline()) {\n\t\t\tIOLoginData::savePlayer(buyerPlayer);\n\t\t\tdelete buyerPlayer;\n\t\t} else {\n\t\t\tbuyerPlayer->onReceiveMail();\n\t\t}\n\t} else {\n\t\tif (totalPrice > player->bankBalance) {\n\t\t\treturn;\n\t\t}\n\n\t\tplayer->bankBalance -= totalPrice;\n\n\t\tif (it.stackable) {\n\t\t\tuint16_t tmpAmount = amount;\n\t\t\twhile (tmpAmount > 0) {\n\t\t\t\tuint16_t stackCount = std::min<uint16_t>(100, tmpAmount);\n\t\t\t\tItem* item = Item::CreateItem(it.id, stackCount);\n\t\t\t\tif (internalAddItem(player->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\ttmpAmount -= stackCount;\n\t\t\t}\n\t\t} else {\n\t\t\tint32_t subType;\n\t\t\tif (it.charges != 0) {\n\t\t\t\tsubType = it.charges;\n\t\t\t} else {\n\t\t\t\tsubType = -1;\n\t\t\t}\n\n\t\t\tfor (uint16_t i = 0; i < amount; ++i) {\n\t\t\t\tItem* item = Item::CreateItem(it.id, subType);\n\t\t\t\tif (internalAddItem(player->getInbox(), item, INDEX_WHEREEVER, FLAG_NOLIMIT) != RETURNVALUE_NOERROR) {\n\t\t\t\t\tdelete item;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tPlayer* sellerPlayer = getPlayerByGUID(offer.playerId);\n\t\tif (sellerPlayer) {\n\t\t\tsellerPlayer->bankBalance += totalPrice;\n\t\t} else {\n\t\t\tIOLoginData::increaseBankBalance(offer.playerId, totalPrice);\n\t\t}\n\n\t\tplayer->onReceiveMail();\n\t}\n\n\tconst int32_t marketOfferDuration = g_config.getNumber(ConfigManager::MARKET_OFFER_DURATION);\n\n\tIOMarket::appendHistory(player->getGUID(), (offer.type == MARKETACTION_BUY ? MARKETACTION_SELL : MARKETACTION_BUY), offer.itemId, amount, offer.price, offer.timestamp + marketOfferDuration, OFFERSTATE_ACCEPTEDEX);\n\n\tIOMarket::appendHistory(offer.playerId, offer.type, offer.itemId, amount, offer.price, offer.timestamp + marketOfferDuration, OFFERSTATE_ACCEPTED);\n\n\toffer.amount -= amount;\n\n\tif (offer.amount == 0) {\n\t\tIOMarket::deleteOffer(offer.id);\n\t} else {\n\t\tIOMarket::acceptOffer(offer.id, amount);\n\t}\n\n\tplayer->sendMarketEnter(player->getLastDepotId());\n\toffer.timestamp += marketOfferDuration;\n\tplayer->sendMarketAcceptOffer(offer);\n}\n\nvoid Game::parsePlayerExtendedOpcode(uint32_t playerId, uint8_t opcode, const std::string& buffer)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tfor (CreatureEvent* creatureEvent : player->getCreatureEvents(CREATURE_EVENT_EXTENDED_OPCODE)) {\n\t\tcreatureEvent->executeExtendedOpcode(player, opcode, buffer);\n\t}\n}\n\nstd::forward_list<Item*> Game::getMarketItemList(uint16_t wareId, uint16_t sufficientCount, DepotChest* depotChest, Inbox* inbox)\n{\n\tstd::forward_list<Item*> itemList;\n\tuint16_t count = 0;\n\n\tstd::list<Container*> containers { depotChest, inbox };\n\tdo {\n\t\tContainer* container = containers.front();\n\t\tcontainers.pop_front();\n\n\t\tfor (Item* item : container->getItemList()) {\n\t\t\tContainer* c = item->getContainer();\n\t\t\tif (c && !c->empty()) {\n\t\t\t\tcontainers.push_back(c);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tconst ItemType& itemType = Item::items[item->getID()];\n\t\t\tif (itemType.wareId != wareId) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (c && (!itemType.isContainer() || c->capacity() != itemType.maxItems)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!item->hasMarketAttributes()) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\titemList.push_front(item);\n\n\t\t\tcount += Item::countByType(item, -1);\n\t\t\tif (count >= sufficientCount) {\n\t\t\t\treturn itemList;\n\t\t\t}\n\t\t}\n\t} while (!containers.empty());\n\treturn std::forward_list<Item*>();\n}\n\nvoid Game::forceAddCondition(uint32_t creatureId, Condition* condition)\n{\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (!creature) {\n\t\tdelete condition;\n\t\treturn;\n\t}\n\n\tcreature->addCondition(condition, true);\n}\n\nvoid Game::forceRemoveCondition(uint32_t creatureId, ConditionType_t type)\n{\n\tCreature* creature = getCreatureByID(creatureId);\n\tif (!creature) {\n\t\treturn;\n\t}\n\n\tcreature->removeCondition(type, true);\n}\n\nvoid Game::sendOfflineTrainingDialog(Player* player)\n{\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->hasModalWindowOpen(offlineTrainingWindow.id)) {\n\t\tplayer->sendModalWindow(offlineTrainingWindow);\n\t}\n}\n\nvoid Game::playerAnswerModalWindow(uint32_t playerId, uint32_t modalWindowId, uint8_t button, uint8_t choice)\n{\n\tPlayer* player = getPlayerByID(playerId);\n\tif (!player) {\n\t\treturn;\n\t}\n\n\tif (!player->hasModalWindowOpen(modalWindowId)) {\n\t\treturn;\n\t}\n\n\tplayer->onModalWindowHandled(modalWindowId);\n\n\t// offline training, hardcoded\n\tif (modalWindowId == std::numeric_limits<uint32_t>::max()) {\n\t\tif (button == 1) {\n\t\t\tif (choice == SKILL_SWORD || choice == SKILL_AXE || choice == SKILL_CLUB || choice == SKILL_DISTANCE || choice == SKILL_MAGLEVEL) {\n\t\t\t\tBedItem* bedItem = player->getBedItem();\n\t\t\t\tif (bedItem && bedItem->sleep(player)) {\n\t\t\t\t\tplayer->setOfflineTrainingSkill(choice);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tplayer->sendTextMessage(MESSAGE_EVENT_ADVANCE, \"Offline training aborted.\");\n\t\t}\n\n\t\tplayer->setBedItem(nullptr);\n\t} else {\n\t\tfor (auto creatureEvent : player->getCreatureEvents(CREATURE_EVENT_MODALWINDOW)) {\n\t\t\tcreatureEvent->executeModalWindow(player, modalWindowId, button, choice);\n\t\t}\n\t}\n}\n\nvoid Game::addPlayer(Player* player)\n{\n\tconst std::string& lowercase_name = asLowerCaseString(player->getName());\n\tmappedPlayerNames[lowercase_name] = player;\n\twildcardTree.insert(lowercase_name);\n\tplayers[player->getID()] = player;\n}\n\nvoid Game::removePlayer(Player* player)\n{\n\tconst std::string& lowercase_name = asLowerCaseString(player->getName());\n\tmappedPlayerNames.erase(lowercase_name);\n\twildcardTree.remove(lowercase_name);\n\tplayers.erase(player->getID());\n}\n\nvoid Game::addNpc(Npc* npc)\n{\n\tnpcs[npc->getID()] = npc;\n}\n\nvoid Game::removeNpc(Npc* npc)\n{\n\tnpcs.erase(npc->getID());\n}\n\nvoid Game::addMonster(Monster* monster)\n{\n\tmonsters[monster->getID()] = monster;\n}\n\nvoid Game::removeMonster(Monster* monster)\n{\n\tmonsters.erase(monster->getID());\n}\n\nGuild* Game::getGuild(uint32_t id) const\n{\n\tauto it = guilds.find(id);\n\tif (it == guilds.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nvoid Game::addGuild(Guild* guild)\n{\n\tguilds[guild->getId()] = guild;\n}\n\nvoid Game::removeGuild(uint32_t guildId)\n{\n\tguilds.erase(guildId);\n}\n\nvoid Game::decreaseBrowseFieldRef(const Position& pos)\n{\n\tTile* tile = map.getTile(pos.x, pos.y, pos.z);\n\tif (!tile) {\n\t\treturn;\n\t}\n\n\tauto it = browseFields.find(tile);\n\tif (it != browseFields.end()) {\n\t\tit->second->decrementReferenceCounter();\n\t}\n}\n\nvoid Game::internalRemoveItems(std::vector<Item*> itemList, uint32_t amount, bool stackable)\n{\n\tif (stackable) {\n\t\tfor (Item* item : itemList) {\n\t\t\tif (item->getItemCount() > amount) {\n\t\t\t\tinternalRemoveItem(item, amount);\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tamount -= item->getItemCount();\n\t\t\t\tinternalRemoveItem(item);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (Item* item : itemList) {\n\t\t\tinternalRemoveItem(item);\n\t\t}\n\t}\n}\n\nBedItem* Game::getBedBySleeper(uint32_t guid) const\n{\n\tauto it = bedSleepersMap.find(guid);\n\tif (it == bedSleepersMap.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nvoid Game::setBedSleeper(BedItem* bed, uint32_t guid)\n{\n\tbedSleepersMap[guid] = bed;\n}\n\nvoid Game::removeBedSleeper(uint32_t guid)\n{\n\tauto it = bedSleepersMap.find(guid);\n\tif (it != bedSleepersMap.end()) {\n\t\tbedSleepersMap.erase(it);\n\t}\n}\n\nItem* Game::getUniqueItem(uint16_t uniqueId)\n{\n\tauto it = uniqueItems.find(uniqueId);\n\tif (it == uniqueItems.end()) {\n\t\treturn nullptr;\n\t}\n\treturn it->second;\n}\n\nbool Game::addUniqueItem(uint16_t uniqueId, Item* item)\n{\n\tauto result = uniqueItems.emplace(uniqueId, item);\n\tif (!result.second) {\n\t\tstd::cout << \"Duplicate unique id: \" << uniqueId << std::endl;\n\t}\n\treturn result.second;\n}\n\nvoid Game::removeUniqueItem(uint16_t uniqueId)\n{\n\tauto it = uniqueItems.find(uniqueId);\n\tif (it != uniqueItems.end()) {\n\t\tuniqueItems.erase(it);\n\t}\n}\n", "idx": 1, "id": 13560, "msg": "logical part should be in lua, move it to there.", "proj": "otland-forgottenserver", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -179,6 +179,8 @@ class LanguageServer extends AdvancedJsonRpc\\Dispatcher\n         );\n \n         $this->client = new LanguageClient($reader, $writer);\n+\n+        $this->verboseLog(\"Language server has started.\");\n     }\n \n     /**", "y": 0, "oldf": "<?php\ndeclare(strict_types = 1);\nnamespace Psalm\\Internal\\LanguageServer;\n\nuse AdvancedJsonRpc;\nuse function Amp\\asyncCoroutine;\nuse function Amp\\call;\nuse Amp\\Promise;\nuse Amp\\Success;\nuse function array_combine;\nuse function array_filter;\nuse function array_keys;\nuse function array_map;\nuse function array_shift;\nuse function array_unshift;\nuse function array_values;\nuse function explode;\nuse function implode;\nuse LanguageServerProtocol\\ClientCapabilities;\nuse LanguageServerProtocol\\CompletionOptions;\nuse LanguageServerProtocol\\Diagnostic;\nuse LanguageServerProtocol\\DiagnosticSeverity;\nuse LanguageServerProtocol\\InitializeResult;\nuse LanguageServerProtocol\\Position;\nuse LanguageServerProtocol\\Range;\nuse LanguageServerProtocol\\ServerCapabilities;\nuse LanguageServerProtocol\\SignatureHelpOptions;\nuse LanguageServerProtocol\\TextDocumentSyncKind;\nuse LanguageServerProtocol\\TextDocumentSyncOptions;\nuse function max;\nuse function parse_url;\nuse Psalm\\Internal\\Analyzer\\IssueData;\nuse Psalm\\Internal\\Analyzer\\ProjectAnalyzer;\nuse Psalm\\Internal\\LanguageServer\\Server\\TextDocument;\nuse function rawurlencode;\nuse function str_replace;\nuse function strpos;\nuse function substr;\nuse Throwable;\nuse function trim;\nuse function urldecode;\n\n/**\n * @internal\n */\nclass LanguageServer extends AdvancedJsonRpc\\Dispatcher\n{\n    /**\n     * Handles textDocument/* method calls\n     *\n     * @var ?Server\\TextDocument\n     */\n    public $textDocument;\n\n    /**\n     * @var ProtocolReader\n     */\n    protected $protocolReader;\n\n    /**\n     * @var ProtocolWriter\n     */\n    protected $protocolWriter;\n\n    /**\n     * @var LanguageClient\n     */\n    public $client;\n\n    /**\n     * @var ProjectAnalyzer\n     */\n    protected $project_analyzer;\n\n    /**\n     * @var array<string, string>\n     */\n    protected $onsave_paths_to_analyze = [];\n\n    /**\n     * @var array<string, string>\n     */\n    protected $onchange_paths_to_analyze = [];\n\n    /**\n     * @param ProtocolReader  $reader\n     * @param ProtocolWriter $writer\n     */\n    public function __construct(\n        ProtocolReader $reader,\n        ProtocolWriter $writer,\n        ProjectAnalyzer $project_analyzer\n    ) {\n        parent::__construct($this, '/');\n        $this->project_analyzer = $project_analyzer;\n\n        $this->protocolWriter = $writer;\n\n        $this->protocolReader = $reader;\n        $this->protocolReader->on(\n            'close',\n            /**\n             * @return void\n             */\n            function () {\n                $this->shutdown();\n                $this->exit();\n            }\n        );\n        $this->protocolReader->on(\n            'message',\n            /** @return void */\n            asyncCoroutine(\n                /**\n                 * @return \\Generator<int, \\Amp\\Promise, mixed, void>\n                 */\n                function (Message $msg) {\n                    if (!$msg->body) {\n                        return;\n                    }\n\n                    // Ignore responses, this is the handler for requests and notifications\n                    if (AdvancedJsonRpc\\Response::isResponse($msg->body)) {\n                        return;\n                    }\n\n                    /** @psalm-suppress UndefinedPropertyFetch */\n                    if ($msg->body->method === 'textDocument/signatureHelp') {\n                        $this->doAnalysis();\n                    }\n\n                    $result = null;\n                    $error = null;\n                    try {\n                        // Invoke the method handler to get a result\n                        /**\n                         * @var Promise\n                         * @psalm-suppress UndefinedDocblockClass\n                         */\n                        $dispatched = $this->dispatch($msg->body);\n                        /** @psalm-suppress MixedAssignment */\n                        $result = yield $dispatched;\n                    } catch (AdvancedJsonRpc\\Error $e) {\n                        // If a ResponseError is thrown, send it back in the Response\n                        $error = $e;\n                    } catch (Throwable $e) {\n                        // If an unexpected error occurred, send back an INTERNAL_ERROR error response\n                        $error = new AdvancedJsonRpc\\Error(\n                            (string) $e,\n                            AdvancedJsonRpc\\ErrorCode::INTERNAL_ERROR,\n                            null,\n                            $e\n                        );\n                    }\n                    // Only send a Response for a Request\n                    // Notifications do not send Responses\n                    /**\n                     * @psalm-suppress UndefinedPropertyFetch\n                     * @psalm-suppress MixedArgument\n                     */\n                    if (AdvancedJsonRpc\\Request::isRequest($msg->body)) {\n                        if ($error !== null) {\n                            $responseBody = new AdvancedJsonRpc\\ErrorResponse($msg->body->id, $error);\n                        } else {\n                            $responseBody = new AdvancedJsonRpc\\SuccessResponse($msg->body->id, $result);\n                        }\n                        yield $this->protocolWriter->write(new Message($responseBody));\n                    }\n                }\n            )\n        );\n\n        $this->protocolReader->on(\n            'readMessageGroup',\n            /** @return void */\n            function () {\n                $this->doAnalysis();\n            }\n        );\n\n        $this->client = new LanguageClient($reader, $writer);\n    }\n\n    /**\n     * The initialize request is sent as the first request from the client to the server.\n     *\n     * @param ClientCapabilities $capabilities The capabilities provided by the client (editor)\n     * @param string|null $rootPath The rootPath of the workspace. Is null if no folder is open.\n     * @param int|null $processId The process Id of the parent process that started the server.\n     * Is null if the process has not been started by another process. If the parent process is\n     * not alive then the server should exit (see exit notification) its process.\n     * @psalm-return Promise<InitializeResult>\n     * @psalm-suppress PossiblyUnusedMethod\n     */\n    public function initialize(\n        ClientCapabilities $capabilities,\n        string $rootPath = null,\n        int $processId = null\n    ): Promise {\n        return call(\n            /** @return \\Generator<int, true, mixed, InitializeResult> */\n            function () use ($capabilities, $rootPath, $processId) {\n                // Eventually, this might block on something. Leave it as a generator.\n                /** @psalm-suppress TypeDoesNotContainType */\n                if (false) {\n                    yield true;\n                }\n\n                $codebase = $this->project_analyzer->getCodebase();\n\n                $codebase->scanFiles($this->project_analyzer->threads);\n\n                $codebase->config->visitStubFiles($codebase, null);\n\n                if ($this->textDocument === null) {\n                    $this->textDocument = new TextDocument(\n                        $this,\n                        $codebase,\n                        $this->project_analyzer->onchange_line_limit\n                    );\n                }\n\n                $serverCapabilities = new ServerCapabilities();\n\n                $textDocumentSyncOptions = new TextDocumentSyncOptions();\n\n                if ($this->project_analyzer->onchange_line_limit === 0) {\n                    $textDocumentSyncOptions->change = TextDocumentSyncKind::NONE;\n                } else {\n                    $textDocumentSyncOptions->change = TextDocumentSyncKind::FULL;\n                }\n\n                $serverCapabilities->textDocumentSync = $textDocumentSyncOptions;\n\n                // Support \"Find all symbols\"\n                $serverCapabilities->documentSymbolProvider = false;\n                // Support \"Find all symbols in workspace\"\n                $serverCapabilities->workspaceSymbolProvider = false;\n                // Support \"Go to definition\"\n                $serverCapabilities->definitionProvider = true;\n                // Support \"Find all references\"\n                $serverCapabilities->referencesProvider = false;\n                // Support \"Hover\"\n                $serverCapabilities->hoverProvider = true;\n                // Support \"Completion\"\n\n                if ($this->project_analyzer->provide_completion) {\n                    $serverCapabilities->completionProvider = new CompletionOptions();\n                    $serverCapabilities->completionProvider->resolveProvider = false;\n                    $serverCapabilities->completionProvider->triggerCharacters = ['$', '>', ':'];\n                }\n\n                $serverCapabilities->signatureHelpProvider = new SignatureHelpOptions(['(', ',']);\n\n                // Support global references\n                $serverCapabilities->xworkspaceReferencesProvider = false;\n                $serverCapabilities->xdefinitionProvider = false;\n                $serverCapabilities->dependenciesProvider = false;\n\n                return new InitializeResult($serverCapabilities);\n            }\n        );\n    }\n\n    /**\n     * @psalm-suppress PossiblyUnusedMethod\n     *\n     * @return void\n     */\n    public function initialized()\n    {\n    }\n\n    /**\n     * @return void\n     */\n    public function queueTemporaryFileAnalysis(string $file_path, string $uri)\n    {\n        $this->onchange_paths_to_analyze[$file_path] = $uri;\n    }\n\n    /**\n     * @return void\n     */\n    public function queueFileAnalysis(string $file_path, string $uri)\n    {\n        $this->onsave_paths_to_analyze[$file_path] = $uri;\n    }\n\n    /**\n     * @return void\n     */\n    public function doAnalysis()\n    {\n        $codebase = $this->project_analyzer->getCodebase();\n\n        $all_files_to_analyze = $this->onchange_paths_to_analyze + $this->onsave_paths_to_analyze;\n\n        if (!$all_files_to_analyze) {\n            return;\n        }\n\n        if ($this->onsave_paths_to_analyze) {\n            $codebase->reloadFiles($this->project_analyzer, array_keys($this->onsave_paths_to_analyze));\n        }\n\n        if ($this->onchange_paths_to_analyze) {\n            $codebase->reloadFiles($this->project_analyzer, array_keys($this->onchange_paths_to_analyze));\n        }\n\n        $all_file_paths_to_analyze = array_keys($all_files_to_analyze);\n        $codebase->analyzer->addFilesToAnalyze(array_combine($all_file_paths_to_analyze, $all_file_paths_to_analyze));\n        $codebase->analyzer->analyzeFiles($this->project_analyzer, 1, false);\n\n        $this->emitIssues($all_files_to_analyze);\n\n        $this->onchange_paths_to_analyze = [];\n        $this->onsave_paths_to_analyze = [];\n    }\n\n    /**\n     * @param array<string, string> $uris\n     *\n     * @return void\n     */\n    public function emitIssues(array $uris)\n    {\n        $data = \\Psalm\\IssueBuffer::clear();\n\n        foreach ($uris as $file_path => $uri) {\n            $diagnostics = array_map(\n                function (IssueData $issue_data) use ($file_path) : Diagnostic {\n                    //$check_name = $issue->check_name;\n                    $description = $issue_data->message;\n                    $severity = $issue_data->severity;\n\n                    $start_line = max($issue_data->line_from, 1);\n                    $end_line = $issue_data->line_to;\n                    $start_column = $issue_data->column_from;\n                    $end_column = $issue_data->column_to;\n                    // Language server has 0 based lines and columns, phan has 1-based lines and columns.\n                    $range = new Range(\n                        new Position($start_line - 1, $start_column - 1),\n                        new Position($end_line - 1, $end_column - 1)\n                    );\n                    switch ($severity) {\n                        case \\Psalm\\Config::REPORT_INFO:\n                            $diagnostic_severity = DiagnosticSeverity::WARNING;\n                            break;\n                        case \\Psalm\\Config::REPORT_ERROR:\n                        default:\n                            $diagnostic_severity = DiagnosticSeverity::ERROR;\n                            break;\n                    }\n                    // TODO: copy issue code in 'json' format\n                    return new Diagnostic(\n                        $description,\n                        $range,\n                        null,\n                        $diagnostic_severity,\n                        'Psalm'\n                    );\n                },\n                $data[$file_path] ?? []\n            );\n\n            $this->client->textDocument->publishDiagnostics($uri, $diagnostics);\n        }\n    }\n\n    /**\n     * The shutdown request is sent from the client to the server. It asks the server to shut down,\n     * but to not exit (otherwise the response might not be delivered correctly to the client).\n     * There is a separate exit notification that asks the server to exit.\n     *\n     * @psalm-return Promise<null>\n     */\n    public function shutdown()\n    {\n        $codebase = $this->project_analyzer->getCodebase();\n        $scanned_files = $codebase->scanner->getScannedFiles();\n        $codebase->file_reference_provider->updateReferenceCache(\n            $codebase,\n            $scanned_files\n        );\n        return new Success(null);\n    }\n\n    /**\n     * A notification to ask the server to exit its process.\n     *\n     * @return void\n     */\n    public function exit()\n    {\n        exit(0);\n    }\n\n    /**\n     * Transforms an absolute file path into a URI as used by the language server protocol.\n     *\n     * @param string $filepath\n     *\n     * @return string\n     */\n    public static function pathToUri(string $filepath): string\n    {\n        $filepath = trim(str_replace('\\\\', '/', $filepath), '/');\n        $parts = explode('/', $filepath);\n        // Don't %-encode the colon after a Windows drive letter\n        $first = array_shift($parts);\n        if (substr($first, -1) !== ':') {\n            $first = rawurlencode($first);\n        }\n        $parts = array_map('rawurlencode', $parts);\n        array_unshift($parts, $first);\n        $filepath = implode('/', $parts);\n\n        return 'file:///' . $filepath;\n    }\n\n    /**\n     * Transforms URI into file path\n     *\n     * @param string $uri\n     *\n     * @return string\n     */\n    public static function uriToPath(string $uri)\n    {\n        $fragments = parse_url($uri);\n        if ($fragments === false\n            || !isset($fragments['scheme'])\n            || $fragments['scheme'] !== 'file'\n            || !isset($fragments['path'])\n        ) {\n            throw new \\InvalidArgumentException(\"Not a valid file URI: $uri\");\n        }\n\n        $filepath = urldecode((string) $fragments['path']);\n\n        if (strpos($filepath, ':') !== false) {\n            if ($filepath[0] === '/') {\n                $filepath = substr($filepath, 1);\n            }\n            $filepath = str_replace('/', '\\\\', $filepath);\n        }\n\n        return $filepath;\n    }\n}\n", "idx": 1, "id": 8402, "msg": "", "proj": "vimeo-psalm", "lang": "php", "sampling_weight": 0.0739150857232327}
{"patch": "@@ -104,7 +104,8 @@ options.vnode = vnode => {\n \n \tif (type) {\n \t\t// Alias `class` prop to `className` if available\n-\t\tif (props.class != props.className) {\n+\t\t// only alias on dom elements\n+\t\tif (typeof type === 'string' && props.class != props.className) {\n \t\t\tclassNameDescriptor.enumerable = 'className' in props;\n \t\t\tif (props.className != null) props.class = props.className;\n \t\t\tObject.defineProperty(props, 'className', classNameDescriptor);", "y": 1, "oldf": "import {\n\trender as preactRender,\n\thydrate as preactHydrate,\n\toptions,\n\ttoChildArray,\n\tComponent\n} from 'preact';\nimport { applyEventNormalization } from './events';\n\nconst CAMEL_PROPS = /^(?:accent|alignment|arabic|baseline|cap|clip(?!PathU)|color|fill|flood|font|glyph(?!R)|horiz|marker(?!H|W|U)|overline|paint|stop|strikethrough|stroke|text(?!L)|underline|unicode|units|v|vector|vert|word|writing|x(?!C))[A-Z]/;\n\n// Some libraries like `react-virtualized` explicitly check for this.\nComponent.prototype.isReactComponent = {};\n\nexport const REACT_ELEMENT_TYPE =\n\t(typeof Symbol != 'undefined' && Symbol.for && Symbol.for('react.element')) ||\n\t0xeac7;\n\n/**\n * Proxy render() since React returns a Component reference.\n * @param {import('./internal').VNode} vnode VNode tree to render\n * @param {import('./internal').PreactElement} parent DOM node to render vnode tree into\n * @param {() => void} [callback] Optional callback that will be called after rendering\n * @returns {import('./internal').Component | null} The root component reference or null\n */\nexport function render(vnode, parent, callback) {\n\t// React destroys any existing DOM nodes, see #1727\n\t// ...but only on the first render, see #1828\n\tif (parent._children == null) {\n\t\twhile (parent.firstChild) {\n\t\t\tparent.removeChild(parent.firstChild);\n\t\t}\n\t}\n\n\tpreactRender(vnode, parent);\n\tif (typeof callback == 'function') callback();\n\n\treturn vnode ? vnode._component : null;\n}\n\nexport function hydrate(vnode, parent, callback) {\n\tpreactHydrate(vnode, parent);\n\tif (typeof callback == 'function') callback();\n\n\treturn vnode ? vnode._component : null;\n}\n\nlet oldEventHook = options.event;\noptions.event = e => {\n\tif (oldEventHook) e = oldEventHook(e);\n\te.persist = () => {};\n\tlet stoppedPropagating = false,\n\t\tdefaultPrevented = false;\n\n\tconst origStopPropagation = e.stopPropagation;\n\te.stopPropagation = () => {\n\t\torigStopPropagation.call(e);\n\t\tstoppedPropagating = true;\n\t};\n\n\tconst origPreventDefault = e.preventDefault;\n\te.preventDefault = () => {\n\t\torigPreventDefault.call(e);\n\t\tdefaultPrevented = true;\n\t};\n\n\te.isPropagationStopped = () => stoppedPropagating;\n\te.isDefaultPrevented = () => defaultPrevented;\n\treturn (e.nativeEvent = e);\n};\n\n// Patch in `UNSAFE_*` lifecycle hooks\nfunction setSafeDescriptor(proto, key) {\n\tif (proto['UNSAFE_' + key] && !proto[key]) {\n\t\tObject.defineProperty(proto, key, {\n\t\t\tconfigurable: false,\n\t\t\tget() {\n\t\t\t\treturn this['UNSAFE_' + key];\n\t\t\t},\n\t\t\t// This `set` is only used if a user sets a lifecycle like cWU\n\t\t\t// after setting a lifecycle like UNSAFE_cWU. I doubt anyone\n\t\t\t// actually does this in practice so not testing it\n\t\t\t/* istanbul ignore next */\n\t\t\tset(v) {\n\t\t\t\tthis['UNSAFE_' + key] = v;\n\t\t\t}\n\t\t});\n\t}\n}\n\nlet classNameDescriptor = {\n\tconfigurable: true,\n\tget() {\n\t\treturn this.class;\n\t}\n};\n\nlet oldVNodeHook = options.vnode;\noptions.vnode = vnode => {\n\tvnode.$$typeof = REACT_ELEMENT_TYPE;\n\n\tlet type = vnode.type;\n\tlet props = vnode.props;\n\n\tif (type) {\n\t\t// Alias `class` prop to `className` if available\n\t\tif (props.class != props.className) {\n\t\t\tclassNameDescriptor.enumerable = 'className' in props;\n\t\t\tif (props.className != null) props.class = props.className;\n\t\t\tObject.defineProperty(props, 'className', classNameDescriptor);\n\t\t}\n\n\t\t// Apply DOM VNode compat\n\t\tif (typeof type != 'function') {\n\t\t\t// Apply defaultValue to value\n\t\t\tif (props.defaultValue && props.value !== undefined) {\n\t\t\t\tif (!props.value && props.value !== 0) {\n\t\t\t\t\tprops.value = props.defaultValue;\n\t\t\t\t}\n\t\t\t\tdelete props.defaultValue;\n\t\t\t}\n\n\t\t\t// Add support for array select values: <select value={[]} />\n\t\t\tif (Array.isArray(props.value) && props.multiple && type === 'select') {\n\t\t\t\ttoChildArray(props.children).forEach(child => {\n\t\t\t\t\tif (props.value.indexOf(child.props.value) != -1) {\n\t\t\t\t\t\tchild.props.selected = true;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\tdelete props.value;\n\t\t\t}\n\n\t\t\t// Normalize textarea by setting children to value\n\t\t\tif (props.value != null && type === 'textarea') {\n\t\t\t\tprops.children = props.value;\n\t\t\t\tdelete props.value;\n\t\t\t}\n\n\t\t\t// Normalize DOM vnode properties.\n\t\t\tlet shouldSanitize, attrs, i;\n\t\t\tfor (i in props) if ((shouldSanitize = CAMEL_PROPS.test(i))) break;\n\t\t\tif (shouldSanitize) {\n\t\t\t\tattrs = vnode.props = {};\n\t\t\t\tfor (i in props) {\n\t\t\t\t\tattrs[\n\t\t\t\t\t\tCAMEL_PROPS.test(i) ? i.replace(/[A-Z0-9]/, '-$&').toLowerCase() : i\n\t\t\t\t\t] = props[i];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Events\n\t\tapplyEventNormalization(vnode);\n\n\t\t// Component base class compat\n\t\t// We can't just patch the base component class, because components that use\n\t\t// inheritance and are transpiled down to ES5 will overwrite our patched\n\t\t// getters and setters. See #1941\n\t\tif (\n\t\t\ttypeof type == 'function' &&\n\t\t\t!type._patchedLifecycles &&\n\t\t\ttype.prototype\n\t\t) {\n\t\t\tsetSafeDescriptor(type.prototype, 'componentWillMount');\n\t\t\tsetSafeDescriptor(type.prototype, 'componentWillReceiveProps');\n\t\t\tsetSafeDescriptor(type.prototype, 'componentWillUpdate');\n\t\t\ttype._patchedLifecycles = true;\n\t\t}\n\t}\n\n\tif (oldVNodeHook) oldVNodeHook(vnode);\n};\n", "idx": 1, "id": 15851, "msg": "Only aliasing on dom elements breaks the automatic normalization for regular components. So let's say I have a preact component that uses `class` as a prop in my preact-compat app. If I pass the component a className it won't apply it cause it's relying on `class`, not `className`. I think this is valid for preact-compat mode.", "proj": "preactjs-preact", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -22,6 +22,22 @@ type ReaderSeekerCloser struct {\n \tr io.Reader\n }\n \n+// IsReaderSeekable returns if the underlying reader type can be seeked. A\n+// io.Reader might not actually be seekable if it is the ReaderSeekerCloser\n+// type.\n+func IsReaderSeekable(r io.Reader) bool {\n+\tswitch v := r.(type) {\n+\tcase ReaderSeekerCloser:\n+\t\treturn v.IsSeeker()\n+\tcase *ReaderSeekerCloser:\n+\t\treturn v.IsSeeker()\n+\tcase io.ReadSeeker:\n+\t\treturn true\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n // Read reads from the reader up to size of p. The number of bytes read, and\n // error if it occurred will be returned.\n //", "y": 0, "oldf": "package aws\n\nimport (\n\t\"io\"\n\t\"sync\"\n)\n\n// ReadSeekCloser wraps a io.Reader returning a ReaderSeekerCloser. Should\n// only be used with an io.Reader that is also an io.Seeker. Doing so may\n// cause request signature errors, or request body's not sent for GET, HEAD\n// and DELETE HTTP methods.\n//\n// Deprecated: Should only be used with io.ReadSeeker. If using for\n// S3 PutObject to stream content use s3manager.Uploader instead.\nfunc ReadSeekCloser(r io.Reader) ReaderSeekerCloser {\n\treturn ReaderSeekerCloser{r}\n}\n\n// ReaderSeekerCloser represents a reader that can also delegate io.Seeker and\n// io.Closer interfaces to the underlying object if they are available.\ntype ReaderSeekerCloser struct {\n\tr io.Reader\n}\n\n// Read reads from the reader up to size of p. The number of bytes read, and\n// error if it occurred will be returned.\n//\n// If the reader is not an io.Reader zero bytes read, and nil error will be returned.\n//\n// Performs the same functionality as io.Reader Read\nfunc (r ReaderSeekerCloser) Read(p []byte) (int, error) {\n\tswitch t := r.r.(type) {\n\tcase io.Reader:\n\t\treturn t.Read(p)\n\t}\n\treturn 0, nil\n}\n\n// Seek sets the offset for the next Read to offset, interpreted according to\n// whence: 0 means relative to the origin of the file, 1 means relative to the\n// current offset, and 2 means relative to the end. Seek returns the new offset\n// and an error, if any.\n//\n// If the ReaderSeekerCloser is not an io.Seeker nothing will be done.\nfunc (r ReaderSeekerCloser) Seek(offset int64, whence int) (int64, error) {\n\tswitch t := r.r.(type) {\n\tcase io.Seeker:\n\t\treturn t.Seek(offset, whence)\n\t}\n\treturn int64(0), nil\n}\n\n// IsSeeker returns if the underlying reader is also a seeker.\nfunc (r ReaderSeekerCloser) IsSeeker() bool {\n\t_, ok := r.r.(io.Seeker)\n\treturn ok\n}\n\n// Close closes the ReaderSeekerCloser.\n//\n// If the ReaderSeekerCloser is not an io.Closer nothing will be done.\nfunc (r ReaderSeekerCloser) Close() error {\n\tswitch t := r.r.(type) {\n\tcase io.Closer:\n\t\treturn t.Close()\n\t}\n\treturn nil\n}\n\n// A WriteAtBuffer provides a in memory buffer supporting the io.WriterAt interface\n// Can be used with the s3manager.Downloader to download content to a buffer\n// in memory. Safe to use concurrently.\ntype WriteAtBuffer struct {\n\tbuf []byte\n\tm   sync.Mutex\n\n\t// GrowthCoeff defines the growth rate of the internal buffer. By\n\t// default, the growth rate is 1, where expanding the internal\n\t// buffer will allocate only enough capacity to fit the new expected\n\t// length.\n\tGrowthCoeff float64\n}\n\n// NewWriteAtBuffer creates a WriteAtBuffer with an internal buffer\n// provided by buf.\nfunc NewWriteAtBuffer(buf []byte) *WriteAtBuffer {\n\treturn &WriteAtBuffer{buf: buf}\n}\n\n// WriteAt writes a slice of bytes to a buffer starting at the position provided\n// The number of bytes written will be returned, or error. Can overwrite previous\n// written slices if the write ats overlap.\nfunc (b *WriteAtBuffer) WriteAt(p []byte, pos int64) (n int, err error) {\n\tpLen := len(p)\n\texpLen := pos + int64(pLen)\n\tb.m.Lock()\n\tdefer b.m.Unlock()\n\tif int64(len(b.buf)) < expLen {\n\t\tif int64(cap(b.buf)) < expLen {\n\t\t\tif b.GrowthCoeff < 1 {\n\t\t\t\tb.GrowthCoeff = 1\n\t\t\t}\n\t\t\tnewBuf := make([]byte, expLen, int64(b.GrowthCoeff*float64(expLen)))\n\t\t\tcopy(newBuf, b.buf)\n\t\t\tb.buf = newBuf\n\t\t}\n\t\tb.buf = b.buf[:expLen]\n\t}\n\tcopy(b.buf[pos:], p)\n\treturn pLen, nil\n}\n\n// Bytes returns a slice of bytes written to the buffer.\nfunc (b *WriteAtBuffer) Bytes() []byte {\n\tb.m.Lock()\n\tdefer b.m.Unlock()\n\treturn b.buf\n}\n", "idx": 1, "id": 9088, "msg": "", "proj": "aws-aws-sdk-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -15,6 +15,36 @@ describe 'videos/_watch_video.html.erb' do\n     expect { render_view(video) }.to_not raise_error\n   end\n \n+  context \"for a trail video\" do\n+    it \"displays a 'Mark as Complete' button\" do\n+      video = build_video(part_of_trail: true)\n+\n+      render_view(video)\n+\n+      expect(rendered).to have_complete_button\n+    end\n+  end\n+\n+  context \"for a show video\" do\n+    it \"does not display a 'Mark as Complete' button\" do\n+      video = build_video(part_of_trail: false)\n+\n+      render_view(video)\n+\n+      expect(rendered).not_to have_complete_button\n+    end\n+  end\n+\n+  def build_video(part_of_trail:)\n+    build_stubbed(:video).tap do |video|\n+      allow(video).to receive(:part_of_trail?).and_return(part_of_trail)\n+    end\n+  end\n+\n+  def have_complete_button\n+    have_css(\".mark-as-complete\")\n+  end\n+\n   def render_view(video)\n     render 'videos/watch_video', video: video\n   end", "y": 1, "oldf": "require \"rails_helper\"\n\ndescribe 'videos/_watch_video.html.erb' do\n  it \"includes a video's notes as html\" do\n    video = Video.new(wistia_id: '123', notes: 'Some notes')\n\n    render_view(video)\n\n    expect(rendered).to include(video.notes_html)\n  end\n\n  it \"can still render a video without notes\" do\n    video = Video.new(wistia_id: '123')\n\n    expect { render_view(video) }.to_not raise_error\n  end\n\n  def render_view(video)\n    render 'videos/watch_video', video: video\n  end\nend\n", "idx": 1, "id": 14771, "msg": "Rename `have_complete_button` to `have_complete_button?`.", "proj": "thoughtbot-upcase", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -649,6 +649,9 @@ class TabbedBrowser(tabwidget.TabWidget):\n             log.webview.debug(\"on_current_changed got called with invalid \"\n                               \"index {}\".format(idx))\n             return\n+        if self._now_focused is not None:\n+            # save current input mode before switching tab\n+            self._now_focused._input_mode = modeman.instance(self._win_id).mode\n \n         log.modes.debug(\"Current tab changed, focusing {!r}\".format(tab))\n         tab.setFocus()", "y": 0, "oldf": "# vim: ft=python fileencoding=utf-8 sts=4 sw=4 et:\n\n# Copyright 2014-2017 Florian Bruhin (The Compiler) <mail@qutebrowser.org>\n#\n# This file is part of qutebrowser.\n#\n# qutebrowser is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# qutebrowser is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with qutebrowser.  If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"The main tabbed browser widget.\"\"\"\n\nimport functools\n\nimport attr\nfrom PyQt5.QtWidgets import QSizePolicy\nfrom PyQt5.QtCore import pyqtSignal, pyqtSlot, QTimer, QUrl\nfrom PyQt5.QtGui import QIcon\n\nfrom qutebrowser.config import config\nfrom qutebrowser.keyinput import modeman\nfrom qutebrowser.mainwindow import tabwidget, mainwindow\nfrom qutebrowser.browser import signalfilter, browsertab\nfrom qutebrowser.utils import (log, usertypes, utils, qtutils, objreg,\n                               urlutils, message, jinja)\n\n\n@attr.s\nclass UndoEntry:\n\n    \"\"\"Information needed for :undo.\"\"\"\n\n    url = attr.ib()\n    history = attr.ib()\n    index = attr.ib()\n    pinned = attr.ib()\n\n\nclass TabDeletedError(Exception):\n\n    \"\"\"Exception raised when _tab_index is called for a deleted tab.\"\"\"\n\n\nclass TabbedBrowser(tabwidget.TabWidget):\n\n    \"\"\"A TabWidget with QWebViews inside.\n\n    Provides methods to manage tabs, convenience methods to interact with the\n    current tab (cur_*) and filters signals to re-emit them when they occurred\n    in the currently visible tab.\n\n    For all tab-specific signals (cur_*) emitted by a tab, this happens:\n       - the signal gets filtered with _filter_signals and self.cur_* gets\n         emitted if the signal occurred in the current tab.\n\n    Attributes:\n        search_text/search_options: Search parameters which are shared between\n                                    all tabs.\n        _win_id: The window ID this tabbedbrowser is associated with.\n        _filter: A SignalFilter instance.\n        _now_focused: The tab which is focused now.\n        _tab_insert_idx_left: Where to insert a new tab with\n                              tabs.new_tab_position set to 'prev'.\n        _tab_insert_idx_right: Same as above, for 'next'.\n        _undo_stack: List of lists of UndoEntry objects of closed tabs.\n        shutting_down: Whether we're currently shutting down.\n        _local_marks: Jump markers local to each page\n        _global_marks: Jump markers used across all pages\n        default_window_icon: The qutebrowser window icon\n        private: Whether private browsing is on for this window.\n\n    Signals:\n        cur_progress: Progress of the current tab changed (load_progress).\n        cur_load_started: Current tab started loading (load_started)\n        cur_load_finished: Current tab finished loading (load_finished)\n        cur_url_changed: Current URL changed.\n        cur_link_hovered: Link hovered in current tab (link_hovered)\n        cur_scroll_perc_changed: Scroll percentage of current tab changed.\n                                 arg 1: x-position in %.\n                                 arg 2: y-position in %.\n        cur_load_status_changed: Loading status of current tab changed.\n        close_window: The last tab was closed, close this window.\n        resized: Emitted when the browser window has resized, so the completion\n                 widget can adjust its size to it.\n                 arg: The new size.\n        current_tab_changed: The current tab changed to the emitted tab.\n        new_tab: Emits the new WebView and its index when a new tab is opened.\n    \"\"\"\n\n    cur_progress = pyqtSignal(int)\n    cur_load_started = pyqtSignal()\n    cur_load_finished = pyqtSignal(bool)\n    cur_url_changed = pyqtSignal(QUrl)\n    cur_link_hovered = pyqtSignal(str)\n    cur_scroll_perc_changed = pyqtSignal(int, int)\n    cur_load_status_changed = pyqtSignal(str)\n    cur_fullscreen_requested = pyqtSignal(bool)\n    close_window = pyqtSignal()\n    resized = pyqtSignal('QRect')\n    current_tab_changed = pyqtSignal(browsertab.AbstractTab)\n    new_tab = pyqtSignal(browsertab.AbstractTab, int)\n\n    def __init__(self, *, win_id, private, parent=None):\n        super().__init__(win_id, parent)\n        self._win_id = win_id\n        self._tab_insert_idx_left = 0\n        self._tab_insert_idx_right = -1\n        self.shutting_down = False\n        self.tabCloseRequested.connect(self.on_tab_close_requested)\n        self.new_tab_requested.connect(self.tabopen)\n        self.currentChanged.connect(self.on_current_changed)\n        self.cur_load_started.connect(self.on_cur_load_started)\n        self.cur_fullscreen_requested.connect(self.tabBar().maybe_hide)\n        self.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)\n        self._undo_stack = []\n        self._filter = signalfilter.SignalFilter(win_id, self)\n        self._now_focused = None\n        self.search_text = None\n        self.search_options = {}\n        self._local_marks = {}\n        self._global_marks = {}\n        self.default_window_icon = self.window().windowIcon()\n        self.private = private\n        config.instance.changed.connect(self._on_config_changed)\n\n    def __repr__(self):\n        return utils.get_repr(self, count=self.count())\n\n    @pyqtSlot(str)\n    def _on_config_changed(self, option):\n        if option == 'tabs.favicons.show':\n            self._update_favicons()\n        elif option == 'window.title_format':\n            self._update_window_title()\n        elif option in ['tabs.title.format', 'tabs.title.format_pinned']:\n            self._update_tab_titles()\n\n    def _tab_index(self, tab):\n        \"\"\"Get the index of a given tab.\n\n        Raises TabDeletedError if the tab doesn't exist anymore.\n        \"\"\"\n        try:\n            idx = self.indexOf(tab)\n        except RuntimeError as e:\n            log.webview.debug(\"Got invalid tab ({})!\".format(e))\n            raise TabDeletedError(e)\n        if idx == -1:\n            log.webview.debug(\"Got invalid tab (index is -1)!\")\n            raise TabDeletedError(\"index is -1!\")\n        return idx\n\n    def widgets(self):\n        \"\"\"Get a list of open tab widgets.\n\n        We don't implement this as generator so we can delete tabs while\n        iterating over the list.\n        \"\"\"\n        widgets = []\n        for i in range(self.count()):\n            widget = self.widget(i)\n            if widget is None:\n                log.webview.debug(\"Got None-widget in tabbedbrowser!\")\n            else:\n                widgets.append(widget)\n        return widgets\n\n    def _update_window_title(self, field=None):\n        \"\"\"Change the window title to match the current tab.\n\n        Args:\n            idx: The tab index to update.\n            field: A field name which was updated. If given, the title\n                   is only set if the given field is in the template.\n        \"\"\"\n        title_format = config.val.window.title_format\n        if field is not None and ('{' + field + '}') not in title_format:\n            return\n\n        idx = self.currentIndex()\n        if idx == -1:\n            # (e.g. last tab removed)\n            log.webview.debug(\"Not updating window title because index is -1\")\n            return\n        fields = self.get_tab_fields(idx)\n        fields['id'] = self._win_id\n\n        title = title_format.format(**fields)\n        self.window().setWindowTitle(title)\n\n    def _connect_tab_signals(self, tab):\n        \"\"\"Set up the needed signals for tab.\"\"\"\n        # filtered signals\n        tab.link_hovered.connect(\n            self._filter.create(self.cur_link_hovered, tab))\n        tab.load_progress.connect(\n            self._filter.create(self.cur_progress, tab))\n        tab.load_finished.connect(\n            self._filter.create(self.cur_load_finished, tab))\n        tab.load_started.connect(\n            self._filter.create(self.cur_load_started, tab))\n        tab.scroller.perc_changed.connect(\n            self._filter.create(self.cur_scroll_perc_changed, tab))\n        tab.url_changed.connect(\n            self._filter.create(self.cur_url_changed, tab))\n        tab.load_status_changed.connect(\n            self._filter.create(self.cur_load_status_changed, tab))\n        tab.fullscreen_requested.connect(\n            self._filter.create(self.cur_fullscreen_requested, tab))\n        # misc\n        tab.scroller.perc_changed.connect(self.on_scroll_pos_changed)\n        tab.url_changed.connect(\n            functools.partial(self.on_url_changed, tab))\n        tab.title_changed.connect(\n            functools.partial(self.on_title_changed, tab))\n        tab.icon_changed.connect(\n            functools.partial(self.on_icon_changed, tab))\n        tab.load_progress.connect(\n            functools.partial(self.on_load_progress, tab))\n        tab.load_finished.connect(\n            functools.partial(self.on_load_finished, tab))\n        tab.load_started.connect(\n            functools.partial(self.on_load_started, tab))\n        tab.window_close_requested.connect(\n            functools.partial(self.on_window_close_requested, tab))\n        tab.renderer_process_terminated.connect(\n            functools.partial(self._on_renderer_process_terminated, tab))\n        tab.new_tab_requested.connect(self.tabopen)\n        if not self.private:\n            web_history = objreg.get('web-history')\n            tab.add_history_item.connect(web_history.add_from_tab)\n\n    def current_url(self):\n        \"\"\"Get the URL of the current tab.\n\n        Intended to be used from command handlers.\n\n        Return:\n            The current URL as QUrl.\n        \"\"\"\n        idx = self.currentIndex()\n        return super().tab_url(idx)\n\n    def shutdown(self):\n        \"\"\"Try to shut down all tabs cleanly.\"\"\"\n        self.shutting_down = True\n        for tab in self.widgets():\n            self._remove_tab(tab)\n\n    def tab_close_prompt_if_pinned(self, tab, force, yes_action):\n        \"\"\"Helper method for tab_close.\n\n        If tab is pinned, prompt. If not, run yes_action.\n        If tab is destroyed, abort question.\n        \"\"\"\n        if tab.data.pinned and not force:\n            message.confirm_async(\n                title='Pinned Tab',\n                text=\"Are you sure you want to close a pinned tab?\",\n                yes_action=yes_action, default=False, abort_on=[tab.destroyed])\n        else:\n            yes_action()\n\n    def close_tab(self, tab, *, add_undo=True, new_undo=True):\n        \"\"\"Close a tab.\n\n        Args:\n            tab: The QWebView to be closed.\n            add_undo: Whether the tab close can be undone.\n            new_undo: Whether the undo entry should be a new item in the stack.\n        \"\"\"\n        last_close = config.val.tabs.last_close\n        count = self.count()\n\n        if last_close == 'ignore' and count == 1:\n            return\n\n        self._remove_tab(tab, add_undo=add_undo, new_undo=new_undo)\n\n        if count == 1:  # We just closed the last tab above.\n            if last_close == 'close':\n                self.close_window.emit()\n            elif last_close == 'blank':\n                self.openurl(QUrl('about:blank'), newtab=True)\n            elif last_close == 'startpage':\n                for url in config.val.url.start_pages:\n                    self.openurl(url, newtab=True)\n            elif last_close == 'default-page':\n                self.openurl(config.val.url.default_page, newtab=True)\n\n    def _remove_tab(self, tab, *, add_undo=True, new_undo=True, crashed=False):\n        \"\"\"Remove a tab from the tab list and delete it properly.\n\n        Args:\n            tab: The QWebView to be closed.\n            add_undo: Whether the tab close can be undone.\n            new_undo: Whether the undo entry should be a new item in the stack.\n            crashed: Whether we're closing a tab with crashed renderer process.\n        \"\"\"\n        idx = self.indexOf(tab)\n        if idx == -1:\n            if crashed:\n                return\n            raise TabDeletedError(\"tab {} is not contained in \"\n                                  \"TabbedWidget!\".format(tab))\n        if tab is self._now_focused:\n            self._now_focused = None\n        if tab is objreg.get('last-focused-tab', None, scope='window',\n                             window=self._win_id):\n            objreg.delete('last-focused-tab', scope='window',\n                          window=self._win_id)\n\n        if tab.url().isEmpty():\n            # There are some good reasons why a URL could be empty\n            # (target=\"_blank\" with a download, see [1]), so we silently ignore\n            # this.\n            # [1] https://github.com/qutebrowser/qutebrowser/issues/163\n            pass\n        elif not tab.url().isValid():\n            # We display a warning for URLs which are not empty but invalid -\n            # but we don't return here because we want the tab to close either\n            # way.\n            urlutils.invalid_url_error(tab.url(), \"saving tab\")\n        elif add_undo:\n            try:\n                history_data = tab.history.serialize()\n            except browsertab.WebTabError:\n                pass  # special URL\n            else:\n                entry = UndoEntry(tab.url(), history_data, idx,\n                                  tab.data.pinned)\n                if new_undo or not self._undo_stack:\n                    self._undo_stack.append([entry])\n                else:\n                    self._undo_stack[-1].append(entry)\n\n        tab.shutdown()\n        self.removeTab(idx)\n        if not crashed:\n            # WORKAROUND for a segfault when we delete the crashed tab.\n            # see https://bugreports.qt.io/browse/QTBUG-58698\n            tab.layout().unwrap()\n            tab.deleteLater()\n\n    def undo(self):\n        \"\"\"Undo removing of a tab or tabs.\"\"\"\n        # Remove unused tab which may be created after the last tab is closed\n        last_close = config.val.tabs.last_close\n        use_current_tab = False\n        if last_close in ['blank', 'startpage', 'default-page']:\n            only_one_tab_open = self.count() == 1\n            no_history = len(self.widget(0).history) == 1\n            urls = {\n                'blank': QUrl('about:blank'),\n                'startpage': config.val.url.start_pages[0],\n                'default-page': config.val.url.default_page,\n            }\n            first_tab_url = self.widget(0).url()\n            last_close_urlstr = urls[last_close].toString().rstrip('/')\n            first_tab_urlstr = first_tab_url.toString().rstrip('/')\n            last_close_url_used = first_tab_urlstr == last_close_urlstr\n            use_current_tab = (only_one_tab_open and no_history and\n                               last_close_url_used)\n\n        for entry in reversed(self._undo_stack.pop()):\n            if use_current_tab:\n                self.openurl(entry.url, newtab=False)\n                newtab = self.widget(0)\n                use_current_tab = False\n            else:\n                newtab = self.tabopen(entry.url, background=False,\n                                      idx=entry.index)\n\n            newtab.history.deserialize(entry.history)\n            self.set_tab_pinned(newtab, entry.pinned)\n\n    @pyqtSlot('QUrl', bool)\n    def openurl(self, url, newtab):\n        \"\"\"Open a URL, used as a slot.\n\n        Args:\n            url: The URL to open as QUrl.\n            newtab: True to open URL in a new tab, False otherwise.\n        \"\"\"\n        qtutils.ensure_valid(url)\n        if newtab or self.currentWidget() is None:\n            self.tabopen(url, background=False)\n        else:\n            self.currentWidget().openurl(url)\n\n    @pyqtSlot(int)\n    def on_tab_close_requested(self, idx):\n        \"\"\"Close a tab via an index.\"\"\"\n        tab = self.widget(idx)\n        if tab is None:\n            log.webview.debug(\"Got invalid tab {} for index {}!\".format(\n                tab, idx))\n            return\n        self.tab_close_prompt_if_pinned(\n            tab, False, lambda: self.close_tab(tab))\n\n    @pyqtSlot(browsertab.AbstractTab)\n    def on_window_close_requested(self, widget):\n        \"\"\"Close a tab with a widget given.\"\"\"\n        try:\n            self.close_tab(widget)\n        except TabDeletedError:\n            log.webview.debug(\"Requested to close {!r} which does not \"\n                              \"exist!\".format(widget))\n\n    @pyqtSlot('QUrl')\n    @pyqtSlot('QUrl', bool)\n    @pyqtSlot('QUrl', bool, bool)\n    def tabopen(self, url=None, background=None, related=True, idx=None, *,\n                ignore_tabs_are_windows=False):\n        \"\"\"Open a new tab with a given URL.\n\n        Inner logic for open-tab and open-tab-bg.\n        Also connect all the signals we need to _filter_signals.\n\n        Args:\n            url: The URL to open as QUrl or None for an empty tab.\n            background: Whether to open the tab in the background.\n                        if None, the `tabs.background_tabs`` setting decides.\n            related: Whether the tab was opened from another existing tab.\n                     If this is set, the new position might be different. With\n                     the default settings we handle it like Chromium does:\n                         - Tabs from clicked links etc. are to the right of\n                           the current (related=True).\n                         - Explicitly opened tabs are at the very right\n                           (related=False)\n            idx: The index where the new tab should be opened.\n            ignore_tabs_are_windows: If given, never open a new window, even\n                                     with tabs.tabs_are_windows set.\n\n        Return:\n            The opened WebView instance.\n        \"\"\"\n        if url is not None:\n            qtutils.ensure_valid(url)\n        log.webview.debug(\"Creating new tab with URL {}, background {}, \"\n                          \"related {}, idx {}\".format(\n                              url, background, related, idx))\n\n        if (config.val.tabs.tabs_are_windows and self.count() > 0 and\n                not ignore_tabs_are_windows):\n            window = mainwindow.MainWindow(private=self.private)\n            window.show()\n            tabbed_browser = objreg.get('tabbed-browser', scope='window',\n                                        window=window.win_id)\n            return tabbed_browser.tabopen(url=url, background=background,\n                                          related=related)\n\n        tab = browsertab.create(win_id=self._win_id, private=self.private,\n                                parent=self)\n        self._connect_tab_signals(tab)\n\n        if idx is None:\n            idx = self._get_new_tab_idx(related)\n        self.insertTab(idx, tab, \"\")\n\n        if url is not None:\n            tab.openurl(url)\n\n        if background is None:\n            background = config.val.tabs.background\n        if background:\n            # Make sure the background tab has the correct initial size.\n            # With a foreground tab, it's going to be resized correctly by the\n            # layout anyways.\n            tab.resize(self.currentWidget().size())\n            self.tab_index_changed.emit(self.currentIndex(), self.count())\n        else:\n            self.setCurrentWidget(tab)\n\n        tab.show()\n        self.new_tab.emit(tab, idx)\n        return tab\n\n    def _get_new_tab_idx(self, related):\n        \"\"\"Get the index of a tab to insert.\n\n        Args:\n            related: Whether the tab was opened from another tab (as a \"child\")\n\n        Return:\n            The index of the new tab.\n        \"\"\"\n        if related:\n            pos = config.val.tabs.new_position.related\n        else:\n            pos = config.val.tabs.new_position.unrelated\n        if pos == 'prev':\n            idx = self._tab_insert_idx_left\n            # On first sight, we'd think we have to decrement\n            # self._tab_insert_idx_left here, as we want the next tab to be\n            # *before* the one we just opened. However, since we opened a tab\n            # *before* the currently focused tab, indices will shift by\n            # 1 automatically.\n        elif pos == 'next':\n            idx = self._tab_insert_idx_right\n            self._tab_insert_idx_right += 1\n        elif pos == 'first':\n            idx = 0\n        elif pos == 'last':\n            idx = -1\n        else:\n            raise ValueError(\"Invalid tabs.new_position '{}'.\".format(pos))\n        log.webview.debug(\"tabs.new_position {} -> opening new tab at {}, \"\n                          \"next left: {} / right: {}\".format(\n                              pos, idx, self._tab_insert_idx_left,\n                              self._tab_insert_idx_right))\n        return idx\n\n    def _update_favicons(self):\n        \"\"\"Update favicons when config was changed.\"\"\"\n        for i, tab in enumerate(self.widgets()):\n            if config.val.tabs.favicons.show:\n                self.setTabIcon(i, tab.icon())\n                if config.val.tabs.tabs_are_windows:\n                    self.window().setWindowIcon(tab.icon())\n            else:\n                self.setTabIcon(i, QIcon())\n                if config.val.tabs.tabs_are_windows:\n                    self.window().setWindowIcon(self.default_window_icon)\n\n    @pyqtSlot()\n    def on_load_started(self, tab):\n        \"\"\"Clear icon and update title when a tab started loading.\n\n        Args:\n            tab: The tab where the signal belongs to.\n        \"\"\"\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n        self._update_tab_title(idx)\n        if tab.data.keep_icon:\n            tab.data.keep_icon = False\n        else:\n            self.setTabIcon(idx, QIcon())\n            if (config.val.tabs.tabs_are_windows and\n                    config.val.tabs.favicons.show):\n                self.window().setWindowIcon(self.default_window_icon)\n        if idx == self.currentIndex():\n            self._update_window_title()\n\n    @pyqtSlot()\n    def on_cur_load_started(self):\n        \"\"\"Leave insert/hint mode when loading started.\"\"\"\n        modeman.leave(self._win_id, usertypes.KeyMode.insert, 'load started',\n                      maybe=True)\n        modeman.leave(self._win_id, usertypes.KeyMode.hint, 'load started',\n                      maybe=True)\n\n    @pyqtSlot(browsertab.AbstractTab, str)\n    def on_title_changed(self, tab, text):\n        \"\"\"Set the title of a tab.\n\n        Slot for the title_changed signal of any tab.\n\n        Args:\n            tab: The WebView where the title was changed.\n            text: The text to set.\n        \"\"\"\n        if not text:\n            log.webview.debug(\"Ignoring title change to '{}'.\".format(text))\n            return\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n        log.webview.debug(\"Changing title for idx {} to '{}'\".format(\n            idx, text))\n        self.set_page_title(idx, text)\n        if idx == self.currentIndex():\n            self._update_window_title()\n\n    @pyqtSlot(browsertab.AbstractTab, QUrl)\n    def on_url_changed(self, tab, url):\n        \"\"\"Set the new URL as title if there's no title yet.\n\n        Args:\n            tab: The WebView where the title was changed.\n            url: The new URL.\n        \"\"\"\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n\n        if not self.page_title(idx):\n            self.set_page_title(idx, url.toDisplayString())\n\n    @pyqtSlot(browsertab.AbstractTab, QIcon)\n    def on_icon_changed(self, tab, icon):\n        \"\"\"Set the icon of a tab.\n\n        Slot for the iconChanged signal of any tab.\n\n        Args:\n            tab: The WebView where the title was changed.\n            icon: The new icon\n        \"\"\"\n        if not config.val.tabs.favicons.show:\n            return\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n        self.setTabIcon(idx, icon)\n        if config.val.tabs.tabs_are_windows:\n            self.window().setWindowIcon(icon)\n\n    @pyqtSlot(usertypes.KeyMode)\n    def on_mode_left(self, mode):\n        \"\"\"Give focus to current tab if command mode was left.\"\"\"\n        if mode in [usertypes.KeyMode.command, usertypes.KeyMode.prompt,\n                    usertypes.KeyMode.yesno]:\n            widget = self.currentWidget()\n            log.modes.debug(\"Left status-input mode, focusing {!r}\".format(\n                widget))\n            if widget is None:\n                return\n            widget.setFocus()\n\n    @pyqtSlot(int)\n    def on_current_changed(self, idx):\n        \"\"\"Set last-focused-tab and leave hinting mode when focus changed.\"\"\"\n        if idx == -1 or self.shutting_down:\n            # closing the last tab (before quitting) or shutting down\n            return\n        tab = self.widget(idx)\n        if tab is None:\n            log.webview.debug(\"on_current_changed got called with invalid \"\n                              \"index {}\".format(idx))\n            return\n\n        log.modes.debug(\"Current tab changed, focusing {!r}\".format(tab))\n        tab.setFocus()\n\n        modes_to_leave = [usertypes.KeyMode.hint, usertypes.KeyMode.caret]\n        if not config.val.tabs.persist_mode_on_change:\n            modes_to_leave += [usertypes.KeyMode.insert,\n                               usertypes.KeyMode.passthrough]\n        for mode in modes_to_leave:\n            modeman.leave(self._win_id, mode, 'tab changed', maybe=True)\n\n        if self._now_focused is not None:\n            objreg.register('last-focused-tab', self._now_focused, update=True,\n                            scope='window', window=self._win_id)\n        self._now_focused = tab\n        self.current_tab_changed.emit(tab)\n        QTimer.singleShot(0, self._update_window_title)\n        self._tab_insert_idx_left = self.currentIndex()\n        self._tab_insert_idx_right = self.currentIndex() + 1\n\n    @pyqtSlot()\n    def on_cmd_return_pressed(self):\n        \"\"\"Set focus when the commandline closes.\"\"\"\n        log.modes.debug(\"Commandline closed, focusing {!r}\".format(self))\n\n    def on_load_progress(self, tab, perc):\n        \"\"\"Adjust tab indicator on load progress.\"\"\"\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n        start = config.val.colors.tabs.indicator.start\n        stop = config.val.colors.tabs.indicator.stop\n        system = config.val.colors.tabs.indicator.system\n        color = utils.interpolate_color(start, stop, perc, system)\n        self.set_tab_indicator_color(idx, color)\n        self._update_tab_title(idx)\n        if idx == self.currentIndex():\n            self._update_window_title()\n\n    def on_load_finished(self, tab, ok):\n        \"\"\"Adjust tab indicator when loading finished.\"\"\"\n        try:\n            idx = self._tab_index(tab)\n        except TabDeletedError:\n            # We can get signals for tabs we already deleted...\n            return\n        if ok:\n            start = config.val.colors.tabs.indicator.start\n            stop = config.val.colors.tabs.indicator.stop\n            system = config.val.colors.tabs.indicator.system\n            color = utils.interpolate_color(start, stop, 100, system)\n        else:\n            color = config.val.colors.tabs.indicator.error\n        self.set_tab_indicator_color(idx, color)\n        self._update_tab_title(idx)\n        if idx == self.currentIndex():\n            self._update_window_title()\n            tab.handle_auto_insert_mode(ok)\n\n    @pyqtSlot()\n    def on_scroll_pos_changed(self):\n        \"\"\"Update tab and window title when scroll position changed.\"\"\"\n        idx = self.currentIndex()\n        if idx == -1:\n            # (e.g. last tab removed)\n            log.webview.debug(\"Not updating scroll position because index is \"\n                              \"-1\")\n            return\n        self._update_window_title('scroll_pos')\n        self._update_tab_title(idx, 'scroll_pos')\n\n    def _on_renderer_process_terminated(self, tab, status, code):\n        \"\"\"Show an error when a renderer process terminated.\"\"\"\n        if status == browsertab.TerminationStatus.normal:\n            return\n\n        messages = {\n            browsertab.TerminationStatus.abnormal:\n                \"Renderer process exited with status {}\".format(code),\n            browsertab.TerminationStatus.crashed:\n                \"Renderer process crashed\",\n            browsertab.TerminationStatus.killed:\n                \"Renderer process was killed\",\n            browsertab.TerminationStatus.unknown:\n                \"Renderer process did not start\",\n        }\n        msg = messages[status]\n\n        def show_error_page(html):\n            tab.set_html(html)\n            log.webview.error(msg)\n\n        if qtutils.version_check('5.9', compiled=False):\n            url_string = tab.url(requested=True).toDisplayString()\n            error_page = jinja.render(\n                'error.html', title=\"Error loading {}\".format(url_string),\n                url=url_string, error=msg)\n            QTimer.singleShot(100, lambda: show_error_page(error_page))\n        else:\n            # WORKAROUND for https://bugreports.qt.io/browse/QTBUG-58698\n            message.error(msg)\n            self._remove_tab(tab, crashed=True)\n            if self.count() == 0:\n                self.tabopen(QUrl('about:blank'))\n\n    def resizeEvent(self, e):\n        \"\"\"Extend resizeEvent of QWidget to emit a resized signal afterwards.\n\n        Args:\n            e: The QResizeEvent\n        \"\"\"\n        super().resizeEvent(e)\n        self.resized.emit(self.geometry())\n\n    def wheelEvent(self, e):\n        \"\"\"Override wheelEvent of QWidget to forward it to the focused tab.\n\n        Args:\n            e: The QWheelEvent\n        \"\"\"\n        if self._now_focused is not None:\n            self._now_focused.wheelEvent(e)\n        else:\n            e.ignore()\n\n    def set_mark(self, key):\n        \"\"\"Set a mark at the current scroll position in the current tab.\n\n        Args:\n            key: mark identifier; capital indicates a global mark\n        \"\"\"\n        # strip the fragment as it may interfere with scrolling\n        try:\n            url = self.current_url().adjusted(QUrl.RemoveFragment)\n        except qtutils.QtValueError:\n            # show an error only if the mark is not automatically set\n            if key != \"'\":\n                message.error(\"Failed to set mark: url invalid\")\n            return\n        point = self.currentWidget().scroller.pos_px()\n\n        if key.isupper():\n            self._global_marks[key] = point, url\n        else:\n            if url not in self._local_marks:\n                self._local_marks[url] = {}\n            self._local_marks[url][key] = point\n\n    def jump_mark(self, key):\n        \"\"\"Jump to the mark named by `key`.\n\n        Args:\n            key: mark identifier; capital indicates a global mark\n        \"\"\"\n        try:\n            # consider urls that differ only in fragment to be identical\n            urlkey = self.current_url().adjusted(QUrl.RemoveFragment)\n        except qtutils.QtValueError:\n            urlkey = None\n\n        tab = self.currentWidget()\n\n        if key.isupper():\n            if key in self._global_marks:\n                point, url = self._global_marks[key]\n\n                def callback(ok):\n                    \"\"\"Scroll once loading finished.\"\"\"\n                    if ok:\n                        self.cur_load_finished.disconnect(callback)\n                        tab.scroller.to_point(point)\n\n                self.openurl(url, newtab=False)\n                self.cur_load_finished.connect(callback)\n            else:\n                message.error(\"Mark {} is not set\".format(key))\n        elif urlkey is None:\n            message.error(\"Current URL is invalid!\")\n        elif urlkey in self._local_marks and key in self._local_marks[urlkey]:\n            point = self._local_marks[urlkey][key]\n\n            # save the pre-jump position in the special ' mark\n            # this has to happen after we read the mark, otherwise jump_mark\n            # \"'\" would just jump to the current position every time\n            self.set_mark(\"'\")\n\n            tab.scroller.to_point(point)\n        else:\n            message.error(\"Mark {} is not set\".format(key))\n", "idx": 1, "id": 20448, "msg": "", "proj": "qutebrowser-qutebrowser", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -11,7 +11,6 @@ import (\n \n \t\"github.com/influxdata/flux/ast\"\n \t\"github.com/influxdata/flux/codes\"\n-\t\"github.com/influxdata/flux/dependencies\"\n \t\"github.com/influxdata/flux/internal/errors\"\n \t\"github.com/influxdata/flux/interpreter\"\n \t\"github.com/influxdata/flux/parser\"", "y": 0, "oldf": "package flux\n\nimport (\n\t\"context\"\n\tstderrors \"errors\"\n\t\"fmt\"\n\t\"path\"\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/influxdata/flux/ast\"\n\t\"github.com/influxdata/flux/codes\"\n\t\"github.com/influxdata/flux/dependencies\"\n\t\"github.com/influxdata/flux/internal/errors\"\n\t\"github.com/influxdata/flux/interpreter\"\n\t\"github.com/influxdata/flux/parser\"\n\t\"github.com/influxdata/flux/semantic\"\n\t\"github.com/influxdata/flux/values\"\n)\n\nconst (\n\tTablesParameter = \"tables\"\n\ttableKindKey    = \"kind\"\n\ttableParentsKey = \"parents\"\n\ttableSpecKey    = \"spec\"\n\n\tNowOption = \"now\"\n\tnowPkg    = \"universe\"\n)\n\n// Parse parses a Flux script and produces an ast.Package.\nfunc Parse(flux string) (*ast.Package, error) {\n\tastPkg := parser.ParseSource(flux)\n\tif ast.Check(astPkg) > 0 {\n\t\treturn nil, ast.GetError(astPkg)\n\t}\n\n\treturn astPkg, nil\n}\n\n// Eval accepts a Flux script and evaluates it to produce a set of side effects (as a slice of values) and a scope.\nfunc Eval(ctx context.Context, deps dependencies.Interface, flux string, opts ...ScopeMutator) ([]interpreter.SideEffect, values.Scope, error) {\n\tastPkg, err := Parse(flux)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn EvalAST(ctx, deps, astPkg, opts...)\n}\n\n// EvalAST accepts a Flux AST and evaluates it to produce a set of side effects (as a slice of values) and a scope.\nfunc EvalAST(ctx context.Context, deps dependencies.Interface, astPkg *ast.Package, opts ...ScopeMutator) ([]interpreter.SideEffect, values.Scope, error) {\n\tsemPkg, err := semantic.New(astPkg)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tpkg := interpreter.NewPackage(\"\")\n\titrp := interpreter.NewInterpreter(pkg)\n\t// Create a scope for execution whose parent is a copy of the prelude and whose current scope is the package.\n\t// A copy of the prelude must be used since options can be mutated.\n\tscope := values.NewNestedScope(preludeScope.Copy(), pkg)\n\n\tfor _, opt := range opts {\n\t\topt(scope)\n\t}\n\n\tsideEffects, err := itrp.Eval(ctx, deps, semPkg, scope, StdLib())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn sideEffects, scope, nil\n}\n\n// ScopeMutator is any function that mutates the scope of an identifier.\ntype ScopeMutator = func(values.Scope)\n\n// SetOption returns a func that adds a var binding to a scope.\nfunc SetOption(pkg, name string, v values.Value) ScopeMutator {\n\treturn func(scope values.Scope) {\n\t\tscope.SetOption(pkg, name, v)\n\t}\n}\n\n// SetNowOption returns a ScopeMutator that sets the `now` option to the given time.\nfunc SetNowOption(now time.Time) ScopeMutator {\n\treturn SetOption(nowPkg, NowOption, generateNowFunc(now))\n}\n\nfunc generateNowFunc(now time.Time) values.Function {\n\ttimeVal := values.NewTime(values.ConvertTime(now))\n\tftype := semantic.NewFunctionPolyType(semantic.FunctionPolySignature{\n\t\tReturn: semantic.Time,\n\t})\n\tcall := func(ctx context.Context, deps dependencies.Interface, args values.Object) (values.Value, error) {\n\t\treturn timeVal, nil\n\t}\n\tsideEffect := false\n\treturn values.NewFunction(NowOption, ftype, call, sideEffect)\n}\n\ntype CreateOperationSpec func(args Arguments, a *Administration) (OperationSpec, error)\n\n// set of builtins\nvar (\n\tfinalized bool\n\n\tbuiltinPackages = make(map[string]*ast.Package)\n\n\t// list of packages included in the prelude.\n\t// Packages must be listed in import order\n\tprelude = []string{\n\t\t\"universe\",\n\t\t\"influxdata/influxdb\",\n\t}\n\tpreludeScope = &scopeSet{\n\t\tpackages: make([]*interpreter.Package, len(prelude)),\n\t}\n\tstdlib = &importer{make(map[string]*interpreter.Package)}\n)\n\ntype scopeSet struct {\n\tpackages []*interpreter.Package\n}\n\nfunc (s *scopeSet) Lookup(name string) (values.Value, bool) {\n\tfor _, pkg := range s.packages {\n\t\tif v, ok := pkg.Get(name); ok {\n\t\t\tif _, ok := v.(values.Package); ok {\n\t\t\t\t// prelude should not expose any imported packages\n\t\t\t\treturn nil, false\n\t\t\t}\n\t\t\treturn v, ok\n\t\t}\n\t}\n\treturn nil, false\n}\nfunc (s *scopeSet) LocalLookup(name string) (values.Value, bool) {\n\t// scopeSet is always a top level scope\n\treturn s.Lookup(name)\n}\n\nfunc (s *scopeSet) Set(name string, v values.Value) {\n\tpanic(\"cannot mutate the universe block\")\n}\nfunc (s *scopeSet) SetOption(pkg, name string, v values.Value) (bool, error) {\n\tfor _, p := range s.packages {\n\t\tif _, ok := p.Get(name); ok || p.Name() == pkg {\n\t\t\tp.SetOption(name, v)\n\t\t\treturn true, nil\n\t\t}\n\t}\n\treturn false, nil\n}\n\nfunc (s *scopeSet) Nest(obj values.Object) values.Scope {\n\treturn values.NewNestedScope(s, obj)\n}\n\nfunc (s *scopeSet) Pop() values.Scope {\n\treturn nil\n}\n\nfunc (s *scopeSet) Size() int {\n\tvar size int\n\tfor _, pkg := range s.packages {\n\t\tsize += pkg.Len()\n\t}\n\treturn size\n}\n\nfunc (s *scopeSet) Range(f func(k string, v values.Value)) {\n\tfor _, pkg := range s.packages {\n\t\tif pkg == nil {\n\t\t\tpanic(`nil package in scope; try importing \"github.com/influxdata/flux/builtin\"`)\n\t\t}\n\t\tpkg.Range(func(k string, v values.Value) {\n\t\t\tif _, ok := v.(values.Package); ok {\n\t\t\t\t// prelude should not expose any imported packages\n\t\t\t\treturn\n\t\t\t}\n\t\t\tf(k, v)\n\t\t})\n\t}\n}\n\nfunc (s *scopeSet) LocalRange(f func(k string, v values.Value)) {\n\t// scopeSet is always a top level scope\n\ts.Range(f)\n}\n\nfunc (s *scopeSet) SetReturn(v values.Value) {\n\tpanic(\"cannot set return value on universe block\")\n}\n\nfunc (s *scopeSet) Return() values.Value {\n\treturn nil\n}\n\nfunc (s *scopeSet) Copy() values.Scope {\n\tpackages := make([]*interpreter.Package, len(s.packages))\n\tfor i, pkg := range s.packages {\n\t\tpackages[i] = pkg.Copy()\n\t}\n\treturn &scopeSet{\n\t\tpackages: packages,\n\t}\n}\n\n// StdLib returns an importer for the Flux standard library.\nfunc StdLib() interpreter.Importer {\n\treturn stdlib.Copy()\n}\n\n// Prelude returns a scope object representing the Flux universe block\nfunc Prelude() values.Scope {\n\tif !finalized {\n\t\tpanic(\"builtins not finalized\")\n\t}\n\treturn preludeScope.Nest(nil)\n}\n\n// RegisterPackage adds a builtin package\nfunc RegisterPackage(pkg *ast.Package) {\n\tif finalized {\n\t\tpanic(stderrors.New(\"already finalized, cannot register builtin package\"))\n\t}\n\tif _, ok := builtinPackages[pkg.Path]; ok {\n\t\tpanic(fmt.Errorf(\"duplicate builtin package %q\", pkg.Path))\n\t}\n\tbuiltinPackages[pkg.Path] = pkg\n\t_, ok := stdlib.pkgs[pkg.Path]\n\tif !ok {\n\t\t// Lazy creation of interpreter package\n\t\t// registration order is not known so we must create it lazily\n\t\tstdlib.pkgs[pkg.Path] = interpreter.NewPackage(path.Base(pkg.Path))\n\t}\n}\n\n// RegisterPackageValue adds a value for an identifier in a builtin package\nfunc RegisterPackageValue(pkgpath, name string, value values.Value) {\n\tregisterPackageValue(pkgpath, name, value, false)\n}\n\n// ReplacePackageValue replaces a value for an identifier in a builtin package\nfunc ReplacePackageValue(pkgpath, name string, value values.Value) {\n\tregisterPackageValue(pkgpath, name, value, true)\n}\n\nfunc registerPackageValue(pkgpath, name string, value values.Value, replace bool) {\n\tif finalized {\n\t\tpanic(stderrors.New(\"already finalized, cannot register builtin package value\"))\n\t}\n\tpackg, ok := stdlib.pkgs[pkgpath]\n\tif !ok {\n\t\t// Lazy creation of interpreter package\n\t\t// registration order is not known so we must create it lazily\n\t\tpackg = interpreter.NewPackage(path.Base(pkgpath))\n\t\tstdlib.pkgs[pkgpath] = packg\n\t}\n\tif _, ok := packg.Get(name); ok && !replace {\n\t\tpanic(fmt.Errorf(\"duplicate builtin package value %q %q\", pkgpath, name))\n\t} else if !ok && replace {\n\t\tpanic(fmt.Errorf(\"missing builtin package value %q %q\", pkgpath, name))\n\t}\n\tpackg.Set(name, value)\n}\n\n// FunctionValue creates a values.Value from the operation spec and signature.\n// Name is the name of the function as it would be called.\n// c is a function reference of type CreateOperationSpec\n// sig is a function signature type that specifies the names and types of each argument for the function.\nfunc FunctionValue(name string, c CreateOperationSpec, sig semantic.FunctionPolySignature) values.Value {\n\treturn functionValue(name, c, sig, false)\n}\n\n// FunctionValueWithSideEffect creates a values.Value from the operation spec and signature.\n// Name is the name of the function as it would be called.\n// c is a function reference of type CreateOperationSpec\n// sig is a function signature type that specifies the names and types of each argument for the function.\nfunc FunctionValueWithSideEffect(name string, c CreateOperationSpec, sig semantic.FunctionPolySignature) values.Value {\n\treturn functionValue(name, c, sig, true)\n}\n\nfunc functionValue(name string, c CreateOperationSpec, sig semantic.FunctionPolySignature, sideEffects bool) values.Value {\n\tif c == nil {\n\t\tc = func(args Arguments, a *Administration) (OperationSpec, error) {\n\t\t\treturn nil, fmt.Errorf(\"function %q is not implemented\", name)\n\t\t}\n\t}\n\treturn &function{\n\t\tt:             semantic.NewFunctionPolyType(sig),\n\t\tname:          name,\n\t\tcreateOpSpec:  c,\n\t\thasSideEffect: sideEffects,\n\t}\n}\n\n// FinalizeBuiltIns must be called to complete registration.\n// Future calls to RegisterFunction or RegisterPackageValue will panic.\nfunc FinalizeBuiltIns() {\n\tif finalized {\n\t\tpanic(\"already finalized\")\n\t}\n\tfinalized = true\n\n\tfor i, path := range prelude {\n\t\tpkg, ok := stdlib.ImportPackageObject(path)\n\t\tif !ok {\n\t\t\tpanic(fmt.Sprintf(\"missing prelude package %q\", path))\n\t\t}\n\t\tpreludeScope.packages[i] = pkg\n\t}\n\n\tif err := evalBuiltInPackages(); err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc evalBuiltInPackages() error {\n\torder, err := packageOrder(prelude, builtinPackages)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, astPkg := range order {\n\t\tif ast.Check(astPkg) > 0 {\n\t\t\terr := ast.GetError(astPkg)\n\t\t\treturn errors.Wrapf(err, codes.Inherit, \"failed to parse builtin package %q\", astPkg.Path)\n\t\t}\n\t\tsemPkg, err := semantic.New(astPkg)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, codes.Inherit, \"failed to create semantic graph for builtin package %q\", astPkg.Path)\n\t\t}\n\n\t\tpkg := stdlib.pkgs[astPkg.Path]\n\t\tif pkg == nil {\n\t\t\treturn errors.Wrapf(err, codes.Inherit, \"package does not exist %q\", astPkg.Path)\n\t\t}\n\n\t\t// Validate packages before evaluating them\n\t\tif err := validatePackageBuiltins(pkg, astPkg); err != nil {\n\t\t\treturn errors.Wrapf(err, codes.Inherit, \"package has invalid builtins %q\", astPkg.Path)\n\t\t}\n\n\t\titrp := interpreter.NewInterpreter(pkg)\n\t\tif _, err := itrp.Eval(context.Background(), dependencies.NewEmpty(), semPkg, preludeScope.Nest(pkg), stdlib); err != nil {\n\t\t\treturn errors.Wrapf(err, codes.Inherit, \"failed to evaluate builtin package %q\", astPkg.Path)\n\t\t}\n\t}\n\treturn nil\n}\n\n// validatePackageBuiltins ensures that all package builtins have both an AST builtin statement and a registered value.\nfunc validatePackageBuiltins(pkg *interpreter.Package, astPkg *ast.Package) error {\n\tbuiltinStmts := make(map[string]*ast.BuiltinStatement)\n\tast.Walk(ast.CreateVisitor(func(n ast.Node) {\n\t\tif bs, ok := n.(*ast.BuiltinStatement); ok {\n\t\t\tbuiltinStmts[bs.ID.Name] = bs\n\t\t}\n\t}), astPkg)\n\n\tmissing := make([]string, 0, len(builtinStmts))\n\textra := make([]string, 0, len(builtinStmts))\n\n\tfor n := range builtinStmts {\n\t\tif _, ok := pkg.Get(n); !ok {\n\t\t\tmissing = append(missing, n)\n\t\t\tcontinue\n\t\t}\n\t\t// TODO(nathanielc): Ensure that the value's type matches the type expression\n\t}\n\tpkg.Range(func(k string, v values.Value) {\n\t\tif _, ok := builtinStmts[k]; !ok {\n\t\t\textra = append(extra, k)\n\t\t\treturn\n\t\t}\n\t})\n\tif len(missing) > 0 || len(extra) > 0 {\n\t\treturn fmt.Errorf(\"missing builtin values %v, extra builtin values %v\", missing, extra)\n\t}\n\treturn nil\n}\n\nvar TableObjectType = semantic.NewObjectPolyType(\n\t//TODO: When values.Value support polytyped values, we can add the commented fields back in\n\tmap[string]semantic.PolyType{\n\t\ttableKindKey: semantic.String,\n\t\t//tableSpecKey:    semantic.Tvar(1),\n\t\t//tableParentsKey: semantic.Tvar(2),\n\t},\n\tnil,\n\t//semantic.LabelSet{tableKindKey, tableSpecKey, tableParentsKey},\n\tsemantic.LabelSet{tableKindKey},\n)\nvar _ = tableSpecKey // So that linter doesn't think tableSpecKey is unused, considering above TODO.\n\nvar TableObjectMonoType semantic.Type\n\nfunc init() {\n\tTableObjectMonoType, _ = TableObjectType.MonoType()\n}\n\n// IDer produces the mapping of table Objects to OperationIDs\ntype IDer interface {\n\tID(*TableObject) OperationID\n}\n\n// IDerOpSpec is the interface any operation spec that needs\n// access to OperationIDs in the query spec must implement.\ntype IDerOpSpec interface {\n\tIDer(ider IDer)\n}\n\n// TableObject represents the value returned by a transformation.\n// As such, it holds the OperationSpec of the transformation it is associated with,\n// and it is a values.Value (and, also, a values.Object).\n// It can be compiled and executed as a flux.Program by using a lang.TableObjectCompiler.\ntype TableObject struct {\n\t// TODO(Josh): Remove args once the\n\t// OperationSpec interface has an Equal method.\n\targs    Arguments\n\tKind    OperationKind\n\tSpec    OperationSpec\n\tParents values.Array\n}\n\nfunc (t *TableObject) Operation(ider IDer) *Operation {\n\tif iderOpSpec, ok := t.Spec.(IDerOpSpec); ok {\n\t\tiderOpSpec.IDer(ider)\n\t}\n\n\treturn &Operation{\n\t\tID:   ider.ID(t),\n\t\tSpec: t.Spec,\n\t}\n}\nfunc (t *TableObject) IsNull() bool {\n\treturn false\n}\nfunc (t *TableObject) String() string {\n\tstr := new(strings.Builder)\n\tt.str(str, false)\n\treturn str.String()\n}\nfunc (t *TableObject) str(b *strings.Builder, arrow bool) {\n\tmultiParent := t.Parents.Len() > 1\n\tif multiParent {\n\t\tb.WriteString(\"( \")\n\t}\n\tt.Parents.Range(func(i int, p values.Value) {\n\t\tparent := p.Object().(*TableObject)\n\t\tparent.str(b, !multiParent)\n\t\tif multiParent {\n\t\t\tb.WriteString(\"; \")\n\t\t}\n\t})\n\tif multiParent {\n\t\tb.WriteString(\" ) -> \")\n\t}\n\tb.WriteString(string(t.Kind))\n\tif arrow {\n\t\tb.WriteString(\" -> \")\n\t}\n}\n\nfunc (t *TableObject) Type() semantic.Type {\n\ttyp, _ := TableObjectType.MonoType()\n\treturn typ\n}\nfunc (t *TableObject) PolyType() semantic.PolyType {\n\treturn TableObjectType\n}\n\nfunc (t *TableObject) Str() string {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.String))\n}\nfunc (t *TableObject) Bytes() []byte {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Bytes))\n}\nfunc (t *TableObject) Int() int64 {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Int))\n}\nfunc (t *TableObject) UInt() uint64 {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.UInt))\n}\nfunc (t *TableObject) Float() float64 {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Float))\n}\nfunc (t *TableObject) Bool() bool {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Bool))\n}\nfunc (t *TableObject) Time() values.Time {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Time))\n}\nfunc (t *TableObject) Duration() values.Duration {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Duration))\n}\nfunc (t *TableObject) Regexp() *regexp.Regexp {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Regexp))\n}\nfunc (t *TableObject) Array() values.Array {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Array))\n}\nfunc (t *TableObject) Object() values.Object {\n\treturn t\n}\nfunc (t *TableObject) Equal(rhs values.Value) bool {\n\tif t.Type() != rhs.Type() {\n\t\treturn false\n\t}\n\tr := rhs.Object()\n\tif t.Len() != r.Len() {\n\t\treturn false\n\t}\n\tvar isEqual = true\n\t// Range over both TableObjects and\n\t// compare their properties for equality\n\tt.Range(func(k string, v values.Value) {\n\t\tw, ok := r.Get(k)\n\t\tisEqual = isEqual && ok && v.Equal(w)\n\t})\n\treturn isEqual\n}\nfunc (t *TableObject) Function() values.Function {\n\tpanic(values.UnexpectedKind(semantic.Object, semantic.Function))\n}\n\nfunc (t *TableObject) Get(name string) (values.Value, bool) {\n\tswitch name {\n\tcase tableKindKey:\n\t\treturn values.NewString(string(t.Kind)), true\n\tcase tableParentsKey:\n\t\treturn t.Parents, true\n\tdefault:\n\t\treturn t.args.Get(name)\n\t}\n}\n\nfunc (t *TableObject) Set(name string, v values.Value) {\n\t// immutable\n}\n\nfunc (t *TableObject) Len() int {\n\treturn len(t.args.GetAll()) + 2\n}\n\nfunc (t *TableObject) Range(f func(name string, v values.Value)) {\n\tfor _, arg := range t.args.GetAll() {\n\t\tval, _ := t.args.Get(arg)\n\t\tf(arg, val)\n\t}\n\tf(tableKindKey, values.NewString(string(t.Kind)))\n\tf(tableParentsKey, t.Parents)\n}\n\n// FunctionSignature returns a standard functions signature which accepts a table piped argument,\n// with any additional arguments.\nfunc FunctionSignature(parameters map[string]semantic.PolyType, required []string) semantic.FunctionPolySignature {\n\tif parameters == nil {\n\t\tparameters = make(map[string]semantic.PolyType)\n\t}\n\tparameters[TablesParameter] = TableObjectType\n\treturn semantic.FunctionPolySignature{\n\t\tParameters:   parameters,\n\t\tRequired:     semantic.LabelSet(required),\n\t\tReturn:       TableObjectType,\n\t\tPipeArgument: TablesParameter,\n\t}\n}\n\ntype Administration struct {\n\tparents values.Array\n}\n\nfunc newAdministration() *Administration {\n\treturn &Administration{\n\t\t// TODO(nathanielc): Once we can support recursive types change this to,\n\t\t// interpreter.NewArray(TableObjectType)\n\t\tparents: values.NewArray(semantic.EmptyObject),\n\t}\n}\n\n// AddParentFromArgs reads the args for the `table` argument and adds the value as a parent.\nfunc (a *Administration) AddParentFromArgs(args Arguments) error {\n\tparent, err := args.GetRequiredObject(TablesParameter)\n\tif err != nil {\n\t\treturn err\n\t}\n\tp, ok := parent.(*TableObject)\n\tif !ok {\n\t\treturn fmt.Errorf(\"argument is not a table object: got %T\", parent)\n\t}\n\ta.AddParent(p)\n\treturn nil\n}\n\n// AddParent instructs the evaluation Context that a new edge should be created from the parent to the current operation.\n// Duplicate parents will be removed, so the caller need not concern itself with which parents have already been added.\nfunc (a *Administration) AddParent(np *TableObject) {\n\t// Check for duplicates\n\tfound := false\n\ta.parents.Range(func(i int, v values.Value) {\n\t\tif p, ok := v.(*TableObject); ok && p == np {\n\t\t\tfound = true\n\t\t}\n\t})\n\tif !found {\n\t\ta.parents.Append(np)\n\t}\n}\n\ntype function struct {\n\tname          string\n\tt             semantic.PolyType\n\tcreateOpSpec  CreateOperationSpec\n\thasSideEffect bool\n}\n\nfunc (f *function) Type() semantic.Type {\n\t// TODO(nathanielc): Update values.Value interface to use PolyTypes\n\tt, _ := f.t.MonoType()\n\treturn t\n}\nfunc (f *function) PolyType() semantic.PolyType {\n\treturn f.t\n}\nfunc (f *function) IsNull() bool {\n\treturn false\n}\nfunc (f *function) Str() string {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.String))\n}\nfunc (f *function) Bytes() []byte {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Bytes))\n}\nfunc (f *function) Int() int64 {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Int))\n}\nfunc (f *function) UInt() uint64 {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.UInt))\n}\nfunc (f *function) Float() float64 {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Float))\n}\nfunc (f *function) Bool() bool {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Bool))\n}\nfunc (f *function) Time() values.Time {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Time))\n}\nfunc (f *function) Duration() values.Duration {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Duration))\n}\nfunc (f *function) Regexp() *regexp.Regexp {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Regexp))\n}\nfunc (f *function) Array() values.Array {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Array))\n}\nfunc (f *function) Object() values.Object {\n\tpanic(values.UnexpectedKind(semantic.Function, semantic.Object))\n}\nfunc (f *function) Function() values.Function {\n\treturn f\n}\nfunc (f *function) Equal(rhs values.Value) bool {\n\tif f.Type() != rhs.Type() {\n\t\treturn false\n\t}\n\tv, ok := rhs.(*function)\n\treturn ok && (f == v)\n}\nfunc (f *function) HasSideEffect() bool {\n\treturn f.hasSideEffect\n}\n\nfunc (f *function) Call(ctx context.Context, deps dependencies.Interface, args values.Object) (values.Value, error) {\n\treturn interpreter.DoFunctionCall(f.call, args)\n}\n\nfunc (f *function) call(args interpreter.Arguments) (values.Value, error) {\n\ta := newAdministration()\n\targuments := Arguments{Arguments: args}\n\tspec, err := f.createOpSpec(arguments, a)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tt := &TableObject{\n\t\targs:    arguments,\n\t\tKind:    spec.Kind(),\n\t\tSpec:    spec,\n\t\tParents: a.parents,\n\t}\n\treturn t, nil\n}\nfunc (f *function) String() string {\n\treturn fmt.Sprintf(\"%v\", f.t)\n}\n\ntype Arguments struct {\n\tinterpreter.Arguments\n}\n\nfunc (a Arguments) GetTime(name string) (Time, bool, error) {\n\tv, ok := a.Get(name)\n\tif !ok {\n\t\treturn Time{}, false, nil\n\t}\n\tqt, err := ToQueryTime(v)\n\tif err != nil {\n\t\treturn Time{}, ok, err\n\t}\n\treturn qt, ok, nil\n}\n\nfunc (a Arguments) GetRequiredTime(name string) (Time, error) {\n\tqt, ok, err := a.GetTime(name)\n\tif err != nil {\n\t\treturn Time{}, err\n\t}\n\tif !ok {\n\t\treturn Time{}, fmt.Errorf(\"missing required keyword argument %q\", name)\n\t}\n\treturn qt, nil\n}\n\nfunc (a Arguments) GetDuration(name string) (Duration, bool, error) {\n\tv, ok := a.Get(name)\n\tif !ok {\n\t\treturn 0, false, nil\n\t}\n\treturn Duration(v.Duration()), true, nil\n}\n\nfunc (a Arguments) GetRequiredDuration(name string) (Duration, error) {\n\td, ok, err := a.GetDuration(name)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"missing required keyword argument %q\", name)\n\t}\n\treturn d, nil\n}\n\nfunc ToQueryTime(value values.Value) (Time, error) {\n\tswitch value.Type().Nature() {\n\tcase semantic.Time:\n\t\treturn Time{\n\t\t\tAbsolute: value.Time().Time(),\n\t\t}, nil\n\tcase semantic.Duration:\n\t\treturn Time{\n\t\t\tRelative:   value.Duration().Duration(),\n\t\t\tIsRelative: true,\n\t\t}, nil\n\tcase semantic.Int:\n\t\treturn Time{\n\t\t\tAbsolute: time.Unix(value.Int(), 0),\n\t\t}, nil\n\tdefault:\n\t\treturn Time{}, fmt.Errorf(\"value is not a time, got %v\", value.Type())\n\t}\n}\n\ntype importer struct {\n\tpkgs map[string]*interpreter.Package\n}\n\nfunc (imp *importer) Copy() *importer {\n\tpackages := make(map[string]*interpreter.Package, len(imp.pkgs))\n\tfor k, v := range imp.pkgs {\n\t\tpackages[k] = v.Copy()\n\t}\n\treturn &importer{\n\t\tpkgs: packages,\n\t}\n}\n\nfunc (imp *importer) Import(path string) (semantic.PackageType, bool) {\n\tp, ok := imp.pkgs[path]\n\tif !ok {\n\t\treturn semantic.PackageType{}, false\n\t}\n\treturn semantic.PackageType{\n\t\tName: p.Name(),\n\t\tType: p.PolyType(),\n\t}, true\n}\n\nfunc (imp *importer) ImportPackageObject(path string) (*interpreter.Package, bool) {\n\tp, ok := imp.pkgs[path]\n\treturn p, ok\n}\n\n// packageOrder determines a safe order to process builtin packages such that all dependent packages are previously processed.\nfunc packageOrder(prelude []string, pkgs map[string]*ast.Package) (order []*ast.Package, err error) {\n\t//TODO(nathanielc): Add import cycle detection, this is not needed until this code is promoted to work with third party imports\n\n\t// Always import prelude first so other packages need not explicitly import the prelude packages.\n\tfor _, path := range prelude {\n\t\tpkg := pkgs[path]\n\t\torder, err = insertPkg(pkg, pkgs, order)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\t// Import all other packages\n\tfor _, pkg := range pkgs {\n\t\torder, err = insertPkg(pkg, pkgs, order)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn\n}\n\nfunc insertPkg(pkg *ast.Package, pkgs map[string]*ast.Package, order []*ast.Package) (_ []*ast.Package, err error) {\n\timports := findImports(pkg)\n\tfor _, path := range imports {\n\t\tdep, ok := pkgs[path]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"unknown builtin package %q\", path)\n\t\t}\n\t\torder, err = insertPkg(dep, pkgs, order)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn appendPkg(pkg, order), nil\n}\n\nfunc appendPkg(pkg *ast.Package, pkgs []*ast.Package) []*ast.Package {\n\tif containsPkg(pkg.Path, pkgs) {\n\t\treturn pkgs\n\t}\n\treturn append(pkgs, pkg)\n}\n\nfunc containsPkg(path string, pkgs []*ast.Package) bool {\n\tfor _, pkg := range pkgs {\n\t\tif pkg.Path == path {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc findImports(pkg *ast.Package) (imports []string) {\n\tfor _, f := range pkg.Files {\n\t\tfor _, i := range f.Imports {\n\t\t\timports = append(imports, i.Path.Value)\n\t\t}\n\t}\n\treturn\n}\n", "idx": 1, "id": 11859, "msg": "", "proj": "influxdata-flux", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -180,7 +180,7 @@ func (s *ServiceStatusDesc) JSONString() (string, error) {\n // HumanString returns the stringified ServiceStatusDesc struct with human readable format.\n func (s *ServiceStatusDesc) HumanString() string {\n \tvar b bytes.Buffer\n-\twriter := tabwriter.NewWriter(&b, minCellWidth, tabWidth, cellPaddingWidth, paddingChar, noAdditionalFormatting)\n+\twriter := tabwriter.NewWriter(&b, minCellWidth, tabWidth, statusCellPaddingWidth, paddingChar, noAdditionalFormatting)\n \tfmt.Fprint(writer, color.Bold.Sprint(\"Service Status\\n\\n\"))\n \twriter.Flush()\n \tfmt.Fprintf(writer, \"  %s %v / %v running tasks (%v pending)\\n\", statusColor(s.Service.Status),", "y": 0, "oldf": "// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// SPDX-License-Identifier: Apache-2.0\n\npackage describe\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"text/tabwriter\"\n\n\t\"github.com/aws/copilot-cli/internal/pkg/aws/aas\"\n\t\"github.com/aws/copilot-cli/internal/pkg/aws/cloudwatch\"\n\t\"github.com/aws/copilot-cli/internal/pkg/aws/ecs\"\n\trg \"github.com/aws/copilot-cli/internal/pkg/aws/resourcegroups\"\n\t\"github.com/aws/copilot-cli/internal/pkg/aws/sessions\"\n\t\"github.com/aws/copilot-cli/internal/pkg/deploy\"\n\t\"github.com/aws/copilot-cli/internal/pkg/term/color\"\n)\n\nconst (\n\tecsServiceResourceType = \"ecs:service\"\n)\n\ntype alarmStatusGetter interface {\n\tAlarmsWithTags(tags map[string]string) ([]cloudwatch.AlarmStatus, error)\n\tAlarmStatus(alarms []string) ([]cloudwatch.AlarmStatus, error)\n}\n\ntype resourcesGetter interface {\n\tGetResourcesByTags(resourceType string, tags map[string]string) ([]*rg.Resource, error)\n}\n\ntype ecsServiceGetter interface {\n\tServiceTasks(clusterName, serviceName string) ([]*ecs.Task, error)\n\tService(clusterName, serviceName string) (*ecs.Service, error)\n}\n\ntype autoscalingAlarmNamesGetter interface {\n\tECSServiceAlarmNames(cluster, service string) ([]string, error)\n}\n\n// ServiceStatus retrieves status of a service.\ntype ServiceStatus struct {\n\tapp string\n\tenv string\n\tsvc string\n\n\tecsSvc ecsServiceGetter\n\tcwSvc  alarmStatusGetter\n\taasSvc autoscalingAlarmNamesGetter\n\trgSvc  resourcesGetter\n}\n\n// ServiceStatusDesc contains the status for a service.\ntype ServiceStatusDesc struct {\n\tService ecs.ServiceStatus\n\tTasks   []ecs.TaskStatus         `json:\"tasks\"`\n\tAlarms  []cloudwatch.AlarmStatus `json:\"alarms\"`\n}\n\n// NewServiceStatusConfig contains fields that initiates ServiceStatus struct.\ntype NewServiceStatusConfig struct {\n\tApp         string\n\tEnv         string\n\tSvc         string\n\tConfigStore ConfigStoreSvc\n}\n\n// NewServiceStatus instantiates a new ServiceStatus struct.\nfunc NewServiceStatus(opt *NewServiceStatusConfig) (*ServiceStatus, error) {\n\tenv, err := opt.ConfigStore.GetEnvironment(opt.App, opt.Env)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get environment %s: %w\", opt.Env, err)\n\t}\n\tsess, err := sessions.NewProvider().FromRole(env.ManagerRoleARN, env.Region)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"session for role %s and region %s: %w\", env.ManagerRoleARN, env.Region, err)\n\t}\n\treturn &ServiceStatus{\n\t\tapp:    opt.App,\n\t\tenv:    opt.Env,\n\t\tsvc:    opt.Svc,\n\t\trgSvc:  rg.New(sess),\n\t\tcwSvc:  cloudwatch.New(sess),\n\t\tecsSvc: ecs.New(sess),\n\t\taasSvc: aas.New(sess),\n\t}, nil\n}\n\nfunc (s *ServiceStatus) getServiceArn() (*ecs.ServiceArn, error) {\n\tsvcResources, err := s.rgSvc.GetResourcesByTags(ecsServiceResourceType, map[string]string{\n\t\tdeploy.AppTagKey:     s.app,\n\t\tdeploy.EnvTagKey:     s.env,\n\t\tdeploy.ServiceTagKey: s.svc,\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(svcResources) == 0 {\n\t\treturn nil, fmt.Errorf(\"cannot find service arn in service stack resource\")\n\t}\n\tserviceArn := ecs.ServiceArn(svcResources[0].ARN)\n\treturn &serviceArn, nil\n}\n\n// Describe returns status of a service.\nfunc (s *ServiceStatus) Describe() (*ServiceStatusDesc, error) {\n\tserviceArn, err := s.getServiceArn()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get service ARN: %w\", err)\n\t}\n\tclusterName, err := serviceArn.ClusterName()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get cluster name: %w\", err)\n\t}\n\tserviceName, err := serviceArn.ServiceName()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get service name: %w\", err)\n\t}\n\tservice, err := s.ecsSvc.Service(clusterName, serviceName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get service %s: %w\", serviceName, err)\n\t}\n\ttasks, err := s.ecsSvc.ServiceTasks(clusterName, serviceName)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get tasks for service %s: %w\", serviceName, err)\n\t}\n\tvar taskStatus []ecs.TaskStatus\n\tfor _, task := range tasks {\n\t\tstatus, err := task.TaskStatus()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"get status for task %s: %w\", *task.TaskArn, err)\n\t\t}\n\t\ttaskStatus = append(taskStatus, *status)\n\t}\n\tvar alarms []cloudwatch.AlarmStatus\n\ttaggedAlarms, err := s.cwSvc.AlarmsWithTags(map[string]string{\n\t\tdeploy.AppTagKey:     s.app,\n\t\tdeploy.EnvTagKey:     s.env,\n\t\tdeploy.ServiceTagKey: s.svc,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get tagged CloudWatch alarms: %w\", err)\n\t}\n\talarms = append(alarms, taggedAlarms...)\n\tautoscalingAlarms, err := s.ecsServiceAutoscalingAlarms(clusterName, serviceName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\talarms = append(alarms, autoscalingAlarms...)\n\treturn &ServiceStatusDesc{\n\t\tService: service.ServiceStatus(),\n\t\tTasks:   taskStatus,\n\t\tAlarms:  alarms,\n\t}, nil\n}\n\nfunc (s *ServiceStatus) ecsServiceAutoscalingAlarms(cluster, service string) ([]cloudwatch.AlarmStatus, error) {\n\talarmNames, err := s.aasSvc.ECSServiceAlarmNames(cluster, service)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"retrieve auto scaling alarm names for ECS service %s/%s: %w\", cluster, service, err)\n\t}\n\talarms, err := s.cwSvc.AlarmStatus(alarmNames)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"get auto scaling CloudWatch alarms: %w\", err)\n\t}\n\treturn alarms, nil\n}\n\n// JSONString returns the stringified ServiceStatusDesc struct with json format.\nfunc (s *ServiceStatusDesc) JSONString() (string, error) {\n\tb, err := json.Marshal(s)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"marshal services: %w\", err)\n\t}\n\treturn fmt.Sprintf(\"%s\\n\", b), nil\n}\n\n// HumanString returns the stringified ServiceStatusDesc struct with human readable format.\nfunc (s *ServiceStatusDesc) HumanString() string {\n\tvar b bytes.Buffer\n\twriter := tabwriter.NewWriter(&b, minCellWidth, tabWidth, cellPaddingWidth, paddingChar, noAdditionalFormatting)\n\tfmt.Fprint(writer, color.Bold.Sprint(\"Service Status\\n\\n\"))\n\twriter.Flush()\n\tfmt.Fprintf(writer, \"  %s %v / %v running tasks (%v pending)\\n\", statusColor(s.Service.Status),\n\t\ts.Service.RunningCount, s.Service.DesiredCount, s.Service.DesiredCount-s.Service.RunningCount)\n\tfmt.Fprint(writer, color.Bold.Sprint(\"\\nLast Deployment\\n\\n\"))\n\twriter.Flush()\n\tfmt.Fprintf(writer, \"  %s\\t%s\\n\", \"Updated At\", humanizeTime(s.Service.LastDeploymentAt))\n\tfmt.Fprintf(writer, \"  %s\\t%s\\n\", \"Task Definition\", s.Service.TaskDefinition)\n\tfmt.Fprint(writer, color.Bold.Sprint(\"\\nTask Status\\n\\n\"))\n\twriter.Flush()\n\tfmt.Fprintf(writer, \"  %s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", \"ID\", \"Image Digest\", \"Last Status\", \"Health Status\", \"Started At\", \"Stopped At\")\n\tfor _, task := range s.Tasks {\n\t\tfmt.Fprint(writer, task.HumanString())\n\t}\n\tfmt.Fprint(writer, color.Bold.Sprint(\"\\nAlarms\\n\\n\"))\n\twriter.Flush()\n\tfmt.Fprintf(writer, \"  %s\\t%s\\t%s\\t%s\\n\", \"Name\", \"Health\", \"Last Updated\", \"Reason\")\n\tfor _, alarm := range s.Alarms {\n\t\tupdatedTimeSince := humanizeTime(alarm.UpdatedTimes)\n\t\tfmt.Fprintf(writer, \"  %s\\t%s\\t%s\\t%s\\n\", alarm.Name, alarm.Status, updatedTimeSince, alarm.Reason)\n\t}\n\twriter.Flush()\n\treturn b.String()\n}\n\nfunc statusColor(status string) string {\n\tswitch status {\n\tcase \"ACTIVE\":\n\t\treturn color.Green.Sprint(status)\n\tcase \"DRAINING\":\n\t\treturn color.Yellow.Sprint(status)\n\tdefault:\n\t\treturn color.Red.Sprint(status)\n\t}\n}\n", "idx": 3, "id": 15158, "msg": "", "proj": "aws-copilot-cli", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -11,6 +11,7 @@ type QuicError struct {\n \tErrorMessage       string\n \tisTimeout          bool\n \tisApplicationError bool\n+\tdelayed            bool\n }\n \n var _ net.Error = &QuicError{}", "y": 1, "oldf": "package qerr\n\nimport (\n\t\"fmt\"\n\t\"net\"\n)\n\n// A QuicError consists of an error code plus a error reason\ntype QuicError struct {\n\tErrorCode          ErrorCode\n\tErrorMessage       string\n\tisTimeout          bool\n\tisApplicationError bool\n}\n\nvar _ net.Error = &QuicError{}\n\n// Error creates a new QuicError instance\nfunc Error(errorCode ErrorCode, errorMessage string) *QuicError {\n\treturn &QuicError{\n\t\tErrorCode:    errorCode,\n\t\tErrorMessage: errorMessage,\n\t}\n}\n\n// TimeoutError creates a new QuicError instance for a timeout error\nfunc TimeoutError(errorMessage string) *QuicError {\n\treturn &QuicError{\n\t\tErrorMessage: errorMessage,\n\t\tisTimeout:    true,\n\t}\n}\n\n// CryptoError create a new QuicError instance for a crypto error\nfunc CryptoError(tlsAlert uint8, errorMessage string) *QuicError {\n\treturn &QuicError{\n\t\tErrorCode:    0x100 + ErrorCode(tlsAlert),\n\t\tErrorMessage: errorMessage,\n\t}\n}\n\nfunc ApplicationError(errorCode ErrorCode, errorMessage string) *QuicError {\n\treturn &QuicError{\n\t\tErrorCode:          errorCode,\n\t\tErrorMessage:       errorMessage,\n\t\tisApplicationError: true,\n\t}\n}\n\nfunc (e *QuicError) Error() string {\n\tif e.isApplicationError {\n\t\tif len(e.ErrorMessage) == 0 {\n\t\t\treturn fmt.Sprintf(\"Application error %#x\", uint64(e.ErrorCode))\n\t\t}\n\t\treturn fmt.Sprintf(\"Application error %#x: %s\", uint64(e.ErrorCode), e.ErrorMessage)\n\t}\n\tif len(e.ErrorMessage) == 0 {\n\t\treturn e.ErrorCode.Error()\n\t}\n\treturn fmt.Sprintf(\"%s: %s\", e.ErrorCode.String(), e.ErrorMessage)\n}\n\n// IsCryptoError says if this error is a crypto error\nfunc (e *QuicError) IsCryptoError() bool {\n\treturn e.ErrorCode.isCryptoError()\n}\n\n// Temporary says if the error is temporary.\nfunc (e *QuicError) Temporary() bool {\n\treturn false\n}\n\n// Timeout says if this error is a timeout.\nfunc (e *QuicError) Timeout() bool {\n\treturn e.isTimeout\n}\n\n// ToQuicError converts an arbitrary error to a QuicError. It leaves QuicErrors\n// unchanged, and properly handles `ErrorCode`s.\nfunc ToQuicError(err error) *QuicError {\n\tswitch e := err.(type) {\n\tcase *QuicError:\n\t\treturn e\n\tcase ErrorCode:\n\t\treturn Error(e, \"\")\n\t}\n\treturn Error(InternalError, err.Error())\n}\n", "idx": 1, "id": 8378, "msg": "It looks like this is only ever true for (1) remote close before handshake completes and (2) version negotiation failure. I propose we replace \"delay\" with \"isInsecureClose\" or similar throughout. I think that would be clearer.", "proj": "lucas-clemente-quic-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -722,6 +722,32 @@ public abstract class ValueSourceParser implements NamedListInitializedPlugin {\n \n         TInfo tinfo = parseTerm(fp); // would have made this parser a new separate class and registered it, but this handy method is private :/\n \n+        IndexSchema schema = fp.getReq().getCore().getLatestSchema();\n+        final FieldType fieldType = schema.getFieldType(tinfo.field);\n+\n+        final String payloadEncoder = PayloadUtils.getPayloadEncoder(fieldType);\n+        if (payloadEncoder.equals(\"identity\")) {\n+\n+          ValueSource defaultValueSource;\n+          if (fp.hasMoreArguments()) {\n+            defaultValueSource = fp.parseValueSource();\n+          } else {\n+            defaultValueSource = new LiteralValueSource(\"\");\n+          }\n+\n+          if (fp.hasMoreArguments()) {\n+              // functions are not supported with strings\n+              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Invalid payload function: \" + fp.parseArg());\n+          }\n+\n+          return new StringPayloadValueSource(\n+              tinfo.field,\n+              tinfo.val,\n+              tinfo.indexedField,\n+              tinfo.indexedBytes.get(),\n+              defaultValueSource);\n+        }\n+\n         ValueSource defaultValueSource;\n         if (fp.hasMoreArguments()) {\n           defaultValueSource = fp.parseValueSource();", "y": 1, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.search;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.apache.lucene.index.LeafReaderContext;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.queries.function.FunctionScoreQuery;\nimport org.apache.lucene.queries.function.FunctionValues;\nimport org.apache.lucene.queries.function.ValueSource;\nimport org.apache.lucene.queries.function.docvalues.BoolDocValues;\nimport org.apache.lucene.queries.function.docvalues.DoubleDocValues;\nimport org.apache.lucene.queries.function.docvalues.LongDocValues;\nimport org.apache.lucene.queries.function.valuesource.*;\nimport org.apache.lucene.queries.payloads.PayloadDecoder;\nimport org.apache.lucene.queries.payloads.PayloadFunction;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.TermQuery;\nimport org.apache.lucene.search.spell.JaroWinklerDistance;\nimport org.apache.lucene.search.spell.LevenshteinDistance;\nimport org.apache.lucene.search.spell.NGramDistance;\nimport org.apache.lucene.search.spell.StringDistance;\nimport org.apache.lucene.util.BytesRefBuilder;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.request.SolrRequestInfo;\nimport org.apache.solr.schema.CurrencyFieldType;\nimport org.apache.solr.schema.FieldType;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.SchemaField;\nimport org.apache.solr.schema.StrField;\nimport org.apache.solr.schema.TextField;\nimport org.apache.solr.search.facet.AggValueSource;\nimport org.apache.solr.search.facet.AvgAgg;\nimport org.apache.solr.search.facet.CountAgg;\nimport org.apache.solr.search.facet.CountValsAgg;\nimport org.apache.solr.search.facet.HLLAgg;\nimport org.apache.solr.search.facet.MinMaxAgg;\nimport org.apache.solr.search.facet.MissingAgg;\nimport org.apache.solr.search.facet.PercentileAgg;\nimport org.apache.solr.search.facet.RelatednessAgg;\nimport org.apache.solr.search.facet.StddevAgg;\nimport org.apache.solr.search.facet.SumAgg;\nimport org.apache.solr.search.facet.SumsqAgg;\nimport org.apache.solr.search.facet.UniqueAgg;\nimport org.apache.solr.search.facet.UniqueBlockAgg;\nimport org.apache.solr.search.facet.VarianceAgg;\nimport org.apache.solr.search.function.CollapseScoreFunction;\nimport org.apache.solr.search.function.ConcatStringFunction;\nimport org.apache.solr.search.function.EqualFunction;\nimport org.apache.solr.search.function.OrdFieldSource;\nimport org.apache.solr.search.function.ReverseOrdFieldSource;\nimport org.apache.solr.search.function.SolrComparisonBoolFunction;\nimport org.apache.solr.search.function.distance.GeoDistValueSourceParser;\nimport org.apache.solr.search.function.distance.GeohashFunction;\nimport org.apache.solr.search.function.distance.GeohashHaversineFunction;\nimport org.apache.solr.search.function.distance.HaversineFunction;\nimport org.apache.solr.search.function.distance.SquaredEuclideanFunction;\nimport org.apache.solr.search.function.distance.StringDistanceFunction;\nimport org.apache.solr.search.function.distance.VectorDistanceFunction;\nimport org.apache.solr.search.join.ChildFieldValueSourceParser;\nimport org.apache.solr.util.DateMathParser;\nimport org.apache.solr.util.PayloadUtils;\nimport org.apache.solr.util.plugin.NamedListInitializedPlugin;\nimport org.locationtech.spatial4j.distance.DistanceUtils;\n\n/**\n * A factory that parses user queries to generate ValueSource instances.\n * Intended usage is to create pluggable, named functions for use in function queries.\n */\npublic abstract class ValueSourceParser implements NamedListInitializedPlugin {\n  /**\n   * Initialize the plugin.\n   */\n  @Override\n  public void init(NamedList args) {}\n\n  /**\n   * Parse the user input into a ValueSource.\n   */\n  public abstract ValueSource parse(FunctionQParser fp) throws SyntaxError;\n\n  /** standard functions supported by default, filled in static class initialization */\n  private static final Map<String, ValueSourceParser> standardVSParsers = new HashMap<>();\n  \n  /** standard functions supported by default */\n  public static final Map<String, ValueSourceParser> standardValueSourceParsers\n    = Collections.unmodifiableMap(standardVSParsers);\n\n  /** Adds a new parser for the name and returns any existing one that was overridden.\n   *  This is not thread safe.\n   */\n  private static ValueSourceParser addParser(String name, ValueSourceParser p) {\n    return standardVSParsers.put(name, p);\n  }\n\n  /** Adds a new parser for the name and returns any existing one that was overridden.\n   *  This is not thread safe.\n   */\n  private static ValueSourceParser addParser(NamedParser p) {\n    return standardVSParsers.put(p.name(), p);\n  }\n\n  private static void alias(String source, String dest) {\n    standardVSParsers.put(dest, standardVSParsers.get(source));\n  }\n\n  static {\n    addParser(\"testfunc\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        final ValueSource source = fp.parseValueSource();\n        return new TestValueSource(source);\n      }\n    });\n    addParser(\"ord\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        String field = fp.parseId();\n        return new OrdFieldSource(field);\n      }\n    });\n    addParser(\"literal\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new LiteralValueSource(fp.parseArg());\n      }\n    });\n    addParser(\"threadid\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new LongConstValueSource(Thread.currentThread().getId());\n      }\n    });\n    addParser(\"sleep\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        int ms = fp.parseInt();\n        ValueSource source = fp.parseValueSource();\n        try {\n          Thread.sleep(ms);\n        } catch (InterruptedException e) {\n          throw new RuntimeException(e);\n        }\n        return source;\n      }\n    });\n    addParser(\"rord\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        String field = fp.parseId();\n        return new ReverseOrdFieldSource(field);\n      }\n    });\n    addParser(\"top\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        // top(vs) is now a no-op\n        ValueSource source = fp.parseValueSource();\n        return source;\n      }\n    });\n    addParser(\"linear\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource source = fp.parseValueSource();\n        float slope = fp.parseFloat();\n        float intercept = fp.parseFloat();\n        return new LinearFloatFunction(source, slope, intercept);\n      }\n    });\n    addParser(\"recip\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource source = fp.parseValueSource();\n        float m = fp.parseFloat();\n        float a = fp.parseFloat();\n        float b = fp.parseFloat();\n        return new ReciprocalFloatFunction(source, m, a, b);\n      }\n    });\n    addParser(\"scale\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource source = fp.parseValueSource();\n        float min = fp.parseFloat();\n        float max = fp.parseFloat();\n        return new ScaleFloatFunction(source, min, max);\n      }\n    });\n    addParser(\"div\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource a = fp.parseValueSource();\n        ValueSource b = fp.parseValueSource();\n        return new DivFloatFunction(a, b);\n      }\n    });\n    addParser(\"mod\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource a = fp.parseValueSource();\n        ValueSource b = fp.parseValueSource();\n        return new DualFloatFunction(a, b) {\n          @Override\n          protected String name() {\n            return \"mod\";\n          }\n          @Override\n          protected float func(int doc, FunctionValues aVals, FunctionValues bVals) throws IOException {\n            return aVals.floatVal(doc) % bVals.floatVal(doc);\n          }\n        };\n      }\n    });\n    addParser(\"map\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource source = fp.parseValueSource();\n        float min = fp.parseFloat();\n        float max = fp.parseFloat();\n        ValueSource target = fp.parseValueSource();\n        ValueSource def = fp.hasMoreArguments() ? fp.parseValueSource() : null;\n        return new RangeMapFloatFunction(source, min, max, target, def);\n      }\n    });\n\n    addParser(\"abs\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource source = fp.parseValueSource();\n        return new SimpleFloatFunction(source) {\n          @Override\n          protected String name() {\n            return \"abs\";\n          }\n\n          @Override\n          protected float func(int doc, FunctionValues vals) throws IOException {\n            return Math.abs(vals.floatVal(doc));\n          }\n        };\n      }\n    });\n    addParser(\"cscore\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new CollapseScoreFunction();\n      }\n    });\n    addParser(\"sum\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new SumFloatFunction(sources.toArray(new ValueSource[sources.size()]));\n      }\n    });\n    alias(\"sum\",\"add\");    \n\n    addParser(\"product\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new ProductFloatFunction(sources.toArray(new ValueSource[sources.size()]));\n      }\n    });\n    alias(\"product\",\"mul\");\n\n    addParser(\"sub\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource a = fp.parseValueSource();\n        ValueSource b = fp.parseValueSource();\n        return new DualFloatFunction(a, b) {\n          @Override\n          protected String name() {\n            return \"sub\";\n          }\n\n          @Override\n          protected float func(int doc, FunctionValues aVals, FunctionValues bVals) throws IOException {\n            return aVals.floatVal(doc) - bVals.floatVal(doc);\n          }\n        };\n      }\n    });\n    addParser(\"vector\", new ValueSourceParser(){\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new VectorValueSource(fp.parseValueSourceList());\n      }\n    });\n    addParser(\"query\", new ValueSourceParser() {\n      // boost(query($q),rating)\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        Query q = fp.parseNestedQuery();\n        float defVal = 0.0f;\n        if (fp.hasMoreArguments()) {\n          defVal = fp.parseFloat();\n        }\n        return new QueryValueSource(q, defVal);\n      }\n    });\n    addParser(\"boost\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        Query q = fp.parseNestedQuery();\n        ValueSource vs = fp.parseValueSource();\n        return new QueryValueSource(FunctionScoreQuery.boostByValue(q, vs.asDoubleValuesSource()), 0.0f);\n      }\n    });\n    addParser(\"joindf\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        String f0 = fp.parseArg();\n        String qf = fp.parseArg();\n        return new JoinDocFreqValueSource( f0, qf );\n      }\n    });\n\n    addParser(\"geodist\", new GeoDistValueSourceParser());\n\n    addParser(\"hsin\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n\n        double radius = fp.parseDouble();\n        //SOLR-2114, make the convert flag required, since the parser doesn't support much in the way of lookahead or the ability to convert a String into a ValueSource\n        boolean convert = Boolean.parseBoolean(fp.parseArg());\n        \n        MultiValueSource pv1;\n        MultiValueSource pv2;\n\n        ValueSource one = fp.parseValueSource();\n        ValueSource two = fp.parseValueSource();\n        if (fp.hasMoreArguments()) {\n          pv1 = new VectorValueSource(Arrays.asList(one, two));//x1, y1\n          pv2 = new VectorValueSource(Arrays.asList(fp.parseValueSource(), fp.parseValueSource()));//x2, y2\n        } else {\n          //check to see if we have multiValue source\n          if (one instanceof MultiValueSource && two instanceof MultiValueSource){\n            pv1 = (MultiValueSource) one;\n            pv2 = (MultiValueSource) two;\n          } else {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                    \"Input must either be 2 MultiValueSources, or there must be 4 ValueSources\");\n          }\n        }\n\n        return new HaversineFunction(pv1, pv2, radius, convert);\n      }\n    });\n\n    addParser(\"ghhsin\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        double radius = fp.parseDouble();\n\n        ValueSource gh1 = fp.parseValueSource();\n        ValueSource gh2 = fp.parseValueSource();\n\n        return new GeohashHaversineFunction(gh1, gh2, radius);\n      }\n    });\n\n    addParser(\"geohash\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n\n        ValueSource lat = fp.parseValueSource();\n        ValueSource lon = fp.parseValueSource();\n\n        return new GeohashFunction(lat, lon);\n      }\n    });\n    addParser(\"strdist\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n\n        ValueSource str1 = fp.parseValueSource();\n        ValueSource str2 = fp.parseValueSource();\n        String distClass = fp.parseArg();\n\n        StringDistance dist = null;\n        if (distClass.equalsIgnoreCase(\"jw\")) {\n          dist = new JaroWinklerDistance();\n        } else if (distClass.equalsIgnoreCase(\"edit\")) {\n          dist = new LevenshteinDistance();\n        } else if (distClass.equalsIgnoreCase(\"ngram\")) {\n          int ngram = 2;\n          if (fp.hasMoreArguments()) {\n            ngram = fp.parseInt();\n          }\n          dist = new NGramDistance(ngram);\n        } else {\n          dist = fp.req.getCore().getResourceLoader().newInstance(distClass, StringDistance.class);\n        }\n        return new StringDistanceFunction(str1, str2, dist);\n      }\n    });\n    addParser(\"field\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n\n        String fieldName = fp.parseArg();\n        SchemaField f = fp.getReq().getSchema().getField(fieldName);\n        if (fp.hasMoreArguments()) {\n          // multivalued selector option\n          String s = fp.parseArg();\n          FieldType.MultiValueSelector selector = FieldType.MultiValueSelector.lookup(s);\n          if (null == selector) {\n            throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                                    \"Multi-Valued field selector '\"+s+\"' not supported\");\n          }\n          return f.getType().getSingleValueSource(selector, f, fp);\n        }\n        // simple field ValueSource\n        return f.getType().getValueSource(f, fp);\n      }\n    });\n    addParser(\"currency\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n\n        String fieldName = fp.parseArg();\n        SchemaField f = fp.getReq().getSchema().getField(fieldName);\n        if (! (f.getType() instanceof CurrencyFieldType)) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n                                  \"Currency function input must be the name of a CurrencyFieldType: \" + fieldName);\n        }\n        CurrencyFieldType ft = (CurrencyFieldType) f.getType();\n        String code = fp.hasMoreArguments() ? fp.parseArg() : null;\n        return ft.getConvertedValueSource(code, ft.getValueSource(f, fp));\n      }\n    });\n\n    addParser(new DoubleParser(\"rad\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return vals.doubleVal(doc) * DistanceUtils.DEGREES_TO_RADIANS;\n      }\n    });\n    addParser(new DoubleParser(\"deg\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return vals.doubleVal(doc) * DistanceUtils.RADIANS_TO_DEGREES;\n      }\n    });\n    addParser(new DoubleParser(\"sqrt\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.sqrt(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"cbrt\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.cbrt(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"log\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.log10(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"ln\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.log(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"exp\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.exp(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"sin\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.sin(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"cos\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.cos(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"tan\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.tan(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"asin\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.asin(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"acos\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.acos(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"atan\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.atan(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"sinh\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.sinh(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"cosh\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.cosh(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"tanh\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.tanh(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"ceil\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.ceil(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"floor\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.floor(vals.doubleVal(doc));\n      }\n    });\n    addParser(new DoubleParser(\"rint\") {\n      @Override\n      public double func(int doc, FunctionValues vals) throws IOException {\n        return Math.rint(vals.doubleVal(doc));\n      }\n    });\n    addParser(new Double2Parser(\"pow\") {\n      @Override\n      public double func(int doc, FunctionValues a, FunctionValues b) throws IOException {\n        return Math.pow(a.doubleVal(doc), b.doubleVal(doc));\n      }\n    });\n    addParser(new Double2Parser(\"hypot\") {\n      @Override\n      public double func(int doc, FunctionValues a, FunctionValues b) throws IOException {\n        return Math.hypot(a.doubleVal(doc), b.doubleVal(doc));\n      }\n    });\n    addParser(new Double2Parser(\"atan2\") {\n      @Override\n      public double func(int doc, FunctionValues a, FunctionValues b) throws IOException {\n        return Math.atan2(a.doubleVal(doc), b.doubleVal(doc));\n      }\n    });\n    addParser(\"max\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new MaxFloatFunction(sources.toArray(new ValueSource[sources.size()]));\n      }\n    });\n    addParser(\"min\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new MinFloatFunction(sources.toArray(new ValueSource[sources.size()]));\n      }\n    });\n\n    addParser(\"sqedist\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        MVResult mvr = getMultiValueSources(sources);\n\n        return new SquaredEuclideanFunction(mvr.mv1, mvr.mv2);\n      }\n    });\n\n    addParser(\"dist\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        float power = fp.parseFloat();\n        List<ValueSource> sources = fp.parseValueSourceList();\n        MVResult mvr = getMultiValueSources(sources);\n        return new VectorDistanceFunction(power, mvr.mv1, mvr.mv2);\n      }\n    });\n    addParser(\"ms\", new DateValueSourceParser());\n\n    \n    addParser(\"pi\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new DoubleConstValueSource(Math.PI);\n      }\n    });\n    addParser(\"e\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new DoubleConstValueSource(Math.E);\n      }\n    });\n\n\n    addParser(\"docfreq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        TInfo tinfo = parseTerm(fp);\n        return new DocFreqValueSource(tinfo.field, tinfo.val, tinfo.indexedField, tinfo.indexedBytes.get());\n      }\n    });\n\n    addParser(\"totaltermfreq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        TInfo tinfo = parseTerm(fp);\n        return new TotalTermFreqValueSource(tinfo.field, tinfo.val, tinfo.indexedField, tinfo.indexedBytes.get());\n      }\n    });\n    alias(\"totaltermfreq\",\"ttf\");\n\n    addParser(\"sumtotaltermfreq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        String field = fp.parseArg();\n        return new SumTotalTermFreqValueSource(field);\n      }\n    });\n    alias(\"sumtotaltermfreq\",\"sttf\");\n\n    addParser(\"idf\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        TInfo tinfo = parseTerm(fp);\n        return new IDFValueSource(tinfo.field, tinfo.val, tinfo.indexedField, tinfo.indexedBytes.get());\n      }\n    });\n\n    addParser(\"termfreq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        TInfo tinfo = parseTerm(fp);\n        return new TermFreqValueSource(tinfo.field, tinfo.val, tinfo.indexedField, tinfo.indexedBytes.get());\n      }\n    });\n\n    addParser(\"tf\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        TInfo tinfo = parseTerm(fp);\n        return new TFValueSource(tinfo.field, tinfo.val, tinfo.indexedField, tinfo.indexedBytes.get());\n      }\n    });\n\n    addParser(\"norm\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        String field = fp.parseArg();\n        return new NormValueSource(field);\n      }\n    });\n\n    addParser(\"maxdoc\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new MaxDocValueSource();\n      }\n    });\n\n    addParser(\"numdocs\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new NumDocsValueSource();\n      }\n    });\n\n    addParser(\"payload\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        // payload(field,value[,default, ['min|max|average|first']])\n        //   defaults to \"average\" and 0.0 default value\n\n        TInfo tinfo = parseTerm(fp); // would have made this parser a new separate class and registered it, but this handy method is private :/\n\n        ValueSource defaultValueSource;\n        if (fp.hasMoreArguments()) {\n          defaultValueSource = fp.parseValueSource();\n        } else {\n          defaultValueSource = new ConstValueSource(0.0f);\n        }\n\n        PayloadFunction payloadFunction = null;\n        String func = \"average\";\n        if (fp.hasMoreArguments()) {\n          func = fp.parseArg();\n        }\n        payloadFunction = PayloadUtils.getPayloadFunction(func);\n\n        // Support func=\"first\" by payloadFunction=null\n        if(payloadFunction == null && !\"first\".equals(func)) {\n          // not \"first\" (or average, min, or max)\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Invalid payload function: \" + func);\n        }\n\n        IndexSchema schema = fp.getReq().getCore().getLatestSchema();\n        PayloadDecoder decoder = schema.getPayloadDecoder(tinfo.field);\n\n        if (decoder==null) {\n          throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No payload decoder found for field: \" + tinfo.field);\n        }\n\n        return new FloatPayloadValueSource(\n            tinfo.field,\n            tinfo.val,\n            tinfo.indexedField,\n            tinfo.indexedBytes.get(),\n            decoder,\n            payloadFunction,\n            defaultValueSource);\n      }\n    });\n\n    addParser(\"true\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new BoolConstValueSource(true);\n      }\n    });\n\n    addParser(\"false\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) {\n        return new BoolConstValueSource(false);\n      }\n    });\n\n    addParser(\"exists\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource vs = fp.parseValueSource();\n        return new SimpleBoolFunction(vs) {\n          @Override\n          protected String name() {\n            return \"exists\";\n          }\n          @Override\n          protected boolean func(int doc, FunctionValues vals) throws IOException {\n            return vals.exists(doc);\n          }\n        };\n      }\n    });\n\n    addParser(\"not\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource vs = fp.parseValueSource();\n        return new SimpleBoolFunction(vs) {\n          @Override\n          protected boolean func(int doc, FunctionValues vals) throws IOException {\n            return !vals.boolVal(doc);\n          }\n          @Override\n          protected String name() {\n            return \"not\";\n          }\n        };\n      }\n    });\n\n\n    addParser(\"and\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new MultiBoolFunction(sources) {\n          @Override\n          protected String name() {\n            return \"and\";\n          }\n          @Override\n          protected boolean func(int doc, FunctionValues[] vals) throws IOException {\n            for (FunctionValues dv : vals)\n              if (!dv.boolVal(doc)) return false;\n            return true;\n          }\n        };\n      }\n    });\n\n    addParser(\"or\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new MultiBoolFunction(sources) {\n          @Override\n          protected String name() {\n            return \"or\";\n          }\n          @Override\n          protected boolean func(int doc, FunctionValues[] vals) throws IOException {\n            for (FunctionValues dv : vals)\n              if (dv.boolVal(doc)) return true;\n            return false;\n          }\n        };\n      }\n    });\n\n    addParser(\"xor\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new MultiBoolFunction(sources) {\n          @Override\n          protected String name() {\n            return \"xor\";\n          }\n          @Override\n          protected boolean func(int doc, FunctionValues[] vals) throws IOException {\n            int nTrue=0, nFalse=0;\n            for (FunctionValues dv : vals) {\n              if (dv.boolVal(doc)) nTrue++;\n              else nFalse++;\n            }\n            return nTrue != 0 && nFalse != 0;\n          }\n        };\n      }\n    });\n\n    addParser(\"if\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource ifValueSource = fp.parseValueSource();\n        ValueSource trueValueSource = fp.parseValueSource();\n        ValueSource falseValueSource = fp.parseValueSource();\n\n        return new IfFunction(ifValueSource, trueValueSource, falseValueSource);\n      }\n    });\n\n    addParser(\"gt\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource lhsValSource = fp.parseValueSource();\n        ValueSource rhsValSource = fp.parseValueSource();\n\n        return new SolrComparisonBoolFunction(lhsValSource, rhsValSource, \"gt\", (cmp) -> cmp > 0);\n      }\n    });\n\n    addParser(\"lt\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource lhsValSource = fp.parseValueSource();\n        ValueSource rhsValSource = fp.parseValueSource();\n\n        return new SolrComparisonBoolFunction(lhsValSource, rhsValSource, \"lt\", (cmp) -> cmp < 0);\n      }\n    });\n\n    addParser(\"gte\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource lhsValSource = fp.parseValueSource();\n        ValueSource rhsValSource = fp.parseValueSource();\n\n        return new SolrComparisonBoolFunction(lhsValSource, rhsValSource, \"gte\", (cmp) -> cmp >= 0);\n\n      }\n    });\n\n    addParser(\"lte\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource lhsValSource = fp.parseValueSource();\n        ValueSource rhsValSource = fp.parseValueSource();\n\n        return new SolrComparisonBoolFunction(lhsValSource, rhsValSource, \"lte\", (cmp) -> cmp <= 0);\n      }\n    });\n\n    addParser(\"eq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        ValueSource lhsValSource = fp.parseValueSource();\n        ValueSource rhsValSource = fp.parseValueSource();\n\n        return new EqualFunction(lhsValSource, rhsValSource, \"eq\");\n      }\n    });\n\n    addParser(\"def\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new DefFunction(fp.parseValueSourceList());\n      }\n    });\n\n    addParser(\"concat\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        List<ValueSource> sources = fp.parseValueSourceList();\n        return new ConcatStringFunction(sources.toArray(new ValueSource[sources.size()]));\n      }\n    });\n\n\n    addParser(\"agg\", new ValueSourceParser() {\n      @Override\n      public AggValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return fp.parseAgg(FunctionQParser.FLAG_DEFAULT);\n      }\n    });\n\n    addParser(\"agg_count\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new CountAgg();\n      }\n    });\n\n    addParser(\"agg_unique\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new UniqueAgg(fp.parseArg());\n      }\n    });\n\n    addParser(\"agg_uniqueBlock\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new UniqueBlockAgg(fp.parseArg());\n      }\n    });\n\n    addParser(\"agg_hll\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new HLLAgg(fp.parseArg());\n      }\n    });\n\n    addParser(\"agg_sum\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new SumAgg(fp.parseValueSource());\n      }\n    });\n\n    addParser(\"agg_avg\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new AvgAgg(fp.parseValueSource());\n      }\n    });\n\n    addParser(\"agg_sumsq\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new SumsqAgg(fp.parseValueSource());\n      }\n    });\n\n    addParser(\"agg_variance\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new VarianceAgg(fp.parseValueSource());\n      }\n    });\n    \n    addParser(\"agg_stddev\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new StddevAgg(fp.parseValueSource());\n      }\n    });\n\n    addParser(\"agg_missing\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new MissingAgg(fp.parseValueSource(FunctionQParser.FLAG_DEFAULT | FunctionQParser.FLAG_USE_FIELDNAME_SOURCE));\n      }\n    });\n\n    addParser(\"agg_countvals\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new CountValsAgg(fp.parseValueSource(FunctionQParser.FLAG_DEFAULT | FunctionQParser.FLAG_USE_FIELDNAME_SOURCE));\n      }\n    });\n    \n    /***\n     addParser(\"agg_multistat\", new ValueSourceParser() {\n    @Override\n    public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n    return null;\n    }\n    });\n     ***/\n\n    addParser(\"agg_min\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new MinMaxAgg(\"min\", fp.parseValueSource(FunctionQParser.FLAG_DEFAULT | FunctionQParser.FLAG_USE_FIELDNAME_SOURCE));\n      }\n    });\n\n    addParser(\"agg_max\", new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        return new MinMaxAgg(\"max\", fp.parseValueSource(FunctionQParser.FLAG_DEFAULT | FunctionQParser.FLAG_USE_FIELDNAME_SOURCE));\n      }\n    });\n\n    addParser(\"agg_percentile\", new PercentileAgg.Parser());\n    \n    addParser(\"agg_\" + RelatednessAgg.NAME, new ValueSourceParser() {\n      @Override\n      public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n        // TODO: (fore & back)-ground should be optional -- use hasMoreArguments\n        // if only one arg, assume it's the foreground\n        // (background is the one that will most commonly just be \"*:*\")\n        // see notes in RelatednessAgg constructor about why we don't do this yet\n        RelatednessAgg agg = new RelatednessAgg(fp.parseNestedQuery(), fp.parseNestedQuery());\n        agg.setOpts(fp);\n        return agg;\n      }\n    });\n    \n    addParser(\"childfield\", new ChildFieldValueSourceParser());\n  }\n\n  ///////////////////////////////////////////////////////////////////////////////\n  ///////////////////////////////////////////////////////////////////////////////\n  ///////////////////////////////////////////////////////////////////////////////\n\n\n  private static TInfo parseTerm(FunctionQParser fp) throws SyntaxError {\n    TInfo tinfo = new TInfo();\n\n    tinfo.indexedField = tinfo.field = fp.parseArg();\n    tinfo.val = fp.parseArg();\n    tinfo.indexedBytes = new BytesRefBuilder();\n\n    FieldType ft = fp.getReq().getSchema().getFieldTypeNoEx(tinfo.field);\n    if (ft == null) ft = new StrField();\n\n    if (ft instanceof TextField) {\n      // need to do analysis on the term\n      String indexedVal = tinfo.val;\n      Query q = ft.getFieldQuery(fp, fp.getReq().getSchema().getFieldOrNull(tinfo.field), tinfo.val);\n      if (q instanceof TermQuery) {\n        Term term = ((TermQuery)q).getTerm();\n        tinfo.indexedField = term.field();\n        indexedVal = term.text();\n      }\n      tinfo.indexedBytes.copyChars(indexedVal);\n    } else {\n      ft.readableToIndexed(tinfo.val, tinfo.indexedBytes);\n    }\n\n    return tinfo;\n  }\n\n  private static void splitSources(int dim, List<ValueSource> sources, List<ValueSource> dest1, List<ValueSource> dest2) {\n    //Get dim value sources for the first vector\n    for (int i = 0; i < dim; i++) {\n      dest1.add(sources.get(i));\n    }\n    //Get dim value sources for the second vector\n    for (int i = dim; i < sources.size(); i++) {\n      dest2.add(sources.get(i));\n    }\n  }\n\n  private static MVResult getMultiValueSources(List<ValueSource> sources) {\n    MVResult mvr = new MVResult();\n    if (sources.size() % 2 != 0) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Illegal number of sources.  There must be an even number of sources\");\n    }\n    if (sources.size() == 2) {\n\n      //check to see if these are MultiValueSource\n      boolean s1MV = sources.get(0) instanceof MultiValueSource;\n      boolean s2MV = sources.get(1) instanceof MultiValueSource;\n      if (s1MV && s2MV) {\n        mvr.mv1 = (MultiValueSource) sources.get(0);\n        mvr.mv2 = (MultiValueSource) sources.get(1);\n      } else if (s1MV ||\n              s2MV) {\n        //if one is a MultiValueSource, than the other one needs to be too.\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Illegal number of sources.  There must be an even number of sources\");\n      } else {\n        mvr.mv1 = new VectorValueSource(Collections.singletonList(sources.get(0)));\n        mvr.mv2 = new VectorValueSource(Collections.singletonList(sources.get(1)));\n      }\n    } else {\n      int dim = sources.size() / 2;\n      List<ValueSource> sources1 = new ArrayList<>(dim);\n      List<ValueSource> sources2 = new ArrayList<>(dim);\n      //Get dim value sources for the first vector\n      splitSources(dim, sources, sources1, sources2);\n      mvr.mv1 = new VectorValueSource(sources1);\n      mvr.mv2 = new VectorValueSource(sources2);\n    }\n\n    return mvr;\n  }\n\n  private static class MVResult {\n    MultiValueSource mv1;\n    MultiValueSource mv2;\n  }\n\n  private static class TInfo {\n    String field;\n    String val;\n    String indexedField;\n    BytesRefBuilder indexedBytes;\n  }\n\n}\n\n\nclass DateValueSourceParser extends ValueSourceParser {\n  @Override\n  public void init(NamedList args) {\n  }\n\n  public Date getDate(FunctionQParser fp, String arg) {\n    if (arg == null) return null;\n    // check character index 1 to be a digit.  Index 0 might be a +/-.\n    if (arg.startsWith(\"NOW\") || (arg.length() > 1 && Character.isDigit(arg.charAt(1)))) {\n      Date now = null;//TODO pull from params?\n      return DateMathParser.parseMath(now, arg);\n    }\n    return null;\n  }\n\n  public ValueSource getValueSource(FunctionQParser fp, String arg) {\n    if (arg == null) return null;\n    SchemaField f = fp.req.getSchema().getField(arg);\n    return f.getType().getValueSource(f, fp);\n  }\n\n  @Override\n  public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n    String first = fp.parseArg();\n    String second = fp.parseArg();\n    if (first == null) first = \"NOW\";\n\n    Date d1 = getDate(fp, first);\n    ValueSource v1 = d1 == null ? getValueSource(fp, first) : null;\n\n    Date d2 = getDate(fp, second);\n    ValueSource v2 = d2 == null ? getValueSource(fp, second) : null;\n\n    // d     constant\n    // v     field\n    // dd    constant\n    // dv    subtract field from constant\n    // vd    subtract constant from field\n    // vv    subtract fields\n\n    final long ms1 = (d1 == null) ? 0 : d1.getTime();\n    final long ms2 = (d2 == null) ? 0 : d2.getTime();\n\n    // \"d,dd\" handle both constant cases\n\n    if (d1 != null && v2 == null) {\n      return new LongConstValueSource(ms1 - ms2);\n    }\n\n    // \"v\" just the date field\n    if (v1 != null && v2 == null && d2 == null) {\n      return v1;\n    }\n\n\n    // \"dv\"\n    if (d1 != null && v2 != null)\n      return new DualFloatFunction(new LongConstValueSource(ms1), v2) {\n        @Override\n        protected String name() {\n          return \"ms\";\n        }\n\n        @Override\n        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) throws IOException {\n          return ms1 - bVals.longVal(doc);\n        }\n      };\n\n    // \"vd\"\n    if (v1 != null && d2 != null)\n      return new DualFloatFunction(v1, new LongConstValueSource(ms2)) {\n        @Override\n        protected String name() {\n          return \"ms\";\n        }\n\n        @Override\n        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) throws IOException {\n          return aVals.longVal(doc) - ms2;\n        }\n      };\n\n    // \"vv\"\n    if (v1 != null && v2 != null)\n      return new DualFloatFunction(v1, v2) {\n        @Override\n        protected String name() {\n          return \"ms\";\n        }\n\n        @Override\n        protected float func(int doc, FunctionValues aVals, FunctionValues bVals) throws IOException {\n          return aVals.longVal(doc) - bVals.longVal(doc);\n        }\n      };\n\n    return null; // shouldn't happen\n  }\n\n}\n\n\n// Private for now - we need to revisit how to handle typing in function queries\nclass LongConstValueSource extends ConstNumberSource {\n  final long constant;\n  final double dv;\n  final float fv;\n\n  public LongConstValueSource(long constant) {\n    this.constant = constant;\n    this.dv = constant;\n    this.fv = constant;\n  }\n\n  @Override\n  public String description() {\n    return \"const(\" + constant + \")\";\n  }\n\n  @Override\n  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {\n    return new LongDocValues(this) {\n      @Override\n      public float floatVal(int doc) {\n        return fv;\n      }\n\n      @Override\n      public int intVal(int doc) {\n        return (int) constant;\n      }\n\n      @Override\n      public long longVal(int doc) {\n        return constant;\n      }\n\n      @Override\n      public double doubleVal(int doc) {\n        return dv;\n      }\n\n      @Override\n      public String toString(int doc) {\n        return description();\n      }\n    };\n  }\n\n  @Override\n  public int hashCode() {\n    return (int) constant + (int) (constant >>> 32);\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (LongConstValueSource.class != o.getClass()) return false;\n    LongConstValueSource other = (LongConstValueSource) o;\n    return this.constant == other.constant;\n  }\n\n  @Override\n  public int getInt() {\n    return (int)constant;\n  }\n\n  @Override\n  public long getLong() {\n    return constant;\n  }\n\n  @Override\n  public float getFloat() {\n    return fv;\n  }\n\n  @Override\n  public double getDouble() {\n    return dv;\n  }\n\n  @Override\n  public Number getNumber() {\n    return constant;\n  }\n\n  @Override\n  public boolean getBool() {\n    return constant != 0;\n  }\n}\n\n\nabstract class NamedParser extends ValueSourceParser {\n  private final String name;\n  public NamedParser(String name) {\n    this.name = name;\n  }\n  public String name() {\n    return name;\n  }\n}\n\n\nabstract class DoubleParser extends NamedParser {\n  public DoubleParser(String name) {\n    super(name);\n  }\n\n  public abstract double func(int doc, FunctionValues vals) throws IOException;\n\n  @Override\n  public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n    return new Function(fp.parseValueSource());\n  }\n\n  class Function extends SingleFunction {\n    public Function(ValueSource source) {\n      super(source);\n    }\n\n    @Override\n    public String name() {\n      return DoubleParser.this.name();\n    }\n\n    @Override\n    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {\n      final FunctionValues vals =  source.getValues(context, readerContext);\n      return new DoubleDocValues(this) {\n        @Override\n        public double doubleVal(int doc) throws IOException {\n          return func(doc, vals);\n        }\n        @Override\n        public String toString(int doc) throws IOException {\n          return name() + '(' + vals.toString(doc) + ')';\n        }\n      };\n    }\n  }\n}\n\n\nabstract class Double2Parser extends NamedParser {\n  public Double2Parser(String name) {\n    super(name);\n  }\n\n  public abstract double func(int doc, FunctionValues a, FunctionValues b) throws IOException;\n\n  @Override\n  public ValueSource parse(FunctionQParser fp) throws SyntaxError {\n    return new Function(fp.parseValueSource(), fp.parseValueSource());\n  }\n\n  class Function extends ValueSource {\n    private final ValueSource a;\n    private final ValueSource b;\n\n   /**\n     * @param   a  the base.\n     * @param   b  the exponent.\n     */\n    public Function(ValueSource a, ValueSource b) {\n      this.a = a;\n      this.b = b;\n    }\n\n    @Override\n    public String description() {\n      return name() + \"(\" + a.description() + \",\" + b.description() + \")\";\n    }\n\n    @Override\n    public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {\n      final FunctionValues aVals =  a.getValues(context, readerContext);\n      final FunctionValues bVals =  b.getValues(context, readerContext);\n      return new DoubleDocValues(this) {\n        @Override\n        public double doubleVal(int doc) throws IOException {\n          return func(doc, aVals, bVals);\n        }\n        @Override\n        public String toString(int doc) throws IOException {\n          return name() + '(' + aVals.toString(doc) + ',' + bVals.toString(doc) + ')';\n        }\n      };\n    }\n\n    @Override\n    public void createWeight(Map context, IndexSearcher searcher) throws IOException {\n    }\n\n    @Override\n    public int hashCode() {\n      int h = a.hashCode();\n      h ^= (h << 13) | (h >>> 20);\n      h += b.hashCode();\n      h ^= (h << 23) | (h >>> 10);\n      h += name().hashCode();\n      return h;\n    }\n\n    @Override\n    public boolean equals(Object o) {\n      if (this.getClass() != o.getClass()) return false;\n      Function other = (Function)o;\n      return this.a.equals(other.a)\n          && this.b.equals(other.b);\n    }\n  }\n\n}\n\n\nclass BoolConstValueSource extends ConstNumberSource {\n  final boolean constant;\n\n  public BoolConstValueSource(boolean constant) {\n    this.constant = constant;\n  }\n\n  @Override\n  public String description() {\n    return \"const(\" + constant + \")\";\n  }\n\n  @Override\n  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {\n    return new BoolDocValues(this) {\n      @Override\n      public boolean boolVal(int doc) {\n        return constant;\n      }\n    };\n  }\n\n  @Override\n  public int hashCode() {\n    return constant ? 0x12345678 : 0x87654321;\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    if (BoolConstValueSource.class != o.getClass()) return false;\n    BoolConstValueSource other = (BoolConstValueSource) o;\n    return this.constant == other.constant;\n  }\n\n  @Override\n  public int getInt() {\n    return constant ? 1 : 0;\n  }\n\n  @Override\n  public long getLong() {\n    return constant ? 1 : 0;\n  }\n\n  @Override\n  public float getFloat() {\n    return constant ? 1 : 0;\n  }\n\n  @Override\n  public double getDouble() {\n    return constant ? 1 : 0;\n  }\n\n  @Override\n  public Number getNumber() {\n    return constant ? 1 : 0;\n  }\n\n  @Override\n  public boolean getBool() {\n    return constant;\n  }\n}\n\n\nclass TestValueSource extends ValueSource {\n  ValueSource source;\n  \n  public TestValueSource(ValueSource source) {\n    this.source = source;\n  }\n  \n  @Override\n  public FunctionValues getValues(Map context, LeafReaderContext readerContext) throws IOException {\n    if (context.get(this) == null) {\n      SolrRequestInfo requestInfo = SolrRequestInfo.getRequestInfo();\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"testfunc: unweighted value source detected.  delegate=\"+source + \" request=\" + (requestInfo==null ? \"null\" : requestInfo.getReq()));\n    }\n    return source.getValues(context, readerContext);\n  }\n\n  @Override\n  public boolean equals(Object o) {\n    return o instanceof TestValueSource && source.equals(((TestValueSource)o).source);\n  }\n\n  @Override\n  public int hashCode() {\n    return source.hashCode() + TestValueSource.class.hashCode();\n  }\n\n  @Override\n  public String description() {\n    return \"testfunc(\" + source.description() + ')';\n  }\n\n  @Override\n  public void createWeight(Map context, IndexSearcher searcher) throws IOException {\n    context.put(this, this);\n  }\n\n  @Override\n  public SortField getSortField(boolean reverse) {\n    return super.getSortField(reverse);\n  }\n}\n", "idx": 1, "id": 31269, "msg": "Nitpick: turn into terciary operator.", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -551,6 +551,12 @@ func (engine *DockerStatsEngine) getTaskHealthUnsafe(taskARN string) *ecstcs.Tas\n \t\t\tHealthStatus:  aws.String(healthInfo.Status.BackendStatus()),\n \t\t\tStatusSince:   aws.Time(healthInfo.Since.UTC()),\n \t\t}\n+\n+\t\tstatusMessage, isNil := engine.getContainerStatusMessage(dockerContainer, taskARN)\n+\t\tif !isNil {\n+\t\t\tcontainerHealth.StatusMessage = aws.String(statusMessage)\n+\t\t}\n+\n \t\tcontainerHealths = append(containerHealths, containerHealth)\n \t}\n ", "y": 0, "oldf": "// Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n// not use this file except in compliance with the License. A copy of the\n// License is located at\n//\n//\thttp://aws.amazon.com/apache2.0/\n//\n// or in the \"license\" file accompanying this file. This file is distributed\n// on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n// express or implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage stats\n\n//go:generate mockgen -destination=mock/$GOFILE -copyright_file=../../scripts/copyright_file github.com/aws/amazon-ecs-agent/agent/stats Engine\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cihub/seelog\"\n\t\"github.com/pborman/uuid\"\n\t\"github.com/pkg/errors\"\n\n\tapicontainer \"github.com/aws/amazon-ecs-agent/agent/api/container\"\n\tapicontainerstatus \"github.com/aws/amazon-ecs-agent/agent/api/container/status\"\n\tapitask \"github.com/aws/amazon-ecs-agent/agent/api/task\"\n\t\"github.com/aws/amazon-ecs-agent/agent/config\"\n\t\"github.com/aws/amazon-ecs-agent/agent/dockerclient\"\n\t\"github.com/aws/amazon-ecs-agent/agent/dockerclient/dockerapi\"\n\tecsengine \"github.com/aws/amazon-ecs-agent/agent/engine\"\n\t\"github.com/aws/amazon-ecs-agent/agent/eventstream\"\n\t\"github.com/aws/amazon-ecs-agent/agent/stats/resolver\"\n\t\"github.com/aws/amazon-ecs-agent/agent/tcs/model/ecstcs\"\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/docker/docker/api/types\"\n)\n\nconst (\n\tcontainerChangeHandler = \"DockerStatsEngineDockerEventsHandler\"\n\tqueueResetThreshold    = 2 * dockerclient.StatsInactivityTimeout\n\thostNetworkMode        = \"host\"\n\tnoneNetworkMode        = \"none\"\n)\n\nvar (\n\t// EmptyMetricsError indicates an error for a task when there are no container\n\t// metrics to report\n\tEmptyMetricsError = errors.New(\"stats engine: no task metrics to report\")\n\t// EmptyHealthMetricsError indicates an error for a task when there are no container\n\t// health metrics to report\n\tEmptyHealthMetricsError = errors.New(\"stats engine: no task health metrics to report\")\n)\n\n// DockerContainerMetadataResolver implements ContainerMetadataResolver for\n// DockerTaskEngine.\ntype DockerContainerMetadataResolver struct {\n\tdockerTaskEngine *ecsengine.DockerTaskEngine\n}\n\n// Engine defines methods to be implemented by the engine struct. It is\n// defined to make testing easier.\ntype Engine interface {\n\tGetInstanceMetrics() (*ecstcs.MetricsMetadata, []*ecstcs.TaskMetric, error)\n\tContainerDockerStats(taskARN string, containerID string) (*types.StatsJSON, *NetworkStatsPerSec, error)\n\tGetTaskHealthMetrics() (*ecstcs.HealthMetadata, []*ecstcs.TaskHealth, error)\n}\n\n// DockerStatsEngine is used to monitor docker container events and to report\n// utilization metrics of the same.\n\ntype DockerStatsEngine struct {\n\tctx                        context.Context\n\tstopEngine                 context.CancelFunc\n\tclient                     dockerapi.DockerClient\n\tcluster                    string\n\tcontainerInstanceArn       string\n\tlock                       sync.RWMutex\n\tconfig                     *config.Config\n\tcontainerChangeEventStream *eventstream.EventStream\n\tresolver                   resolver.ContainerMetadataResolver\n\t// tasksToContainers maps task arns to a map of container ids to StatsContainer objects.\n\ttasksToContainers map[string]map[string]*StatsContainer\n\t// tasksToHealthCheckContainers map task arns to the containers that has health check enabled\n\ttasksToHealthCheckContainers map[string]map[string]*StatsContainer\n\t// tasksToDefinitions maps task arns to task definition name and family metadata objects.\n\ttasksToDefinitions map[string]*taskDefinition\n\ttaskToTaskStats    map[string]*StatsTask\n}\n\n// ResolveTask resolves the api task object, given container id.\nfunc (resolver *DockerContainerMetadataResolver) ResolveTask(dockerID string) (*apitask.Task, error) {\n\tif resolver.dockerTaskEngine == nil {\n\t\treturn nil, fmt.Errorf(\"Docker task engine uninitialized\")\n\t}\n\ttask, found := resolver.dockerTaskEngine.State().TaskByID(dockerID)\n\tif !found {\n\t\treturn nil, fmt.Errorf(\"Could not map docker id to task: %s\", dockerID)\n\t}\n\n\treturn task, nil\n}\n\nfunc (resolver *DockerContainerMetadataResolver) ResolveTaskByARN(taskArn string) (*apitask.Task, error) {\n\tif resolver.dockerTaskEngine == nil {\n\t\treturn nil, fmt.Errorf(\"docker task engine uninitialized\")\n\t}\n\ttask, found := resolver.dockerTaskEngine.State().TaskByArn(taskArn)\n\tif !found {\n\t\treturn nil, fmt.Errorf(\"could not map task arn to task: %s\", taskArn)\n\t}\n\treturn task, nil\n\n}\n\n// ResolveContainer resolves the api container object, given container id.\nfunc (resolver *DockerContainerMetadataResolver) ResolveContainer(dockerID string) (*apicontainer.DockerContainer, error) {\n\tif resolver.dockerTaskEngine == nil {\n\t\treturn nil, fmt.Errorf(\"Docker task engine uninitialized\")\n\t}\n\tcontainer, found := resolver.dockerTaskEngine.State().ContainerByID(dockerID)\n\tif !found {\n\t\treturn nil, fmt.Errorf(\"Could not map docker id to container: %s\", dockerID)\n\t}\n\n\treturn container, nil\n}\n\n// NewDockerStatsEngine creates a new instance of the DockerStatsEngine object.\n// MustInit() must be called to initialize the fields of the new event listener.\nfunc NewDockerStatsEngine(cfg *config.Config, client dockerapi.DockerClient, containerChangeEventStream *eventstream.EventStream) *DockerStatsEngine {\n\treturn &DockerStatsEngine{\n\t\tclient:                       client,\n\t\tresolver:                     nil,\n\t\tconfig:                       cfg,\n\t\ttasksToContainers:            make(map[string]map[string]*StatsContainer),\n\t\ttasksToHealthCheckContainers: make(map[string]map[string]*StatsContainer),\n\t\ttasksToDefinitions:           make(map[string]*taskDefinition),\n\t\ttaskToTaskStats:              make(map[string]*StatsTask),\n\t\tcontainerChangeEventStream:   containerChangeEventStream,\n\t}\n}\n\n// synchronizeState goes through all the containers on the instance to synchronize the state on agent start\nfunc (engine *DockerStatsEngine) synchronizeState() error {\n\tlistContainersResponse := engine.client.ListContainers(engine.ctx, false, dockerclient.ListContainersTimeout)\n\tif listContainersResponse.Error != nil {\n\t\treturn listContainersResponse.Error\n\t}\n\n\tfor _, containerID := range listContainersResponse.DockerIDs {\n\t\tengine.addAndStartStatsContainer(containerID)\n\t}\n\n\treturn nil\n}\n\n// addAndStartStatsContainer add the container into stats engine and start collecting the container stats\nfunc (engine *DockerStatsEngine) addAndStartStatsContainer(containerID string) {\n\tengine.lock.Lock()\n\tdefer engine.lock.Unlock()\n\tstatsContainer, statsTaskContainer, err := engine.addContainerUnsafe(containerID)\n\tif err != nil {\n\t\tseelog.Debugf(\"Adding container to stats watch list failed, container: %s, err: %v\", containerID, err)\n\t\treturn\n\t}\n\n\tif engine.config.DisableMetrics.Enabled() || statsContainer == nil {\n\t\treturn\n\t}\n\n\tstatsContainer.StartStatsCollection()\n\n\ttask, err := engine.resolver.ResolveTask(containerID)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tdockerContainer, errResolveContainer := engine.resolver.ResolveContainer(containerID)\n\tif errResolveContainer != nil {\n\t\tseelog.Debugf(\"Could not map container ID to container, container: %s, err: %s\", containerID, err)\n\t\treturn\n\t}\n\n\tif task.IsNetworkModeAWSVPC() {\n\t\t// Start stats collector only for pause container\n\t\tif statsTaskContainer != nil && dockerContainer.Container.Type == apicontainer.ContainerCNIPause {\n\t\t\tstatsTaskContainer.StartStatsCollection()\n\t\t} else {\n\t\t\tseelog.Debugf(\"stats task container is nil, cannot start task stats collection\")\n\t\t}\n\t}\n\n}\n\n// MustInit initializes fields of the DockerStatsEngine object.\nfunc (engine *DockerStatsEngine) MustInit(ctx context.Context, taskEngine ecsengine.TaskEngine, cluster string, containerInstanceArn string) error {\n\tderivedCtx, cancel := context.WithCancel(ctx)\n\tengine.stopEngine = cancel\n\n\tengine.ctx = derivedCtx\n\t// TODO ensure that this is done only once per engine object\n\tseelog.Info(\"Initializing stats engine\")\n\tengine.cluster = cluster\n\tengine.containerInstanceArn = containerInstanceArn\n\n\tvar err error\n\tengine.resolver, err = newDockerContainerMetadataResolver(taskEngine)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Subscribe to the container change event stream\n\terr = engine.containerChangeEventStream.Subscribe(containerChangeHandler, engine.handleDockerEvents)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"Failed to subscribe to container change event stream, err %v\", err)\n\t}\n\terr = engine.synchronizeState()\n\tif err != nil {\n\t\tseelog.Warnf(\"Synchronize the container state failed, err: %v\", err)\n\t}\n\n\tgo engine.waitToStop()\n\treturn nil\n}\n\n// Shutdown cleans up the resources after the stats engine.\nfunc (engine *DockerStatsEngine) Shutdown() {\n\tengine.stopEngine()\n\tengine.Disable()\n}\n\n// Disable prevents this engine from managing any additional tasks.\nfunc (engine *DockerStatsEngine) Disable() {\n\tengine.lock.Lock()\n}\n\n// waitToStop waits for the container change event stream close ans stop collection metrics\nfunc (engine *DockerStatsEngine) waitToStop() {\n\t// Waiting for the event stream to close\n\t<-engine.containerChangeEventStream.Context().Done()\n\tseelog.Debug(\"Event stream closed, stop listening to the event stream\")\n\tengine.containerChangeEventStream.Unsubscribe(containerChangeHandler)\n\tengine.removeAll()\n}\n\n// removeAll stops the periodic usage data collection for all containers\nfunc (engine *DockerStatsEngine) removeAll() {\n\tengine.lock.Lock()\n\tdefer engine.lock.Unlock()\n\n\tfor task, containers := range engine.tasksToContainers {\n\t\tfor _, statsContainer := range containers {\n\t\t\tstatsContainer.StopStatsCollection()\n\t\t}\n\t\tdelete(engine.tasksToContainers, task)\n\t}\n\n\tfor task := range engine.tasksToHealthCheckContainers {\n\t\tdelete(engine.tasksToContainers, task)\n\t}\n}\n\nfunc (engine *DockerStatsEngine) addToStatsTaskMapUnsafe(task *apitask.Task, dockerContainerName string,\n\tcontainerType apicontainer.ContainerType) {\n\tvar statsTaskContainer *StatsTask\n\tif task.IsNetworkModeAWSVPC() && containerType == apicontainer.ContainerCNIPause {\n\t\t// Excluding the pause container\n\t\tnumberOfContainers := len(task.Containers) - 1\n\t\tvar taskExists bool\n\t\tstatsTaskContainer, taskExists = engine.taskToTaskStats[task.Arn]\n\t\tif !taskExists {\n\t\t\tcontainerInspect, err := engine.client.InspectContainer(engine.ctx, dockerContainerName,\n\t\t\t\tdockerclient.InspectContainerTimeout)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcontainerpid := strconv.Itoa(containerInspect.State.Pid)\n\t\t\tstatsTaskContainer, err = newStatsTaskContainer(task.Arn, containerpid, numberOfContainers,\n\t\t\t\tengine.resolver, engine.config.PollingMetricsWaitDuration)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tengine.taskToTaskStats[task.Arn] = statsTaskContainer\n\t\t} else {\n\t\t\tstatsTaskContainer.TaskMetadata.NumberContainers = numberOfContainers\n\t\t}\n\t}\n}\n\n// addContainerUnsafe adds a container to the map of containers being watched.\nfunc (engine *DockerStatsEngine) addContainerUnsafe(dockerID string) (*StatsContainer, *StatsTask, error) {\n\t// Make sure that this container belongs to a task and that the task\n\t// is not terminal.\n\ttask, err := engine.resolver.ResolveTask(dockerID)\n\tif err != nil {\n\t\treturn nil, nil, errors.Wrapf(err, \"could not map container to task, ignoring container: %s\", dockerID)\n\t}\n\n\tif len(task.Arn) == 0 || len(task.Family) == 0 {\n\t\treturn nil, nil, errors.Errorf(\"stats add container: invalid task fields, arn: %s, familiy: %s\", task.Arn, task.Family)\n\t}\n\n\tif task.GetKnownStatus().Terminal() {\n\t\treturn nil, nil, errors.Errorf(\"stats add container: task is terminal, ignoring container: %s, task: %s\", dockerID, task.Arn)\n\t}\n\n\tstatsContainer, err := newStatsContainer(dockerID, engine.client, engine.resolver, engine.config)\n\tif err != nil {\n\t\treturn nil, nil, errors.Wrapf(err, \"could not map docker container ID to container, ignoring container: %s\", dockerID)\n\t}\n\n\tseelog.Debugf(\"Adding container to stats watch list, id: %s, task: %s\", dockerID, task.Arn)\n\tengine.tasksToDefinitions[task.Arn] = &taskDefinition{family: task.Family, version: task.Version}\n\n\tdockerContainer, errResolveContainer := engine.resolver.ResolveContainer(dockerID)\n\tif errResolveContainer != nil {\n\t\tseelog.Debugf(\"Could not map container ID to container, container: %s, err: %s\", dockerID, err)\n\t}\n\n\twatchStatsContainer := false\n\tif !engine.config.DisableMetrics.Enabled() {\n\t\t// Adding container to the map for collecting stats\n\t\twatchStatsContainer = engine.addToStatsContainerMapUnsafe(task.Arn, dockerID, statsContainer,\n\t\t\tengine.containerMetricsMapUnsafe)\n\t\tif errResolveContainer == nil {\n\t\t\tengine.addToStatsTaskMapUnsafe(task, dockerContainer.DockerName, dockerContainer.Container.Type)\n\t\t}\n\t}\n\n\tif errResolveContainer == nil && dockerContainer.Container.HealthStatusShouldBeReported() {\n\t\t// Track the container health status\n\t\tengine.addToStatsContainerMapUnsafe(task.Arn, dockerID, statsContainer, engine.healthCheckContainerMapUnsafe)\n\t\tseelog.Debugf(\"Adding container to stats health check watch list, id: %s, task: %s\", dockerID, task.Arn)\n\t}\n\n\tif !watchStatsContainer {\n\t\treturn nil, nil, nil\n\t}\n\treturn statsContainer, engine.taskToTaskStats[task.Arn], nil\n}\n\nfunc (engine *DockerStatsEngine) containerMetricsMapUnsafe() map[string]map[string]*StatsContainer {\n\treturn engine.tasksToContainers\n}\n\nfunc (engine *DockerStatsEngine) healthCheckContainerMapUnsafe() map[string]map[string]*StatsContainer {\n\treturn engine.tasksToHealthCheckContainers\n}\n\n// addToStatsContainerMapUnsafe adds the statscontainer into stats for tracking and returns a boolean indicates\n// whether this container should be tracked for collecting metrics\nfunc (engine *DockerStatsEngine) addToStatsContainerMapUnsafe(\n\ttaskARN, containerID string,\n\tstatsContainer *StatsContainer,\n\tstatsMapToUpdate func() map[string]map[string]*StatsContainer) bool {\n\n\ttaskToContainerMap := statsMapToUpdate()\n\n\t// Check if this container is already being watched.\n\t_, taskExists := taskToContainerMap[taskARN]\n\tif taskExists {\n\t\t// task arn exists in map.\n\t\t_, containerExists := taskToContainerMap[taskARN][containerID]\n\t\tif containerExists {\n\t\t\t// container arn exists in map.\n\t\t\tseelog.Debugf(\"Container already being watched, ignoring, id: %s\", containerID)\n\t\t\treturn false\n\t\t}\n\t} else {\n\t\t// Create a map for the task arn if it doesn't exist yet.\n\t\ttaskToContainerMap[taskARN] = make(map[string]*StatsContainer)\n\t}\n\ttaskToContainerMap[taskARN][containerID] = statsContainer\n\n\treturn true\n}\n\n// GetInstanceMetrics gets all task metrics and instance metadata from stats engine.\nfunc (engine *DockerStatsEngine) GetInstanceMetrics() (*ecstcs.MetricsMetadata, []*ecstcs.TaskMetric, error) {\n\tvar taskMetrics []*ecstcs.TaskMetric\n\tidle := engine.isIdle()\n\tmetricsMetadata := &ecstcs.MetricsMetadata{\n\t\tCluster:           aws.String(engine.cluster),\n\t\tContainerInstance: aws.String(engine.containerInstanceArn),\n\t\tIdle:              aws.Bool(idle),\n\t\tMessageId:         aws.String(uuid.NewRandom().String()),\n\t}\n\n\tif idle {\n\t\tseelog.Debug(\"Instance is idle. No task metrics to report\")\n\t\tfin := true\n\t\tmetricsMetadata.Fin = &fin\n\t\treturn metricsMetadata, taskMetrics, nil\n\t}\n\n\tengine.lock.Lock()\n\tdefer engine.lock.Unlock()\n\n\tfor taskArn := range engine.tasksToContainers {\n\t\tcontainerMetrics, err := engine.taskContainerMetricsUnsafe(taskArn)\n\t\tif err != nil {\n\t\t\tseelog.Debugf(\"Error getting container metrics for task: %s, err: %v\", taskArn, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(containerMetrics) == 0 {\n\t\t\tseelog.Debugf(\"Empty containerMetrics for task, ignoring, task: %s\", taskArn)\n\t\t\tcontinue\n\t\t}\n\n\t\ttaskDef, exists := engine.tasksToDefinitions[taskArn]\n\t\tif !exists {\n\t\t\tseelog.Debugf(\"Could not map task to definition, task: %s\", taskArn)\n\t\t\tcontinue\n\t\t}\n\n\t\tmetricTaskArn := taskArn\n\t\ttaskMetric := &ecstcs.TaskMetric{\n\t\t\tTaskArn:               &metricTaskArn,\n\t\t\tTaskDefinitionFamily:  &taskDef.family,\n\t\t\tTaskDefinitionVersion: &taskDef.version,\n\t\t\tContainerMetrics:      containerMetrics,\n\t\t}\n\t\ttaskMetrics = append(taskMetrics, taskMetric)\n\t}\n\n\tif len(taskMetrics) == 0 {\n\t\t// Not idle. Expect taskMetrics to be there.\n\t\treturn nil, nil, EmptyMetricsError\n\t}\n\n\tengine.resetStatsUnsafe()\n\treturn metricsMetadata, taskMetrics, nil\n}\n\n// GetTaskHealthMetrics returns the container health metrics\nfunc (engine *DockerStatsEngine) GetTaskHealthMetrics() (*ecstcs.HealthMetadata, []*ecstcs.TaskHealth, error) {\n\tvar taskHealths []*ecstcs.TaskHealth\n\tmetadata := &ecstcs.HealthMetadata{\n\t\tCluster:           aws.String(engine.cluster),\n\t\tContainerInstance: aws.String(engine.containerInstanceArn),\n\t\tMessageId:         aws.String(uuid.NewRandom().String()),\n\t}\n\n\tif !engine.containerHealthsToMonitor() {\n\t\treturn metadata, taskHealths, nil\n\t}\n\n\tengine.lock.RLock()\n\tdefer engine.lock.RUnlock()\n\n\tfor taskARN := range engine.tasksToHealthCheckContainers {\n\t\ttaskHealth := engine.getTaskHealthUnsafe(taskARN)\n\t\tif taskHealth == nil {\n\t\t\tcontinue\n\t\t}\n\t\ttaskHealths = append(taskHealths, taskHealth)\n\t}\n\n\tif len(taskHealths) == 0 {\n\t\treturn nil, nil, EmptyHealthMetricsError\n\t}\n\n\treturn metadata, taskHealths, nil\n}\n\nfunc (engine *DockerStatsEngine) isIdle() bool {\n\tengine.lock.RLock()\n\tdefer engine.lock.RUnlock()\n\n\treturn len(engine.tasksToContainers) == 0\n}\n\nfunc (engine *DockerStatsEngine) containerHealthsToMonitor() bool {\n\tengine.lock.RLock()\n\tdefer engine.lock.RUnlock()\n\n\treturn len(engine.tasksToHealthCheckContainers) != 0\n}\n\n// stopTrackingContainerUnsafe removes the StatsContainer from stats engine and\n// returns true if the container is stopped or no longer tracked in agent. Otherwise\n// it does nothing and return false\nfunc (engine *DockerStatsEngine) stopTrackingContainerUnsafe(container *StatsContainer, taskARN string) bool {\n\tterminal, err := container.terminal()\n\tif err != nil {\n\t\t// Error determining if the container is terminal. This means that the container\n\t\t// id could not be resolved to a container that is being tracked by the\n\t\t// docker task engine. If the docker task engine has already removed\n\t\t// the container from its state, there's no point in stats engine tracking the\n\t\t// container. So, clean-up anyway.\n\t\tseelog.Warnf(\"Error determining if the container %s is terminal, removing from stats, err: %v\", container.containerMetadata.DockerID, err)\n\t\tengine.doRemoveContainerUnsafe(container, taskARN)\n\t\treturn true\n\t}\n\tif terminal {\n\t\t// Container is in known terminal state. Stop collection metrics.\n\t\tseelog.Infof(\"Container %s is terminal, removing from stats\", container.containerMetadata.DockerID)\n\t\tengine.doRemoveContainerUnsafe(container, taskARN)\n\t\treturn true\n\t}\n\n\treturn false\n}\n\nfunc (engine *DockerStatsEngine) getTaskHealthUnsafe(taskARN string) *ecstcs.TaskHealth {\n\t// Acquire the task definition information\n\ttaskDefinition, ok := engine.tasksToDefinitions[taskARN]\n\tif !ok {\n\t\tseelog.Debugf(\"Could not map task to definitions, task: %s\", taskARN)\n\t\treturn nil\n\t}\n\t// Check all the stats container for the task\n\tcontainers, ok := engine.tasksToHealthCheckContainers[taskARN]\n\tif !ok {\n\t\tseelog.Debugf(\"Could not map task to health containers, task: %s\", taskARN)\n\t\treturn nil\n\t}\n\n\t// Aggregate container health information for all the containers in the task\n\tvar containerHealths []*ecstcs.ContainerHealth\n\tfor _, container := range containers {\n\t\t// check if the container is stopped/untracked, and remove it from stats\n\t\t//engine if needed\n\t\tif engine.stopTrackingContainerUnsafe(container, taskARN) {\n\t\t\tcontinue\n\t\t}\n\t\tdockerContainer, err := engine.resolver.ResolveContainer(container.containerMetadata.DockerID)\n\t\tif err != nil {\n\t\t\tseelog.Debugf(\"Could not resolve the Docker ID in agent state: %s\", container.containerMetadata.DockerID)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if the container has health check enabled\n\t\tif !dockerContainer.Container.HealthStatusShouldBeReported() {\n\t\t\tcontinue\n\t\t}\n\n\t\thealthInfo := dockerContainer.Container.GetHealthStatus()\n\t\tif healthInfo.Since == nil {\n\t\t\t// container was started but the health status isn't ready\n\t\t\thealthInfo.Since = aws.Time(time.Now())\n\t\t}\n\t\tcontainerHealth := &ecstcs.ContainerHealth{\n\t\t\tContainerName: aws.String(dockerContainer.Container.Name),\n\t\t\tHealthStatus:  aws.String(healthInfo.Status.BackendStatus()),\n\t\t\tStatusSince:   aws.Time(healthInfo.Since.UTC()),\n\t\t}\n\t\tcontainerHealths = append(containerHealths, containerHealth)\n\t}\n\n\tif len(containerHealths) == 0 {\n\t\treturn nil\n\t}\n\n\ttaskHealth := &ecstcs.TaskHealth{\n\t\tContainers:            containerHealths,\n\t\tTaskArn:               aws.String(taskARN),\n\t\tTaskDefinitionFamily:  aws.String(taskDefinition.family),\n\t\tTaskDefinitionVersion: aws.String(taskDefinition.version),\n\t}\n\n\treturn taskHealth\n}\n\n// handleDockerEvents must be called after openEventstream; it processes each\n// event that it reads from the docker event stream.\nfunc (engine *DockerStatsEngine) handleDockerEvents(events ...interface{}) error {\n\tfor _, event := range events {\n\t\tdockerContainerChangeEvent, ok := event.(dockerapi.DockerContainerChangeEvent)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"Unexpected event received, expected docker container change event\")\n\t\t}\n\n\t\tswitch dockerContainerChangeEvent.Status {\n\t\tcase apicontainerstatus.ContainerRunning:\n\t\t\tengine.addAndStartStatsContainer(dockerContainerChangeEvent.DockerID)\n\t\tcase apicontainerstatus.ContainerStopped:\n\t\t\tengine.removeContainer(dockerContainerChangeEvent.DockerID)\n\t\tdefault:\n\t\t\tseelog.Debugf(\"Ignoring event for container, id: %s, status: %d\", dockerContainerChangeEvent.DockerID, dockerContainerChangeEvent.Status)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// removeContainer deletes the container from the map of containers being watched.\n// It also stops the periodic usage data collection for the container.\nfunc (engine *DockerStatsEngine) removeContainer(dockerID string) {\n\tengine.lock.Lock()\n\tdefer engine.lock.Unlock()\n\n\t// Make sure that this container belongs to a task.\n\ttask, err := engine.resolver.ResolveTask(dockerID)\n\tif err != nil {\n\t\tseelog.Debugf(\"Could not map container to task, ignoring, err: %v, id: %s\", err, dockerID)\n\t\treturn\n\t}\n\n\t_, taskExists := engine.tasksToContainers[task.Arn]\n\tif !taskExists {\n\t\tseelog.Debugf(\"Container not being watched, id: %s\", dockerID)\n\t\treturn\n\t}\n\n\t// task arn exists in map.\n\tcontainer, containerExists := engine.tasksToContainers[task.Arn][dockerID]\n\tif !containerExists {\n\t\t// container arn does not exist in map.\n\t\tseelog.Debugf(\"Container not being watched, id: %s\", dockerID)\n\t\treturn\n\t}\n\n\tengine.doRemoveContainerUnsafe(container, task.Arn)\n}\n\n// newDockerContainerMetadataResolver returns a new instance of DockerContainerMetadataResolver.\nfunc newDockerContainerMetadataResolver(taskEngine ecsengine.TaskEngine) (*DockerContainerMetadataResolver, error) {\n\tdockerTaskEngine, ok := taskEngine.(*ecsengine.DockerTaskEngine)\n\tif !ok {\n\t\t// Error type casting docker task engine.\n\t\treturn nil, fmt.Errorf(\"Could not load docker task engine\")\n\t}\n\n\tresolver := &DockerContainerMetadataResolver{\n\t\tdockerTaskEngine: dockerTaskEngine,\n\t}\n\n\treturn resolver, nil\n}\n\n// taskContainerMetricsUnsafe gets all container metrics for a task arn.\nfunc (engine *DockerStatsEngine) taskContainerMetricsUnsafe(taskArn string) ([]*ecstcs.ContainerMetric, error) {\n\tcontainerMap, taskExists := engine.tasksToContainers[taskArn]\n\tif !taskExists {\n\t\treturn nil, fmt.Errorf(\"task not found\")\n\t}\n\n\tvar containerMetrics []*ecstcs.ContainerMetric\n\tfor _, container := range containerMap {\n\t\tdockerID := container.containerMetadata.DockerID\n\t\t// Check if the container is terminal. If it is, make sure that it is\n\t\t// cleaned up properly. We might sometimes miss events from docker task\n\t\t// engine and this helps in reconciling the state. The tcs client's\n\t\t// GetInstanceMetrics probe is used as the trigger for this.\n\t\tif engine.stopTrackingContainerUnsafe(container, taskArn) {\n\t\t\tcontinue\n\t\t}\n\n\t\t// CPU and Memory are both critical, so skip the container if either of these fail.\n\t\tcpuStatsSet, err := container.statsQueue.GetCPUStatsSet()\n\t\tif err != nil {\n\t\t\tseelog.Infof(\"cloudwatch metrics for container %v not collected, reason (cpu): %v\", dockerID, err)\n\t\t\tcontinue\n\t\t}\n\t\tmemoryStatsSet, err := container.statsQueue.GetMemoryStatsSet()\n\t\tif err != nil {\n\t\t\tseelog.Infof(\"cloudwatch metrics for container %v not collected, reason (memory): %v\", dockerID, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tcontainerMetric := &ecstcs.ContainerMetric{\n\t\t\tContainerName:  &container.containerMetadata.Name,\n\t\t\tCpuStatsSet:    cpuStatsSet,\n\t\t\tMemoryStatsSet: memoryStatsSet,\n\t\t}\n\n\t\tstorageStatsSet, err := container.statsQueue.GetStorageStatsSet()\n\t\tif err != nil {\n\t\t\tseelog.Warnf(\"Error getting storage stats, err: %v, container: %v\", err, dockerID)\n\t\t} else {\n\t\t\tcontainerMetric.StorageStatsSet = storageStatsSet\n\t\t}\n\n\t\ttask, err := engine.resolver.ResolveTask(dockerID)\n\t\tif err != nil {\n\t\t\tseelog.Warnf(\"Task not found for container ID: %s\", dockerID)\n\t\t} else {\n\t\t\t// send network stats for default/bridge/nat/awsvpc network modes\n\t\t\tif !task.IsNetworkModeAWSVPC() && container.containerMetadata.NetworkMode != hostNetworkMode &&\n\t\t\t\tcontainer.containerMetadata.NetworkMode != noneNetworkMode {\n\t\t\t\tnetworkStatsSet, err := container.statsQueue.GetNetworkStatsSet()\n\t\t\t\tif err != nil {\n\t\t\t\t\t// we log the error and still continue to publish cpu, memory stats\n\t\t\t\t\tseelog.Warnf(\"Error getting network stats: %v, container: %v\", err, dockerID)\n\t\t\t\t} else {\n\t\t\t\t\tcontainerMetric.NetworkStatsSet = networkStatsSet\n\t\t\t\t}\n\t\t\t} else if task.IsNetworkModeAWSVPC() {\n\t\t\t\ttaskStatsMap, taskExistsInTaskStats := engine.taskToTaskStats[taskArn]\n\t\t\t\tif !taskExistsInTaskStats {\n\t\t\t\t\treturn nil, fmt.Errorf(\"task not found\")\n\t\t\t\t}\n\t\t\t\tif dockerContainer, err := engine.resolver.ResolveContainer(dockerID); err != nil {\n\t\t\t\t\tseelog.Debugf(\"Could not map container ID to container, container: %s, err: %s\", dockerID, err)\n\t\t\t\t} else {\n\t\t\t\t\t// do not add network stats for pause container\n\t\t\t\t\tif dockerContainer.Container.Type != apicontainer.ContainerCNIPause {\n\t\t\t\t\t\tnetworkStats, err := taskStatsMap.StatsQueue.GetNetworkStatsSet()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tseelog.Warnf(\"error getting network stats: %v, task: %v\", err, taskArn)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tcontainerMetric.NetworkStatsSet = networkStats\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcontainerMetrics = append(containerMetrics, containerMetric)\n\t}\n\treturn containerMetrics, nil\n}\n\nfunc (engine *DockerStatsEngine) doRemoveContainerUnsafe(container *StatsContainer, taskArn string) {\n\tcontainer.StopStatsCollection()\n\tdockerID := container.containerMetadata.DockerID\n\tdelete(engine.tasksToContainers[taskArn], dockerID)\n\tseelog.Debugf(\"Deleted container from tasks, id: %s\", dockerID)\n\n\tif len(engine.tasksToContainers[taskArn]) == 0 {\n\t\t// No containers in task, delete task arn from map.\n\t\tdelete(engine.tasksToContainers, taskArn)\n\t\t// No need to verify if the key exists in tasksToDefinitions.\n\t\t// Delete will do nothing if the specified key doesn't exist.\n\t\tdelete(engine.tasksToDefinitions, taskArn)\n\t\tseelog.Debugf(\"Deleted task from tasks, arn: %s\", taskArn)\n\t}\n\n\t// Remove the container from health container watch list\n\tif _, ok := engine.tasksToHealthCheckContainers[taskArn][dockerID]; !ok {\n\t\treturn\n\t}\n\n\tdelete(engine.tasksToHealthCheckContainers[taskArn], dockerID)\n\tif len(engine.tasksToHealthCheckContainers[taskArn]) == 0 {\n\t\tdelete(engine.tasksToHealthCheckContainers, taskArn)\n\t\tseelog.Debugf(\"Deleted task from container health watch list, arn: %s\", taskArn)\n\t}\n}\n\n// resetStatsUnsafe resets stats for all watched containers.\nfunc (engine *DockerStatsEngine) resetStatsUnsafe() {\n\tfor _, containerMap := range engine.tasksToContainers {\n\t\tfor _, container := range containerMap {\n\t\t\tcontainer.statsQueue.Reset()\n\t\t}\n\t}\n}\n\n// ContainerDockerStats returns the last stored raw docker stats object for a container\nfunc (engine *DockerStatsEngine) ContainerDockerStats(taskARN string, containerID string) (*types.StatsJSON, *NetworkStatsPerSec, error) {\n\tengine.lock.RLock()\n\tdefer engine.lock.RUnlock()\n\n\tcontainerIDToStatsContainer, ok := engine.tasksToContainers[taskARN]\n\ttaskToTaskStats := engine.taskToTaskStats\n\tif !ok {\n\t\treturn nil, nil, errors.Errorf(\"stats engine: task '%s' for container '%s' not found\",\n\t\t\ttaskARN, containerID)\n\t}\n\n\tcontainer, ok := containerIDToStatsContainer[containerID]\n\tif !ok {\n\t\treturn nil, nil, errors.Errorf(\"stats engine: container not found: %s\", containerID)\n\t}\n\tcontainerStats := container.statsQueue.GetLastStat()\n\tcontainerNetworkRateStats := container.statsQueue.GetLastNetworkStatPerSec()\n\n\t// Insert network stats in container stats\n\ttask, err := engine.resolver.ResolveTaskByARN(taskARN)\n\tif err != nil {\n\t\treturn nil, nil, errors.Errorf(\"stats engine: task '%s' not found\",\n\t\t\ttaskARN)\n\t}\n\n\tif task.IsNetworkModeAWSVPC() {\n\t\ttaskStats, ok := taskToTaskStats[taskARN]\n\t\tif ok {\n\t\t\tif taskStats.StatsQueue.GetLastStat() != nil {\n\t\t\t\tcontainerStats.Networks = taskStats.StatsQueue.GetLastStat().Networks\n\t\t\t}\n\t\t\tcontainerNetworkRateStats = taskStats.StatsQueue.GetLastNetworkStatPerSec()\n\t\t} else {\n\t\t\tseelog.Warnf(\"Network stats not found for container %s\", containerID)\n\t\t}\n\t}\n\n\treturn containerStats, containerNetworkRateStats, nil\n}\n", "idx": 4, "id": 26503, "msg": "", "proj": "aws-amazon-ecs-agent", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -476,16 +476,28 @@ static int pack_json_payload(int operation_extracted, int operation_extra_size,\n             continue;\n         }\n         \n-        ret = msgpack_pack_object(mp_pck, kv->key);\n-        if (ret < 0) {\n-            return ret;\n+        len = kv->key.via.str.size;\n+        for (j = 0; j < len_to_be_removed; j++) {\n+            removed = to_be_removed[j];\n+            if (flb_sds_cmp(removed, kv->key.via.str.ptr, len) == 0) {\n+                key_not_found = 0;\n+                break;\n+            }\n         }\n-        ret = msgpack_pack_object(mp_pck, kv->val);\n-        if (ret < 0) {\n-            return ret;\n+        \n+        if (key_not_found) {\n+            ret = msgpack_pack_object(mp_pck, kv->key);\n+            if (ret < 0) {\n+                return ret;\n+            }\n+            ret = msgpack_pack_object(mp_pck, kv->val);\n+            if (ret < 0) {\n+                return ret;\n+            }\n         }\n     }\n \n+    flb_sds_destroy(local_resource_id_key);\n     return 0;\n }\n static int stackdriver_format(struct flb_config *config,", "y": 0, "oldf": "/* -*- Mode: C; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- */\n\n/*  Fluent Bit\n *  ==========\n *  Copyright (C) 2019-2020 The Fluent Bit Authors\n *  Copyright (C) 2015-2018 Treasure Data Inc.\n *\n *  Licensed under the Apache License, Version 2.0 (the \"License\");\n *  you may not use this file except in compliance with the License.\n *  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n *  Unless required by applicable law or agreed to in writing, software\n *  distributed under the License is distributed on an \"AS IS\" BASIS,\n *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n *  See the License for the specific language governing permissions and\n *  limitations under the License.\n */\n\n#include <fluent-bit/flb_output_plugin.h>\n#include <fluent-bit/flb_http_client.h>\n#include <fluent-bit/flb_pack.h>\n#include <fluent-bit/flb_utils.h>\n#include <fluent-bit/flb_time.h>\n#include <fluent-bit/flb_oauth2.h>\n\n#include <msgpack.h>\n\n#include \"gce_metadata.h\"\n#include \"stackdriver.h\"\n#include \"stackdriver_conf.h\"\n#include \"stackdriver_operation.h\"\n#include <mbedtls/base64.h>\n#include <mbedtls/sha256.h>\n\n/*\n * Base64 Encoding in JWT must:\n *\n * - remove any trailing padding '=' character\n * - replace '+' with '-'\n * - replace '/' with '_'\n *\n * ref: https://www.rfc-editor.org/rfc/rfc7515.txt Appendix C\n */\nint jwt_base64_url_encode(unsigned char *out_buf, size_t out_size,\n                          unsigned char *in_buf, size_t in_size,\n                          size_t *olen)\n\n{\n    int i;\n    size_t len;\n\n    /* do normal base64 encoding */\n    mbedtls_base64_encode(out_buf, out_size - 1,\n                          &len, in_buf, in_size);\n\n    /* Replace '+' and '/' characters */\n    for (i = 0; i < len && out_buf[i] != '='; i++) {\n        if (out_buf[i] == '+') {\n            out_buf[i] = '-';\n        }\n        else if (out_buf[i] == '/') {\n            out_buf[i] = '_';\n        }\n    }\n\n    /* Now 'i' becomes the new length */\n    *olen = i;\n    return 0;\n}\n\n\nstatic int jwt_encode(char *payload, char *secret,\n                      char **out_signature, size_t *out_size,\n                      struct flb_stackdriver *ctx)\n{\n    int ret;\n    int len;\n    int buf_size;\n    size_t olen;\n    char *buf;\n    char *sigd;\n    char *headers = \"{\\\"alg\\\": \\\"RS256\\\", \\\"typ\\\": \\\"JWT\\\"}\";\n    unsigned char sha256_buf[32] = {0};\n    mbedtls_sha256_context sha256_ctx;\n    mbedtls_rsa_context *rsa;\n    flb_sds_t out;\n    mbedtls_pk_context pk_ctx;\n    unsigned char sig[256] = {0};\n\n    buf_size = (strlen(payload) + strlen(secret)) * 2;\n    buf = flb_malloc(buf_size);\n    if (!buf) {\n        flb_errno();\n        return -1;\n    }\n\n    /* Encode header */\n    len = strlen(headers);\n    mbedtls_base64_encode((unsigned char *) buf, buf_size - 1,\n                          &olen, (unsigned char *) headers, len);\n\n    /* Create buffer to store JWT */\n    out = flb_sds_create_size(2048);\n    if (!out) {\n        flb_errno();\n        flb_free(buf);\n        return -1;\n    }\n\n    /* Append header */\n    flb_sds_cat(out, buf, olen);\n    flb_sds_cat(out, \".\", 1);\n\n    /* Encode Payload */\n    len = strlen(payload);\n    jwt_base64_url_encode((unsigned char *) buf, buf_size,\n                          (unsigned char *) payload, len, &olen);\n\n    /* Append Payload */\n    flb_sds_cat(out, buf, olen);\n\n    /* do sha256() of base64(header).base64(payload) */\n    mbedtls_sha256_init(&sha256_ctx);\n    mbedtls_sha256_starts(&sha256_ctx, 0);\n    mbedtls_sha256_update(&sha256_ctx, (const unsigned char *) out,\n                          flb_sds_len(out));\n    mbedtls_sha256_finish(&sha256_ctx, sha256_buf);\n\n    /* In mbedTLS cert length must include the null byte */\n    len = strlen(secret) + 1;\n\n    /* Load Private Key */\n    mbedtls_pk_init(&pk_ctx);\n    ret = mbedtls_pk_parse_key(&pk_ctx,\n                               (unsigned char *) secret, len, NULL, 0);\n    if (ret != 0) {\n        flb_plg_error(ctx->ins, \"error loading private key\");\n        flb_free(buf);\n        flb_sds_destroy(out);\n        return -1;\n    }\n\n    /* Create RSA context */\n    rsa = mbedtls_pk_rsa(pk_ctx);\n    if (!rsa) {\n        flb_plg_error(ctx->ins, \"error creating RSA context\");\n        flb_free(buf);\n        flb_sds_destroy(out);\n        mbedtls_pk_free(&pk_ctx);\n        return -1;\n    }\n\n    ret = mbedtls_rsa_pkcs1_sign(rsa, NULL, NULL,\n                                 MBEDTLS_RSA_PRIVATE, MBEDTLS_MD_SHA256,\n                                 0, (unsigned char *) sha256_buf, sig);\n    if (ret != 0) {\n        flb_plg_error(ctx->ins, \"error signing SHA256\");\n        flb_free(buf);\n        flb_sds_destroy(out);\n        mbedtls_pk_free(&pk_ctx);\n        return -1;\n    }\n\n    sigd = flb_malloc(2048);\n    if (!sigd) {\n        flb_errno();\n        flb_free(buf);\n        flb_sds_destroy(out);\n        mbedtls_pk_free(&pk_ctx);\n        return -1;\n    }\n\n    jwt_base64_url_encode((unsigned char *) sigd, 2048, sig, 256, &olen);\n\n    flb_sds_cat(out, \".\", 1);\n    flb_sds_cat(out, sigd, olen);\n\n    *out_signature = out;\n    *out_size = flb_sds_len(out);\n\n    flb_free(buf);\n    flb_free(sigd);\n    mbedtls_pk_free(&pk_ctx);\n\n    return 0;\n}\n\n/* Create a new oauth2 context and get a oauth2 token */\nstatic int get_oauth2_token(struct flb_stackdriver *ctx)\n{\n    int ret;\n    char *token;\n    char *sig_data;\n    size_t sig_size;\n    time_t issued;\n    time_t expires;\n    char payload[1024];\n\n    /* Create oauth2 context */\n    ctx->o = flb_oauth2_create(ctx->config, FLB_STD_AUTH_URL, 3000);\n    if (!ctx->o) {\n        flb_plg_error(ctx->ins, \"cannot create oauth2 context\");\n        return -1;\n    }\n\n    /* In case of using metadata server, fetch token from there */\n    if (ctx->metadata_server_auth) {\n        return gce_metadata_read_token(ctx);\n    }\n\n    /* JWT encode for oauth2 */\n    issued = time(NULL);\n    expires = issued + FLB_STD_TOKEN_REFRESH;\n\n    snprintf(payload, sizeof(payload) - 1,\n             \"{\\\"iss\\\": \\\"%s\\\", \\\"scope\\\": \\\"%s\\\", \"\n             \"\\\"aud\\\": \\\"%s\\\", \\\"exp\\\": %lu, \\\"iat\\\": %lu}\",\n             ctx->client_email, FLB_STD_SCOPE,\n             FLB_STD_AUTH_URL,\n             expires, issued);\n\n    /* Compose JWT signature */\n    ret = jwt_encode(payload, ctx->private_key, &sig_data, &sig_size, ctx);\n    if (ret != 0) {\n        flb_plg_error(ctx->ins, \"JWT signature generation failed\");\n        return -1;\n    }\n    flb_plg_debug(ctx->ins, \"JWT signature:\\n%s\", sig_data);\n\n    ret = flb_oauth2_payload_append(ctx->o,\n                                    \"grant_type\", -1,\n                                    \"urn:ietf:params:oauth:\"\n                                    \"grant-type:jwt-bearer\", -1);\n    if (ret == -1) {\n        flb_plg_error(ctx->ins, \"error appending oauth2 params\");\n        flb_sds_destroy(sig_data);\n        return -1;\n    }\n\n    ret = flb_oauth2_payload_append(ctx->o,\n                                    \"assertion\", -1,\n                                    sig_data, sig_size);\n    if (ret == -1) {\n        flb_plg_error(ctx->ins, \"error appending oauth2 params\");\n        flb_sds_destroy(sig_data);\n        return -1;\n    }\n    flb_sds_destroy(sig_data);\n\n    /* Retrieve access token */\n    token = flb_oauth2_token_get(ctx->o);\n    if (!token) {\n        flb_plg_error(ctx->ins, \"error retrieving oauth2 access token\");\n        return -1;\n    }\n\n    return 0;\n}\n\nstatic char *get_google_token(struct flb_stackdriver *ctx)\n{\n    int ret = 0;\n\n    if (!ctx->o) {\n        ret = get_oauth2_token(ctx);\n    }\n    else if (flb_oauth2_token_expired(ctx->o) == FLB_TRUE) {\n        flb_oauth2_destroy(ctx->o);\n        ret = get_oauth2_token(ctx);\n    }\n\n    if (ret != 0) {\n        return NULL;\n    }\n\n    return ctx->o->access_token;\n}\n\nstatic int cb_stackdriver_init(struct flb_output_instance *ins,\n                          struct flb_config *config, void *data)\n{\n    int ret;\n    int io_flags = FLB_IO_TLS;\n    char *token;\n    struct flb_stackdriver *ctx;\n\n    /* Create config context */\n    ctx = flb_stackdriver_conf_create(ins, config);\n    if (!ctx) {\n        flb_plg_error(ins, \"configuration failed\");\n        return -1;\n    }\n\n    /* Set context */\n    flb_output_set_context(ins, ctx);\n\n    /* Network mode IPv6 */\n    if (ins->host.ipv6 == FLB_TRUE) {\n        io_flags |= FLB_IO_IPV6;\n    }\n\n    /* Create Upstream context for Stackdriver Logging (no oauth2 service) */\n    ctx->u = flb_upstream_create_url(config, FLB_STD_WRITE_URL,\n                                     io_flags, &ins->tls);\n    ctx->metadata_u = flb_upstream_create_url(config, \"http://metadata.google.internal\",\n                                     FLB_IO_TCP, NULL);\n    if (!ctx->u) {\n        flb_plg_error(ctx->ins, \"upstream creation failed\");\n        return -1;\n    }\n    if (!ctx->metadata_u) {\n        flb_plg_error(ctx->ins, \"metadata upstream creation failed\");\n        return -1;\n    }\n\n    /* Upstream Sync flags */\n    ctx->u->flags &= ~FLB_IO_ASYNC;\n    ctx->metadata_u->flags &= ~FLB_IO_ASYNC;\n\n    if (ins->test_mode == FLB_FALSE) {\n        /* Retrieve oauth2 token */\n        token = get_google_token(ctx);\n        if (!token) {\n            flb_plg_warn(ctx->ins, \"token retrieval failed\");\n        }\n    }\n\n    if (ctx->metadata_server_auth) {\n        ret = gce_metadata_read_project_id(ctx);\n        if (ret == -1) {\n            return -1;\n        }\n\n        ret = gce_metadata_read_zone(ctx);\n        if (ret == -1) {\n            return -1;\n        }\n\n        ret = gce_metadata_read_instance_id(ctx);\n        if (ret == -1) {\n            return -1;\n        }\n\n    }\n    return 0;\n}\n\nstatic int validate_severity_level(severity_t * s,\n                                   const char * str,\n                                   const unsigned int str_size)\n{\n    int i = 0;\n\n    const static struct {\n        severity_t s;\n        const unsigned int str_size;\n        const char * str;\n    }   enum_mapping[] = {\n        {FLB_STD_EMERGENCY, 9, \"EMERGENCY\"},\n        {FLB_STD_EMERGENCY, 5, \"EMERG\"    },\n\n        {FLB_STD_ALERT    , 1, \"A\"        },\n        {FLB_STD_ALERT    , 5, \"ALERT\"    },\n\n        {FLB_STD_CRITICAL , 1, \"C\"        },\n        {FLB_STD_CRITICAL , 1, \"F\"        },\n        {FLB_STD_CRITICAL , 4, \"CRIT\"     },\n        {FLB_STD_CRITICAL , 5, \"FATAL\"    },\n        {FLB_STD_CRITICAL , 8, \"CRITICAL\" },\n\n        {FLB_STD_ERROR    , 1, \"E\"        },\n        {FLB_STD_ERROR    , 3, \"ERR\"      },\n        {FLB_STD_ERROR    , 5, \"ERROR\"    },\n        {FLB_STD_ERROR    , 6, \"SEVERE\"   },\n\n        {FLB_STD_WARNING  , 1, \"W\"        },\n        {FLB_STD_WARNING  , 4, \"WARN\"     },\n        {FLB_STD_WARNING  , 7, \"WARNING\"  },\n\n        {FLB_STD_NOTICE   , 1, \"N\"        },\n        {FLB_STD_NOTICE   , 6, \"NOTICE\"   },\n\n        {FLB_STD_INFO     , 1, \"I\"        },\n        {FLB_STD_INFO     , 4, \"INFO\"     },\n\n        {FLB_STD_DEBUG    , 1, \"D\"        },\n        {FLB_STD_DEBUG    , 5, \"DEBUG\"    },\n        {FLB_STD_DEBUG    , 5, \"TRACE\"    },\n        {FLB_STD_DEBUG    , 9, \"TRACE_INT\"},\n        {FLB_STD_DEBUG    , 4, \"FINE\"     },\n        {FLB_STD_DEBUG    , 5, \"FINER\"    },\n        {FLB_STD_DEBUG    , 6, \"FINEST\"   },\n        {FLB_STD_DEBUG    , 6, \"CONFIG\"   },\n\n        {FLB_STD_DEFAULT  , 7, \"DEFAULT\"  }\n    };\n\n    for (i = 0; i < sizeof (enum_mapping) / sizeof (enum_mapping[0]); ++i) {\n        if (enum_mapping[i].str_size != str_size) {\n            continue;\n        }\n\n        if (strncasecmp(str, enum_mapping[i].str, str_size) == 0) {\n            *s = enum_mapping[i].s;\n            return 0;\n        }\n    }\n    return -1;\n}\n\nstatic int get_msgpack_obj(msgpack_object * subobj, const msgpack_object * o,\n                           const flb_sds_t key, const int key_size,\n                           msgpack_object_type type)\n{\n    int i = 0;\n    msgpack_object_kv * p = NULL;\n\n    if (o == NULL || subobj == NULL) {\n        return -1;\n    }\n\n    for (i = 0; i < o->via.map.size; i++) {\n        p = &o->via.map.ptr[i];\n        if (p->val.type != type) {\n            continue;\n        }\n\n        if (flb_sds_cmp(key, p->key.via.str.ptr, p->key.via.str.size) == 0) {\n            *subobj = p->val;\n            return 0;\n        }\n    }\n    return -1;\n}\n\nstatic int get_severity_level(severity_t * s, const msgpack_object * o,\n                              const flb_sds_t key)\n{\n    msgpack_object tmp;\n    if (get_msgpack_obj(&tmp, o, key, flb_sds_len(key), MSGPACK_OBJECT_STR) == 0\n        && validate_severity_level(s, tmp.via.str.ptr, tmp.via.str.size) == 0) {\n        return 0;\n    }\n    *s = 0;\n    return -1;\n}\n\nstatic int pack_json_payload(int operation_extracted, int operation_extra_size, \n                             msgpack_packer* mp_pck, msgpack_object *obj)\n{\n    /* Specified fields include operation, sourceLocation ... */\n    int to_remove = 0;\n    int ret;\n    msgpack_object_kv *kv = obj->via.map.ptr;\n    msgpack_object_kv *const kvend = obj->via.map.ptr + obj->via.map.size;\n\n    if (operation_extracted == FLB_TRUE && operation_extra_size == 0) {\n        to_remove += 1;\n    }\n\n    ret = msgpack_pack_map(mp_pck, obj->via.map.size - to_remove);\n    if (ret < 0) {\n        return ret;\n    }\n    \n    for(; kv != kvend; ++kv\t) {\n        if (strncmp(OPERATION_FIELD_IN_JSON, kv->key.via.str.ptr, kv->key.via.str.size) == 0 \n            && kv->val.type == MSGPACK_OBJECT_MAP) {\n\n            if (operation_extra_size > 0) {\n                msgpack_pack_object(mp_pck, kv->key);\n                pack_extra_operation_subfields(mp_pck, &kv->val, operation_extra_size);\n            }\n            continue;\n        }\n        \n        ret = msgpack_pack_object(mp_pck, kv->key);\n        if (ret < 0) {\n            return ret;\n        }\n        ret = msgpack_pack_object(mp_pck, kv->val);\n        if (ret < 0) {\n            return ret;\n        }\n    }\n\n    return 0;\n}\nstatic int stackdriver_format(struct flb_config *config,\n                              struct flb_input_instance *ins,\n                              void *plugin_context,\n                              const char *tag, int tag_len,\n                              const void *data, size_t bytes,\n                              void **out_data, size_t *out_size)\n{\n    int len;\n    int array_size = 0;\n    /* The default value is 3: timestamp, jsonPayload, logName. */\n    int entry_size = 3; \n    size_t s;\n    size_t off = 0;\n    char path[PATH_MAX];\n    char time_formatted[255];\n    struct tm tm;\n    struct flb_time tms;\n    msgpack_object *obj;\n    msgpack_unpacked result;\n    msgpack_sbuffer mp_sbuf;\n    msgpack_packer mp_pck;\n    flb_sds_t out_buf;\n    struct flb_stackdriver *ctx = plugin_context;\n\n    /* Parameters in severity */\n    int severity_extracted = FLB_FALSE;\n    severity_t severity;\n\n    /* Parameters in Operation */\n    flb_sds_t operation_id;\n    flb_sds_t operation_producer;\n    int operation_first = FLB_FALSE;\n    int operation_last = FLB_FALSE;\n    int operation_extracted = FLB_FALSE;\n    int operation_extra_size = 0;\n\n    /* Count number of records */\n    array_size = flb_mp_count(data, bytes);\n\n    /* Create temporal msgpack buffer */\n    msgpack_sbuffer_init(&mp_sbuf);\n    msgpack_packer_init(&mp_pck, &mp_sbuf, msgpack_sbuffer_write);\n\n    /*\n     * Pack root map (resource & entries):\n     *\n     * {\"resource\": {\"type\": \"...\", \"labels\": {...},\n     *  \"entries\": []\n     */\n    msgpack_pack_map(&mp_pck, 2);\n\n    msgpack_pack_str(&mp_pck, 8);\n    msgpack_pack_str_body(&mp_pck, \"resource\", 8);\n\n    /* type & labels */\n    msgpack_pack_map(&mp_pck, 2);\n\n    /* type */\n    msgpack_pack_str(&mp_pck, 4);\n    msgpack_pack_str_body(&mp_pck, \"type\", 4);\n    msgpack_pack_str(&mp_pck, flb_sds_len(ctx->resource));\n    msgpack_pack_str_body(&mp_pck, ctx->resource,\n                          flb_sds_len(ctx->resource));\n\n    msgpack_pack_str(&mp_pck, 6);\n    msgpack_pack_str_body(&mp_pck, \"labels\", 6);\n\n    if (strcmp(ctx->resource, \"global\") == 0) {\n      /* global resource has field project_id */\n      msgpack_pack_map(&mp_pck, 1);\n      msgpack_pack_str(&mp_pck, 10);\n      msgpack_pack_str_body(&mp_pck, \"project_id\", 10);\n      msgpack_pack_str(&mp_pck, flb_sds_len(ctx->project_id));\n      msgpack_pack_str_body(&mp_pck,\n                            ctx->project_id, flb_sds_len(ctx->project_id));\n    }\n    else if (strcmp(ctx->resource, \"gce_instance\") == 0) {\n      /* gce_instance resource has fields project_id, zone, instance_id */\n      msgpack_pack_map(&mp_pck, 3);\n\n      msgpack_pack_str(&mp_pck, 10);\n      msgpack_pack_str_body(&mp_pck, \"project_id\", 10);\n      msgpack_pack_str(&mp_pck, flb_sds_len(ctx->project_id));\n      msgpack_pack_str_body(&mp_pck,\n                            ctx->project_id, flb_sds_len(ctx->project_id));\n\n      msgpack_pack_str(&mp_pck, 4);\n      msgpack_pack_str_body(&mp_pck, \"zone\", 4);\n      msgpack_pack_str(&mp_pck, flb_sds_len(ctx->zone));\n      msgpack_pack_str_body(&mp_pck, ctx->zone, flb_sds_len(ctx->zone));\n\n      msgpack_pack_str(&mp_pck, 11);\n      msgpack_pack_str_body(&mp_pck, \"instance_id\", 11);\n      msgpack_pack_str(&mp_pck, flb_sds_len(ctx->instance_id));\n      msgpack_pack_str_body(&mp_pck,\n                            ctx->instance_id, flb_sds_len(ctx->instance_id));\n    }\n\n    msgpack_pack_str(&mp_pck, 7);\n    msgpack_pack_str_body(&mp_pck, \"entries\", 7);\n\n    /* Append entries */\n    msgpack_pack_array(&mp_pck, array_size);\n\n    off = 0;\n    msgpack_unpacked_init(&result);\n    while (msgpack_unpack_next(&result, data, bytes, &off) == MSGPACK_UNPACK_SUCCESS) {\n        /* Get timestamp */\n        flb_time_pop_from_msgpack(&tms, &result, &obj);\n\n        /*\n         * Pack entry\n         *\n         * {\n         *  \"logName\": \"...\",\n         *  \"jsonPayload\": {...},\n         *  \"timestamp\": \"...\"\n         * }\n         */\n        \n        /* Extract severity */\n         if (ctx->severity_key\n            && get_severity_level(&severity, obj, ctx->severity_key) == 0) {\n            severity_extracted = FLB_TRUE;\n            entry_size += 1;\n        }\n\n        /* Extract operation */\n        operation_id = flb_sds_create(\"\");\n        operation_producer = flb_sds_create(\"\");\n        operation_first = FLB_FALSE;\n        operation_last = FLB_FALSE;\n        operation_extra_size = 0;\n        operation_extracted = extract_operation(&operation_id, &operation_producer,\n                                                &operation_first, &operation_last, obj, &operation_extra_size);\n        \n        if (operation_extracted == FLB_TRUE) {\n            entry_size += 1;\n        }\n\n        msgpack_pack_map(&mp_pck, entry_size);\n        \n        /* Add severity into the log entry */\n        if (severity_extracted == FLB_TRUE) {\n            msgpack_pack_str(&mp_pck, 8);\n            msgpack_pack_str_body(&mp_pck, \"severity\", 8);\n            msgpack_pack_int(&mp_pck, severity);\n        }\n\n        /* Add operation field into the log entry */\n        if (operation_extracted == FLB_TRUE) {\n            add_operation_field(&operation_id, &operation_producer,\n                                &operation_first, &operation_last, &mp_pck);\n        }\n        \n        /* Clean up id and producer if operation extracted */\n        flb_sds_destroy(operation_id);\n        flb_sds_destroy(operation_producer);\n\n        /* jsonPayload */\n        msgpack_pack_str(&mp_pck, 11);\n        msgpack_pack_str_body(&mp_pck, \"jsonPayload\", 11);\n        pack_json_payload(operation_extracted, operation_extra_size, &mp_pck, obj);\n\n        /* logName */\n        len = snprintf(path, sizeof(path) - 1,\n                       \"projects/%s/logs/%s\", ctx->project_id, tag);\n\n        msgpack_pack_str(&mp_pck, 7);\n        msgpack_pack_str_body(&mp_pck, \"logName\", 7);\n        msgpack_pack_str(&mp_pck, len);\n        msgpack_pack_str_body(&mp_pck, path, len);\n\n        /* timestamp */\n        msgpack_pack_str(&mp_pck, 9);\n        msgpack_pack_str_body(&mp_pck, \"timestamp\", 9);\n\n        /* Format the time */\n        gmtime_r(&tms.tm.tv_sec, &tm);\n        s = strftime(time_formatted, sizeof(time_formatted) - 1,\n                     FLB_STD_TIME_FMT, &tm);\n        len = snprintf(time_formatted + s, sizeof(time_formatted) - 1 - s,\n                       \".%09\" PRIu64 \"Z\", (uint64_t) tms.tm.tv_nsec);\n        s += len;\n\n        msgpack_pack_str(&mp_pck, s);\n        msgpack_pack_str_body(&mp_pck, time_formatted, s);\n    }\n\n    /* Convert from msgpack to JSON */\n    out_buf = flb_msgpack_raw_to_json_sds(mp_sbuf.data, mp_sbuf.size);\n    msgpack_sbuffer_destroy(&mp_sbuf);\n\n    if (!out_buf) {\n        flb_plg_error(ctx->ins, \"error formatting JSON payload\");\n        msgpack_unpacked_destroy(&result);\n        return -1;\n    }\n\n    *out_data = out_buf;\n    *out_size = flb_sds_len(out_buf);\n\n    return 0;\n}\n\nstatic void set_authorization_header(struct flb_http_client *c,\n                                     char *token)\n{\n    int len;\n    char header[512];\n\n    len = snprintf(header, sizeof(header) - 1,\n                   \"Bearer %s\", token);\n    flb_http_add_header(c, \"Authorization\", 13, header, len);\n}\n\nstatic void cb_stackdriver_flush(const void *data, size_t bytes,\n                                 const char *tag, int tag_len,\n                                 struct flb_input_instance *i_ins,\n                                 void *out_context,\n                                 struct flb_config *config)\n{\n    (void) i_ins;\n    (void) config;\n    int ret;\n    int ret_code = FLB_RETRY;\n    size_t b_sent;\n    char *token;\n    flb_sds_t payload_buf;\n    size_t payload_size;\n    void *out_buf;\n    size_t out_size;\n    struct flb_stackdriver *ctx = out_context;\n    struct flb_upstream_conn *u_conn;\n    struct flb_http_client *c;\n\n    /* Get upstream connection */\n    u_conn = flb_upstream_conn_get(ctx->u);\n    if (!u_conn) {\n        FLB_OUTPUT_RETURN(FLB_RETRY);\n    }\n\n    /* Reformat msgpack to stackdriver JSON payload */\n    ret = stackdriver_format(config, i_ins,\n                             ctx,\n                             tag, tag_len,\n                             data, bytes,\n                             &out_buf, &out_size);\n    if (ret != 0) {\n        flb_upstream_conn_release(u_conn);\n        FLB_OUTPUT_RETURN(FLB_RETRY);\n    }\n\n    payload_buf = (flb_sds_t) out_buf;\n    payload_size = out_size;\n\n    /* Get or renew Token */\n    token = get_google_token(ctx);\n    if (!token) {\n        flb_plg_error(ctx->ins, \"cannot retrieve oauth2 token\");\n        flb_upstream_conn_release(u_conn);\n        flb_sds_destroy(payload_buf);\n        FLB_OUTPUT_RETURN(FLB_RETRY);\n    }\n\n    /* Compose HTTP Client request */\n    c = flb_http_client(u_conn, FLB_HTTP_POST, FLB_STD_WRITE_URI,\n                        payload_buf, payload_size, NULL, 0, NULL, 0);\n\n    flb_http_buffer_size(c, 4192);\n\n    flb_http_add_header(c, \"User-Agent\", 10, \"Fluent-Bit\", 10);\n    flb_http_add_header(c, \"Content-Type\", 12, \"application/json\", 16);\n\n    /* Compose and append Authorization header */\n    set_authorization_header(c, token);\n\n    /* Send HTTP request */\n    ret = flb_http_do(c, &b_sent);\n\n    /* validate response */\n    if (ret != 0) {\n        flb_plg_warn(ctx->ins, \"http_do=%i\", ret);\n        ret_code = FLB_RETRY;\n    }\n    else {\n        /* The request was issued successfully, validate the 'error' field */\n        flb_plg_debug(ctx->ins, \"HTTP Status=%i\", c->resp.status);\n        if (c->resp.status == 200) {\n            ret_code = FLB_OK;\n        }\n        else {\n            if (c->resp.payload_size > 0) {\n                /* we got an error */\n                flb_plg_warn(ctx->ins, \"error\\n%s\",\n                             c->resp.payload);\n            }\n            else {\n                flb_plg_debug(ctx->ins, \"response\\n%s\",\n                              c->resp.payload);\n            }\n            ret_code = FLB_RETRY;\n        }\n    }\n\n    /* Cleanup */\n    flb_sds_destroy(payload_buf);\n    flb_http_client_destroy(c);\n    flb_upstream_conn_release(u_conn);\n\n    /* Done */\n    FLB_OUTPUT_RETURN(ret_code);\n}\n\nstatic int cb_stackdriver_exit(void *data, struct flb_config *config)\n{\n    struct flb_stackdriver *ctx = data;\n\n    if (!ctx) {\n        return -1;\n    }\n\n    flb_stackdriver_conf_destroy(ctx);\n    return 0;\n}\n\nstruct flb_output_plugin out_stackdriver_plugin = {\n    .name         = \"stackdriver\",\n    .description  = \"Send events to Google Stackdriver Logging\",\n    .cb_init      = cb_stackdriver_init,\n    .cb_flush     = cb_stackdriver_flush,\n    .cb_exit      = cb_stackdriver_exit,\n\n    /* Test */\n    .test_formatter.callback = stackdriver_format,\n\n    /* Plugin flags */\n    .flags          = FLB_OUTPUT_NET | FLB_IO_TLS,\n};\n", "idx": 3, "id": 11949, "msg": "", "proj": "fluent-fluent-bit", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -27,8 +27,6 @@ import org.springframework.util.ClassUtils;\n import org.springframework.web.method.support.HandlerMethodArgumentResolver;\n import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n \n-import java.util.List;\n-\n /**\n  * {@link Configuration} for OAuth 2.0 Client support.\n  *", "y": 0, "oldf": "/*\n * Copyright 2002-2018 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.springframework.security.config.annotation.web.configuration;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Import;\nimport org.springframework.context.annotation.ImportSelector;\nimport org.springframework.core.type.AnnotationMetadata;\nimport org.springframework.security.oauth2.client.registration.ClientRegistrationRepository;\nimport org.springframework.security.oauth2.client.web.OAuth2AuthorizedClientRepository;\nimport org.springframework.security.oauth2.client.web.method.annotation.OAuth2AuthorizedClientArgumentResolver;\nimport org.springframework.util.ClassUtils;\nimport org.springframework.web.method.support.HandlerMethodArgumentResolver;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurer;\n\nimport java.util.List;\n\n/**\n * {@link Configuration} for OAuth 2.0 Client support.\n *\n * <p>\n * This {@code Configuration} is conditionally imported by {@link OAuth2ImportSelector}\n * when the {@code spring-security-oauth2-client} module is present on the classpath.\n *\n * @author Joe Grandja\n * @since 5.1\n * @see OAuth2ImportSelector\n */\n@Import(OAuth2ClientConfiguration.OAuth2ClientWebMvcImportSelector.class)\nfinal class OAuth2ClientConfiguration {\n\n\tstatic class OAuth2ClientWebMvcImportSelector implements ImportSelector {\n\n\t\t@Override\n\t\tpublic String[] selectImports(AnnotationMetadata importingClassMetadata) {\n\t\t\tboolean webmvcPresent = ClassUtils.isPresent(\n\t\t\t\t\"org.springframework.web.servlet.DispatcherServlet\", getClass().getClassLoader());\n\n\t\t\treturn webmvcPresent ?\n\t\t\t\tnew String[] { \"org.springframework.security.config.annotation.web.configuration.OAuth2ClientConfiguration.OAuth2ClientWebMvcSecurityConfiguration\" } :\n\t\t\t\tnew String[] {};\n\t\t}\n\t}\n\n\t@Configuration\n\tstatic class OAuth2ClientWebMvcSecurityConfiguration implements WebMvcConfigurer {\n\t\tprivate ClientRegistrationRepository clientRegistrationRepository;\n\t\tprivate OAuth2AuthorizedClientRepository authorizedClientRepository;\n\n\t\t@Override\n\t\tpublic void addArgumentResolvers(List<HandlerMethodArgumentResolver> argumentResolvers) {\n\t\t\tif (this.clientRegistrationRepository != null && this.authorizedClientRepository != null) {\n\t\t\t\tOAuth2AuthorizedClientArgumentResolver authorizedClientArgumentResolver =\n\t\t\t\t\t\tnew OAuth2AuthorizedClientArgumentResolver(\n\t\t\t\t\t\t\t\tthis.clientRegistrationRepository, this.authorizedClientRepository);\n\t\t\t\targumentResolvers.add(authorizedClientArgumentResolver);\n\t\t\t}\n\t\t}\n\n\t\t@Autowired(required = false)\n\t\tpublic void setClientRegistrationRepository(List<ClientRegistrationRepository> clientRegistrationRepositories) {\n\t\t\tif (clientRegistrationRepositories.size() == 1) {\n\t\t\t\tthis.clientRegistrationRepository = clientRegistrationRepositories.get(0);\n\t\t\t}\n\t\t}\n\n\t\t@Autowired(required = false)\n\t\tpublic void setAuthorizedClientRepository(List<OAuth2AuthorizedClientRepository> authorizedClientRepositories) {\n\t\t\tif (authorizedClientRepositories.size() == 1) {\n\t\t\t\tthis.authorizedClientRepository = authorizedClientRepositories.get(0);\n\t\t\t}\n\t\t}\n\t}\n}\n", "idx": 3, "id": 12474, "msg": "", "proj": "spring-projects-spring-security", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -485,13 +485,19 @@ process_token:\n \t\t\t\t\t\tcxxParserNewStatement();\n \t\t\t\t\tbreak;\n \t\t\t\t\tcase CXXKeywordTHROW:\n-\t\t\t\t\t\t// ignore when inside a function\n-\t\t\t\t\t\tif(cxxScopeGetType() == CXXScopeTypeFunction)\n+\t\t\t\t\t\t// We ignore whole \"throw expressions\" as they contain nothing useful\n+\t\t\t\t\t\t// and may confuse us. We keep \"throw\" when used as exception specification,\n+\t\t\t\t\t\t// and this is certainly outside of a function and when the token chain\n+\t\t\t\t\t\t// already contains at least a type, an identifier and a parenthesis.\n+\t\t\t\t\t\t// This check seems excessive but keep in mind that we deal with\n+\t\t\t\t\t\t// broken input and we might also be wrong about the current scope.\n+\t\t\t\t\t\tif((cxxScopeGetType() == CXXScopeTypeFunction) || (g_cxx.pTokenChain->iCount < 3))\n \t\t\t\t\t\t{\n+\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Skipping throw statement\");\n \t\t\t\t\t\t\tif(!cxxParserParseUpToOneOf(CXXTokenTypeSemicolon | CXXTokenTypeEOF,\n \t\t\t\t\t\t\t\t   false))\n \t\t\t\t\t\t\t{\n-\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse return/continue/break\");\n+\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to skip throw statement\");\n \t\t\t\t\t\t\t\treturn false;\n \t\t\t\t\t\t\t}\n \t\t\t\t\t\t\tcxxParserNewStatement();", "y": 1, "oldf": "/*\n*   Copyright (c) 2016, Szymon Tomasz Stefanek\n*\n*   This source code is released for free distribution under the terms of the\n*   GNU General Public License version 2 or (at your option) any later version.\n*\n*   This module contains functions for parsing and scanning C++ source files\n*/\n#include \"cxx_parser.h\"\n#include \"cxx_parser_internal.h\"\n\n#include \"cxx_debug.h\"\n#include \"cxx_keyword.h\"\n#include \"cxx_token.h\"\n#include \"cxx_token_chain.h\"\n#include \"cxx_scope.h\"\n#include \"cxx_tag.h\"\n\n#include \"parse.h\"\n#include \"vstring.h\"\n#include \"../cpreprocessor.h\"\n#include \"debug.h\"\n#include \"keyword.h\"\n#include \"read.h\"\n\n#include \"cxx_subparser_internal.h\"\n\n#include <string.h>\n\nbool cxxParserParseBlockHandleOpeningBracket(void)\n{\n\tCXX_DEBUG_ENTER();\n\n\tCXX_DEBUG_ASSERT(\n\t\t\tg_cxx.pToken->eType == CXXTokenTypeOpeningBracket,\n\t\t\t\"This must be called when pointing at an opening bracket!\"\n\t\t);\n\n\tenum CXXScopeType eScopeType = cxxScopeGetType();\n\tbool bIsCPP = cxxParserCurrentLanguageIsCPP();\n\tCXXToken * pAux;\n\n\tif(\n\t\t\t(\n\t\t\t\t// something = {...}\n\t\t\t\t(g_cxx.pToken->pPrev) &&\n\t\t\t\tcxxTokenTypeIs(g_cxx.pToken->pPrev,CXXTokenTypeAssignment) &&\n\t\t\t\t(\n\t\t\t\t\t(eScopeType == CXXScopeTypeFunction) ||\n\t\t\t\t\t(eScopeType == CXXScopeTypeNamespace)\n\t\t\t\t)\n\t\t\t) || (\n\t\t\t\tbIsCPP &&\n\t\t\t\t(g_cxx.pToken->pPrev) &&\n\t\t\t\t(\n\t\t\t\t\t(\n\t\t\t\t\t\t// T { arg1, arg2, ... } (1)\n\t\t\t\t\t\t// T object { arg1, arg2, ... } (2)\n\t\t\t\t\t\t// new T { arg1, arg2, ... } (3)\n\t\t\t\t\t\t// Class::Class() : member { arg1, arg2, ... } { (4)\n\t\t\t\t\t\tcxxTokenTypeIs(g_cxx.pToken->pPrev,CXXTokenTypeIdentifier) &&\n\t\t\t\t\t\t(\n\t\t\t\t\t\t\t// case 1\n\t\t\t\t\t\t\t(!g_cxx.pToken->pPrev->pPrev) ||\n\t\t\t\t\t\t\t// case 4\n\t\t\t\t\t\t\tcxxTokenTypeIsOneOf(\n\t\t\t\t\t\t\t\t\tg_cxx.pToken->pPrev->pPrev,\n\t\t\t\t\t\t\t\t\tCXXTokenTypeSingleColon | CXXTokenTypeComma\n\t\t\t\t\t\t\t) ||\n\t\t\t\t\t\t\t// cases 1,2,3 but not 4\n\t\t\t\t\t\t\t(\n\t\t\t\t\t\t\t\t// more parts of typename or maybe the \"new\" keyword before the identifier\n\t\t\t\t\t\t\t\tcxxTokenTypeIsOneOf(\n\t\t\t\t\t\t\t\t\t\tg_cxx.pToken->pPrev->pPrev,\n\t\t\t\t\t\t\t\t\t\tCXXTokenTypeIdentifier | CXXTokenTypeStar | CXXTokenTypeAnd |\n\t\t\t\t\t\t\t\t\t\tCXXTokenTypeGreaterThanSign | CXXTokenTypeKeyword\n\t\t\t\t\t\t\t\t) &&\n\t\t\t\t\t\t\t\t// but no parenthesis (discard things like bool test() Q_DECL_NO_THROW { ... })\n\t\t\t\t\t\t\t\t(!(pAux = cxxTokenChainPreviousTokenOfType(\n\t\t\t\t\t\t\t\t\t\tg_cxx.pToken->pPrev->pPrev,\n\t\t\t\t\t\t\t\t\t\tCXXTokenTypeParenthesisChain\n\t\t\t\t\t\t\t\t\t))\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t) &&\n\t\t\t\t\t\t// \"override\" is handled as identifier since it's a keyword only after function signatures\n\t\t\t\t\t\t(strcmp(vStringValue(g_cxx.pToken->pPrev->pszWord),\"override\") != 0)\n\t\t\t\t\t) || (\n\t\t\t\t\t\t// type var[][][]..[] { ... }\n\t\t\t\t\t\t// (but not '[] { ... }' which is a parameterless lambda)\n\t\t\t\t\t\tcxxTokenTypeIs(g_cxx.pToken->pPrev,CXXTokenTypeSquareParenthesisChain) &&\n\t\t\t\t\t\t(\n\t\t\t\t\t\t\tpAux = cxxTokenChainPreviousTokenNotOfType(\n\t\t\t\t\t\t\t\t\tg_cxx.pToken->pPrev,\n\t\t\t\t\t\t\t\t\tCXXTokenTypeSquareParenthesisChain\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t) &&\n\t\t\t\t\t\tcxxTokenTypeIs(pAux,CXXTokenTypeIdentifier)\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t) || (\n\t\t\t\t// return { }\n\t\t\t\t(!g_cxx.pToken->pPrev) &&\n\t\t\t\t(g_cxx.uKeywordState & CXXParserKeywordStateSeenReturn)\n\t\t\t)\n\t\t)\n\t{\n\t\t// array or list-like initialisation\n\t\tbool bRet = cxxParserParseAndCondenseCurrentSubchain(\n\t\t\t\tCXXTokenTypeOpeningBracket | CXXTokenTypeOpeningParenthesis |\n\t\t\t\t\tCXXTokenTypeOpeningSquareParenthesis,\n\t\t\t\tfalse,\n\t\t\t\ttrue\n\t\t\t);\n\n\t\tCXX_DEBUG_LEAVE_TEXT(\"Handled array or list-like initialisation or return\");\n\t\treturn bRet;\n\t}\n\n\t// In C++ mode check for lambdas\n\tCXXToken * pParenthesis;\n\n\tif(\n\t\tbIsCPP &&\n\t\t(pParenthesis = cxxParserOpeningBracketIsLambda())\n\t)\n\t{\n\t\tif(!cxxParserHandleLambda(pParenthesis))\n\t\t{\n\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Lambda handling failed\");\n\t\t\treturn false;\n\t\t}\n\n\t\t// Note that here we're leaving the token chain \"alive\" so further parsing can be performed.\n\t\tCXX_DEBUG_LEAVE_TEXT(\"Lambda handling succeeded\");\n\t\treturn true;\n\t}\n\n\tint iScopes;\n\tint iCorkQueueIndex = CORK_NIL;\n\n\tCXXFunctionSignatureInfo oInfo;\n\n\tif(eScopeType != CXXScopeTypeFunction)\n\t{\n\t\t// very likely a function definition\n\t\t// (but may be also a toplevel block, like \"extern \"C\" { ... }\")\n\t\tiScopes = cxxParserExtractFunctionSignatureBeforeOpeningBracket(&oInfo,&iCorkQueueIndex);\n\n\t\t// FIXME: Handle syntax (5) of list initialization:\n\t\t//        Class::Class() : member { arg1, arg2, ... } {...\n\t} else {\n\t\t// some kind of other block:\n\t\t// - anonymous block\n\t\t// - block after for(),while(),foreach(),if() and other similar stuff\n\t\t// (note that {}-style initializers have been handled above and thus are excluded)\n\n\t\tiScopes = 0;\n\t}\n\n\tcxxParserNewStatement();\n\n\tif(!cxxParserParseBlock(true))\n\t{\n\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse nested block\");\n\t\treturn false;\n\t}\n\n\tif(iScopes < 1)\n\t{\n\t\tCXX_DEBUG_LEAVE_TEXT(\"The block was not a function\");\n\t\treturn true;\n\t}\n\n\tunsigned long uEndPosition = getInputLineNumber();\n\n\t// If the function contained a \"try\" keyword before the opening bracket\n\t// then it's likely to be a function-try-block and should be followed by a catch\n\t// block that is in the same scope.\n\n\tif(oInfo.uFlags & CXXFunctionSignatureInfoFunctionTryBlock)\n\t{\n\t\t// look for the catch blocks.\n\t\tCXX_DEBUG_PRINT(\"The function is a function-try-block: looking for catch blocks\");\n\n\t\tfor(;;)\n\t\t{\n\t\t\tCXX_DEBUG_PRINT(\"Looking ahead for a catch block...\");\n\n\t\t\tif(!cxxParserParseNextToken())\n\t\t\t\tbreak; // EOF\n\n\t\t\tif(!cxxTokenIsKeyword(g_cxx.pToken,CXXKeywordCATCH))\n\t\t\t{\n\t\t\t\t// No more catches. Unget and exit.\n\t\t\t\tCXX_DEBUG_PRINT(\"No more catch blocks\");\n\t\t\t\tcxxParserUngetCurrentToken();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t// assume it's a catch block.\n\n\t\t\tCXX_DEBUG_PRINT(\"Found catch block\");\n\n\t\t\tif(!cxxParserParseIfForWhileSwitchCatchParenthesis())\n\t\t\t{\n\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse the catch parenthesis\");\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\t// the standard requires a bracket here (catch block is always a compound statement).\n\n\t\t\tcxxParserNewStatement();\n\n\t\t\tif(!cxxParserParseNextToken())\n\t\t\t{\n\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Found EOF while looking for catch() block: playing nice\");\n\t\t\t\tbreak; // EOF (would be a syntax error!)\n\t\t\t}\n\n\t\t\tif(!cxxTokenTypeIs(g_cxx.pToken,CXXTokenTypeOpeningBracket))\n\t\t\t{\n\t\t\t\t// Aargh...\n\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Found something unexpected while looking for catch() block: playing nice\");\n\t\t\t\tcxxParserUngetCurrentToken();\n\t\t\t\tbreak; // (would be a syntax error!)\n\t\t\t}\n\n\t\t\tif(!cxxParserParseBlock(true))\n\t\t\t\treturn false;\n\n \t\t\tuEndPosition = getInputLineNumber();\n \t\t}\n \t}\n\n\tif(iCorkQueueIndex > CORK_NIL)\n\t\tcxxParserSetEndLineForTagInCorkQueue(iCorkQueueIndex,uEndPosition);\n\n\twhile(iScopes > 0)\n\t{\n\t\tcxxScopePop();\n\t\tiScopes--;\n\t}\n\n\tCXX_DEBUG_LEAVE();\n\treturn true;\n}\n\nstatic bool cxxParserParseBlockInternal(bool bExpectClosingBracket)\n{\n\tCXX_DEBUG_ENTER();\n\n\t//char * szScopeName = cxxScopeGetFullName();\n\t//CXX_DEBUG_PRINT(\"Scope name is '%s'\",szScopeName ? szScopeName : \"\");\n\n\tcxxParserNewStatement();\n\n\tif(bExpectClosingBracket)\n\t{\n\t\t// FIXME: this cpp handling is kind of broken:\n\t\t//        it works only because the moon is in the correct phase.\n\t\tcppBeginStatement();\n\t}\n\n\tfor(;;)\n\t{\n\t\tif(!cxxParserParseNextToken())\n\t\t{\nfound_eof:\n\n\t\t\tif(bExpectClosingBracket)\n\t\t\t{\n\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\n\t\t\t\t\t\t\"Syntax error: found EOF in block but a closing \" \\\n\t\t\t\t\t\t\t\"bracket was expected!\"\n\t\t\t\t\t);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tCXX_DEBUG_LEAVE_TEXT(\"EOF in main block\");\n\t\t\treturn true; // EOF\n\t\t}\n\nprocess_token:\n\n\t\tCXX_DEBUG_PRINT(\n\t\t\t\t\"Token '%s' of type 0x%02x\",\n\t\t\t\tvStringValue(g_cxx.pToken->pszWord),\n\t\t\t\tg_cxx.pToken->eType\n\t\t\t);\n\n\t\tif (cxxTokenTypeIs(g_cxx.pToken,CXXTokenTypeIdentifier)\n\t\t\t&& cxxScopeGetType() == CXXScopeTypeClass\n\t\t\t&& cxxSubparserNewIdentifierAsHeadOfMemberNotify(g_cxx.pToken))\n\t\t{\n\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch(g_cxx.pToken->eType)\n\t\t{\n\t\t\tcase CXXTokenTypeKeyword:\n\t\t\t{\n\t\t\t\tswitch(g_cxx.pToken->eKeyword)\n\t\t\t\t{\n\t\t\t\t\tcase CXXKeywordNAMESPACE:\n\t\t\t\t\t{\n\t\t\t\t\t\tenum CXXScopeType eScopeType = cxxScopeGetType();\n\n\t\t\t\t\t\tif(\n\t\t\t\t\t\t\t\t(\n\t\t\t\t\t\t\t\t\t// toplevel or nested within a namespace\n\t\t\t\t\t\t\t\t\t(eScopeType == CXXScopeTypeNamespace) ||\n\t\t\t\t\t\t\t\t\t// namespace X = Y inside a function\n\t\t\t\t\t\t\t\t\t(eScopeType == CXXScopeTypeFunction)\n\t\t\t\t\t\t\t\t) && (\n\t\t\t\t\t\t\t\t\t// either certainly C++\n\t\t\t\t\t\t\t\t\tg_cxx.bConfirmedCPPLanguage ||\n\t\t\t\t\t\t\t\t\t// or a \"sane\" namespace syntax\n\t\t\t\t\t\t\t\t\t(\n\t\t\t\t\t\t\t\t\t\t!cxxTokenChainPreviousTokenOfType(\n\t\t\t\t\t\t\t\t\t\t\t\tg_cxx.pToken,\n\t\t\t\t\t\t\t\t\t\t\t\tCXXTokenTypeStar |\n\t\t\t\t\t\t\t\t\t\t\t\tCXXTokenTypeAnd |\n\t\t\t\t\t\t\t\t\t\t\t\tCXXTokenTypeKeyword\n\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif(!cxxParserParseNamespace())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse namespace\");\n\t\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// If we're pretty sure this is C++ then this is a syntax error.\n\t\t\t\t\t\t\t// If we're not sure (namely when we're in a *.h file) then\n\t\t\t\t\t\t\t// let's try to be flexible: treat the namespace keyword as an identifier.\n\t\t\t\t\t\t\tif(!g_cxx.bConfirmedCPPLanguage)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\n\t\t\t\t\t\t\t\t\t\"Found namespace in unexpected place, but we're not sure it's really C++ \"\n\t\t\t\t\t\t\t\t\t\"so we'll treat it as an identifier instead\"\n\t\t\t\t\t\t\t\t);\n\t\t\t\t\t\t\t\tg_cxx.pToken->eType = CXXTokenTypeIdentifier;\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\n\t\t\t\t\t\t\t\t\"Found namespace in a wrong place: we're probably out of sync\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordTEMPLATE:\n\t\t\t\t\t\tif(!cxxParserParseTemplatePrefix())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse template\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Here we are just after the \"template<parameters>\" prefix.\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordTYPEDEF:\n\t\t\t\t\t\t// Mark the next declaration as a typedef\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenTypedef;\n\t\t\t\t\t\tcxxTokenChainClear(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordENUM:\n\t\t\t\t\t\tif(!cxxParserParseEnum())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse enum\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordCLASS:\n\t\t\t\t\t\tif(!cxxParserParseClassStructOrUnion(CXXKeywordCLASS,CXXTagCPPKindCLASS,CXXScopeTypeClass))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse class/struct/union\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordSTRUCT:\n\t\t\t\t\t\tif(!cxxParserParseClassStructOrUnion(CXXKeywordSTRUCT,CXXTagKindSTRUCT,CXXScopeTypeStruct))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse class/struct/union\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordUNION:\n\t\t\t\t\t\tif(!cxxParserParseClassStructOrUnion(CXXKeywordUNION,CXXTagKindUNION,CXXScopeTypeUnion))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse class/struct/union\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordPUBLIC:\n\t\t\t\t\tcase CXXKeywordPROTECTED:\n\t\t\t\t\tcase CXXKeywordPRIVATE:\n\t\t\t\t\t\t// Note that the class keyword has its own handler\n\t\t\t\t\t\t// so the only possibility here is an access specifier\n\t\t\t\t\t\tif(!cxxParserParseAccessSpecifier())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse access specifier\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordUSING:\n\t\t\t\t\t\tif(!cxxParserParseUsingClause())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse using clause\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordIF:\n\t\t\t\t\tcase CXXKeywordFOR:\n\t\t\t\t\tcase CXXKeywordWHILE:\n\t\t\t\t\tcase CXXKeywordSWITCH:\n\t\t\t\t\tcase CXXKeywordCATCH:\n\t\t\t\t\t\tif(!cxxParserParseIfForWhileSwitchCatchParenthesis())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse if/for/while/switch/catch parenthesis\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Now we're just before the block that follows the parenthesis.\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t// Force the cpp preprocessor to think that we're in the middle of a statement.\n\t\t\t\t\t\tcppBeginStatement();\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordTRY:\n\t\t\t\t\t\t// We parse try in different ways depending on the context.\n\t\t\t\t\t\t// Inside a function, and without preceding tokens it's assumed to be\n\t\t\t\t\t\t// a plain try {} catch {} block. This is easy.\n\t\t\t\t\t\t// Out of a function it's likely to be a function try block:\n\t\t\t\t\t\t//    int f(int n = 2) try { ... } catch { ... }\n\t\t\t\t\t\t// Inside a function but with some preceding tokens it's likely to be a\n\t\t\t\t\t\t// lambda expressed as function-try-block.\n\t\t\t\t\t\t//    auto f() -> void try { ... } catch { ... }\n\t\t\t\t\t\tif((cxxScopeGetType() != CXXScopeTypeFunction) || g_cxx.pToken->pPrev)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_PRINT(\"Found try that looks like a function-try-block\");\n\t\t\t\t\t\t\t// Maybe function-try-block.\n\t\t\t\t\t\t\t// Keep in the chain and continue parsing.\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Fall through.\n\t\t\t\t\tcase CXXKeywordELSE:\n\t\t\t\t\tcase CXXKeywordDO:\n\t\t\t\t\t\t// parse as normal statement/block\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t// Force the cpp preprocessor to think that we're in the middle of a statement.\n\t\t\t\t\t\tcppBeginStatement();\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordRETURN:\n\t\t\t\t\t\tif(cxxParserCurrentLanguageIsCPP())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t// may be followed by a lambda, otherwise it's not interesting.\n\t\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenReturn;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// ignore\n\t\t\t\t\t\t\tif(!cxxParserParseUpToOneOf(CXXTokenTypeSemicolon | CXXTokenTypeEOF,\n\t\t\t\t\t\t\t\t   false))\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse return\");\n\t\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordCONTINUE:\n\t\t\t\t\tcase CXXKeywordBREAK:\n\t\t\t\t\tcase CXXKeywordGOTO:\n\t\t\t\t\t\t// ignore\n\t\t\t\t\t\tif(!cxxParserParseUpToOneOf(CXXTokenTypeSemicolon | CXXTokenTypeEOF,\n\t\t\t\t\t\t\t   false))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse continue/break/goto\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordTHROW:\n\t\t\t\t\t\t// ignore when inside a function\n\t\t\t\t\t\tif(cxxScopeGetType() == CXXScopeTypeFunction)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif(!cxxParserParseUpToOneOf(CXXTokenTypeSemicolon | CXXTokenTypeEOF,\n\t\t\t\t\t\t\t\t   false))\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse return/continue/break\");\n\t\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordCASE:\n\t\t\t\t\t\t// ignore\n\t\t\t\t\t\tif(!cxxParserParseUpToOneOf(\n\t\t\t\t\t\t\t\tCXXTokenTypeSemicolon | CXXTokenTypeEOF | CXXTokenTypeSingleColon,\n\t\t\t\t\t\t\t\tfalse\n\t\t\t\t\t\t\t))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse case keyword\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordEXTERN:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenExtern;\n\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\n\t\t\t\t\t\tif(!cxxParserParseNextToken())\n\t\t\t\t\t\t\tgoto found_eof;\n\n\t\t\t\t\t\tif(cxxTokenTypeIs(g_cxx.pToken,CXXTokenTypeStringConstant))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t// assume extern \"language\"\n\n\t\t\t\t\t\t\t// Strictly speaking this is a C++ only syntax.\n\t\t\t\t\t\t\t// However we allow it also in C as it doesn't really hurt.\n\n\t\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\n\t\t\t\t\t\t\t// Note that extern \"C\" may be followed by a block with declarations\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t//   extern \"C\" { ... }\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t// However in this case the declarations are ALSO definitions\n\t\t\t\t\t\t\t// and extern \"C\" is used only to specify the name mangling mode.\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t//   extern \"C\" int x; <-- a declaration and not a definition\n\t\t\t\t\t\t\t//   extern \"C\" { int x; } <-- a declaration and definition: x IS defined\n\t\t\t\t\t\t\t//                             here and is NOT extern.\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t// A variable in an extern \"C\" block has to be re-declared extern again\n\t\t\t\t\t\t\t// to be really treated as declaration only.\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t//   extern \"C\" { extern int x; }\n\t\t\t\t\t\t\t//\n\t\t\t\t\t\t\t// So in this case we do NOT treat the inner declarations as extern\n\t\t\t\t\t\t\t// and we don't need specific handling code for this case.\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// something else: handle it the normal way\n\t\t\t\t\t\t\tgoto process_token;\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordSTATIC:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenStatic;\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordINLINE:\n\t\t\t\t\tcase CXXKeyword__INLINE:\n\t\t\t\t\tcase CXXKeyword__INLINE__:\n\t\t\t\t\tcase CXXKeyword__FORCEINLINE:\n\t\t\t\t\tcase CXXKeyword__FORCEINLINE__:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenInline;\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordEXPLICIT:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenExplicit;\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordOPERATOR:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenOperator;\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordVIRTUAL:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenVirtual;\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordMUTABLE:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenMutable;\n\t\t\t\t\t\tcxxTokenChainDestroyLast(g_cxx.pTokenChain);\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordFRIEND:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenFriend;\n\t\t\t\t\tbreak;\n\t\t\t\t\t// \"const\" and \"volatile\" are part of the type. Don't treat them specially\n\t\t\t\t\t// and don't attempt to extract an eventual typedef yet,\n\t\t\t\t\t// as there might be a struct/class/union keyword following.\n\t\t\t\t\tcase CXXKeywordVOLATILE:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenVolatile;\n\t\t\t\t\tbreak;\n\t\t\t\t\tcase CXXKeywordCONST:\n\t\t\t\t\t\tg_cxx.uKeywordState |= CXXParserKeywordStateSeenConst;\n\t\t\t\t\tbreak;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tif(g_cxx.uKeywordState & CXXParserKeywordStateSeenTypedef)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tg_cxx.uKeywordState &= ~CXXParserKeywordStateSeenTypedef;\n\t\t\t\t\t\t\tif(!cxxParserParseGenericTypedef())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse generic typedef\");\n\t\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeSemicolon:\n\t\t\t{\n\t\t\t\tif(\n\t\t\t\t\t\t(cxxParserCurrentLanguageIsC()) &&\n\t\t\t\t\t\tcxxScopeIsGlobal() &&\n\t\t\t\t\t\t(!(g_cxx.uKeywordState & CXXParserKeywordStateSeenExtern)) &&\n\t\t\t\t\t\t(!(g_cxx.uKeywordState & CXXParserKeywordStateSeenTypedef))\n\t\t\t\t\t)\n\t\t\t\t{\n\t\t\t\t\t// Special handling of K&R style function declarations.\n\t\t\t\t\t// We might be in the following situation:\n\t\t\t\t\t//\n\t\t\t\t\t//  type whatever fname(par1,par2) int par1; int par2; {\n\t\t\t\t\t//                                        ^\n\t\t\t\t\t//\n\t\t\t\t\tswitch(cxxParserMaybeParseKnRStyleFunctionDefinition())\n\t\t\t\t\t{\n\t\t\t\t\t\tcase 1:\n\t\t\t\t\t\t\t// K&R parser did the job and started a new statement\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase 0:\n\t\t\t\t\t\t\t// something else\n\t\t\t\t\t\t\tcxxParserAnalyzeOtherStatement();\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to check for K&R style function definition\");\n\t\t\t\t\t\t\treturn false;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// K&R style function declarations not allowed here.\n\t\t\t\t\tcxxParserAnalyzeOtherStatement();\n\t\t\t\t}\n\t\t\t\tcxxParserNewStatement();\n\t\t\t}\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeSingleColon:\n\t\t\t{\n\t\t\t\t// label ?\n\t\t\t\tif(\n\t\t\t\t\t\t(g_cxx.pTokenChain->iCount == 2) &&\n\t\t\t\t\t\tcxxTokenTypeIs(\n\t\t\t\t\t\t\t\tcxxTokenChainFirst(g_cxx.pTokenChain),\n\t\t\t\t\t\t\t\tCXXTokenTypeIdentifier\n\t\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\t{\n\t\t\t\t\tCXXToken * pFirst = cxxTokenChainFirst(g_cxx.pTokenChain);\n\t\t\t\t\t// assume it's label\n\t\t\t\t\ttagEntryInfo * tag = cxxTagBegin(CXXTagKindLABEL,pFirst);\n\n\t\t\t\t\tif(tag)\n\t\t\t\t\t{\n\t\t\t\t\t\ttag->isFileScope = true;\n\t\t\t\t\t\tcxxTagCommit();\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// what is this? (default: and similar things have been handled at keyword level)\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeOpeningBracket:\n\t\t\t\tif(!cxxParserParseBlockHandleOpeningBracket())\n\t\t\t\t{\n\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to handle opening bracket\");\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeClosingBracket:\n\t\t\t\t// scope finished\n\t\t\t\tif(!bExpectClosingBracket)\n\t\t\t\t{\n\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\n\t\t\t\t\t\t\"Found unexpected closing bracket: probably preprocessing problem\"\n\t\t\t\t\t);\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Closing bracket!\");\n\t\t\t\tcxxParserNewStatement();\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeOpeningParenthesis:\n\t\t\tcase CXXTokenTypeOpeningSquareParenthesis:\n\t\t\t\tif(!cxxParserParseAndCondenseCurrentSubchain(\n\t\t\t\t\t\tCXXTokenTypeOpeningBracket | CXXTokenTypeOpeningParenthesis |\n\t\t\t\t\t\t\tCXXTokenTypeOpeningSquareParenthesis,\n\t\t\t\t\t\ttrue,\n\t\t\t\t\t\tfalse\n\t\t\t\t\t))\n\t\t\t\t{\n\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Parsing the parenthesis failed\");\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\n\t\t\t\tif(cxxTokenTypeIs(g_cxx.pToken,CXXTokenTypeEOF))\n\t\t\t\t{\n\t\t\t\t\tif(bExpectClosingBracket)\n\t\t\t\t\t{\n\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\n\t\t\t\t\t\t\t\t\"Syntax error: found EOF in block but a closing bracket was expected!\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t\treturn true; // EOF\n\t\t\t\t}\n\t\t\tbreak;\n\t\t\tcase CXXTokenTypeIdentifier:\n\t\t\t\tif(g_cxx.uKeywordState & CXXParserKeywordStateSeenTypedef)\n\t\t\t\t{\n\t\t\t\t\tg_cxx.uKeywordState &= ~CXXParserKeywordStateSeenTypedef;\n\t\t\t\t\tif(!cxxParserParseGenericTypedef())\n\t\t\t\t\t{\n\t\t\t\t\t\tCXX_DEBUG_LEAVE_TEXT(\"Failed to parse generic typedef\");\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t\tcxxParserNewStatement();\n\t\t\t\t}\n\t\t\t\telse if (cxxScopeGetType() == CXXScopeTypeClass)\n\t\t\t\t\tcxxSubparserUnknownIdentifierInClassNotify(g_cxx.pToken);\n\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t// something else we didn't handle\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tCXX_DEBUG_LEAVE_TEXT(\"WARNING: Not reached\");\n\treturn true;\n}\n\n//\n// This is the toplevel scanning function. It's a forward-only scanner that keeps\n// accumulating tokens in the chain until either a characteristic token is found\n// or the statement ends. When a characteristic token is found it usually enters\n// a specialized scanning routine (e.g for classes, namespaces, structs...).\n// When the statement ends without finding any characteristic token the chain\n// is passed to an analysis routine which does a second scan pass.\n//\nbool cxxParserParseBlock(bool bExpectClosingBracket)\n{\n\tcxxSubparserNotifyEnterBlock ();\n\n\tcppPushExternalParserBlock();\n\tbool bRet = cxxParserParseBlockInternal(bExpectClosingBracket);\n\tcppPopExternalParserBlock();\n\n\tcxxSubparserNotifyLeaveBlock ();\n\n\treturn bRet;\n}\n", "idx": 1, "id": 18010, "msg": "I thinks CXX_DEBUG_PRINT is suitable than CXX_DEBUG_LEAVE_TEXT because the controls is not returned caller here.", "proj": "universal-ctags-ctags", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -152,13 +152,13 @@ namespace pwiz.Skyline.Model.Results\n             return FilterSpectrumList(spectra, filters, HighAccQ1, false);\n         }\n \n-        public ExtractedSpectrum FilterQ3SpectrumList(MsDataSpectrum[] spectra, bool useDriftTimeHighEnergyOffset)\n+        public ExtractedSpectrum FilterQ3SpectrumList(MsDataSpectrum[] spectra, bool useIonMobilityHighEnergyOffset)\n         {\n             // All-ions extraction for MS1 scans only\n             if (Q1 == 0)\n                 return null;\n \n-            return FilterSpectrumList(spectra, Ms2ProductFilters, HighAccQ3, useDriftTimeHighEnergyOffset);\n+            return FilterSpectrumList(spectra, Ms2ProductFilters, HighAccQ3, useIonMobilityHighEnergyOffset);\n         }\n \n         /// <summary>", "y": 0, "oldf": "\ufeff/*\r\n * Original author: Brendan MacLean <brendanx .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2010 University of Washington - Seattle, WA\r\n * \r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\n\r\nusing System;\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing System.Drawing;\r\nusing System.Linq;\r\nusing pwiz.Common.Chemistry;\r\nusing pwiz.Common.Collections;\r\nusing pwiz.ProteowizardWrapper;\r\nusing pwiz.Skyline.Model.DocSettings;\r\nusing pwiz.Skyline.Model.Results.RemoteApi.GeneratedCode;\r\n\r\nnamespace pwiz.Skyline.Model.Results\r\n{\r\n    public sealed class SpectrumFilterPair : IComparable<SpectrumFilterPair>\r\n    {\r\n        private static readonly SpectrumProductFilter[] EMPTY_FILTERS = new SpectrumProductFilter[0];\r\n\r\n        public SpectrumFilterPair(PrecursorTextId precursorTextId, Color peptideColor, int id, double? minTime, double? maxTime,\r\n            double highEnergyIonMobilityValueOffset, bool highAccQ1, bool highAccQ3)\r\n        {\r\n            Id = id;\r\n            ModifiedSequence = precursorTextId.Target;\r\n            PeptideColor = peptideColor;\r\n            Q1 = precursorTextId.PrecursorMz;\r\n            Extractor = precursorTextId.Extractor;\r\n            MinTime = minTime;\r\n            MaxTime = maxTime;\r\n            IonMobilityInfo = precursorTextId.IonMobility;\r\n            HighEnergyIonMobilityValueOffset = highEnergyIonMobilityValueOffset;\r\n            MinIonMobilityValue = IonMobilityInfo.IsEmpty ? null : IonMobilityInfo.IonMobility.Mobility - (IonMobilityInfo.IonMobilityExtractionWindowWidth??0)/2;\r\n            MaxIonMobilityValue = IonMobilityInfo.IsEmpty ? null : MinIonMobilityValue + (IonMobilityInfo.IonMobilityExtractionWindowWidth ?? 0);\r\n            HighAccQ1 = highAccQ1;\r\n            HighAccQ3 = highAccQ3;\r\n\r\n            Ms1ProductFilters = SimProductFilters = Ms2ProductFilters = EMPTY_FILTERS;\r\n\r\n            if (Q1 == 0)\r\n            {\r\n                Ms1ProductFilters = new[] {new SpectrumProductFilter(SignedMz.ZERO, 0)};\r\n                SimProductFilters = Ms1ProductFilters;  // We want TIC and BPC for all scans, even if they have narrow machine settings and look like SIM\r\n            }\r\n        }\r\n\r\n        public SpectrumFilterPair(ChromatogramRequestDocumentChromatogramGroup requestGroup)\r\n        {\r\n            Q1 = new SignedMz(requestGroup.PrecursorMz);\r\n            ModifiedSequence = new Target(requestGroup.ModifiedSequence);\r\n            switch (requestGroup.Extractor)\r\n            {\r\n                case RemoteApi.GeneratedCode.ChromExtractor.BasePeak:\r\n                    Extractor = ChromExtractor.base_peak;\r\n                    break;\r\n                case RemoteApi.GeneratedCode.ChromExtractor.Summed:\r\n                    Extractor = ChromExtractor.summed;\r\n                    break;\r\n            }\r\n            if (requestGroup.MinTimeSpecified)\r\n            {\r\n                MinTime = requestGroup.MinTime;\r\n            }\r\n            if (requestGroup.MaxTimeSpecified)\r\n            {\r\n                MaxTime = requestGroup.MaxTime;\r\n            }\r\n            switch (requestGroup.Source)\r\n            {\r\n                case RemoteApi.GeneratedCode.ChromSource.Ms1:\r\n                    Ms1ProductFilters = requestGroup.Chromatogram.Select(\r\n                        product => new SpectrumProductFilter(product.ProductMz, product.MzWindow)).ToArray();\r\n                    HighAccQ1 = requestGroup.MassErrors;\r\n                    break;\r\n                case RemoteApi.GeneratedCode.ChromSource.Ms2:\r\n                    Ms2ProductFilters = requestGroup.Chromatogram.Select(\r\n                        product => new SpectrumProductFilter(product.ProductMz, product.MzWindow)).ToArray();\r\n                    HighAccQ3 = requestGroup.MassErrors;\r\n                    break;\r\n            }\r\n        }\r\n\r\n        public int Id { get; private set; }\r\n        public ChromExtractor Extractor { get; private set; }\r\n        public bool HighAccQ1 { get; private set; }\r\n        public bool HighAccQ3 { get; private set; }\r\n        public Target ModifiedSequence { get; private set; }\r\n        public Color PeptideColor { get; private set; }\r\n        public SignedMz Q1 { get; private set; }\r\n        public double? MinTime { get; private set; }\r\n        public double? MaxTime { get; private set; }\r\n        public double? MinIonMobilityValue { get; set; }\r\n        public double? MaxIonMobilityValue { get; set; }\r\n        private double HighEnergyIonMobilityValueOffset { get; set; }\r\n        private IonMobilityFilter IonMobilityInfo { get; set; }\r\n        private SpectrumProductFilter[] Ms1ProductFilters { get; set; }\r\n        private SpectrumProductFilter[] SimProductFilters { get; set; }\r\n        public SpectrumProductFilter[] Ms2ProductFilters { get; set; }\r\n\r\n        public int AddQ1FilterValues(IEnumerable<SignedMz> filterValues, Func<double, double> getFilterWindow)\r\n        {\r\n            int filterCount = AddFilterValues(MergeFilters(Ms1ProductFilters, filterValues).Distinct(),\r\n                getFilterWindow, filters => Ms1ProductFilters = filters);\r\n            // Make complete copies for SIM scans. Some day these may be different.\r\n            SimProductFilters = Ms1ProductFilters.Select(f => new SpectrumProductFilter(f.TargetMz, f.FilterWidth)).ToArray();\r\n            return filterCount * 2;\r\n        }\r\n\r\n        public int AddQ3FilterValues(IEnumerable<SignedMz> filterValues, Func<double, double> getFilterWindow)\r\n        {\r\n            return AddFilterValues(MergeFilters(Ms2ProductFilters, filterValues).Distinct(),\r\n                getFilterWindow, filters => Ms2ProductFilters = filters);\r\n        }\r\n\r\n        private static IEnumerable<SignedMz> MergeFilters(IEnumerable<SpectrumProductFilter> existing, IEnumerable<SignedMz> added)\r\n        {\r\n            if (existing == null)\r\n                return added;\r\n            return existing.Select(f => f.TargetMz).Union(added);\r\n        }\r\n\r\n        private int AddFilterValues(IEnumerable<SignedMz> filterValues,\r\n                                            Func<double, double> getFilterWindow,\r\n                                            Action<SpectrumProductFilter[]> setFilters)\r\n        {\r\n            var arrayFilters = filterValues.OrderBy(mz => mz)\r\n                .Select(mz => new SpectrumProductFilter(mz, getFilterWindow(mz)))\r\n                .ToArray();\r\n            setFilters(arrayFilters);\r\n            return arrayFilters.Length;\r\n        }\r\n\r\n        public ExtractedSpectrum FilterQ1SpectrumList(MsDataSpectrum[] spectra, bool isSimSpectra = false)\r\n        {\r\n            var filters = isSimSpectra ? SimProductFilters : Ms1ProductFilters;\r\n            return FilterSpectrumList(spectra, filters, HighAccQ1, false);\r\n        }\r\n\r\n        public ExtractedSpectrum FilterQ3SpectrumList(MsDataSpectrum[] spectra, bool useDriftTimeHighEnergyOffset)\r\n        {\r\n            // All-ions extraction for MS1 scans only\r\n            if (Q1 == 0)\r\n                return null;\r\n\r\n            return FilterSpectrumList(spectra, Ms2ProductFilters, HighAccQ3, useDriftTimeHighEnergyOffset);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Apply the filter to a list of spectra.  In \"normal\" operation\r\n        /// this list has a length of one. For ion mobility data it\r\n        /// may be a list of spectra with the same retention time but\r\n        /// different ion mobility values. For Agilent Mse data it may be\r\n        /// a list of MS2 spectra that need averaging (or even a list\r\n        /// of MS2 spectra with mixed retention and ion mobility values).  Averaging\r\n        /// is done by unique retention time count, rather than by spectrum\r\n        /// count, so that ion mobility data ion counts are additive (we're\r\n        /// trying to measure ions per injection, basically).\r\n        /// </summary>\r\n        private ExtractedSpectrum FilterSpectrumList(MsDataSpectrum[] spectra,\r\n            SpectrumProductFilter[] productFilters, bool highAcc, bool useDriftTimeHighEnergyOffset)\r\n        {\r\n            int targetCount = 1;\r\n            if (Q1 == 0)\r\n                highAcc = false;    // No mass error for all-ions extraction\r\n            else\r\n            {\r\n                if (productFilters.Length == 0)\r\n                    return null;\r\n                targetCount = productFilters.Length;\r\n            }\r\n\r\n            float[] extractedIntensities = new float[targetCount];\r\n            float[] massErrors = highAcc ? new float[targetCount] : null;\r\n            double[] meanErrors = highAcc ? new double[targetCount] : null;\r\n\r\n            int spectrumCount = 0;\r\n            int rtCount = 0;\r\n            double lastRT = 0;\r\n            var specIndexFirst = 0;\r\n            var specIndexLast = spectra.Length;\r\n            if (specIndexLast > 1 && MinIonMobilityValue.HasValue)\r\n            {\r\n                // Only inspect the range of spectra that match our ion mobility window\r\n                var im0 = spectra[0].IonMobility.Mobility;\r\n                var im1 = spectra[1].IonMobility.Mobility;\r\n                if (im0.HasValue && im1.HasValue)\r\n                {\r\n                    var offset = useDriftTimeHighEnergyOffset ? HighEnergyIonMobilityValueOffset : 0;\r\n\r\n                    if (im0 < im1)\r\n                    {\r\n                        // Binary search for first spectrum in ascending ion mobility range (as in drift time)\r\n                        var im = MinIonMobilityValue.Value + offset;\r\n                        specIndexFirst = CollectionUtil.BinarySearch(spectra, s => (s.IonMobility.Mobility ?? 0).CompareTo(im), true);\r\n                    }\r\n                    else if (im0 > im1)\r\n                    {\r\n                        // Binary search for first spectrum in descending ion mobility range (as in TIMS)\r\n                        var im = MaxIonMobilityValue.Value + offset;\r\n                        specIndexFirst = CollectionUtil.BinarySearch(spectra, s => im.CompareTo(s.IonMobility.Mobility ?? 0), true);\r\n                    }\r\n\r\n                    if (specIndexFirst < 0)\r\n                    {\r\n                        specIndexFirst = ~specIndexFirst;\r\n                    }\r\n                }\r\n            }\r\n            for (var specIndex = specIndexFirst; specIndex < specIndexLast; specIndex++)\r\n            {\r\n                var spectrum = spectra[specIndex];\r\n                // If these are spectra from distinct retention times, average them.\r\n                // Note that for ion mobility data we will see fewer retention time changes \r\n                // than the total spectra count - ascending DT within each RT.  Within a\r\n                // single retention time the ions are additive.\r\n                var rt = spectrum.RetentionTime ?? 0;\r\n                if (lastRT != rt)\r\n                {\r\n                    rtCount++;\r\n                    lastRT = rt;\r\n                }\r\n\r\n                // Filter on scan polarity\r\n                if (Q1.IsNegative != spectrum.NegativeCharge)\r\n                    continue;\r\n                spectrumCount++;\r\n\r\n                // Filter on ion mobility, if any\r\n                if (!ContainsIonMobilityValue(spectrum.IonMobility, useDriftTimeHighEnergyOffset))\r\n                {\r\n                    if (specIndex > specIndexFirst && specIndexFirst > 0)\r\n                    {\r\n                        break; // We have left the range of interesting ion mobilities\r\n                    }\r\n                    continue;\r\n                }\r\n                var mzArray = spectrum.Mzs;\r\n                if ((mzArray == null) || (mzArray.Length==0))\r\n                    continue;\r\n\r\n                // It's not unusual for mzarray and centerArray to have no overlap, esp. with ion mobility data\r\n                if (Q1 != 0)\r\n                {\r\n                    var lastProductFilter = productFilters[targetCount - 1];\r\n                    if ((lastProductFilter.TargetMz.Value + lastProductFilter.FilterWidth/2) < mzArray[0])\r\n                        continue;\r\n                }\r\n\r\n                var intensityArray = spectrum.Intensities;\r\n\r\n                // Search for matching peaks for each Q3 filter\r\n                // Use binary search to get to the first m/z value to be considered more quickly\r\n                // This should help MS1 where isotope distributions will be very close in m/z\r\n                // It should also help MS/MS when more selective, larger fragment ions are used,\r\n                // since then a lot of less selective, smaller peaks must be skipped\r\n                int iPeak = 0;\r\n                for (int targetIndex = 0; targetIndex < targetCount; targetIndex++)\r\n                {\r\n                    // Look for the first peak that is greater than the start of the filter\r\n                    double targetMz = 0, endFilter = double.MaxValue;\r\n                    if (Q1 != 0)\r\n                    {\r\n                        var productFilter = productFilters[targetIndex];\r\n                        targetMz = productFilter.TargetMz;\r\n                        double filterWindow = productFilter.FilterWidth;\r\n                        double startFilter = targetMz - filterWindow / 2;\r\n                        endFilter = startFilter + filterWindow;\r\n\r\n                        if (iPeak < mzArray.Length)\r\n                        {\r\n                            iPeak = Array.BinarySearch(mzArray, iPeak, mzArray.Length - iPeak, startFilter);\r\n                            if (iPeak < 0)\r\n                                iPeak = ~iPeak;\r\n                        }\r\n                        if (iPeak >= mzArray.Length)\r\n                            break; // No further overlap\r\n                    }\r\n\r\n                    // Add the intensity values of all peaks that pass the filter\r\n                    double totalIntensity = extractedIntensities[targetIndex]; // Start with the value from the previous spectrum, if any\r\n                    double meanError =  highAcc ? meanErrors[targetIndex] : 0;\r\n                    for (int iNext = iPeak; iNext < mzArray.Length && mzArray[iNext] < endFilter; iNext++)\r\n                    {\r\n                        double mz = mzArray[iNext];\r\n                        double intensity = intensityArray[iNext];\r\n                    \r\n                        if (Extractor == ChromExtractor.summed)\r\n                            totalIntensity += intensity;\r\n                        else if (intensity > totalIntensity)\r\n                        {\r\n                            totalIntensity = intensity;\r\n                            meanError = 0;\r\n                        }\r\n\r\n                        // Accumulate weighted mean mass error for summed, or take a single\r\n                        // mass error of the most intense peak for base peak.\r\n                        if (highAcc && (Extractor == ChromExtractor.summed || meanError == 0))\r\n                        {\r\n                            if (totalIntensity > 0.0)\r\n                            {\r\n                                double deltaPeak = mz - targetMz;\r\n                                meanError += (deltaPeak - meanError) * intensity / totalIntensity;\r\n                            }\r\n                        }\r\n                    }\r\n                    extractedIntensities[targetIndex] = (float) totalIntensity;\r\n                    if (meanErrors != null)\r\n                        meanErrors[targetIndex] = meanError;\r\n                }\r\n                \r\n            }\r\n            if (spectrumCount == 0)\r\n            {\r\n                return null;\r\n            }\r\n            if (meanErrors != null)\r\n            {\r\n                for (int i = 0; i < targetCount; i++)\r\n                    massErrors[i] = (float)SequenceMassCalc.GetPpm(productFilters[i].TargetMz, meanErrors[i]);\r\n            }\r\n\r\n            // If we summed across spectra of different retention times, scale per\r\n            // unique retention time (but not per ion mobility value)\r\n            if ((Extractor == ChromExtractor.summed) && (rtCount > 1))\r\n            {\r\n                float scale = (float)(1.0 / rtCount);\r\n                for (int i = 0; i < targetCount; i++)\r\n                    extractedIntensities[i] *= scale;\r\n            }\r\n            var dtFilter = GetIonMobilityWindow(useDriftTimeHighEnergyOffset);\r\n            return new ExtractedSpectrum(ModifiedSequence,\r\n                PeptideColor,\r\n                Q1,\r\n                dtFilter, \r\n                Extractor,\r\n                Id,\r\n                productFilters,\r\n                extractedIntensities,\r\n                massErrors);\r\n        }\r\n\r\n        public int CompareTo(SpectrumFilterPair other)\r\n        {\r\n            return Comparer.Default.Compare(Q1, other.Q1);\r\n        }\r\n\r\n        public bool ContainsRetentionTime(double retentionTime)\r\n        {\r\n            return (!MinTime.HasValue || MinTime.Value <= retentionTime) &&\r\n                (!MaxTime.HasValue || MaxTime.Value >= retentionTime);\r\n        }\r\n\r\n        public IEnumerable<ChromatogramRequestDocumentChromatogramGroup> ToChromatogramRequestDocumentChromatogramGroups()\r\n        {\r\n            // TODO(bspratt) how to communicate scan polarity to Chorus?\r\n            if (null != Ms1ProductFilters)\r\n            {\r\n                var chromatograms = new List<ChromatogramRequestDocumentChromatogramGroupChromatogram>();\r\n                foreach (var spectrumProductFilter in Ms1ProductFilters)\r\n                {\r\n                    var product = new ChromatogramRequestDocumentChromatogramGroupChromatogram\r\n                    {\r\n                        ProductMz = spectrumProductFilter.TargetMz.RawValue, // Negative ion mode values serialize as negative numbers\r\n                        MzWindow = spectrumProductFilter.FilterWidth,\r\n                    };\r\n                    chromatograms.Add(product);\r\n                }\r\n                if (chromatograms.Count > 0)\r\n                {\r\n                    yield return MakeChromatogramRequestDocumentChromatogramGroup(ChromSource.ms1, HighAccQ1 && 0 != Q1, chromatograms);\r\n                }\r\n            }\r\n            if (null != Ms2ProductFilters)\r\n            {\r\n                var chromatograms = new List<ChromatogramRequestDocumentChromatogramGroupChromatogram>();\r\n                foreach (var spectrumProductFilter in Ms2ProductFilters)\r\n                {\r\n                    var product = new ChromatogramRequestDocumentChromatogramGroupChromatogram\r\n                    {\r\n                        ProductMz = spectrumProductFilter.TargetMz.RawValue, // Negative ion mode values serialize as negative numbers\r\n                        MzWindow = spectrumProductFilter.FilterWidth,\r\n                    };\r\n                    chromatograms.Add(product);\r\n                }\r\n                if (chromatograms.Count > 0)\r\n                {\r\n                    yield return\r\n                        MakeChromatogramRequestDocumentChromatogramGroup(ChromSource.fragment, HighAccQ3, chromatograms);\r\n                }\r\n            }\r\n        }\r\n\r\n        public IEnumerable<int> ProductFilterIds\r\n        {\r\n            get\r\n            {\r\n                foreach (var filters in new[] {Ms1ProductFilters, SimProductFilters, Ms2ProductFilters})\r\n                {\r\n                    foreach (var spectrumProductFilter in filters)\r\n                    {\r\n                        yield return spectrumProductFilter.FilterId;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n\r\n        public void AddChromKeys(List<ChromKey> listChromKeys)\r\n        {\r\n            AddChromKeys(ChromSource.ms1, Ms1ProductFilters, false, listChromKeys);\r\n            AddChromKeys(ChromSource.sim, SimProductFilters, false, listChromKeys);\r\n            AddChromKeys(ChromSource.fragment, Ms2ProductFilters, true, listChromKeys);\r\n        }\r\n\r\n        private void AddChromKeys(ChromSource source, SpectrumProductFilter[] productFilters, bool highEnergy,\r\n                                  List<ChromKey> listChromKeys)\r\n        {\r\n            if (null != productFilters)\r\n            {\r\n                var ionMobility = GetIonMobilityWindow(highEnergy);\r\n                foreach (var spectrumProductFilter in productFilters)\r\n                {\r\n                    spectrumProductFilter.FilterId = listChromKeys.Count;\r\n                    var key = new ChromKey(ModifiedSequence,\r\n                        Q1,\r\n                        ionMobility,\r\n                        spectrumProductFilter.TargetMz,\r\n                        0,  // CE value (Shimadzu SRM only)\r\n                        spectrumProductFilter.FilterWidth,\r\n                        source,\r\n                        Extractor,\r\n                        true,\r\n                        true,\r\n                        MinTime,\r\n                        MaxTime);\r\n                    listChromKeys.Add(key);\r\n                }\r\n            }\r\n        }\r\n\r\n        private ChromatogramRequestDocumentChromatogramGroup MakeChromatogramRequestDocumentChromatogramGroup(\r\n            ChromSource chromSource, bool calculateMassErrors, IEnumerable<ChromatogramRequestDocumentChromatogramGroupChromatogram> chromatograms)\r\n        {\r\n            ChromatogramRequestDocumentChromatogramGroup docFilterPair = new ChromatogramRequestDocumentChromatogramGroup\r\n            {\r\n                ModifiedSequence = ModifiedSequence != null ? ModifiedSequence.ToString() : null,\r\n                PrecursorMz = Q1.RawValue,  // A negative ion mode precursor will be serialized as a negative mz value\r\n                MassErrors = calculateMassErrors,\r\n            };\r\n            switch (Extractor)\r\n            {\r\n                case ChromExtractor.base_peak:\r\n                    docFilterPair.Extractor = RemoteApi.GeneratedCode.ChromExtractor.BasePeak;\r\n                    break;\r\n                case ChromExtractor.summed:\r\n                    docFilterPair.Extractor = RemoteApi.GeneratedCode.ChromExtractor.Summed;\r\n                    break;\r\n            }\r\n            if (MinTime.HasValue)\r\n            {\r\n                docFilterPair.MinTime = MinTime.Value;\r\n                docFilterPair.MinTimeSpecified = true;\r\n            }\r\n            if (MaxTime.HasValue)\r\n            {\r\n                docFilterPair.MaxTime = MaxTime.Value;\r\n                docFilterPair.MaxTimeSpecified = true;\r\n            }\r\n            if (MinIonMobilityValue.HasValue && MaxIonMobilityValue.HasValue)\r\n            {\r\n                docFilterPair.DriftTime = (MinIonMobilityValue.Value + MaxIonMobilityValue.Value)/2;\r\n                if (ChromSource.fragment == chromSource) // Use high energy offset for fragments\r\n                {\r\n                    docFilterPair.DriftTime += HighEnergyIonMobilityValueOffset;\r\n                }\r\n                docFilterPair.DriftTimeSpecified = true;\r\n                docFilterPair.DriftTimeWindow = MaxIonMobilityValue.Value - MinIonMobilityValue.Value;\r\n                docFilterPair.DriftTimeWindowSpecified = true;\r\n            }\r\n            switch (chromSource)\r\n            {\r\n                case ChromSource.ms1:\r\n                    docFilterPair.Source = RemoteApi.GeneratedCode.ChromSource.Ms1;\r\n                    break;\r\n                case ChromSource.fragment:\r\n                    docFilterPair.Source = RemoteApi.GeneratedCode.ChromSource.Ms2;\r\n                    break;\r\n                case ChromSource.sim:\r\n                    docFilterPair.Source = RemoteApi.GeneratedCode.ChromSource.Sim;\r\n                    break;\r\n            }\r\n            docFilterPair.Chromatogram = chromatograms.ToArray();\r\n            return docFilterPair;\r\n        }\r\n\r\n        public bool HasIonMobilityFAIMS()\r\n        {\r\n            return IonMobilityInfo.HasIonMobilityValue && \r\n                   IonMobilityInfo.IonMobility.Units == eIonMobilityUnits.compensation_V;\r\n        }\r\n\r\n        public bool ContainsIonMobilityValue(IonMobilityValue ionMobility, bool highEnergy)\r\n        {\r\n            if (!ionMobility.HasValue)\r\n                return true; // It doesn't NOT have the ion mobility, since there isn't one\r\n            double offset = highEnergy ? HighEnergyIonMobilityValueOffset : 0;\r\n            return (!MinIonMobilityValue.HasValue || MinIonMobilityValue.Value+offset <= ionMobility.Mobility) &&\r\n                (!MaxIonMobilityValue.HasValue || MaxIonMobilityValue.Value+offset >= ionMobility.Mobility);\r\n        }\r\n\r\n        public IonMobilityFilter GetIonMobilityWindow(bool highEnergy)\r\n        {\r\n            if (MinIonMobilityValue.HasValue && MaxIonMobilityValue.HasValue)\r\n            {\r\n                // High energy (product ion) scans may have a faster ion mobility, as in Waters MsE\r\n                double offset = highEnergy ? HighEnergyIonMobilityValueOffset : 0;\r\n                var width = MaxIonMobilityValue.Value - MinIonMobilityValue.Value;\r\n                var center = offset + MinIonMobilityValue.Value + 0.5*width;\r\n                return IonMobilityFilter.GetIonMobilityFilter(IonMobilityValue.GetIonMobilityValue(center, IonMobilityInfo.IonMobility.Units), width, IonMobilityInfo.CollisionalCrossSectionSqA);\r\n            }\r\n            else\r\n            {\r\n                return IonMobilityFilter.EMPTY;\r\n            }\r\n        }\r\n\r\n        public bool MatchesDdaPrecursor(SignedMz precursorMz)\r\n        {\r\n            return Ms1ProductFilters.Any(filter => 0 == filter.TargetMz.CompareTolerant(precursorMz, filter.FilterWidth));\r\n        }\r\n    }\r\n\r\n    public class SpectrumProductFilter\r\n    {\r\n        public SpectrumProductFilter(double targetMz, double filterWidth) :\r\n            this(new SignedMz(targetMz), filterWidth)\r\n        {\r\n        }\r\n\r\n        public SpectrumProductFilter(SignedMz targetMz, double filterWidth)\r\n        {\r\n            TargetMz = targetMz;\r\n            FilterWidth = filterWidth;\r\n        }\r\n\r\n        public SignedMz TargetMz { get; private set; }\r\n        public double FilterWidth { get; private set; }\r\n        public int FilterId { get; set; }\r\n\r\n        #region object overrides\r\n\r\n        protected bool Equals(SpectrumProductFilter other)\r\n        {\r\n            return TargetMz.Equals(other.TargetMz) && FilterWidth.Equals(other.FilterWidth) && FilterId == other.FilterId;\r\n        }\r\n\r\n        public override bool Equals(object obj)\r\n        {\r\n            if (ReferenceEquals(null, obj)) return false;\r\n            if (ReferenceEquals(this, obj)) return true;\r\n            if (obj.GetType() != GetType()) return false;\r\n            return Equals((SpectrumProductFilter) obj);\r\n        }\r\n\r\n        public override int GetHashCode()\r\n        {\r\n            unchecked\r\n            {\r\n                var hashCode = TargetMz.GetHashCode();\r\n                hashCode = (hashCode*397) ^ FilterWidth.GetHashCode();\r\n                hashCode = (hashCode*397) ^ FilterId;\r\n                return hashCode;\r\n            }\r\n        }\r\n\r\n        #endregion\r\n    }\r\n}", "idx": 7, "id": 12635, "msg": "", "proj": "ProteoWizard-pwiz", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -58,24 +58,20 @@ void Graph::bfs(int starting)\n \n int main()\n {\n-    // Number of vertices is 8\n-    Graph graph(8);\n+    // Number of vertices\n+    int n;\n+    cin >> n;\n+    Graph graph(n);\n \n+    int edges;\n+    cin >> edges;\n     // Create edges between vertices\n-    graph.addEdge(0, 1);\n-    graph.addEdge(0, 2);\n-    graph.addEdge(1, 2);\n-    graph.addEdge(1, 4);\n-    graph.addEdge(2, 0);\n-    graph.addEdge(2, 3);\n-    graph.addEdge(3, 3);\n-    graph.addEdge(3, 6);\n-    graph.addEdge(4, 0);\n-    graph.addEdge(4, 5);\n-    graph.addEdge(5, 6);\n-    graph.addEdge(5, 7);\n-    graph.addEdge(6, 2);\n-    graph.addEdge(7, 3);\n+    int u, v;\n+    for (int i = 0; i < edges; i++)\n+    {\n+        cin >> u >> v;\n+        graph.addEdge(u, v);\n+    }\n \n     cout << \"Breadth First Traversal is : \";\n     graph.bfs(0);", "y": 1, "oldf": "#include <iostream>\n#include <vector>\n#include <queue>\n\nusing namespace std;\n\nclass Graph\n{\n    int numberVertex;\n    vector <int> *adjacency;\n\n  public:\n    // Constructor to initialise graph\n    Graph(int numberVertex)\n    {\n        this->numberVertex = numberVertex;\n        adjacency = new vector <int> [numberVertex];\n    }\n\n    // Function to add edge between source and destination\n    void addEdge(int source, int destination)\n    {\n        adjacency[source].push_back(destination);\n    }\n\n    // Function to perform Breadth First Search\n    void bfs(int starting);\n};\n\nvoid Graph::bfs(int starting)\n{\n    bool visited[numberVertex];\n\n    for (int i = 0; i < numberVertex; i++)\n        visited[i] = false;\n\n    queue <int> queue_vertex;\n\n    visited[starting] = true;\n    queue_vertex.push(starting);\n\n    while (!queue_vertex.empty())\n    {\n        starting = queue_vertex.front();\n        cout << starting << \" \";\n        queue_vertex.pop();\n\n        for (vector <int> :: iterator it = adjacency[starting].begin(); it != adjacency[starting].end(); ++it)\n        {\n            if(!visited[*it])\n            {\n                visited[*it] = true;\n                queue_vertex.push(*it);\n            }\n        }\n    }\n}\n\nint main()\n{\n    // Number of vertices is 8\n    Graph graph(8);\n\n    // Create edges between vertices\n    graph.addEdge(0, 1);\n    graph.addEdge(0, 2);\n    graph.addEdge(1, 2);\n    graph.addEdge(1, 4);\n    graph.addEdge(2, 0);\n    graph.addEdge(2, 3);\n    graph.addEdge(3, 3);\n    graph.addEdge(3, 6);\n    graph.addEdge(4, 0);\n    graph.addEdge(4, 5);\n    graph.addEdge(5, 6);\n    graph.addEdge(5, 7);\n    graph.addEdge(6, 2);\n    graph.addEdge(7, 3);\n\n    cout << \"Breadth First Traversal is : \";\n    graph.bfs(0);\n\n    return 0;\n}\n\n\n/* Output\n\nBreadth First Traversal is : 0 1 2 4 3 5 6 7\n\n*/\n", "idx": 1, "id": 13092, "msg": "Please remove the space before `:`", "proj": "jainaman224-Algo_Ds_Notes", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -51,11 +51,24 @@ bool Print(T val, Type type, int /*indent*/, Type * /*union_type*/,\n            const IDLOptions &opts, std::string *_text) {\n   std::string &text = *_text;\n   if (type.enum_def && opts.output_enum_identifiers) {\n-    auto ev = type.enum_def->ReverseLookup(static_cast<int64_t>(val));\n-    if (ev) {\n-      text += \"\\\"\";\n-      text += ev->name;\n-      text += \"\\\"\";\n+    std::vector<EnumVal const *> enum_values;\n+    if (auto ev = type.enum_def->ReverseLookup(static_cast<int64_t>(val))) {\n+      enum_values.push_back(ev);\n+    }\n+    else if (val && type.enum_def->attributes.Lookup(\"bit_flags\")) {\n+      for (auto it = type.enum_def->Vals().begin(),\n+                e = type.enum_def->Vals().end();\n+           it != e; ++it) {\n+        if ((*it)->GetAsInt64() & static_cast<int64_t>(val))\n+          enum_values.push_back(*it);\n+      }\n+    }\n+\n+    if (!enum_values.empty()) {\n+      text += '\\\"';\n+      for(auto it = enum_values.begin(), e = enum_values.end(); it != e; ++it)\n+        text += (*it)->name + ' ';\n+      text[text.length() - 1] = '\\\"';\n       return true;\n     }\n   }", "y": 1, "oldf": "/*\n * Copyright 2014 Google Inc. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// independent from idl_parser, since this code is not needed for most clients\n\n#include \"flatbuffers/flatbuffers.h\"\n#include \"flatbuffers/flexbuffers.h\"\n#include \"flatbuffers/idl.h\"\n#include \"flatbuffers/util.h\"\n\nnamespace flatbuffers {\n\nstatic bool GenStruct(const StructDef &struct_def, const Table *table,\n                      int indent, const IDLOptions &opts, std::string *_text);\n\n// If indentation is less than 0, that indicates we don't want any newlines\n// either.\nconst char *NewLine(const IDLOptions &opts) {\n  return opts.indent_step >= 0 ? \"\\n\" : \"\";\n}\n\nint Indent(const IDLOptions &opts) { return std::max(opts.indent_step, 0); }\n\n// Output an identifier with or without quotes depending on strictness.\nvoid OutputIdentifier(const std::string &name, const IDLOptions &opts,\n                      std::string *_text) {\n  std::string &text = *_text;\n  if (opts.strict_json) text += \"\\\"\";\n  text += name;\n  if (opts.strict_json) text += \"\\\"\";\n}\n\n// Print (and its template specialization below for pointers) generate text\n// for a single FlatBuffer value into JSON format.\n// The general case for scalars:\ntemplate<typename T>\nbool Print(T val, Type type, int /*indent*/, Type * /*union_type*/,\n           const IDLOptions &opts, std::string *_text) {\n  std::string &text = *_text;\n  if (type.enum_def && opts.output_enum_identifiers) {\n    auto ev = type.enum_def->ReverseLookup(static_cast<int64_t>(val));\n    if (ev) {\n      text += \"\\\"\";\n      text += ev->name;\n      text += \"\\\"\";\n      return true;\n    }\n  }\n\n  if (type.base_type == BASE_TYPE_BOOL) {\n    text += val != 0 ? \"true\" : \"false\";\n  } else {\n    text += NumToString(val);\n  }\n\n  return true;\n}\n\n// Print a vector or an array of JSON values, comma seperated, wrapped in \"[]\".\ntemplate<typename T, typename Container>\nbool PrintContainer(const Container &c, size_t size, Type type, int indent,\n                    const IDLOptions &opts, std::string *_text) {\n  std::string &text = *_text;\n  text += \"[\";\n  text += NewLine(opts);\n  for (uoffset_t i = 0; i < size; i++) {\n    if (i) {\n      if (!opts.protobuf_ascii_alike) text += \",\";\n      text += NewLine(opts);\n    }\n    text.append(indent + Indent(opts), ' ');\n    if (IsStruct(type)) {\n      if (!Print(reinterpret_cast<const void *>(c.Data() +\n                                                i * type.struct_def->bytesize),\n                 type, indent + Indent(opts), nullptr, opts, _text)) {\n        return false;\n      }\n    } else {\n      if (!Print(c[i], type, indent + Indent(opts), nullptr, opts, _text)) {\n        return false;\n      }\n    }\n  }\n  text += NewLine(opts);\n  text.append(indent, ' ');\n  text += \"]\";\n  return true;\n}\n\ntemplate<typename T>\nbool PrintVector(const Vector<T> &v, Type type, int indent,\n                 const IDLOptions &opts, std::string *_text) {\n  return PrintContainer<T, Vector<T>>(v, v.size(), type, indent, opts, _text);\n}\n\n// Print an array a sequence of JSON values, comma separated, wrapped in \"[]\".\ntemplate<typename T>\nbool PrintArray(const Array<T, 0xFFFF> &a, size_t size, Type type, int indent,\n                const IDLOptions &opts, std::string *_text) {\n  return PrintContainer<T, Array<T, 0xFFFF>>(a, size, type, indent, opts,\n                                             _text);\n}\n\n// Specialization of Print above for pointer types.\ntemplate<>\nbool Print<const void *>(const void *val, Type type, int indent,\n                         Type *union_type, const IDLOptions &opts,\n                         std::string *_text) {\n  switch (type.base_type) {\n    case BASE_TYPE_UNION:\n      // If this assert hits, you have an corrupt buffer, a union type field\n      // was not present or was out of range.\n      FLATBUFFERS_ASSERT(union_type);\n      return Print<const void *>(val, *union_type, indent, nullptr, opts,\n                                 _text);\n    case BASE_TYPE_STRUCT:\n      if (!GenStruct(*type.struct_def, reinterpret_cast<const Table *>(val),\n                     indent, opts, _text)) {\n        return false;\n      }\n      break;\n    case BASE_TYPE_STRING: {\n      auto s = reinterpret_cast<const String *>(val);\n      if (!EscapeString(s->c_str(), s->size(), _text, opts.allow_non_utf8,\n                        opts.natural_utf8)) {\n        return false;\n      }\n      break;\n    }\n    case BASE_TYPE_VECTOR: {\n      const auto vec_type = type.VectorType();\n      // Call PrintVector above specifically for each element type:\n      // clang-format off\n      switch (vec_type.base_type) {\n        #define FLATBUFFERS_TD(ENUM, IDLTYPE, \\\n          CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, RTYPE, KTYPE) \\\n          case BASE_TYPE_ ## ENUM: \\\n            if (!PrintVector<CTYPE>( \\\n                  *reinterpret_cast<const Vector<CTYPE> *>(val), \\\n                  vec_type, indent, opts, _text)) { \\\n              return false; \\\n            } \\\n            break;\n          FLATBUFFERS_GEN_TYPES(FLATBUFFERS_TD)\n        #undef FLATBUFFERS_TD\n      }\n      // clang-format on\n      break;\n    }\n    case BASE_TYPE_ARRAY: {\n      const auto vec_type = type.VectorType();\n      // Call PrintArray above specifically for each element type:\n      // clang-format off\n      switch (vec_type.base_type) {\n        #define FLATBUFFERS_TD(ENUM, IDLTYPE, \\\n        CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, RTYPE, KTYPE) \\\n        case BASE_TYPE_ ## ENUM: \\\n          if (!PrintArray<CTYPE>( \\\n              *reinterpret_cast<const Array<CTYPE, 0xFFFF> *>(val), \\\n              type.fixed_length, \\\n              vec_type, indent, opts, _text)) { \\\n          return false; \\\n          } \\\n          break;\n        FLATBUFFERS_GEN_TYPES_SCALAR(FLATBUFFERS_TD)\n        FLATBUFFERS_GEN_TYPES_POINTER(FLATBUFFERS_TD)\n        #undef FLATBUFFERS_TD\n        case BASE_TYPE_ARRAY: FLATBUFFERS_ASSERT(0);\n      }\n      // clang-format on\n      break;\n    }\n    default: FLATBUFFERS_ASSERT(0);\n  }\n  return true;\n}\n\ntemplate<typename T> static T GetFieldDefault(const FieldDef &fd) {\n  T val;\n  auto check = StringToNumber(fd.value.constant.c_str(), &val);\n  (void)check;\n  FLATBUFFERS_ASSERT(check);\n  return val;\n}\n\n// Generate text for a scalar field.\ntemplate<typename T>\nstatic bool GenField(const FieldDef &fd, const Table *table, bool fixed,\n                     const IDLOptions &opts, int indent, std::string *_text) {\n  return Print(\n      fixed ? reinterpret_cast<const Struct *>(table)->GetField<T>(\n                  fd.value.offset)\n            : table->GetField<T>(fd.value.offset, GetFieldDefault<T>(fd)),\n      fd.value.type, indent, nullptr, opts, _text);\n}\n\nstatic bool GenStruct(const StructDef &struct_def, const Table *table,\n                      int indent, const IDLOptions &opts, std::string *_text);\n\n// Generate text for non-scalar field.\nstatic bool GenFieldOffset(const FieldDef &fd, const Table *table, bool fixed,\n                           int indent, Type *union_type, const IDLOptions &opts,\n                           std::string *_text) {\n  const void *val = nullptr;\n  if (fixed) {\n    // The only non-scalar fields in structs are structs or arrays.\n    FLATBUFFERS_ASSERT(IsStruct(fd.value.type) || IsArray(fd.value.type));\n    val = reinterpret_cast<const Struct *>(table)->GetStruct<const void *>(\n        fd.value.offset);\n  } else if (fd.flexbuffer) {\n    auto vec = table->GetPointer<const Vector<uint8_t> *>(fd.value.offset);\n    auto root = flexbuffers::GetRoot(vec->data(), vec->size());\n    root.ToString(true, opts.strict_json, *_text);\n    return true;\n  } else if (fd.nested_flatbuffer) {\n    auto vec = table->GetPointer<const Vector<uint8_t> *>(fd.value.offset);\n    auto root = GetRoot<Table>(vec->data());\n    return GenStruct(*fd.nested_flatbuffer, root, indent, opts, _text);\n  } else {\n    val = IsStruct(fd.value.type)\n              ? table->GetStruct<const void *>(fd.value.offset)\n              : table->GetPointer<const void *>(fd.value.offset);\n  }\n  return Print(val, fd.value.type, indent, union_type, opts, _text);\n}\n\n// Generate text for a struct or table, values separated by commas, indented,\n// and bracketed by \"{}\"\nstatic bool GenStruct(const StructDef &struct_def, const Table *table,\n                      int indent, const IDLOptions &opts, std::string *_text) {\n  std::string &text = *_text;\n  text += \"{\";\n  int fieldout = 0;\n  Type *union_type = nullptr;\n  for (auto it = struct_def.fields.vec.begin();\n       it != struct_def.fields.vec.end(); ++it) {\n    FieldDef &fd = **it;\n    auto is_present = struct_def.fixed || table->CheckField(fd.value.offset);\n    auto output_anyway = opts.output_default_scalars_in_json &&\n                         IsScalar(fd.value.type.base_type) && !fd.deprecated;\n    if (is_present || output_anyway) {\n      if (fieldout++) {\n        if (!opts.protobuf_ascii_alike) text += \",\";\n      }\n      text += NewLine(opts);\n      text.append(indent + Indent(opts), ' ');\n      OutputIdentifier(fd.name, opts, _text);\n      if (!opts.protobuf_ascii_alike ||\n          (fd.value.type.base_type != BASE_TYPE_STRUCT &&\n           fd.value.type.base_type != BASE_TYPE_VECTOR))\n        text += \":\";\n      text += \" \";\n      switch (fd.value.type.base_type) {\n          // clang-format off\n          #define FLATBUFFERS_TD(ENUM, IDLTYPE, \\\n            CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, RTYPE, KTYPE) \\\n            case BASE_TYPE_ ## ENUM: \\\n              if (!GenField<CTYPE>(fd, table, struct_def.fixed, \\\n                                   opts, indent + Indent(opts), _text)) { \\\n                return false; \\\n              } \\\n              break;\n          FLATBUFFERS_GEN_TYPES_SCALAR(FLATBUFFERS_TD)\n        #undef FLATBUFFERS_TD\n        // Generate drop-thru case statements for all pointer types:\n        #define FLATBUFFERS_TD(ENUM, IDLTYPE, \\\n          CTYPE, JTYPE, GTYPE, NTYPE, PTYPE, RTYPE, KTYPE) \\\n          case BASE_TYPE_ ## ENUM:\n          FLATBUFFERS_GEN_TYPES_POINTER(FLATBUFFERS_TD)\n          FLATBUFFERS_GEN_TYPE_ARRAY(FLATBUFFERS_TD)\n        #undef FLATBUFFERS_TD\n            if (!GenFieldOffset(fd, table, struct_def.fixed, indent + Indent(opts),\n                                union_type, opts, _text)) {\n              return false;\n            }\n            break;\n          // clang-format on\n      }\n      if (fd.value.type.base_type == BASE_TYPE_UTYPE) {\n        auto enum_val = fd.value.type.enum_def->ReverseLookup(\n            table->GetField<uint8_t>(fd.value.offset, 0), true);\n        union_type = enum_val ? &enum_val->union_type : nullptr;\n      }\n    }\n  }\n  text += NewLine(opts);\n  text.append(indent, ' ');\n  text += \"}\";\n  return true;\n}\n\n// Generate a text representation of a flatbuffer in JSON format.\nbool GenerateTextFromTable(const Parser &parser, const void *table,\n                           const std::string &table_name, std::string *_text) {\n  auto struct_def = parser.LookupStruct(table_name);\n  if (struct_def == nullptr) {\n    return false;\n  }\n  auto &text = *_text;\n  text.reserve(1024);  // Reduce amount of inevitable reallocs.\n  auto root = static_cast<const Table *>(table);\n  if (!GenStruct(*struct_def, root, 0, parser.opts, &text)) {\n    return false;\n  }\n  text += NewLine(parser.opts);\n  return true;\n}\n\n// Generate a text representation of a flatbuffer in JSON format.\nbool GenerateText(const Parser &parser, const void *flatbuffer,\n                  std::string *_text) {\n  std::string &text = *_text;\n  FLATBUFFERS_ASSERT(parser.root_struct_def_);  // call SetRootType()\n  text.reserve(1024);               // Reduce amount of inevitable reallocs.\n  auto root = parser.opts.size_prefixed ?\n      GetSizePrefixedRoot<Table>(flatbuffer) : GetRoot<Table>(flatbuffer);\n  if (!GenStruct(*parser.root_struct_def_, root, 0, parser.opts, _text)) {\n    return false;\n  }\n  text += NewLine(parser.opts);\n  return true;\n}\n\nstd::string TextFileName(const std::string &path,\n                         const std::string &file_name) {\n  return path + file_name + \".json\";\n}\n\nbool GenerateTextFile(const Parser &parser, const std::string &path,\n                      const std::string &file_name) {\n  if (!parser.builder_.GetSize() || !parser.root_struct_def_) return true;\n  std::string text;\n  if (!GenerateText(parser, parser.builder_.GetBufferPointer(), &text)) {\n    return false;\n  }\n  return flatbuffers::SaveFile(TextFileName(path, file_name).c_str(), text,\n                               false);\n}\n\nstd::string TextMakeRule(const Parser &parser, const std::string &path,\n                         const std::string &file_name) {\n  if (!parser.builder_.GetSize() || !parser.root_struct_def_) return \"\";\n  std::string filebase =\n      flatbuffers::StripPath(flatbuffers::StripExtension(file_name));\n  std::string make_rule = TextFileName(path, filebase) + \": \" + file_name;\n  auto included_files =\n      parser.GetIncludedFilesRecursive(parser.root_struct_def_->file);\n  for (auto it = included_files.begin(); it != included_files.end(); ++it) {\n    make_rule += \" \" + *it;\n  }\n  return make_rule;\n}\n\n}  // namespace flatbuffers\n", "idx": 1, "id": 16292, "msg": "`uint64_t` will be better here to avoid overflow (UBSAN) if `T` of `val` is uint64_t. By the grammar, `bit_flag` enums should have unsigned underlying type.", "proj": "google-flatbuffers", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -15,7 +15,6 @@ end\n I18n.load_path += Dir[File.join(mydir, 'locales', '**/*.yml')]\n I18n.reload! if I18n.backend.initialized?\n \n-\n module Faker\n   class Config\n     @locale = nil", "y": 0, "oldf": "# -*- coding: utf-8 -*-\nmydir = File.expand_path(File.dirname(__FILE__))\n\nbegin\n  require 'psych'\nrescue LoadError\nend\n\nrequire 'i18n'\nrequire 'set' # Fixes a bug in i18n 0.6.11\n\nif I18n.respond_to?(:enforce_available_locales=)\n  I18n.enforce_available_locales = true\nend\nI18n.load_path += Dir[File.join(mydir, 'locales', '**/*.yml')]\nI18n.reload! if I18n.backend.initialized?\n\n\nmodule Faker\n  class Config\n    @locale = nil\n    @random = nil\n\n    class << self\n      attr_writer :locale\n      attr_writer :random\n\n      def locale\n        @locale || I18n.locale\n      end\n\n      def own_locale\n        @locale\n      end\n\n      def random\n        @random || Random::DEFAULT\n      end\n    end\n  end\n\n  class Base\n    Numbers = Array(0..9)\n    ULetters = Array('A'..'Z')\n    Letters = ULetters + Array('a'..'z')\n\n    class << self\n      ## make sure numerify results doesn\u2019t start with a zero\n      def numerify(number_string)\n        number_string.sub(/#/) { (rand(9)+1).to_s }.gsub(/#/) { rand(10).to_s }\n      end\n\n      def letterify(letter_string)\n        letter_string.gsub(/\\?/) { sample(ULetters) }\n      end\n\n      def bothify(string)\n        letterify(numerify(string))\n      end\n\n      # Given a regular expression, attempt to generate a string\n      # that would match it.  This is a rather simple implementation,\n      # so don't be shocked if it blows up on you in a spectacular fashion.\n      #\n      # It does not handle ., *, unbounded ranges such as {1,},\n      # extensions such as (?=), character classes, some abbreviations\n      # for character classes, and nested parentheses.\n      #\n      # I told you it was simple. :) It's also probably dog-slow,\n      # so you shouldn't use it.\n      #\n      # It will take a regex like this:\n      #\n      # /^[A-PR-UWYZ0-9][A-HK-Y0-9][AEHMNPRTVXY0-9]?[ABEHMNPRVWXY0-9]? {1,2}[0-9][ABD-HJLN-UW-Z]{2}$/\n      #\n      # and generate a string like this:\n      #\n      # \"U3V  3TP\"\n      #\n      def regexify(re)\n        re = re.source if re.respond_to?(:source) # Handle either a Regexp or a String that looks like a Regexp\n        re.\n          gsub(/^\\/?\\^?/, '').gsub(/\\$?\\/?$/, '').                                                                      # Ditch the anchors\n          gsub(/\\{(\\d+)\\}/, '{\\1,\\1}').gsub(/\\?/, '{0,1}').                                                             # All {2} become {2,2} and ? become {0,1}\n          gsub(/(\\[[^\\]]+\\])\\{(\\d+),(\\d+)\\}/) {|match| $1 * sample(Array(Range.new($2.to_i, $3.to_i))) }.                # [12]{1,2} becomes [12] or [12][12]\n          gsub(/(\\([^\\)]+\\))\\{(\\d+),(\\d+)\\}/) {|match| $1 * sample(Array(Range.new($2.to_i, $3.to_i))) }.                # (12|34){1,2} becomes (12|34) or (12|34)(12|34)\n          gsub(/(\\\\?.)\\{(\\d+),(\\d+)\\}/) {|match| $1 * sample(Array(Range.new($2.to_i, $3.to_i))) }.                      # A{1,2} becomes A or AA or \\d{3} becomes \\d\\d\\d\n          gsub(/\\((.*?)\\)/) {|match| sample(match.gsub(/[\\(\\)]/, '').split('|')) }.                                      # (this|that) becomes 'this' or 'that'\n          gsub(/\\[([^\\]]+)\\]/) {|match| match.gsub(/(\\w\\-\\w)/) {|range| sample(Array(Range.new(*range.split('-')))) } }. # All A-Z inside of [] become C (or X, or whatever)\n          gsub(/\\[([^\\]]+)\\]/) {|match| sample($1.split('')) }.                                                          # All [ABC] become B (or A or C)\n          gsub('\\d') {|match| sample(Numbers) }.\n          gsub('\\w') {|match| sample(Letters) }\n      end\n\n      # Helper for the common approach of grabbing a translation\n      # with an array of values and selecting one of them.\n      def fetch(key)\n        fetched = sample(translate(\"faker.#{key}\"))\n        if fetched && fetched.match(/^\\//) && fetched.match(/\\/$/) # A regex\n          regexify(fetched)\n        else\n          fetched\n        end\n      end\n\n      # Helper for the common approach of grabbing a translation\n      # with an array of values and returning all of them.\n      def fetch_all(key)\n        fetched = translate(\"faker.#{key}\")\n        fetched = fetched.last if fetched.size <= 1\n        if !fetched.respond_to?(:sample) && fetched.match(/^\\//) && fetched.match(/\\/$/) # A regex\n          regexify(fetched)\n        else\n          fetched\n        end\n      end\n\n      # Load formatted strings from the locale, \"parsing\" them\n      # into method calls that can be used to generate a\n      # formatted translation: e.g., \"#{first_name} #{last_name}\".\n      def parse(key)\n        fetched = fetch(key)\n        parts = fetched.scan(/(\\(?)#\\{([A-Za-z]+\\.)?([^\\}]+)\\}([^#]+)?/).map {|prefix, kls, meth, etc|\n          # If the token had a class Prefix (e.g., Name.first_name)\n          # grab the constant, otherwise use self\n          cls = kls ? Faker.const_get(kls.chop) : self\n\n          # If an optional leading parentheses is not present, prefix.should == \"\", otherwise prefix.should == \"(\"\n          # In either case the information will be retained for reconstruction of the string.\n          text = prefix\n\n          # If the class has the method, call it, otherwise\n          # fetch the transation (i.e., faker.name.first_name)\n          text += cls.respond_to?(meth) ? cls.send(meth) : fetch(\"#{(kls || self).to_s.split('::').last.downcase}.#{meth.downcase}\")\n\n          # And tack on spaces, commas, etc. left over in the string\n          text += etc.to_s\n        }\n        # If the fetched key couldn't be parsed, then fallback to numerify\n        parts.any? ? parts.join : numerify(fetched)\n      end\n\n      # Call I18n.translate with our configured locale if no\n      # locale is specified\n      def translate(*args)\n        opts = args.last.is_a?(Hash) ? args.pop : {}\n        opts[:locale] ||= Faker::Config.locale\n        opts[:raise] = true\n        I18n.translate(*(args.push(opts)))\n      rescue I18n::MissingTranslationData\n        opts = args.last.is_a?(Hash) ? args.pop : {}\n        opts[:locale] = :en\n\n        # Super-simple fallback -- fallback to en if the\n        # translation was missing.  If the translation isn't\n        # in en either, then it will raise again.\n        I18n.translate(*(args.push(opts)))\n      end\n\n      # Executes block with given locale set.\n      def with_locale(tmp_locale = nil)\n        current_locale = Faker::Config.own_locale\n        Faker::Config.locale = tmp_locale\n        I18n.with_locale(tmp_locale) { yield }\n      ensure\n        Faker::Config.locale = current_locale\n      end\n\n      def flexible(key)\n        @flexible_key = key\n      end\n\n      # You can add whatever you want to the locale file, and it will get caught here.\n      # E.g., in your locale file, create a\n      #   name:\n      #     girls_name: [\"Alice\", \"Cheryl\", \"Tatiana\"]\n      # Then you can call Faker::Name.girls_name and it will act like #first_name\n      def method_missing(m, *args, &block)\n        super unless @flexible_key\n\n        # Use the alternate form of translate to get a nil rather than a \"missing translation\" string\n        if translation = translate(:faker)[@flexible_key][m]\n          sample(translation)\n        else\n          super\n        end\n      end\n\n      # Generates a random value between the interval\n      def rand_in_range(from, to)\n        from, to = to, from if to < from\n        rand(from..to)\n      end\n\n      def unique(max_retries = 10_000)\n        @unique_generator ||= UniqueGenerator.new(self, max_retries)\n      end\n\n      def sample(list)\n        list.respond_to?(:sample) ? list.sample(random: Faker::Config.random) : list\n      end\n\n      def shuffle(list)\n        list.shuffle(random: Faker::Config.random)\n      end\n\n      def rand(max = nil)\n        if max.nil?\n          Faker::Config.random.rand\n        elsif max.is_a?(Range) || max.to_i > 0\n          Faker::Config.random.rand(max)\n        else\n          0\n        end\n      end\n    end\n  end\nend\n\nDir.glob(File.join(File.dirname(__FILE__), 'faker','*.rb')).sort.each {|f| require f }\n\nrequire 'extensions/array'\nrequire 'extensions/symbol'\n\nrequire 'helpers/char'\nrequire 'helpers/unique_generator'\n", "idx": 2, "id": 8166, "msg": "", "proj": "faker-ruby-faker", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -363,6 +363,7 @@ type JSApiStreamListResponse struct {\n \tApiResponse\n \tApiPaged\n \tStreams []*StreamInfo `json:\"streams\"`\n+\tMissing []string      `json:\"missing_stream_names,omitempty\"`\n }\n \n const JSApiStreamListResponseType = \"io.nats.jetstream.api.v1.stream_list_response\"", "y": 1, "oldf": "// Copyright 2020-2021 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/nats-io/nuid\"\n)\n\n// Request API subjects for JetStream.\nconst (\n\t// All API endpoints.\n\tjsAllAPI = \"$JS.API.>\"\n\n\t// For constructing JetStream domain prefixes.\n\tjsDomainAPI = \"$JS.%s.API.>\"\n\n\tJSApiPrefix = \"$JS.API\"\n\n\t// JSApiAccountInfo is for obtaining general information about JetStream for this account.\n\t// Will return JSON response.\n\tJSApiAccountInfo = \"$JS.API.INFO\"\n\n\t// JSApiTemplateCreate is the endpoint to create new stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateCreate  = \"$JS.API.STREAM.TEMPLATE.CREATE.*\"\n\tJSApiTemplateCreateT = \"$JS.API.STREAM.TEMPLATE.CREATE.%s\"\n\n\t// JSApiTemplates is the endpoint to list all stream template names for this account.\n\t// Will return JSON response.\n\tJSApiTemplates = \"$JS.API.STREAM.TEMPLATE.NAMES\"\n\n\t// JSApiTemplateInfo is for obtaining general information about a named stream template.\n\t// Will return JSON response.\n\tJSApiTemplateInfo  = \"$JS.API.STREAM.TEMPLATE.INFO.*\"\n\tJSApiTemplateInfoT = \"$JS.API.STREAM.TEMPLATE.INFO.%s\"\n\n\t// JSApiTemplateDelete is the endpoint to delete stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateDelete  = \"$JS.API.STREAM.TEMPLATE.DELETE.*\"\n\tJSApiTemplateDeleteT = \"$JS.API.STREAM.TEMPLATE.DELETE.%s\"\n\n\t// JSApiStreamCreate is the endpoint to create new streams.\n\t// Will return JSON response.\n\tJSApiStreamCreate  = \"$JS.API.STREAM.CREATE.*\"\n\tJSApiStreamCreateT = \"$JS.API.STREAM.CREATE.%s\"\n\n\t// JSApiStreamUpdate is the endpoint to update existing streams.\n\t// Will return JSON response.\n\tJSApiStreamUpdate  = \"$JS.API.STREAM.UPDATE.*\"\n\tJSApiStreamUpdateT = \"$JS.API.STREAM.UPDATE.%s\"\n\n\t// JSApiStreams is the endpoint to list all stream names for this account.\n\t// Will return JSON response.\n\tJSApiStreams = \"$JS.API.STREAM.NAMES\"\n\t// JSApiStreamList is the endpoint that will return all detailed stream information\n\tJSApiStreamList = \"$JS.API.STREAM.LIST\"\n\n\t// JSApiStreamInfo is for obtaining general information about a named stream.\n\t// Will return JSON response.\n\tJSApiStreamInfo  = \"$JS.API.STREAM.INFO.*\"\n\tJSApiStreamInfoT = \"$JS.API.STREAM.INFO.%s\"\n\n\t// JSApiStreamDelete is the endpoint to delete streams.\n\t// Will return JSON response.\n\tJSApiStreamDelete  = \"$JS.API.STREAM.DELETE.*\"\n\tJSApiStreamDeleteT = \"$JS.API.STREAM.DELETE.%s\"\n\n\t// JSApiStreamPurge is the endpoint to purge streams.\n\t// Will return JSON response.\n\tJSApiStreamPurge  = \"$JS.API.STREAM.PURGE.*\"\n\tJSApiStreamPurgeT = \"$JS.API.STREAM.PURGE.%s\"\n\n\t// JSApiStreamSnapshot is the endpoint to snapshot streams.\n\t// Will return a stream of chunks with a nil chunk as EOF to\n\t// the deliver subject. Caller should respond to each chunk\n\t// with a nil body response for ack flow.\n\tJSApiStreamSnapshot  = \"$JS.API.STREAM.SNAPSHOT.*\"\n\tJSApiStreamSnapshotT = \"$JS.API.STREAM.SNAPSHOT.%s\"\n\n\t// JSApiStreamRestore is the endpoint to restore a stream from a snapshot.\n\t// Caller should respond to each chunk with a nil body response.\n\tJSApiStreamRestore  = \"$JS.API.STREAM.RESTORE.*\"\n\tJSApiStreamRestoreT = \"$JS.API.STREAM.RESTORE.%s\"\n\n\t// JSApiMsgDelete is the endpoint to delete messages from a stream.\n\t// Will return JSON response.\n\tJSApiMsgDelete  = \"$JS.API.STREAM.MSG.DELETE.*\"\n\tJSApiMsgDeleteT = \"$JS.API.STREAM.MSG.DELETE.%s\"\n\n\t// JSApiMsgGet is the template for direct requests for a message by its stream sequence number.\n\t// Will return JSON response.\n\tJSApiMsgGet  = \"$JS.API.STREAM.MSG.GET.*\"\n\tJSApiMsgGetT = \"$JS.API.STREAM.MSG.GET.%s\"\n\n\t// JSApiConsumerCreate is the endpoint to create ephemeral consumers for streams.\n\t// Will return JSON response.\n\tJSApiConsumerCreate  = \"$JS.API.CONSUMER.CREATE.*\"\n\tJSApiConsumerCreateT = \"$JS.API.CONSUMER.CREATE.%s\"\n\n\t// JSApiDurableCreate is the endpoint to create durable consumers for streams.\n\t// You need to include the stream and consumer name in the subject.\n\tJSApiDurableCreate  = \"$JS.API.CONSUMER.DURABLE.CREATE.*.*\"\n\tJSApiDurableCreateT = \"$JS.API.CONSUMER.DURABLE.CREATE.%s.%s\"\n\n\t// JSApiConsumers is the endpoint to list all consumer names for the stream.\n\t// Will return JSON response.\n\tJSApiConsumers  = \"$JS.API.CONSUMER.NAMES.*\"\n\tJSApiConsumersT = \"$JS.API.CONSUMER.NAMES.%s\"\n\n\t// JSApiConsumerList is the endpoint that will return all detailed consumer information\n\tJSApiConsumerList = \"$JS.API.CONSUMER.LIST.*\"\n\n\t// JSApiConsumerInfo is for obtaining general information about a consumer.\n\t// Will return JSON response.\n\tJSApiConsumerInfo  = \"$JS.API.CONSUMER.INFO.*.*\"\n\tJSApiConsumerInfoT = \"$JS.API.CONSUMER.INFO.%s.%s\"\n\n\t// JSApiConsumerDelete is the endpoint to delete consumers.\n\t// Will return JSON response.\n\tJSApiConsumerDelete  = \"$JS.API.CONSUMER.DELETE.*.*\"\n\tJSApiConsumerDeleteT = \"$JS.API.CONSUMER.DELETE.%s.%s\"\n\n\t// JSApiRequestNextT is the prefix for the request next message(s) for a consumer in worker/pull mode.\n\tJSApiRequestNextT = \"$JS.API.CONSUMER.MSG.NEXT.%s.%s\"\n\n\t// jsRequestNextPre\n\tjsRequestNextPre = \"$JS.API.CONSUMER.MSG.NEXT.\"\n\n\t// For snapshots and restores. The ack will have additional tokens.\n\tjsSnapshotAckT    = \"$JS.SNAPSHOT.ACK.%s.%s\"\n\tjsRestoreDeliverT = \"$JS.SNAPSHOT.RESTORE.%s.%s\"\n\n\t// JSApiStreamRemovePeer is the endpoint to remove a peer from a clustered stream and its consumers.\n\t// Will return JSON response.\n\tJSApiStreamRemovePeer  = \"$JS.API.STREAM.PEER.REMOVE.*\"\n\tJSApiStreamRemovePeerT = \"$JS.API.STREAM.PEER.REMOVE.%s\"\n\n\t// JSApiStreamLeaderStepDown is the endpoint to have stream leader stepdown.\n\t// Will return JSON response.\n\tJSApiStreamLeaderStepDown  = \"$JS.API.STREAM.LEADER.STEPDOWN.*\"\n\tJSApiStreamLeaderStepDownT = \"$JS.API.STREAM.LEADER.STEPDOWN.%s\"\n\n\t// JSApiConsumerLeaderStepDown is the endpoint to have consumer leader stepdown.\n\t// Will return JSON response.\n\tJSApiConsumerLeaderStepDown  = \"$JS.API.CONSUMER.LEADER.STEPDOWN.*.*\"\n\tJSApiConsumerLeaderStepDownT = \"$JS.API.CONSUMER.LEADER.STEPDOWN.%s.%s\"\n\n\t// JSApiLeaderStepDown is the endpoint to have our metaleader stepdown.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiLeaderStepDown = \"$JS.API.META.LEADER.STEPDOWN\"\n\n\t// JSApiRemoveServer is the endpoint to remove a peer server from the cluster.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiRemoveServer = \"$JS.API.SERVER.REMOVE\"\n\n\t// jsAckT is the template for the ack message stream coming back from a consumer\n\t// when they ACK/NAK, etc a message.\n\tjsAckT   = \"$JS.ACK.%s.%s\"\n\tjsAckPre = \"$JS.ACK.\"\n\n\t// jsFlowControl is for flow control subjects.\n\tjsFlowControlPre = \"$JS.FC.\"\n\t// jsFlowControl is for FC responses.\n\tjsFlowControl = \"$JS.FC.%s.%s.*\"\n\n\t// JSAdvisoryPrefix is a prefix for all JetStream advisories.\n\tJSAdvisoryPrefix = \"$JS.EVENT.ADVISORY\"\n\n\t// JSMetricPrefix is a prefix for all JetStream metrics.\n\tJSMetricPrefix = \"$JS.EVENT.METRIC\"\n\n\t// JSMetricConsumerAckPre is a metric containing ack latency.\n\tJSMetricConsumerAckPre = \"$JS.EVENT.METRIC.CONSUMER.ACK\"\n\n\t// JSAdvisoryConsumerMaxDeliveryExceedPre is a notification published when a message exceeds its delivery threshold.\n\tJSAdvisoryConsumerMaxDeliveryExceedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MAX_DELIVERIES\"\n\n\t// JSAdvisoryConsumerMsgTerminatedPre is a notification published when a message has been terminated.\n\tJSAdvisoryConsumerMsgTerminatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MSG_TERMINATED\"\n\n\t// JSAdvisoryStreamCreatedPre notification that a stream was created.\n\tJSAdvisoryStreamCreatedPre = \"$JS.EVENT.ADVISORY.STREAM.CREATED\"\n\n\t// JSAdvisoryStreamDeletedPre notification that a stream was deleted.\n\tJSAdvisoryStreamDeletedPre = \"$JS.EVENT.ADVISORY.STREAM.DELETED\"\n\n\t// JSAdvisoryStreamUpdatedPre notification that a stream was updated.\n\tJSAdvisoryStreamUpdatedPre = \"$JS.EVENT.ADVISORY.STREAM.UPDATED\"\n\n\t// JSAdvisoryConsumerCreatedPre notification that a template created.\n\tJSAdvisoryConsumerCreatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.CREATED\"\n\n\t// JSAdvisoryConsumerDeletedPre notification that a template deleted.\n\tJSAdvisoryConsumerDeletedPre = \"$JS.EVENT.ADVISORY.CONSUMER.DELETED\"\n\n\t// JSAdvisoryStreamSnapshotCreatePre notification that a snapshot was created.\n\tJSAdvisoryStreamSnapshotCreatePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_CREATE\"\n\n\t// JSAdvisoryStreamSnapshotCompletePre notification that a snapshot was completed.\n\tJSAdvisoryStreamSnapshotCompletePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_COMPLETE\"\n\n\t// JSAdvisoryStreamRestoreCreatePre notification that a restore was start.\n\tJSAdvisoryStreamRestoreCreatePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_CREATE\"\n\n\t// JSAdvisoryStreamRestoreCompletePre notification that a restore was completed.\n\tJSAdvisoryStreamRestoreCompletePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_COMPLETE\"\n\n\t// JSAdvisoryStreamLeaderElectedPre notification that a replicated stream has elected a leader.\n\tJSAdvisoryStreamLeaderElectedPre = \"$JS.EVENT.ADVISORY.STREAM.LEADER_ELECTED\"\n\n\t// JSAdvisoryStreamQuorumLostPre notification that a stream and its consumers are stalled.\n\tJSAdvisoryStreamQuorumLostPre = \"$JS.EVENT.ADVISORY.STREAM.QUORUM_LOST\"\n\n\t// JSAdvisoryConsumerLeaderElectedPre notification that a replicated consumer has elected a leader.\n\tJSAdvisoryConsumerLeaderElectedPre = \"$JS.EVENT.ADVISORY.CONSUMER.LEADER_ELECTED\"\n\n\t// JSAdvisoryConsumerQuorumLostPre notification that a consumer is stalled.\n\tJSAdvisoryConsumerQuorumLostPre = \"$JS.EVENT.ADVISORY.CONSUMER.QUORUM_LOST\"\n\n\t// JSAdvisoryServerOutOfStorage notification that a server has no more storage.\n\tJSAdvisoryServerOutOfStorage = \"$JS.EVENT.ADVISORY.SERVER.OUT_OF_STORAGE\"\n\n\t// JSAdvisoryServerRemoved notification that a server has been removed from the system.\n\tJSAdvisoryServerRemoved = \"$JS.EVENT.ADVISORY.SERVER.REMOVED\"\n\n\t// JSAuditAdvisory is a notification about JetStream API access.\n\t// FIXME - Add in details about who..\n\tJSAuditAdvisory = \"$JS.EVENT.ADVISORY.API\"\n)\n\n// JSMaxDescription is the maximum description length for streams and consumers.\nconst JSMaxDescriptionLen = 4 * 1024\n\n// JSMaxNameLen is the maximum name lengths for streams, consumers and templates.\nconst JSMaxNameLen = 256\n\n// Responses for API calls.\n\n// ApiResponse is a standard response from the JetStream JSON API\ntype ApiResponse struct {\n\tType  string    `json:\"type\"`\n\tError *ApiError `json:\"error,omitempty\"`\n}\n\n// ToError checks if the response has a error and if it does converts it to an error avoiding\n// the pitfalls described by https://yourbasic.org/golang/gotcha-why-nil-error-not-equal-nil/\nfunc (r *ApiResponse) ToError() error {\n\tif r.Error == nil {\n\t\treturn nil\n\t}\n\n\treturn r.Error\n}\n\nconst JSApiOverloadedType = \"io.nats.jetstream.api.v1.system_overloaded\"\n\n// ApiPaged includes variables used to create paged responses from the JSON API\ntype ApiPaged struct {\n\tTotal  int `json:\"total\"`\n\tOffset int `json:\"offset\"`\n\tLimit  int `json:\"limit\"`\n}\n\n// ApiPagedRequest includes parameters allowing specific pages to be requests from APIs responding with ApiPaged\ntype ApiPagedRequest struct {\n\tOffset int `json:\"offset\"`\n}\n\n// JSApiAccountInfoResponse reports back information on jetstream for this account.\ntype JSApiAccountInfoResponse struct {\n\tApiResponse\n\t*JetStreamAccountStats\n}\n\nconst JSApiAccountInfoResponseType = \"io.nats.jetstream.api.v1.account_info_response\"\n\n// JSApiStreamCreateResponse stream creation.\ntype JSApiStreamCreateResponse struct {\n\tApiResponse\n\t*StreamInfo\n\tDidCreate bool `json:\"did_create,omitempty\"`\n}\n\nconst JSApiStreamCreateResponseType = \"io.nats.jetstream.api.v1.stream_create_response\"\n\n// JSApiStreamDeleteResponse stream removal.\ntype JSApiStreamDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamDeleteResponseType = \"io.nats.jetstream.api.v1.stream_delete_response\"\n\ntype JSApiStreamInfoRequest struct {\n\tDeletedDetails bool `json:\"deleted_details,omitempty\"`\n}\n\ntype JSApiStreamInfoResponse struct {\n\tApiResponse\n\t*StreamInfo\n}\n\nconst JSApiStreamInfoResponseType = \"io.nats.jetstream.api.v1.stream_info_response\"\n\n// JSApiNamesLimit is the maximum entries we will return for streams or consumers lists.\n// TODO(dlc) - with header or request support could request chunked response.\nconst JSApiNamesLimit = 1024\nconst JSApiListLimit = 256\n\ntype JSApiStreamNamesRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamNamesResponse list of streams.\n// A nil request is valid and means all streams.\ntype JSApiStreamNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []string `json:\"streams\"`\n}\n\nconst JSApiStreamNamesResponseType = \"io.nats.jetstream.api.v1.stream_names_response\"\n\ntype JSApiStreamListRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamListResponse list of detailed stream information.\n// A nil request is valid and means all streams.\ntype JSApiStreamListResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []*StreamInfo `json:\"streams\"`\n}\n\nconst JSApiStreamListResponseType = \"io.nats.jetstream.api.v1.stream_list_response\"\n\n// JSApiStreamPurgeRequest is optional request information to the purge API.\n// Subject will filter the purge request to only messages that match the subject, which can have wildcards.\n// Sequence will purge up to but not including this sequence and can be combined with subject filtering.\n// Keep will specify how many messages to keep. This can also be combined with subject filtering.\n// Note that Sequence and Keep are mutually exclusive, so both can not be set at the same time.\ntype JSApiStreamPurgeRequest struct {\n\t// Purge up to but not including sequence.\n\tSequence uint64 `json:\"seq,omitempty\"`\n\t// Subject to match against messages for the purge command.\n\tSubject string `json:\"filter,omitempty\"`\n\t// Number of messages to keep.\n\tKeep uint64 `json:\"keep,omitempty\"`\n}\n\ntype JSApiStreamPurgeResponse struct {\n\tApiResponse\n\tSuccess bool   `json:\"success,omitempty\"`\n\tPurged  uint64 `json:\"purged\"`\n}\n\nconst JSApiStreamPurgeResponseType = \"io.nats.jetstream.api.v1.stream_purge_response\"\n\n// JSApiStreamUpdateResponse for updating a stream.\ntype JSApiStreamUpdateResponse struct {\n\tApiResponse\n\t*StreamInfo\n}\n\nconst JSApiStreamUpdateResponseType = \"io.nats.jetstream.api.v1.stream_update_response\"\n\n// JSApiMsgDeleteRequest delete message request.\ntype JSApiMsgDeleteRequest struct {\n\tSeq     uint64 `json:\"seq\"`\n\tNoErase bool   `json:\"no_erase,omitempty\"`\n}\n\ntype JSApiMsgDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMsgDeleteResponseType = \"io.nats.jetstream.api.v1.stream_msg_delete_response\"\n\ntype JSApiStreamSnapshotRequest struct {\n\t// Subject to deliver the chunks to for the snapshot.\n\tDeliverSubject string `json:\"deliver_subject\"`\n\t// Do not include consumers in the snapshot.\n\tNoConsumers bool `json:\"no_consumers,omitempty\"`\n\t// Optional chunk size preference.\n\t// Best to just let server select.\n\tChunkSize int `json:\"chunk_size,omitempty\"`\n\t// Check all message's checksums prior to snapshot.\n\tCheckMsgs bool `json:\"jsck,omitempty\"`\n}\n\n// JSApiStreamSnapshotResponse is the direct response to the snapshot request.\ntype JSApiStreamSnapshotResponse struct {\n\tApiResponse\n\t// Configuration of the given stream.\n\tConfig *StreamConfig `json:\"config,omitempty\"`\n\t// Current State for the given stream.\n\tState *StreamState `json:\"state,omitempty\"`\n}\n\nconst JSApiStreamSnapshotResponseType = \"io.nats.jetstream.api.v1.stream_snapshot_response\"\n\n// JSApiStreamRestoreRequest is the required restore request.\ntype JSApiStreamRestoreRequest struct {\n\t// Configuration of the given stream.\n\tConfig StreamConfig `json:\"config\"`\n\t// Current State for the given stream.\n\tState StreamState `json:\"state\"`\n}\n\n// JSApiStreamRestoreResponse is the direct response to the restore request.\ntype JSApiStreamRestoreResponse struct {\n\tApiResponse\n\t// Subject to deliver the chunks to for the snapshot restore.\n\tDeliverSubject string `json:\"deliver_subject\"`\n}\n\nconst JSApiStreamRestoreResponseType = \"io.nats.jetstream.api.v1.stream_restore_response\"\n\n// JSApiStreamRemovePeerRequest is the required remove peer request.\ntype JSApiStreamRemovePeerRequest struct {\n\t// Server name of the peer to be removed.\n\tPeer string `json:\"peer\"`\n}\n\n// JSApiStreamRemovePeerResponse is the response to a remove peer request.\ntype JSApiStreamRemovePeerResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamRemovePeerResponseType = \"io.nats.jetstream.api.v1.stream_remove_peer_response\"\n\n// JSApiStreamLeaderStepDownResponse is the response to a leader stepdown request.\ntype JSApiStreamLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.stream_leader_stepdown_response\"\n\n// JSApiConsumerLeaderStepDownResponse is the response to a consumer leader stepdown request.\ntype JSApiConsumerLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.consumer_leader_stepdown_response\"\n\n// JSApiLeaderStepdownRequest allows placement control over the meta leader placement.\ntype JSApiLeaderStepdownRequest struct {\n\tPlacement *Placement `json:\"placement,omitempty\"`\n}\n\n// JSApiLeaderStepDownResponse is the response to a meta leader stepdown request.\ntype JSApiLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.meta_leader_stepdown_response\"\n\n// JSApiMetaServerRemoveRequest will remove a peer from the meta group.\ntype JSApiMetaServerRemoveRequest struct {\n\t// Server name of the peer to be removed.\n\tServer string `json:\"peer\"`\n}\n\n// JSApiMetaServerRemoveResponse is the response to a peer removal request in the meta group.\ntype JSApiMetaServerRemoveResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMetaServerRemoveResponseType = \"io.nats.jetstream.api.v1.meta_server_remove_response\"\n\n// JSApiMsgGetRequest get a message request.\ntype JSApiMsgGetRequest struct {\n\tSeq     uint64 `json:\"seq,omitempty\"`\n\tLastFor string `json:\"last_by_subj,omitempty\"`\n}\n\ntype JSApiMsgGetResponse struct {\n\tApiResponse\n\tMessage *StoredMsg `json:\"message,omitempty\"`\n}\n\nconst JSApiMsgGetResponseType = \"io.nats.jetstream.api.v1.stream_msg_get_response\"\n\n// JSWaitQueueDefaultMax is the default max number of outstanding requests for pull consumers.\nconst JSWaitQueueDefaultMax = 512\n\ntype JSApiConsumerCreateResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerCreateResponseType = \"io.nats.jetstream.api.v1.consumer_create_response\"\n\ntype JSApiConsumerDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerDeleteResponseType = \"io.nats.jetstream.api.v1.consumer_delete_response\"\n\ntype JSApiConsumerInfoResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerInfoResponseType = \"io.nats.jetstream.api.v1.consumer_info_response\"\n\ntype JSApiConsumersRequest struct {\n\tApiPagedRequest\n}\n\ntype JSApiConsumerNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []string `json:\"consumers\"`\n}\n\nconst JSApiConsumerNamesResponseType = \"io.nats.jetstream.api.v1.consumer_names_response\"\n\ntype JSApiConsumerListResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []*ConsumerInfo `json:\"consumers\"`\n}\n\nconst JSApiConsumerListResponseType = \"io.nats.jetstream.api.v1.consumer_list_response\"\n\n// JSApiConsumerGetNextRequest is for getting next messages for pull based consumers.\ntype JSApiConsumerGetNextRequest struct {\n\tExpires time.Duration `json:\"expires,omitempty\"`\n\tBatch   int           `json:\"batch,omitempty\"`\n\tNoWait  bool          `json:\"no_wait,omitempty\"`\n}\n\n// JSApiStreamTemplateCreateResponse for creating templates.\ntype JSApiStreamTemplateCreateResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateCreateResponseType = \"io.nats.jetstream.api.v1.stream_template_create_response\"\n\ntype JSApiStreamTemplateDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamTemplateDeleteResponseType = \"io.nats.jetstream.api.v1.stream_template_delete_response\"\n\n// JSApiStreamTemplateInfoResponse for information about stream templates.\ntype JSApiStreamTemplateInfoResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateInfoResponseType = \"io.nats.jetstream.api.v1.stream_template_info_response\"\n\ntype JSApiStreamTemplatesRequest struct {\n\tApiPagedRequest\n}\n\n// JSApiStreamTemplateNamesResponse list of templates\ntype JSApiStreamTemplateNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tTemplates []string `json:\"streams\"`\n}\n\nconst JSApiStreamTemplateNamesResponseType = \"io.nats.jetstream.api.v1.stream_template_names_response\"\n\n// Default max API calls outstanding.\nconst defaultMaxJSApiOut = int64(4096)\n\n// Max API calls outstanding.\nvar maxJSApiOut = defaultMaxJSApiOut\n\nfunc (js *jetStream) apiDispatch(sub *subscription, c *client, acc *Account, subject, reply string, rmsg []byte) {\n\tjs.mu.RLock()\n\ts, rr := js.srv, js.apiSubs.Match(subject)\n\tjs.mu.RUnlock()\n\n\thdr, _ := c.msgParts(rmsg)\n\tif len(getHeader(ClientInfoHdr, hdr)) == 0 {\n\t\t// Check if this is the system account. We will let these through for the account info only.\n\t\tif s.SystemAccount() != acc || subject != JSApiAccountInfo {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Shortcircuit.\n\tif len(rr.psubs)+len(rr.qsubs) == 0 {\n\t\treturn\n\t}\n\n\t// We should only have psubs and only 1 per result.\n\t// FIXME(dlc) - Should we respond here with NoResponders or error?\n\tif len(rr.psubs) != 1 {\n\t\ts.Warnf(\"Malformed JetStream API Request: [%s] %q\", subject, rmsg)\n\t\treturn\n\t}\n\tjsub := rr.psubs[0]\n\n\t// If this is directly from a client connection ok to do in place.\n\tif c.kind != ROUTER && c.kind != GATEWAY {\n\t\tjsub.icb(sub, c, acc, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\t// If we are here we have received this request over a non client connection.\n\t// We need to make sure not to block. We will spin a Go routine per but also make\n\t// sure we do not have too many outstanding.\n\tif apiOut := atomic.AddInt64(&js.apiCalls, 1); apiOut > maxJSApiOut {\n\t\tatomic.AddInt64(&js.apiCalls, -1)\n\t\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\t\tif err == nil {\n\t\t\tresp := &ApiResponse{Type: JSApiOverloadedType, Error: NewJSInsufficientResourcesError()}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t} else {\n\t\t\ts.Warnf(badAPIRequestT, rmsg)\n\t\t}\n\t\ts.Warnf(\"JetStream API limit exceeded: %d calls outstanding\", apiOut)\n\t\treturn\n\t}\n\n\t// If we are here we can properly dispatch this API call.\n\t// Copy the message and the client. Client for the pubArgs\n\t// but note the JSAPI only uses the hdr index to piece apart\n\t// the header from the msg body. No other references are needed.\n\t// FIXME(dlc) - Should cleanup eventually and make sending\n\t// and receiving internal messages more formal.\n\trmsg = copyBytes(rmsg)\n\tclient := &client{srv: s, kind: JETSTREAM}\n\tclient.pa = c.pa\n\n\t// Dispatch the API call to its own Go routine.\n\tgo func() {\n\t\tjsub.icb(sub, client, acc, subject, reply, rmsg)\n\t\tatomic.AddInt64(&js.apiCalls, -1)\n\t}()\n}\n\nfunc (s *Server) setJetStreamExportSubs() error {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\n\t// This is the catch all now for all JetStream API calls.\n\tif _, err := s.sysSubscribe(jsAllAPI, js.apiDispatch); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s.SystemAccount().AddServiceExport(jsAllAPI, nil); err != nil {\n\t\ts.Warnf(\"Error setting up jetstream service exports: %v\", err)\n\t\treturn err\n\t}\n\n\t// API handles themselves.\n\tpairs := []struct {\n\t\tsubject string\n\t\thandler msgHandler\n\t}{\n\t\t{JSApiAccountInfo, s.jsAccountInfoRequest},\n\t\t{JSApiTemplateCreate, s.jsTemplateCreateRequest},\n\t\t{JSApiTemplates, s.jsTemplateNamesRequest},\n\t\t{JSApiTemplateInfo, s.jsTemplateInfoRequest},\n\t\t{JSApiTemplateDelete, s.jsTemplateDeleteRequest},\n\t\t{JSApiStreamCreate, s.jsStreamCreateRequest},\n\t\t{JSApiStreamUpdate, s.jsStreamUpdateRequest},\n\t\t{JSApiStreams, s.jsStreamNamesRequest},\n\t\t{JSApiStreamList, s.jsStreamListRequest},\n\t\t{JSApiStreamInfo, s.jsStreamInfoRequest},\n\t\t{JSApiStreamDelete, s.jsStreamDeleteRequest},\n\t\t{JSApiStreamPurge, s.jsStreamPurgeRequest},\n\t\t{JSApiStreamSnapshot, s.jsStreamSnapshotRequest},\n\t\t{JSApiStreamRestore, s.jsStreamRestoreRequest},\n\t\t{JSApiStreamRemovePeer, s.jsStreamRemovePeerRequest},\n\t\t{JSApiStreamLeaderStepDown, s.jsStreamLeaderStepDownRequest},\n\t\t{JSApiConsumerLeaderStepDown, s.jsConsumerLeaderStepDownRequest},\n\t\t{JSApiMsgDelete, s.jsMsgDeleteRequest},\n\t\t{JSApiMsgGet, s.jsMsgGetRequest},\n\t\t{JSApiConsumerCreate, s.jsConsumerCreateRequest},\n\t\t{JSApiDurableCreate, s.jsDurableCreateRequest},\n\t\t{JSApiConsumers, s.jsConsumerNamesRequest},\n\t\t{JSApiConsumerList, s.jsConsumerListRequest},\n\t\t{JSApiConsumerInfo, s.jsConsumerInfoRequest},\n\t\t{JSApiConsumerDelete, s.jsConsumerDeleteRequest},\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tfor _, p := range pairs {\n\t\tsub := &subscription{subject: []byte(p.subject), icb: p.handler}\n\t\tif err := js.apiSubs.Insert(sub); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) sendAPIResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPI()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nfunc (s *Server) sendAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPIErr()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nconst errRespDelay = 500 * time.Millisecond\n\nfunc (s *Server) sendDelayedAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string, rg *raftGroup) {\n\tvar quitCh <-chan struct{}\n\tif rg != nil && rg.node != nil {\n\t\tquitCh = rg.node.QuitC()\n\t}\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tselect {\n\t\tcase <-quitCh:\n\t\tcase <-s.quitCh:\n\t\tcase <-time.After(errRespDelay):\n\t\t\tacc.trackAPIErr()\n\t\t\tif reply != _EMPTY_ {\n\t\t\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t\t\t}\n\t\t\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n\t\t}\n\t})\n}\n\nfunc (s *Server) getRequestInfo(c *client, raw []byte) (pci *ClientInfo, acc *Account, hdr, msg []byte, err error) {\n\thdr, msg = c.msgParts(raw)\n\tvar ci ClientInfo\n\n\tif len(hdr) > 0 {\n\t\tif err := json.Unmarshal(getHeader(ClientInfoHdr, hdr), &ci); err != nil {\n\t\t\treturn nil, nil, nil, nil, err\n\t\t}\n\t}\n\n\tif ci.Service != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Service)\n\t} else if ci.Account != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Account)\n\t} else {\n\t\t// Direct $SYS access.\n\t\tacc = c.acc\n\t\tif acc == nil {\n\t\t\tacc = s.SystemAccount()\n\t\t}\n\t}\n\tif acc == nil {\n\t\treturn nil, nil, nil, nil, ErrMissingAccount\n\t}\n\treturn &ci, acc, hdr, msg, nil\n}\n\nfunc (a *Account) trackAPI() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.mu.Lock()\n\t\tjsa.usage.api++\n\t\tjsa.apiTotal++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tjsa.mu.Unlock()\n\t}\n}\n\nfunc (a *Account) trackAPIErr() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.mu.Lock()\n\t\tjsa.usage.api++\n\t\tjsa.apiTotal++\n\t\tjsa.usage.err++\n\t\tjsa.apiErrors++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tjs := jsa.js\n\t\tjsa.mu.Unlock()\n\t\tatomic.AddInt64(&js.apiErrors, 1)\n\t}\n}\n\nconst badAPIRequestT = \"Malformed JetStream API Request: %q\"\n\n// Helper function to check on JetStream being enabled but also on status of leafnodes\n// If the local account is not enabled but does have leafnode connectivity we will not\n// want to error immediately and let the other side decide.\nfunc (a *Account) checkJetStream() (enabled, shouldError bool) {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.js != nil, a.nleafs+a.nrleafs == 0\n}\n\n// Request for current usage and limits for this account.\nfunc (s *Server) jsAccountInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiAccountInfoResponse{ApiResponse: ApiResponse{Type: JSApiAccountInfoResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif !doErr {\n\t\t\treturn\n\t\t}\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t} else {\n\t\tstats := acc.JetStreamUsage()\n\t\tresp.JetStreamAccountStats = &stats\n\t}\n\tb, err := json.Marshal(resp)\n\tif err != nil {\n\t\treturn\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), string(b))\n}\n\n// Helpers for token extraction.\nfunc templateNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\nfunc streamNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 5)\n}\n\nfunc consumerNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\n// Request to create a new template.\nfunc (s *Server) jsTemplateCreateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateCreateResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar cfg StreamTemplateConfig\n\tif err := json.Unmarshal(msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\ttemplateName := templateNameFromSubject(subject)\n\tif templateName != cfg.Name {\n\t\tresp.Error = NewJSTemplateNameNotMatchSubjectError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tt, err := acc.addStreamTemplate(&cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\ttcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: tcfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all template names.\nfunc (s *Server) jsTemplateNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateNamesResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamTemplatesRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tts := acc.templates()\n\tsort.Slice(ts, func(i, j int) bool {\n\t\treturn strings.Compare(ts[i].StreamTemplateConfig.Name, ts[j].StreamTemplateConfig.Name) < 0\n\t})\n\n\ttcnt := len(ts)\n\tif offset > tcnt {\n\t\toffset = tcnt\n\t}\n\n\tfor _, t := range ts[offset:] {\n\t\tt.mu.Lock()\n\t\tname := t.Name\n\t\tt.mu.Unlock()\n\t\tresp.Templates = append(resp.Templates, name)\n\t\tif len(resp.Templates) >= JSApiNamesLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = tcnt\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\tif resp.Templates == nil {\n\t\tresp.Templates = []string{}\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream template.\nfunc (s *Server) jsTemplateInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateInfoResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\tt, err := acc.lookupStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\tcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: cfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a stream template.\nfunc (s *Server) jsTemplateDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateDeleteResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\terr = acc.deleteStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc (s *Server) jsonResponse(v interface{}) string {\n\tb, err := json.Marshal(v)\n\tif err != nil {\n\t\ts.Warnf(\"Problem marshaling JSON for JetStream API:\", err)\n\t\treturn \"\"\n\t}\n\treturn string(b)\n}\n\n// Request to create a stream.\nfunc (s *Server) jsStreamCreateRequest(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tvar cfg StreamConfig\n\tif err := json.Unmarshal(msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\thasStream := func(streamName string) (bool, int32, []string) {\n\t\tvar exists bool\n\t\tvar maxMsgSize int32\n\t\tvar subs []string\n\t\tif s.JetStreamIsClustered() {\n\t\t\tif js, _ := s.getJetStreamCluster(); js != nil {\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif sa := js.streamAssignment(acc.Name, streamName); sa != nil {\n\t\t\t\t\tmaxMsgSize = sa.Config.MaxMsgSize\n\t\t\t\t\tsubs = sa.Config.Subjects\n\t\t\t\t\texists = true\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t}\n\t\t} else if mset, err := acc.lookupStream(streamName); err == nil {\n\t\t\tmaxMsgSize = mset.cfg.MaxMsgSize\n\t\t\tsubs = mset.cfg.Subjects\n\t\t\texists = true\n\t\t}\n\t\treturn exists, maxMsgSize, subs\n\t}\n\n\tvar streamSubs []string\n\tvar deliveryPrefixes []string\n\tvar apiPrefixes []string\n\n\t// Do some pre-checking for mirror config to avoid cycles in clustered mode.\n\tif cfg.Mirror != nil {\n\t\tif len(cfg.Subjects) > 0 {\n\t\t\tresp.Error = NewJSMirrorWithSubjectsError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif len(cfg.Sources) > 0 {\n\t\t\tresp.Error = NewJSMirrorWithSourcesError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.FilterSubject != _EMPTY_ {\n\t\t\tresp.Error = NewJSMirrorWithSubjectFiltersError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.OptStartSeq > 0 && cfg.Mirror.OptStartTime != nil {\n\t\t\tresp.Error = NewJSMirrorWithStartSeqAndTimeError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Duplicates != time.Duration(0) {\n\t\t\tresp.Error = &ApiError{Code: 400, Description: \"stream mirrors do not make use of a de-duplication window\"}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// We do not require other stream to exist anymore, but if we can see it check payloads.\n\t\texists, maxMsgSize, subs := hasStream(cfg.Mirror.Name)\n\t\tif len(subs) > 0 {\n\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t}\n\t\tif exists && cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\tresp.Error = NewJSMirrorMaxMessageSizeTooBigError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.External != nil {\n\t\t\tif cfg.Mirror.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, cfg.Mirror.External.DeliverPrefix)\n\t\t\t}\n\t\t\tif cfg.Mirror.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, cfg.Mirror.External.ApiPrefix)\n\t\t\t}\n\t\t}\n\t}\n\tif len(cfg.Sources) > 0 {\n\t\tfor _, src := range cfg.Sources {\n\t\t\tif src.External == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texists, maxMsgSize, subs := hasStream(src.Name)\n\t\t\tif len(subs) > 0 {\n\t\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t\t}\n\t\t\tif src.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, src.External.DeliverPrefix)\n\t\t\t}\n\t\t\tif src.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, src.External.ApiPrefix)\n\t\t\t}\n\t\t\tif exists && cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\t\tresp.Error = NewJSSourceMaxMessageSizeTooBigError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// check prefix overlap with subjects\n\tfor _, pfx := range deliveryPrefixes {\n\t\tif !IsValidPublishSubject(pfx) {\n\t\t\tresp.Error = NewJSStreamInvalidExternalDeliverySubjError(pfx)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tfor _, sub := range streamSubs {\n\t\t\tif SubjectsCollide(sub, fmt.Sprintf(\"%s.%s\", pfx, sub)) {\n\t\t\t\tresp.Error = NewJSStreamExternalDelPrefixOverlapsError(pfx, sub)\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// check if api prefixes overlap\n\tfor _, apiPfx := range apiPrefixes {\n\t\tif !IsValidPublishSubject(apiPfx) {\n\t\t\tresp.Error = &ApiError{Code: 400, Description: fmt.Sprintf(\"stream external api prefix %q must be a valid subject without wildcards\", apiPfx)}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif SubjectsCollide(apiPfx, JSApiPrefix) {\n\t\t\tresp.Error = NewJSStreamExternalApiOverlapError(apiPfx, JSApiPrefix)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRequest(ci, acc, subject, reply, rmsg, &cfg)\n\t\treturn\n\t}\n\n\tmset, err := acc.addStream(&cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config()}\n\tresp.DidCreate = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to update a stream.\nfunc (s *Server) jsStreamUpdateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tvar ncfg StreamConfig\n\tif err := json.Unmarshal(msg, &ncfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tcfg, err := checkStreamCfg(&ncfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamInvalidConfigError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamUpdateRequest(ci, acc, subject, reply, rmsg, &cfg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := mset.update(&cfg); err != nil {\n\t\tresp.Error = NewJSStreamUpdateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs, _ := s.getJetStreamCluster()\n\n\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config(), Cluster: js.clusterInfo(mset.raftGroup())}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all stream names.\nfunc (s *Server) jsStreamNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamNamesResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamNamesRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar numStreams int\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tfor stream, sa := range cc.streams[acc.Name] {\n\t\t\tif IsNatsErr(sa.err, JSClusterNotAssignedErr) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif filter != _EMPTY_ {\n\t\t\t\t// These could not have subjects auto-filled in since they are raw and unprocessed.\n\t\t\t\tif len(sa.Config.Subjects) == 0 {\n\t\t\t\t\tif SubjectsCollide(filter, sa.Config.Name) {\n\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfor _, subj := range sa.Config.Subjects {\n\t\t\t\t\t\tif SubjectsCollide(filter, subj) {\n\t\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t}\n\t\t}\n\t\tjs.mu.RUnlock()\n\t\tif len(resp.Streams) > 1 {\n\t\t\tsort.Slice(resp.Streams, func(i, j int) bool { return strings.Compare(resp.Streams[i], resp.Streams[j]) < 0 })\n\t\t}\n\t\tnumStreams = len(resp.Streams)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t\tresp.Streams = resp.Streams[:offset]\n\t\t}\n\t} else {\n\t\tmsets := acc.filteredStreams(filter)\n\t\t// Since we page results order matters.\n\t\tif len(msets) > 1 {\n\t\t\tsort.Slice(msets, func(i, j int) bool {\n\t\t\t\treturn strings.Compare(msets[i].cfg.Name, msets[j].cfg.Name) < 0\n\t\t\t})\n\t\t}\n\n\t\tnumStreams = len(msets)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t}\n\n\t\tfor _, mset := range msets[offset:] {\n\t\t\tresp.Streams = append(resp.Streams, mset.cfg.Name)\n\t\t\tif len(resp.Streams) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numStreams\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed stream info.\n// TODO(dlc) - combine with above long term\nfunc (s *Server) jsStreamListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiStreamListResponseType},\n\t\tStreams:     []*StreamInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamListRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\t// Need to copy these off before sending..\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() { s.jsClusteredStreamListRequest(acc, ci, filter, offset, subject, reply, msg) })\n\t\treturn\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar msets []*stream\n\tif filter == _EMPTY_ {\n\t\tmsets = acc.streams()\n\t} else {\n\t\tmsets = acc.filteredStreams(filter)\n\t}\n\n\tsort.Slice(msets, func(i, j int) bool {\n\t\treturn strings.Compare(msets[i].cfg.Name, msets[j].cfg.Name) < 0\n\t})\n\n\tscnt := len(msets)\n\tif offset > scnt {\n\t\toffset = scnt\n\t}\n\n\tfor _, mset := range msets[offset:] {\n\t\tresp.Streams = append(resp.Streams, &StreamInfo{\n\t\t\tCreated: mset.createdTime(),\n\t\t\tState:   mset.state(),\n\t\t\tConfig:  mset.config(),\n\t\t\tMirror:  mset.mirrorInfo(),\n\t\t\tSources: mset.sourcesInfo()},\n\t\t)\n\t\tif len(resp.Streams) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = scnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream.\nfunc (s *Server) jsStreamInfoRequest(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, hdr, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamInfoResponseType}}\n\n\t// If someone creates a duplicate stream that is identical we will get this request forwarded to us.\n\t// Make sure the response type is for a create call.\n\tif rt := getHeader(JSResponseType, hdr); len(rt) > 0 && string(rt) == jsCreateResponse {\n\t\tresp.ApiResponse.Type = JSApiStreamCreateResponseType\n\t}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, streamName)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tisLeaderless := js.isGroupLeaderless(sa.Group)\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(streamName) && !isLeaderless {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), sa.Group)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar details bool\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamInfoRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tdetails = req.DeletedDetails\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tconfig := mset.config()\n\n\tjs, _ := s.getJetStreamCluster()\n\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated: mset.createdTime(),\n\t\tState:   mset.stateWithDetail(details),\n\t\tConfig:  config,\n\t\tDomain:  s.getOpts().JetStreamDomain,\n\t\tCluster: js.clusterInfo(mset.raftGroup()),\n\t}\n\tif mset.isMirror() {\n\t\tresp.StreamInfo.Mirror = mset.mirrorInfo()\n\t} else if mset.hasSources() {\n\t\tresp.StreamInfo.Sources = mset.sourcesInfo()\n\t}\n\n\t// Check for out of band catchups.\n\tif mset.hasCatchupPeers() {\n\t\tmset.checkClusterInfo(resp.StreamInfo)\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a stream leader stepdown.\nfunc (s *Server) jsStreamLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiStreamLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(sa.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\tif !acc.JetStreamIsStreamLeader(name) {\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Call actual stepdown.\n\tif mset != nil {\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\t}\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a consumer leader stepdown.\nfunc (s *Server) jsConsumerLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiConsumerLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tstream := tokenAt(subject, 6)\n\tconsumer := tokenAt(subject, 7)\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\tvar ca *consumerAssignment\n\tif sa.consumers != nil {\n\t\tca = sa.consumers[consumer]\n\t}\n\tif ca == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(ca.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif !acc.JetStreamIsConsumerLeader(stream, consumer) {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Call actual stepdown.\n\to.raftNode().StepDown()\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to remove a peer from a clustered stream.\nfunc (s *Server) jsStreamRemovePeerRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamRemovePeerResponse{ApiResponse: ApiResponse{Type: JSApiStreamRemovePeerResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are meta leader.\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRemovePeerRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif req.Peer == _EMPTY_ {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif sa == nil {\n\t\t// No stream present.\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\t// Peers here is a server name, convert to node name.\n\tnodeName := string(getHash(req.Peer))\n\n\tjs.mu.RLock()\n\trg := sa.Group\n\tisMember := rg.isMember(nodeName)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are a member.\n\tif !isMember {\n\t\tresp.Error = NewJSClusterPeerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we have a valid peer member set for removal.\n\tif !js.removePeerFromStream(sa, nodeName) {\n\t\tresp.Error = NewJSPeerRemapError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have the metaleader remove a peer from the system.\nfunc (s *Server) jsLeaderServerRemoveRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar resp = JSApiMetaServerRemoveResponse{ApiResponse: ApiResponse{Type: JSApiMetaServerRemoveResponseType}}\n\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiMetaServerRemoveRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar found string\n\tjs.mu.RLock()\n\tfor _, p := range cc.meta.Peers() {\n\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\tif ok && si.(nodeInfo).name == req.Server {\n\t\t\tfound = p.ID\n\t\t\tbreak\n\t\t}\n\t}\n\tjs.mu.RUnlock()\n\n\tif found == _EMPTY_ {\n\t\tresp.Error = NewJSClusterServerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// So we have a valid peer.\n\tjs.mu.Lock()\n\tcc.meta.ProposeRemovePeer(found)\n\tjs.mu.Unlock()\n\n\tresp.Success = true\n\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n}\n\n// Request to have the meta leader stepdown.\n// These will only be received the the meta leaders, so less checking needed.\nfunc (s *Server) jsLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar preferredLeader string\n\tvar resp = JSApiLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiLeaderStepDownResponseType}}\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiLeaderStepdownRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif len(req.Placement.Tags) > 0 {\n\t\t\t// Tags currently not supported.\n\t\t\tresp.Error = NewJSClusterTagsError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tcn := req.Placement.Cluster\n\t\tvar peers []string\n\t\tourID := cc.meta.ID()\n\t\tfor _, p := range cc.meta.Peers() {\n\t\t\tif si, ok := s.nodeToInfo.Load(p.ID); ok && si != nil {\n\t\t\t\tif ni := si.(nodeInfo); ni.offline || ni.cluster != cn || p.ID == ourID {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tpeers = append(peers, p.ID)\n\t\t\t}\n\t\t}\n\t\tif len(peers) == 0 {\n\t\t\tresp.Error = NewJSClusterNoPeersError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Randomize and select.\n\t\tif len(peers) > 1 {\n\t\t\trand.Shuffle(len(peers), func(i, j int) { peers[i], peers[j] = peers[j], peers[i] })\n\t\t}\n\t\tpreferredLeader = peers[0]\n\t}\n\n\t// Call actual stepdown.\n\terr = cc.meta.StepDown(preferredLeader)\n\tif err != nil {\n\t\tresp.Error = NewJSRaftGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc isEmptyRequest(req []byte) bool {\n\tif len(req) == 0 {\n\t\treturn true\n\t}\n\tif bytes.Equal(req, []byte(\"{}\")) {\n\t\treturn true\n\t}\n\t// If we are here we didn't get our simple match, but still could be valid.\n\tvar v interface{}\n\tif err := json.Unmarshal(req, &v); err != nil {\n\t\treturn false\n\t}\n\tvm, ok := v.(map[string]interface{})\n\tif !ok {\n\t\treturn false\n\t}\n\treturn len(vm) == 0\n}\n\n// Request to delete a stream.\nfunc (s *Server) jsStreamDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\n\t// Clustered.\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamDeleteRequest(ci, acc, stream, subject, reply, msg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := mset.delete(); err != nil {\n\t\tresp.Error = NewJSStreamDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a message.\n// This expects a stream sequence number as the msg body.\nfunc (s *Server) jsMsgDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgDeleteResponse{ApiResponse: ApiResponse{Type: JSApiMsgDeleteResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgDeleteRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyDelete {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(errors.New(\"message delete not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredMsgDeleteRequest(ci, acc, mset, stream, subject, reply, &req, rmsg)\n\t\treturn\n\t}\n\n\tvar removed bool\n\tif req.NoErase {\n\t\tremoved, err = mset.removeMsg(req.Seq)\n\t} else {\n\t\tremoved, err = mset.eraseMsg(req.Seq)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(err, Unless(err))\n\t} else if !removed {\n\t\tresp.Error = NewJSSequenceNotFoundError(req.Seq)\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to get a raw stream message.\nfunc (s *Server) jsMsgGetRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgGetResponse{ApiResponse: ApiResponse{Type: JSApiMsgGetResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgGetRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check that we do not have both options set.\n\tif req.Seq > 0 && req.LastFor != _EMPTY_ || req.Seq == 0 && req.LastFor == _EMPTY_ {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar subj string\n\tvar hdr []byte\n\tvar data []byte\n\tvar ts int64\n\tseq := req.Seq\n\n\tif req.Seq > 0 {\n\t\tsubj, hdr, data, ts, err = mset.store.LoadMsg(req.Seq)\n\t} else {\n\t\tsubj, seq, hdr, data, ts, err = mset.store.LoadLastMsg(req.LastFor)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSNoMessageFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Message = &StoredMsg{\n\t\tSubject:  subj,\n\t\tSequence: seq,\n\t\tHeader:   hdr,\n\t\tData:     data,\n\t\tTime:     time.Unix(0, ts).UTC(),\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to purge a stream.\nfunc (s *Server) jsStreamPurgeRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar purgeRequest *JSApiStreamPurgeRequest\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamPurgeRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif req.Sequence > 0 && req.Keep > 0 {\n\t\t\tresp.Error = NewJSBadRequestError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tpurgeRequest = &req\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyPurge {\n\t\tresp.Error = NewJSStreamPurgeFailedError(errors.New(\"stream purge not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamPurgeRequest(ci, acc, mset, stream, subject, reply, rmsg, purgeRequest)\n\t\treturn\n\t}\n\n\tpurged, err := mset.purge(purgeRequest)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Purged = purged\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to restore a stream.\nfunc (s *Server) jsStreamRestoreRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamIsLeader() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRestoreRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tif stream != req.Config.Name && req.Config.Name == _EMPTY_ {\n\t\treq.Config.Name = stream\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRestoreRequest(ci, acc, &req, stream, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tif _, err := acc.lookupStream(stream); err == nil {\n\t\tresp.Error = NewJSStreamNameExistError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\ts.processStreamRestore(ci, acc, &req.Config, subject, reply, string(msg))\n}\n\nfunc (s *Server) processStreamRestore(ci *ClientInfo, acc *Account, cfg *StreamConfig, subject, reply, msg string) <-chan error {\n\tjs := s.getJetStream()\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\n\tsnapDir := path.Join(js.config.StoreDir, snapStagingDir)\n\tif _, err := os.Stat(snapDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(snapDir, defaultDirPerms); err != nil {\n\t\t\tresp.Error = &ApiError{Code: 503, Description: \"JetStream unable to create temp storage for restore\"}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn nil\n\t\t}\n\t}\n\n\ttfile, err := ioutil.TempFile(snapDir, \"js-restore-\")\n\tif err != nil {\n\t\tresp.Error = NewJSTempStorageFailedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\tstreamName := cfg.Name\n\ts.Noticef(\"Starting restore for stream '%s > %s'\", acc.Name, streamName)\n\n\tstart := time.Now().UTC()\n\tdomain := s.getOpts().JetStreamDomain\n\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCreatePre+\".\"+streamName, &JSRestoreCreateAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSRestoreCreateAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: start,\n\t\t},\n\t\tStream: streamName,\n\t\tClient: ci,\n\t\tDomain: domain,\n\t})\n\n\t// Create our internal subscription to accept the snapshot.\n\trestoreSubj := fmt.Sprintf(jsRestoreDeliverT, streamName, nuid.Next())\n\n\ttype result struct {\n\t\terr   error\n\t\treply string\n\t}\n\n\t// For signaling to upper layers.\n\tresultCh := make(chan result, 1)\n\tactiveCh := make(chan int, 32)\n\n\tvar total int\n\n\t// FIXM(dlc) - Probably take out of network path eventually due to disk I/O?\n\tprocessChunk := func(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\t\t// We require reply subjects to communicate back failures, flow etc. If they do not have one log and cancel.\n\t\tif reply == _EMPTY_ {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' requires reply subject for each chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Account client messages have \\r\\n on end. This is an error.\n\t\tif len(msg) < LEN_CR_LF {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' received short chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Adjust.\n\t\tmsg = msg[:len(msg)-LEN_CR_LF]\n\n\t\t// This means we are complete with our transfer from the client.\n\t\tif len(msg) == 0 {\n\t\t\ts.Debugf(\"Finished staging restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\tresultCh <- result{err, reply}\n\t\t\treturn\n\t\t}\n\n\t\t// We track total and check on server limits.\n\t\t// TODO(dlc) - We could check apriori and cancel initial request if we know it won't fit.\n\t\ttotal += len(msg)\n\t\tif js.wouldExceedLimits(FileStorage, total) {\n\t\t\ts.resourcesExeededError()\n\t\t\tresultCh <- result{NewJSInsufficientResourcesError(), reply}\n\t\t\treturn\n\t\t}\n\n\t\t// Append chunk to temp file. Mark as issue if we encounter an error.\n\t\tif n, err := tfile.Write(msg); n != len(msg) || err != nil {\n\t\t\tresultCh <- result{err, reply}\n\t\t\tif reply != _EMPTY_ {\n\t\t\t\ts.sendInternalAccountMsg(acc, reply, \"-ERR 'storage failure during restore'\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tactiveCh <- len(msg)\n\n\t\ts.sendInternalAccountMsg(acc, reply, nil)\n\t}\n\n\tsub, err := acc.subscribeInternal(restoreSubj, processChunk)\n\tif err != nil {\n\t\ttfile.Close()\n\t\tos.Remove(tfile.Name())\n\t\tresp.Error = NewJSRestoreSubscribeFailedError(err, restoreSubj)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\t// Mark the subject so the end user knows where to send the snapshot chunks.\n\tresp.DeliverSubject = restoreSubj\n\ts.sendAPIResponse(ci, acc, subject, reply, msg, s.jsonResponse(resp))\n\n\tdoneCh := make(chan error, 1)\n\n\t// Monitor the progress from another Go routine.\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tdefer func() {\n\t\t\ttfile.Close()\n\t\t\tos.Remove(tfile.Name())\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t}()\n\n\t\tconst activityInterval = 5 * time.Second\n\t\tnotActive := time.NewTimer(activityInterval)\n\t\tdefer notActive.Stop()\n\n\t\ttotal := 0\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase result := <-resultCh:\n\t\t\t\terr := result.err\n\t\t\t\tvar mset *stream\n\n\t\t\t\t// If we staged properly go ahead and do restore now.\n\t\t\t\tif err == nil {\n\t\t\t\t\ts.Debugf(\"Finalizing restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\t\t\ttfile.Seek(0, 0)\n\t\t\t\t\tmset, err = acc.RestoreStream(cfg, tfile)\n\t\t\t\t} else {\n\t\t\t\t\terrStr := err.Error()\n\t\t\t\t\ttmp := []rune(errStr)\n\t\t\t\t\ttmp[0] = unicode.ToUpper(tmp[0])\n\t\t\t\t\ts.Warnf(errStr)\n\t\t\t\t}\n\n\t\t\t\tend := time.Now().UTC()\n\n\t\t\t\t// TODO(rip) - Should this have the error code in it??\n\t\t\t\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCompletePre+\".\"+streamName, &JSRestoreCompleteAdvisory{\n\t\t\t\t\tTypedEvent: TypedEvent{\n\t\t\t\t\t\tType: JSRestoreCompleteAdvisoryType,\n\t\t\t\t\t\tID:   nuid.Next(),\n\t\t\t\t\t\tTime: end,\n\t\t\t\t\t},\n\t\t\t\t\tStream: streamName,\n\t\t\t\t\tStart:  start,\n\t\t\t\t\tEnd:    end,\n\t\t\t\t\tBytes:  int64(total),\n\t\t\t\t\tClient: ci,\n\t\t\t\t\tDomain: domain,\n\t\t\t\t})\n\n\t\t\t\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\tresp.Error = NewJSStreamRestoreError(err, Unless(err))\n\t\t\t\t\ts.Warnf(\"Restore failed for %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), streamName, acc.Name, end.Sub(start))\n\t\t\t\t} else {\n\t\t\t\t\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config()}\n\t\t\t\t\ts.Noticef(\"Completed restore of %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), streamName, acc.Name, end.Sub(start))\n\t\t\t\t}\n\n\t\t\t\t// On the last EOF, send back the stream info or error status.\n\t\t\t\ts.sendInternalAccountMsg(acc, result.reply, s.jsonResponse(&resp))\n\t\t\t\t// Signal to the upper layers.\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\tcase n := <-activeCh:\n\t\t\t\ttotal += n\n\t\t\t\tnotActive.Reset(activityInterval)\n\t\t\tcase <-notActive.C:\n\t\t\t\terr := fmt.Errorf(\"restore for stream '%s > %s' is stalled\", acc, streamName)\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\treturn doneCh\n}\n\n// Process a snapshot request.\nfunc (s *Server) jsStreamSnapshotRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tsmsg := string(msg)\n\tstream := streamNameFromSubject(subject)\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() && !acc.JetStreamIsStreamLeader(stream) {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamSnapshotResponse{ApiResponse: ApiResponse{Type: JSApiStreamSnapshotResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar req JSApiStreamSnapshotRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !IsValidSubject(req.DeliverSubject) {\n\t\tresp.Error = NewJSSnapshotDeliverSubjectInvalidError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We will do the snapshot in a go routine as well since check msgs may\n\t// stall this go routine.\n\tgo func() {\n\t\tif req.CheckMsgs {\n\t\t\ts.Noticef(\"Starting health check and snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t} else {\n\t\t\ts.Noticef(\"Starting snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t}\n\n\t\tstart := time.Now().UTC()\n\n\t\tsr, err := mset.snapshot(0, req.CheckMsgs, !req.NoConsumers)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"Snapshot of stream '%s > %s' failed: %v\", mset.jsa.account.Name, mset.name(), err)\n\t\t\tresp.Error = NewJSStreamSnapshotError(err, Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tconfig := mset.config()\n\t\tresp.State = &sr.State\n\t\tresp.Config = &config\n\n\t\ts.sendAPIResponse(ci, acc, subject, reply, smsg, s.jsonResponse(resp))\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCreatePre+\".\"+mset.name(), &JSSnapshotCreateAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCreatedAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tState:  sr.State,\n\t\t\tClient: ci,\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\t// Now do the real streaming.\n\t\ts.streamSnapshot(ci, acc, mset, sr, &req)\n\n\t\tend := time.Now().UTC()\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCompletePre+\".\"+mset.name(), &JSSnapshotCompleteAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCompleteAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: end,\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tStart:  start,\n\t\t\tEnd:    end,\n\t\t\tClient: ci,\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\ts.Noticef(\"Completed snapshot of %s for stream '%s > %s' in %v\",\n\t\t\tfriendlyBytes(int64(sr.State.Bytes)),\n\t\t\tmset.jsa.account.Name,\n\t\t\tmset.name(),\n\t\t\tend.Sub(start))\n\t}()\n}\n\n// Default chunk size for now.\nconst defaultSnapshotChunkSize = 256 * 1024\nconst defaultSnapshotWindowSize = 32 * 1024 * 1024 // 32MB\n\n// streamSnapshot will stream out our snapshot to the reply subject.\nfunc (s *Server) streamSnapshot(ci *ClientInfo, acc *Account, mset *stream, sr *SnapshotResult, req *JSApiStreamSnapshotRequest) {\n\tchunkSize := req.ChunkSize\n\tif chunkSize == 0 {\n\t\tchunkSize = defaultSnapshotChunkSize\n\t}\n\t// Setup for the chunk stream.\n\treply := req.DeliverSubject\n\tr := sr.Reader\n\tdefer r.Close()\n\n\t// Check interest for the snapshot deliver subject.\n\tinch := make(chan bool, 1)\n\tacc.sl.RegisterNotification(req.DeliverSubject, inch)\n\tdefer acc.sl.ClearNotification(req.DeliverSubject, inch)\n\thasInterest := <-inch\n\tif !hasInterest {\n\t\t// Allow 2 seconds or so for interest to show up.\n\t\tselect {\n\t\tcase <-inch:\n\t\tcase <-time.After(2 * time.Second):\n\t\t}\n\t}\n\n\t// Create our ack flow handler.\n\t// This is very simple for now.\n\tacks := make(chan struct{}, 1)\n\tacks <- struct{}{}\n\n\t// Track bytes outstanding.\n\tvar out int32\n\n\t// We will place sequence number and size of chunk sent in the reply.\n\tackSubj := fmt.Sprintf(jsSnapshotAckT, mset.name(), nuid.Next())\n\tackSub, _ := mset.subscribeInternalUnlocked(ackSubj+\".>\", func(_ *subscription, _ *client, _ *Account, subject, _ string, _ []byte) {\n\t\tcs, _ := strconv.Atoi(tokenAt(subject, 6))\n\t\t// This is very crude and simple, but ok for now.\n\t\t// This only matters when sending multiple chunks.\n\t\tif atomic.AddInt32(&out, int32(-cs)) < defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase acks <- struct{}{}:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t})\n\tdefer mset.unsubscribeUnlocked(ackSub)\n\n\t// TODO(dlc) - Add in NATS-Chunked-Sequence header\n\n\tfor index := 1; ; index++ {\n\t\tchunk := make([]byte, chunkSize)\n\t\tn, err := r.Read(chunk)\n\t\tchunk = chunk[:n]\n\t\tif err != nil {\n\t\t\tif n > 0 {\n\t\t\t\tmset.outq.send(&jsPubMsg{reply, _EMPTY_, _EMPTY_, nil, chunk, nil, 0, nil})\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t// Wait on acks for flow control if past our window size.\n\t\t// Wait up to 1ms for now if no acks received.\n\t\tif atomic.LoadInt32(&out) > defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase <-acks:\n\t\t\tcase <-inch: // Lost interest\n\t\t\t\tgoto done\n\t\t\tcase <-time.After(10 * time.Millisecond):\n\t\t\t}\n\t\t}\n\t\tackReply := fmt.Sprintf(\"%s.%d.%d\", ackSubj, len(chunk), index)\n\t\tmset.outq.send(&jsPubMsg{reply, _EMPTY_, ackReply, nil, chunk, nil, 0, nil})\n\t\tatomic.AddInt32(&out, int32(len(chunk)))\n\t}\ndone:\n\t// Send last EOF\n\t// TODO(dlc) - place hash in header\n\tmset.outq.send(&jsPubMsg{reply, _EMPTY_, _EMPTY_, nil, nil, nil, 0, nil})\n}\n\n// Request to create a durable consumer.\nfunc (s *Server) jsDurableCreateRequest(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\ts.jsConsumerCreate(sub, c, acc, subject, reply, msg, true)\n}\n\n// Request to create a consumer.\nfunc (s *Server) jsConsumerCreateRequest(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\ts.jsConsumerCreate(sub, c, acc, subject, reply, msg, false)\n}\n\nfunc (s *Server) jsConsumerCreate(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte, expectDurable bool) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\n\tvar streamName string\n\tif expectDurable {\n\t\tstreamName = tokenAt(subject, 6)\n\t} else {\n\t\tstreamName = tokenAt(subject, 5)\n\t}\n\n\tvar req CreateConsumerRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We reject if flow control is set without heartbeats.\n\tif req.Config.FlowControl && req.Config.Heartbeat == 0 {\n\t\tresp.Error = NewJSConsumerWithFlowControlNeedsHeartbeatsError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Make sure we have sane defaults.\n\tsetConsumerConfigDefaults(&req.Config)\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tif req.Config.Direct {\n\t\t\t// Check to see if we have this stream and are the stream leader.\n\t\t\tif !acc.JetStreamIsStreamLeader(streamName) {\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tjs, cc := s.getJetStreamCluster()\n\t\t\tif js == nil || cc == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Make sure we are meta leader.\n\t\t\tif !s.JetStreamIsLeader() {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif streamName != req.Stream {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif expectDurable {\n\t\tif numTokens(subject) != 7 {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotInSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Now check on requirements for durable request.\n\t\tif req.Config.Durable == _EMPTY_ {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotSetError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tconsumerName := tokenAt(subject, 7)\n\t\tif consumerName != req.Config.Durable {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotMatchSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tif numTokens(subject) != 5 {\n\t\t\tresp.Error = NewJSConsumerEphemeralWithDurableInSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif req.Config.Durable != _EMPTY_ {\n\t\t\tresp.Error = NewJSConsumerEphemeralWithDurableNameError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\tif s.JetStreamIsClustered() && !req.Config.Direct {\n\t\ts.jsClusteredConsumerRequest(ci, acc, subject, reply, rmsg, req.Stream, &req.Config)\n\t\treturn\n\t}\n\n\tstream, err := acc.lookupStream(req.Stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\to, err := stream.addConsumer(&req.Config)\n\tif err != nil {\n\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.ConsumerInfo = o.info()\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all consumer names.\nfunc (s *Server) jsConsumerNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerNamesResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerNamesResponseType},\n\t\tConsumers:   []string{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tvar numConsumers int\n\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tsas := cc.streams[acc.Name]\n\t\tif sas == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tsa := sas[streamName]\n\t\tif sa == nil || sa.err != nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tfor consumer := range sa.consumers {\n\t\t\tresp.Consumers = append(resp.Consumers, consumer)\n\t\t}\n\t\tif len(resp.Consumers) > 1 {\n\t\t\tsort.Slice(resp.Consumers, func(i, j int) bool { return strings.Compare(resp.Consumers[i], resp.Consumers[j]) < 0 })\n\t\t}\n\t\tnumConsumers = len(resp.Consumers)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t\tresp.Consumers = resp.Consumers[:offset]\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t} else {\n\t\tmset, err := acc.lookupStream(streamName)\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tobs := mset.getPublicConsumers()\n\t\tsort.Slice(obs, func(i, j int) bool {\n\t\t\treturn strings.Compare(obs[i].name, obs[j].name) < 0\n\t\t})\n\n\t\tnumConsumers = len(obs)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t}\n\n\t\tfor _, o := range obs[offset:] {\n\t\t\tresp.Consumers = append(resp.Consumers, o.String())\n\t\t\tif len(resp.Consumers) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numConsumers\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed consumer information.\nfunc (s *Server) jsConsumerListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerListResponseType},\n\t\tConsumers:   []*ConsumerInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() {\n\t\t\ts.jsClusteredConsumerListRequest(acc, ci, offset, streamName, subject, reply, msg)\n\t\t})\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.getPublicConsumers()\n\tsort.Slice(obs, func(i, j int) bool {\n\t\treturn strings.Compare(obs[i].name, obs[j].name) < 0\n\t})\n\n\tocnt := len(obs)\n\tif offset > ocnt {\n\t\toffset = ocnt\n\t}\n\n\tfor _, o := range obs[offset:] {\n\t\tresp.Consumers = append(resp.Consumers, o.info())\n\t\tif len(resp.Consumers) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = ocnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about an consumer.\nfunc (s *Server) jsConsumerInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tconsumerName := consumerNameFromSubject(subject)\n\n\tvar resp = JSApiConsumerInfoResponse{ApiResponse: ApiResponse{Type: JSApiConsumerInfoResponseType}}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the consumer is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa, ca := cc.isLeader(), js.streamAssignment(acc.Name, streamName), js.consumerAssignment(acc.Name, streamName, consumerName)\n\t\tourID := cc.meta.ID()\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && ca == nil {\n\t\t\t// We can't find the consumer, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif sa == nil {\n\t\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here the consumer is not present.\n\t\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if ca == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(ca.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the consumer assigned and a leader, so only the consumer leader should answer.\n\t\tif !acc.JetStreamIsConsumerLeader(streamName, consumerName) {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), ca.Group)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ca == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// We have a consumer assignment.\n\t\t\tjs.mu.RLock()\n\t\t\tvar node RaftNode\n\t\t\tif rg := ca.Group; rg != nil && rg.node != nil && rg.isMember(ourID) {\n\t\t\t\tnode = rg.node\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\t// Check if we should ignore all together.\n\t\t\tif node == nil {\n\t\t\t\t// We have been assigned and are pending.\n\t\t\t\tif ca.pending {\n\t\t\t\t\t// Send our config and defaults for state and no cluster info.\n\t\t\t\t\tresp.ConsumerInfo = &ConsumerInfo{\n\t\t\t\t\t\tStream:  ca.Stream,\n\t\t\t\t\t\tName:    ca.Name,\n\t\t\t\t\t\tCreated: ca.Created,\n\t\t\t\t\t\tConfig:  ca.Config,\n\t\t\t\t\t}\n\t\t\t\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif node != nil && (node.GroupLeader() != _EMPTY_ || node.HadPreviousLeader()) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here we are a member and this is just a new consumer that does not have a leader yet.\n\t\t\t// Will fall through and return what we have. All consumers can respond but this should be very rare\n\t\t\t// but makes more sense to clients when they try to create, get a consumer exists, and then do consumer info.\n\t\t}\n\t}\n\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumerName)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.ConsumerInfo = obs.info()\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete an Consumer.\nfunc (s *Server) jsConsumerDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerDeleteResponse{ApiResponse: ApiResponse{Type: JSApiConsumerDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\tconsumer := consumerNameFromSubject(subject)\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredConsumerDeleteRequest(ci, acc, stream, consumer, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumer)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif err := obs.delete(); err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// sendJetStreamAPIAuditAdvisor will send the audit event for a given event.\nfunc (s *Server) sendJetStreamAPIAuditAdvisory(ci *ClientInfo, acc *Account, subject, request, response string) {\n\ts.publishAdvisory(acc, JSAuditAdvisory, JSAPIAudit{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSAPIAuditType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tServer:   s.Name(),\n\t\tClient:   ci,\n\t\tSubject:  subject,\n\t\tRequest:  request,\n\t\tResponse: response,\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t})\n}\n", "idx": 1, "id": 14259, "msg": "This is a \"stream list response\" object, so I think \"missing,omitempty\" should be enough (no need for `_stream_names`)", "proj": "nats-io-nats-server", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -18,6 +18,7 @@ package drivertest // import \"gocloud.dev/internal/docstore/drivertest\"\n \n import (\n \t\"context\"\n+\t\"math\"\n \t\"math/rand\"\n \t\"sync\"\n \t\"testing\"", "y": 0, "oldf": "// Copyright 2019 The Go Cloud Development Kit Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package drivertest provides a conformance test for implementations of\n// driver.\npackage drivertest // import \"gocloud.dev/internal/docstore/drivertest\"\n\nimport (\n\t\"context\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/go-cmp/cmp\"\n\t\"github.com/google/uuid\"\n\t\"gocloud.dev/gcerrors\"\n\tds \"gocloud.dev/internal/docstore\"\n\t\"gocloud.dev/internal/docstore/driver\"\n)\n\n// Harness descibes the functionality test harnesses must provide to run\n// conformance tests.\ntype Harness interface {\n\t// MakeCollection makes a driver.Collection for testing.\n\tMakeCollection(context.Context) (driver.Collection, error)\n\n\t// Close closes resources used by the harness.\n\tClose()\n}\n\n// HarnessMaker describes functions that construct a harness for running tests.\n// It is called exactly once per test; Harness.Close() will be called when the test is complete.\ntype HarnessMaker func(ctx context.Context, t *testing.T) (Harness, error)\n\n// Enum of types not supported by native codecs. We chose to describe this negatively\n// (types that aren't supported rather than types that are) to make the more\n// inclusive cases easier to write. A driver can return nil for\n// CodecTester.UnsupportedTypes, then add values from this enum one by one until all\n// tests pass.\ntype UnsupportedType int\n\nconst (\n\t// Native codec doesn't support any unsigned integer type\n\tUint UnsupportedType = iota\n\t// Native codec doesn't support any complex type\n\tComplex\n\t// Native codec doesn't support arrays\n\tArrays\n\t// Native codec doesn't support full time precision\n\tNanosecondTimes\n)\n\n// CodecTester describes functions that encode and decode values using both the\n// docstore codec for a provider, and that provider's own \"native\" codec.\ntype CodecTester interface {\n\tUnsupportedTypes() []UnsupportedType\n\tNativeEncode(interface{}) (interface{}, error)\n\tNativeDecode(value, dest interface{}) error\n\tDocstoreEncode(interface{}) (interface{}, error)\n\tDocstoreDecode(value, dest interface{}) error\n}\n\n// RunConformanceTests runs conformance tests for provider implementations of docstore.\nfunc RunConformanceTests(t *testing.T, newHarness HarnessMaker, ct CodecTester) {\n\tt.Run(\"Create\", func(t *testing.T) { withCollection(t, newHarness, testCreate) })\n\tt.Run(\"Put\", func(t *testing.T) { withCollection(t, newHarness, testPut) })\n\tt.Run(\"Replace\", func(t *testing.T) { withCollection(t, newHarness, testReplace) })\n\tt.Run(\"Get\", func(t *testing.T) { withCollection(t, newHarness, testGet) })\n\tt.Run(\"Delete\", func(t *testing.T) { withCollection(t, newHarness, testDelete) })\n\tt.Run(\"Update\", func(t *testing.T) { withCollection(t, newHarness, testUpdate) })\n\tt.Run(\"Data\", func(t *testing.T) { withCollection(t, newHarness, testData) })\n\tt.Run(\"Codec\", func(t *testing.T) { testCodec(t, ct) })\n}\n\nconst KeyField = \"_id\"\n\nfunc withCollection(t *testing.T, newHarness HarnessMaker, f func(*testing.T, *ds.Collection)) {\n\tctx := context.Background()\n\th, err := newHarness(ctx, t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdefer h.Close()\n\n\tdc, err := h.MakeCollection(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tcoll := ds.NewCollection(dc)\n\tf(t, coll)\n}\n\ntype docmap = map[string]interface{}\n\nvar nonexistentDoc = docmap{KeyField: \"doesNotExist\"}\n\nfunc testCreate(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tnamed := docmap{KeyField: \"testCreate1\", \"b\": true}\n\tunnamed := docmap{\"b\": false}\n\t// Attempt to clean up\n\tdefer func() {\n\t\t_, _ = coll.Actions().Delete(named).Delete(unnamed).Do(ctx)\n\t}()\n\n\tcreateThenGet := func(doc docmap) {\n\t\tt.Helper()\n\t\tif err := coll.Create(ctx, doc); err != nil {\n\t\t\tt.Fatalf(\"Create: %v\", err)\n\t\t}\n\t\tgot := docmap{KeyField: doc[KeyField]}\n\t\tif err := coll.Get(ctx, got); err != nil {\n\t\t\tt.Fatalf(\"Get: %v\", err)\n\t\t}\n\t\t// got has a revision field that doc doesn't have.\n\t\tdoc[ds.RevisionField] = got[ds.RevisionField]\n\t\tif diff := cmp.Diff(got, doc); diff != \"\" {\n\t\t\tt.Fatal(diff)\n\t\t}\n\t}\n\n\tcreateThenGet(named)\n\tcreateThenGet(unnamed)\n\n\t// Can't create an existing doc.\n\tif err := coll.Create(ctx, named); err == nil {\n\t\tt.Error(\"got nil, want error\")\n\t}\n}\n\nfunc testPut(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tnamed := docmap{KeyField: \"testPut1\", \"b\": true}\n\t// Create a new doc.\n\tmust(coll.Put(ctx, named))\n\tgot := docmap{KeyField: named[KeyField]}\n\tmust(coll.Get(ctx, got))\n\tnamed[ds.RevisionField] = got[ds.RevisionField] // copy returned revision field\n\tif diff := cmp.Diff(got, named); diff != \"\" {\n\t\tt.Fatalf(diff)\n\t}\n\n\t// Replace an existing doc.\n\tnamed[\"b\"] = false\n\tmust(coll.Put(ctx, named))\n\tmust(coll.Get(ctx, got))\n\tnamed[ds.RevisionField] = got[ds.RevisionField]\n\tif diff := cmp.Diff(got, named); diff != \"\" {\n\t\tt.Fatalf(diff)\n\t}\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, func(dm docmap) error {\n\t\t\treturn coll.Put(ctx, dm)\n\t\t})\n\t})\n}\n\nfunc testReplace(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tdoc1 := docmap{KeyField: \"testReplace\", \"s\": \"a\"}\n\tmust(coll.Put(ctx, doc1))\n\tdoc1[\"s\"] = \"b\"\n\tmust(coll.Replace(ctx, doc1))\n\tgot := docmap{KeyField: doc1[KeyField]}\n\tmust(coll.Get(ctx, got))\n\tdoc1[ds.RevisionField] = got[ds.RevisionField] // copy returned revision field\n\tif diff := cmp.Diff(got, doc1); diff != \"\" {\n\t\tt.Fatalf(diff)\n\t}\n\t// Can't replace a nonexistent doc.\n\tif err := coll.Replace(ctx, nonexistentDoc); err == nil {\n\t\tt.Fatal(\"got nil, want error\")\n\t}\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, func(dm docmap) error {\n\t\t\treturn coll.Replace(ctx, dm)\n\t\t})\n\t})\n\n}\n\nfunc testGet(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tdoc := docmap{\n\t\tKeyField: \"testGet1\",\n\t\t\"s\":      \"a string\",\n\t\t\"i\":      int64(95),\n\t\t\"f\":      32.3,\n\t}\n\tmust(coll.Put(ctx, doc))\n\t// If only the key fields are present, the full document is populated.\n\tgot := docmap{KeyField: doc[KeyField]}\n\tmust(coll.Get(ctx, got))\n\tdoc[ds.RevisionField] = got[ds.RevisionField] // copy returned revision field\n\tif diff := cmp.Diff(got, doc); diff != \"\" {\n\t\tt.Error(diff)\n\t}\n\t// TODO(jba): test with field paths\n}\n\nfunc testDelete(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tdoc := docmap{KeyField: \"testDelete\"}\n\tif n, err := coll.Actions().Put(doc).Delete(doc).Do(ctx); err != nil {\n\t\tt.Fatalf(\"after %d successful actions: %v\", n, err)\n\t}\n\t// The document should no longer exist.\n\tif err := coll.Get(ctx, doc); err == nil {\n\t\tt.Error(\"want error, got nil\")\n\t}\n\t// Delete doesn't fail if the doc doesn't exist.\n\tif err := coll.Delete(ctx, nonexistentDoc); err != nil {\n\t\tt.Errorf(\"delete nonexistent doc: want nil, got %v\", err)\n\t}\n\n\t// Delete will fail if the revision field is mismatched.\n\tgot := docmap{KeyField: doc[KeyField]}\n\tif _, err := coll.Actions().Put(doc).Get(got).Do(ctx); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdoc[\"x\"] = \"y\"\n\tif err := coll.Put(ctx, doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := coll.Delete(ctx, got); gcerrors.Code(err) != gcerrors.FailedPrecondition {\n\t\tt.Errorf(\"got %v, want FailedPrecondition\", err)\n\t}\n}\n\nfunc testUpdate(t *testing.T, coll *ds.Collection) {\n\tctx := context.Background()\n\tdoc := docmap{KeyField: \"testUpdate\", \"a\": \"A\", \"b\": \"B\"}\n\tif err := coll.Put(ctx, doc); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tgot := docmap{KeyField: doc[KeyField]}\n\t_, err := coll.Actions().Update(doc, ds.Mods{\n\t\t\"a\": \"X\",\n\t\t\"b\": nil,\n\t\t\"c\": \"C\",\n\t}).Get(got).Do(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\twant := docmap{\n\t\tKeyField:         doc[KeyField],\n\t\tds.RevisionField: got[ds.RevisionField],\n\t\t\"a\":              \"X\",\n\t\t\"c\":              \"C\",\n\t}\n\tif !cmp.Equal(got, want) {\n\t\tt.Errorf(\"got %v, want %v\", got, want)\n\t}\n\n\t// TODO(jba): test that empty mods is a no-op.\n\n\t// Can't update a nonexistent doc.\n\tif err := coll.Update(ctx, nonexistentDoc, ds.Mods{\"x\": \"y\"}); err == nil {\n\t\tt.Error(\"nonexistent document: got nil, want error\")\n\t}\n\n\tt.Run(\"revision\", func(t *testing.T) {\n\t\ttestRevisionField(t, coll, func(dm docmap) error {\n\t\t\treturn coll.Update(ctx, dm, ds.Mods{\"s\": \"c\"})\n\t\t})\n\t})\n}\n\nfunc testRevisionField(t *testing.T, coll *ds.Collection, write func(docmap) error) {\n\tctx := context.Background()\n\tmust := func(err error) {\n\t\tt.Helper()\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\tdoc1 := docmap{KeyField: \"testRevisionField\", \"s\": \"a\"}\n\tmust(coll.Put(ctx, doc1))\n\tgot := docmap{KeyField: doc1[KeyField]}\n\tmust(coll.Get(ctx, got))\n\trev, ok := got[ds.RevisionField]\n\tif !ok || rev == nil {\n\t\tt.Fatal(\"missing revision field\")\n\t}\n\tgot[\"s\"] = \"b\"\n\t// A write should succeed, because the document hasn't changed since it was gotten.\n\tif err := write(got); err != nil {\n\t\tt.Fatalf(\"write with revision field got %v, want nil\", err)\n\t}\n\t// This write should fail: got's revision field hasn't changed, but the stored document has.\n\terr := write(got)\n\tif c := gcerrors.Code(err); c != gcerrors.FailedPrecondition && c != gcerrors.NotFound {\n\t\tt.Errorf(\"write with old revision field: got %v, wanted FailedPrecondition or NotFound\", err)\n\t}\n}\n\nfunc testData(t *testing.T, coll *ds.Collection) {\n\t// All Go integer types are supported, but they all come back as int64.\n\tctx := context.Background()\n\tfor _, test := range []struct {\n\t\tin, want interface{}\n\t}{\n\t\t{int(-1), int64(-1)},\n\t\t{int8(-8), int64(-8)},\n\t\t{int16(-16), int64(-16)},\n\t\t{int32(-32), int64(-32)},\n\t\t{int64(-64), int64(-64)},\n\t\t{uint(1), int64(1)},\n\t\t{uint8(8), int64(8)},\n\t\t{uint16(16), int64(16)},\n\t\t{uint32(32), int64(32)},\n\t\t{uint64(64), int64(64)},\n\t\t{float32(3.5), float64(3.5)},\n\t\t{[]byte{0, 1, 2}, []byte{0, 1, 2}},\n\t} {\n\t\tdoc := docmap{KeyField: \"testData\", \"val\": test.in}\n\t\tgot := docmap{KeyField: doc[KeyField]}\n\t\tif _, err := coll.Actions().Put(doc).Get(got).Do(ctx); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\twant := docmap{\n\t\t\t\"val\":            test.want,\n\t\t\tKeyField:         doc[KeyField],\n\t\t\tds.RevisionField: got[ds.RevisionField],\n\t\t}\n\t\tif len(got) != len(want) {\n\t\t\tt.Errorf(\"%v: got %v, want %v\", test.in, got, want)\n\t\t} else if g := got[\"val\"]; !cmp.Equal(g, test.want) {\n\t\t\tt.Errorf(\"%v: got %v (%T), want %v (%T)\", test.in, g, g, test.want, test.want)\n\t\t}\n\t}\n\n\t// TODO: strings: valid vs. invalid unicode\n\n}\n\nfunc testCodec(t *testing.T, ct CodecTester) {\n\tif ct == nil {\n\t\tt.Skip(\"no CodecTester\")\n\t}\n\t// A time with non-zero milliseconds, but zero nanoseconds.\n\tmilliTime := time.Date(2019, time.March, 27, 0, 0, 0, 5*1e6, time.UTC)\n\t// A time with non-zero nanoseconds.\n\tnanoTime := time.Date(2019, time.March, 27, 0, 0, 0, 5*1e6+7, time.UTC)\n\n\tcheck := func(in, dec interface{}, encode func(interface{}) (interface{}, error), decode func(interface{}, interface{}) error) {\n\t\tt.Helper()\n\t\tenc, err := encode(in)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%+v\", err)\n\t\t}\n\t\tif err := decode(enc, dec); err != nil {\n\t\t\tt.Fatalf(\"%+v\", err)\n\t\t}\n\t\tif diff := cmp.Diff(in, dec); diff != \"\" {\n\t\t\tt.Error(diff)\n\t\t}\n\t}\n\n\t// A round trip with the docstore codec should work for all docstore-supported types,\n\t// regardless of native driver support.\n\ttype DocstoreRoundTrip struct {\n\t\tN  *int\n\t\tI  int\n\t\tU  uint\n\t\tF  float64\n\t\tC  complex128\n\t\tSt string\n\t\tB  bool\n\t\tBy []byte\n\t\tL  []int\n\t\tA  [2]int\n\t\tM  map[string]bool\n\t\tP  *string\n\t\tT  time.Time\n\t}\n\t// TODO(jba): add more fields: structs; embedding.\n\n\ts := \"bar\"\n\tdsrt := &DocstoreRoundTrip{\n\t\tN:  nil,\n\t\tI:  1,\n\t\tU:  2,\n\t\tF:  2.5,\n\t\tC:  complex(3.0, 4.0),\n\t\tSt: \"foo\",\n\t\tB:  true,\n\t\tL:  []int{3, 4, 5},\n\t\tA:  [2]int{6, 7},\n\t\tM:  map[string]bool{\"a\": true, \"b\": false},\n\t\tBy: []byte{6, 7, 8},\n\t\tP:  &s,\n\t\tT:  milliTime,\n\t}\n\n\tcheck(dsrt, &DocstoreRoundTrip{}, ct.DocstoreEncode, ct.DocstoreDecode)\n\n\t// Test native-to-docstore and docstore-to-native round trips with a smaller set\n\t// of types.\n\n\t// All native codecs should support these types. If one doesn't, remove it from this\n\t// struct and make a new single-field struct for it.\n\ttype NativeMinimal struct {\n\t\tN  *int\n\t\tI  int\n\t\tF  float64\n\t\tSt string\n\t\tB  bool\n\t\tBy []byte\n\t\tL  []int\n\t\tM  map[string]bool\n\t\tP  *string\n\t\tT  time.Time\n\t}\n\tnm := &NativeMinimal{\n\t\tN:  nil,\n\t\tI:  1,\n\t\tF:  2.5,\n\t\tSt: \"foo\",\n\t\tB:  true,\n\t\tL:  []int{3, 4, 5},\n\t\tM:  map[string]bool{\"a\": true, \"b\": false},\n\t\tBy: []byte{6, 7, 8},\n\t\tP:  &s,\n\t\tT:  milliTime,\n\t}\n\tcheck(nm, &NativeMinimal{}, ct.DocstoreEncode, ct.NativeDecode)\n\tcheck(nm, &NativeMinimal{}, ct.NativeEncode, ct.DocstoreDecode)\n\n\t// Test various other types, unless they are unsupported.\n\tunsupported := map[UnsupportedType]bool{}\n\tfor _, u := range ct.UnsupportedTypes() {\n\t\tunsupported[u] = true\n\t}\n\n\t// Unsigned integers.\n\tif !unsupported[Uint] {\n\t\ttype Uint struct {\n\t\t\tU uint\n\t\t}\n\t\tu := &Uint{10}\n\t\tcheck(u, &Uint{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(u, &Uint{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\n\t// Complex numbers.\n\tif !unsupported[Complex] {\n\t\ttype Complex struct {\n\t\t\tC complex128\n\t\t}\n\t\tc := &Complex{complex(11, 12)}\n\t\tcheck(c, &Complex{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(c, &Complex{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\n\t// Arrays.\n\tif !unsupported[Arrays] {\n\t\ttype Arrays struct {\n\t\t\tA [2]int\n\t\t}\n\t\ta := &Arrays{[2]int{13, 14}}\n\t\tcheck(a, &Arrays{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(a, &Arrays{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n\t// Nanosecond-precision time.\n\ttype NT struct {\n\t\tT time.Time\n\t}\n\tnt := &NT{nanoTime}\n\tif unsupported[NanosecondTimes] {\n\t\t// Expect rounding to the nearest millisecond.\n\t\tcheck := func(encode func(interface{}) (interface{}, error), decode func(interface{}, interface{}) error) {\n\t\t\tenc, err := encode(nt)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\tvar got NT\n\t\t\tif err := decode(enc, &got); err != nil {\n\t\t\t\tt.Fatalf(\"%+v\", err)\n\t\t\t}\n\t\t\twant := nt.T.Round(time.Millisecond)\n\t\t\tif !got.T.Equal(want) {\n\t\t\t\tt.Errorf(\"got %v, want %v\", got.T, want)\n\t\t\t}\n\t\t}\n\t\tcheck(ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(ct.NativeEncode, ct.DocstoreDecode)\n\t} else {\n\t\t// Expect perfect round-tripping of nanosecond times.\n\t\tcheck(nt, &NT{}, ct.DocstoreEncode, ct.NativeDecode)\n\t\tcheck(nt, &NT{}, ct.NativeEncode, ct.DocstoreDecode)\n\t}\n}\n\n// Call when running tests that will be replayed.\n// Each seed value will result in UniqueString producing the same sequence of values.\nfunc MakeUniqueStringDeterministicForTesting(seed int64) {\n\tr := &randReader{r: rand.New(rand.NewSource(seed))}\n\tuuid.SetRand(r)\n}\n\ntype randReader struct {\n\tmu sync.Mutex\n\tr  *rand.Rand\n}\n\nfunc (r *randReader) Read(buf []byte) (int, error) {\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\treturn r.r.Read(buf)\n}\n", "idx": 1, "id": 16210, "msg": "", "proj": "google-go-cloud", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -277,6 +277,8 @@ const util = {\n       const androidNtpTilesResDest = path.join(config.srcDir, 'components', 'ntp_tiles', 'resources')\n       const androidResTemplateSource = path.join(config.projects['brave-core'].dir, 'android', 'java', 'res_template')\n       const androidResTemplateDest = path.join(config.srcDir, 'chrome', 'android', 'java', 'res_template')\n+      const androidSource = path.join(config.projects['brave-core'].dir, 'chromium_src', 'chrome', 'android')\n+      const androidSourceDest = path.join(config.srcDir, 'chrome', 'android')\n \n       // Mapping for copying Brave's Android resource into chromium folder.\n       const copyAndroidResourceMapping = {", "y": 1, "oldf": "const path = require('path')\nconst { spawn, spawnSync } = require('child_process')\nconst config = require('./config')\nconst fs = require('fs-extra')\nconst crypto = require('crypto')\nconst autoGeneratedBraveToChromiumMapping = Object.assign({}, require('./l10nUtil').autoGeneratedBraveToChromiumMapping)\nconst os = require('os')\n\nconst fixPywin32 = (options = {}) => {\n  if (process.platform !== 'win32') {\n    return\n  }\n  console.log(\"Manually installing pywin32 python module\")\n  util.run('python', ['-m', 'pip', 'install', 'pywin32'], options)\n}\n\nconst runGClient = (args, options = {}) => {\n  if (config.gClientVerbose) args.push('--verbose')\n  options.cwd = options.cwd || config.rootDir\n  options = mergeWithDefault(options)\n  options.env.GCLIENT_FILE = config.gClientFile\n  util.run('gclient', args, options)\n  fixPywin32(options)\n}\n\nconst mergeWithDefault = (options) => {\n  return Object.assign({}, config.defaultOptions, options)\n}\n\nconst util = {\n  run: (cmd, args = [], options = {}) => {\n    console.log(options.cwd + ':', cmd, args.join(' '))\n    const continueOnFail = options.continueOnFail\n    delete options.continueOnFail\n\n    const prog = spawnSync(cmd, args, options)\n    if (prog.status !== 0) {\n      if (!continueOnFail) {\n        console.log(prog.stdout && prog.stdout.toString())\n        console.error(prog.stderr && prog.stderr.toString())\n        process.exit(1)\n      }\n    }\n    return prog\n  },\n\n  runAsync: (cmd, args = [], options = {}) => {\n    let { continueOnFail, verbose, ...cmdOptions } = options\n    if (verbose) {\n      console.log(cmd, args.join(' '))\n    }\n    return new Promise((resolve, reject) => {\n      const prog = spawn(cmd, args, cmdOptions)\n      let stderr = ''\n      let stdout = ''\n      prog.stderr.on('data', data => {\n        stderr += data\n      })\n      prog.stdout.on('data', data => {\n        stdout += data\n      })\n      prog.on('close', statusCode => {\n        const hasFailed = statusCode !== 0\n        if (verbose && (!hasFailed || continueOnFail)) {\n          console.log(stdout)\n          if (stderr) {\n            console.error(stderr)\n          }\n        }\n        if (hasFailed) {\n          const err = new Error(`Program ${cmd} exited with error code ${statusCode}.`)\n          err.stderr = stderr\n          err.stdout = stdout\n          reject(err)\n          if (!continueOnFail) {\n            console.log(err.message)\n            console.log(stdout)\n            console.error(stderr)\n            process.exit(1)\n          }\n          return\n        }\n        resolve(stdout)\n      })\n    })\n  },\n\n  runGitAsync: function (repoPath, gitArgs, verbose = false, logError = false) {\n    return util.runAsync('git', gitArgs, { cwd: repoPath, verbose, continueOnFail: true })\n      .catch(err => {\n        if (logError) {\n          console.error(err.message)\n          console.error(`Git arguments were: ${gitArgs.join(' ')}`)\n          console.log(err.stdout)\n          console.error(err.stderr)\n        }\n        return Promise.reject(err)\n      })\n  },\n\n  buildGClientConfig: () => {\n    function replacer(key, value) {\n      return value;\n    }\n\n    let solutions = config.projectNames.filter((projectName) => config.projects[projectName].ref).map((projectName) => {\n      let project = config.projects[projectName]\n      return {\n        managed: \"%False%\",\n        name: project.gclientName,\n        url: project.url,\n        custom_deps: project.custom_deps\n      }\n    })\n\n    let cache_dir = process.env.GIT_CACHE_PATH ? ('\\ncache_dir = \"' + process.env.GIT_CACHE_PATH + '\"\\n') : '\\n'\n\n    let out = 'solutions = ' + JSON.stringify(solutions, replacer, 2)\n      .replace(/\"%None%\"/g, \"None\").replace(/\"%False%\"/g, \"False\") + cache_dir\n\n    if (config.targetOS === 'android') {\n      out = out + \"target_os = [ 'android' ]\"\n    } else if (config.targetOS === 'ios') {\n      out = out + \"target_os = [ 'ios' ]\"\n    }\n\n    fs.writeFileSync(config.defaultGClientFile, out)\n  },\n\n  calculateFileChecksum: (filename) => {\n    // adapted from https://github.com/roryrjb/md5-file\n    const BUFFER_SIZE = 8192\n    const fd = fs.openSync(filename, 'r')\n    const buffer = Buffer.alloc(BUFFER_SIZE)\n    const md5 = crypto.createHash('md5')\n\n    try {\n      let bytesRead\n      do {\n        bytesRead = fs.readSync(fd, buffer, 0, BUFFER_SIZE)\n        md5.update(buffer.slice(0, bytesRead))\n      } while (bytesRead === BUFFER_SIZE)\n    } finally {\n      fs.closeSync(fd)\n    }\n\n    return md5.digest('hex')\n  },\n\n  updateBranding: () => {\n    console.log('update branding...')\n    const chromeComponentsDir = path.join(config.srcDir, 'components')\n    const braveComponentsDir = path.join(config.projects['brave-core'].dir, 'components')\n    const chromeAppDir = path.join(config.srcDir, 'chrome', 'app')\n    const braveAppDir = path.join(config.projects['brave-core'].dir, 'app')\n    const chromeBrowserResourcesDir = path.join(config.srcDir, 'chrome', 'browser', 'resources')\n    const braveBrowserResourcesDir = path.join(config.projects['brave-core'].dir, 'browser', 'resources')\n    const braveAppVectorIconsDir = path.join(config.projects['brave-core'].dir, 'vector_icons', 'chrome', 'app')\n    const chromeAndroidJavaStringsTranslationsDir = path.join(config.srcDir, 'chrome', 'android', 'java', 'strings', 'translations')\n    const braveAndroidJavaStringsTranslationsDir = path.join(config.projects['brave-core'].dir, 'android', 'java', 'strings', 'translations')\n\n    let fileMap = new Set();\n    // The following 3 entries we map to the same name, not the chromium equivalent name for copying back\n    autoGeneratedBraveToChromiumMapping[path.join(braveAppDir, 'brave_strings.grd')] = path.join(chromeAppDir, 'brave_strings.grd')\n    autoGeneratedBraveToChromiumMapping[path.join(braveAppDir, 'settings_brave_strings.grdp')] = path.join(chromeAppDir, 'settings_brave_strings.grdp')\n    autoGeneratedBraveToChromiumMapping[path.join(braveComponentsDir, 'components_brave_strings.grd')] = path.join(chromeComponentsDir, 'components_brave_strings.grd')\n\n    Object.entries(autoGeneratedBraveToChromiumMapping).forEach(mapping => fileMap.add(mapping))\n\n    // Copy xtb files for:\n    // brave/app/resources/chromium_strings*.xtb\n    // brave/app/resources/generated_resoruces*.xtb\n    // brave/components/strings/components_chromium_strings*.xtb\n    // brave/android/java/strings/translations/android_chrome_strings*.xtb\n    fileMap.add([path.join(braveAppDir, 'resources'), path.join(chromeAppDir, 'resources')])\n    fileMap.add([path.join(braveComponentsDir, 'strings'), path.join(chromeComponentsDir, 'strings')])\n    fileMap.add([braveAndroidJavaStringsTranslationsDir, chromeAndroidJavaStringsTranslationsDir])\n    // By overwriting, we don't need to modify some grd files.\n    fileMap.add([path.join(braveAppDir, 'theme', 'brave'), path.join(chromeAppDir, 'theme', 'brave')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'brave'), path.join(chromeAppDir, 'theme', 'chromium')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_100_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_100_percent', 'brave')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_200_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_200_percent', 'brave')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_100_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_100_percent', 'chromium')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_200_percent', 'brave'), path.join(chromeAppDir, 'theme', 'default_200_percent', 'chromium')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_100_percent', 'common'), path.join(chromeAppDir, 'theme', 'default_100_percent', 'common')])\n    fileMap.add([path.join(braveAppDir, 'theme', 'default_200_percent', 'common'), path.join(chromeAppDir, 'theme', 'default_200_percent', 'common')])\n    fileMap.add([path.join(braveComponentsDir, 'resources', 'default_100_percent', 'brave'), path.join(chromeComponentsDir, 'resources', 'default_100_percent', 'chromium')])\n    fileMap.add([path.join(braveComponentsDir, 'resources', 'default_200_percent', 'brave'), path.join(chromeComponentsDir, 'resources', 'default_200_percent', 'chromium')])\n    fileMap.add([path.join(braveAppVectorIconsDir, 'vector_icons', 'brave'), path.join(chromeAppDir, 'vector_icons', 'brave')])\n    // Copy chrome-logo-faded.png for replacing chrome logo of welcome page with brave's on Win8.\n    fileMap.add([path.join(braveBrowserResourcesDir, 'chrome-logo-faded.png'), path.join(chromeBrowserResourcesDir, 'chrome-logo-faded.png')])\n    // Copy to make our ${branding_path_component}_behaviors.cc\n    fileMap.add([path.join(config.projects['brave-core'].dir, 'chromium_src', 'chrome', 'installer', 'setup', 'brave_behaviors.cc'),\n                 path.join(config.srcDir, 'chrome', 'installer', 'setup', 'brave_behaviors.cc')])\n\n    for (const [source, output] of fileMap) {\n      if (!fs.existsSync(source)) {\n        console.warn(`Warning: The following file-system entry was not found for copying contents to a chromium destination: ${source}. Consider removing the entry from the file-map, or investigating whether the correct source code reference is checked out.`)\n        continue\n      }\n\n      let sourceFiles = []\n\n      // get all the files if source if a directory\n      if (fs.statSync(source).isDirectory()) {\n        sourceFiles = util.walkSync(source)\n      } else {\n        sourceFiles = [source]\n      }\n\n      for (const sourceFile of sourceFiles) {\n        let destinationFile = path.join(output, path.relative(source, sourceFile))\n\n        // The destination file might be newer when updating chromium so\n        // we check for an exact match on the timestamp. We use seconds instead\n        // of ms because utimesSync doesn't set the times with ms precision\n        if (!fs.existsSync(destinationFile) ||\n            Math.floor(new Date(fs.statSync(sourceFile).mtimeMs).getTime() / 1000) !=\n            Math.floor(new Date(fs.statSync(destinationFile).mtimeMs).getTime() / 1000)) {\n          fs.copySync(sourceFile, destinationFile)\n          // can't set the date in the past so update the source file\n          // to match the newly copied destionation file\n          const date = fs.statSync(destinationFile).mtime\n          fs.utimesSync(sourceFile, date, date)\n          console.log(sourceFile + ' copied to ' + destinationFile)\n        }\n      }\n    }\n\n    if (process.platform === 'darwin') {\n      // Copy proper mac app icon for channel to chrome/app/theme/mac/app.icns.\n      // Each channel's app icons are stored in brave/app/theme/$channel/app.icns.\n      // With this copying, we don't need to modify chrome/BUILD.gn for this.\n      const iconSource = path.join(braveAppDir, 'theme', 'brave', 'mac', config.channel, 'app.icns')\n      const iconDest = path.join(chromeAppDir, 'theme', 'brave', 'mac', 'app.icns')\n      if (!fs.existsSync(iconDest) ||\n          util.calculateFileChecksum(iconSource) != util.calculateFileChecksum(iconDest)) {\n        console.log('copy app icon')\n        fs.copySync(iconSource, iconDest)\n      }\n\n      // Copy branding file\n      let branding_file_name = 'BRANDING'\n      if (config.channel)\n        branding_file_name = branding_file_name + '.' + config.channel\n\n      const brandingSource = path.join(braveAppDir, 'theme', 'brave', branding_file_name)\n      const brandingDest = path.join(chromeAppDir, 'theme', 'brave', 'BRANDING')\n      if (!fs.existsSync(brandingDest) ||\n          util.calculateFileChecksum(brandingSource) != util.calculateFileChecksum(brandingDest)) {\n        console.log('copy branding file')\n        fs.copySync(brandingSource, brandingDest)\n      }\n    }\n    if (config.targetOS === 'android') {\n\n      let androidIconSet = ''\n      if (config.channel === 'development') {\n        androidIconSet = 'res_brave_default'\n      } else if (config.channel === '') {\n        androidIconSet = 'res_brave'\n      } else if (config.channel === 'beta') {\n        androidIconSet = 'res_brave_beta'\n      } else if (config.channel === 'dev') {\n        androidIconSet = 'res_brave_dev'\n      } else if (config.channel === 'nightly') {\n        androidIconSet = 'res_brave_nightly'\n      }\n\n      const androidIconSource = path.join(braveAppDir, 'theme', 'brave', 'android', androidIconSet)\n      const androidIconDest = path.join(config.srcDir, 'chrome', 'android', 'java', 'res_chromium')\n      const androidResSource = path.join(config.projects['brave-core'].dir, 'android', 'java', 'res')\n      const androidResDest = path.join(config.srcDir, 'chrome', 'android', 'java', 'res')\n      const androidResNightSource = path.join(config.projects['brave-core'].dir, 'android', 'java', 'res_night')\n      const androidResNightDest = path.join(config.srcDir, 'chrome', 'android', 'java', 'res_night')\n      const androidNtpTilesResSource = path.join(config.projects['brave-core'].dir, 'components', 'ntp_tiles', 'resources')\n      const androidNtpTilesResDest = path.join(config.srcDir, 'components', 'ntp_tiles', 'resources')\n      const androidResTemplateSource = path.join(config.projects['brave-core'].dir, 'android', 'java', 'res_template')\n      const androidResTemplateDest = path.join(config.srcDir, 'chrome', 'android', 'java', 'res_template')\n\n      // Mapping for copying Brave's Android resource into chromium folder.\n      const copyAndroidResourceMapping = {\n        [androidIconSource]: androidIconDest,\n        [androidResSource]: androidResDest,\n        [androidResNightSource]: androidResNightDest,\n        [androidNtpTilesResSource]: androidNtpTilesResDest,\n        [androidResTemplateSource]: androidResTemplateDest\n      }\n\n      console.log('copy Android app icons and app resources')\n      Object.entries(copyAndroidResourceMapping).map(([sourcePath, destPath]) => {\n        let androidSourceFiles = []\n        if (fs.statSync(sourcePath).isDirectory()) {\n          androidSourceFiles = util.walkSync(sourcePath)\n        } else {\n          androidSourceFiles = [sourcePath]\n        }\n\n        for (const androidSourceFile of androidSourceFiles) {\n          let destinationFile = path.join(destPath, path.relative(sourcePath, androidSourceFile))\n          if (!fs.existsSync(destinationFile) || util.calculateFileChecksum(androidSourceFile) != util.calculateFileChecksum(destinationFile)) {\n            fs.copySync(androidSourceFile, destinationFile)\n          }\n        }\n      })\n    }\n  },\n\n  // Chromium compares pre-installed midl files and generated midl files from IDL during the build to check integrity.\n  // Generated files during the build time and upstream pre-installed files are different because we use different IDL file.\n  // So, we should copy our pre-installed files to overwrite upstream pre-installed files.\n  // After checking, pre-installed files are copied to gen dir and they are used to compile.\n  // So, this copying in every build doesn't affect compile performance.\n  updateOmahaMidlFiles: () => {\n    console.log('update omaha midl files...')\n    const srcDir = path.join(config.projects['brave-core'].dir, 'win_build_output', 'midl', 'google_update')\n    const dstDir = path.join(config.srcDir, 'third_party', 'win_build_output', 'midl', 'google_update')\n    fs.copySync(srcDir, dstDir)\n  },\n\n  // To build w/o much modification of upstream file, bundling mode is used. To build with this mode,\n  // widevine header file and cdm lib is needed. So, we use fake cdm lib. It only used by gn checking.\n  // Real cdm lib is only donwloaded and installed when user accepts via content settings bubble\n  // because we don't ship cdm lib by default.\n  // Latest version and download url are inserted to cdm header file and brave-core refers it.\n  prepareWidevineCdmBuild: () => {\n    const widevineDir = path.join(config.srcDir, 'third_party', 'widevine', 'cdm', 'linux', 'x64')\n    fs.ensureDirSync(widevineDir)\n\n    const widevineConfig = {\n      widevineDir,\n      headerFileContent: '',\n      configuredVersion: config.widevineVersion,\n      widevineCdmHeaderFilePath: path.join(widevineDir, 'widevine_cdm_version.h'),\n      fakeWidevineCdmLibFilePath: path.join(widevineDir, 'libwidevinecdm.so'),\n      fakeManifestJson: path.join(widevineDir, 'manifest.json')\n    }\n\n    widevineConfig.headerFileContent =\n`#ifndef WIDEVINE_CDM_VERSION_H_\n#define WIDEVINE_CDM_VERSION_H_\n#define WIDEVINE_CDM_VERSION_STRING \\\"${widevineConfig.configuredVersion}\\\"\n#define WIDEVINE_CDM_DOWNLOAD_URL_STRING \\\"https://redirector.gvt1.com/edgedl/widevine-cdm/${widevineConfig.configuredVersion}-linux-x64.zip\\\"\n#endif  // WIDEVINE_CDM_VERSION_H_`\n\n    // If version file or fake lib file aren't existed, create them.\n    if (!fs.existsSync(widevineConfig.widevineCdmHeaderFilePath) ||\n        !fs.existsSync(widevineConfig.fakeWidevineCdmLibFilePath) ||\n        !fs.existsSync(widevineConfig.fakeManifestJson)) {\n      util.doPrepareWidevineCdmBuild(widevineConfig)\n      return\n    }\n\n    // Check version file has latest version. If not create it.\n    // This can prevent unnecessary build by touched version file.\n    const installedHeaderFileContent = fs.readFileSync(widevineConfig.widevineCdmHeaderFilePath, 'utf8')\n    if (installedHeaderFileContent !== widevineConfig.headerFileContent) {\n      console.log(\"Current version file includes different version with latest\")\n      util.doPrepareWidevineCdmBuild(widevineConfig)\n    }\n  },\n\n  doPrepareWidevineCdmBuild: (widevineConfig) => {\n    console.log('prepare widevine cdm build in linux')\n\n    fs.writeFileSync(widevineConfig.widevineCdmHeaderFilePath, widevineConfig.headerFileContent)\n    fs.writeFileSync(widevineConfig.fakeWidevineCdmLibFilePath, '')\n    fs.writeFileSync(widevineConfig.fakeManifestJson, '{}')\n\n    // During the create_dist, /usr/lib/rpm/elfdeps requires that binaries have an exectuable bit set.\n    fs.chmodSync(widevineConfig.fakeWidevineCdmLibFilePath, 0o755)\n  },\n\n  signApp: (options = config.defaultOptions) => {\n    console.log('signing ...')\n    if (process.platform === 'win32') {\n      // Sign binaries used for widevine sig file generation.\n      // Other binaries will be done during the create_dist.\n      // Then, both are merged when archive for installer is created.\n      util.signWinBinaries()\n    } else {\n      util.run('ninja', ['-C', config.outputDir, config.signTarget], options)\n    }\n  },\n\n  // TODO(bridiver) - this should move to gn and windows should call signApp like other platforms\n  signWinBinaries: () => {\n    // Copy & sign only binaries for widevine sig file generation.\n    // With this, create_dist doesn't trigger rebuild because original binaries is not modified.\n    const dir = path.join(config.outputDir, 'signed_binaries')\n    if (!fs.existsSync(dir))\n      fs.mkdirSync(dir);\n\n    fs.copySync(path.join(config.outputDir, 'brave.exe'), path.join(dir, 'brave.exe'));\n    fs.copySync(path.join(config.outputDir, 'chrome.dll'), path.join(dir, 'chrome.dll'));\n    fs.copySync(path.join(config.outputDir, 'chrome_child.dll'), path.join(dir, 'chrome_child.dll'));\n\n     const core_dir = config.projects['brave-core'].dir\n    util.run('python', [path.join(core_dir, 'script', 'sign_binaries.py'), '--build_dir=' + dir])\n  },\n\n  generateWidevineSigFiles: () => {\n    if (process.platform !== 'win32')\n      return\n\n    const cert = config.sign_widevine_cert\n    const key = config.sign_widevine_key\n    const passwd = config.sign_widevine_passwd\n    const sig_generator = config.signature_generator\n    let src_dir = path.join(config.outputDir, 'signed_binaries')\n\n    if (config.skip_signing || process.env.CERT === undefined || process.env.SIGNTOOL_ARGS === undefined)\n      src_dir = config.outputDir\n\n    console.log('generate Widevine sig files...')\n\n    util.run('python', [sig_generator, '--input_file=' + path.join(src_dir, 'brave.exe'),\n        '--flags=1',\n        '--certificate=' + cert,\n        '--private_key=' + key,\n        '--output_file=' + path.join(config.outputDir, 'brave.exe.sig'),\n        '--private_key_passphrase=' + passwd])\n    util.run('python', [sig_generator, '--input_file=' + path.join(src_dir, 'chrome.dll'),\n        '--flags=0',\n        '--certificate=' + cert,\n        '--private_key=' + key,\n        '--output_file=' + path.join(config.outputDir, 'chrome.dll.sig'),\n        '--private_key_passphrase=' + passwd])\n    util.run('python', [sig_generator, '--input_file=' + path.join(src_dir, 'chrome_child.dll'),\n        '--flags=0',\n        '--certificate=' + cert,\n        '--private_key=' + key,\n        '--output_file=' + path.join(config.outputDir, 'chrome_child.dll.sig'),\n        '--private_key_passphrase=' + passwd])\n  },\n\n  buildTarget: (options = config.defaultOptions) => {\n    console.log('building ' + config.buildTarget + '...')\n\n    if (process.platform === 'win32') util.updateOmahaMidlFiles()\n    if (process.platform === 'linux') util.prepareWidevineCdmBuild()\n\n    let num_compile_failure = 1\n    if (config.ignore_compile_failure)\n      num_compile_failure = 0\n\n    const args = util.buildArgsToString(config.buildArgs())\n    util.run('gn', ['gen', config.outputDir, '--args=\"' + args + '\"'], options)\n\n    let ninjaOpts = [\n      '-C', config.outputDir, config.buildTarget,\n      '-k', num_compile_failure,\n      ...config.extraNinjaOpts\n    ]\n    util.run('ninja', ninjaOpts, options)\n  },\n\n  generateXcodeWorkspace: (options = config.defaultOptions) => {\n    console.log('generating Xcode workspace for \"' + config.xcode_gen_target + '\"...')\n\n    const args = util.buildArgsToString(config.buildArgs())\n    const genScript = path.join(config.rootDir, 'vendor', 'gn-project-generators', 'xcode.py')\n\n    const genArgs = [\n      'gen', config.outputDir + \"_Xcode\",\n      '--args=\"' + args + '\"',\n      '--ide=json',\n      '--json-ide-script=\"' + genScript + '\"',\n      '--filters=\"' + config.xcode_gen_target + '\"'\n    ]\n\n    util.run('gn', genArgs, options)\n  },\n\n  lint: (options = {}) => {\n    if (!options.base) {\n      options.base = 'origin/master';\n    }\n    let cmd_options = config.defaultOptions\n    cmd_options.cwd = config.projects['brave-core'].dir\n    util.run('vpython', [path.join(config.rootDir, 'scripts', 'lint.py'),\n        '--project_root=' + config.srcDir,\n        '--base_branch=' + options.base], cmd_options)\n  },\n\n  fixDepotTools: (options = {}) => {\n    if (process.platform !== 'win32') {\n      util.run('git', ['-C', config.depotToolsDir, 'clean', '-fxd'], options)\n      util.run('git', ['-C', config.depotToolsDir, 'reset', '--hard', 'HEAD'], options)\n      return\n    }\n    // On Windows:\n    // When depot_tools are already installed they redirect git to their own\n    // version which resides in a bootstrap-*_bin directory. So when we try to\n    // do git clean -fxd we fail because the git executable is in use in that\n    // directory. Get around that by using regular git.\n    let git_exes = util.run('where', ['git'], {shell: true})\n    let git_exe = '\"' + git_exes.stdout.toString().split(os.EOL)[0] + '\"'\n    if (git_exe === '\"\"') git_exe = 'git'\n    util.run(git_exe, ['-C', config.depotToolsDir, 'clean', '-fxd'], options)\n    util.run(git_exe, ['-C', config.depotToolsDir, 'reset', '--hard', 'HEAD'], options)\n\n    // Get around the error in updating depot_tools on windows due to pylint.bat\n    // file transitioning from untracked to a committed file. When\n    // update_depot_tools script tries to use git rebase it errors out. This is\n    // already fixed upstream, but we need a workaround for\n    // now. See https://bugs.chromium.org/p/chromium/issues/detail?id=996359\n    // The commit id in git merge-base command below is when pylint.bat was\n    // added to git.\n    let cmd_options = Object.assign({}, options)\n    cmd_options.continueOnFail = true\n    let is_fixed = util.run('git',\n      ['-C', config.depotToolsDir, 'merge-base', '--is-ancestor', '53297790de09e48c91678367b48528afbc9f71c1', 'HEAD'], cmd_options)\n    // If merge-base succeeds the exit code is 0.\n    if (!is_fixed.status) return\n    console.log(\"Manually updating depot_tools as a workaround for https://crbug.com/996359\")\n    util.run('git', ['-C', config.depotToolsDir, 'fetch', 'origin'], options)\n    util.run('git', ['-C', config.depotToolsDir, 'checkout', 'origin/master'], options)\n    util.run('git', ['-C', config.depotToolsDir, 'reset', '--hard', 'origin/master'], options)\n  },\n\n  submoduleSync: (options = {}) => {\n    if (!options.cwd) options.cwd = config.rootDir // default cwd `./src` may not exist yet\n    options = mergeWithDefault(options)\n    util.run('git', ['submodule', 'sync'], options)\n    util.run('git', ['submodule', 'update', '--init', '--recursive'], options)\n    util.fixDepotTools(options)\n  },\n\n  gclientSync: (reset = false, options = {}) => {\n    let args = ['sync', '--force', '--nohooks', '--with_branch_heads', '--with_tags']\n    if (reset)\n      args.push('--upstream')\n    runGClient(args, options)\n  },\n\n  gclientRunhooks: (options = {}) => {\n    runGClient(['runhooks'], options)\n  },\n\n  fetch: (gitRepoPath) => {\n    return util.runGitAsync(gitRepoPath, ['fetch', '--all', '--tags'])\n  },\n\n  setGitVersion: async (gitRepoPath, version, alwaysReset = false) => {\n    await util.runGitAsync(gitRepoPath, ['clean', '-f'])\n    let shouldReset = alwaysReset\n    if (!shouldReset) {\n      const headSHA = await util.runGitAsync(gitRepoPath, ['rev-parse', 'HEAD'])\n      const targetSHA = await util.runGitAsync(gitRepoPath, ['rev-parse', version])\n      shouldReset = (headSHA !== targetSHA)\n    }\n    if (shouldReset) {\n      await util.runGitAsync(gitRepoPath, ['reset', '--hard', version])\n    }\n    return shouldReset\n  },\n\n  buildArgsToString: (buildArgs) => {\n    let args = ''\n    for (let arg in buildArgs) {\n      let val = buildArgs[arg]\n      if (typeof val === 'string') {\n        val = '\"' + val + '\"'\n      } else {\n        val = JSON.stringify(val)\n      }\n      args += arg + '=' + val + ' '\n    }\n    return args.replace(/\"/g,'\\\\\"')\n  },\n\n  walkSync: (dir, filter = null, filelist = []) => {\n    fs.readdirSync(dir).forEach(file => {\n      if (fs.statSync(path.join(dir, file)).isDirectory()) {\n        filelist = util.walkSync(path.join(dir, file), filter, filelist)\n      } else if (!filter || filter.call(null, file)) {\n        filelist = filelist.concat(path.join(dir, file))\n      }\n    })\n    return filelist\n  }\n}\n\nmodule.exports = util\n", "idx": 1, "id": 6357, "msg": "Maybe call it `androidJavaSource`/`androidJavaDest`? Other looks good", "proj": "brave-brave-browser", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -543,7 +543,6 @@ func (c *temporalImpl) startMatching(hosts map[string][]string, startWG *sync.Wa\n \t}\n \tparams.ClusterMetadataConfig = c.clusterMetadataConfig\n \tparams.MetricsClient = metrics.NewClient(params.MetricsScope, metrics.GetMetricsServiceIdx(params.Name, c.logger))\n-\tparams.DynamicConfigClient = newIntegrationConfigClient(dynamicconfig.NewNoopClient())\n \tparams.ArchivalMetadata = c.archiverMetadata\n \tparams.ArchiverProvider = c.archiverProvider\n ", "y": 0, "oldf": "// The MIT License\n//\n// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n//\n// Copyright (c) 2020 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage host\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/uber-go/tally\"\n\t\"github.com/uber/tchannel-go\"\n\t\"go.temporal.io/api/workflowservice/v1\"\n\tsdkclient \"go.temporal.io/sdk/client\"\n\t\"go.uber.org/fx\"\n\n\t\"go.temporal.io/server/common/persistence/visibility\"\n\tesclient \"go.temporal.io/server/common/persistence/visibility/store/elasticsearch/client\"\n\n\t\"google.golang.org/grpc\"\n\n\t\"go.temporal.io/server/api/adminservice/v1\"\n\t\"go.temporal.io/server/api/historyservice/v1\"\n\t\"go.temporal.io/server/common\"\n\tcarchiver \"go.temporal.io/server/common/archiver\"\n\t\"go.temporal.io/server/common/archiver/provider\"\n\t\"go.temporal.io/server/common/authorization\"\n\t\"go.temporal.io/server/common/cluster\"\n\t\"go.temporal.io/server/common/config\"\n\t\"go.temporal.io/server/common/dynamicconfig\"\n\t\"go.temporal.io/server/common/log\"\n\t\"go.temporal.io/server/common/log/tag\"\n\t\"go.temporal.io/server/common/membership\"\n\t\"go.temporal.io/server/common/metrics\"\n\t\"go.temporal.io/server/common/namespace\"\n\t\"go.temporal.io/server/common/persistence\"\n\tpersistenceClient \"go.temporal.io/server/common/persistence/client\"\n\t\"go.temporal.io/server/common/resolver\"\n\t\"go.temporal.io/server/common/resource\"\n\t\"go.temporal.io/server/common/rpc\"\n\t\"go.temporal.io/server/service/frontend\"\n\t\"go.temporal.io/server/service/history\"\n\t\"go.temporal.io/server/service/matching\"\n\t\"go.temporal.io/server/service/worker\"\n\t\"go.temporal.io/server/service/worker/archiver\"\n\t\"go.temporal.io/server/service/worker/replicator\"\n)\n\n// Temporal hosts all of temporal services in one process\ntype Temporal interface {\n\tStart() error\n\tStop()\n\tGetAdminClient() adminservice.AdminServiceClient\n\tGetFrontendClient() workflowservice.WorkflowServiceClient\n\tGetHistoryClient() historyservice.HistoryServiceClient\n\tGetExecutionManager() persistence.ExecutionManager\n\tRefreshNamespaceCache()\n}\n\ntype (\n\ttemporalImpl struct {\n\t\tfrontendService resource.Resource\n\t\tmatchingService resource.Resource\n\t\thistoryServices []resource.Resource\n\t\tworkerService   resource.Resource\n\n\t\tadminClient                      adminservice.AdminServiceClient\n\t\tfrontendClient                   workflowservice.WorkflowServiceClient\n\t\thistoryClient                    historyservice.HistoryServiceClient\n\t\tlogger                           log.Logger\n\t\tclusterMetadataConfig            *config.ClusterMetadata\n\t\tpersistenceConfig                config.Persistence\n\t\tmetadataMgr                      persistence.MetadataManager\n\t\tclusterMetadataMgr               persistence.ClusterMetadataManager\n\t\tshardMgr                         persistence.ShardManager\n\t\ttaskMgr                          persistence.TaskManager\n\t\texecutionManager                 persistence.ExecutionManager\n\t\tnamespaceReplicationQueue        persistence.NamespaceReplicationQueue\n\t\tshutdownCh                       chan struct{}\n\t\tshutdownWG                       sync.WaitGroup\n\t\tclusterNo                        int // cluster number\n\t\treplicator                       *replicator.Replicator\n\t\tclientWorker                     archiver.ClientWorker\n\t\tarchiverMetadata                 carchiver.ArchivalMetadata\n\t\tarchiverProvider                 provider.ArchiverProvider\n\t\thistoryConfig                    *HistoryConfig\n\t\tesConfig                         *esclient.Config\n\t\tesClient                         esclient.Client\n\t\tworkerConfig                     *WorkerConfig\n\t\tmockAdminClient                  map[string]adminservice.AdminServiceClient\n\t\tnamespaceReplicationTaskExecutor namespace.ReplicationTaskExecutor\n\t}\n\n\t// HistoryConfig contains configs for history service\n\tHistoryConfig struct {\n\t\tNumHistoryShards       int32\n\t\tNumHistoryHosts        int\n\t\tHistoryCountLimitError int\n\t\tHistoryCountLimitWarn  int\n\t}\n\n\t// TemporalParams contains everything needed to bootstrap Temporal\n\tTemporalParams struct {\n\t\tClusterMetadataConfig            *config.ClusterMetadata\n\t\tPersistenceConfig                config.Persistence\n\t\tMetadataMgr                      persistence.MetadataManager\n\t\tClusterMetadataManager           persistence.ClusterMetadataManager\n\t\tShardMgr                         persistence.ShardManager\n\t\tExecutionManager                 persistence.ExecutionManager\n\t\tTaskMgr                          persistence.TaskManager\n\t\tNamespaceReplicationQueue        persistence.NamespaceReplicationQueue\n\t\tLogger                           log.Logger\n\t\tClusterNo                        int\n\t\tArchiverMetadata                 carchiver.ArchivalMetadata\n\t\tArchiverProvider                 provider.ArchiverProvider\n\t\tEnableReadHistoryFromArchival    bool\n\t\tHistoryConfig                    *HistoryConfig\n\t\tESConfig                         *esclient.Config\n\t\tESClient                         esclient.Client\n\t\tWorkerConfig                     *WorkerConfig\n\t\tMockAdminClient                  map[string]adminservice.AdminServiceClient\n\t\tNamespaceReplicationTaskExecutor namespace.ReplicationTaskExecutor\n\t}\n\n\tmembershipFactoryImpl struct {\n\t\tserviceName string\n\t\thosts       map[string][]string\n\t}\n)\n\n// NewTemporal returns an instance that hosts full temporal in one process\nfunc NewTemporal(params *TemporalParams) *temporalImpl {\n\treturn &temporalImpl{\n\t\tlogger:                           params.Logger,\n\t\tclusterMetadataConfig:            params.ClusterMetadataConfig,\n\t\tpersistenceConfig:                params.PersistenceConfig,\n\t\tmetadataMgr:                      params.MetadataMgr,\n\t\tclusterMetadataMgr:               params.ClusterMetadataManager,\n\t\tshardMgr:                         params.ShardMgr,\n\t\ttaskMgr:                          params.TaskMgr,\n\t\texecutionManager:                 params.ExecutionManager,\n\t\tnamespaceReplicationQueue:        params.NamespaceReplicationQueue,\n\t\tshutdownCh:                       make(chan struct{}),\n\t\tclusterNo:                        params.ClusterNo,\n\t\tesConfig:                         params.ESConfig,\n\t\tesClient:                         params.ESClient,\n\t\tarchiverMetadata:                 params.ArchiverMetadata,\n\t\tarchiverProvider:                 params.ArchiverProvider,\n\t\thistoryConfig:                    params.HistoryConfig,\n\t\tworkerConfig:                     params.WorkerConfig,\n\t\tmockAdminClient:                  params.MockAdminClient,\n\t\tnamespaceReplicationTaskExecutor: params.NamespaceReplicationTaskExecutor,\n\t}\n}\n\nfunc (c *temporalImpl) enableWorker() bool {\n\treturn c.workerConfig.EnableArchiver || c.workerConfig.EnableReplicator\n}\n\nfunc (c *temporalImpl) Start() error {\n\thosts := make(map[string][]string)\n\thosts[common.FrontendServiceName] = []string{c.FrontendGRPCAddress()}\n\thosts[common.MatchingServiceName] = []string{c.MatchingGRPCServiceAddress()}\n\thosts[common.HistoryServiceName] = c.HistoryServiceAddress(3)\n\tif c.enableWorker() {\n\t\thosts[common.WorkerServiceName] = []string{c.WorkerGRPCServiceAddress()}\n\t}\n\n\t// create temporal-system namespace, this must be created before starting\n\t// the services - so directly use the metadataManager to create this\n\tif err := c.createSystemNamespace(); err != nil {\n\t\treturn err\n\t}\n\n\tvar startWG sync.WaitGroup\n\tstartWG.Add(2)\n\tgo c.startHistory(hosts, &startWG)\n\tgo c.startMatching(hosts, &startWG)\n\tstartWG.Wait()\n\n\tstartWG.Add(1)\n\tgo c.startFrontend(hosts, &startWG)\n\tstartWG.Wait()\n\n\tif c.enableWorker() {\n\t\tstartWG.Add(1)\n\t\tgo c.startWorker(hosts, &startWG)\n\t\tstartWG.Wait()\n\t}\n\n\treturn nil\n}\n\nfunc (c *temporalImpl) Stop() {\n\tif c.enableWorker() {\n\t\tc.shutdownWG.Add(4)\n\t\tc.workerService.Stop()\n\t} else {\n\t\tc.shutdownWG.Add(3)\n\t}\n\tc.frontendService.Stop()\n\tfor _, historyService := range c.historyServices {\n\t\thistoryService.Stop()\n\t}\n\tc.matchingService.Stop()\n\tif c.workerConfig.EnableReplicator {\n\t\tc.replicator.Stop()\n\t}\n\tif c.workerConfig.EnableArchiver {\n\t\tc.clientWorker.Stop()\n\t}\n\tclose(c.shutdownCh)\n\tc.shutdownWG.Wait()\n}\n\nfunc (c *temporalImpl) FrontendGRPCAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7134\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8134\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9134\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10134\"\n\tdefault:\n\t\treturn \"127.0.0.1:7134\"\n\t}\n}\n\nfunc (c *temporalImpl) FrontendRingpopAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7124\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8124\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9124\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10124\"\n\tdefault:\n\t\treturn \"127.0.0.1:7124\"\n\t}\n}\n\n// penultimatePortDigit: 2 - ringpop, 3 - gRPC\nfunc (c *temporalImpl) HistoryServiceAddress(penultimatePortDigit int) []string {\n\tvar hosts []string\n\tstartPort := penultimatePortDigit * 10\n\tswitch c.clusterNo {\n\tcase 0:\n\t\tstartPort += 7201\n\tcase 1:\n\t\tstartPort += 8201\n\tcase 2:\n\t\tstartPort += 9201\n\tcase 3:\n\t\tstartPort += 10201\n\tdefault:\n\t\tstartPort += 7201\n\t}\n\tfor i := 0; i < c.historyConfig.NumHistoryHosts; i++ {\n\t\tport := startPort + i\n\t\thosts = append(hosts, fmt.Sprintf(\"127.0.0.1:%v\", port))\n\t}\n\n\tc.logger.Info(\"History hosts\", tag.Addresses(hosts))\n\treturn hosts\n}\n\nfunc (c *temporalImpl) MatchingGRPCServiceAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7136\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8136\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9136\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10136\"\n\tdefault:\n\t\treturn \"127.0.0.1:7136\"\n\t}\n}\n\nfunc (c *temporalImpl) MatchingServiceRingpopAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7126\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8126\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9126\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10126\"\n\tdefault:\n\t\treturn \"127.0.0.1:7126\"\n\t}\n}\n\nfunc (c *temporalImpl) WorkerGRPCServiceAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7138\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8138\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9138\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10138\"\n\tdefault:\n\t\treturn \"127.0.0.1:7138\"\n\t}\n}\n\nfunc (c *temporalImpl) WorkerServiceRingpopAddress() string {\n\tswitch c.clusterNo {\n\tcase 0:\n\t\treturn \"127.0.0.1:7128\"\n\tcase 1:\n\t\treturn \"127.0.0.1:8128\"\n\tcase 2:\n\t\treturn \"127.0.0.1:9128\"\n\tcase 3:\n\t\treturn \"127.0.0.1:10128\"\n\tdefault:\n\t\treturn \"127.0.0.1:7128\"\n\t}\n}\n\nfunc (c *temporalImpl) GetAdminClient() adminservice.AdminServiceClient {\n\treturn c.adminClient\n}\n\nfunc (c *temporalImpl) GetFrontendClient() workflowservice.WorkflowServiceClient {\n\treturn c.frontendClient\n}\n\nfunc (c *temporalImpl) GetHistoryClient() historyservice.HistoryServiceClient {\n\treturn c.historyClient\n}\n\nfunc (c *temporalImpl) startFrontend(hosts map[string][]string, startWG *sync.WaitGroup) {\n\tparams := &resource.BootstrapParams{}\n\tparams.DCRedirectionPolicy = config.DCRedirectionPolicy{}\n\tparams.Name = common.FrontendServiceName\n\tparams.Logger = c.logger\n\tparams.ThrottledLogger = c.logger\n\tparams.RPCFactory = newRPCFactoryImpl(common.FrontendServiceName, c.FrontendGRPCAddress(), c.FrontendRingpopAddress(),\n\t\tc.logger)\n\tparams.MetricsScope = tally.NewTestScope(common.FrontendServiceName, make(map[string]string))\n\tparams.MembershipFactoryInitializer = func(x persistenceClient.Bean, y log.Logger) (resource.MembershipMonitorFactory, error) {\n\t\treturn newMembershipFactory(params.Name, hosts), nil\n\t}\n\tparams.ClusterMetadataConfig = c.clusterMetadataConfig\n\tparams.MetricsClient = metrics.NewClient(params.MetricsScope, metrics.GetMetricsServiceIdx(params.Name, c.logger))\n\tparams.DynamicConfigClient = newIntegrationConfigClient(dynamicconfig.NewNoopClient())\n\tparams.ArchivalMetadata = c.archiverMetadata\n\tparams.ArchiverProvider = c.archiverProvider\n\tparams.ESConfig = c.esConfig\n\tparams.ESClient = c.esClient\n\tparams.Authorizer = authorization.NewNoopAuthorizer()\n\n\tvar err error\n\tparams.PersistenceConfig, err = copyPersistenceConfig(c.persistenceConfig)\n\tif err != nil {\n\t\tc.logger.Fatal(\"Failed to copy persistence config for frontend\", tag.Error(err))\n\t}\n\tparams.PersistenceServiceResolver = resolver.NewNoopResolver()\n\n\tif c.esConfig != nil {\n\t\tesDataStoreName := \"es-visibility\"\n\t\tparams.PersistenceConfig.AdvancedVisibilityStore = esDataStoreName\n\t\tparams.PersistenceConfig.DataStores[esDataStoreName] = config.DataStore{\n\t\t\tElasticsearch: c.esConfig,\n\t\t}\n\t}\n\n\tstoppedCh := make(chan struct{})\n\tvar frontendService *frontend.Service\n\tfeApp := fx.New(\n\t\tfx.Supply(\n\t\t\tparams,\n\t\t\tstoppedCh,\n\t\t),\n\t\tfrontend.Module,\n\t\tfx.Populate(&frontendService),\n\t)\n\terr = feApp.Err()\n\tif err != nil {\n\t\tparams.Logger.Fatal(\"unable to construct frontend service\", tag.Error(err))\n\t}\n\n\tif c.mockAdminClient != nil {\n\t\tclientBean := frontendService.GetClientBean()\n\t\tif clientBean != nil {\n\t\t\tfor serviceName, client := range c.mockAdminClient {\n\t\t\t\tclientBean.SetRemoteAdminClient(serviceName, client)\n\t\t\t}\n\t\t}\n\t}\n\n\tc.frontendService = frontendService\n\tconnection := params.RPCFactory.CreateFrontendGRPCConnection(c.FrontendGRPCAddress())\n\tc.frontendClient = NewFrontendClient(connection)\n\tc.adminClient = NewAdminClient(connection)\n\tgo frontendService.Start()\n\n\tstartWG.Done()\n\t<-c.shutdownCh\n\tc.shutdownWG.Done()\n}\n\nfunc (c *temporalImpl) startHistory(\n\thosts map[string][]string,\n\tstartWG *sync.WaitGroup,\n) {\n\tmembershipPorts := c.HistoryServiceAddress(2)\n\tfor i, grpcPort := range c.HistoryServiceAddress(3) {\n\t\tparams := &resource.BootstrapParams{}\n\t\tparams.Name = common.HistoryServiceName\n\t\tparams.Logger = c.logger\n\t\tparams.ThrottledLogger = c.logger\n\t\tparams.RPCFactory = newRPCFactoryImpl(common.HistoryServiceName, grpcPort, membershipPorts[i], c.logger)\n\t\tparams.MetricsScope = tally.NewTestScope(common.HistoryServiceName, make(map[string]string))\n\t\tparams.MembershipFactoryInitializer = func(x persistenceClient.Bean, y log.Logger) (resource.MembershipMonitorFactory, error) {\n\t\t\treturn newMembershipFactory(params.Name, hosts), nil\n\t\t}\n\t\tparams.ClusterMetadataConfig = c.clusterMetadataConfig\n\t\tparams.MetricsClient = metrics.NewClient(params.MetricsScope, metrics.GetMetricsServiceIdx(params.Name, c.logger))\n\t\tintegrationClient := newIntegrationConfigClient(dynamicconfig.NewNoopClient())\n\t\tc.overrideHistoryDynamicConfig(integrationClient)\n\t\tparams.DynamicConfigClient = integrationClient\n\n\t\tvar err error\n\t\tparams.SdkClient, err = sdkclient.NewClient(sdkclient.Options{\n\t\t\tHostPort:     c.FrontendGRPCAddress(),\n\t\t\tNamespace:    common.SystemLocalNamespace,\n\t\t\tMetricsScope: params.MetricsScope,\n\t\t\tConnectionOptions: sdkclient.ConnectionOptions{\n\t\t\t\tDisableHealthCheck: true,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed to create client for history\", tag.Error(err))\n\t\t}\n\n\t\tparams.ArchivalMetadata = c.archiverMetadata\n\t\tparams.ArchiverProvider = c.archiverProvider\n\t\tparams.ESConfig = c.esConfig\n\t\tparams.ESClient = c.esClient\n\n\t\tparams.PersistenceConfig, err = copyPersistenceConfig(c.persistenceConfig)\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed to copy persistence config for history\", tag.Error(err))\n\t\t}\n\t\tparams.PersistenceServiceResolver = resolver.NewNoopResolver()\n\n\t\tif c.esConfig != nil {\n\t\t\tesDataStoreName := \"es-visibility\"\n\t\t\tparams.PersistenceConfig.AdvancedVisibilityStore = esDataStoreName\n\t\t\tparams.PersistenceConfig.DataStores[esDataStoreName] = config.DataStore{\n\t\t\t\tElasticsearch: c.esConfig,\n\t\t\t}\n\t\t}\n\n\t\tstoppedCh := make(chan struct{})\n\t\tvar historyService *history.Service\n\t\tapp := fx.New(\n\t\t\tfx.Supply(\n\t\t\t\tparams,\n\t\t\t\tstoppedCh,\n\t\t\t),\n\t\t\thistory.Module,\n\t\t\tfx.Populate(&historyService))\n\t\terr = app.Err()\n\t\tif err != nil {\n\t\t\tparams.Logger.Fatal(\"unable to construct history service\", tag.Error(err))\n\t\t}\n\n\t\tif c.mockAdminClient != nil {\n\t\t\tclientBean := historyService.GetClientBean()\n\t\t\tif clientBean != nil {\n\t\t\t\tfor serviceName, client := range c.mockAdminClient {\n\t\t\t\t\tclientBean.SetRemoteAdminClient(serviceName, client)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// TODO: this is not correct when there are multiple history hosts as later client will overwrite previous ones.\n\t\t// However current interface for getting history client doesn't specify which client it needs and the tests that use this API\n\t\t// depends on the fact that there's only one history host.\n\t\t// Need to change those tests and modify the interface for getting history client.\n\t\thistoryConnection, err := rpc.Dial(c.HistoryServiceAddress(3)[0], nil, c.logger)\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed to create connection for history\", tag.Error(err))\n\t\t}\n\n\t\tc.historyClient = NewHistoryClient(historyConnection)\n\t\tc.historyServices = append(c.historyServices, historyService)\n\n\t\tgo historyService.Start()\n\t}\n\n\tstartWG.Done()\n\t<-c.shutdownCh\n\tc.shutdownWG.Done()\n}\n\nfunc (c *temporalImpl) startMatching(hosts map[string][]string, startWG *sync.WaitGroup) {\n\n\tparams := &resource.BootstrapParams{}\n\tparams.Name = common.MatchingServiceName\n\tparams.Logger = c.logger\n\tparams.ThrottledLogger = c.logger\n\tparams.RPCFactory = newRPCFactoryImpl(common.MatchingServiceName, c.MatchingGRPCServiceAddress(), c.MatchingServiceRingpopAddress(), c.logger)\n\tparams.MetricsScope = tally.NewTestScope(common.MatchingServiceName, make(map[string]string))\n\tparams.MembershipFactoryInitializer = func(x persistenceClient.Bean, y log.Logger) (resource.MembershipMonitorFactory, error) {\n\t\treturn newMembershipFactory(params.Name, hosts), nil\n\t}\n\tparams.ClusterMetadataConfig = c.clusterMetadataConfig\n\tparams.MetricsClient = metrics.NewClient(params.MetricsScope, metrics.GetMetricsServiceIdx(params.Name, c.logger))\n\tparams.DynamicConfigClient = newIntegrationConfigClient(dynamicconfig.NewNoopClient())\n\tparams.ArchivalMetadata = c.archiverMetadata\n\tparams.ArchiverProvider = c.archiverProvider\n\n\tvar err error\n\tparams.PersistenceConfig, err = copyPersistenceConfig(c.persistenceConfig)\n\tif err != nil {\n\t\tc.logger.Fatal(\"Failed to copy persistence config for matching\", tag.Error(err))\n\t}\n\tparams.PersistenceServiceResolver = resolver.NewNoopResolver()\n\n\tstoppedCh := make(chan struct{})\n\tvar matchingService *matching.Service\n\tapp := fx.New(\n\t\tfx.Supply(stoppedCh, params),\n\t\tmatching.Module,\n\t\tfx.Populate(&matchingService),\n\t)\n\terr = app.Err()\n\tif err != nil {\n\t\tparams.Logger.Fatal(\"unable to start matching service\", tag.Error(err))\n\t}\n\tif c.mockAdminClient != nil {\n\t\tclientBean := matchingService.GetClientBean()\n\t\tif clientBean != nil {\n\t\t\tfor serviceName, client := range c.mockAdminClient {\n\t\t\t\tclientBean.SetRemoteAdminClient(serviceName, client)\n\t\t\t}\n\t\t}\n\t}\n\tc.matchingService = matchingService\n\tgo c.matchingService.Start()\n\n\tstartWG.Done()\n\t<-c.shutdownCh\n\tc.shutdownWG.Done()\n}\n\nfunc (c *temporalImpl) startWorker(hosts map[string][]string, startWG *sync.WaitGroup) {\n\tparams := &resource.BootstrapParams{}\n\tparams.Name = common.WorkerServiceName\n\tparams.Logger = c.logger\n\tparams.ThrottledLogger = c.logger\n\tparams.RPCFactory = newRPCFactoryImpl(common.WorkerServiceName, c.WorkerGRPCServiceAddress(), c.WorkerServiceRingpopAddress(), c.logger)\n\tparams.MetricsScope = tally.NewTestScope(common.WorkerServiceName, make(map[string]string))\n\tparams.MembershipFactoryInitializer = func(x persistenceClient.Bean, y log.Logger) (resource.MembershipMonitorFactory, error) {\n\t\treturn newMembershipFactory(params.Name, hosts), nil\n\t}\n\tparams.ClusterMetadataConfig = c.clusterMetadataConfig\n\tparams.MetricsClient = metrics.NewClient(params.MetricsScope, metrics.GetMetricsServiceIdx(params.Name, c.logger))\n\tparams.DynamicConfigClient = newIntegrationConfigClient(dynamicconfig.NewNoopClient())\n\tparams.ArchivalMetadata = c.archiverMetadata\n\tparams.ArchiverProvider = c.archiverProvider\n\n\tvar err error\n\tparams.PersistenceConfig, err = copyPersistenceConfig(c.persistenceConfig)\n\tif err != nil {\n\t\tc.logger.Fatal(\"Failed to copy persistence config for worker\", tag.Error(err))\n\t}\n\tparams.PersistenceServiceResolver = resolver.NewNoopResolver()\n\n\tparams.SdkClient, err = sdkclient.NewClient(sdkclient.Options{\n\t\tHostPort:     c.FrontendGRPCAddress(),\n\t\tNamespace:    common.SystemLocalNamespace,\n\t\tMetricsScope: params.MetricsScope,\n\t\tConnectionOptions: sdkclient.ConnectionOptions{\n\t\t\tDisableHealthCheck: true,\n\t\t},\n\t})\n\tif err != nil {\n\t\tc.logger.Fatal(\"Failed to create client for worker\", tag.Error(err))\n\t}\n\n\tservice, err := resource.New(\n\t\tparams,\n\t\tcommon.WorkerServiceName,\n\t\tdynamicconfig.GetIntPropertyFn(5000),\n\t\tdynamicconfig.GetIntPropertyFn(5000),\n\t\tdynamicconfig.GetIntPropertyFn(10000),\n\t)\n\tif err != nil {\n\t\tparams.Logger.Fatal(\"unable to create worker service\", tag.Error(err))\n\t}\n\tc.workerService = service\n\tservice.Start()\n\n\tclusterMetadata := cluster.NewTestClusterMetadata(c.clusterMetadataConfig)\n\tvar replicatorNamespaceCache namespace.Registry\n\tif c.workerConfig.EnableReplicator {\n\t\tmetadataManager := persistence.NewMetadataPersistenceMetricsClient(c.metadataMgr, service.GetMetricsClient(), c.logger)\n\t\treplicatorNamespaceCache = namespace.NewRegistry(metadataManager, clusterMetadata.IsGlobalNamespaceEnabled(), service.GetMetricsClient(), service.GetLogger())\n\t\treplicatorNamespaceCache.Start()\n\t\tc.startWorkerReplicator(service, clusterMetadata)\n\t}\n\n\tvar clientWorkerNamespaceCache namespace.Registry\n\tif c.workerConfig.EnableArchiver {\n\t\tmetadataProxyManager := persistence.NewMetadataPersistenceMetricsClient(c.metadataMgr, service.GetMetricsClient(), c.logger)\n\t\tclientWorkerNamespaceCache = namespace.NewRegistry(metadataProxyManager, clusterMetadata.IsGlobalNamespaceEnabled(), service.GetMetricsClient(), service.GetLogger())\n\t\tclientWorkerNamespaceCache.Start()\n\t\tc.startWorkerClientWorker(params, service, clientWorkerNamespaceCache)\n\t}\n\n\tstartWG.Done()\n\t<-c.shutdownCh\n\tif c.workerConfig.EnableReplicator {\n\t\treplicatorNamespaceCache.Stop()\n\t}\n\tif c.workerConfig.EnableArchiver {\n\t\tclientWorkerNamespaceCache.Stop()\n\t}\n\tc.shutdownWG.Done()\n}\n\nfunc (c *temporalImpl) startWorkerReplicator(service resource.Resource, clusterMetadata cluster.Metadata) {\n\tserviceResolver, err := service.GetMembershipMonitor().GetResolver(common.WorkerServiceName)\n\tif err != nil {\n\t\tc.logger.Fatal(\"Fail to start replicator when start worker\", tag.Error(err))\n\t}\n\tc.replicator = replicator.NewReplicator(\n\t\tclusterMetadata,\n\t\tservice.GetClientBean(),\n\t\tc.logger,\n\t\tservice.GetMetricsClient(),\n\t\tservice.GetHostInfo(),\n\t\tserviceResolver,\n\t\tc.namespaceReplicationQueue,\n\t\tc.namespaceReplicationTaskExecutor,\n\t)\n\tc.replicator.Start()\n}\n\nfunc (c *temporalImpl) startWorkerClientWorker(params *resource.BootstrapParams, service resource.Resource, namespaceRegistry namespace.Registry) {\n\tworkerConfig := worker.NewConfig(params)\n\tworkerConfig.ArchiverConfig.ArchiverConcurrency = dynamicconfig.GetIntPropertyFn(10)\n\n\tbc := &archiver.BootstrapContainer{\n\t\tSdkClient:        params.SdkClient,\n\t\tMetricsClient:    service.GetMetricsClient(),\n\t\tLogger:           c.logger,\n\t\tHistoryV2Manager: c.executionManager,\n\t\tNamespaceCache:   namespaceRegistry,\n\t\tConfig:           workerConfig.ArchiverConfig,\n\t\tArchiverProvider: c.archiverProvider,\n\t}\n\tc.clientWorker = archiver.NewClientWorker(bc)\n\tif err := c.clientWorker.Start(); err != nil {\n\t\tc.clientWorker.Stop()\n\t\tc.logger.Fatal(\"Fail to start archiver when start worker\", tag.Error(err))\n\t}\n}\n\nfunc (c *temporalImpl) createSystemNamespace() error {\n\terr := c.metadataMgr.InitializeSystemNamespaces(c.clusterMetadataConfig.CurrentClusterName)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create temporal-system namespace: %v\", err)\n\t}\n\treturn nil\n}\n\nfunc (c *temporalImpl) GetExecutionManager() persistence.ExecutionManager {\n\treturn c.executionManager\n}\n\nfunc (c *temporalImpl) overrideHistoryDynamicConfig(client *dynamicClient) {\n\tclient.OverrideValue(dynamicconfig.ReplicationTaskProcessorStartWait, time.Nanosecond)\n\n\tif c.workerConfig.EnableIndexer {\n\t\tclient.OverrideValue(dynamicconfig.AdvancedVisibilityWritingMode, visibility.AdvancedVisibilityWritingModeDual)\n\t}\n\tif c.historyConfig.HistoryCountLimitWarn != 0 {\n\t\tclient.OverrideValue(dynamicconfig.HistoryCountLimitWarn, c.historyConfig.HistoryCountLimitWarn)\n\t}\n\tif c.historyConfig.HistoryCountLimitError != 0 {\n\t\tclient.OverrideValue(dynamicconfig.HistoryCountLimitError, c.historyConfig.HistoryCountLimitError)\n\t}\n}\n\nfunc (c *temporalImpl) RefreshNamespaceCache() {\n\tc.frontendService.GetNamespaceRegistry().Refresh()\n\tc.matchingService.GetNamespaceRegistry().Refresh()\n\tfor _, r := range c.historyServices {\n\t\tr.GetNamespaceRegistry().Refresh()\n\t}\n\tif c.workerService != nil {\n\t\tc.workerService.GetNamespaceRegistry().Refresh()\n\t}\n}\n\n// copyPersistenceConfig makes a deepcopy of persistence config.\n// This is just a temp fix for the race condition of persistence config.\n// The race condition happens because all the services are using the same datastore map in the config.\n// Also all services will retry to modify the maxQPS field in the datastore during start up and use the modified maxQPS value to create a persistence factory.\nfunc copyPersistenceConfig(pConfig config.Persistence) (config.Persistence, error) {\n\tcopiedDataStores := make(map[string]config.DataStore)\n\tfor name, value := range pConfig.DataStores {\n\t\tcopiedDataStore := config.DataStore{}\n\t\tencodedDataStore, err := json.Marshal(value)\n\t\tif err != nil {\n\t\t\treturn pConfig, err\n\t\t}\n\n\t\tif err = json.Unmarshal(encodedDataStore, &copiedDataStore); err != nil {\n\t\t\treturn pConfig, err\n\t\t}\n\t\tcopiedDataStores[name] = copiedDataStore\n\t}\n\tpConfig.DataStores = copiedDataStores\n\treturn pConfig, nil\n}\n\nfunc newMembershipFactory(serviceName string, hosts map[string][]string) resource.MembershipMonitorFactory {\n\treturn &membershipFactoryImpl{\n\t\tserviceName: serviceName,\n\t\thosts:       hosts,\n\t}\n}\n\nfunc (p *membershipFactoryImpl) GetMembershipMonitor() (membership.Monitor, error) {\n\treturn newSimpleMonitor(p.serviceName, p.hosts), nil\n}\n\ntype rpcFactoryImpl struct {\n\tserviceName        string\n\tringpopServiceName string\n\tgrpcHostPort       string\n\tringpopHostPort    string\n\tlogger             log.Logger\n\n\tsync.Mutex\n\tlistener       net.Listener\n\tringpopChannel *tchannel.Channel\n\tserverCfg      config.GroupTLS\n}\n\nfunc (c *rpcFactoryImpl) GetFrontendGRPCServerOptions() ([]grpc.ServerOption, error) {\n\treturn nil, nil\n}\n\nfunc (c *rpcFactoryImpl) GetInternodeGRPCServerOptions() ([]grpc.ServerOption, error) {\n\treturn nil, nil\n}\n\nfunc (c *rpcFactoryImpl) CreateFrontendGRPCConnection(hostName string) *grpc.ClientConn {\n\treturn c.CreateGRPCConnection(hostName)\n}\n\nfunc (c *rpcFactoryImpl) CreateInternodeGRPCConnection(hostName string) *grpc.ClientConn {\n\treturn c.CreateGRPCConnection(hostName)\n}\n\nfunc newRPCFactoryImpl(sName, grpcHostPort, ringpopHostPort string, logger log.Logger) common.RPCFactory {\n\treturn &rpcFactoryImpl{\n\t\tserviceName:     sName,\n\t\tgrpcHostPort:    grpcHostPort,\n\t\tringpopHostPort: ringpopHostPort,\n\t\tlogger:          logger,\n\t}\n}\n\nfunc (c *rpcFactoryImpl) GetGRPCListener() net.Listener {\n\tif c.listener != nil {\n\t\treturn c.listener\n\t}\n\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tif c.listener == nil {\n\t\tvar err error\n\t\tc.listener, err = net.Listen(\"tcp\", c.grpcHostPort)\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed create gRPC listener\", tag.Error(err), tag.Service(c.serviceName), tag.Address(c.grpcHostPort))\n\t\t}\n\n\t\tc.logger.Info(\"Created gRPC listener\", tag.Service(c.serviceName), tag.Address(c.grpcHostPort))\n\t}\n\n\treturn c.listener\n}\n\nfunc (c *rpcFactoryImpl) GetRingpopChannel() *tchannel.Channel {\n\tif c.ringpopChannel != nil {\n\t\treturn c.ringpopChannel\n\t}\n\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tif c.ringpopChannel == nil {\n\t\tringpopServiceName := fmt.Sprintf(\"%v-ringpop\", c.serviceName)\n\n\t\tvar err error\n\t\tc.ringpopChannel, err = tchannel.NewChannel(ringpopServiceName, nil)\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed to create ringpop TChannel\", tag.Error(err))\n\t\t}\n\n\t\terr = c.ringpopChannel.ListenAndServe(c.ringpopHostPort)\n\t\tif err != nil {\n\t\t\tc.logger.Fatal(\"Failed to start ringpop listener\", tag.Error(err), tag.Address(c.ringpopHostPort))\n\t\t}\n\t}\n\n\treturn c.ringpopChannel\n}\n\n// CreateGRPCConnection creates connection for gRPC calls\nfunc (c *rpcFactoryImpl) CreateGRPCConnection(hostName string) *grpc.ClientConn {\n\tconnection, err := rpc.Dial(hostName, nil, c.logger)\n\tif err != nil {\n\t\tc.logger.Fatal(\"Failed to create gRPC connection\", tag.Error(err))\n\t}\n\n\treturn connection\n}\n", "idx": 9, "id": 12873, "msg": "", "proj": "temporalio-temporal", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,20 @@\n+//\n+//  Copyright (C) 2021 Greg Landrum and other RDKit contributors\n+//\n+//   @@ All Rights Reserved @@\n+//  This file is part of the RDKit.\n+//  The contents are covered by the terms of the BSD license\n+//  which is included in the file license.txt, found at the root\n+//  of the RDKit source tree.\n+//\n+#include <iostream>\n+#include \"MonomerInfo.h\"\n+\n+using namespace RDKit;\n+\n+//! allows AtomPDBResidueInfo objects to be dumped to streams\n+std::ostream &operator<<(std::ostream &target, const AtomPDBResidueInfo &apri) {\n+  target << apri.getName() << \"(\" << apri.getSerialNumber()\n+         << \"): \" << apri.getResidueName() << \"-\" << apri.getResidueNumber();\n+  return target;\n+}", "y": 1, "oldf": "", "idx": 1, "id": 23595, "msg": "Is there a reason not to use PDB style output? I find it more \"comfortable\" due to having read too many pdb files... :)", "proj": "rdkit-rdkit", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -621,7 +621,9 @@ bool CoreChecks::VerifyFramebufferAndRenderPassLayouts(RenderPassCreateVersion r\n                 LayoutUseCheckAndMessage layout_check(subresource_map);\n                 bool subres_skip = false;\n                 auto pos = subresource_map->Find(view_state->normalized_subresource_range);\n-                for (; pos != subresource_map->End() && !subres_skip; ++pos) {\n+                // IncrementInterval skips over all the subresources that have the same state as we just checked, incrementing to\n+                // the next \"constant value\" range\n+                for (; !(pos.AtEnd()) && !subres_skip; pos.IncrementInterval()) {\n                     const VkImageSubresource &subres = pos->subresource;\n \n                     // Allow for differing depth and stencil layouts", "y": 1, "oldf": "/* Copyright (c) 2015-2020 The Khronos Group Inc.\n * Copyright (c) 2015-2020 Valve Corporation\n * Copyright (c) 2015-2020 LunarG, Inc.\n * Copyright (C) 2015-2020 Google Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * Author: Mark Lobodzinski <mark@lunarg.com>\n * Author: Dave Houlton <daveh@lunarg.com>\n * Shannon McPherson <shannon@lunarg.com>\n */\n\n#include <cmath>\n#include <set>\n#include <sstream>\n#include <string>\n\n#include \"vk_enum_string_helper.h\"\n#include \"vk_format_utils.h\"\n#include \"vk_layer_data.h\"\n#include \"vk_layer_utils.h\"\n#include \"vk_layer_logging.h\"\n#include \"vk_typemap_helper.h\"\n\n#include \"chassis.h\"\n#include \"core_validation.h\"\n#include \"shader_validation.h\"\n#include \"descriptor_sets.h\"\n#include \"buffer_validation.h\"\n\n// Transfer VkImageSubresourceLayers into VkImageSubresourceRange struct\nstatic VkImageSubresourceRange RangeFromLayers(const VkImageSubresourceLayers &subresource_layers) {\n    VkImageSubresourceRange subresource_range;\n    subresource_range.aspectMask = subresource_layers.aspectMask;\n    subresource_range.baseArrayLayer = subresource_layers.baseArrayLayer;\n    subresource_range.layerCount = subresource_layers.layerCount;\n    subresource_range.baseMipLevel = subresource_layers.mipLevel;\n    subresource_range.levelCount = 1;\n    return subresource_range;\n}\n\nstatic VkImageSubresourceRange MakeImageFullRange(const VkImageCreateInfo &create_info) {\n    const auto format = create_info.format;\n    VkImageSubresourceRange init_range{0, 0, VK_REMAINING_MIP_LEVELS, 0, VK_REMAINING_ARRAY_LAYERS};\n\n#ifdef VK_USE_PLATFORM_ANDROID_KHR\n    const VkExternalFormatANDROID *pExternalFormatANDROID = lvl_find_in_chain<VkExternalFormatANDROID>(&create_info);\n    bool isExternalFormatConversion = (pExternalFormatANDROID != nullptr && pExternalFormatANDROID->externalFormat != 0);\n#else\n    bool isExternalFormatConversion = false;\n#endif\n\n    if (FormatIsColor(format) || FormatIsMultiplane(format) || isExternalFormatConversion) {\n        init_range.aspectMask = VK_IMAGE_ASPECT_COLOR_BIT;  // Normalization will expand this for multiplane\n    } else {\n        init_range.aspectMask =\n            (FormatHasDepth(format) ? VK_IMAGE_ASPECT_DEPTH_BIT : 0) | (FormatHasStencil(format) ? VK_IMAGE_ASPECT_STENCIL_BIT : 0);\n    }\n    return NormalizeSubresourceRange(create_info, init_range);\n}\n\nstd::vector<VkImageView> FRAMEBUFFER_STATE::GetUsedAttachments(\n    const safe_VkSubpassDescription2 &subpasses, const std::vector<IMAGE_VIEW_STATE *> &imagelessFramebufferAttachments) {\n    std::vector<VkImageView> attachment_views(createInfo.attachmentCount, VK_NULL_HANDLE);\n\n    const bool imageless = (createInfo.flags & VK_FRAMEBUFFER_CREATE_IMAGELESS_BIT) ? true : false;\n\n    for (uint32_t index = 0; index < subpasses.inputAttachmentCount; ++index) {\n        const uint32_t attachment_index = subpasses.pInputAttachments[index].attachment;\n        if (attachment_index != VK_ATTACHMENT_UNUSED) {\n            if (imageless) {\n                attachment_views[attachment_index] = imagelessFramebufferAttachments[attachment_index]->image_view;\n            } else {\n                attachment_views[attachment_index] = createInfo.pAttachments[attachment_index];\n            }\n        }\n    }\n    for (uint32_t index = 0; index < subpasses.colorAttachmentCount; ++index) {\n        const uint32_t attachment_index = subpasses.pColorAttachments[index].attachment;\n        if (attachment_index != VK_ATTACHMENT_UNUSED) {\n            if (imageless) {\n                attachment_views[attachment_index] = imagelessFramebufferAttachments[attachment_index]->image_view;\n            } else {\n                attachment_views[attachment_index] = createInfo.pAttachments[attachment_index];\n            }\n        }\n        if (subpasses.pResolveAttachments) {\n            const uint32_t attachment_index2 = subpasses.pResolveAttachments[index].attachment;\n            if (attachment_index2 != VK_ATTACHMENT_UNUSED) {\n                if (imageless) {\n                    attachment_views[attachment_index2] = imagelessFramebufferAttachments[attachment_index2]->image_view;\n                } else {\n                    attachment_views[attachment_index2] = createInfo.pAttachments[attachment_index2];\n                }\n            }\n        }\n    }\n    if (subpasses.pDepthStencilAttachment) {\n        const uint32_t attachment_index = subpasses.pDepthStencilAttachment->attachment;\n        if (attachment_index != VK_ATTACHMENT_UNUSED) {\n            if (imageless) {\n                attachment_views[attachment_index] = imagelessFramebufferAttachments[attachment_index]->image_view;\n            } else {\n                attachment_views[attachment_index] = createInfo.pAttachments[attachment_index];\n            }\n        }\n    }\n    return attachment_views;\n}\n\nIMAGE_STATE::IMAGE_STATE(VkDevice dev, VkImage img, const VkImageCreateInfo *pCreateInfo)\n    : image(img),\n      safe_create_info(pCreateInfo),\n      createInfo(*safe_create_info.ptr()),\n      valid(false),\n      acquired(false),\n      shared_presentable(false),\n      layout_locked(false),\n      get_sparse_reqs_called(false),\n      sparse_metadata_required(false),\n      sparse_metadata_bound(false),\n      has_ahb_format(false),\n      is_swapchain_image(false),\n      ahb_format(0),\n      full_range{MakeImageFullRange(createInfo)},\n      create_from_swapchain(VK_NULL_HANDLE),\n      bind_swapchain(VK_NULL_HANDLE),\n      bind_swapchain_imageIndex(0),\n      range_encoder(full_range),\n      disjoint(false),\n      plane0_memory_requirements_checked(false),\n      plane1_memory_requirements_checked(false),\n      plane2_memory_requirements_checked(false),\n      subresource_encoder(full_range),\n      fragment_encoder(nullptr),\n      store_device_as_workaround(dev),  // TODO REMOVE WHEN encoder can be const\n      sparse_requirements{} {\n    if ((createInfo.sharingMode == VK_SHARING_MODE_CONCURRENT) && (createInfo.queueFamilyIndexCount > 0)) {\n        uint32_t *pQueueFamilyIndices = new uint32_t[createInfo.queueFamilyIndexCount];\n        for (uint32_t i = 0; i < createInfo.queueFamilyIndexCount; i++) {\n            pQueueFamilyIndices[i] = pCreateInfo->pQueueFamilyIndices[i];\n        }\n        createInfo.pQueueFamilyIndices = pQueueFamilyIndices;\n    }\n\n    if (createInfo.flags & VK_IMAGE_CREATE_SPARSE_BINDING_BIT) {\n        sparse = true;\n    }\n\n    auto *externalMemoryInfo = lvl_find_in_chain<VkExternalMemoryImageCreateInfo>(pCreateInfo->pNext);\n    if (externalMemoryInfo) {\n        external_memory_handle = externalMemoryInfo->handleTypes;\n    }\n}\n\nbool IMAGE_STATE::IsCreateInfoEqual(const VkImageCreateInfo &other_createInfo) const {\n    bool is_equal = (createInfo.sType == other_createInfo.sType) && (createInfo.flags == other_createInfo.flags);\n    is_equal = is_equal && IsImageTypeEqual(other_createInfo) && IsFormatEqual(other_createInfo);\n    is_equal = is_equal && IsMipLevelsEqual(other_createInfo) && IsArrayLayersEqual(other_createInfo);\n    is_equal = is_equal && IsUsageEqual(other_createInfo) && IsInitialLayoutEqual(other_createInfo);\n    is_equal = is_equal && IsExtentEqual(other_createInfo) && IsTilingEqual(other_createInfo);\n    is_equal = is_equal && IsSamplesEqual(other_createInfo) && IsSharingModeEqual(other_createInfo);\n    return is_equal &&\n           ((createInfo.sharingMode == VK_SHARING_MODE_CONCURRENT) ? IsQueueFamilyIndicesEqual(other_createInfo) : true);\n}\n\n// Check image compatibility rules for VK_NV_dedicated_allocation_image_aliasing\nbool IMAGE_STATE::IsCreateInfoDedicatedAllocationImageAliasingCompatible(const VkImageCreateInfo &other_createInfo) const {\n    bool is_compatible = (createInfo.sType == other_createInfo.sType) && (createInfo.flags == other_createInfo.flags);\n    is_compatible = is_compatible && IsImageTypeEqual(other_createInfo) && IsFormatEqual(other_createInfo);\n    is_compatible = is_compatible && IsMipLevelsEqual(other_createInfo);\n    is_compatible = is_compatible && IsUsageEqual(other_createInfo) && IsInitialLayoutEqual(other_createInfo);\n    is_compatible = is_compatible && IsSamplesEqual(other_createInfo) && IsSharingModeEqual(other_createInfo);\n    is_compatible = is_compatible &&\n                    ((createInfo.sharingMode == VK_SHARING_MODE_CONCURRENT) ? IsQueueFamilyIndicesEqual(other_createInfo) : true);\n    is_compatible = is_compatible && IsTilingEqual(other_createInfo);\n\n    is_compatible = is_compatible && createInfo.extent.width <= other_createInfo.extent.width &&\n                    createInfo.extent.height <= other_createInfo.extent.height &&\n                    createInfo.extent.depth <= other_createInfo.extent.depth &&\n                    createInfo.arrayLayers <= other_createInfo.arrayLayers;\n    return is_compatible;\n}\n\nbool IMAGE_STATE::IsCompatibleAliasing(IMAGE_STATE *other_image_state) {\n    if (!is_swapchain_image && !other_image_state->is_swapchain_image &&\n        !(createInfo.flags & other_image_state->createInfo.flags & VK_IMAGE_CREATE_ALIAS_BIT))\n        return false;\n    if ((create_from_swapchain == VK_NULL_HANDLE) && binding.mem_state &&\n        (binding.mem_state == other_image_state->binding.mem_state) && (binding.offset == other_image_state->binding.offset) &&\n        IsCreateInfoEqual(other_image_state->createInfo)) {\n        return true;\n    }\n    if ((bind_swapchain == other_image_state->bind_swapchain) && (bind_swapchain != VK_NULL_HANDLE)) {\n        return true;\n    }\n    return false;\n}\n\nIMAGE_VIEW_STATE::IMAGE_VIEW_STATE(const std::shared_ptr<IMAGE_STATE> &im, VkImageView iv, const VkImageViewCreateInfo *ci)\n    : image_view(iv),\n      create_info(*ci),\n      normalized_subresource_range(NormalizeSubresourceRange(*im, ci->subresourceRange)),\n      range_generator(im->subresource_encoder, normalized_subresource_range),\n      samplerConversion(VK_NULL_HANDLE),\n      image_state(im) {\n    auto *conversionInfo = lvl_find_in_chain<VkSamplerYcbcrConversionInfo>(create_info.pNext);\n    if (conversionInfo) samplerConversion = conversionInfo->conversion;\n    if (image_state) {\n        // A light normalization of the createInfo range\n        auto &sub_res_range = create_info.subresourceRange;\n        sub_res_range.levelCount = ResolveRemainingLevels(&sub_res_range, image_state->createInfo.mipLevels);\n        sub_res_range.layerCount = ResolveRemainingLayers(&sub_res_range, image_state->createInfo.arrayLayers);\n\n        // Cache a full normalization (for \"full image/whole image\" comparisons)\n        // normalized_subresource_range = NormalizeSubresourceRange(*image_state, ci->subresourceRange);\n        samples = image_state->createInfo.samples;\n\n        if (image_state->has_ahb_format) {\n            // When the image has a external format the views format must be VK_FORMAT_UNDEFINED and it is required to use a sampler\n            // Ycbcr conversion. Thus we can't extract any meaningful information from the format parameter. As a Sampler Ycbcr\n            // conversion must be used the shader type is always float.\n            descriptor_format_bits = DESCRIPTOR_REQ_COMPONENT_TYPE_FLOAT;\n        } else {\n            descriptor_format_bits = DescriptorRequirementsBitsFromFormat(create_info.format);\n        }\n    }\n}\n\nbool IMAGE_VIEW_STATE::OverlapSubresource(const IMAGE_VIEW_STATE &compare_view) const {\n    if (image_view == compare_view.image_view) {\n        return true;\n    }\n    if (image_state->image != compare_view.image_state->image) {\n        return false;\n    }\n    if (normalized_subresource_range.aspectMask != compare_view.normalized_subresource_range.aspectMask) {\n        return false;\n    }\n\n    // compare if overlap mip level\n    if ((normalized_subresource_range.baseMipLevel < compare_view.normalized_subresource_range.baseMipLevel) &&\n        ((normalized_subresource_range.baseMipLevel + normalized_subresource_range.levelCount) <=\n         compare_view.normalized_subresource_range.baseMipLevel)) {\n        return false;\n    }\n\n    if ((normalized_subresource_range.baseMipLevel > compare_view.normalized_subresource_range.baseMipLevel) &&\n        (normalized_subresource_range.baseMipLevel >=\n         (compare_view.normalized_subresource_range.baseMipLevel + compare_view.normalized_subresource_range.levelCount))) {\n        return false;\n    }\n\n    // compare if overlap array layer\n    if ((normalized_subresource_range.baseArrayLayer < compare_view.normalized_subresource_range.baseArrayLayer) &&\n        ((normalized_subresource_range.baseArrayLayer + normalized_subresource_range.layerCount) <=\n         compare_view.normalized_subresource_range.baseArrayLayer)) {\n        return false;\n    }\n\n    if ((normalized_subresource_range.baseArrayLayer > compare_view.normalized_subresource_range.baseArrayLayer) &&\n        (normalized_subresource_range.baseArrayLayer >=\n         (compare_view.normalized_subresource_range.baseArrayLayer + compare_view.normalized_subresource_range.layerCount))) {\n        return false;\n    }\n    return true;\n}\n\nconst cvdescriptorset::Descriptor *CMD_BUFFER_STATE::GetDescriptor(VkShaderStageFlagBits shader_stage, uint32_t set,\n                                                                   uint32_t binding, uint32_t index) const {\n    VkPipelineBindPoint bind_point;\n\n    if (shader_stage & VK_SHADER_STAGE_ALL_GRAPHICS) {\n        bind_point = VK_PIPELINE_BIND_POINT_GRAPHICS;\n    } else if (shader_stage & VK_SHADER_STAGE_COMPUTE_BIT) {\n        bind_point = VK_PIPELINE_BIND_POINT_COMPUTE;\n    } else {\n        bind_point = VK_PIPELINE_BIND_POINT_RAY_TRACING_KHR;\n    }\n\n    return GetDescriptor(bind_point, set, binding, index);\n}\n\nconst cvdescriptorset::Descriptor *CMD_BUFFER_STATE::GetDescriptor(VkPipelineBindPoint bind_point, uint32_t set, uint32_t binding,\n                                                                   uint32_t index) const {\n    const auto last_bound_it = lastBound.find(bind_point);\n    if (last_bound_it == lastBound.cend()) {\n        return nullptr;\n    }\n    if (set >= last_bound_it->second.per_set.size()) {\n        return nullptr;\n    }\n    return last_bound_it->second.per_set[set].bound_descriptor_set->GetDescriptorFromBinding(binding, index);\n}\n\nuint32_t FullMipChainLevels(uint32_t height, uint32_t width, uint32_t depth) {\n    // uint cast applies floor()\n    return 1u + (uint32_t)log2(std::max({height, width, depth}));\n}\n\nuint32_t FullMipChainLevels(VkExtent3D extent) { return FullMipChainLevels(extent.height, extent.width, extent.depth); }\n\nuint32_t FullMipChainLevels(VkExtent2D extent) { return FullMipChainLevels(extent.height, extent.width); }\n\nbool CoreChecks::FindLayouts(VkImage image, std::vector<VkImageLayout> &layouts) const {\n    auto image_state = GetImageState(image);\n    if (!image_state) return false;\n\n    const auto *layout_range_map = GetLayoutRangeMap(imageLayoutMap, image);\n    if (!layout_range_map) return false;\n    // TODO: FindLayouts function should mutate into a ValidatePresentableLayout with the loop wrapping the LogError\n    //       from the caller. You can then use decode to add the subresource of the range::begin to the error message.\n\n    // TODO: what is this test and what is it supposed to do?! -- the logic doesn't match the comment below?!\n\n    // TODO: Make this robust for >1 aspect mask. Now it will just say ignore potential errors in this case.\n    if (layout_range_map->size() >= (image_state->createInfo.arrayLayers * image_state->createInfo.mipLevels + 1)) {\n        return false;\n    }\n\n    for (auto entry : *layout_range_map) {\n        layouts.push_back(entry.second);\n    }\n    return true;\n}\n\n// Set image layout for given VkImageSubresourceRange struct\nvoid CoreChecks::SetImageLayout(CMD_BUFFER_STATE *cb_node, const IMAGE_STATE &image_state,\n                                const VkImageSubresourceRange &image_subresource_range, VkImageLayout layout,\n                                VkImageLayout expected_layout) {\n    auto *subresource_map = GetImageSubresourceLayoutMap(cb_node, image_state);\n    assert(subresource_map);  // the non-const getter must return a valid pointer\n    if (subresource_map->SetSubresourceRangeLayout(*cb_node, image_subresource_range, layout, expected_layout)) {\n        cb_node->image_layout_change_count++;  // Change the version of this data to force revalidation\n    }\n    for (const auto &image : image_state.aliasing_images) {\n        auto alias_state = GetImageState(image);\n        // The map state of the aliases should all be in sync, so no need to check the return value\n        subresource_map = GetImageSubresourceLayoutMap(cb_node, *alias_state);\n        assert(subresource_map);\n        subresource_map->SetSubresourceRangeLayout(*cb_node, image_subresource_range, layout, expected_layout);\n    }\n}\n\n// Set the initial image layout for all slices of an image view\nvoid CoreChecks::SetImageViewInitialLayout(CMD_BUFFER_STATE *cb_node, const IMAGE_VIEW_STATE &view_state, VkImageLayout layout) {\n    if (disabled[image_layout_validation]) {\n        return;\n    }\n    IMAGE_STATE *image_state = view_state.image_state.get();\n    auto *subresource_map = GetImageSubresourceLayoutMap(cb_node, *image_state);\n    subresource_map->SetSubresourceRangeInitialLayout(*cb_node, layout, view_state);\n    for (const auto &image : image_state->aliasing_images) {\n        image_state = GetImageState(image);\n        subresource_map = GetImageSubresourceLayoutMap(cb_node, *image_state);\n        subresource_map->SetSubresourceRangeInitialLayout(*cb_node, layout, view_state);\n    }\n}\n\n// Set the initial image layout for a passed non-normalized subresource range\nvoid CoreChecks::SetImageInitialLayout(CMD_BUFFER_STATE *cb_node, const IMAGE_STATE &image_state,\n                                       const VkImageSubresourceRange &range, VkImageLayout layout) {\n    auto *subresource_map = GetImageSubresourceLayoutMap(cb_node, image_state);\n    assert(subresource_map);\n    subresource_map->SetSubresourceRangeInitialLayout(*cb_node, NormalizeSubresourceRange(image_state, range), layout);\n    for (const auto &image : image_state.aliasing_images) {\n        auto alias_state = GetImageState(image);\n        subresource_map = GetImageSubresourceLayoutMap(cb_node, *alias_state);\n        assert(subresource_map);\n        subresource_map->SetSubresourceRangeInitialLayout(*cb_node, NormalizeSubresourceRange(*alias_state, range), layout);\n    }\n}\n\nvoid CoreChecks::SetImageInitialLayout(CMD_BUFFER_STATE *cb_node, VkImage image, const VkImageSubresourceRange &range,\n                                       VkImageLayout layout) {\n    const IMAGE_STATE *image_state = GetImageState(image);\n    if (!image_state) return;\n    SetImageInitialLayout(cb_node, *image_state, range, layout);\n};\n\nvoid CoreChecks::SetImageInitialLayout(CMD_BUFFER_STATE *cb_node, const IMAGE_STATE &image_state,\n                                       const VkImageSubresourceLayers &layers, VkImageLayout layout) {\n    SetImageInitialLayout(cb_node, image_state, RangeFromLayers(layers), layout);\n}\n\n// Set image layout for all slices of an image view\nvoid CoreChecks::SetImageViewLayout(CMD_BUFFER_STATE *cb_node, const IMAGE_VIEW_STATE &view_state, VkImageLayout layout,\n                                    VkImageLayout layoutStencil) {\n    IMAGE_STATE *image_state = view_state.image_state.get();\n\n    VkImageSubresourceRange sub_range = view_state.normalized_subresource_range;\n    // When changing the layout of a 3D image subresource via a 2D or 2D_ARRRAY image view, all depth slices of\n    // the subresource mip level(s) are transitioned, ignoring any layers restriction in the subresource info.\n    if ((image_state->createInfo.imageType == VK_IMAGE_TYPE_3D) && (view_state.create_info.viewType != VK_IMAGE_VIEW_TYPE_3D)) {\n        sub_range.baseArrayLayer = 0;\n        sub_range.layerCount = image_state->createInfo.extent.depth;\n    }\n\n    if (sub_range.aspectMask == (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT) && layoutStencil != kInvalidLayout) {\n        sub_range.aspectMask = VK_IMAGE_ASPECT_DEPTH_BIT;\n        SetImageLayout(cb_node, *image_state, sub_range, layout);\n        sub_range.aspectMask = VK_IMAGE_ASPECT_STENCIL_BIT;\n        SetImageLayout(cb_node, *image_state, sub_range, layoutStencil);\n    } else {\n        SetImageLayout(cb_node, *image_state, sub_range, layout);\n    }\n}\n\nbool CoreChecks::ValidateRenderPassLayoutAgainstFramebufferImageUsage(RenderPassCreateVersion rp_version, VkImageLayout layout,\n                                                                      VkImage image, VkImageView image_view,\n                                                                      VkFramebuffer framebuffer, VkRenderPass renderpass,\n                                                                      uint32_t attachment_index, const char *variable_name) const {\n    bool skip = false;\n    auto image_state = GetImageState(image);\n    const char *vuid;\n    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);\n    const char *function_name = use_rp2 ? \"vkCmdBeginRenderPass2()\" : \"vkCmdBeginRenderPass()\";\n\n    if (!image_state) {\n        LogObjectList objlist(image);\n        objlist.add(renderpass);\n        objlist.add(framebuffer);\n        objlist.add(image_view);\n        skip |=\n            LogError(image, \"VUID-VkRenderPassBeginInfo-framebuffer-parameter\",\n                     \"%s: RenderPass %s uses %s where pAttachments[%\" PRIu32 \"] = %s, which refers to an invalid image\",\n                     function_name, report_data->FormatHandle(renderpass).c_str(), report_data->FormatHandle(framebuffer).c_str(),\n                     attachment_index, report_data->FormatHandle(image_view).c_str());\n        return skip;\n    }\n\n    auto image_usage = image_state->createInfo.usage;\n    const auto stencil_usage_info = lvl_find_in_chain<VkImageStencilUsageCreateInfo>(image_state->createInfo.pNext);\n    if (stencil_usage_info) {\n        image_usage |= stencil_usage_info->stencilUsage;\n    }\n\n    // Check for layouts that mismatch image usages in the framebuffer\n    if (layout == VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL && !(image_usage & VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT)) {\n        vuid = use_rp2 ? \"VUID-vkCmdBeginRenderPass2-initialLayout-03094\" : \"VUID-vkCmdBeginRenderPass-initialLayout-00895\";\n        LogObjectList objlist(image);\n        objlist.add(renderpass);\n        objlist.add(framebuffer);\n        objlist.add(image_view);\n        skip |= LogError(objlist, vuid,\n                         \"%s: Layout/usage mismatch for attachment %u in %s\"\n                         \" - the %s is %s but the image attached to %s via %s\"\n                         \" was not created with VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT\",\n                         function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                         string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                         report_data->FormatHandle(image_view).c_str());\n    }\n\n    if (layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL &&\n        !(image_usage & (VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT | VK_IMAGE_USAGE_SAMPLED_BIT))) {\n        vuid = use_rp2 ? \"VUID-vkCmdBeginRenderPass2-initialLayout-03097\" : \"VUID-vkCmdBeginRenderPass-initialLayout-00897\";\n        LogObjectList objlist(image);\n        objlist.add(renderpass);\n        objlist.add(framebuffer);\n        objlist.add(image_view);\n        skip |= LogError(objlist, vuid,\n                         \"%s: Layout/usage mismatch for attachment %u in %s\"\n                         \" - the %s is %s but the image attached to %s via %s\"\n                         \" was not created with VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT or VK_IMAGE_USAGE_SAMPLED_BIT\",\n                         function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                         string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                         report_data->FormatHandle(image_view).c_str());\n    }\n\n    if (layout == VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL && !(image_usage & VK_IMAGE_USAGE_TRANSFER_SRC_BIT)) {\n        vuid = use_rp2 ? \"VUID-vkCmdBeginRenderPass2-initialLayout-03098\" : \"VUID-vkCmdBeginRenderPass-initialLayout-00898\";\n        LogObjectList objlist(image);\n        objlist.add(renderpass);\n        objlist.add(framebuffer);\n        objlist.add(image_view);\n        skip |= LogError(objlist, vuid,\n                         \"%s: Layout/usage mismatch for attachment %u in %s\"\n                         \" - the %s is %s but the image attached to %s via %s\"\n                         \" was not created with VK_IMAGE_USAGE_TRANSFER_SRC_BIT\",\n                         function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                         string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                         report_data->FormatHandle(image_view).c_str());\n    }\n\n    if (layout == VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL && !(image_usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT)) {\n        vuid = use_rp2 ? \"VUID-vkCmdBeginRenderPass2-initialLayout-03099\" : \"VUID-vkCmdBeginRenderPass-initialLayout-00899\";\n        LogObjectList objlist(image);\n        objlist.add(renderpass);\n        objlist.add(framebuffer);\n        objlist.add(image_view);\n        skip |= LogError(objlist, vuid,\n                         \"%s: Layout/usage mismatch for attachment %u in %s\"\n                         \" - the %s is %s but the image attached to %s via %s\"\n                         \" was not created with VK_IMAGE_USAGE_TRANSFER_DST_BIT\",\n                         function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                         string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                         report_data->FormatHandle(image_view).c_str());\n    }\n\n    if (device_extensions.vk_khr_maintenance2) {\n        if ((layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL ||\n             layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL ||\n             layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||\n             layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) &&\n            !(image_usage & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT)) {\n            vuid = use_rp2 ? \"VUID-vkCmdBeginRenderPass2-initialLayout-03096\" : \"VUID-vkCmdBeginRenderPass-initialLayout-01758\";\n            LogObjectList objlist(image);\n            objlist.add(renderpass);\n            objlist.add(framebuffer);\n            objlist.add(image_view);\n            skip |= LogError(objlist, vuid,\n                             \"%s: Layout/usage mismatch for attachment %u in %s\"\n                             \" - the %s is %s but the image attached to %s via %s\"\n                             \" was not created with VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT\",\n                             function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                             string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                             report_data->FormatHandle(image_view).c_str());\n        }\n    } else {\n        // The create render pass 2 extension requires maintenance 2 (the previous branch), so no vuid switch needed here.\n        if ((layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL ||\n             layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) &&\n            !(image_usage & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT)) {\n            LogObjectList objlist(image);\n            objlist.add(renderpass);\n            objlist.add(framebuffer);\n            objlist.add(image_view);\n            skip |= LogError(objlist, \"VUID-vkCmdBeginRenderPass-initialLayout-00896\",\n                             \"%s: Layout/usage mismatch for attachment %u in %s\"\n                             \" - the %s is %s but the image attached to %s via %s\"\n                             \" was not created with VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT\",\n                             function_name, attachment_index, report_data->FormatHandle(renderpass).c_str(), variable_name,\n                             string_VkImageLayout(layout), report_data->FormatHandle(framebuffer).c_str(),\n                             report_data->FormatHandle(image_view).c_str());\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::VerifyFramebufferAndRenderPassLayouts(RenderPassCreateVersion rp_version, const CMD_BUFFER_STATE *pCB,\n                                                       const VkRenderPassBeginInfo *pRenderPassBegin,\n                                                       const FRAMEBUFFER_STATE *framebuffer_state) const {\n    bool skip = false;\n    auto const pRenderPassInfo = GetRenderPassState(pRenderPassBegin->renderPass)->createInfo.ptr();\n    auto const &framebufferInfo = framebuffer_state->createInfo;\n    const VkImageView *attachments = framebufferInfo.pAttachments;\n\n    auto render_pass = GetRenderPassState(pRenderPassBegin->renderPass)->renderPass;\n    auto framebuffer = framebuffer_state->framebuffer;\n\n    if (pRenderPassInfo->attachmentCount != framebufferInfo.attachmentCount) {\n        skip |= LogError(pCB->commandBuffer, kVUID_Core_DrawState_InvalidRenderpass,\n                         \"You cannot start a render pass using a framebuffer with a different number of attachments.\");\n    }\n\n    const auto *attachmentInfo = lvl_find_in_chain<VkRenderPassAttachmentBeginInfoKHR>(pRenderPassBegin->pNext);\n    if (((framebufferInfo.flags & VK_FRAMEBUFFER_CREATE_IMAGELESS_BIT_KHR) != 0) && attachmentInfo != nullptr) {\n        attachments = attachmentInfo->pAttachments;\n    }\n\n    if (attachments != nullptr) {\n        const auto *const_pCB = static_cast<const CMD_BUFFER_STATE *>(pCB);\n        for (uint32_t i = 0; i < pRenderPassInfo->attachmentCount; ++i) {\n            auto image_view = attachments[i];\n            auto view_state = GetImageViewState(image_view);\n\n            if (!view_state) {\n                LogObjectList objlist(pRenderPassBegin->renderPass);\n                objlist.add(framebuffer_state->framebuffer);\n                objlist.add(image_view);\n                skip |= LogError(objlist, \"VUID-VkRenderPassBeginInfo-framebuffer-parameter\",\n                                 \"vkCmdBeginRenderPass(): %s pAttachments[%\" PRIu32 \"] = %s is not a valid VkImageView handle\",\n                                 report_data->FormatHandle(framebuffer_state->framebuffer).c_str(), i,\n                                 report_data->FormatHandle(image_view).c_str());\n                continue;\n            }\n\n            const VkImage image = view_state->create_info.image;\n            const IMAGE_STATE *image_state = GetImageState(image);\n\n            if (!image_state) {\n                LogObjectList objlist(pRenderPassBegin->renderPass);\n                objlist.add(framebuffer_state->framebuffer);\n                objlist.add(image_view);\n                objlist.add(image);\n                skip |= LogError(objlist, \"VUID-VkRenderPassBeginInfo-framebuffer-parameter\",\n                                 \"vkCmdBeginRenderPass(): %s pAttachments[%\" PRIu32 \"] =  %s references non-extant %s.\",\n                                 report_data->FormatHandle(framebuffer_state->framebuffer).c_str(), i,\n                                 report_data->FormatHandle(image_view).c_str(), report_data->FormatHandle(image).c_str());\n                continue;\n            }\n            auto attachment_initial_layout = pRenderPassInfo->pAttachments[i].initialLayout;\n            auto final_layout = pRenderPassInfo->pAttachments[i].finalLayout;\n\n            // Default to expecting stencil in the same layout.\n            auto attachment_stencil_initial_layout = attachment_initial_layout;\n\n            // If a separate layout is specified, look for that.\n            const auto *attachment_description_stencil_layout =\n                lvl_find_in_chain<VkAttachmentDescriptionStencilLayoutKHR>(pRenderPassInfo->pAttachments[i].pNext);\n            if (attachment_description_stencil_layout) {\n                attachment_stencil_initial_layout = attachment_description_stencil_layout->stencilInitialLayout;\n            }\n\n            // Cast pCB to const because we don't want to create entries that don't exist here (in case the key changes to something\n            // in common with the non-const version.)\n            const ImageSubresourceLayoutMap *subresource_map =\n                (attachment_initial_layout != VK_IMAGE_LAYOUT_UNDEFINED) ? GetImageSubresourceLayoutMap(const_pCB, image) : nullptr;\n\n            if (subresource_map) {  // If no layout information for image yet, will be checked at QueueSubmit time\n                LayoutUseCheckAndMessage layout_check(subresource_map);\n                bool subres_skip = false;\n                auto pos = subresource_map->Find(view_state->normalized_subresource_range);\n                for (; pos != subresource_map->End() && !subres_skip; ++pos) {\n                    const VkImageSubresource &subres = pos->subresource;\n\n                    // Allow for differing depth and stencil layouts\n                    VkImageLayout check_layout = attachment_initial_layout;\n                    if (subres.aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) check_layout = attachment_stencil_initial_layout;\n\n                    if (!layout_check.Check(subres, check_layout, pos->current_layout, pos->initial_layout)) {\n                        subres_skip |= LogError(\n                            device, kVUID_Core_DrawState_InvalidRenderpass,\n                            \"You cannot start a render pass using attachment %u where the render pass initial layout is %s \"\n                            \"and the %s layout of the attachment is %s. The layouts must match, or the render \"\n                            \"pass initial layout for the attachment must be VK_IMAGE_LAYOUT_UNDEFINED\",\n                            i, string_VkImageLayout(check_layout), layout_check.message, string_VkImageLayout(layout_check.layout));\n                    }\n                }\n\n                skip |= subres_skip;\n            }\n\n            ValidateRenderPassLayoutAgainstFramebufferImageUsage(rp_version, attachment_initial_layout, image, image_view,\n                                                                 framebuffer, render_pass, i, \"initial layout\");\n\n            ValidateRenderPassLayoutAgainstFramebufferImageUsage(rp_version, final_layout, image, image_view, framebuffer,\n                                                                 render_pass, i, \"final layout\");\n        }\n\n        for (uint32_t j = 0; j < pRenderPassInfo->subpassCount; ++j) {\n            auto &subpass = pRenderPassInfo->pSubpasses[j];\n            for (uint32_t k = 0; k < pRenderPassInfo->pSubpasses[j].inputAttachmentCount; ++k) {\n                auto &attachment_ref = subpass.pInputAttachments[k];\n                if (attachment_ref.attachment != VK_ATTACHMENT_UNUSED) {\n                    auto image_view = attachments[attachment_ref.attachment];\n                    auto view_state = GetImageViewState(image_view);\n\n                    if (view_state) {\n                        auto image = view_state->create_info.image;\n                        ValidateRenderPassLayoutAgainstFramebufferImageUsage(rp_version, attachment_ref.layout, image, image_view,\n                                                                             framebuffer, render_pass, attachment_ref.attachment,\n                                                                             \"input attachment layout\");\n                    }\n                }\n            }\n\n            for (uint32_t k = 0; k < pRenderPassInfo->pSubpasses[j].colorAttachmentCount; ++k) {\n                auto &attachment_ref = subpass.pColorAttachments[k];\n                if (attachment_ref.attachment != VK_ATTACHMENT_UNUSED) {\n                    auto image_view = attachments[attachment_ref.attachment];\n                    auto view_state = GetImageViewState(image_view);\n\n                    if (view_state) {\n                        auto image = view_state->create_info.image;\n                        ValidateRenderPassLayoutAgainstFramebufferImageUsage(rp_version, attachment_ref.layout, image, image_view,\n                                                                             framebuffer, render_pass, attachment_ref.attachment,\n                                                                             \"color attachment layout\");\n                        if (subpass.pResolveAttachments) {\n                            ValidateRenderPassLayoutAgainstFramebufferImageUsage(\n                                rp_version, attachment_ref.layout, image, image_view, framebuffer, render_pass,\n                                attachment_ref.attachment, \"resolve attachment layout\");\n                        }\n                    }\n                }\n            }\n\n            if (pRenderPassInfo->pSubpasses[j].pDepthStencilAttachment) {\n                auto &attachment_ref = *subpass.pDepthStencilAttachment;\n                if (attachment_ref.attachment != VK_ATTACHMENT_UNUSED) {\n                    auto image_view = attachments[attachment_ref.attachment];\n                    auto view_state = GetImageViewState(image_view);\n\n                    if (view_state) {\n                        auto image = view_state->create_info.image;\n                        ValidateRenderPassLayoutAgainstFramebufferImageUsage(rp_version, attachment_ref.layout, image, image_view,\n                                                                             framebuffer, render_pass, attachment_ref.attachment,\n                                                                             \"input attachment layout\");\n                    }\n                }\n            }\n        }\n    }\n    return skip;\n}\n\nvoid CoreChecks::TransitionAttachmentRefLayout(CMD_BUFFER_STATE *pCB, FRAMEBUFFER_STATE *pFramebuffer,\n                                               const safe_VkAttachmentReference2 &ref) {\n    if (ref.attachment != VK_ATTACHMENT_UNUSED) {\n        IMAGE_VIEW_STATE *image_view = nullptr;\n        if (pFramebuffer->createInfo.flags & VK_FRAMEBUFFER_CREATE_IMAGELESS_BIT_KHR) {\n            const auto attachment_info =\n                lvl_find_in_chain<VkRenderPassAttachmentBeginInfoKHR>(pCB->activeRenderPassBeginInfo.pNext);\n            if (attachment_info) image_view = GetImageViewState(attachment_info->pAttachments[ref.attachment]);\n        } else {\n            image_view = GetAttachmentImageViewState(pCB, pFramebuffer, ref.attachment);\n        }\n        if (image_view) {\n            VkImageLayout stencil_layout = kInvalidLayout;\n            const auto *attachment_reference_stencil_layout = lvl_find_in_chain<VkAttachmentReferenceStencilLayoutKHR>(ref.pNext);\n            if (attachment_reference_stencil_layout) {\n                stencil_layout = attachment_reference_stencil_layout->stencilLayout;\n            }\n\n            SetImageViewLayout(pCB, *image_view, ref.layout, stencil_layout);\n        }\n    }\n}\n\nvoid CoreChecks::TransitionSubpassLayouts(CMD_BUFFER_STATE *pCB, const RENDER_PASS_STATE *render_pass_state,\n                                          const int subpass_index, FRAMEBUFFER_STATE *framebuffer_state) {\n    assert(render_pass_state);\n\n    if (framebuffer_state) {\n        auto const &subpass = render_pass_state->createInfo.pSubpasses[subpass_index];\n        for (uint32_t j = 0; j < subpass.inputAttachmentCount; ++j) {\n            TransitionAttachmentRefLayout(pCB, framebuffer_state, subpass.pInputAttachments[j]);\n        }\n        for (uint32_t j = 0; j < subpass.colorAttachmentCount; ++j) {\n            TransitionAttachmentRefLayout(pCB, framebuffer_state, subpass.pColorAttachments[j]);\n        }\n        if (subpass.pDepthStencilAttachment) {\n            TransitionAttachmentRefLayout(pCB, framebuffer_state, *subpass.pDepthStencilAttachment);\n        }\n    }\n}\n\n// Transition the layout state for renderpass attachments based on the BeginRenderPass() call. This includes:\n// 1. Transition into initialLayout state\n// 2. Transition from initialLayout to layout used in subpass 0\nvoid CoreChecks::TransitionBeginRenderPassLayouts(CMD_BUFFER_STATE *cb_state, const RENDER_PASS_STATE *render_pass_state,\n                                                  FRAMEBUFFER_STATE *framebuffer_state) {\n    // First transition into initialLayout\n    auto const rpci = render_pass_state->createInfo.ptr();\n    for (uint32_t i = 0; i < rpci->attachmentCount; ++i) {\n        IMAGE_VIEW_STATE *view_state = nullptr;\n        if (framebuffer_state->createInfo.flags & VK_FRAMEBUFFER_CREATE_IMAGELESS_BIT_KHR) {\n            const auto attachment_info =\n                lvl_find_in_chain<VkRenderPassAttachmentBeginInfoKHR>(cb_state->activeRenderPassBeginInfo.pNext);\n            if (attachment_info) view_state = GetImageViewState(attachment_info->pAttachments[i]);\n        } else {\n            view_state = GetAttachmentImageViewState(cb_state, framebuffer_state, i);\n        }\n        if (view_state) {\n            VkImageLayout stencil_layout = kInvalidLayout;\n            const auto *attachment_description_stencil_layout =\n                lvl_find_in_chain<VkAttachmentDescriptionStencilLayoutKHR>(rpci->pAttachments[i].pNext);\n            if (attachment_description_stencil_layout) {\n                stencil_layout = attachment_description_stencil_layout->stencilInitialLayout;\n            }\n\n            SetImageViewLayout(cb_state, *view_state, rpci->pAttachments[i].initialLayout, stencil_layout);\n        }\n    }\n    // Now transition for first subpass (index 0)\n    TransitionSubpassLayouts(cb_state, render_pass_state, 0, framebuffer_state);\n}\n\nbool VerifyAspectsPresent(VkImageAspectFlags aspect_mask, VkFormat format) {\n    if ((aspect_mask & VK_IMAGE_ASPECT_COLOR_BIT) != 0) {\n        if (!(FormatIsColor(format) || FormatIsMultiplane(format))) return false;\n    }\n    if ((aspect_mask & VK_IMAGE_ASPECT_DEPTH_BIT) != 0) {\n        if (!FormatHasDepth(format)) return false;\n    }\n    if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) != 0) {\n        if (!FormatHasStencil(format)) return false;\n    }\n    if (0 !=\n        (aspect_mask & (VK_IMAGE_ASPECT_PLANE_0_BIT_KHR | VK_IMAGE_ASPECT_PLANE_1_BIT_KHR | VK_IMAGE_ASPECT_PLANE_2_BIT_KHR))) {\n        if (FormatPlaneCount(format) == 1) return false;\n    }\n    return true;\n}\n\n// Verify an ImageMemoryBarrier's old/new ImageLayouts are compatible with the Image's ImageUsageFlags.\nbool CoreChecks::ValidateBarrierLayoutToImageUsage(const VkImageMemoryBarrier &img_barrier, bool new_not_old,\n                                                   VkImageUsageFlags usage_flags, const char *func_name,\n                                                   const char *barrier_pname) const {\n    bool skip = false;\n    const VkImageLayout layout = (new_not_old) ? img_barrier.newLayout : img_barrier.oldLayout;\n    const char *msg_code = kVUIDUndefined;  // sentinel value meaning \"no error\"\n\n    switch (layout) {\n        case VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01208\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_DEPTH_STENCIL_ATTACHMENT_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01209\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01210\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL:\n            if ((usage_flags & (VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT)) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01211\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_TRANSFER_SRC_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01212\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_TRANSFER_DST_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01213\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_SHADING_RATE_OPTIMAL_NV:\n            if ((usage_flags & VK_IMAGE_USAGE_SHADING_RATE_IMAGE_BIT_NV) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-02088\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01658\";\n            }\n            break;\n        case VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL:\n            if ((usage_flags & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT) == 0) {\n                msg_code = \"VUID-VkImageMemoryBarrier-oldLayout-01659\";\n            }\n            break;\n        default:\n            // Other VkImageLayout values do not have VUs defined in this context.\n            break;\n    }\n\n    if (msg_code != kVUIDUndefined) {\n        skip |= LogError(img_barrier.image, msg_code,\n                         \"%s: Image barrier %s %s Layout=%s is not compatible with %s usage flags 0x%\" PRIx32 \".\", func_name,\n                         barrier_pname, ((new_not_old) ? \"new\" : \"old\"), string_VkImageLayout(layout),\n                         report_data->FormatHandle(img_barrier.image).c_str(), usage_flags);\n    }\n    return skip;\n}\n\n// Verify image barriers are compatible with the images they reference.\nbool CoreChecks::ValidateBarriersToImages(const CMD_BUFFER_STATE *cb_state, uint32_t imageMemoryBarrierCount,\n                                          const VkImageMemoryBarrier *pImageMemoryBarriers, const char *func_name) const {\n    bool skip = false;\n\n    // Scoreboard for checking for duplicate and inconsistent barriers to images\n    struct ImageBarrierScoreboardEntry {\n        uint32_t index;\n        // This is designed for temporary storage within the scope of the API call.  If retained storage of the barriers is\n        // required, copies should be made and smart or unique pointers used in some other stucture (or this one refactored)\n        const VkImageMemoryBarrier *barrier;\n    };\n    using ImageBarrierScoreboardSubresMap = std::unordered_map<VkImageSubresourceRange, ImageBarrierScoreboardEntry>;\n    using ImageBarrierScoreboardImageMap = std::unordered_map<VkImage, ImageBarrierScoreboardSubresMap>;\n\n    // Scoreboard for duplicate layout transition barriers within the list\n    // Pointers retained in the scoreboard only have the lifetime of *this* call (i.e. within the scope of the API call)\n    ImageBarrierScoreboardImageMap layout_transitions;\n\n    for (uint32_t i = 0; i < imageMemoryBarrierCount; ++i) {\n        const auto &img_barrier = pImageMemoryBarriers[i];\n        const std::string barrier_pname = \"pImageMemoryBarrier[\" + std::to_string(i) + \"]\";\n\n        // Update the scoreboard of layout transitions and check for barriers affecting the same image and subresource\n        // TODO: a higher precision could be gained by adapting the command_buffer image_layout_map logic looking for conflicts\n        // at a per sub-resource level\n        if (img_barrier.oldLayout != img_barrier.newLayout) {\n            const ImageBarrierScoreboardEntry new_entry{i, &img_barrier};\n            const auto image_it = layout_transitions.find(img_barrier.image);\n            if (image_it != layout_transitions.end()) {\n                auto &subres_map = image_it->second;\n                auto subres_it = subres_map.find(img_barrier.subresourceRange);\n                if (subres_it != subres_map.end()) {\n                    auto &entry = subres_it->second;\n                    if ((entry.barrier->newLayout != img_barrier.oldLayout) &&\n                        (img_barrier.oldLayout != VK_IMAGE_LAYOUT_UNDEFINED)) {\n                        const VkImageSubresourceRange &range = img_barrier.subresourceRange;\n                        skip = LogError(\n                            cb_state->commandBuffer, \"VUID-VkImageMemoryBarrier-oldLayout-01197\",\n                            \"%s: %s conflicts with earlier entry pImageMemoryBarrier[%u]. %s\"\n                            \" subresourceRange: aspectMask=%u baseMipLevel=%u levelCount=%u, baseArrayLayer=%u, layerCount=%u; \"\n                            \"conflicting barrier transitions image layout from %s when earlier barrier transitioned to layout %s.\",\n                            func_name, barrier_pname.c_str(), entry.index, report_data->FormatHandle(img_barrier.image).c_str(),\n                            range.aspectMask, range.baseMipLevel, range.levelCount, range.baseArrayLayer, range.layerCount,\n                            string_VkImageLayout(img_barrier.oldLayout), string_VkImageLayout(entry.barrier->newLayout));\n                    }\n                    entry = new_entry;\n                } else {\n                    subres_map[img_barrier.subresourceRange] = new_entry;\n                }\n            } else {\n                layout_transitions[img_barrier.image][img_barrier.subresourceRange] = new_entry;\n            }\n        }\n\n        auto image_state = GetImageState(img_barrier.image);\n        if (image_state) {\n            VkImageUsageFlags usage_flags = image_state->createInfo.usage;\n            skip |= ValidateBarrierLayoutToImageUsage(img_barrier, false, usage_flags, func_name, barrier_pname.c_str());\n            skip |= ValidateBarrierLayoutToImageUsage(img_barrier, true, usage_flags, func_name, barrier_pname.c_str());\n\n            // Make sure layout is able to be transitioned, currently only presented shared presentable images are locked\n            if (image_state->layout_locked) {\n                // TODO: Add unique id for error when available\n                skip |= LogError(\n                    img_barrier.image, 0,\n                    \"%s: Attempting to transition shared presentable %s\"\n                    \" from layout %s to layout %s, but image has already been presented and cannot have its layout transitioned.\",\n                    func_name, report_data->FormatHandle(img_barrier.image).c_str(), string_VkImageLayout(img_barrier.oldLayout),\n                    string_VkImageLayout(img_barrier.newLayout));\n            }\n\n            const VkImageCreateInfo &image_create_info = image_state->createInfo;\n            const VkFormat image_format = image_create_info.format;\n            const VkImageAspectFlags aspect_mask = img_barrier.subresourceRange.aspectMask;\n            // For a Depth/Stencil image both aspects MUST be set\n            if (FormatIsDepthAndStencil(image_format)) {\n                if (enabled_features.core12.separateDepthStencilLayouts) {\n                    if (!(aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT))) {\n                        skip |=\n                            LogError(img_barrier.image, \"VUID-VkImageMemoryBarrier-image-03319\",\n                                     \"%s: Image barrier %s references %s of format %s that must have either the depth or stencil \"\n                                     \"aspects set, but its aspectMask is 0x%\" PRIx32 \".\",\n                                     func_name, barrier_pname.c_str(), report_data->FormatHandle(img_barrier.image).c_str(),\n                                     string_VkFormat(image_format), aspect_mask);\n                    }\n                } else {\n                    auto const ds_mask = VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT;\n                    if ((aspect_mask & ds_mask) != (ds_mask)) {\n                        const char *vuid = device_extensions.vk_khr_separate_depth_stencil_layouts\n                                               ? \"VUID-VkImageMemoryBarrier-image-03320\"\n                                               : \"VUID-VkImageMemoryBarrier-image-01207\";\n                        skip |= LogError(img_barrier.image, vuid,\n                                         \"%s: Image barrier %s references %s of format %s that must have the depth and stencil \"\n                                         \"aspects set, but its aspectMask is 0x%\" PRIx32 \".\",\n                                         func_name, barrier_pname.c_str(), report_data->FormatHandle(img_barrier.image).c_str(),\n                                         string_VkFormat(image_format), aspect_mask);\n                    }\n                }\n            }\n\n            const auto *subresource_map = GetImageSubresourceLayoutMap(cb_state, img_barrier.image);\n            if (img_barrier.oldLayout == VK_IMAGE_LAYOUT_UNDEFINED) {\n                // TODO: Set memory invalid which is in mem_tracker currently\n                // Not sure if this needs to be in the ForRange traversal, pulling it out as it is currently invariant with\n                // subresource.\n            } else if (subresource_map && !QueueFamilyIsExternal(img_barrier.srcQueueFamilyIndex)) {\n                bool subres_skip = false;\n                LayoutUseCheckAndMessage layout_check(subresource_map);\n                VkImageSubresourceRange normalized_isr = NormalizeSubresourceRange(*image_state, img_barrier.subresourceRange);\n                for (auto pos = subresource_map->Find(normalized_isr); (pos != subresource_map->End()) && !subres_skip; ++pos) {\n                    const auto &value = *pos;\n                    if (!layout_check.Check(value.subresource, img_barrier.oldLayout, value.current_layout, value.initial_layout)) {\n                        subres_skip = LogError(\n                            cb_state->commandBuffer, \"VUID-VkImageMemoryBarrier-oldLayout-01197\",\n                            \"%s: For %s you cannot transition the layout of aspect=%d level=%d layer=%d from %s when the \"\n                            \"%s layout is %s.\",\n                            func_name, report_data->FormatHandle(img_barrier.image).c_str(), value.subresource.aspectMask,\n                            value.subresource.mipLevel, value.subresource.arrayLayer, string_VkImageLayout(img_barrier.oldLayout),\n                            layout_check.message, string_VkImageLayout(layout_check.layout));\n                    }\n                }\n                skip |= subres_skip;\n            }\n\n            // checks color format and (single-plane or non-disjoint)\n            // if ycbcr extension is not supported then single-plane and non-disjoint are always both true\n            if ((FormatIsColor(image_format) == true) &&\n                ((FormatIsMultiplane(image_format) == false) || (image_state->disjoint == false))) {\n                if (aspect_mask != VK_IMAGE_ASPECT_COLOR_BIT) {\n                    const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                           ? \"VUID-VkImageMemoryBarrier-image-01671\"\n                                           : \"VUID-VkImageMemoryBarrier-image-02902\";\n                    skip |= LogError(img_barrier.image, vuid,\n                                     \"%s: Image barrier %s references %s of format %s that must be only VK_IMAGE_ASPECT_COLOR_BIT, \"\n                                     \"but its aspectMask is 0x%\" PRIx32 \".\",\n                                     func_name, barrier_pname.c_str(), report_data->FormatHandle(img_barrier.image).c_str(),\n                                     string_VkFormat(image_format), aspect_mask);\n                }\n            }\n\n            VkImageAspectFlags valid_disjoint_mask =\n                VK_IMAGE_ASPECT_PLANE_0_BIT | VK_IMAGE_ASPECT_PLANE_1_BIT | VK_IMAGE_ASPECT_PLANE_2_BIT | VK_IMAGE_ASPECT_COLOR_BIT;\n            if ((FormatIsMultiplane(image_format) == true) && (image_state->disjoint == true) &&\n                ((aspect_mask & valid_disjoint_mask) == 0)) {\n                skip |= LogError(img_barrier.image, \"VUID-VkImageMemoryBarrier-image-01672\",\n                                 \"%s: Image barrier %s references %s of format %s has aspectMask (0x%\" PRIx32\n                                 \") but needs to include either an VK_IMAGE_ASPECT_PLANE_*_BIT or VK_IMAGE_ASPECT_COLOR_BIT.\",\n                                 func_name, barrier_pname.c_str(), report_data->FormatHandle(img_barrier.image).c_str(),\n                                 string_VkFormat(image_format), aspect_mask);\n            }\n\n            if ((FormatPlaneCount(image_format) == 2) && ((aspect_mask & VK_IMAGE_ASPECT_PLANE_2_BIT) != 0)) {\n                skip |= LogError(img_barrier.image, \"VUID-VkImageMemoryBarrier-image-01673\",\n                                 \"%s: Image barrier %s references %s of format %s has only two planes but included \"\n                                 \"VK_IMAGE_ASPECT_PLANE_2_BIT in its aspectMask (0x%\" PRIx32 \").\",\n                                 func_name, barrier_pname.c_str(), report_data->FormatHandle(img_barrier.image).c_str(),\n                                 string_VkFormat(image_format), aspect_mask);\n            }\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::IsReleaseOp(CMD_BUFFER_STATE *cb_state, const VkImageMemoryBarrier &barrier) const {\n    if (!IsTransferOp(&barrier)) return false;\n\n    auto pool = cb_state->command_pool.get();\n    return pool && TempIsReleaseOp<VkImageMemoryBarrier, true>(pool, &barrier);\n}\n\ntemplate <typename Barrier>\nbool CoreChecks::ValidateQFOTransferBarrierUniqueness(const char *func_name, const CMD_BUFFER_STATE *cb_state,\n                                                      uint32_t barrier_count, const Barrier *barriers) const {\n    using BarrierRecord = QFOTransferBarrier<Barrier>;\n    bool skip = false;\n    auto pool = cb_state->command_pool.get();\n    auto &barrier_sets = GetQFOBarrierSets(cb_state, typename BarrierRecord::Tag());\n    const char *barrier_name = BarrierRecord::BarrierName();\n    const char *handle_name = BarrierRecord::HandleName();\n    const char *transfer_type = nullptr;\n    for (uint32_t b = 0; b < barrier_count; b++) {\n        if (!IsTransferOp(&barriers[b])) continue;\n        const BarrierRecord *barrier_record = nullptr;\n        if (TempIsReleaseOp<Barrier, true /* Assume IsTransfer */>(pool, &barriers[b]) &&\n            !QueueFamilyIsExternal(barriers[b].dstQueueFamilyIndex)) {\n            const auto found = barrier_sets.release.find(barriers[b]);\n            if (found != barrier_sets.release.cend()) {\n                barrier_record = &(*found);\n                transfer_type = \"releasing\";\n            }\n        } else if (IsAcquireOp<Barrier, true /*Assume IsTransfer */>(pool, &barriers[b]) &&\n                   !QueueFamilyIsExternal(barriers[b].srcQueueFamilyIndex)) {\n            const auto found = barrier_sets.acquire.find(barriers[b]);\n            if (found != barrier_sets.acquire.cend()) {\n                barrier_record = &(*found);\n                transfer_type = \"acquiring\";\n            }\n        }\n        if (barrier_record != nullptr) {\n            skip |= LogWarning(cb_state->commandBuffer, BarrierRecord::ErrMsgDuplicateQFOInCB(),\n                               \"%s: %s at index %\" PRIu32 \" %s queue ownership of %s (%s), from srcQueueFamilyIndex %\" PRIu32\n                               \" to dstQueueFamilyIndex %\" PRIu32 \" duplicates existing barrier recorded in this command buffer.\",\n                               func_name, barrier_name, b, transfer_type, handle_name,\n                               report_data->FormatHandle(barrier_record->handle).c_str(), barrier_record->srcQueueFamilyIndex,\n                               barrier_record->dstQueueFamilyIndex);\n        }\n    }\n    return skip;\n}\n\nVulkanTypedHandle BarrierTypedHandle(const VkImageMemoryBarrier &barrier) {\n    return VulkanTypedHandle(barrier.image, kVulkanObjectTypeImage);\n}\n\nconst IMAGE_STATE *BarrierHandleState(const ValidationStateTracker &device_state, const VkImageMemoryBarrier &barrier) {\n    return device_state.GetImageState(barrier.image);\n}\n\nVulkanTypedHandle BarrierTypedHandle(const VkBufferMemoryBarrier &barrier) {\n    return VulkanTypedHandle(barrier.buffer, kVulkanObjectTypeBuffer);\n}\n\nconst BUFFER_STATE *BarrierHandleState(const ValidationStateTracker &device_state, const VkBufferMemoryBarrier &barrier) {\n    return device_state.GetBufferState(barrier.buffer);\n}\n\nVkBuffer BarrierHandle(const VkBufferMemoryBarrier &barrier) { return barrier.buffer; }\n\ntemplate <typename Barrier>\nvoid CoreChecks::RecordBarrierArrayValidationInfo(const char *func_name, CMD_BUFFER_STATE *cb_state, uint32_t barrier_count,\n                                                  const Barrier *barriers) {\n    auto pool = cb_state->command_pool.get();\n    auto &barrier_sets = GetQFOBarrierSets(cb_state, typename QFOTransferBarrier<Barrier>::Tag());\n    for (uint32_t b = 0; b < barrier_count; b++) {\n        auto &barrier = barriers[b];\n        if (IsTransferOp(&barrier)) {\n            if (TempIsReleaseOp<Barrier, true /* Assume IsTransfer*/>(pool, &barrier) &&\n                !QueueFamilyIsExternal(barrier.dstQueueFamilyIndex)) {\n                barrier_sets.release.emplace(barrier);\n            } else if (IsAcquireOp<Barrier, true /*Assume IsTransfer */>(pool, &barrier) &&\n                       !QueueFamilyIsExternal(barrier.srcQueueFamilyIndex)) {\n                barrier_sets.acquire.emplace(barrier);\n            }\n        }\n\n        const uint32_t src_queue_family = barrier.srcQueueFamilyIndex;\n        const uint32_t dst_queue_family = barrier.dstQueueFamilyIndex;\n        if (!QueueFamilyIsIgnored(src_queue_family) && !QueueFamilyIsIgnored(dst_queue_family)) {\n            // Only enqueue submit time check if it is needed. If more submit time checks are added, change the criteria\n            // TODO create a better named list, or rename the submit time lists to something that matches the broader usage...\n            auto handle_state = BarrierHandleState(*this, barrier);\n            bool mode_concurrent = handle_state ? handle_state->createInfo.sharingMode == VK_SHARING_MODE_CONCURRENT : false;\n            if (!mode_concurrent) {\n                const auto typed_handle = BarrierTypedHandle(barrier);\n                cb_state->queue_submit_functions.emplace_back(\n                    [func_name, cb_state, typed_handle, src_queue_family, dst_queue_family](\n                        const ValidationStateTracker *device_data, const QUEUE_STATE *queue_state) {\n                        return ValidateConcurrentBarrierAtSubmit(device_data, queue_state, func_name, cb_state, typed_handle,\n                                                                 src_queue_family, dst_queue_family);\n                    });\n            }\n        }\n    }\n}\n\nbool CoreChecks::ValidateBarriersQFOTransferUniqueness(const char *func_name, const CMD_BUFFER_STATE *cb_state,\n                                                       uint32_t bufferBarrierCount, const VkBufferMemoryBarrier *pBufferMemBarriers,\n                                                       uint32_t imageMemBarrierCount,\n                                                       const VkImageMemoryBarrier *pImageMemBarriers) const {\n    bool skip = false;\n    skip |= ValidateQFOTransferBarrierUniqueness(func_name, cb_state, bufferBarrierCount, pBufferMemBarriers);\n    skip |= ValidateQFOTransferBarrierUniqueness(func_name, cb_state, imageMemBarrierCount, pImageMemBarriers);\n    return skip;\n}\n\nvoid CoreChecks::RecordBarrierValidationInfo(const char *func_name, CMD_BUFFER_STATE *cb_state, uint32_t bufferBarrierCount,\n                                             const VkBufferMemoryBarrier *pBufferMemBarriers, uint32_t imageMemBarrierCount,\n                                             const VkImageMemoryBarrier *pImageMemBarriers) {\n    RecordBarrierArrayValidationInfo(func_name, cb_state, bufferBarrierCount, pBufferMemBarriers);\n    RecordBarrierArrayValidationInfo(func_name, cb_state, imageMemBarrierCount, pImageMemBarriers);\n}\n\ntemplate <typename BarrierRecord, typename Scoreboard>\nbool CoreChecks::ValidateAndUpdateQFOScoreboard(const debug_report_data *report_data, const CMD_BUFFER_STATE *cb_state,\n                                                const char *operation, const BarrierRecord &barrier, Scoreboard *scoreboard) const {\n    // Record to the scoreboard or report that we have a duplication\n    bool skip = false;\n    auto inserted = scoreboard->insert(std::make_pair(barrier, cb_state));\n    if (!inserted.second && inserted.first->second != cb_state) {\n        // This is a duplication (but don't report duplicates from the same CB, as we do that at record time\n        LogObjectList objlist(cb_state->commandBuffer);\n        objlist.add(barrier.handle);\n        objlist.add(inserted.first->second->commandBuffer);\n        skip = LogWarning(objlist, BarrierRecord::ErrMsgDuplicateQFOInSubmit(),\n                          \"%s: %s %s queue ownership of %s (%s), from srcQueueFamilyIndex %\" PRIu32\n                          \" to dstQueueFamilyIndex %\" PRIu32 \" duplicates existing barrier submitted in this batch from %s.\",\n                          \"vkQueueSubmit()\", BarrierRecord::BarrierName(), operation, BarrierRecord::HandleName(),\n                          report_data->FormatHandle(barrier.handle).c_str(), barrier.srcQueueFamilyIndex,\n                          barrier.dstQueueFamilyIndex, report_data->FormatHandle(inserted.first->second->commandBuffer).c_str());\n    }\n    return skip;\n}\n\ntemplate <typename Barrier>\nbool CoreChecks::ValidateQueuedQFOTransferBarriers(const CMD_BUFFER_STATE *cb_state,\n                                                   QFOTransferCBScoreboards<Barrier> *scoreboards) const {\n    using BarrierRecord = QFOTransferBarrier<Barrier>;\n    using TypeTag = typename BarrierRecord::Tag;\n    bool skip = false;\n    const auto &cb_barriers = GetQFOBarrierSets(cb_state, TypeTag());\n    const GlobalQFOTransferBarrierMap<Barrier> &global_release_barriers = GetGlobalQFOReleaseBarrierMap(TypeTag());\n    const char *barrier_name = BarrierRecord::BarrierName();\n    const char *handle_name = BarrierRecord::HandleName();\n    // No release should have an extant duplicate (WARNING)\n    for (const auto &release : cb_barriers.release) {\n        // Check the global pending release barriers\n        const auto set_it = global_release_barriers.find(release.handle);\n        if (set_it != global_release_barriers.cend()) {\n            const QFOTransferBarrierSet<Barrier> &set_for_handle = set_it->second;\n            const auto found = set_for_handle.find(release);\n            if (found != set_for_handle.cend()) {\n                skip |= LogWarning(cb_state->commandBuffer, BarrierRecord::ErrMsgDuplicateQFOSubmitted(),\n                                   \"%s: %s releasing queue ownership of %s (%s), from srcQueueFamilyIndex %\" PRIu32\n                                   \" to dstQueueFamilyIndex %\" PRIu32\n                                   \" duplicates existing barrier queued for execution, without intervening acquire operation.\",\n                                   \"vkQueueSubmit()\", barrier_name, handle_name, report_data->FormatHandle(found->handle).c_str(),\n                                   found->srcQueueFamilyIndex, found->dstQueueFamilyIndex);\n            }\n        }\n        skip |= ValidateAndUpdateQFOScoreboard(report_data, cb_state, \"releasing\", release, &scoreboards->release);\n    }\n    // Each acquire must have a matching release (ERROR)\n    for (const auto &acquire : cb_barriers.acquire) {\n        const auto set_it = global_release_barriers.find(acquire.handle);\n        bool matching_release_found = false;\n        if (set_it != global_release_barriers.cend()) {\n            const QFOTransferBarrierSet<Barrier> &set_for_handle = set_it->second;\n            matching_release_found = set_for_handle.find(acquire) != set_for_handle.cend();\n        }\n        if (!matching_release_found) {\n            skip |= LogError(cb_state->commandBuffer, BarrierRecord::ErrMsgMissingQFOReleaseInSubmit(),\n                             \"%s: in submitted command buffer %s acquiring ownership of %s (%s), from srcQueueFamilyIndex %\" PRIu32\n                             \" to dstQueueFamilyIndex %\" PRIu32 \" has no matching release barrier queued for execution.\",\n                             \"vkQueueSubmit()\", barrier_name, handle_name, report_data->FormatHandle(acquire.handle).c_str(),\n                             acquire.srcQueueFamilyIndex, acquire.dstQueueFamilyIndex);\n        }\n        skip |= ValidateAndUpdateQFOScoreboard(report_data, cb_state, \"acquiring\", acquire, &scoreboards->acquire);\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateQueuedQFOTransfers(const CMD_BUFFER_STATE *cb_state,\n                                            QFOTransferCBScoreboards<VkImageMemoryBarrier> *qfo_image_scoreboards,\n                                            QFOTransferCBScoreboards<VkBufferMemoryBarrier> *qfo_buffer_scoreboards) const {\n    bool skip = false;\n    skip |= ValidateQueuedQFOTransferBarriers<VkImageMemoryBarrier>(cb_state, qfo_image_scoreboards);\n    skip |= ValidateQueuedQFOTransferBarriers<VkBufferMemoryBarrier>(cb_state, qfo_buffer_scoreboards);\n    return skip;\n}\n\ntemplate <typename Barrier>\nvoid CoreChecks::RecordQueuedQFOTransferBarriers(CMD_BUFFER_STATE *cb_state) {\n    using BarrierRecord = QFOTransferBarrier<Barrier>;\n    using TypeTag = typename BarrierRecord::Tag;\n    const auto &cb_barriers = GetQFOBarrierSets(cb_state, TypeTag());\n    GlobalQFOTransferBarrierMap<Barrier> &global_release_barriers = GetGlobalQFOReleaseBarrierMap(TypeTag());\n\n    // Add release barriers from this submit to the global map\n    for (const auto &release : cb_barriers.release) {\n        // the global barrier list is mapped by resource handle to allow cleanup on resource destruction\n        // NOTE: We're using [] because creation of a Set is a needed side effect for new handles\n        global_release_barriers[release.handle].insert(release);\n    }\n\n    // Erase acquired barriers from this submit from the global map -- essentially marking releases as consumed\n    for (const auto &acquire : cb_barriers.acquire) {\n        // NOTE: We're not using [] because we don't want to create entries for missing releases\n        auto set_it = global_release_barriers.find(acquire.handle);\n        if (set_it != global_release_barriers.end()) {\n            QFOTransferBarrierSet<Barrier> &set_for_handle = set_it->second;\n            set_for_handle.erase(acquire);\n            if (set_for_handle.size() == 0) {  // Clean up empty sets\n                global_release_barriers.erase(set_it);\n            }\n        }\n    }\n}\n\nvoid CoreChecks::RecordQueuedQFOTransfers(CMD_BUFFER_STATE *cb_state) {\n    RecordQueuedQFOTransferBarriers<VkImageMemoryBarrier>(cb_state);\n    RecordQueuedQFOTransferBarriers<VkBufferMemoryBarrier>(cb_state);\n}\n\n// Avoid making the template globally visible by exporting the one instance of it we need.\nvoid CoreChecks::EraseQFOImageRelaseBarriers(const VkImage &image) { EraseQFOReleaseBarriers<VkImageMemoryBarrier>(image); }\n\nvoid CoreChecks::TransitionImageLayouts(CMD_BUFFER_STATE *cb_state, uint32_t memBarrierCount,\n                                        const VkImageMemoryBarrier *pImgMemBarriers) {\n    for (uint32_t i = 0; i < memBarrierCount; ++i) {\n        const auto &mem_barrier = pImgMemBarriers[i];\n\n        // For ownership transfers, the barrier is specified twice; as a release\n        // operation on the yielding queue family, and as an acquire operation\n        // on the acquiring queue family. This barrier may also include a layout\n        // transition, which occurs 'between' the two operations. For validation\n        // purposes it doesn't seem important which side performs the layout\n        // transition, but it must not be performed twice. We'll arbitrarily\n        // choose to perform it as part of the acquire operation.\n        //\n        // However, we still need to record initial layout for the \"initial layout\" validation\n        const bool is_release_op = IsReleaseOp(cb_state, mem_barrier);\n\n        auto *image_state = GetImageState(mem_barrier.image);\n        if (!image_state) continue;\n        RecordTransitionImageLayout(cb_state, image_state, mem_barrier, is_release_op);\n    }\n}\n\nvoid CoreChecks::RecordTransitionImageLayout(CMD_BUFFER_STATE *cb_state, const IMAGE_STATE *image_state,\n                                             const VkImageMemoryBarrier &mem_barrier, bool is_release_op) {\n    VkImageSubresourceRange normalized_isr = NormalizeSubresourceRange(*image_state, mem_barrier.subresourceRange);\n    const auto &image_create_info = image_state->createInfo;\n\n    // Special case for 3D images with VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT_KHR flag bit, where <extent.depth> and\n    // <arrayLayers> can potentially alias. When recording layout for the entire image, pre-emptively record layouts\n    // for all (potential) layer sub_resources.\n    if (0 != (image_create_info.flags & VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT_KHR)) {\n        normalized_isr.baseArrayLayer = 0;\n        normalized_isr.layerCount = image_create_info.extent.depth;  // Treat each depth slice as a layer subresource\n    }\n\n    VkImageLayout initial_layout = mem_barrier.oldLayout;\n\n    // Layout transitions in external instance are not tracked, so don't validate initial layout.\n    if (QueueFamilyIsExternal(mem_barrier.srcQueueFamilyIndex)) {\n        initial_layout = VK_IMAGE_LAYOUT_UNDEFINED;\n    }\n\n    if (is_release_op) {\n        SetImageInitialLayout(cb_state, *image_state, normalized_isr, mem_barrier.oldLayout);\n    } else {\n        SetImageLayout(cb_state, *image_state, normalized_isr, mem_barrier.newLayout, initial_layout);\n    }\n}\n\nbool CoreChecks::VerifyImageLayout(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *image_state,\n                                   const VkImageSubresourceRange &range, VkImageAspectFlags aspect_mask,\n                                   VkImageLayout explicit_layout, VkImageLayout optimal_layout, const char *caller,\n                                   const char *layout_invalid_msg_code, const char *layout_mismatch_msg_code, bool *error) const {\n    if (disabled[image_layout_validation]) return false;\n    assert(cb_node);\n    assert(image_state);\n    const auto image = image_state->image;\n    bool skip = false;\n\n    const auto *subresource_map = GetImageSubresourceLayoutMap(cb_node, image);\n    if (subresource_map) {\n        bool subres_skip = false;\n        LayoutUseCheckAndMessage layout_check(subresource_map, aspect_mask);\n        for (auto pos = subresource_map->Find(range); (pos != subresource_map->End()) && !subres_skip; ++pos) {\n            if (!layout_check.Check(pos->subresource, explicit_layout, pos->current_layout, pos->initial_layout)) {\n                *error = true;\n                subres_skip |= LogError(cb_node->commandBuffer, layout_mismatch_msg_code,\n                                        \"%s: Cannot use %s (layer=%u mip=%u) with specific layout %s that doesn't match the \"\n                                        \"%s layout %s.\",\n                                        caller, report_data->FormatHandle(image).c_str(), pos->subresource.arrayLayer,\n                                        pos->subresource.mipLevel, string_VkImageLayout(explicit_layout), layout_check.message,\n                                        string_VkImageLayout(layout_check.layout));\n            }\n        }\n        skip |= subres_skip;\n    }\n\n    // If optimal_layout is not UNDEFINED, check that layout matches optimal for this case\n    if ((VK_IMAGE_LAYOUT_UNDEFINED != optimal_layout) && (explicit_layout != optimal_layout)) {\n        if (VK_IMAGE_LAYOUT_GENERAL == explicit_layout) {\n            if (image_state->createInfo.tiling != VK_IMAGE_TILING_LINEAR) {\n                // LAYOUT_GENERAL is allowed, but may not be performance optimal, flag as perf warning.\n                skip |= LogPerformanceWarning(cb_node->commandBuffer, kVUID_Core_DrawState_InvalidImageLayout,\n                                              \"%s: For optimal performance %s layout should be %s instead of GENERAL.\", caller,\n                                              report_data->FormatHandle(image).c_str(), string_VkImageLayout(optimal_layout));\n            }\n        } else if (device_extensions.vk_khr_shared_presentable_image) {\n            if (image_state->shared_presentable) {\n                if (VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR != explicit_layout) {\n                    skip |=\n                        LogError(device, layout_invalid_msg_code,\n                                 \"%s: Layout for shared presentable image is %s but must be VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR.\",\n                                 caller, string_VkImageLayout(optimal_layout));\n                }\n            }\n        } else {\n            *error = true;\n            skip |= LogError(cb_node->commandBuffer, layout_invalid_msg_code,\n                             \"%s: Layout for %s is %s but can only be %s or VK_IMAGE_LAYOUT_GENERAL.\", caller,\n                             report_data->FormatHandle(image).c_str(), string_VkImageLayout(explicit_layout),\n                             string_VkImageLayout(optimal_layout));\n        }\n    }\n    return skip;\n}\nbool CoreChecks::VerifyImageLayout(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *image_state,\n                                   const VkImageSubresourceLayers &subLayers, VkImageLayout explicit_layout,\n                                   VkImageLayout optimal_layout, const char *caller, const char *layout_invalid_msg_code,\n                                   const char *layout_mismatch_msg_code, bool *error) const {\n    return VerifyImageLayout(cb_node, image_state, RangeFromLayers(subLayers), explicit_layout, optimal_layout, caller,\n                             layout_invalid_msg_code, layout_mismatch_msg_code, error);\n}\n\nvoid CoreChecks::TransitionFinalSubpassLayouts(CMD_BUFFER_STATE *pCB, const VkRenderPassBeginInfo *pRenderPassBegin,\n                                               FRAMEBUFFER_STATE *framebuffer_state) {\n    auto renderPass = GetRenderPassState(pRenderPassBegin->renderPass);\n    if (!renderPass) return;\n\n    const VkRenderPassCreateInfo2KHR *pRenderPassInfo = renderPass->createInfo.ptr();\n    if (framebuffer_state) {\n        IMAGE_VIEW_STATE *view_state = nullptr;\n        for (uint32_t i = 0; i < pRenderPassInfo->attachmentCount; ++i) {\n            if (framebuffer_state->createInfo.flags & VK_FRAMEBUFFER_CREATE_IMAGELESS_BIT_KHR) {\n                const auto attachment_info = lvl_find_in_chain<VkRenderPassAttachmentBeginInfoKHR>(pRenderPassBegin->pNext);\n                if (attachment_info) view_state = GetImageViewState(attachment_info->pAttachments[i]);\n            } else {\n                view_state = GetAttachmentImageViewState(pCB, framebuffer_state, i);\n            }\n            if (view_state) {\n                VkImageLayout stencil_layout = kInvalidLayout;\n                const auto *attachment_description_stencil_layout =\n                    lvl_find_in_chain<VkAttachmentDescriptionStencilLayoutKHR>(pRenderPassInfo->pAttachments[i].pNext);\n                if (attachment_description_stencil_layout) {\n                    stencil_layout = attachment_description_stencil_layout->stencilFinalLayout;\n                }\n                SetImageViewLayout(pCB, *view_state, pRenderPassInfo->pAttachments[i].finalLayout, stencil_layout);\n            }\n        }\n    }\n}\n\n#ifdef VK_USE_PLATFORM_ANDROID_KHR\n// Android-specific validation that uses types defined only with VK_USE_PLATFORM_ANDROID_KHR\n// This could also move into a seperate core_validation_android.cpp file... ?\n\n//\n// AHB-specific validation within non-AHB APIs\n//\nbool CoreChecks::ValidateCreateImageANDROID(const debug_report_data *report_data, const VkImageCreateInfo *create_info) const {\n    bool skip = false;\n\n    const VkExternalFormatANDROID *ext_fmt_android = lvl_find_in_chain<VkExternalFormatANDROID>(create_info->pNext);\n    if (ext_fmt_android) {\n        if (0 != ext_fmt_android->externalFormat) {\n            if (VK_FORMAT_UNDEFINED != create_info->format) {\n                skip |=\n                    LogError(device, \"VUID-VkImageCreateInfo-pNext-01974\",\n                             \"vkCreateImage(): VkImageCreateInfo struct has a chained VkExternalFormatANDROID struct with non-zero \"\n                             \"externalFormat, but the VkImageCreateInfo's format is not VK_FORMAT_UNDEFINED.\");\n            }\n\n            if (0 != (VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT & create_info->flags)) {\n                skip |= LogError(device, \"VUID-VkImageCreateInfo-pNext-02396\",\n                                 \"vkCreateImage(): VkImageCreateInfo struct has a chained VkExternalFormatANDROID struct with \"\n                                 \"non-zero externalFormat, but flags include VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT.\");\n            }\n\n            if (0 != (~VK_IMAGE_USAGE_SAMPLED_BIT & create_info->usage)) {\n                skip |= LogError(device, \"VUID-VkImageCreateInfo-pNext-02397\",\n                                 \"vkCreateImage(): VkImageCreateInfo struct has a chained VkExternalFormatANDROID struct with \"\n                                 \"non-zero externalFormat, but usage includes bits (0x%\" PRIx64\n                                 \") other than VK_IMAGE_USAGE_SAMPLED_BIT.\",\n                                 create_info->usage);\n            }\n\n            if (VK_IMAGE_TILING_OPTIMAL != create_info->tiling) {\n                skip |= LogError(device, \"VUID-VkImageCreateInfo-pNext-02398\",\n                                 \"vkCreateImage(): VkImageCreateInfo struct has a chained VkExternalFormatANDROID struct with \"\n                                 \"non-zero externalFormat, but layout is not VK_IMAGE_TILING_OPTIMAL.\");\n            }\n        }\n\n        if ((0 != ext_fmt_android->externalFormat) &&\n            (ahb_ext_formats_map.find(ext_fmt_android->externalFormat) == ahb_ext_formats_map.end())) {\n            skip |= LogError(device, \"VUID-VkExternalFormatANDROID-externalFormat-01894\",\n                             \"vkCreateImage(): Chained VkExternalFormatANDROID struct contains a non-zero externalFormat (%\" PRIu64\n                             \") which has \"\n                             \"not been previously retrieved by vkGetAndroidHardwareBufferPropertiesANDROID().\",\n                             ext_fmt_android->externalFormat);\n        }\n    }\n\n    if ((nullptr == ext_fmt_android) || (0 == ext_fmt_android->externalFormat)) {\n        if (VK_FORMAT_UNDEFINED == create_info->format) {\n            skip |=\n                LogError(device, \"VUID-VkImageCreateInfo-pNext-01975\",\n                         \"vkCreateImage(): VkImageCreateInfo struct's format is VK_FORMAT_UNDEFINED, but either does not have a \"\n                         \"chained VkExternalFormatANDROID struct or the struct exists but has an externalFormat of 0.\");\n        }\n    }\n\n    const VkExternalMemoryImageCreateInfo *emici = lvl_find_in_chain<VkExternalMemoryImageCreateInfo>(create_info->pNext);\n    if (emici && (emici->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_ANDROID_HARDWARE_BUFFER_BIT_ANDROID)) {\n        if (create_info->imageType != VK_IMAGE_TYPE_2D) {\n            skip |=\n                LogError(device, \"VUID-VkImageCreateInfo-pNext-02393\",\n                         \"vkCreateImage(): VkImageCreateInfo struct with imageType %s has chained VkExternalMemoryImageCreateInfo \"\n                         \"struct with handleType VK_EXTERNAL_MEMORY_HANDLE_TYPE_ANDROID_HARDWARE_BUFFER_BIT_ANDROID.\",\n                         string_VkImageType(create_info->imageType));\n        }\n\n        if ((create_info->mipLevels != 1) && (create_info->mipLevels != FullMipChainLevels(create_info->extent))) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-pNext-02394\",\n                             \"vkCreateImage(): VkImageCreateInfo struct with chained VkExternalMemoryImageCreateInfo struct of \"\n                             \"handleType VK_EXTERNAL_MEMORY_HANDLE_TYPE_ANDROID_HARDWARE_BUFFER_BIT_ANDROID \"\n                             \"specifies mipLevels = %\" PRId32 \" (full chain mipLevels are %\" PRId32 \").\",\n                             create_info->mipLevels, FullMipChainLevels(create_info->extent));\n        }\n    }\n\n    return skip;\n}\n\nbool CoreChecks::ValidateCreateImageViewANDROID(const VkImageViewCreateInfo *create_info) const {\n    bool skip = false;\n    const IMAGE_STATE *image_state = GetImageState(create_info->image);\n\n    if (image_state->has_ahb_format) {\n        if (VK_FORMAT_UNDEFINED != create_info->format) {\n            skip |= LogError(create_info->image, \"VUID-VkImageViewCreateInfo-image-02399\",\n                             \"vkCreateImageView(): VkImageViewCreateInfo struct has a chained VkExternalFormatANDROID struct, but \"\n                             \"format member is %s and must be VK_FORMAT_UNDEFINED.\",\n                             string_VkFormat(create_info->format));\n        }\n\n        // Chain must include a compatible ycbcr conversion\n        bool conv_found = false;\n        uint64_t external_format = 0;\n        const VkSamplerYcbcrConversionInfo *ycbcr_conv_info = lvl_find_in_chain<VkSamplerYcbcrConversionInfo>(create_info->pNext);\n        if (ycbcr_conv_info != nullptr) {\n            VkSamplerYcbcrConversion conv_handle = ycbcr_conv_info->conversion;\n            if (ycbcr_conversion_ahb_fmt_map.find(conv_handle) != ycbcr_conversion_ahb_fmt_map.end()) {\n                conv_found = true;\n                external_format = ycbcr_conversion_ahb_fmt_map.at(conv_handle);\n            }\n        }\n        if ((!conv_found) || (external_format != image_state->ahb_format)) {\n            skip |= LogError(create_info->image, \"VUID-VkImageViewCreateInfo-image-02400\",\n                             \"vkCreateImageView(): VkImageViewCreateInfo struct has a chained VkExternalFormatANDROID struct with \"\n                             \"an externalFormat (%\" PRIu64\n                             \") but needs a chained VkSamplerYcbcrConversionInfo struct with a VkSamplerYcbcrConversion created \"\n                             \"with the same external format.\",\n                             image_state->ahb_format);\n        }\n\n        // Errors in create_info swizzles\n        if (IsIdentitySwizzle(create_info->components) == false) {\n            skip |= LogError(\n                create_info->image, \"VUID-VkImageViewCreateInfo-image-02401\",\n                \"vkCreateImageView(): VkImageViewCreateInfo struct has a chained VkExternalFormatANDROID struct, but \"\n                \"includes one or more non-identity component swizzles, r swizzle = %s, g swizzle = %s, b swizzle = %s, a swizzle \"\n                \"= %s.\",\n                string_VkComponentSwizzle(create_info->components.r), string_VkComponentSwizzle(create_info->components.g),\n                string_VkComponentSwizzle(create_info->components.b), string_VkComponentSwizzle(create_info->components.a));\n        }\n    }\n\n    return skip;\n}\n\nbool CoreChecks::ValidateGetImageSubresourceLayoutANDROID(const VkImage image) const {\n    bool skip = false;\n\n    const IMAGE_STATE *image_state = GetImageState(image);\n    if (image_state != nullptr) {\n        if (image_state->external_ahb && (0 == image_state->GetBoundMemory().size())) {\n            skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-image-01895\",\n                             \"vkGetImageSubresourceLayout(): Attempt to query layout from an image created with \"\n                             \"VK_EXTERNAL_MEMORY_HANDLE_TYPE_ANDROID_HARDWARE_BUFFER_BIT_ANDROID handleType which has not yet been \"\n                             \"bound to memory.\");\n        }\n    }\n    return skip;\n}\n\n#else\n\nbool CoreChecks::ValidateCreateImageANDROID(const debug_report_data *report_data, const VkImageCreateInfo *create_info) const {\n    return false;\n}\n\nbool CoreChecks::ValidateCreateImageViewANDROID(const VkImageViewCreateInfo *create_info) const { return false; }\n\nbool CoreChecks::ValidateGetImageSubresourceLayoutANDROID(const VkImage image) const { return false; }\n\n#endif  // VK_USE_PLATFORM_ANDROID_KHR\n\nbool CoreChecks::ValidateImageFormatFeatures(const VkImageCreateInfo *pCreateInfo) const {\n    bool skip = false;\n\n    // validates based on imageCreateFormatFeatures from vkspec.html#resources-image-creation-limits\n    VkFormatFeatureFlags tiling_features = VK_FORMAT_FEATURE_FLAG_BITS_MAX_ENUM;\n    const VkImageTiling image_tiling = pCreateInfo->tiling;\n    const VkFormat image_format = pCreateInfo->format;\n\n    if (image_format == VK_FORMAT_UNDEFINED) {\n        // VU 01975 states format can't be undefined unless an android externalFormat\n#ifdef VK_USE_PLATFORM_ANDROID_KHR\n        const VkExternalFormatANDROID *ext_fmt_android = lvl_find_in_chain<VkExternalFormatANDROID>(pCreateInfo->pNext);\n        if ((image_tiling == VK_IMAGE_TILING_OPTIMAL) && (ext_fmt_android != nullptr) && (0 != ext_fmt_android->externalFormat)) {\n            auto it = ahb_ext_formats_map.find(ext_fmt_android->externalFormat);\n            if (it != ahb_ext_formats_map.end()) {\n                tiling_features = it->second;\n            }\n        }\n#endif\n    } else if (image_tiling == VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT) {\n        uint64_t drm_format_modifier = 0;\n        const VkImageDrmFormatModifierExplicitCreateInfoEXT *drm_explicit =\n            lvl_find_in_chain<VkImageDrmFormatModifierExplicitCreateInfoEXT>(pCreateInfo->pNext);\n        const VkImageDrmFormatModifierListCreateInfoEXT *drm_implicit =\n            lvl_find_in_chain<VkImageDrmFormatModifierListCreateInfoEXT>(pCreateInfo->pNext);\n\n        if (drm_explicit != nullptr) {\n            drm_format_modifier = drm_explicit->drmFormatModifier;\n        } else {\n            // VUID 02261 makes sure its only explict or implict in parameter checking\n            assert(drm_implicit != nullptr);\n            for (uint32_t i = 0; i < drm_implicit->drmFormatModifierCount; i++) {\n                drm_format_modifier |= drm_implicit->pDrmFormatModifiers[i];\n            }\n        }\n\n        VkFormatProperties2 format_properties_2 = {VK_STRUCTURE_TYPE_FORMAT_PROPERTIES_2, nullptr};\n        VkDrmFormatModifierPropertiesListEXT drm_properties_list = {VK_STRUCTURE_TYPE_DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT,\n                                                                    nullptr};\n        format_properties_2.pNext = (void *)&drm_properties_list;\n        DispatchGetPhysicalDeviceFormatProperties2(physical_device, image_format, &format_properties_2);\n        std::vector<VkDrmFormatModifierPropertiesEXT> drm_properties;\n        drm_properties.resize(drm_properties_list.drmFormatModifierCount);\n        drm_properties_list.pDrmFormatModifierProperties = &drm_properties[0];\n        DispatchGetPhysicalDeviceFormatProperties2(physical_device, image_format, &format_properties_2);\n\n        for (uint32_t i = 0; i < drm_properties_list.drmFormatModifierCount; i++) {\n            if ((drm_properties_list.pDrmFormatModifierProperties[i].drmFormatModifier & drm_format_modifier) != 0) {\n                tiling_features |= drm_properties_list.pDrmFormatModifierProperties[i].drmFormatModifierTilingFeatures;\n            }\n        }\n    } else {\n        VkFormatProperties format_properties = GetPDFormatProperties(image_format);\n        tiling_features = (image_tiling == VK_IMAGE_TILING_LINEAR) ? format_properties.linearTilingFeatures\n                                                                   : format_properties.optimalTilingFeatures;\n    }\n\n    // Lack of disjoint format feature support while using the flag\n    if (FormatIsMultiplane(image_format) && ((pCreateInfo->flags & VK_IMAGE_CREATE_DISJOINT_BIT) != 0) &&\n        ((tiling_features & VK_FORMAT_FEATURE_DISJOINT_BIT) == 0)) {\n        skip |= LogError(device, \"VUID-VkImageCreateInfo-imageCreateFormatFeatures-02260\",\n                         \"vkCreateImage(): can't use VK_IMAGE_CREATE_DISJOINT_BIT because %s doesn't support \"\n                         \"VK_FORMAT_FEATURE_DISJOINT_BIT based on imageCreateFormatFeatures.\",\n                         string_VkFormat(pCreateInfo->format));\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCreateImage(VkDevice device, const VkImageCreateInfo *pCreateInfo,\n                                            const VkAllocationCallbacks *pAllocator, VkImage *pImage) const {\n    bool skip = false;\n\n    if (device_extensions.vk_android_external_memory_android_hardware_buffer) {\n        skip |= ValidateCreateImageANDROID(report_data, pCreateInfo);\n    } else {  // These checks are omitted or replaced when Android HW Buffer extension is active\n        if (pCreateInfo->format == VK_FORMAT_UNDEFINED) {\n            return LogError(device, \"VUID-VkImageCreateInfo-format-00943\",\n                            \"vkCreateImage(): VkFormat for image must not be VK_FORMAT_UNDEFINED.\");\n        }\n    }\n\n    if (pCreateInfo->flags & VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT) {\n        if (VK_IMAGE_TYPE_2D != pCreateInfo->imageType) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-flags-00949\",\n                             \"vkCreateImage(): Image type must be VK_IMAGE_TYPE_2D when VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT \"\n                             \"flag bit is set\");\n        }\n\n        if ((pCreateInfo->extent.width != pCreateInfo->extent.height) || (pCreateInfo->arrayLayers < 6)) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-imageType-00954\",\n                             \"vkCreateImage(): If VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT flag bit is set, width (%d) must equal \"\n                             \"height (%d) and arrayLayers (%d) must be >= 6.\",\n                             pCreateInfo->extent.width, pCreateInfo->extent.height, pCreateInfo->arrayLayers);\n        }\n    }\n\n    const VkPhysicalDeviceLimits *device_limits = &phys_dev_props.limits;\n    VkImageUsageFlags attach_flags = VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT |\n                                     VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT;\n    if ((pCreateInfo->usage & attach_flags) && (pCreateInfo->extent.width > device_limits->maxFramebufferWidth)) {\n        skip |= LogError(device, \"VUID-VkImageCreateInfo-usage-00964\",\n                         \"vkCreateImage(): Image usage flags include a frame buffer attachment bit and image width exceeds device \"\n                         \"maxFramebufferWidth.\");\n    }\n\n    if ((pCreateInfo->usage & attach_flags) && (pCreateInfo->extent.height > device_limits->maxFramebufferHeight)) {\n        skip |= LogError(device, \"VUID-VkImageCreateInfo-usage-00965\",\n                         \"vkCreateImage(): Image usage flags include a frame buffer attachment bit and image height exceeds device \"\n                         \"maxFramebufferHeight\");\n    }\n\n    if (device_extensions.vk_ext_fragment_density_map || device_extensions.vk_ext_fragment_density_map_2) {\n        uint32_t ceiling_width =\n            (uint32_t)ceil((float)device_limits->maxFramebufferWidth /\n                           std::max((float)phys_dev_ext_props.fragment_density_map_props.minFragmentDensityTexelSize.width, 1.0f));\n        if ((pCreateInfo->usage & VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT) && (pCreateInfo->extent.width > ceiling_width)) {\n            skip |=\n                LogError(device, \"VUID-VkImageCreateInfo-usage-02559\",\n                         \"vkCreateImage(): Image usage flags include a fragment density map bit and image width (%u) exceeds the \"\n                         \"ceiling of device \"\n                         \"maxFramebufferWidth (%u) / minFragmentDensityTexelSize.width (%u). The ceiling value: %u\",\n                         pCreateInfo->extent.width, device_limits->maxFramebufferWidth,\n                         phys_dev_ext_props.fragment_density_map_props.minFragmentDensityTexelSize.width, ceiling_width);\n        }\n\n        uint32_t ceiling_height =\n            (uint32_t)ceil((float)device_limits->maxFramebufferHeight /\n                           std::max((float)phys_dev_ext_props.fragment_density_map_props.minFragmentDensityTexelSize.height, 1.0f));\n        if ((pCreateInfo->usage & VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT) && (pCreateInfo->extent.height > ceiling_height)) {\n            skip |=\n                LogError(device, \"VUID-VkImageCreateInfo-usage-02560\",\n                         \"vkCreateImage(): Image usage flags include a fragment density map bit and image height (%u) exceeds the \"\n                         \"ceiling of device \"\n                         \"maxFramebufferHeight (%u) / minFragmentDensityTexelSize.height (%u). The ceiling value: %u\",\n                         pCreateInfo->extent.height, device_limits->maxFramebufferHeight,\n                         phys_dev_ext_props.fragment_density_map_props.minFragmentDensityTexelSize.height, ceiling_height);\n        }\n    }\n\n    VkImageFormatProperties format_limits = {};\n    VkResult result = VK_SUCCESS;\n    if (pCreateInfo->tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT) {\n        result = DispatchGetPhysicalDeviceImageFormatProperties(physical_device, pCreateInfo->format, pCreateInfo->imageType,\n                                                                pCreateInfo->tiling, pCreateInfo->usage, pCreateInfo->flags,\n                                                                &format_limits);\n    } else {\n        auto modifier_list = lvl_find_in_chain<VkImageDrmFormatModifierListCreateInfoEXT>(pCreateInfo->pNext);\n        auto explicit_modifier = lvl_find_in_chain<VkImageDrmFormatModifierExplicitCreateInfoEXT>(pCreateInfo->pNext);\n        if (modifier_list) {\n            for (uint32_t i = 0; i < modifier_list->drmFormatModifierCount; i++) {\n                auto drm_format_modifier = lvl_init_struct<VkPhysicalDeviceImageDrmFormatModifierInfoEXT>();\n                drm_format_modifier.drmFormatModifier = modifier_list->pDrmFormatModifiers[i];\n                auto image_format_info = lvl_init_struct<VkPhysicalDeviceImageFormatInfo2>(&drm_format_modifier);\n                image_format_info.type = pCreateInfo->imageType;\n                image_format_info.format = pCreateInfo->format;\n                image_format_info.tiling = pCreateInfo->tiling;\n                image_format_info.usage = pCreateInfo->usage;\n                image_format_info.flags = pCreateInfo->flags;\n                auto image_format_properties = lvl_init_struct<VkImageFormatProperties2>();\n\n                result =\n                    DispatchGetPhysicalDeviceImageFormatProperties2(physical_device, &image_format_info, &image_format_properties);\n                format_limits = image_format_properties.imageFormatProperties;\n\n                /* The application gives a list of modifier and the driver\n                 * selects one. If one is wrong, stop there.\n                 */\n                if (result != VK_SUCCESS) break;\n            }\n        } else if (explicit_modifier) {\n            auto drm_format_modifier = lvl_init_struct<VkPhysicalDeviceImageDrmFormatModifierInfoEXT>();\n            drm_format_modifier.drmFormatModifier = explicit_modifier->drmFormatModifier;\n            auto image_format_info = lvl_init_struct<VkPhysicalDeviceImageFormatInfo2>(&drm_format_modifier);\n            image_format_info.type = pCreateInfo->imageType;\n            image_format_info.format = pCreateInfo->format;\n            image_format_info.tiling = pCreateInfo->tiling;\n            image_format_info.usage = pCreateInfo->usage;\n            image_format_info.flags = pCreateInfo->flags;\n            auto image_format_properties = lvl_init_struct<VkImageFormatProperties2>();\n\n            result = DispatchGetPhysicalDeviceImageFormatProperties2(physical_device, &image_format_info, &image_format_properties);\n            format_limits = image_format_properties.imageFormatProperties;\n        }\n    }\n\n    // 1. vkGetPhysicalDeviceImageFormatProperties[2] only success code is VK_SUCCESS\n    // 2. If call returns an error, then \"imageCreateImageFormatPropertiesList\" is defined to be the empty list\n    // 3. All values in 02251 are undefined if \"imageCreateImageFormatPropertiesList\" is empty.\n    if (result != VK_SUCCESS) {\n        // External memory will always have a \"imageCreateImageFormatPropertiesList\" so skip\n#ifdef VK_USE_PLATFORM_ANDROID_KHR\n        if (!lvl_find_in_chain<VkExternalFormatANDROID>(pCreateInfo->pNext))\n#endif  // VK_USE_PLATFORM_ANDROID_KHR\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-imageCreateMaxMipLevels-02251\",\n                             \"vkCreateImage(): Format %s is not supported for this combination of parameters and \"\n                             \"VkGetPhysicalDeviceImageFormatProperties returned back %s.\",\n                             string_VkFormat(pCreateInfo->format), string_VkResult(result));\n    } else {\n        if (pCreateInfo->mipLevels > format_limits.maxMipLevels) {\n            const char *format_string = string_VkFormat(pCreateInfo->format);\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-mipLevels-02255\",\n                             \"vkCreateImage(): Image mip levels=%d exceed image format maxMipLevels=%d for format %s.\",\n                             pCreateInfo->mipLevels, format_limits.maxMipLevels, format_string);\n        }\n\n        uint64_t texel_count = (uint64_t)pCreateInfo->extent.width * (uint64_t)pCreateInfo->extent.height *\n                               (uint64_t)pCreateInfo->extent.depth * (uint64_t)pCreateInfo->arrayLayers *\n                               (uint64_t)pCreateInfo->samples;\n        uint64_t total_size = (uint64_t)std::ceil(FormatTexelSize(pCreateInfo->format) * texel_count);\n\n        // Round up to imageGranularity boundary\n        VkDeviceSize imageGranularity = phys_dev_props.limits.bufferImageGranularity;\n        uint64_t ig_mask = imageGranularity - 1;\n        total_size = (total_size + ig_mask) & ~ig_mask;\n\n        if (total_size > format_limits.maxResourceSize) {\n            skip |= LogWarning(device, kVUID_Core_Image_InvalidFormatLimitsViolation,\n                               \"vkCreateImage(): resource size exceeds allowable maximum Image resource size = 0x%\" PRIxLEAST64\n                               \", maximum resource size = 0x%\" PRIxLEAST64 \" \",\n                               total_size, format_limits.maxResourceSize);\n        }\n\n        if (pCreateInfo->arrayLayers > format_limits.maxArrayLayers) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-arrayLayers-02256\",\n                             \"vkCreateImage(): arrayLayers=%d exceeds allowable maximum supported by format of %d.\",\n                             pCreateInfo->arrayLayers, format_limits.maxArrayLayers);\n        }\n\n        if ((pCreateInfo->samples & format_limits.sampleCounts) == 0) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-samples-02258\",\n                             \"vkCreateImage(): samples %s is not supported by format 0x%.8X.\",\n                             string_VkSampleCountFlagBits(pCreateInfo->samples), format_limits.sampleCounts);\n        }\n\n        if (pCreateInfo->extent.width > format_limits.maxExtent.width) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-extent-02252\",\n                             \"vkCreateImage(): extent.width %u exceeds allowable maximum image extent width %u.\",\n                             pCreateInfo->extent.width, format_limits.maxExtent.width);\n        }\n\n        if (pCreateInfo->extent.height > format_limits.maxExtent.height) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-extent-02253\",\n                             \"vkCreateImage(): extent.height %u exceeds allowable maximum image extent height %u.\",\n                             pCreateInfo->extent.height, format_limits.maxExtent.height);\n        }\n\n        if (pCreateInfo->extent.depth > format_limits.maxExtent.depth) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-extent-02254\",\n                             \"vkCreateImage(): extent.depth %u exceeds allowable maximum image extent depth %u.\",\n                             pCreateInfo->extent.depth, format_limits.maxExtent.depth);\n        }\n    }\n\n    // Tests for \"Formats requiring sampler YCBCR conversion for VK_IMAGE_ASPECT_COLOR_BIT image views\"\n    if (FormatRequiresYcbcrConversion(pCreateInfo->format)) {\n        if (!enabled_features.ycbcr_image_array_features.ycbcrImageArrays && pCreateInfo->arrayLayers != 1) {\n            const char *error_vuid = (device_extensions.vk_ext_ycbcr_image_arrays) ? \"VUID-VkImageCreateInfo-format-02653\"\n                                                                                   : \"VUID-VkImageCreateInfo-format-02564\";\n            skip |= LogError(device, error_vuid,\n                             \"vkCreateImage(): arrayLayers = %d, but when the ycbcrImagesArrays feature is not enabled and using a \"\n                             \"YCbCr Conversion format, arrayLayers must be 1\",\n                             pCreateInfo->arrayLayers);\n        }\n\n        if (pCreateInfo->mipLevels != 1) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-format-02561\",\n                             \"vkCreateImage(): mipLevels = %d, but when using a YCbCr Conversion format, mipLevels must be 1\",\n                             pCreateInfo->arrayLayers);\n        }\n\n        if (pCreateInfo->samples != VK_SAMPLE_COUNT_1_BIT) {\n            skip |= LogError(\n                device, \"VUID-VkImageCreateInfo-format-02562\",\n                \"vkCreateImage(): samples = %s, but when using a YCbCr Conversion format, samples must be VK_SAMPLE_COUNT_1_BIT\",\n                string_VkSampleCountFlagBits(pCreateInfo->samples));\n        }\n\n        if (pCreateInfo->imageType != VK_IMAGE_TYPE_2D) {\n            skip |= LogError(\n                device, \"VUID-VkImageCreateInfo-format-02563\",\n                \"vkCreateImage(): imageType = %s, but when using a YCbCr Conversion format, imageType must be VK_IMAGE_TYPE_2D \",\n                string_VkImageType(pCreateInfo->imageType));\n        }\n    }\n\n    if (device_extensions.vk_khr_maintenance2) {\n        if (pCreateInfo->flags & VK_IMAGE_CREATE_BLOCK_TEXEL_VIEW_COMPATIBLE_BIT) {\n            if (!(FormatIsCompressed_BC(pCreateInfo->format) || FormatIsCompressed_ASTC(pCreateInfo->format) ||\n                  FormatIsCompressed_ETC2_EAC(pCreateInfo->format))) {\n                skip |= LogError(device, \"VUID-VkImageCreateInfo-flags-01572\",\n                                 \"vkCreateImage(): If pCreateInfo->flags contains VK_IMAGE_CREATE_BLOCK_TEXEL_VIEW_COMPATIBLE_BIT, \"\n                                 \"format must be block, ETC or ASTC compressed, but is %s\",\n                                 string_VkFormat(pCreateInfo->format));\n            }\n            if (!(pCreateInfo->flags & VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT)) {\n                skip |= LogError(device, \"VUID-VkImageCreateInfo-flags-01573\",\n                                 \"vkCreateImage(): If pCreateInfo->flags contains VK_IMAGE_CREATE_BLOCK_TEXEL_VIEW_COMPATIBLE_BIT, \"\n                                 \"flags must also contain VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT.\");\n            }\n        }\n    }\n\n    if (pCreateInfo->sharingMode == VK_SHARING_MODE_CONCURRENT && pCreateInfo->pQueueFamilyIndices) {\n        skip |= ValidatePhysicalDeviceQueueFamilies(pCreateInfo->queueFamilyIndexCount, pCreateInfo->pQueueFamilyIndices,\n                                                    \"vkCreateImage\", \"pCreateInfo->pQueueFamilyIndices\",\n                                                    \"VUID-VkImageCreateInfo-sharingMode-01420\");\n    }\n\n    if (!FormatIsMultiplane(pCreateInfo->format) && !(pCreateInfo->flags & VK_IMAGE_CREATE_ALIAS_BIT) &&\n        (pCreateInfo->flags & VK_IMAGE_CREATE_DISJOINT_BIT)) {\n        skip |=\n            LogError(device, \"VUID-VkImageCreateInfo-format-01577\",\n                     \"vkCreateImage(): format is %s and flags are %s. The flags should not include VK_IMAGE_CREATE_DISJOINT_BIT.\",\n                     string_VkFormat(pCreateInfo->format), string_VkImageCreateFlags(pCreateInfo->flags).c_str());\n    }\n\n    const auto swapchain_create_info = lvl_find_in_chain<VkImageSwapchainCreateInfoKHR>(pCreateInfo->pNext);\n    if (swapchain_create_info != nullptr) {\n        if (swapchain_create_info->swapchain != VK_NULL_HANDLE) {\n            const SWAPCHAIN_NODE *swapchain_state = GetSwapchainState(swapchain_create_info->swapchain);\n            const VkSwapchainCreateFlagsKHR swapchain_flags = swapchain_state->createInfo.flags;\n\n            // Validate rest of Swapchain Image create check that require swapchain state\n            const char *vuid = \"VUID-VkImageSwapchainCreateInfoKHR-swapchain-00995\";\n            if (((swapchain_flags & VK_SWAPCHAIN_CREATE_SPLIT_INSTANCE_BIND_REGIONS_BIT_KHR) != 0) &&\n                ((pCreateInfo->flags & VK_IMAGE_CREATE_SPLIT_INSTANCE_BIND_REGIONS_BIT) == 0)) {\n                skip |= LogError(\n                    device, vuid,\n                    \"vkCreateImage(): Swapchain was created with VK_SWAPCHAIN_CREATE_SPLIT_INSTANCE_BIND_REGIONS_BIT_KHR flag so \"\n                    \"all swapchain images must have the VK_IMAGE_CREATE_SPLIT_INSTANCE_BIND_REGIONS_BIT flag set.\");\n            }\n            if (((swapchain_flags & VK_SWAPCHAIN_CREATE_PROTECTED_BIT_KHR) != 0) &&\n                ((pCreateInfo->flags & VK_IMAGE_CREATE_PROTECTED_BIT) == 0)) {\n                skip |= LogError(device, vuid,\n                                 \"vkCreateImage(): Swapchain was created with VK_SWAPCHAIN_CREATE_PROTECTED_BIT_KHR flag so all \"\n                                 \"swapchain images must have the VK_IMAGE_CREATE_PROTECTED_BIT flag set.\");\n            }\n            const VkImageCreateFlags mutable_flags = (VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT | VK_IMAGE_CREATE_EXTENDED_USAGE_BIT_KHR);\n            if (((swapchain_flags & VK_SWAPCHAIN_CREATE_MUTABLE_FORMAT_BIT_KHR) != 0) &&\n                ((pCreateInfo->flags & mutable_flags) != mutable_flags)) {\n                skip |= LogError(device, vuid,\n                                 \"vkCreateImage(): Swapchain was created with VK_SWAPCHAIN_CREATE_MUTABLE_FORMAT_BIT_KHR flag so \"\n                                 \"all swapchain images must have the VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT and \"\n                                 \"VK_IMAGE_CREATE_EXTENDED_USAGE_BIT_KHR flags both set.\");\n            }\n        }\n    }\n\n    if ((pCreateInfo->flags & VK_IMAGE_CREATE_PROTECTED_BIT) != 0) {\n        if (enabled_features.core11.protectedMemory == VK_FALSE) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-flags-01890\",\n                             \"vkCreateImage(): the protectedMemory device feature is disabled: Images cannot be created with the \"\n                             \"VK_IMAGE_CREATE_PROTECTED_BIT set.\");\n        }\n        const VkImageCreateFlags invalid_flags =\n            VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT | VK_IMAGE_CREATE_SPARSE_ALIASED_BIT;\n        if ((pCreateInfo->flags & invalid_flags) != 0) {\n            skip |= LogError(device, \"VUID-VkImageCreateInfo-None-01891\",\n                             \"vkCreateImage(): VK_IMAGE_CREATE_PROTECTED_BIT is set so no sparse create flags can be used at same \"\n                             \"time (VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT | \"\n                             \"VK_IMAGE_CREATE_SPARSE_ALIASED_BIT).\");\n        }\n    }\n\n    skip |= ValidateImageFormatFeatures(pCreateInfo);\n\n    return skip;\n}\n\nvoid CoreChecks::PostCallRecordCreateImage(VkDevice device, const VkImageCreateInfo *pCreateInfo,\n                                           const VkAllocationCallbacks *pAllocator, VkImage *pImage, VkResult result) {\n    if (VK_SUCCESS != result) return;\n\n    StateTracker::PostCallRecordCreateImage(device, pCreateInfo, pAllocator, pImage, result);\n    auto image_state = Get<IMAGE_STATE>(*pImage);\n    AddInitialLayoutintoImageLayoutMap(*image_state, imageLayoutMap);\n}\n\nbool CoreChecks::PreCallValidateDestroyImage(VkDevice device, VkImage image, const VkAllocationCallbacks *pAllocator) const {\n    const IMAGE_STATE *image_state = GetImageState(image);\n    const VulkanTypedHandle obj_struct(image, kVulkanObjectTypeImage);\n    bool skip = false;\n    if (image_state) {\n        skip |= ValidateObjectNotInUse(image_state, obj_struct, \"vkDestroyImage\", \"VUID-vkDestroyImage-image-01000\");\n    }\n    return skip;\n}\n\nvoid CoreChecks::PreCallRecordDestroyImage(VkDevice device, VkImage image, const VkAllocationCallbacks *pAllocator) {\n    // Clean up validation specific data\n    EraseQFOReleaseBarriers<VkImageMemoryBarrier>(image);\n\n    imageLayoutMap.erase(image);\n\n    // Clean up generic image state\n    StateTracker::PreCallRecordDestroyImage(device, image, pAllocator);\n}\n\nbool CoreChecks::ValidateImageAttributes(const IMAGE_STATE *image_state, const VkImageSubresourceRange &range,\n                                         const char *param_name) const {\n    bool skip = false;\n    const VkImage image = image_state->image;\n    const VkFormat format = image_state->createInfo.format;\n\n    if (range.aspectMask != VK_IMAGE_ASPECT_COLOR_BIT) {\n        skip |= LogError(image, \"VUID-vkCmdClearColorImage-aspectMask-02498\",\n                         \"vkCmdClearColorImage(): %s.aspectMasks must only be set to VK_IMAGE_ASPECT_COLOR_BIT.\", param_name);\n    }\n\n    if (FormatIsDepthOrStencil(format)) {\n        skip |= LogError(image, \"VUID-vkCmdClearColorImage-image-00007\",\n                         \"vkCmdClearColorImage(): %s called with image %s which has a depth/stencil format (%s).\", param_name,\n                         report_data->FormatHandle(image).c_str(), string_VkFormat(format));\n    } else if (FormatIsCompressed(format)) {\n        skip |= LogError(image, \"VUID-vkCmdClearColorImage-image-00007\",\n                         \"vkCmdClearColorImage(): %s called with image %s which has a compressed format (%s).\", param_name,\n                         report_data->FormatHandle(image).c_str(), string_VkFormat(format));\n    }\n\n    if (!(image_state->createInfo.usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT)) {\n        skip |=\n            LogError(image, \"VUID-vkCmdClearColorImage-image-00002\",\n                     \"vkCmdClearColorImage() %s called with image %s which was created without VK_IMAGE_USAGE_TRANSFER_DST_BIT.\",\n                     param_name, report_data->FormatHandle(image).c_str());\n    }\n    return skip;\n}\n\nbool CoreChecks::VerifyClearImageLayout(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *image_state,\n                                        const VkImageSubresourceRange &range, VkImageLayout dest_image_layout,\n                                        const char *func_name) const {\n    bool skip = false;\n    if (strcmp(func_name, \"vkCmdClearDepthStencilImage()\") == 0) {\n        if ((dest_image_layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) && (dest_image_layout != VK_IMAGE_LAYOUT_GENERAL)) {\n            skip |= LogError(image_state->image, \"VUID-vkCmdClearDepthStencilImage-imageLayout-00012\",\n                             \"%s: Layout for cleared image is %s but can only be TRANSFER_DST_OPTIMAL or GENERAL.\", func_name,\n                             string_VkImageLayout(dest_image_layout));\n        }\n\n    } else {\n        assert(strcmp(func_name, \"vkCmdClearColorImage()\") == 0);\n        if (!device_extensions.vk_khr_shared_presentable_image) {\n            if ((dest_image_layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) && (dest_image_layout != VK_IMAGE_LAYOUT_GENERAL)) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdClearColorImage-imageLayout-00005\",\n                                 \"%s: Layout for cleared image is %s but can only be TRANSFER_DST_OPTIMAL or GENERAL.\", func_name,\n                                 string_VkImageLayout(dest_image_layout));\n            }\n        } else {\n            if ((dest_image_layout != VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL) && (dest_image_layout != VK_IMAGE_LAYOUT_GENERAL) &&\n                (dest_image_layout != VK_IMAGE_LAYOUT_SHARED_PRESENT_KHR)) {\n                skip |= LogError(\n                    image_state->image, \"VUID-vkCmdClearColorImage-imageLayout-01394\",\n                    \"%s: Layout for cleared image is %s but can only be TRANSFER_DST_OPTIMAL, SHARED_PRESENT_KHR, or GENERAL.\",\n                    func_name, string_VkImageLayout(dest_image_layout));\n            }\n        }\n    }\n\n    // Cast to const to prevent creation at validate time.\n    const auto *subresource_map = GetImageSubresourceLayoutMap(cb_node, image_state->image);\n    if (subresource_map) {\n        bool subres_skip = false;\n        LayoutUseCheckAndMessage layout_check(subresource_map);\n        VkImageSubresourceRange normalized_isr = NormalizeSubresourceRange(*image_state, range);\n        for (auto pos = subresource_map->Find(normalized_isr); (pos != subresource_map->End()) && !subres_skip; ++pos) {\n            if (!layout_check.Check(pos->subresource, dest_image_layout, pos->current_layout, pos->initial_layout)) {\n                const char *error_code = \"VUID-vkCmdClearColorImage-imageLayout-00004\";\n                if (strcmp(func_name, \"vkCmdClearDepthStencilImage()\") == 0) {\n                    error_code = \"VUID-vkCmdClearDepthStencilImage-imageLayout-00011\";\n                } else {\n                    assert(strcmp(func_name, \"vkCmdClearColorImage()\") == 0);\n                }\n                subres_skip |= LogError(cb_node->commandBuffer, error_code,\n                                        \"%s: Cannot clear an image whose layout is %s and doesn't match the %s layout %s.\",\n                                        func_name, string_VkImageLayout(dest_image_layout), layout_check.message,\n                                        string_VkImageLayout(layout_check.layout));\n            }\n        }\n        skip |= subres_skip;\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdClearColorImage(VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,\n                                                   const VkClearColorValue *pColor, uint32_t rangeCount,\n                                                   const VkImageSubresourceRange *pRanges) const {\n    bool skip = false;\n    // TODO : Verify memory is in VK_IMAGE_STATE_CLEAR state\n    const auto *cb_node = GetCBState(commandBuffer);\n    const auto *image_state = GetImageState(image);\n    if (cb_node && image_state) {\n        skip |= ValidateMemoryIsBoundToImage(image_state, \"vkCmdClearColorImage()\", \"VUID-vkCmdClearColorImage-image-00003\");\n        skip |= ValidateCmdQueueFlags(cb_node, \"vkCmdClearColorImage()\", VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT,\n                                      \"VUID-vkCmdClearColorImage-commandBuffer-cmdpool\");\n        skip |= ValidateCmd(cb_node, CMD_CLEARCOLORIMAGE, \"vkCmdClearColorImage()\");\n        if (device_extensions.vk_khr_maintenance1) {\n            skip |= ValidateImageFormatFeatureFlags(image_state, VK_FORMAT_FEATURE_TRANSFER_DST_BIT, \"vkCmdClearColorImage\",\n                                                    \"VUID-vkCmdClearColorImage-image-01993\");\n        }\n        skip |= InsideRenderPass(cb_node, \"vkCmdClearColorImage()\", \"VUID-vkCmdClearColorImage-renderpass\");\n        skip |=\n            ValidateProtectedImage(cb_node, image_state, \"vkCmdClearColorImage()\", \"VUID-vkCmdClearColorImage-commandBuffer-01805\");\n        skip |= ValidateUnprotectedImage(cb_node, image_state, \"vkCmdClearColorImage()\",\n                                         \"VUID-vkCmdClearColorImage-commandBuffer-01806\");\n        for (uint32_t i = 0; i < rangeCount; ++i) {\n            std::string param_name = \"pRanges[\" + std::to_string(i) + \"]\";\n            skip |= ValidateCmdClearColorSubresourceRange(image_state, pRanges[i], param_name.c_str());\n            skip |= ValidateImageAttributes(image_state, pRanges[i], param_name.c_str());\n            skip |= VerifyClearImageLayout(cb_node, image_state, pRanges[i], imageLayout, \"vkCmdClearColorImage()\");\n        }\n        // Tests for \"Formats requiring sampler Y\u2019CBCR conversion for VK_IMAGE_ASPECT_COLOR_BIT image views\"\n        if (FormatRequiresYcbcrConversion(image_state->createInfo.format)) {\n            skip |= LogError(device, \"VUID-vkCmdClearColorImage-image-01545\",\n                             \"vkCmdClearColorImage(): format (%s) must not be one of the formats requiring sampler YCBCR \"\n                             \"conversion for VK_IMAGE_ASPECT_COLOR_BIT image views\",\n                             string_VkFormat(image_state->createInfo.format));\n        }\n    }\n    return skip;\n}\n\nvoid CoreChecks::PreCallRecordCmdClearColorImage(VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,\n                                                 const VkClearColorValue *pColor, uint32_t rangeCount,\n                                                 const VkImageSubresourceRange *pRanges) {\n    StateTracker::PreCallRecordCmdClearColorImage(commandBuffer, image, imageLayout, pColor, rangeCount, pRanges);\n\n    auto cb_node = GetCBState(commandBuffer);\n    auto image_state = GetImageState(image);\n    if (cb_node && image_state) {\n        for (uint32_t i = 0; i < rangeCount; ++i) {\n            SetImageInitialLayout(cb_node, image, pRanges[i], imageLayout);\n        }\n    }\n}\n\nbool CoreChecks::PreCallValidateCmdClearDepthStencilImage(VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,\n                                                          const VkClearDepthStencilValue *pDepthStencil, uint32_t rangeCount,\n                                                          const VkImageSubresourceRange *pRanges) const {\n    bool skip = false;\n\n    // TODO : Verify memory is in VK_IMAGE_STATE_CLEAR state\n    const auto *cb_node = GetCBState(commandBuffer);\n    const auto *image_state = GetImageState(image);\n    if (cb_node && image_state) {\n        const VkFormat image_format = image_state->createInfo.format;\n        skip |= ValidateMemoryIsBoundToImage(image_state, \"vkCmdClearDepthStencilImage()\",\n                                             \"VUID-vkCmdClearDepthStencilImage-image-00010\");\n        skip |= ValidateCmdQueueFlags(cb_node, \"vkCmdClearDepthStencilImage()\", VK_QUEUE_GRAPHICS_BIT,\n                                      \"VUID-vkCmdClearDepthStencilImage-commandBuffer-cmdpool\");\n        skip |= ValidateCmd(cb_node, CMD_CLEARDEPTHSTENCILIMAGE, \"vkCmdClearDepthStencilImage()\");\n        if (device_extensions.vk_khr_maintenance1) {\n            skip |= ValidateImageFormatFeatureFlags(image_state, VK_FORMAT_FEATURE_TRANSFER_DST_BIT, \"vkCmdClearDepthStencilImage\",\n                                                    \"VUID-vkCmdClearDepthStencilImage-image-01994\");\n        }\n        skip |= InsideRenderPass(cb_node, \"vkCmdClearDepthStencilImage()\", \"VUID-vkCmdClearDepthStencilImage-renderpass\");\n        skip |= ValidateProtectedImage(cb_node, image_state, \"vkCmdClearDepthStencilImage()\",\n                                       \"VUID-vkCmdClearDepthStencilImage-commandBuffer-01807\");\n        skip |= ValidateUnprotectedImage(cb_node, image_state, \"vkCmdClearDepthStencilImage()\",\n                                         \"VUID-vkCmdClearDepthStencilImage-commandBuffer-01808\");\n\n        bool any_include_aspect_depth_bit = false;\n        bool any_include_aspect_stencil_bit = false;\n\n        for (uint32_t i = 0; i < rangeCount; ++i) {\n            std::string param_name = \"pRanges[\" + std::to_string(i) + \"]\";\n            skip |= ValidateCmdClearDepthSubresourceRange(image_state, pRanges[i], param_name.c_str());\n            skip |= VerifyClearImageLayout(cb_node, image_state, pRanges[i], imageLayout, \"vkCmdClearDepthStencilImage()\");\n            // Image aspect must be depth or stencil or both\n            VkImageAspectFlags valid_aspects = VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT;\n            if (((pRanges[i].aspectMask & valid_aspects) == 0) || ((pRanges[i].aspectMask & ~valid_aspects) != 0)) {\n                skip |= LogError(commandBuffer, \"VUID-vkCmdClearDepthStencilImage-aspectMask-02824\",\n                                 \"vkCmdClearDepthStencilImage(): pRanges[%u].aspectMask can only be VK_IMAGE_ASPECT_DEPTH_BIT \"\n                                 \"and/or VK_IMAGE_ASPECT_STENCIL_BIT.\",\n                                 i);\n            }\n            if ((pRanges[i].aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) != 0) {\n                any_include_aspect_depth_bit = true;\n                if (FormatHasDepth(image_format) == false) {\n                    skip |= LogError(commandBuffer, \"VUID-vkCmdClearDepthStencilImage-image-02826\",\n                                     \"vkCmdClearDepthStencilImage(): pRanges[%u].aspectMask has a VK_IMAGE_ASPECT_DEPTH_BIT but %s \"\n                                     \"doesn't have a depth component.\",\n                                     i, string_VkFormat(image_format));\n                }\n            }\n            if ((pRanges[i].aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) != 0) {\n                any_include_aspect_stencil_bit = true;\n                if (FormatHasStencil(image_format) == false) {\n                    skip |= LogError(commandBuffer, \"VUID-vkCmdClearDepthStencilImage-image-02825\",\n                                     \"vkCmdClearDepthStencilImage(): pRanges[%u].aspectMask has a VK_IMAGE_ASPECT_STENCIL_BIT but \"\n                                     \"%s doesn't have a stencil component.\",\n                                     i, string_VkFormat(image_format));\n                }\n            }\n        }\n        if (any_include_aspect_stencil_bit) {\n            const auto image_stencil_struct = lvl_find_in_chain<VkImageStencilUsageCreateInfoEXT>(image_state->createInfo.pNext);\n            if (image_stencil_struct != nullptr) {\n                if ((image_stencil_struct->stencilUsage & VK_IMAGE_USAGE_TRANSFER_DST_BIT) == 0) {\n                    skip |=\n                        LogError(device, \"VUID-vkCmdClearDepthStencilImage-pRanges-02658\",\n                                 \"vkCmdClearDepthStencilImage(): an element of pRanges.aspect includes VK_IMAGE_ASPECT_STENCIL_BIT \"\n                                 \"and image was created with separate stencil usage, VK_IMAGE_USAGE_TRANSFER_DST_BIT must be \"\n                                 \"included in VkImageStencilUsageCreateInfo::stencilUsage used to create image\");\n                }\n            } else {\n                if ((image_state->createInfo.usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT) == 0) {\n                    skip |= LogError(\n                        device, \"VUID-vkCmdClearDepthStencilImage-pRanges-02659\",\n                        \"vkCmdClearDepthStencilImage(): an element of pRanges.aspect includes VK_IMAGE_ASPECT_STENCIL_BIT and \"\n                        \"image was not created with separate stencil usage, VK_IMAGE_USAGE_TRANSFER_DST_BIT must be included \"\n                        \"in VkImageCreateInfo::usage used to create image\");\n                }\n            }\n        }\n        if (any_include_aspect_depth_bit && (image_state->createInfo.usage & VK_IMAGE_USAGE_TRANSFER_DST_BIT) == 0) {\n            skip |= LogError(device, \"VUID-vkCmdClearDepthStencilImage-pRanges-02660\",\n                             \"vkCmdClearDepthStencilImage(): an element of pRanges.aspect includes VK_IMAGE_ASPECT_DEPTH_BIT, \"\n                             \"VK_IMAGE_USAGE_TRANSFER_DST_BIT must be included in VkImageCreateInfo::usage used to create image\");\n        }\n        if (image_state && !FormatIsDepthOrStencil(image_format)) {\n            skip |= LogError(image, \"VUID-vkCmdClearDepthStencilImage-image-00014\",\n                             \"vkCmdClearDepthStencilImage(): called with image %s which doesn't have a depth/stencil format (%s).\",\n                             report_data->FormatHandle(image).c_str(), string_VkFormat(image_format));\n        }\n        if (VK_IMAGE_USAGE_TRANSFER_DST_BIT != (VK_IMAGE_USAGE_TRANSFER_DST_BIT & image_state->createInfo.usage)) {\n            skip |= LogError(image, \"VUID-vkCmdClearDepthStencilImage-image-00009\",\n                             \"vkCmdClearDepthStencilImage(): called with image %s which was not created with the \"\n                             \"VK_IMAGE_USAGE_TRANSFER_DST_BIT set.\",\n                             report_data->FormatHandle(image).c_str());\n        }\n    }\n    return skip;\n}\n\nvoid CoreChecks::PreCallRecordCmdClearDepthStencilImage(VkCommandBuffer commandBuffer, VkImage image, VkImageLayout imageLayout,\n                                                        const VkClearDepthStencilValue *pDepthStencil, uint32_t rangeCount,\n                                                        const VkImageSubresourceRange *pRanges) {\n    StateTracker::PreCallRecordCmdClearDepthStencilImage(commandBuffer, image, imageLayout, pDepthStencil, rangeCount, pRanges);\n    auto cb_node = GetCBState(commandBuffer);\n    auto image_state = GetImageState(image);\n    if (cb_node && image_state) {\n        for (uint32_t i = 0; i < rangeCount; ++i) {\n            SetImageInitialLayout(cb_node, image, pRanges[i], imageLayout);\n        }\n    }\n}\n\n// Returns true if [x, xoffset] and [y, yoffset] overlap\nstatic bool RangesIntersect(int32_t start, uint32_t start_offset, int32_t end, uint32_t end_offset) {\n    bool result = false;\n    uint32_t intersection_min = std::max(static_cast<uint32_t>(start), static_cast<uint32_t>(end));\n    uint32_t intersection_max = std::min(static_cast<uint32_t>(start) + start_offset, static_cast<uint32_t>(end) + end_offset);\n\n    if (intersection_max > intersection_min) {\n        result = true;\n    }\n    return result;\n}\n\n// Returns true if source area of first vkImageCopy/vkImageCopy2KHR region intersects dest area of second region\n// It is assumed that these are copy regions within a single image (otherwise no possibility of collision)\ntemplate <typename RegionType>\nstatic bool RegionIntersects(const RegionType *rgn0, const RegionType *rgn1, VkImageType type, bool is_multiplane) {\n    bool result = false;\n\n    // Separate planes within a multiplane image cannot intersect\n    if (is_multiplane && (rgn0->srcSubresource.aspectMask != rgn1->dstSubresource.aspectMask)) {\n        return result;\n    }\n\n    if ((rgn0->srcSubresource.mipLevel == rgn1->dstSubresource.mipLevel) &&\n        (RangesIntersect(rgn0->srcSubresource.baseArrayLayer, rgn0->srcSubresource.layerCount, rgn1->dstSubresource.baseArrayLayer,\n                         rgn1->dstSubresource.layerCount))) {\n        result = true;\n        switch (type) {\n            case VK_IMAGE_TYPE_3D:\n                result &= RangesIntersect(rgn0->srcOffset.z, rgn0->extent.depth, rgn1->dstOffset.z, rgn1->extent.depth);\n                // fall through\n            case VK_IMAGE_TYPE_2D:\n                result &= RangesIntersect(rgn0->srcOffset.y, rgn0->extent.height, rgn1->dstOffset.y, rgn1->extent.height);\n                // fall through\n            case VK_IMAGE_TYPE_1D:\n                result &= RangesIntersect(rgn0->srcOffset.x, rgn0->extent.width, rgn1->dstOffset.x, rgn1->extent.width);\n                break;\n            default:\n                // Unrecognized or new IMAGE_TYPE enums will be caught in parameter_validation\n                assert(false);\n        }\n    }\n    return result;\n}\n\n// Returns non-zero if offset and extent exceed image extents\nstatic const uint32_t x_bit = 1;\nstatic const uint32_t y_bit = 2;\nstatic const uint32_t z_bit = 4;\nstatic uint32_t ExceedsBounds(const VkOffset3D *offset, const VkExtent3D *extent, const VkExtent3D *image_extent) {\n    uint32_t result = 0;\n    // Extents/depths cannot be negative but checks left in for clarity\n    if ((offset->z + extent->depth > image_extent->depth) || (offset->z < 0) ||\n        ((offset->z + static_cast<int32_t>(extent->depth)) < 0)) {\n        result |= z_bit;\n    }\n    if ((offset->y + extent->height > image_extent->height) || (offset->y < 0) ||\n        ((offset->y + static_cast<int32_t>(extent->height)) < 0)) {\n        result |= y_bit;\n    }\n    if ((offset->x + extent->width > image_extent->width) || (offset->x < 0) ||\n        ((offset->x + static_cast<int32_t>(extent->width)) < 0)) {\n        result |= x_bit;\n    }\n    return result;\n}\n\n// Test if two VkExtent3D structs are equivalent\nstatic inline bool IsExtentEqual(const VkExtent3D *extent, const VkExtent3D *other_extent) {\n    bool result = true;\n    if ((extent->width != other_extent->width) || (extent->height != other_extent->height) ||\n        (extent->depth != other_extent->depth)) {\n        result = false;\n    }\n    return result;\n}\n\n// Test if the extent argument has all dimensions set to 0.\nstatic inline bool IsExtentAllZeroes(const VkExtent3D *extent) {\n    return ((extent->width == 0) && (extent->height == 0) && (extent->depth == 0));\n}\n\n// Returns the image transfer granularity for a specific image scaled by compressed block size if necessary.\nVkExtent3D CoreChecks::GetScaledItg(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *img) const {\n    // Default to (0, 0, 0) granularity in case we can't find the real granularity for the physical device.\n    VkExtent3D granularity = {0, 0, 0};\n    auto pPool = cb_node->command_pool.get();\n    if (pPool) {\n        granularity = GetPhysicalDeviceState()->queue_family_properties[pPool->queueFamilyIndex].minImageTransferGranularity;\n        if (FormatIsCompressed(img->createInfo.format) || FormatIsSinglePlane_422(img->createInfo.format)) {\n            auto block_size = FormatTexelBlockExtent(img->createInfo.format);\n            granularity.width *= block_size.width;\n            granularity.height *= block_size.height;\n        }\n    }\n    return granularity;\n}\n\n// Test elements of a VkExtent3D structure against alignment constraints contained in another VkExtent3D structure\nstatic inline bool IsExtentAligned(const VkExtent3D *extent, const VkExtent3D *granularity) {\n    bool valid = true;\n    if ((SafeModulo(extent->depth, granularity->depth) != 0) || (SafeModulo(extent->width, granularity->width) != 0) ||\n        (SafeModulo(extent->height, granularity->height) != 0)) {\n        valid = false;\n    }\n    return valid;\n}\n\n// Check elements of a VkOffset3D structure against a queue family's Image Transfer Granularity values\nbool CoreChecks::CheckItgOffset(const CMD_BUFFER_STATE *cb_node, const VkOffset3D *offset, const VkExtent3D *granularity,\n                                const uint32_t i, const char *function, const char *member, const char *vuid) const {\n    bool skip = false;\n    VkExtent3D offset_extent = {};\n    offset_extent.width = static_cast<uint32_t>(abs(offset->x));\n    offset_extent.height = static_cast<uint32_t>(abs(offset->y));\n    offset_extent.depth = static_cast<uint32_t>(abs(offset->z));\n    if (IsExtentAllZeroes(granularity)) {\n        // If the queue family image transfer granularity is (0, 0, 0), then the offset must always be (0, 0, 0)\n        if (IsExtentAllZeroes(&offset_extent) == false) {\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: pRegion[%d].%s (x=%d, y=%d, z=%d) must be (x=0, y=0, z=0) when the command buffer's queue family \"\n                             \"image transfer granularity is (w=0, h=0, d=0).\",\n                             function, i, member, offset->x, offset->y, offset->z);\n        }\n    } else {\n        // If the queue family image transfer granularity is not (0, 0, 0), then the offset dimensions must always be even\n        // integer multiples of the image transfer granularity.\n        if (IsExtentAligned(&offset_extent, granularity) == false) {\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: pRegion[%d].%s (x=%d, y=%d, z=%d) dimensions must be even integer multiples of this command \"\n                             \"buffer's queue family image transfer granularity (w=%d, h=%d, d=%d).\",\n                             function, i, member, offset->x, offset->y, offset->z, granularity->width, granularity->height,\n                             granularity->depth);\n        }\n    }\n    return skip;\n}\n\n// Check elements of a VkExtent3D structure against a queue family's Image Transfer Granularity values\nbool CoreChecks::CheckItgExtent(const CMD_BUFFER_STATE *cb_node, const VkExtent3D *extent, const VkOffset3D *offset,\n                                const VkExtent3D *granularity, const VkExtent3D *subresource_extent, const VkImageType image_type,\n                                const uint32_t i, const char *function, const char *member, const char *vuid) const {\n    bool skip = false;\n    if (IsExtentAllZeroes(granularity)) {\n        // If the queue family image transfer granularity is (0, 0, 0), then the extent must always match the image\n        // subresource extent.\n        if (IsExtentEqual(extent, subresource_extent) == false) {\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: pRegion[%d].%s (w=%d, h=%d, d=%d) must match the image subresource extents (w=%d, h=%d, d=%d) \"\n                             \"when the command buffer's queue family image transfer granularity is (w=0, h=0, d=0).\",\n                             function, i, member, extent->width, extent->height, extent->depth, subresource_extent->width,\n                             subresource_extent->height, subresource_extent->depth);\n        }\n    } else {\n        // If the queue family image transfer granularity is not (0, 0, 0), then the extent dimensions must always be even\n        // integer multiples of the image transfer granularity or the offset + extent dimensions must always match the image\n        // subresource extent dimensions.\n        VkExtent3D offset_extent_sum = {};\n        offset_extent_sum.width = static_cast<uint32_t>(abs(offset->x)) + extent->width;\n        offset_extent_sum.height = static_cast<uint32_t>(abs(offset->y)) + extent->height;\n        offset_extent_sum.depth = static_cast<uint32_t>(abs(offset->z)) + extent->depth;\n        bool x_ok = true;\n        bool y_ok = true;\n        bool z_ok = true;\n        switch (image_type) {\n            case VK_IMAGE_TYPE_3D:\n                z_ok = ((0 == SafeModulo(extent->depth, granularity->depth)) ||\n                        (subresource_extent->depth == offset_extent_sum.depth));\n                // fall through\n            case VK_IMAGE_TYPE_2D:\n                y_ok = ((0 == SafeModulo(extent->height, granularity->height)) ||\n                        (subresource_extent->height == offset_extent_sum.height));\n                // fall through\n            case VK_IMAGE_TYPE_1D:\n                x_ok = ((0 == SafeModulo(extent->width, granularity->width)) ||\n                        (subresource_extent->width == offset_extent_sum.width));\n                break;\n            default:\n                // Unrecognized or new IMAGE_TYPE enums will be caught in parameter_validation\n                assert(false);\n        }\n        if (!(x_ok && y_ok && z_ok)) {\n            skip |=\n                LogError(cb_node->commandBuffer, vuid,\n                         \"%s: pRegion[%d].%s (w=%d, h=%d, d=%d) dimensions must be even integer multiples of this command \"\n                         \"buffer's queue family image transfer granularity (w=%d, h=%d, d=%d) or offset (x=%d, y=%d, z=%d) + \"\n                         \"extent (w=%d, h=%d, d=%d) must match the image subresource extents (w=%d, h=%d, d=%d).\",\n                         function, i, member, extent->width, extent->height, extent->depth, granularity->width, granularity->height,\n                         granularity->depth, offset->x, offset->y, offset->z, extent->width, extent->height, extent->depth,\n                         subresource_extent->width, subresource_extent->height, subresource_extent->depth);\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateImageMipLevel(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *img, uint32_t mip_level,\n                                       const uint32_t i, const char *function, const char *member, const char *vuid) const {\n    bool skip = false;\n    if (mip_level >= img->createInfo.mipLevels) {\n        skip |= LogError(cb_node->commandBuffer, vuid, \"In %s, pRegions[%u].%s.mipLevel is %u, but provided %s has %u mip levels.\",\n                         function, i, member, mip_level, report_data->FormatHandle(img->image).c_str(), img->createInfo.mipLevels);\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateImageArrayLayerRange(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *img, const uint32_t base_layer,\n                                              const uint32_t layer_count, const uint32_t i, const char *function,\n                                              const char *member, const char *vuid) const {\n    bool skip = false;\n    if (base_layer >= img->createInfo.arrayLayers || layer_count > img->createInfo.arrayLayers ||\n        (base_layer + layer_count) > img->createInfo.arrayLayers) {\n        skip |= LogError(cb_node->commandBuffer, vuid,\n                         \"In %s, pRegions[%u].%s.baseArrayLayer is %u and .layerCount is \"\n                         \"%u, but provided %s has %u array layers.\",\n                         function, i, member, base_layer, layer_count, report_data->FormatHandle(img->image).c_str(),\n                         img->createInfo.arrayLayers);\n    }\n    return skip;\n}\n\n// Check valid usage Image Transfer Granularity requirements for elements of a VkBufferImageCopy/VkBufferImageCopy2KHR structure\ntemplate <typename BufferImageCopyRegionType>\nbool CoreChecks::ValidateCopyBufferImageTransferGranularityRequirements(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *img,\n                                                                        const BufferImageCopyRegionType *region, const uint32_t i,\n                                                                        const char *function, const char *vuid) const {\n    bool skip = false;\n    VkExtent3D granularity = GetScaledItg(cb_node, img);\n    skip |= CheckItgOffset(cb_node, &region->imageOffset, &granularity, i, function, \"imageOffset\", vuid);\n    VkExtent3D subresource_extent = GetImageSubresourceExtent(img, &region->imageSubresource);\n    skip |= CheckItgExtent(cb_node, &region->imageExtent, &region->imageOffset, &granularity, &subresource_extent,\n                           img->createInfo.imageType, i, function, \"imageExtent\", vuid);\n    return skip;\n}\n\n// Check valid usage Image Transfer Granularity requirements for elements of a VkImageCopy/VkImageCopy2KHR structure\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCopyImageTransferGranularityRequirements(const CMD_BUFFER_STATE *cb_node, const IMAGE_STATE *src_img,\n                                                                  const IMAGE_STATE *dst_img, const RegionType *region,\n                                                                  const uint32_t i, const char *function,\n                                                                  CopyCommandVersion version) const {\n    bool skip = false;\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *vuid;\n\n    // Source image checks\n    VkExtent3D granularity = GetScaledItg(cb_node, src_img);\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcOffset-01783\" : \"VUID-vkCmdCopyImage-srcOffset-01783\";\n    skip |= CheckItgOffset(cb_node, &region->srcOffset, &granularity, i, function, \"srcOffset\", vuid);\n    VkExtent3D subresource_extent = GetImageSubresourceExtent(src_img, &region->srcSubresource);\n    const VkExtent3D extent = region->extent;\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcOffset-01783\" : \"VUID-vkCmdCopyImage-srcOffset-01783\";\n    skip |= CheckItgExtent(cb_node, &extent, &region->srcOffset, &granularity, &subresource_extent, src_img->createInfo.imageType,\n                           i, function, \"extent\", vuid);\n\n    // Destination image checks\n    granularity = GetScaledItg(cb_node, dst_img);\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstOffset-01784\" : \"VUID-vkCmdCopyImage-dstOffset-01784\";\n    skip |= CheckItgOffset(cb_node, &region->dstOffset, &granularity, i, function, \"dstOffset\", vuid);\n    // Adjust dest extent, if necessary\n    const VkExtent3D dest_effective_extent =\n        GetAdjustedDestImageExtent(src_img->createInfo.format, dst_img->createInfo.format, extent);\n    subresource_extent = GetImageSubresourceExtent(dst_img, &region->dstSubresource);\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstOffset-01784\" : \"VUID-vkCmdCopyImage-dstOffset-01784\";\n    skip |= CheckItgExtent(cb_node, &dest_effective_extent, &region->dstOffset, &granularity, &subresource_extent,\n                           dst_img->createInfo.imageType, i, function, \"extent\", vuid);\n    return skip;\n}\n\n// Validate contents of a VkImageCopy or VkImageCopy2KHR struct\ntemplate <typename ImageCopyRegionType>\nbool CoreChecks::ValidateImageCopyData(const uint32_t regionCount, const ImageCopyRegionType *ic_regions,\n                                       const IMAGE_STATE *src_state, const IMAGE_STATE *dst_state,\n                                       CopyCommandVersion version) const {\n    bool skip = false;\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdCopyImage2KHR()\" : \"vkCmdCopyImage()\";\n    const char *vuid;\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        const ImageCopyRegionType region = ic_regions[i];\n\n        // For comp<->uncomp copies, the copy extent for the dest image must be adjusted\n        const VkExtent3D src_copy_extent = region.extent;\n        const VkExtent3D dst_copy_extent =\n            GetAdjustedDestImageExtent(src_state->createInfo.format, dst_state->createInfo.format, region.extent);\n\n        bool slice_override = false;\n        uint32_t depth_slices = 0;\n\n        // Special case for copying between a 1D/2D array and a 3D image\n        // TBD: This seems like the only way to reconcile 3 mutually-exclusive VU checks for 2D/3D copies. Heads up.\n        if ((VK_IMAGE_TYPE_3D == src_state->createInfo.imageType) && (VK_IMAGE_TYPE_3D != dst_state->createInfo.imageType)) {\n            depth_slices = region.dstSubresource.layerCount;  // Slice count from 2D subresource\n            slice_override = (depth_slices != 1);\n        } else if ((VK_IMAGE_TYPE_3D == dst_state->createInfo.imageType) && (VK_IMAGE_TYPE_3D != src_state->createInfo.imageType)) {\n            depth_slices = region.srcSubresource.layerCount;  // Slice count from 2D subresource\n            slice_override = (depth_slices != 1);\n        }\n\n        // Do all checks on source image\n        if (src_state->createInfo.imageType == VK_IMAGE_TYPE_1D) {\n            if ((0 != region.srcOffset.y) || (1 != src_copy_extent.height)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00146\" : \"VUID-vkCmdCopyImage-srcImage-00146\";\n                skip |= LogError(src_state->image, vuid,\n                                 \"%s: pRegion[%d] srcOffset.y is %d and extent.height is %d. For 1D images these must \"\n                                 \"be 0 and 1, respectively.\",\n                                 func_name, i, region.srcOffset.y, src_copy_extent.height);\n            }\n        }\n\n        if ((src_state->createInfo.imageType == VK_IMAGE_TYPE_1D) && ((0 != region.srcOffset.z) || (1 != src_copy_extent.depth))) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01785\" : \"VUID-vkCmdCopyImage-srcImage-01785\";\n            skip |= LogError(src_state->image, vuid,\n                             \"%s: pRegion[%d] srcOffset.z is %d and extent.depth is %d. For 1D images \"\n                             \"these must be 0 and 1, respectively.\",\n                             func_name, i, region.srcOffset.z, src_copy_extent.depth);\n        }\n\n        if ((src_state->createInfo.imageType == VK_IMAGE_TYPE_2D) && (0 != region.srcOffset.z)) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01787\" : \"VUID-vkCmdCopyImage-srcImage-01787\";\n            skip |= LogError(src_state->image, vuid, \"%s: pRegion[%d] srcOffset.z is %d. For 2D images the z-offset must be 0.\",\n                             func_name, i, region.srcOffset.z);\n        }\n\n        // Source checks that apply only to compressed images (or to _422 images if ycbcr enabled)\n        bool ext_ycbcr = IsExtEnabled(device_extensions.vk_khr_sampler_ycbcr_conversion);\n        if (FormatIsCompressed(src_state->createInfo.format) ||\n            (ext_ycbcr && FormatIsSinglePlane_422(src_state->createInfo.format))) {\n            const VkExtent3D block_size = FormatTexelBlockExtent(src_state->createInfo.format);\n            //  image offsets must be multiples of block dimensions\n            if ((SafeModulo(region.srcOffset.x, block_size.width) != 0) ||\n                (SafeModulo(region.srcOffset.y, block_size.height) != 0) ||\n                (SafeModulo(region.srcOffset.z, block_size.depth) != 0)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01727\" : \"VUID-vkCmdCopyImage-srcImage-01727\";\n                skip |= LogError(src_state->image, vuid,\n                                 \"%s: pRegion[%d] srcOffset (%d, %d) must be multiples of the compressed image's \"\n                                 \"texel width & height (%d, %d).\",\n                                 func_name, i, region.srcOffset.x, region.srcOffset.y, block_size.width, block_size.height);\n            }\n\n            const VkExtent3D mip_extent = GetImageSubresourceExtent(src_state, &(region.srcSubresource));\n            if ((SafeModulo(src_copy_extent.width, block_size.width) != 0) &&\n                (src_copy_extent.width + region.srcOffset.x != mip_extent.width)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01728\" : \"VUID-vkCmdCopyImage-srcImage-01728\";\n                skip |= LogError(src_state->image, vuid,\n                                 \"%s: pRegion[%d] extent width (%d) must be a multiple of the compressed texture block \"\n                                 \"width (%d), or when added to srcOffset.x (%d) must equal the image subresource width (%d).\",\n                                 func_name, i, src_copy_extent.width, block_size.width, region.srcOffset.x, mip_extent.width);\n            }\n\n            // Extent height must be a multiple of block height, or extent+offset height must equal subresource height\n            if ((SafeModulo(src_copy_extent.height, block_size.height) != 0) &&\n                (src_copy_extent.height + region.srcOffset.y != mip_extent.height)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01729\" : \"VUID-vkCmdCopyImage-srcImage-01729\";\n                skip |= LogError(src_state->image, vuid,\n                                 \"%s: pRegion[%d] extent height (%d) must be a multiple of the compressed texture block \"\n                                 \"height (%d), or when added to srcOffset.y (%d) must equal the image subresource height (%d).\",\n                                 func_name, i, src_copy_extent.height, block_size.height, region.srcOffset.y, mip_extent.height);\n            }\n\n            // Extent depth must be a multiple of block depth, or extent+offset depth must equal subresource depth\n            uint32_t copy_depth = (slice_override ? depth_slices : src_copy_extent.depth);\n            if ((SafeModulo(copy_depth, block_size.depth) != 0) && (copy_depth + region.srcOffset.z != mip_extent.depth)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01730\" : \"VUID-vkCmdCopyImage-srcImage-01730\";\n                skip |= LogError(src_state->image, vuid,\n                                 \"%s: pRegion[%d] extent width (%d) must be a multiple of the compressed texture block \"\n                                 \"depth (%d), or when added to srcOffset.z (%d) must equal the image subresource depth (%d).\",\n                                 func_name, i, src_copy_extent.depth, block_size.depth, region.srcOffset.z, mip_extent.depth);\n            }\n        }  // Compressed\n\n        // Do all checks on dest image\n        if (dst_state->createInfo.imageType == VK_IMAGE_TYPE_1D) {\n            if ((0 != region.dstOffset.y) || (1 != dst_copy_extent.height)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-00152\" : \"VUID-vkCmdCopyImage-dstImage-00152\";\n                skip |= LogError(dst_state->image, vuid,\n                                 \"%s: pRegion[%d] dstOffset.y is %d and dst_copy_extent.height is %d. For 1D images \"\n                                 \"these must be 0 and 1, respectively.\",\n                                 func_name, i, region.dstOffset.y, dst_copy_extent.height);\n            }\n        }\n\n        if ((dst_state->createInfo.imageType == VK_IMAGE_TYPE_1D) && ((0 != region.dstOffset.z) || (1 != dst_copy_extent.depth))) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01786\" : \"VUID-vkCmdCopyImage-dstImage-01786\";\n            skip |= LogError(dst_state->image, vuid,\n                             \"%s: pRegion[%d] dstOffset.z is %d and extent.depth is %d. For 1D images these must be 0 \"\n                             \"and 1, respectively.\",\n                             func_name, i, region.dstOffset.z, dst_copy_extent.depth);\n        }\n\n        if ((dst_state->createInfo.imageType == VK_IMAGE_TYPE_2D) && (0 != region.dstOffset.z)) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01788\" : \"VUID-vkCmdCopyImage-dstImage-01788\";\n            skip |= LogError(dst_state->image, vuid, \"%s: pRegion[%d] dstOffset.z is %d. For 2D images the z-offset must be 0.\",\n                             func_name, i, region.dstOffset.z);\n        }\n\n        // Handle difference between Maintenance 1\n        if (device_extensions.vk_khr_maintenance1) {\n            if (src_state->createInfo.imageType == VK_IMAGE_TYPE_3D) {\n                if ((0 != region.srcSubresource.baseArrayLayer) || (1 != region.srcSubresource.layerCount)) {\n                    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-04443\" : \"VUID-vkCmdCopyImage-srcImage-04443\";\n                    skip |= LogError(src_state->image, vuid,\n                                     \"%s: pRegion[%d] srcSubresource.baseArrayLayer is %d and srcSubresource.layerCount \"\n                                     \"is %d. For VK_IMAGE_TYPE_3D images these must be 0 and 1, respectively.\",\n                                     func_name, i, region.srcSubresource.baseArrayLayer, region.srcSubresource.layerCount);\n                }\n            }\n            if (dst_state->createInfo.imageType == VK_IMAGE_TYPE_3D) {\n                if ((0 != region.dstSubresource.baseArrayLayer) || (1 != region.dstSubresource.layerCount)) {\n                    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-04444\" : \"VUID-vkCmdCopyImage-dstImage-04444\";\n                    skip |= LogError(dst_state->image, vuid,\n                                     \"%s: pRegion[%d] dstSubresource.baseArrayLayer is %d and dstSubresource.layerCount \"\n                                     \"is %d. For VK_IMAGE_TYPE_3D images these must be 0 and 1, respectively.\",\n                                     func_name, i, region.dstSubresource.baseArrayLayer, region.dstSubresource.layerCount);\n                }\n            }\n        } else {  // Pre maint 1\n            if (src_state->createInfo.imageType == VK_IMAGE_TYPE_3D || dst_state->createInfo.imageType == VK_IMAGE_TYPE_3D) {\n                if ((0 != region.srcSubresource.baseArrayLayer) || (1 != region.srcSubresource.layerCount)) {\n                    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00139\" : \"VUID-vkCmdCopyImage-srcImage-00139\";\n                    skip |= LogError(src_state->image, vuid,\n                                     \"%s: pRegion[%d] srcSubresource.baseArrayLayer is %d and \"\n                                     \"srcSubresource.layerCount is %d. For copies with either source or dest of type \"\n                                     \"VK_IMAGE_TYPE_3D, these must be 0 and 1, respectively.\",\n                                     func_name, i, region.srcSubresource.baseArrayLayer, region.srcSubresource.layerCount);\n                }\n                if ((0 != region.dstSubresource.baseArrayLayer) || (1 != region.dstSubresource.layerCount)) {\n                    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00139\" : \"VUID-vkCmdCopyImage-srcImage-00139\";\n                    skip |= LogError(dst_state->image, vuid,\n                                     \"%s: pRegion[%d] dstSubresource.baseArrayLayer is %d and \"\n                                     \"dstSubresource.layerCount is %d. For copies with either source or dest of type \"\n                                     \"VK_IMAGE_TYPE_3D, these must be 0 and 1, respectively.\",\n                                     func_name, i, region.dstSubresource.baseArrayLayer, region.dstSubresource.layerCount);\n                }\n            }\n        }\n\n        // Dest checks that apply only to compressed images (or to _422 images if ycbcr enabled)\n        if (FormatIsCompressed(dst_state->createInfo.format) ||\n            (ext_ycbcr && FormatIsSinglePlane_422(dst_state->createInfo.format))) {\n            const VkExtent3D block_size = FormatTexelBlockExtent(dst_state->createInfo.format);\n\n            //  image offsets must be multiples of block dimensions\n            if ((SafeModulo(region.dstOffset.x, block_size.width) != 0) ||\n                (SafeModulo(region.dstOffset.y, block_size.height) != 0) ||\n                (SafeModulo(region.dstOffset.z, block_size.depth) != 0)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01731\" : \"VUID-vkCmdCopyImage-dstImage-01731\";\n                skip |= LogError(dst_state->image, vuid,\n                                 \"%s: pRegion[%d] dstOffset (%d, %d) must be multiples of the compressed image's \"\n                                 \"texel width & height (%d, %d).\",\n                                 func_name, i, region.dstOffset.x, region.dstOffset.y, block_size.width, block_size.height);\n            }\n\n            const VkExtent3D mip_extent = GetImageSubresourceExtent(dst_state, &(region.dstSubresource));\n            if ((SafeModulo(dst_copy_extent.width, block_size.width) != 0) &&\n                (dst_copy_extent.width + region.dstOffset.x != mip_extent.width)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01732\" : \"VUID-vkCmdCopyImage-dstImage-01732\";\n                skip |= LogError(dst_state->image, vuid,\n                                 \"%s: pRegion[%d] dst_copy_extent width (%d) must be a multiple of the compressed texture \"\n                                 \"block width (%d), or when added to dstOffset.x (%d) must equal the image subresource width (%d).\",\n                                 func_name, i, dst_copy_extent.width, block_size.width, region.dstOffset.x, mip_extent.width);\n            }\n\n            // Extent height must be a multiple of block height, or dst_copy_extent+offset height must equal subresource height\n            if ((SafeModulo(dst_copy_extent.height, block_size.height) != 0) &&\n                (dst_copy_extent.height + region.dstOffset.y != mip_extent.height)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01733\" : \"VUID-vkCmdCopyImage-dstImage-01733\";\n                skip |= LogError(dst_state->image, vuid,\n                                 \"%s: pRegion[%d] dst_copy_extent height (%d) must be a multiple of the compressed \"\n                                 \"texture block height (%d), or when added to dstOffset.y (%d) must equal the image subresource \"\n                                 \"height (%d).\",\n                                 func_name, i, dst_copy_extent.height, block_size.height, region.dstOffset.y, mip_extent.height);\n            }\n\n            // Extent depth must be a multiple of block depth, or dst_copy_extent+offset depth must equal subresource depth\n            uint32_t copy_depth = (slice_override ? depth_slices : dst_copy_extent.depth);\n            if ((SafeModulo(copy_depth, block_size.depth) != 0) && (copy_depth + region.dstOffset.z != mip_extent.depth)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01734\" : \"VUID-vkCmdCopyImage-dstImage-01734\";\n                skip |= LogError(dst_state->image, vuid,\n                                 \"%s: pRegion[%d] dst_copy_extent width (%d) must be a multiple of the compressed texture \"\n                                 \"block depth (%d), or when added to dstOffset.z (%d) must equal the image subresource depth (%d).\",\n                                 func_name, i, dst_copy_extent.depth, block_size.depth, region.dstOffset.z, mip_extent.depth);\n            }\n        }  // Compressed\n    }\n    return skip;\n}\n\n// vkCmdCopyImage / vmCmdCopyImage2KHR checks that only apply if the multiplane extension is enabled\ntemplate <typename RegionType>\nbool CoreChecks::CopyImageMultiplaneValidation(VkCommandBuffer command_buffer, const IMAGE_STATE *src_image_state,\n                                               const IMAGE_STATE *dst_image_state, const RegionType region,\n                                               CopyCommandVersion version) const {\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    bool skip = false;\n\n    const char *func_name = is_2khr ? \"vkCmdCopyImage2KHR()\" : \"vkCmdCopyImage()\";\n    const char *vuid;\n\n    // Neither image is multiplane\n    if ((!FormatIsMultiplane(src_image_state->createInfo.format)) && (!FormatIsMultiplane(dst_image_state->createInfo.format))) {\n        // If neither image is multi-plane the aspectMask member of src and dst must match\n        if (region.srcSubresource.aspectMask != region.dstSubresource.aspectMask) {\n            std::stringstream ss;\n            ss << func_name << \": Copy between non-multiplane images with differing aspectMasks ( 0x\" << std::hex\n               << region.srcSubresource.aspectMask << \" and 0x\" << region.dstSubresource.aspectMask << \" )\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01551\" : \"VUID-vkCmdCopyImage-srcImage-01551\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n    } else {\n        // Source image multiplane checks\n        uint32_t planes = FormatPlaneCount(src_image_state->createInfo.format);\n        VkImageAspectFlags aspect = region.srcSubresource.aspectMask;\n        if ((2 == planes) && (aspect != VK_IMAGE_ASPECT_PLANE_0_BIT_KHR) && (aspect != VK_IMAGE_ASPECT_PLANE_1_BIT_KHR)) {\n            std::stringstream ss;\n            ss << func_name << \": Source image aspect mask (0x\" << std::hex << aspect << \") is invalid for 2-plane format\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01552\" : \"VUID-vkCmdCopyImage-srcImage-01552\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n        if ((3 == planes) && (aspect != VK_IMAGE_ASPECT_PLANE_0_BIT_KHR) && (aspect != VK_IMAGE_ASPECT_PLANE_1_BIT_KHR) &&\n            (aspect != VK_IMAGE_ASPECT_PLANE_2_BIT_KHR)) {\n            std::stringstream ss;\n            ss << func_name << \": Source image aspect mask (0x\" << std::hex << aspect << \") is invalid for 3-plane format\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01553\" : \"VUID-vkCmdCopyImage-srcImage-01553\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n        // Single-plane to multi-plane\n        if ((!FormatIsMultiplane(src_image_state->createInfo.format)) && (FormatIsMultiplane(dst_image_state->createInfo.format)) &&\n            (VK_IMAGE_ASPECT_COLOR_BIT != aspect)) {\n            std::stringstream ss;\n            ss << func_name << \": Source image aspect mask (0x\" << std::hex << aspect << \") is not VK_IMAGE_ASPECT_COLOR_BIT\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01557\" : \"VUID-vkCmdCopyImage-dstImage-01557\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // Dest image multiplane checks\n        planes = FormatPlaneCount(dst_image_state->createInfo.format);\n        aspect = region.dstSubresource.aspectMask;\n        if ((2 == planes) && (aspect != VK_IMAGE_ASPECT_PLANE_0_BIT_KHR) && (aspect != VK_IMAGE_ASPECT_PLANE_1_BIT_KHR)) {\n            std::stringstream ss;\n            ss << func_name << \": Dest image aspect mask (0x\" << std::hex << aspect << \") is invalid for 2-plane format\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01554\" : \"VUID-vkCmdCopyImage-dstImage-01554\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n        if ((3 == planes) && (aspect != VK_IMAGE_ASPECT_PLANE_0_BIT_KHR) && (aspect != VK_IMAGE_ASPECT_PLANE_1_BIT_KHR) &&\n            (aspect != VK_IMAGE_ASPECT_PLANE_2_BIT_KHR)) {\n            std::stringstream ss;\n            ss << func_name << \": Dest image aspect mask (0x\" << std::hex << aspect << \") is invalid for 3-plane format\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01555\" : \"VUID-vkCmdCopyImage-dstImage-01555\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n        // Multi-plane to single-plane\n        if ((FormatIsMultiplane(src_image_state->createInfo.format)) && (!FormatIsMultiplane(dst_image_state->createInfo.format)) &&\n            (VK_IMAGE_ASPECT_COLOR_BIT != aspect)) {\n            std::stringstream ss;\n            ss << func_name << \": Dest image aspect mask (0x\" << std::hex << aspect << \") is not VK_IMAGE_ASPECT_COLOR_BIT\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01556\" : \"VUID-vkCmdCopyImage-srcImage-01556\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n    }\n\n    return skip;\n}\n\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCmdCopyImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                      VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                      const RegionType *pRegions, CopyCommandVersion version) const {\n    const auto *cb_node = GetCBState(commandBuffer);\n    const auto *src_image_state = GetImageState(srcImage);\n    const auto *dst_image_state = GetImageState(dstImage);\n    const VkFormat src_format = src_image_state->createInfo.format;\n    const VkFormat dst_format = dst_image_state->createInfo.format;\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    bool skip = false;\n\n    const char *func_name = is_2khr ? \"vkCmdCopyImage2KHR()\" : \"vkCmdCopyImage()\";\n    const CMD_TYPE cmd_type = is_2khr ? CMD_COPYIMAGE2KHR : CMD_COPYIMAGE;\n    const char *vuid;\n\n    skip = ValidateImageCopyData(regionCount, pRegions, src_image_state, dst_image_state, version);\n\n    VkCommandBuffer command_buffer = cb_node->commandBuffer;\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        const RegionType region = pRegions[i];\n\n        // For comp/uncomp copies, the copy extent for the dest image must be adjusted\n        VkExtent3D src_copy_extent = region.extent;\n        VkExtent3D dst_copy_extent = GetAdjustedDestImageExtent(src_format, dst_format, region.extent);\n\n        bool slice_override = false;\n        uint32_t depth_slices = 0;\n\n        // Special case for copying between a 1D/2D array and a 3D image\n        // TBD: This seems like the only way to reconcile 3 mutually-exclusive VU checks for 2D/3D copies. Heads up.\n        if ((VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType) &&\n            (VK_IMAGE_TYPE_3D != dst_image_state->createInfo.imageType)) {\n            depth_slices = region.dstSubresource.layerCount;  // Slice count from 2D subresource\n            slice_override = (depth_slices != 1);\n        } else if ((VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType) &&\n                   (VK_IMAGE_TYPE_3D != src_image_state->createInfo.imageType)) {\n            depth_slices = region.srcSubresource.layerCount;  // Slice count from 2D subresource\n            slice_override = (depth_slices != 1);\n        }\n\n        skip |= ValidateImageSubresourceLayers(cb_node, &region.srcSubresource, func_name, \"srcSubresource\", i);\n        skip |= ValidateImageSubresourceLayers(cb_node, &region.dstSubresource, func_name, \"dstSubresource\", i);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcSubresource-01696\" : \"VUID-vkCmdCopyImage-srcSubresource-01696\";\n        skip |=\n            ValidateImageMipLevel(cb_node, src_image_state, region.srcSubresource.mipLevel, i, func_name, \"srcSubresource\", vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstSubresource-01697\" : \"VUID-vkCmdCopyImage-dstSubresource-01697\";\n        skip |=\n            ValidateImageMipLevel(cb_node, dst_image_state, region.dstSubresource.mipLevel, i, func_name, \"dstSubresource\", vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcSubresource-01698\" : \"VUID-vkCmdCopyImage-srcSubresource-01698\";\n        skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, region.srcSubresource.baseArrayLayer,\n                                             region.srcSubresource.layerCount, i, func_name, \"srcSubresource\", vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstSubresource-01699\" : \"VUID-vkCmdCopyImage-dstSubresource-01699\";\n        skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, region.dstSubresource.baseArrayLayer,\n                                             region.dstSubresource.layerCount, i, func_name, \"dstSubresource\", vuid);\n\n        if (device_extensions.vk_khr_maintenance1) {\n            // No chance of mismatch if we're overriding depth slice count\n            if (!slice_override) {\n                // The number of depth slices in srcSubresource and dstSubresource must match\n                // Depth comes from layerCount for 1D,2D resources, from extent.depth for 3D\n                uint32_t src_slices =\n                    (VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType ? src_copy_extent.depth\n                                                                               : region.srcSubresource.layerCount);\n                uint32_t dst_slices =\n                    (VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType ? dst_copy_extent.depth\n                                                                               : region.dstSubresource.layerCount);\n                if (src_slices != dst_slices) {\n                    vuid = is_2khr ? \"VUID-VkImageCopy2KHR-extent-00140\" : \"VUID-VkImageCopy-extent-00140\";\n                    skip |= LogError(command_buffer, vuid,\n                                     \"%s: number of depth slices in source and destination subresources for \"\n                                     \"pRegions[%u] do not match.\",\n                                     func_name, i);\n                }\n            }\n        } else {\n            // For each region the layerCount member of srcSubresource and dstSubresource must match\n            if (region.srcSubresource.layerCount != region.dstSubresource.layerCount) {\n                vuid = is_2khr ? \"VUID-VkImageCopy2KHR-layerCount-00138\" : \"VUID-VkImageCopy-layerCount-00138\";\n                skip |= LogError(command_buffer, vuid,\n                                 \"%s: number of layers in source and destination subresources for pRegions[%u] do not match\",\n                                 func_name, i);\n            }\n        }\n\n        // Do multiplane-specific checks, if extension enabled\n        if (device_extensions.vk_khr_sampler_ycbcr_conversion) {\n            skip |= CopyImageMultiplaneValidation(command_buffer, src_image_state, dst_image_state, region, version);\n        }\n\n        if (!device_extensions.vk_khr_sampler_ycbcr_conversion) {\n            // not multi-plane, the aspectMask member of srcSubresource and dstSubresource must match\n            if (region.srcSubresource.aspectMask != region.dstSubresource.aspectMask) {\n                std::stringstream ss;\n                ss << func_name << \": Src and dest aspectMasks for each region must match\";\n                vuid = is_2khr ? \"VUID-VkImageCopy2KHR-aspectMask-00137\" : \"VUID-VkImageCopy-aspectMask-00137\";\n                skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n            }\n        }\n\n        // For each region, the aspectMask member of srcSubresource must be present in the source image\n        if (!VerifyAspectsPresent(region.srcSubresource.aspectMask, src_format)) {\n            std::stringstream ss;\n            ss << func_name << \": pRegion[\" << i\n               << \"] srcSubresource.aspectMask cannot specify aspects not present in source image\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-aspectMask-00142\" : \"VUID-vkCmdCopyImage-aspectMask-00142\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // For each region, the aspectMask member of dstSubresource must be present in the destination image\n        if (!VerifyAspectsPresent(region.dstSubresource.aspectMask, dst_format)) {\n            std::stringstream ss;\n            ss << func_name << \": pRegion[\" << i << \"] dstSubresource.aspectMask cannot specify aspects not present in dest image\";\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-aspectMask-00143\" : \"VUID-vkCmdCopyImage-aspectMask-00143\";\n            skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // Each dimension offset + extent limits must fall with image subresource extent\n        VkExtent3D subresource_extent = GetImageSubresourceExtent(src_image_state, &(region.srcSubresource));\n        if (slice_override) src_copy_extent.depth = depth_slices;\n        uint32_t extent_check = ExceedsBounds(&(region.srcOffset), &src_copy_extent, &subresource_extent);\n        if (extent_check & x_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcOffset-00144\" : \"VUID-vkCmdCopyImage-srcOffset-00144\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: Source image pRegion %1d x-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"width [%1d].\",\n                             func_name, i, region.srcOffset.x, src_copy_extent.width, subresource_extent.width);\n        }\n\n        if (extent_check & y_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcOffset-00145\" : \"VUID-vkCmdCopyImage-srcOffset-00145\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: Source image pRegion %1d y-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"height [%1d].\",\n                             func_name, i, region.srcOffset.y, src_copy_extent.height, subresource_extent.height);\n        }\n        if (extent_check & z_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcOffset-00147\" : \"VUID-vkCmdCopyImage-srcOffset-00147\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: Source image pRegion %1d z-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"depth [%1d].\",\n                             func_name, i, region.srcOffset.z, src_copy_extent.depth, subresource_extent.depth);\n        }\n\n        // Adjust dest extent if necessary\n        subresource_extent = GetImageSubresourceExtent(dst_image_state, &(region.dstSubresource));\n        if (slice_override) dst_copy_extent.depth = depth_slices;\n\n        extent_check = ExceedsBounds(&(region.dstOffset), &dst_copy_extent, &subresource_extent);\n        if (extent_check & x_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstOffset-00150\" : \"VUID-vkCmdCopyImage-dstOffset-00150\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: Dest image pRegion %1d x-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"width [%1d].\",\n                             func_name, i, region.dstOffset.x, dst_copy_extent.width, subresource_extent.width);\n        }\n        if (extent_check & y_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstOffset-00151\" : \"VUID-vkCmdCopyImage-dstOffset-00151\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s): Dest image pRegion %1d y-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"height [%1d].\",\n                             func_name, i, region.dstOffset.y, dst_copy_extent.height, subresource_extent.height);\n        }\n        if (extent_check & z_bit) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstOffset-00153\" : \"VUID-vkCmdCopyImage-dstOffset-00153\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: Dest image pRegion %1d z-dimension offset [%1d] + extent [%1d] exceeds subResource \"\n                             \"depth [%1d].\",\n                             func_name, i, region.dstOffset.z, dst_copy_extent.depth, subresource_extent.depth);\n        }\n\n        // The union of all source regions, and the union of all destination regions, specified by the elements of regions,\n        // must not overlap in memory\n        if (src_image_state->image == dst_image_state->image) {\n            for (uint32_t j = 0; j < regionCount; j++) {\n                if (RegionIntersects(&region, &pRegions[j], src_image_state->createInfo.imageType,\n                                     FormatIsMultiplane(src_format))) {\n                    std::stringstream ss;\n                    ss << func_name << \": pRegions[\" << i << \"] src overlaps with pRegions[\" << j << \"].\";\n                    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-pRegions-00124\" : \"VUID-vkCmdCopyImage-pRegions-00124\";\n                    skip |= LogError(command_buffer, vuid, \"%s.\", ss.str().c_str());\n                }\n            }\n        }\n\n        // Check depth for 2D as post Maintaince 1 requires both while prior only required one to be 2D\n        if (device_extensions.vk_khr_maintenance1) {\n            if (((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) &&\n                 (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType)) &&\n                (src_copy_extent.depth != 1)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01790\" : \"VUID-vkCmdCopyImage-srcImage-01790\";\n                skip |= LogError(command_buffer, vuid,\n                                 \"%s: pRegion[%u] both srcImage and dstImage are 2D and extent.depth is %u and has to be 1\",\n                                 func_name, i, src_copy_extent.depth);\n            }\n        } else {\n            if (((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) ||\n                 (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType)) &&\n                (src_copy_extent.depth != 1)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01789\" : \"VUID-vkCmdCopyImage-srcImage-01789\";\n                skip |= LogError(command_buffer, vuid,\n                                 \"%s: pRegion[%u] either srcImage or dstImage is 2D and extent.depth is %u and has to be 1\",\n                                 func_name, i, src_copy_extent.depth);\n            }\n        }\n\n        // Check if 2D with 3D and depth not equal to 2D layerCount\n        if ((VK_IMAGE_TYPE_2D == src_image_state->createInfo.imageType) &&\n            (VK_IMAGE_TYPE_3D == dst_image_state->createInfo.imageType) &&\n            (src_copy_extent.depth != region.srcSubresource.layerCount)) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01791\" : \"VUID-vkCmdCopyImage-srcImage-01791\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: pRegion[%u] srcImage is 2D, dstImage is 3D and extent.depth is %u and has to be \"\n                             \"srcSubresource.layerCount (%u)\",\n                             func_name, i, src_copy_extent.depth, region.srcSubresource.layerCount);\n        } else if ((VK_IMAGE_TYPE_3D == src_image_state->createInfo.imageType) &&\n                   (VK_IMAGE_TYPE_2D == dst_image_state->createInfo.imageType) &&\n                   (src_copy_extent.depth != region.dstSubresource.layerCount)) {\n            vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01792\" : \"VUID-vkCmdCopyImage-dstImage-01792\";\n            skip |= LogError(command_buffer, vuid,\n                             \"%s: pRegion[%u] srcImage is 3D, dstImage is 2D and extent.depth is %u and has to be \"\n                             \"dstSubresource.layerCount (%u)\",\n                             func_name, i, src_copy_extent.depth, region.dstSubresource.layerCount);\n        }\n\n        // Check for multi-plane format compatiblity\n        if (FormatIsMultiplane(src_format) || FormatIsMultiplane(dst_format)) {\n            size_t src_format_size = 0;\n            size_t dst_format_size = 0;\n            if (FormatIsMultiplane(src_format)) {\n                const VkFormat planeFormat = FindMultiplaneCompatibleFormat(src_format, region.srcSubresource.aspectMask);\n                src_format_size = FormatElementSize(planeFormat);\n            } else {\n                src_format_size = FormatElementSize(src_format);\n            }\n            if (FormatIsMultiplane(dst_format)) {\n                const VkFormat planeFormat = FindMultiplaneCompatibleFormat(dst_format, region.dstSubresource.aspectMask);\n                dst_format_size = FormatElementSize(planeFormat);\n            } else {\n                dst_format_size = FormatElementSize(dst_format);\n            }\n            // If size is still zero, then format is invalid and will be caught in another VU\n            if ((src_format_size != dst_format_size) && (src_format_size != 0) && (dst_format_size != 0)) {\n                vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-None-01549\" : \"VUID-vkCmdCopyImage-None-01549\";\n                skip |= LogError(command_buffer, vuid,\n                                 \"%s: pRegions[%u] called with non-compatible image formats. \"\n                                 \"The src format %s with aspectMask %s is not compatible with dst format %s aspectMask %s.\",\n                                 func_name, i, string_VkFormat(src_format),\n                                 string_VkImageAspectFlags(region.srcSubresource.aspectMask).c_str(), string_VkFormat(dst_format),\n                                 string_VkImageAspectFlags(region.dstSubresource.aspectMask).c_str());\n            }\n        }\n    }\n\n    // The formats of non-multiplane src_image and dst_image must be compatible. Formats are considered compatible if their texel\n    // size in bytes is the same between both formats. For example, VK_FORMAT_R8G8B8A8_UNORM is compatible with VK_FORMAT_R32_UINT\n    // because because both texels are 4 bytes in size.\n    if (!FormatIsMultiplane(src_format) && !FormatIsMultiplane(dst_format)) {\n        const char *compatible_vuid =\n            (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                ? (is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01548\" : \"VUID-vkCmdCopyImage-srcImage-01548\")\n                : (is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00135\" : \"VUID-vkCmdCopyImage-srcImage-00135\");\n        // Depth/stencil formats must match exactly.\n        if (FormatIsDepthOrStencil(src_format) || FormatIsDepthOrStencil(dst_format)) {\n            if (src_format != dst_format) {\n                skip |= LogError(command_buffer, compatible_vuid,\n                                 \"%s: Depth/stencil formats must match exactly for src (%s) and dst (%s).\", func_name,\n                                 string_VkFormat(src_format), string_VkFormat(dst_format));\n            }\n        } else {\n            if (FormatElementSize(src_format) != FormatElementSize(dst_format)) {\n                skip |= LogError(command_buffer, compatible_vuid,\n                                 \"%s: Unmatched image format sizes. \"\n                                 \"The src format %s has size of %zu and dst format %s has size of %zu.\",\n                                 func_name, string_VkFormat(src_format), FormatElementSize(src_format), string_VkFormat(dst_format),\n                                 FormatElementSize(dst_format));\n            }\n        }\n    }\n\n    // Source and dest image sample counts must match\n    if (src_image_state->createInfo.samples != dst_image_state->createInfo.samples) {\n        std::stringstream ss;\n        ss << func_name << \" called on image pair with non-identical sample counts.\";\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00136\" : \"VUID-vkCmdCopyImage-srcImage-00136\";\n        skip |= LogError(command_buffer, vuid, \"%s\", ss.str().c_str());\n    }\n\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00127\" : \"VUID-vkCmdCopyImage-srcImage-00127\";\n    skip |= ValidateMemoryIsBoundToImage(src_image_state, func_name, vuid);\n    vuid = is_2khr ? \"\" : \"\";\n    skip |= ValidateMemoryIsBoundToImage(dst_image_state, func_name, \"VUID-vkCmdCopyImage-dstImage-00132\");\n    // Validate that SRC & DST images have correct usage flags set\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-00126\" : \"VUID-vkCmdCopyImage-srcImage-00126\";\n    skip |= ValidateImageUsageFlags(src_image_state, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, true, vuid, func_name,\n                                    \"VK_IMAGE_USAGE_TRANSFER_SRC_BIT\");\n    vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-00131\" : \"VUID-vkCmdCopyImage-dstImage-00131\";\n    skip |= ValidateImageUsageFlags(dst_image_state, VK_IMAGE_USAGE_TRANSFER_DST_BIT, true, vuid, func_name,\n                                    \"VK_IMAGE_USAGE_TRANSFER_DST_BIT\");\n    vuid = is_2khr ? \"VUID-vkCmdCopyImage2KHR-commandBuffer-01825\" : \"VUID-vkCmdCopyImage-commandBuffer-01825\";\n    skip |= ValidateProtectedImage(cb_node, src_image_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyImage2KHR-commandBuffer-01826\" : \"VUID-vkCmdCopyImage-commandBuffer-01826\";\n    skip |= ValidateProtectedImage(cb_node, dst_image_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyImage2KHR-commandBuffer-01827\" : \"VUID-vkCmdCopyImage-commandBuffer-01827\";\n    skip |= ValidateUnprotectedImage(cb_node, dst_image_state, func_name, vuid);\n\n    // Validation for VK_EXT_fragment_density_map\n    if (src_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-02542\" : \"VUID-vkCmdCopyImage-dstImage-02542\";\n        skip |=\n            LogError(command_buffer, vuid,\n                     \"%s: srcImage must not have been created with flags containing VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\", func_name);\n    }\n    if (dst_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-02542\" : \"VUID-vkCmdCopyImage-dstImage-02542\";\n        skip |=\n            LogError(command_buffer, vuid,\n                     \"%s: dstImage must not have been created with flags containing VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\", func_name);\n    }\n\n    if (device_extensions.vk_khr_maintenance1) {\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImage-01995\" : \"VUID-vkCmdCopyImage-srcImage-01995\";\n        skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_TRANSFER_SRC_BIT, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImage-01996\" : \"VUID-vkCmdCopyImage-dstImage-01996\";\n        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_TRANSFER_DST_BIT, func_name, vuid);\n    }\n    vuid = is_2khr ? \"VUID-vkCmdCopyImage2KHR-commandBuffer-cmdpool\" : \"VUID-vkCmdCopyImage-commandBuffer-cmdpool\";\n    skip |= ValidateCmdQueueFlags(cb_node, func_name, VK_QUEUE_TRANSFER_BIT | VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT, vuid);\n    skip |= ValidateCmd(cb_node, cmd_type, func_name);\n    vuid = is_2khr ? \"VUID-vkCmdCopyImage2KHR-renderpass\" : \"VUID-vkCmdCopyImage-renderpass\";\n    skip |= InsideRenderPass(cb_node, func_name, vuid);\n    bool hit_error = false;\n\n    const char *invalid_src_layout_vuid =\n        (src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n            ? (is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImageLayout-01917\" : \"VUID-vkCmdCopyImage-srcImageLayout-01917\")\n            : (is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImageLayout-00129\" : \"VUID-vkCmdCopyImage-srcImageLayout-00129\");\n    const char *invalid_dst_layout_vuid =\n        (dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n            ? (is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImageLayout-01395\" : \"VUID-vkCmdCopyImage-dstImageLayout-01395\")\n            : (is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImageLayout-00134\" : \"VUID-vkCmdCopyImage-dstImageLayout-00134\");\n\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-srcImageLayout-00128\" : \"VUID-vkCmdCopyImage-srcImageLayout-00128\";\n        skip |= VerifyImageLayout(cb_node, src_image_state, pRegions[i].srcSubresource, srcImageLayout,\n                                  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, func_name, invalid_src_layout_vuid, vuid, &hit_error);\n        vuid = is_2khr ? \"VUID-VkCopyImageInfo2KHR-dstImageLayout-00133\" : \"VUID-vkCmdCopyImage-dstImageLayout-00133\";\n        skip |= VerifyImageLayout(cb_node, dst_image_state, pRegions[i].dstSubresource, dstImageLayout,\n                                  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, func_name, invalid_dst_layout_vuid, vuid, &hit_error);\n        skip |= ValidateCopyImageTransferGranularityRequirements(cb_node, src_image_state, dst_image_state, &pRegions[i], i,\n                                                                 func_name, version);\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdCopyImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                             VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                             const VkImageCopy *pRegions) const {\n    return ValidateCmdCopyImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount, pRegions,\n                                COPY_COMMAND_VERSION_1);\n}\n\nbool CoreChecks::PreCallValidateCmdCopyImage2KHR(VkCommandBuffer commandBuffer, const VkCopyImageInfo2KHR *pCopyImageInfo) const {\n    return ValidateCmdCopyImage(commandBuffer, pCopyImageInfo->srcImage, pCopyImageInfo->srcImageLayout, pCopyImageInfo->dstImage,\n                                pCopyImageInfo->dstImageLayout, pCopyImageInfo->regionCount, pCopyImageInfo->pRegions,\n                                COPY_COMMAND_VERSION_2);\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                           VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                           const VkImageCopy *pRegions) {\n    StateTracker::PreCallRecordCmdCopyImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount,\n                                            pRegions);\n    auto cb_node = GetCBState(commandBuffer);\n    auto src_image_state = GetImageState(srcImage);\n    auto dst_image_state = GetImageState(dstImage);\n\n    // Make sure that all image slices are updated to correct layout\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].srcSubresource, srcImageLayout);\n        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].dstSubresource, dstImageLayout);\n    }\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyImage2KHR(VkCommandBuffer commandBuffer, const VkCopyImageInfo2KHR *pCopyImageInfo) {\n    StateTracker::PreCallRecordCmdCopyImage2KHR(commandBuffer, pCopyImageInfo);\n    auto cb_node = GetCBState(commandBuffer);\n    auto src_image_state = GetImageState(pCopyImageInfo->srcImage);\n    auto dst_image_state = GetImageState(pCopyImageInfo->dstImage);\n\n    // Make sure that all image slices are updated to correct layout\n    for (uint32_t i = 0; i < pCopyImageInfo->regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *src_image_state, pCopyImageInfo->pRegions[i].srcSubresource,\n                              pCopyImageInfo->srcImageLayout);\n        SetImageInitialLayout(cb_node, *dst_image_state, pCopyImageInfo->pRegions[i].dstSubresource,\n                              pCopyImageInfo->dstImageLayout);\n    }\n}\n\n// Returns true if sub_rect is entirely contained within rect\nstatic inline bool ContainsRect(VkRect2D rect, VkRect2D sub_rect) {\n    if ((sub_rect.offset.x < rect.offset.x) || (sub_rect.offset.x + sub_rect.extent.width > rect.offset.x + rect.extent.width) ||\n        (sub_rect.offset.y < rect.offset.y) || (sub_rect.offset.y + sub_rect.extent.height > rect.offset.y + rect.extent.height))\n        return false;\n    return true;\n}\n\nbool CoreChecks::ValidateClearAttachmentExtent(VkCommandBuffer command_buffer, uint32_t attachment_index,\n                                               const FRAMEBUFFER_STATE *framebuffer, uint32_t fb_attachment,\n                                               const VkRect2D &render_area, uint32_t rect_count,\n                                               const VkClearRect *clear_rects) const {\n    bool skip = false;\n    const IMAGE_VIEW_STATE *image_view_state = nullptr;\n    if (framebuffer && (fb_attachment != VK_ATTACHMENT_UNUSED) && (fb_attachment < framebuffer->createInfo.attachmentCount)) {\n        image_view_state = GetAttachmentImageViewState(GetCBState(command_buffer), framebuffer, fb_attachment);\n    }\n\n    for (uint32_t j = 0; j < rect_count; j++) {\n        if (!ContainsRect(render_area, clear_rects[j].rect)) {\n            skip |= LogError(command_buffer, \"VUID-vkCmdClearAttachments-pRects-00016\",\n                             \"vkCmdClearAttachments(): The area defined by pRects[%d] is not contained in the area of \"\n                             \"the current render pass instance.\",\n                             j);\n        }\n\n        if (image_view_state) {\n            // The layers specified by a given element of pRects must be contained within every attachment that\n            // pAttachments refers to\n            const auto attachment_layer_count = image_view_state->create_info.subresourceRange.layerCount;\n            if ((clear_rects[j].baseArrayLayer >= attachment_layer_count) ||\n                (clear_rects[j].baseArrayLayer + clear_rects[j].layerCount > attachment_layer_count)) {\n                skip |= LogError(command_buffer, \"VUID-vkCmdClearAttachments-pRects-00017\",\n                                 \"vkCmdClearAttachments(): The layers defined in pRects[%d] are not contained in the layers \"\n                                 \"of pAttachment[%d].\",\n                                 j, attachment_index);\n            }\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdClearAttachments(VkCommandBuffer commandBuffer, uint32_t attachmentCount,\n                                                    const VkClearAttachment *pAttachments, uint32_t rectCount,\n                                                    const VkClearRect *pRects) const {\n    bool skip = false;\n    const CMD_BUFFER_STATE *cb_node = GetCBState(commandBuffer);  // TODO: Should be const, and never modified during validation\n    if (!cb_node) return skip;\n\n    skip |= ValidateCmdQueueFlags(cb_node, \"vkCmdClearAttachments()\", VK_QUEUE_GRAPHICS_BIT,\n                                  \"VUID-vkCmdClearAttachments-commandBuffer-cmdpool\");\n    skip |= ValidateCmd(cb_node, CMD_CLEARATTACHMENTS, \"vkCmdClearAttachments()\");\n    skip |= OutsideRenderPass(cb_node, \"vkCmdClearAttachments()\", \"VUID-vkCmdClearAttachments-renderpass\");\n\n    // Validate that attachment is in reference list of active subpass\n    if (cb_node->activeRenderPass) {\n        const VkRenderPassCreateInfo2KHR *renderpass_create_info = cb_node->activeRenderPass->createInfo.ptr();\n        const uint32_t renderpass_attachment_count = renderpass_create_info->attachmentCount;\n        const VkSubpassDescription2KHR *subpass_desc = &renderpass_create_info->pSubpasses[cb_node->activeSubpass];\n        const auto *framebuffer = cb_node->activeFramebuffer.get();\n        const auto &render_area = cb_node->activeRenderPassBeginInfo.renderArea;\n\n        for (uint32_t attachment_index = 0; attachment_index < attachmentCount; attachment_index++) {\n            auto clear_desc = &pAttachments[attachment_index];\n            uint32_t fb_attachment = VK_ATTACHMENT_UNUSED;\n\n            if (0 == clear_desc->aspectMask) {\n                skip |= LogError(commandBuffer, \"VUID-VkClearAttachment-aspectMask-requiredbitmask\", \" \");\n            } else if (clear_desc->aspectMask & VK_IMAGE_ASPECT_METADATA_BIT) {\n                skip |= LogError(commandBuffer, \"VUID-VkClearAttachment-aspectMask-00020\", \" \");\n            } else if (clear_desc->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) {\n                uint32_t color_attachment = VK_ATTACHMENT_UNUSED;\n                if (clear_desc->colorAttachment < subpass_desc->colorAttachmentCount) {\n                    color_attachment = subpass_desc->pColorAttachments[clear_desc->colorAttachment].attachment;\n                    if ((color_attachment != VK_ATTACHMENT_UNUSED) && (color_attachment >= renderpass_attachment_count)) {\n                        skip |= LogError(\n                            commandBuffer, \"VUID-vkCmdClearAttachments-aspectMask-02501\",\n                            \"vkCmdClearAttachments() pAttachments[%u].colorAttachment=%u is not VK_ATTACHMENT_UNUSED \"\n                            \"and not a valid attachment for %s attachmentCount=%u. Subpass %u pColorAttachment[%u]=%u.\",\n                            attachment_index, clear_desc->colorAttachment,\n                            report_data->FormatHandle(cb_node->activeRenderPass->renderPass).c_str(), cb_node->activeSubpass,\n                            clear_desc->colorAttachment, color_attachment, renderpass_attachment_count);\n\n                        color_attachment = VK_ATTACHMENT_UNUSED;  // Defensive, prevent lookup past end of renderpass attachment\n                    }\n                } else {\n                    skip |= LogError(commandBuffer, \"VUID-vkCmdClearAttachments-aspectMask-02501\",\n                                     \"vkCmdClearAttachments() pAttachments[%u].colorAttachment=%u out of range for %s\"\n                                     \" subpass %u. colorAttachmentCount=%u\",\n                                     attachment_index, clear_desc->colorAttachment,\n                                     report_data->FormatHandle(cb_node->activeRenderPass->renderPass).c_str(),\n                                     cb_node->activeSubpass, subpass_desc->colorAttachmentCount);\n                }\n                fb_attachment = color_attachment;\n\n                if ((clear_desc->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) ||\n                    (clear_desc->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT)) {\n                    char const str[] =\n                        \"vkCmdClearAttachments() aspectMask [%d] must set only VK_IMAGE_ASPECT_COLOR_BIT of a color attachment.\";\n                    skip |= LogError(commandBuffer, \"VUID-VkClearAttachment-aspectMask-00019\", str, attachment_index);\n                }\n            } else {  // Must be depth and/or stencil\n                if (((clear_desc->aspectMask & VK_IMAGE_ASPECT_DEPTH_BIT) != VK_IMAGE_ASPECT_DEPTH_BIT) &&\n                    ((clear_desc->aspectMask & VK_IMAGE_ASPECT_STENCIL_BIT) != VK_IMAGE_ASPECT_STENCIL_BIT)) {\n                    char const str[] = \"vkCmdClearAttachments() aspectMask [%d] is not a valid combination of bits.\";\n                    skip |= LogError(commandBuffer, \"VUID-VkClearAttachment-aspectMask-parameter\", str, attachment_index);\n                }\n                if (!subpass_desc->pDepthStencilAttachment ||\n                    (subpass_desc->pDepthStencilAttachment->attachment == VK_ATTACHMENT_UNUSED)) {\n                    skip |= LogPerformanceWarning(\n                        commandBuffer, kVUID_Core_DrawState_MissingAttachmentReference,\n                        \"vkCmdClearAttachments() depth/stencil clear with no depth/stencil attachment in subpass; ignored\");\n                } else {\n                    fb_attachment = subpass_desc->pDepthStencilAttachment->attachment;\n                }\n            }\n            if (cb_node->createInfo.level == VK_COMMAND_BUFFER_LEVEL_PRIMARY) {\n                skip |= ValidateClearAttachmentExtent(commandBuffer, attachment_index, framebuffer, fb_attachment, render_area,\n                                                      rectCount, pRects);\n            }\n\n            // Once the framebuffer attachment is found, can get the image view state\n            if (framebuffer && (fb_attachment != VK_ATTACHMENT_UNUSED) &&\n                (fb_attachment < framebuffer->createInfo.attachmentCount)) {\n                const IMAGE_VIEW_STATE *image_view_state =\n                    GetAttachmentImageViewState(GetCBState(commandBuffer), framebuffer, fb_attachment);\n                if (image_view_state != nullptr) {\n                    skip |= ValidateProtectedImage(cb_node, image_view_state->image_state.get(), \"vkCmdClearAttachments()\",\n                                                   \"VUID-vkCmdClearAttachments-commandBuffer-02504\");\n                    skip |= ValidateUnprotectedImage(cb_node, image_view_state->image_state.get(), \"vkCmdClearAttachments()\",\n                                                     \"VUID-vkCmdClearAttachments-commandBuffer-02505\");\n                }\n            }\n        }\n    }\n    return skip;\n}\n\nvoid CoreChecks::PreCallRecordCmdClearAttachments(VkCommandBuffer commandBuffer, uint32_t attachmentCount,\n                                                  const VkClearAttachment *pAttachments, uint32_t rectCount,\n                                                  const VkClearRect *pRects) {\n    auto *cb_node = GetCBState(commandBuffer);\n    if (cb_node->activeRenderPass && (cb_node->createInfo.level == VK_COMMAND_BUFFER_LEVEL_SECONDARY)) {\n        const VkRenderPassCreateInfo2KHR *renderpass_create_info = cb_node->activeRenderPass->createInfo.ptr();\n        const VkSubpassDescription2KHR *subpass_desc = &renderpass_create_info->pSubpasses[cb_node->activeSubpass];\n        std::shared_ptr<std::vector<VkClearRect>> clear_rect_copy;\n        for (uint32_t attachment_index = 0; attachment_index < attachmentCount; attachment_index++) {\n            const auto clear_desc = &pAttachments[attachment_index];\n            uint32_t fb_attachment = VK_ATTACHMENT_UNUSED;\n            if ((clear_desc->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) &&\n                (clear_desc->colorAttachment < subpass_desc->colorAttachmentCount)) {\n                fb_attachment = subpass_desc->pColorAttachments[clear_desc->colorAttachment].attachment;\n            } else if ((clear_desc->aspectMask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) &&\n                       subpass_desc->pDepthStencilAttachment) {\n                fb_attachment = subpass_desc->pDepthStencilAttachment->attachment;\n            }\n            if (fb_attachment != VK_ATTACHMENT_UNUSED) {\n                if (!clear_rect_copy) {\n                    // We need a copy of the clear rectangles that will persist until the last lambda executes\n                    // but we want to create it as lazily as possible\n                    clear_rect_copy.reset(new std::vector<VkClearRect>(pRects, pRects + rectCount));\n                }\n                // if a secondary level command buffer inherits the framebuffer from the primary command buffer\n                // (see VkCommandBufferInheritanceInfo), this validation must be deferred until queue submit time\n                auto val_fn = [this, commandBuffer, attachment_index, fb_attachment, rectCount, clear_rect_copy](\n                                  const CMD_BUFFER_STATE *prim_cb, const FRAMEBUFFER_STATE *fb) {\n                    assert(rectCount == clear_rect_copy->size());\n                    const auto &render_area = prim_cb->activeRenderPassBeginInfo.renderArea;\n                    bool skip = false;\n                    skip = ValidateClearAttachmentExtent(commandBuffer, attachment_index, fb, fb_attachment, render_area, rectCount,\n                                                         clear_rect_copy->data());\n                    return skip;\n                };\n                cb_node->cmd_execute_commands_functions.emplace_back(val_fn);\n            }\n        }\n    }\n}\n\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCmdResolveImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                         VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                         const RegionType *pRegions, CopyCommandVersion version) const {\n    const auto *cb_node = GetCBState(commandBuffer);\n    const auto *src_image_state = GetImageState(srcImage);\n    const auto *dst_image_state = GetImageState(dstImage);\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdResolveImage2KHR()\" : \"vkCmdResolveImage()\";\n    const CMD_TYPE cmd_type = is_2khr ? CMD_RESOLVEIMAGE : CMD_RESOLVEIMAGE2KHR;\n    const char *vuid;\n\n    bool skip = false;\n    if (cb_node && src_image_state && dst_image_state) {\n        vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-00256\" : \"VUID-vkCmdResolveImage-srcImage-00256\";\n        skip |= ValidateMemoryIsBoundToImage(src_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-00258\" : \"VUID-vkCmdResolveImage-dstImage-00258\";\n        skip |= ValidateMemoryIsBoundToImage(dst_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdResolveImage2KHR-commandBuffer-cmdpool\" : \"VUID-vkCmdResolveImage-commandBuffer-cmdpool\";\n        skip |= ValidateCmdQueueFlags(cb_node, func_name, VK_QUEUE_GRAPHICS_BIT, vuid);\n        skip |= ValidateCmd(cb_node, cmd_type, func_name);\n        vuid = is_2khr ? \"VUID-vkCmdResolveImage2KHR-renderpass\" : \"VUID-vkCmdResolveImage-renderpass\";\n        skip |= InsideRenderPass(cb_node, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-02003\" : \"VUID-vkCmdResolveImage-dstImage-02003\";\n        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdResolveImage2KHR-commandBuffer-01837\" : \"VUID-vkCmdResolveImage-commandBuffer-01837\";\n        skip |= ValidateProtectedImage(cb_node, src_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdResolveImage2KHR-commandBuffer-01838\" : \"VUID-vkCmdResolveImage-commandBuffer-01838\";\n        skip |= ValidateProtectedImage(cb_node, dst_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdResolveImage2KHR-commandBuffer-01839\" : \"VUID-vkCmdResolveImage-commandBuffer-01839\";\n        skip |= ValidateUnprotectedImage(cb_node, dst_image_state, func_name, vuid);\n\n        // Validation for VK_EXT_fragment_density_map\n        if (src_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-02546\" : \"VUID-vkCmdResolveImage-dstImage-02546\";\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: srcImage must not have been created with flags containing \"\n                             \"VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                             func_name);\n        }\n        if (dst_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-02546\" : \"VUID-vkCmdResolveImage-dstImage-02546\";\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: dstImage must not have been created with flags containing \"\n                             \"VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                             func_name);\n        }\n\n        bool hit_error = false;\n        const char *invalid_src_layout_vuid =\n            is_2khr ? ((src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-VkResolveImageInfo2KHR-srcImageLayout-01400\"\n                           : \"VUID-vkCmdResolveImag2KHR-srcImageLayout-00261\")\n                    : ((src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-vkCmdResolveImage-srcImageLayout-01400\"\n                           : \"VUID-vkCmdResolveImage-srcImageLayout-00261\");\n        const char *invalid_dst_layout_vuid =\n            is_2khr ? ((dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-VkResolveImageInfo2KHR-dstImageLayout-01401\"\n                           : \"VUID-VkResolveImageInfo2KHR-dstImageLayout-00263\")\n                    : ((dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-vkCmdResolveImage-dstImageLayout-01401\"\n                           : \"VUID-vkCmdResolveImage-dstImageLayout-00263\");\n        // For each region, the number of layers in the image subresource should not be zero\n        // For each region, src and dest image aspect must be color only\n        for (uint32_t i = 0; i < regionCount; i++) {\n            const RegionType region = pRegions[i];\n            const VkImageSubresourceLayers src_subresource = region.srcSubresource;\n            const VkImageSubresourceLayers dst_subresource = region.dstSubresource;\n            skip |= ValidateImageSubresourceLayers(cb_node, &src_subresource, func_name, \"srcSubresource\", i);\n            skip |= ValidateImageSubresourceLayers(cb_node, &dst_subresource, func_name, \"dstSubresource\", i);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImageLayout-00260\" : \"VUID-vkCmdResolveImage-srcImageLayout-00260\";\n            skip |= VerifyImageLayout(cb_node, src_image_state, src_subresource, srcImageLayout,\n                                      VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, func_name, invalid_src_layout_vuid, vuid, &hit_error);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImageLayout-00262\" : \"VUID-vkCmdResolveImage-dstImageLayout-00262\";\n            skip |= VerifyImageLayout(cb_node, dst_image_state, dst_subresource, dstImageLayout,\n                                      VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, func_name, invalid_dst_layout_vuid, vuid, &hit_error);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcSubresource-01709\" : \"VUID-vkCmdResolveImage-srcSubresource-01709\";\n            skip |= ValidateImageMipLevel(cb_node, src_image_state, src_subresource.mipLevel, i, func_name, \"srcSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstSubresource-01710\" : \"VUID-vkCmdResolveImage-dstSubresource-01710\";\n            skip |= ValidateImageMipLevel(cb_node, dst_image_state, dst_subresource.mipLevel, i, func_name, \"dstSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcSubresource-01711\" : \"VUID-vkCmdResolveImage-srcSubresource-01711\";\n            skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, src_subresource.baseArrayLayer,\n                                                 src_subresource.layerCount, i, func_name, \"srcSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstSubresource-01712\" : \"VUID-vkCmdResolveImage-dstSubresource-01712\";\n            skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, dst_subresource.baseArrayLayer,\n                                                 dst_subresource.layerCount, i, func_name, \"srcSubresource\", vuid);\n\n            // layer counts must match\n            if (src_subresource.layerCount != dst_subresource.layerCount) {\n                vuid = is_2khr ? \"VUID-VkImageResolve2KHR-layerCount-00267\" : \"VUID-VkImageResolve-layerCount-00267\";\n                skip |=\n                    LogError(cb_node->commandBuffer, vuid,\n                             \"%s: layerCount in source and destination subresource of pRegions[%u] does not match.\", func_name, i);\n            }\n            // For each region, src and dest image aspect must be color only\n            if ((src_subresource.aspectMask != VK_IMAGE_ASPECT_COLOR_BIT) ||\n                (dst_subresource.aspectMask != VK_IMAGE_ASPECT_COLOR_BIT)) {\n                vuid = is_2khr ? \"VUID-VkImageResolve2KHR-aspectMask-00266\" : \"VUID-VkImageResolve-aspectMask-00266\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: src and dest aspectMasks for pRegions[%u] must specify only VK_IMAGE_ASPECT_COLOR_BIT.\",\n                                 func_name, i);\n            }\n\n            const VkImageType src_image_type = src_image_state->createInfo.imageType;\n            const VkImageType dst_image_type = dst_image_state->createInfo.imageType;\n\n            if ((VK_IMAGE_TYPE_3D == src_image_type) || (VK_IMAGE_TYPE_3D == dst_image_type)) {\n                if ((0 != src_subresource.baseArrayLayer) || (1 != src_subresource.layerCount)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-04446\" : \"VUID-vkCmdResolveImage-srcImage-04446\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: pRegions[%u] baseArrayLayer must be 0 and layerCount must be 1 for all \"\n                                     \"subresources if the src or dst image is 3D.\",\n                                     func_name, i);\n                }\n                if ((0 != dst_subresource.baseArrayLayer) || (1 != dst_subresource.layerCount)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-04447\" : \"VUID-vkCmdResolveImage-srcImage-04447\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: pRegions[%u] baseArrayLayer must be 0 and layerCount must be 1 for all \"\n                                     \"subresources if the src or dst image is 3D.\",\n                                     func_name, i);\n                }\n            }\n\n            if (VK_IMAGE_TYPE_1D == src_image_type) {\n                if ((pRegions[i].srcOffset.y != 0) || (pRegions[i].extent.height != 1)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-00271\" : \"VUID-vkCmdResolveImage-srcImage-00271\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: srcImage (%s) is 1D but pRegions[%u] srcOffset.y (%d) is not 0 or \"\n                                     \"extent.height (%u) is not 1.\",\n                                     func_name, report_data->FormatHandle(src_image_state->image).c_str(), i,\n                                     pRegions[i].srcOffset.y, pRegions[i].extent.height);\n                }\n            }\n            if ((VK_IMAGE_TYPE_1D == src_image_type) || (VK_IMAGE_TYPE_2D == src_image_type)) {\n                if ((pRegions[i].srcOffset.z != 0) || (pRegions[i].extent.depth != 1)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-00273\" : \"VUID-vkCmdResolveImage-srcImage-00273\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: srcImage (%s) is 2D but pRegions[%u] srcOffset.z (%d) is not 0 or \"\n                                     \"extent.depth (%u) is not 1.\",\n                                     func_name, report_data->FormatHandle(src_image_state->image).c_str(), i,\n                                     pRegions[i].srcOffset.z, pRegions[i].extent.depth);\n                }\n            }\n\n            if (VK_IMAGE_TYPE_1D == dst_image_type) {\n                if ((pRegions[i].dstOffset.y != 0) || (pRegions[i].extent.height != 1)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-00276\" : \"VUID-vkCmdResolveImage-dstImage-00276\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: dstImage (%s) is 1D but pRegions[%u] dstOffset.y (%d) is not 0 or \"\n                                     \"extent.height (%u) is not 1.\",\n                                     func_name, report_data->FormatHandle(dst_image_state->image).c_str(), i,\n                                     pRegions[i].dstOffset.y, pRegions[i].extent.height);\n                }\n            }\n            if ((VK_IMAGE_TYPE_1D == dst_image_type) || (VK_IMAGE_TYPE_2D == dst_image_type)) {\n                if ((pRegions[i].dstOffset.z != 0) || (pRegions[i].extent.depth != 1)) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-00278\" : \"VUID-vkCmdResolveImage-dstImage-00278\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: dstImage (%s) is 2D but pRegions[%u] dstOffset.z (%d) is not 0 or \"\n                                     \"extent.depth (%u) is not 1.\",\n                                     func_name, report_data->FormatHandle(dst_image_state->image).c_str(), i,\n                                     pRegions[i].dstOffset.z, pRegions[i].extent.depth);\n                }\n            }\n\n            // Each srcImage dimension offset + extent limits must fall with image subresource extent\n            VkExtent3D subresource_extent = GetImageSubresourceExtent(src_image_state, &src_subresource);\n            // MipLevel bound is checked already and adding extra errors with a \"subresource extent of zero\" is confusing to\n            // developer\n            if (src_subresource.mipLevel < src_image_state->createInfo.mipLevels) {\n                uint32_t extent_check = ExceedsBounds(&(region.srcOffset), &(region.extent), &subresource_extent);\n                if ((extent_check & x_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcOffset-00269\" : \"VUID-vkCmdResolveImage-srcOffset-00269\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: srcImage (%s) pRegions[%u] x-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource width [%u].\",\n                                     func_name, report_data->FormatHandle(src_image_state->image).c_str(), i, region.srcOffset.x,\n                                     region.extent.width, subresource_extent.width);\n                }\n\n                if ((extent_check & y_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcOffset-00270\" : \"VUID-vkCmdResolveImage-srcOffset-00270\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: srcImage (%s) pRegions[%u] y-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource height [%u].\",\n                                     func_name, report_data->FormatHandle(src_image_state->image).c_str(), i, region.srcOffset.y,\n                                     region.extent.height, subresource_extent.height);\n                }\n\n                if ((extent_check & z_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(src_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcOffset-00272\" : \"VUID-vkCmdResolveImage-srcOffset-00272\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: srcImage (%s) pRegions[%u] z-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource depth [%u].\",\n                                     func_name, report_data->FormatHandle(src_image_state->image).c_str(), i, region.srcOffset.z,\n                                     region.extent.depth, subresource_extent.depth);\n                }\n            }\n\n            // Each dstImage dimension offset + extent limits must fall with image subresource extent\n            subresource_extent = GetImageSubresourceExtent(dst_image_state, &dst_subresource);\n            // MipLevel bound is checked already and adding extra errors with a \"subresource extent of zero\" is confusing to\n            // developer\n            if (dst_subresource.mipLevel < dst_image_state->createInfo.mipLevels) {\n                uint32_t extent_check = ExceedsBounds(&(region.dstOffset), &(region.extent), &subresource_extent);\n                if ((extent_check & x_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstOffset-00274\" : \"VUID-vkCmdResolveImage-dstOffset-00274\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: dstImage (%s) pRegions[%u] x-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource width [%u].\",\n                                     func_name, report_data->FormatHandle(dst_image_state->image).c_str(), i, region.srcOffset.x,\n                                     region.extent.width, subresource_extent.width);\n                }\n\n                if ((extent_check & y_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstOffset-00275\" : \"VUID-vkCmdResolveImage-dstOffset-00275\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: dstImage (%s) pRegions[%u] y-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource height [%u].\",\n                                     func_name, report_data->FormatHandle(dst_image_state->image).c_str(), i, region.srcOffset.y,\n                                     region.extent.height, subresource_extent.height);\n                }\n\n                if ((extent_check & z_bit) != 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(dst_image_state->image);\n                    vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstOffset-00277\" : \"VUID-vkCmdResolveImage-dstOffset-00277\";\n                    skip |= LogError(objlist, vuid,\n                                     \"%s: dstImage (%s) pRegions[%u] z-dimension offset [%1d] + extent [%u] \"\n                                     \"exceeds subResource depth [%u].\",\n                                     func_name, report_data->FormatHandle(dst_image_state->image).c_str(), i, region.srcOffset.z,\n                                     region.extent.depth, subresource_extent.depth);\n                }\n            }\n        }\n\n        if (src_image_state->createInfo.format != dst_image_state->createInfo.format) {\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-01386\" : \"VUID-vkCmdResolveImage-srcImage-01386\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s: srcImage format (%s) and dstImage format (%s) are not the same.\",\n                             func_name, string_VkFormat(src_image_state->createInfo.format),\n                             string_VkFormat(dst_image_state->createInfo.format));\n        }\n        if (src_image_state->createInfo.imageType != dst_image_state->createInfo.imageType) {\n            skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_MismatchedImageType,\n                               \"%s: srcImage type (%s) and dstImage type (%s) are not the same.\", func_name,\n                               string_VkImageType(src_image_state->createInfo.imageType),\n                               string_VkImageType(dst_image_state->createInfo.imageType));\n        }\n        if (src_image_state->createInfo.samples == VK_SAMPLE_COUNT_1_BIT) {\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-srcImage-00257\" : \"VUID-vkCmdResolveImage-srcImage-00257\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s: srcImage sample count is VK_SAMPLE_COUNT_1_BIT.\", func_name);\n        }\n        if (dst_image_state->createInfo.samples != VK_SAMPLE_COUNT_1_BIT) {\n            vuid = is_2khr ? \"VUID-VkResolveImageInfo2KHR-dstImage-00259\" : \"VUID-vkCmdResolveImage-dstImage-00259\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s: dstImage sample count (%s) is not VK_SAMPLE_COUNT_1_BIT.\",\n                             func_name, string_VkSampleCountFlagBits(dst_image_state->createInfo.samples));\n        }\n    } else {\n        assert(0);\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdResolveImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                                VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                                const VkImageResolve *pRegions) const {\n    return ValidateCmdResolveImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount, pRegions,\n                                   COPY_COMMAND_VERSION_1);\n}\n\nbool CoreChecks::PreCallValidateCmdResolveImage2KHR(VkCommandBuffer commandBuffer,\n                                                    const VkResolveImageInfo2KHR *pResolveImageInfo) const {\n    return ValidateCmdResolveImage(commandBuffer, pResolveImageInfo->srcImage, pResolveImageInfo->srcImageLayout,\n                                   pResolveImageInfo->dstImage, pResolveImageInfo->dstImageLayout, pResolveImageInfo->regionCount,\n                                   pResolveImageInfo->pRegions, COPY_COMMAND_VERSION_2);\n}\n\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                      VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                      const RegionType *pRegions, VkFilter filter, CopyCommandVersion version) const {\n    const auto *cb_node = GetCBState(commandBuffer);\n    const auto *src_image_state = GetImageState(srcImage);\n    const auto *dst_image_state = GetImageState(dstImage);\n\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdBlitImage2KHR()\" : \"vkCmdBlitImage()\";\n    const CMD_TYPE cmd_type = is_2khr ? CMD_BLITIMAGE : CMD_BLITIMAGE2KHR;\n\n    bool skip = false;\n    if (cb_node) {\n        skip |= ValidateCmd(cb_node, cmd_type, func_name);\n    }\n    if (cb_node && src_image_state && dst_image_state) {\n        const char *vuid;\n        const char *location;\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00233\" : \"VUID-vkCmdBlitImage-srcImage-00233\";\n        location = is_2khr ? \"vkCmdBlitImage2KHR(): pBlitImageInfo->srcImage\" : \"vkCmdBlitImage(): srcImage\";\n        skip |= ValidateImageSampleCount(src_image_state, VK_SAMPLE_COUNT_1_BIT, location, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-00234\" : \"VUID-vkCmdBlitImage-dstImage-00234\";\n        location = is_2khr ? \"vkCmdBlitImage2KHR(): pBlitImageInfo->dstImage\" : \"vkCmdBlitImage(): dstImage\";\n        skip |= ValidateImageSampleCount(dst_image_state, VK_SAMPLE_COUNT_1_BIT, location, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00220\" : \"VUID-vkCmdBlitImage-srcImage-00220\";\n        skip |= ValidateMemoryIsBoundToImage(src_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-00225\" : \"VUID-vkCmdBlitImage-dstImage-00225\";\n        skip |= ValidateMemoryIsBoundToImage(dst_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00219\" : \"VUID-vkCmdBlitImage-srcImage-00219\";\n        skip |= ValidateImageUsageFlags(src_image_state, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, true, vuid, func_name,\n                                        \"VK_IMAGE_USAGE_TRANSFER_SRC_BIT\");\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-00224\" : \"VUID-vkCmdBlitImage-dstImage-00224\";\n        skip |= ValidateImageUsageFlags(dst_image_state, VK_IMAGE_USAGE_TRANSFER_DST_BIT, true, vuid, func_name,\n                                        \"VK_IMAGE_USAGE_TRANSFER_DST_BIT\");\n        vuid = is_2khr ? \"VUID-vkCmdBlitImage2KHR-commandBuffer-cmdpool\" : \"VUID-vkCmdBlitImage-commandBuffer-cmdpool\";\n        skip |= ValidateCmdQueueFlags(cb_node, func_name, VK_QUEUE_GRAPHICS_BIT, vuid);\n        skip |= ValidateCmd(cb_node, cmd_type, func_name);\n        vuid = is_2khr ? \"VUID-vkCmdBlitImage2KHR-renderpass\" : \"VUID-vkCmdBlitImage-renderpass\";\n        skip |= InsideRenderPass(cb_node, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-01999\" : \"VUID-vkCmdBlitImage-srcImage-01999\";\n        skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_BLIT_SRC_BIT, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-02000\" : \"VUID-vkCmdBlitImage-dstImage-02000\";\n        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_BLIT_DST_BIT, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdBlitImage2KHR-commandBuffer-01834\" : \"VUID-vkCmdBlitImage-commandBuffer-01834\";\n        skip |= ValidateProtectedImage(cb_node, src_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdBlitImage2KHR-commandBuffer-01835\" : \"VUID-vkCmdBlitImage-commandBuffer-01835\";\n        skip |= ValidateProtectedImage(cb_node, dst_image_state, func_name, vuid);\n        vuid = is_2khr ? \"VUID-vkCmdBlitImage2KHR-commandBuffer-01836\" : \"VUID-vkCmdBlitImage-commandBuffer-01836\";\n        skip |= ValidateUnprotectedImage(cb_node, dst_image_state, func_name, vuid);\n\n        // Validation for VK_EXT_fragment_density_map\n        if (src_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-02545\" : \"VUID-vkCmdBlitImage-dstImage-02545\";\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: srcImage must not have been created with flags containing VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                             func_name);\n        }\n        if (dst_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-02545\" : \"VUID-vkCmdBlitImage-dstImage-02545\";\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: dstImage must not have been created with flags containing VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                             func_name);\n        }\n\n        // TODO: Need to validate image layouts, which will include layout validation for shared presentable images\n\n        VkFormat src_format = src_image_state->createInfo.format;\n        VkFormat dst_format = dst_image_state->createInfo.format;\n        VkImageType src_type = src_image_state->createInfo.imageType;\n        VkImageType dst_type = dst_image_state->createInfo.imageType;\n\n        if (VK_FILTER_LINEAR == filter) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-filter-02001\" : \"VUID-vkCmdBlitImage-filter-02001\";\n            skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT, func_name,\n                                                    vuid);\n        } else if (VK_FILTER_CUBIC_IMG == filter) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-filter-02002\" : \"VUID-vkCmdBlitImage-filter-02002\";\n            skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_CUBIC_BIT_IMG,\n                                                    func_name, vuid);\n        }\n\n        if (FormatRequiresYcbcrConversion(src_format)) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-01561\" : \"VUID-vkCmdBlitImage-srcImage-01561\";\n            skip |= LogError(device, vuid,\n                             \"%s: srcImage format (%s) must not be one of the formats requiring sampler YCBCR \"\n                             \"conversion for VK_IMAGE_ASPECT_COLOR_BIT image views\",\n                             func_name, string_VkFormat(src_format));\n        }\n\n        if (FormatRequiresYcbcrConversion(dst_format)) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-01562\" : \"VUID-vkCmdBlitImage-dstImage-01562\";\n            skip |= LogError(device, vuid,\n                             \"%s: dstImage format (%s) must not be one of the formats requiring sampler YCBCR \"\n                             \"conversion for VK_IMAGE_ASPECT_COLOR_BIT image views\",\n                             func_name, string_VkFormat(dst_format));\n        }\n\n        if ((VK_FILTER_CUBIC_IMG == filter) && (VK_IMAGE_TYPE_3D != src_type)) {\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-filter-00237\" : \"VUID-vkCmdBlitImage-filter-00237\";\n            skip |= LogError(cb_node->commandBuffer, vuid,\n                             \"%s: source image type must be VK_IMAGE_TYPE_3D when cubic filtering is specified.\", func_name);\n        }\n\n        // Validate consistency for unsigned formats\n        if (FormatIsUInt(src_format) != FormatIsUInt(dst_format)) {\n            std::stringstream ss;\n            ss << func_name << \": If one of srcImage and dstImage images has unsigned integer format, \"\n               << \"the other one must also have unsigned integer format.  \"\n               << \"Source format is \" << string_VkFormat(src_format) << \" Destination format is \" << string_VkFormat(dst_format);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00230\" : \"VUID-vkCmdBlitImage-srcImage-00230\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // Validate consistency for signed formats\n        if (FormatIsSInt(src_format) != FormatIsSInt(dst_format)) {\n            std::stringstream ss;\n            ss << func_name << \": If one of srcImage and dstImage images has signed integer format, \"\n               << \"the other one must also have signed integer format.  \"\n               << \"Source format is \" << string_VkFormat(src_format) << \" Destination format is \" << string_VkFormat(dst_format);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00229\" : \"VUID-vkCmdBlitImage-srcImage-00229\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // Validate filter for Depth/Stencil formats\n        if (FormatIsDepthOrStencil(src_format) && (filter != VK_FILTER_NEAREST)) {\n            std::stringstream ss;\n            ss << func_name << \": If the format of srcImage is a depth, stencil, or depth stencil \"\n               << \"then filter must be VK_FILTER_NEAREST.\";\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00232\" : \"VUID-vkCmdBlitImage-srcImage-00232\";\n            skip |= LogError(cb_node->commandBuffer, vuid, \"%s.\", ss.str().c_str());\n        }\n\n        // Validate aspect bits and formats for depth/stencil images\n        if (FormatIsDepthOrStencil(src_format) || FormatIsDepthOrStencil(dst_format)) {\n            if (src_format != dst_format) {\n                std::stringstream ss;\n                ss << func_name << \": If one of srcImage and dstImage images has a format of depth, stencil or depth \"\n                   << \"stencil, the other one must have exactly the same format.  \"\n                   << \"Source format is \" << string_VkFormat(src_format) << \" Destination format is \"\n                   << string_VkFormat(dst_format);\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00231\" : \"VUID-vkCmdBlitImage-srcImage-00231\";\n                skip |= LogError(cb_node->commandBuffer, vuid, \"%s.\", ss.str().c_str());\n            }\n        }  // Depth or Stencil\n\n        // Do per-region checks\n        const char *invalid_src_layout_vuid =\n            is_2khr ? ((src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-VkBlitImageInfo2KHR-srcImageLayout-01398\"\n                           : \"VUID-VkBlitImageInfo2KHR-srcImageLayout-00222\")\n                    : ((src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-vkCmdBlitImage-srcImageLayout-01398\"\n                           : \"VUID-vkCmdBlitImage-srcImageLayout-00222\");\n        const char *invalid_dst_layout_vuid =\n            is_2khr ? ((dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-VkBlitImageInfo2KHR-dstImageLayout-01399\"\n                           : \"VUID-VkBlitImageInfo2KHR-dstImageLayout-00227\")\n                    : ((dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                           ? \"VUID-vkCmdBlitImage-dstImageLayout-01399\"\n                           : \"VUID-vkCmdBlitImage-dstImageLayout-00227\");\n        for (uint32_t i = 0; i < regionCount; i++) {\n            const RegionType rgn = pRegions[i];\n            bool hit_error = false;\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImageLayout-00221\" : \"VUID-vkCmdBlitImage-srcImageLayout-00221\";\n            skip |= VerifyImageLayout(cb_node, src_image_state, rgn.srcSubresource, srcImageLayout,\n                                      VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, func_name, invalid_src_layout_vuid, vuid, &hit_error);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImageLayout-00226\" : \"VUID-vkCmdBlitImage-dstImageLayout-00226\";\n            skip |= VerifyImageLayout(cb_node, dst_image_state, rgn.dstSubresource, dstImageLayout,\n                                      VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, func_name, invalid_dst_layout_vuid, vuid, &hit_error);\n            skip |= ValidateImageSubresourceLayers(cb_node, &rgn.srcSubresource, func_name, \"srcSubresource\", i);\n            skip |= ValidateImageSubresourceLayers(cb_node, &rgn.dstSubresource, func_name, \"dstSubresource\", i);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcSubresource-01705\" : \"VUID-vkCmdBlitImage-srcSubresource-01705\";\n            skip |=\n                ValidateImageMipLevel(cb_node, src_image_state, rgn.srcSubresource.mipLevel, i, func_name, \"srcSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstSubresource-01706\" : \"VUID-vkCmdBlitImage-dstSubresource-01706\";\n            skip |=\n                ValidateImageMipLevel(cb_node, dst_image_state, rgn.dstSubresource.mipLevel, i, func_name, \"dstSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcSubresource-01707\" : \"VUID-vkCmdBlitImage-srcSubresource-01707\";\n            skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, rgn.srcSubresource.baseArrayLayer,\n                                                 rgn.srcSubresource.layerCount, i, func_name, \"srcSubresource\", vuid);\n            vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstSubresource-01708\" : \"VUID-vkCmdBlitImage-dstSubresource-01708\";\n            skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, rgn.dstSubresource.baseArrayLayer,\n                                                 rgn.dstSubresource.layerCount, i, func_name, \"dstSubresource\", vuid);\n            // Warn for zero-sized regions\n            if ((rgn.srcOffsets[0].x == rgn.srcOffsets[1].x) || (rgn.srcOffsets[0].y == rgn.srcOffsets[1].y) ||\n                (rgn.srcOffsets[0].z == rgn.srcOffsets[1].z)) {\n                std::stringstream ss;\n                ss << func_name << \": pRegions[\" << i << \"].srcOffsets specify a zero-volume area.\";\n                skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_InvalidExtents, \"%s\", ss.str().c_str());\n            }\n            if ((rgn.dstOffsets[0].x == rgn.dstOffsets[1].x) || (rgn.dstOffsets[0].y == rgn.dstOffsets[1].y) ||\n                (rgn.dstOffsets[0].z == rgn.dstOffsets[1].z)) {\n                std::stringstream ss;\n                ss << func_name << \": pRegions[\" << i << \"].dstOffsets specify a zero-volume area.\";\n                skip |= LogWarning(cb_node->commandBuffer, kVUID_Core_DrawState_InvalidExtents, \"%s\", ss.str().c_str());\n            }\n\n            // Check that src/dst layercounts match\n            if (rgn.srcSubresource.layerCount != rgn.dstSubresource.layerCount) {\n                vuid = is_2khr ? \"VUID-VkImageBlit2KHR-layerCount-00239\" : \"VUID-VkImageBlit-layerCount-00239\";\n                skip |=\n                    LogError(cb_node->commandBuffer, vuid,\n                             \"%s: layerCount in source and destination subresource of pRegions[%d] does not match.\", func_name, i);\n            }\n\n            if (rgn.srcSubresource.aspectMask != rgn.dstSubresource.aspectMask) {\n                vuid = is_2khr ? \"VUID-VkImageBlit2KHR-aspectMask-00238\" : \"VUID-VkImageBlit-aspectMask-00238\";\n                skip |=\n                    LogError(cb_node->commandBuffer, vuid, \"%s: aspectMask members for pRegion[%d] do not match.\", func_name, i);\n            }\n\n            if (!VerifyAspectsPresent(rgn.srcSubresource.aspectMask, src_format)) {\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-aspectMask-00241\" : \"VUID-vkCmdBlitImage-aspectMask-00241\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] source aspectMask (0x%x) specifies aspects not present in source \"\n                                 \"image format %s.\",\n                                 func_name, i, rgn.srcSubresource.aspectMask, string_VkFormat(src_format));\n            }\n\n            if (!VerifyAspectsPresent(rgn.dstSubresource.aspectMask, dst_format)) {\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-aspectMask-00242\" : \"VUID-vkCmdBlitImage-aspectMask-00242\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] dest aspectMask (0x%x) specifies aspects not present in dest image format %s.\",\n                                 func_name, i, rgn.dstSubresource.aspectMask, string_VkFormat(dst_format));\n            }\n\n            // Validate source image offsets\n            VkExtent3D src_extent = GetImageSubresourceExtent(src_image_state, &(rgn.srcSubresource));\n            if (VK_IMAGE_TYPE_1D == src_type) {\n                if ((0 != rgn.srcOffsets[0].y) || (1 != rgn.srcOffsets[1].y)) {\n                    vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00245\" : \"VUID-vkCmdBlitImage-srcImage-00245\";\n                    skip |= LogError(cb_node->commandBuffer, vuid,\n                                     \"%s: region [%d], source image of type VK_IMAGE_TYPE_1D with srcOffset[].y values \"\n                                     \"of (%1d, %1d). These must be (0, 1).\",\n                                     func_name, i, rgn.srcOffsets[0].y, rgn.srcOffsets[1].y);\n                }\n            }\n\n            if ((VK_IMAGE_TYPE_1D == src_type) || (VK_IMAGE_TYPE_2D == src_type)) {\n                if ((0 != rgn.srcOffsets[0].z) || (1 != rgn.srcOffsets[1].z)) {\n                    vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00247\" : \"VUID-vkCmdBlitImage-srcImage-00247\";\n                    skip |= LogError(cb_node->commandBuffer, vuid,\n                                     \"%s: region [%d], source image of type VK_IMAGE_TYPE_1D or VK_IMAGE_TYPE_2D with \"\n                                     \"srcOffset[].z values of (%1d, %1d). These must be (0, 1).\",\n                                     func_name, i, rgn.srcOffsets[0].z, rgn.srcOffsets[1].z);\n                }\n            }\n\n            bool oob = false;\n            if ((rgn.srcOffsets[0].x < 0) || (rgn.srcOffsets[0].x > static_cast<int32_t>(src_extent.width)) ||\n                (rgn.srcOffsets[1].x < 0) || (rgn.srcOffsets[1].x > static_cast<int32_t>(src_extent.width))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcOffset-00243\" : \"VUID-vkCmdBlitImage-srcOffset-00243\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] srcOffset[].x values (%1d, %1d) exceed srcSubresource width extent (%1d).\",\n                                 func_name, i, rgn.srcOffsets[0].x, rgn.srcOffsets[1].x, src_extent.width);\n            }\n            if ((rgn.srcOffsets[0].y < 0) || (rgn.srcOffsets[0].y > static_cast<int32_t>(src_extent.height)) ||\n                (rgn.srcOffsets[1].y < 0) || (rgn.srcOffsets[1].y > static_cast<int32_t>(src_extent.height))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcOffset-00244\" : \"VUID-vkCmdBlitImage-srcOffset-00244\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] srcOffset[].y values (%1d, %1d) exceed srcSubresource height extent (%1d).\",\n                                 func_name, i, rgn.srcOffsets[0].y, rgn.srcOffsets[1].y, src_extent.height);\n            }\n            if ((rgn.srcOffsets[0].z < 0) || (rgn.srcOffsets[0].z > static_cast<int32_t>(src_extent.depth)) ||\n                (rgn.srcOffsets[1].z < 0) || (rgn.srcOffsets[1].z > static_cast<int32_t>(src_extent.depth))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcOffset-00246\" : \"VUID-vkCmdBlitImage-srcOffset-00246\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] srcOffset[].z values (%1d, %1d) exceed srcSubresource depth extent (%1d).\",\n                                 func_name, i, rgn.srcOffsets[0].z, rgn.srcOffsets[1].z, src_extent.depth);\n            }\n            if (oob) {\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-pRegions-00215\" : \"VUID-vkCmdBlitImage-pRegions-00215\";\n                skip |= LogError(cb_node->commandBuffer, vuid, \"%s: region [%d] source image blit region exceeds image dimensions.\",\n                                 func_name, i);\n            }\n\n            // Validate dest image offsets\n            VkExtent3D dst_extent = GetImageSubresourceExtent(dst_image_state, &(rgn.dstSubresource));\n            if (VK_IMAGE_TYPE_1D == dst_type) {\n                if ((0 != rgn.dstOffsets[0].y) || (1 != rgn.dstOffsets[1].y)) {\n                    vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-00250\" : \"VUID-vkCmdBlitImage-dstImage-00250\";\n                    skip |= LogError(cb_node->commandBuffer, vuid,\n                                     \"%s: region [%d], dest image of type VK_IMAGE_TYPE_1D with dstOffset[].y values of \"\n                                     \"(%1d, %1d). These must be (0, 1).\",\n                                     func_name, i, rgn.dstOffsets[0].y, rgn.dstOffsets[1].y);\n                }\n            }\n\n            if ((VK_IMAGE_TYPE_1D == dst_type) || (VK_IMAGE_TYPE_2D == dst_type)) {\n                if ((0 != rgn.dstOffsets[0].z) || (1 != rgn.dstOffsets[1].z)) {\n                    vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstImage-00252\" : \"VUID-vkCmdBlitImage-dstImage-00252\";\n                    skip |= LogError(cb_node->commandBuffer, vuid,\n                                     \"%s: region [%d], dest image of type VK_IMAGE_TYPE_1D or VK_IMAGE_TYPE_2D with \"\n                                     \"dstOffset[].z values of (%1d, %1d). These must be (0, 1).\",\n                                     func_name, i, rgn.dstOffsets[0].z, rgn.dstOffsets[1].z);\n                }\n            }\n\n            oob = false;\n            if ((rgn.dstOffsets[0].x < 0) || (rgn.dstOffsets[0].x > static_cast<int32_t>(dst_extent.width)) ||\n                (rgn.dstOffsets[1].x < 0) || (rgn.dstOffsets[1].x > static_cast<int32_t>(dst_extent.width))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstOffset-00248\" : \"VUID-vkCmdBlitImage-dstOffset-00248\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] dstOffset[].x values (%1d, %1d) exceed dstSubresource width extent (%1d).\",\n                                 func_name, i, rgn.dstOffsets[0].x, rgn.dstOffsets[1].x, dst_extent.width);\n            }\n            if ((rgn.dstOffsets[0].y < 0) || (rgn.dstOffsets[0].y > static_cast<int32_t>(dst_extent.height)) ||\n                (rgn.dstOffsets[1].y < 0) || (rgn.dstOffsets[1].y > static_cast<int32_t>(dst_extent.height))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstOffset-00249\" : \"VUID-vkCmdBlitImage-dstOffset-00249\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] dstOffset[].y values (%1d, %1d) exceed dstSubresource height extent (%1d).\",\n                                 func_name, i, rgn.dstOffsets[0].y, rgn.dstOffsets[1].y, dst_extent.height);\n            }\n            if ((rgn.dstOffsets[0].z < 0) || (rgn.dstOffsets[0].z > static_cast<int32_t>(dst_extent.depth)) ||\n                (rgn.dstOffsets[1].z < 0) || (rgn.dstOffsets[1].z > static_cast<int32_t>(dst_extent.depth))) {\n                oob = true;\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-dstOffset-00251\" : \"VUID-vkCmdBlitImage-dstOffset-00251\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] dstOffset[].z values (%1d, %1d) exceed dstSubresource depth extent (%1d).\",\n                                 func_name, i, rgn.dstOffsets[0].z, rgn.dstOffsets[1].z, dst_extent.depth);\n            }\n            if (oob) {\n                vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-pRegions-00216\" : \"VUID-vkCmdBlitImage-pRegions-00216\";\n                skip |= LogError(cb_node->commandBuffer, vuid,\n                                 \"%s: region [%d] destination image blit region exceeds image dimensions.\", func_name, i);\n            }\n\n            if ((VK_IMAGE_TYPE_3D == src_type) || (VK_IMAGE_TYPE_3D == dst_type)) {\n                if ((0 != rgn.srcSubresource.baseArrayLayer) || (1 != rgn.srcSubresource.layerCount) ||\n                    (0 != rgn.dstSubresource.baseArrayLayer) || (1 != rgn.dstSubresource.layerCount)) {\n                    vuid = is_2khr ? \"VUID-VkBlitImageInfo2KHR-srcImage-00240\" : \"VUID-vkCmdBlitImage-srcImage-00240\";\n                    skip |= LogError(cb_node->commandBuffer, vuid,\n                                     \"%s: region [%d] blit to/from a 3D image type with a non-zero baseArrayLayer, or a \"\n                                     \"layerCount other than 1.\",\n                                     func_name, i);\n                }\n            }\n        }  // per-region checks\n    } else {\n        assert(0);\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                             VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                             const VkImageBlit *pRegions, VkFilter filter) const {\n    return ValidateCmdBlitImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount, pRegions, filter,\n                                COPY_COMMAND_VERSION_1);\n}\n\nbool CoreChecks::PreCallValidateCmdBlitImage2KHR(VkCommandBuffer commandBuffer, const VkBlitImageInfo2KHR *pBlitImageInfo) const {\n    return ValidateCmdBlitImage(commandBuffer, pBlitImageInfo->srcImage, pBlitImageInfo->srcImageLayout, pBlitImageInfo->dstImage,\n                                pBlitImageInfo->dstImageLayout, pBlitImageInfo->regionCount, pBlitImageInfo->pRegions,\n                                pBlitImageInfo->filter, COPY_COMMAND_VERSION_2);\n}\n\ntemplate <typename RegionType>\nvoid CoreChecks::RecordCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout, VkImage dstImage,\n                                    VkImageLayout dstImageLayout, uint32_t regionCount, const RegionType *pRegions,\n                                    VkFilter filter) {\n    auto cb_node = GetCBState(commandBuffer);\n    auto src_image_state = GetImageState(srcImage);\n    auto dst_image_state = GetImageState(dstImage);\n\n    // Make sure that all image slices are updated to correct layout\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].srcSubresource, srcImageLayout);\n        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].dstSubresource, dstImageLayout);\n    }\n}\n\nvoid CoreChecks::PreCallRecordCmdBlitImage(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                           VkImage dstImage, VkImageLayout dstImageLayout, uint32_t regionCount,\n                                           const VkImageBlit *pRegions, VkFilter filter) {\n    StateTracker::PreCallRecordCmdBlitImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount,\n                                            pRegions, filter);\n    RecordCmdBlitImage(commandBuffer, srcImage, srcImageLayout, dstImage, dstImageLayout, regionCount, pRegions, filter);\n}\n\nvoid CoreChecks::PreCallRecordCmdBlitImage2KHR(VkCommandBuffer commandBuffer, const VkBlitImageInfo2KHR *pBlitImageInfo) {\n    StateTracker::PreCallRecordCmdBlitImage2KHR(commandBuffer, pBlitImageInfo);\n    RecordCmdBlitImage(commandBuffer, pBlitImageInfo->srcImage, pBlitImageInfo->srcImageLayout, pBlitImageInfo->dstImage,\n                       pBlitImageInfo->dstImageLayout, pBlitImageInfo->regionCount, pBlitImageInfo->pRegions,\n                       pBlitImageInfo->filter);\n}\n\nGlobalImageLayoutRangeMap *GetLayoutRangeMap(GlobalImageLayoutMap *map, const IMAGE_STATE &image_state) {\n    assert(map);\n    // This approach allows for a single hash lookup or/create new\n    auto inserted = map->emplace(std::make_pair(image_state.image, nullptr));\n    if (inserted.second) {\n        assert(nullptr == inserted.first->second.get());\n        GlobalImageLayoutRangeMap *layout_map = new GlobalImageLayoutRangeMap(image_state.subresource_encoder.SubresourceCount());\n        inserted.first->second.reset(layout_map);\n        return layout_map;\n    } else {\n        assert(nullptr != inserted.first->second.get());\n        return inserted.first->second.get();\n    }\n    return nullptr;\n}\n\nconst GlobalImageLayoutRangeMap *GetLayoutRangeMap(const GlobalImageLayoutMap &map, VkImage image) {\n    auto it = map.find(image);\n    if (it != map.end()) {\n        return it->second.get();\n    }\n    return nullptr;\n}\n\n// This validates that the initial layout specified in the command buffer for the IMAGE is the same as the global IMAGE layout\nbool CoreChecks::ValidateCmdBufImageLayouts(const CMD_BUFFER_STATE *pCB, const GlobalImageLayoutMap &globalImageLayoutMap,\n                                            GlobalImageLayoutMap *overlayLayoutMap_arg) const {\n    if (disabled[image_layout_validation]) return false;\n    bool skip = false;\n    GlobalImageLayoutMap &overlayLayoutMap = *overlayLayoutMap_arg;\n    // Iterate over the layout maps for each referenced image\n    GlobalImageLayoutRangeMap empty_map(1);\n    for (const auto &layout_map_entry : pCB->image_layout_map) {\n        const auto image = layout_map_entry.first;\n        const auto *image_state = GetImageState(image);\n        if (!image_state) continue;  // Can't check layouts of a dead image\n        const auto &subres_map = layout_map_entry.second;\n        const auto &initial_layout_map = subres_map->GetInitialLayoutMap();\n        // Validate the initial_uses for each subresource referenced\n        if (initial_layout_map.empty()) continue;\n\n        auto *overlay_map = GetLayoutRangeMap(&overlayLayoutMap, *image_state);\n        const auto *global_map = GetLayoutRangeMap(globalImageLayoutMap, image);\n        if (global_map == nullptr) {\n            global_map = &empty_map;\n        }\n\n        // Note: don't know if it would matter\n        // if (global_map->empty() && overlay_map->empty()) // skip this next loop...;\n\n        auto pos = initial_layout_map.begin();\n        const auto end = initial_layout_map.end();\n        sparse_container::parallel_iterator<const ImageSubresourceLayoutMap::LayoutMap> current_layout(*overlay_map, *global_map,\n                                                                                                       pos->first.begin);\n        while (pos != end) {\n            VkImageLayout initial_layout = pos->second;\n            VkImageLayout image_layout = kInvalidLayout;\n            if (current_layout->range.empty()) break;  // When we are past the end of data in overlay and global... stop looking\n            if (current_layout->pos_A->valid) {        // pos_A denotes the overlay map in the parallel iterator\n                image_layout = current_layout->pos_A->lower_bound->second;\n            } else if (current_layout->pos_B->valid) {  // pos_B denotes the global map in the parallel iterator\n                image_layout = current_layout->pos_B->lower_bound->second;\n            }\n            const auto intersected_range = pos->first & current_layout->range;\n            if (initial_layout == VK_IMAGE_LAYOUT_UNDEFINED) {\n                // TODO: Set memory invalid which is in mem_tracker currently\n            } else if (image_layout != initial_layout) {\n                // Need to look up the inital layout *state* to get a bit more information\n                const auto *initial_layout_state = subres_map->GetSubresourceInitialLayoutState(pos->first.begin);\n                assert(initial_layout_state);  // There's no way we should have an initial layout without matching state...\n                bool matches = ImageLayoutMatches(initial_layout_state->aspect_mask, image_layout, initial_layout);\n                if (!matches) {\n                    // We can report all the errors for the intersected range directly\n                    for (auto index : sparse_container::range_view<decltype(intersected_range)>(intersected_range)) {\n                        const auto subresource = image_state->subresource_encoder.Decode(index);\n                        skip |= LogError(\n                            pCB->commandBuffer, kVUID_Core_DrawState_InvalidImageLayout,\n                            \"Submitted command buffer expects %s (subresource: aspectMask 0x%X array layer %u, mip level %u) \"\n                            \"to be in layout %s--instead, current layout is %s.\",\n                            report_data->FormatHandle(image).c_str(), subresource.aspectMask, subresource.arrayLayer,\n                            subresource.mipLevel, string_VkImageLayout(initial_layout), string_VkImageLayout(image_layout));\n                    }\n                }\n            }\n            if (pos->first.includes(intersected_range.end)) {\n                current_layout.seek(intersected_range.end);\n            } else {\n                ++pos;\n                if (pos != end) {\n                    current_layout.seek(pos->first.begin);\n                }\n            }\n        }\n\n        // Update all layout set operations (which will be a subset of the initial_layouts)\n        sparse_container::splice(overlay_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);\n    }\n\n    return skip;\n}\n\nvoid CoreChecks::UpdateCmdBufImageLayouts(CMD_BUFFER_STATE *pCB) {\n    for (const auto &layout_map_entry : pCB->image_layout_map) {\n        const auto image = layout_map_entry.first;\n        const auto &subres_map = layout_map_entry.second;\n        const auto *image_state = GetImageState(image);\n        if (!image_state) continue;  // Can't set layouts of a dead image\n        auto *global_map = GetLayoutRangeMap(&imageLayoutMap, *image_state);\n        sparse_container::splice(global_map, subres_map->GetCurrentLayoutMap(), sparse_container::value_precedence::prefer_source);\n    }\n}\n\n// ValidateLayoutVsAttachmentDescription is a general function where we can validate various state associated with the\n// VkAttachmentDescription structs that are used by the sub-passes of a renderpass. Initial check is to make sure that READ_ONLY\n// layout attachments don't have CLEAR as their loadOp.\nbool CoreChecks::ValidateLayoutVsAttachmentDescription(const debug_report_data *report_data, RenderPassCreateVersion rp_version,\n                                                       const VkImageLayout first_layout, const uint32_t attachment,\n                                                       const VkAttachmentDescription2KHR &attachment_description) const {\n    bool skip = false;\n    const bool use_rp2 = (rp_version == RENDER_PASS_VERSION_2);\n\n    // Verify that initial loadOp on READ_ONLY attachments is not CLEAR\n    // for both loadOp and stencilLoaOp rp2 has it in 1 VU while rp1 has it in 2 VU with half behind Maintenance2 extension\n    // Each is VUID is below in following order: rp2 -> rp1 with Maintenance2 -> rp1 with no extenstion\n    if (attachment_description.loadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {\n        if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||\n                        (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||\n                        (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL))) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo2-pAttachments-02522\",\n                             \"vkCreateRenderPass2(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        } else if ((use_rp2 == false) && (device_extensions.vk_khr_maintenance2) &&\n                   (first_layout == VK_IMAGE_LAYOUT_DEPTH_READ_ONLY_STENCIL_ATTACHMENT_OPTIMAL)) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo-pAttachments-01566\",\n                             \"vkCreateRenderPass(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        } else if ((use_rp2 == false) && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||\n                                          (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL))) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo-pAttachments-00836\",\n                             \"vkCreateRenderPass(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        }\n    }\n\n    // Same as above for loadOp, but for stencilLoadOp\n    if (attachment_description.stencilLoadOp == VK_ATTACHMENT_LOAD_OP_CLEAR) {\n        if (use_rp2 && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||\n                        (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL) ||\n                        (first_layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL))) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo2-pAttachments-02523\",\n                             \"vkCreateRenderPass2(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        } else if ((use_rp2 == false) && (device_extensions.vk_khr_maintenance2) &&\n                   (first_layout == VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_STENCIL_READ_ONLY_OPTIMAL)) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo-pAttachments-01567\",\n                             \"vkCreateRenderPass(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        } else if ((use_rp2 == false) && ((first_layout == VK_IMAGE_LAYOUT_DEPTH_STENCIL_READ_ONLY_OPTIMAL) ||\n                                          (first_layout == VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL))) {\n            skip |= LogError(device, \"VUID-VkRenderPassCreateInfo-pAttachments-02511\",\n                             \"vkCreateRenderPass(): Cannot clear attachment %d with invalid first layout %s.\", attachment,\n                             string_VkImageLayout(first_layout));\n        }\n    }\n\n    return skip;\n}\n\n// Helper function to validate correct usage bits set for buffers or images. Verify that (actual & desired) flags != 0 or, if strict\n// is true, verify that (actual & desired) flags == desired\ntemplate <typename T1>\nbool CoreChecks::ValidateUsageFlags(VkFlags actual, VkFlags desired, VkBool32 strict, const T1 object,\n                                    const VulkanTypedHandle &typed_handle, const char *msgCode, char const *func_name,\n                                    char const *usage_str) const {\n    bool correct_usage = false;\n    bool skip = false;\n    const char *type_str = object_string[typed_handle.type];\n    if (strict) {\n        correct_usage = ((actual & desired) == desired);\n    } else {\n        correct_usage = ((actual & desired) != 0);\n    }\n\n    if (!correct_usage) {\n        // All callers should have a valid VUID\n        assert(msgCode != kVUIDUndefined);\n        skip =\n            LogError(object, msgCode, \"Invalid usage flag for %s used by %s. In this case, %s should have %s set during creation.\",\n                     report_data->FormatHandle(typed_handle).c_str(), func_name, type_str, usage_str);\n    }\n    return skip;\n}\n\n// Helper function to validate usage flags for buffers. For given buffer_state send actual vs. desired usage off to helper above\n// where an error will be flagged if usage is not correct\nbool CoreChecks::ValidateImageUsageFlags(IMAGE_STATE const *image_state, VkFlags desired, bool strict, const char *msgCode,\n                                         char const *func_name, char const *usage_string) const {\n    return ValidateUsageFlags(image_state->createInfo.usage, desired, strict, image_state->image,\n                              VulkanTypedHandle(image_state->image, kVulkanObjectTypeImage), msgCode, func_name, usage_string);\n}\n\nbool CoreChecks::ValidateImageFormatFeatureFlags(IMAGE_STATE const *image_state, VkFormatFeatureFlags desired,\n                                                 char const *func_name, const char *vuid) const {\n    bool skip = false;\n    const VkFormatFeatureFlags image_format_features = image_state->format_features;\n    if ((image_format_features & desired) != desired) {\n        // Same error, but more details if it was an AHB external format\n        if (image_state->has_ahb_format == true) {\n            skip |= LogError(image_state->image, vuid,\n                             \"In %s, VkFormatFeatureFlags (0x%08X) does not support required feature %s for the external format \"\n                             \"found in VkAndroidHardwareBufferFormatPropertiesANDROID::formatFeatures used by %s.\",\n                             func_name, image_format_features, string_VkFormatFeatureFlags(desired).c_str(),\n                             report_data->FormatHandle(image_state->image).c_str());\n        } else {\n            skip |= LogError(image_state->image, vuid,\n                             \"In %s, VkFormatFeatureFlags (0x%08X) does not support required feature %s for format %u used by %s \"\n                             \"with tiling %s.\",\n                             func_name, image_format_features, string_VkFormatFeatureFlags(desired).c_str(),\n                             image_state->createInfo.format, report_data->FormatHandle(image_state->image).c_str(),\n                             string_VkImageTiling(image_state->createInfo.tiling));\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateImageSubresourceLayers(const CMD_BUFFER_STATE *cb_node, const VkImageSubresourceLayers *subresource_layers,\n                                                char const *func_name, char const *member, uint32_t i) const {\n    bool skip = false;\n    // layerCount must not be zero\n    if (subresource_layers->layerCount == 0) {\n        skip |= LogError(cb_node->commandBuffer, \"VUID-VkImageSubresourceLayers-layerCount-01700\",\n                         \"In %s, pRegions[%u].%s.layerCount must not be zero.\", func_name, i, member);\n    }\n    // aspectMask must not contain VK_IMAGE_ASPECT_METADATA_BIT\n    if (subresource_layers->aspectMask & VK_IMAGE_ASPECT_METADATA_BIT) {\n        skip |= LogError(cb_node->commandBuffer, \"VUID-VkImageSubresourceLayers-aspectMask-00168\",\n                         \"In %s, pRegions[%u].%s.aspectMask has VK_IMAGE_ASPECT_METADATA_BIT set.\", func_name, i, member);\n    }\n    // if aspectMask contains COLOR, it must not contain either DEPTH or STENCIL\n    if ((subresource_layers->aspectMask & VK_IMAGE_ASPECT_COLOR_BIT) &&\n        (subresource_layers->aspectMask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT))) {\n        skip |= LogError(cb_node->commandBuffer, \"VUID-VkImageSubresourceLayers-aspectMask-00167\",\n                         \"In %s, pRegions[%u].%s.aspectMask has VK_IMAGE_ASPECT_COLOR_BIT and either VK_IMAGE_ASPECT_DEPTH_BIT or \"\n                         \"VK_IMAGE_ASPECT_STENCIL_BIT set.\",\n                         func_name, i, member);\n    }\n    return skip;\n}\n\n// Helper function to validate usage flags for buffers. For given buffer_state send actual vs. desired usage off to helper above\n// where an error will be flagged if usage is not correct\nbool CoreChecks::ValidateBufferUsageFlags(BUFFER_STATE const *buffer_state, VkFlags desired, bool strict, const char *msgCode,\n                                          char const *func_name, char const *usage_string) const {\n    return ValidateUsageFlags(buffer_state->createInfo.usage, desired, strict, buffer_state->buffer,\n                              VulkanTypedHandle(buffer_state->buffer, kVulkanObjectTypeBuffer), msgCode, func_name, usage_string);\n}\n\nbool CoreChecks::ValidateBufferViewRange(const BUFFER_STATE *buffer_state, const VkBufferViewCreateInfo *pCreateInfo,\n                                         const VkPhysicalDeviceLimits *device_limits) const {\n    bool skip = false;\n\n    const VkDeviceSize &range = pCreateInfo->range;\n    if (range != VK_WHOLE_SIZE) {\n        // Range must be greater than 0\n        if (range <= 0) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-range-00928\",\n                             \"vkCreateBufferView(): If VkBufferViewCreateInfo range (%\" PRIuLEAST64\n                             \") does not equal VK_WHOLE_SIZE, range must be greater than 0.\",\n                             range);\n        }\n        // Range must be a multiple of the element size of format\n        const uint32_t format_size = FormatElementSize(pCreateInfo->format);\n        if (SafeModulo(range, format_size) != 0) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-range-00929\",\n                             \"vkCreateBufferView(): If VkBufferViewCreateInfo range (%\" PRIuLEAST64\n                             \") does not equal VK_WHOLE_SIZE, range must be a multiple of the element size of the format \"\n                             \"(%\" PRIu32 \").\",\n                             range, format_size);\n        }\n        // Range divided by the element size of format must be less than or equal to VkPhysicalDeviceLimits::maxTexelBufferElements\n        if (SafeDivision(range, format_size) > device_limits->maxTexelBufferElements) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-range-00930\",\n                             \"vkCreateBufferView(): If VkBufferViewCreateInfo range (%\" PRIuLEAST64\n                             \") does not equal VK_WHOLE_SIZE, range divided by the element size of the format (%\" PRIu32\n                             \") must be less than or equal to VkPhysicalDeviceLimits::maxTexelBufferElements (%\" PRIuLEAST32 \").\",\n                             range, format_size, device_limits->maxTexelBufferElements);\n        }\n        // The sum of range and offset must be less than or equal to the size of buffer\n        if (range + pCreateInfo->offset > buffer_state->createInfo.size) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-offset-00931\",\n                             \"vkCreateBufferView(): If VkBufferViewCreateInfo range (%\" PRIuLEAST64\n                             \") does not equal VK_WHOLE_SIZE, the sum of offset (%\" PRIuLEAST64\n                             \") and range must be less than or equal to the size of the buffer (%\" PRIuLEAST64 \").\",\n                             range, pCreateInfo->offset, buffer_state->createInfo.size);\n        }\n    } else {\n        const uint32_t format_size = FormatElementSize(pCreateInfo->format);\n\n        // Size of buffer - offset, divided by the element size of format must be less than or equal to\n        // VkPhysicalDeviceLimits::maxTexelBufferElements\n        if (SafeDivision(buffer_state->createInfo.size - pCreateInfo->offset, format_size) >\n            device_limits->maxTexelBufferElements) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-range-04059\",\n                             \"vkCreateBufferView(): If VkBufferViewCreateInfo range (%\" PRIuLEAST64\n                             \") equals VK_WHOLE_SIZE, the buffer's size (%\" PRIuLEAST64 \") minus the offset (%\" PRIuLEAST64\n                             \"), divided by the element size of the format (%\" PRIu32\n                             \") must be less than or equal to VkPhysicalDeviceLimits::maxTexelBufferElements (%\" PRIuLEAST32 \").\",\n                             range, buffer_state->createInfo.size, pCreateInfo->offset, format_size,\n                             device_limits->maxTexelBufferElements);\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateBufferViewBuffer(const BUFFER_STATE *buffer_state, const VkBufferViewCreateInfo *pCreateInfo) const {\n    bool skip = false;\n    const VkFormatProperties format_properties = GetPDFormatProperties(pCreateInfo->format);\n    if ((buffer_state->createInfo.usage & VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT) &&\n        !(format_properties.bufferFeatures & VK_FORMAT_FEATURE_UNIFORM_TEXEL_BUFFER_BIT)) {\n        skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-buffer-00933\",\n                         \"vkCreateBufferView(): If buffer was created with `usage` containing \"\n                         \"VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT, format must \"\n                         \"be supported for uniform texel buffers\");\n    }\n    if ((buffer_state->createInfo.usage & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT) &&\n        !(format_properties.bufferFeatures & VK_FORMAT_FEATURE_STORAGE_TEXEL_BUFFER_BIT)) {\n        skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-buffer-00934\",\n                         \"vkCreateBufferView(): If buffer was created with `usage` containing \"\n                         \"VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, format must \"\n                         \"be supported for storage texel buffers\");\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCreateBuffer(VkDevice device, const VkBufferCreateInfo *pCreateInfo,\n                                             const VkAllocationCallbacks *pAllocator, VkBuffer *pBuffer) const {\n    bool skip = false;\n\n    // TODO: Add check for \"VUID-vkCreateBuffer-flags-00911\"        (sparse address space accounting)\n\n    auto chained_devaddr_struct = lvl_find_in_chain<VkBufferDeviceAddressCreateInfoEXT>(pCreateInfo->pNext);\n    if (chained_devaddr_struct) {\n        if (!(pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT) &&\n            chained_devaddr_struct->deviceAddress != 0) {\n            skip |= LogError(device, \"VUID-VkBufferCreateInfo-deviceAddress-02604\",\n                             \"vkCreateBuffer(): Non-zero VkBufferDeviceAddressCreateInfoEXT::deviceAddress \"\n                             \"requires VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT.\");\n        }\n    }\n\n    auto chained_opaqueaddr_struct = lvl_find_in_chain<VkBufferOpaqueCaptureAddressCreateInfoKHR>(pCreateInfo->pNext);\n    if (chained_opaqueaddr_struct) {\n        if (!(pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR) &&\n            chained_opaqueaddr_struct->opaqueCaptureAddress != 0) {\n            skip |= LogError(device, \"VUID-VkBufferCreateInfo-opaqueCaptureAddress-03337\",\n                             \"vkCreateBuffer(): Non-zero VkBufferOpaqueCaptureAddressCreateInfoKHR::opaqueCaptureAddress\"\n                             \"requires VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR.\");\n        }\n    }\n\n    if ((pCreateInfo->flags & VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_KHR) &&\n        !enabled_features.core12.bufferDeviceAddressCaptureReplay &&\n        !enabled_features.buffer_device_address_ext.bufferDeviceAddressCaptureReplay) {\n        skip |= LogError(\n            device, \"VUID-VkBufferCreateInfo-flags-03338\",\n            \"vkCreateBuffer(): the bufferDeviceAddressCaptureReplay device feature is disabled: Buffers cannot be created with \"\n            \"the VK_BUFFER_CREATE_DEVICE_ADDRESS_CAPTURE_REPLAY_BIT_EXT set.\");\n    }\n\n    if (pCreateInfo->sharingMode == VK_SHARING_MODE_CONCURRENT && pCreateInfo->pQueueFamilyIndices) {\n        skip |= ValidatePhysicalDeviceQueueFamilies(pCreateInfo->queueFamilyIndexCount, pCreateInfo->pQueueFamilyIndices,\n                                                    \"vkCreateBuffer\", \"pCreateInfo->pQueueFamilyIndices\",\n                                                    \"VUID-VkBufferCreateInfo-sharingMode-01419\");\n    }\n\n    if ((pCreateInfo->flags & VK_BUFFER_CREATE_PROTECTED_BIT) != 0) {\n        if (enabled_features.core11.protectedMemory == VK_FALSE) {\n            skip |= LogError(device, \"VUID-VkBufferCreateInfo-flags-01887\",\n                             \"vkCreateBuffer(): the protectedMemory device feature is disabled: Buffers cannot be created with the \"\n                             \"VK_BUFFER_CREATE_PROTECTED_BIT set.\");\n        }\n        const VkBufferCreateFlags invalid_flags =\n            VK_BUFFER_CREATE_SPARSE_BINDING_BIT | VK_BUFFER_CREATE_SPARSE_RESIDENCY_BIT | VK_BUFFER_CREATE_SPARSE_ALIASED_BIT;\n        if ((pCreateInfo->flags & invalid_flags) != 0) {\n            skip |= LogError(device, \"VUID-VkBufferCreateInfo-None-01888\",\n                             \"vkCreateBuffer(): VK_BUFFER_CREATE_PROTECTED_BIT is set so no sparse create flags can be used at \"\n                             \"same time (VK_BUFFER_CREATE_SPARSE_BINDING_BIT | VK_BUFFER_CREATE_SPARSE_RESIDENCY_BIT | \"\n                             \"VK_BUFFER_CREATE_SPARSE_ALIASED_BIT).\");\n        }\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCreateBufferView(VkDevice device, const VkBufferViewCreateInfo *pCreateInfo,\n                                                 const VkAllocationCallbacks *pAllocator, VkBufferView *pView) const {\n    bool skip = false;\n    const BUFFER_STATE *buffer_state = GetBufferState(pCreateInfo->buffer);\n    // If this isn't a sparse buffer, it needs to have memory backing it at CreateBufferView time\n    if (buffer_state) {\n        skip |= ValidateMemoryIsBoundToBuffer(buffer_state, \"vkCreateBufferView()\", \"VUID-VkBufferViewCreateInfo-buffer-00935\");\n        // In order to create a valid buffer view, the buffer must have been created with at least one of the following flags:\n        // UNIFORM_TEXEL_BUFFER_BIT or STORAGE_TEXEL_BUFFER_BIT\n        skip |= ValidateBufferUsageFlags(buffer_state,\n                                         VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT | VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, false,\n                                         \"VUID-VkBufferViewCreateInfo-buffer-00932\", \"vkCreateBufferView()\",\n                                         \"VK_BUFFER_USAGE_[STORAGE|UNIFORM]_TEXEL_BUFFER_BIT\");\n\n        // Buffer view offset must be less than the size of buffer\n        if (pCreateInfo->offset >= buffer_state->createInfo.size) {\n            skip |= LogError(buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-offset-00925\",\n                             \"vkCreateBufferView(): VkBufferViewCreateInfo offset (%\" PRIuLEAST64\n                             \") must be less than the size of the buffer (%\" PRIuLEAST64 \").\",\n                             pCreateInfo->offset, buffer_state->createInfo.size);\n        }\n\n        const VkPhysicalDeviceLimits *device_limits = &phys_dev_props.limits;\n        // Buffer view offset must be a multiple of VkPhysicalDeviceLimits::minTexelBufferOffsetAlignment\n        if ((pCreateInfo->offset % device_limits->minTexelBufferOffsetAlignment) != 0 &&\n            !enabled_features.texel_buffer_alignment_features.texelBufferAlignment) {\n            const char *vuid = device_extensions.vk_ext_texel_buffer_alignment ? \"VUID-VkBufferViewCreateInfo-offset-02749\"\n                                                                               : \"VUID-VkBufferViewCreateInfo-offset-00926\";\n            skip |= LogError(buffer_state->buffer, vuid,\n                             \"vkCreateBufferView(): VkBufferViewCreateInfo offset (%\" PRIuLEAST64\n                             \") must be a multiple of VkPhysicalDeviceLimits::minTexelBufferOffsetAlignment (%\" PRIuLEAST64 \").\",\n                             pCreateInfo->offset, device_limits->minTexelBufferOffsetAlignment);\n        }\n\n        if (enabled_features.texel_buffer_alignment_features.texelBufferAlignment) {\n            VkDeviceSize elementSize = FormatElementSize(pCreateInfo->format);\n            if ((elementSize % 3) == 0) {\n                elementSize /= 3;\n            }\n            if (buffer_state->createInfo.usage & VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT) {\n                VkDeviceSize alignmentRequirement =\n                    phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetAlignmentBytes;\n                if (phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetSingleTexelAlignment) {\n                    alignmentRequirement = std::min(alignmentRequirement, elementSize);\n                }\n                if (SafeModulo(pCreateInfo->offset, alignmentRequirement) != 0) {\n                    skip |= LogError(\n                        buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-buffer-02750\",\n                        \"vkCreateBufferView(): If buffer was created with usage containing \"\n                        \"VK_BUFFER_USAGE_STORAGE_TEXEL_BUFFER_BIT, \"\n                        \"VkBufferViewCreateInfo offset (%\" PRIuLEAST64\n                        \") must be a multiple of the lesser of \"\n                        \"VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::storageTexelBufferOffsetAlignmentBytes (%\" PRIuLEAST64\n                        \") or, if VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::storageTexelBufferOffsetSingleTexelAlignment \"\n                        \"(%\" PRId32\n                        \") is VK_TRUE, the size of a texel of the requested format. \"\n                        \"If the size of a texel is a multiple of three bytes, then the size of a \"\n                        \"single component of format is used instead\",\n                        pCreateInfo->offset, phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetAlignmentBytes,\n                        phys_dev_ext_props.texel_buffer_alignment_props.storageTexelBufferOffsetSingleTexelAlignment);\n                }\n            }\n            if (buffer_state->createInfo.usage & VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT) {\n                VkDeviceSize alignmentRequirement =\n                    phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetAlignmentBytes;\n                if (phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetSingleTexelAlignment) {\n                    alignmentRequirement = std::min(alignmentRequirement, elementSize);\n                }\n                if (SafeModulo(pCreateInfo->offset, alignmentRequirement) != 0) {\n                    skip |= LogError(\n                        buffer_state->buffer, \"VUID-VkBufferViewCreateInfo-buffer-02751\",\n                        \"vkCreateBufferView(): If buffer was created with usage containing \"\n                        \"VK_BUFFER_USAGE_UNIFORM_TEXEL_BUFFER_BIT, \"\n                        \"VkBufferViewCreateInfo offset (%\" PRIuLEAST64\n                        \") must be a multiple of the lesser of \"\n                        \"VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::uniformTexelBufferOffsetAlignmentBytes (%\" PRIuLEAST64\n                        \") or, if VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT::uniformTexelBufferOffsetSingleTexelAlignment \"\n                        \"(%\" PRId32\n                        \") is VK_TRUE, the size of a texel of the requested format. \"\n                        \"If the size of a texel is a multiple of three bytes, then the size of a \"\n                        \"single component of format is used instead\",\n                        pCreateInfo->offset, phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetAlignmentBytes,\n                        phys_dev_ext_props.texel_buffer_alignment_props.uniformTexelBufferOffsetSingleTexelAlignment);\n                }\n            }\n        }\n\n        skip |= ValidateBufferViewRange(buffer_state, pCreateInfo, device_limits);\n\n        skip |= ValidateBufferViewBuffer(buffer_state, pCreateInfo);\n    }\n    return skip;\n}\n\n// For the given format verify that the aspect masks make sense\nbool CoreChecks::ValidateImageAspectMask(VkImage image, VkFormat format, VkImageAspectFlags aspect_mask, const char *func_name,\n                                         const char *vuid) const {\n    bool skip = false;\n    const IMAGE_STATE *image_state = GetImageState(image);\n    // checks color format and (single-plane or non-disjoint)\n    // if ycbcr extension is not supported then single-plane and non-disjoint are always both true\n    if ((FormatIsColor(format)) && ((FormatIsMultiplane(format) == false) || (image_state->disjoint == false))) {\n        if ((aspect_mask & VK_IMAGE_ASPECT_COLOR_BIT) != VK_IMAGE_ASPECT_COLOR_BIT) {\n            skip |= LogError(image, vuid, \"%s: Color image formats must have the VK_IMAGE_ASPECT_COLOR_BIT set.\", func_name);\n        } else if ((aspect_mask & VK_IMAGE_ASPECT_COLOR_BIT) != aspect_mask) {\n            skip |= LogError(image, vuid, \"%s: Color image formats must have ONLY the VK_IMAGE_ASPECT_COLOR_BIT set.\", func_name);\n        }\n    } else if (FormatIsDepthAndStencil(format)) {\n        if ((aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) == 0) {\n            skip |= LogError(image, vuid,\n                             \"%s: Depth/stencil image formats must have at least one of VK_IMAGE_ASPECT_DEPTH_BIT and \"\n                             \"VK_IMAGE_ASPECT_STENCIL_BIT set.\",\n                             func_name);\n        } else if ((aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) != aspect_mask) {\n            skip |= LogError(image, vuid,\n                             \"%s: Combination depth/stencil image formats can have only the VK_IMAGE_ASPECT_DEPTH_BIT and \"\n                             \"VK_IMAGE_ASPECT_STENCIL_BIT set.\",\n                             func_name);\n        }\n    } else if (FormatIsDepthOnly(format)) {\n        if ((aspect_mask & VK_IMAGE_ASPECT_DEPTH_BIT) != VK_IMAGE_ASPECT_DEPTH_BIT) {\n            skip |= LogError(image, vuid, \"%s: Depth-only image formats must have the VK_IMAGE_ASPECT_DEPTH_BIT set.\", func_name);\n        } else if ((aspect_mask & VK_IMAGE_ASPECT_DEPTH_BIT) != aspect_mask) {\n            skip |=\n                LogError(image, vuid, \"%s: Depth-only image formats can have only the VK_IMAGE_ASPECT_DEPTH_BIT set.\", func_name);\n        }\n    } else if (FormatIsStencilOnly(format)) {\n        if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) != VK_IMAGE_ASPECT_STENCIL_BIT) {\n            skip |=\n                LogError(image, vuid, \"%s: Stencil-only image formats must have the VK_IMAGE_ASPECT_STENCIL_BIT set.\", func_name);\n        } else if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) != aspect_mask) {\n            skip |= LogError(image, vuid, \"%s: Stencil-only image formats can have only the VK_IMAGE_ASPECT_STENCIL_BIT set.\",\n                             func_name);\n        }\n    } else if (FormatIsMultiplane(format)) {\n        VkImageAspectFlags valid_flags = VK_IMAGE_ASPECT_COLOR_BIT | VK_IMAGE_ASPECT_PLANE_0_BIT | VK_IMAGE_ASPECT_PLANE_1_BIT;\n        if (3 == FormatPlaneCount(format)) {\n            valid_flags = valid_flags | VK_IMAGE_ASPECT_PLANE_2_BIT;\n        }\n        if ((aspect_mask & valid_flags) != aspect_mask) {\n            skip |=\n                LogError(image, vuid,\n                         \"%s: Multi-plane image formats may have only VK_IMAGE_ASPECT_COLOR_BIT or VK_IMAGE_ASPECT_PLANE_n_BITs \"\n                         \"set, where n = [0, 1, 2].\",\n                         func_name);\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::ValidateImageSubresourceRange(const uint32_t image_mip_count, const uint32_t image_layer_count,\n                                               const VkImageSubresourceRange &subresourceRange, const char *cmd_name,\n                                               const char *param_name, const char *image_layer_count_var_name, const VkImage image,\n                                               SubresourceRangeErrorCodes errorCodes) const {\n    bool skip = false;\n\n    // Validate mip levels\n    if (subresourceRange.baseMipLevel >= image_mip_count) {\n        skip |= LogError(image, errorCodes.base_mip_err,\n                         \"%s: %s.baseMipLevel (= %\" PRIu32\n                         \") is greater or equal to the mip level count of the image (i.e. greater or equal to %\" PRIu32 \").\",\n                         cmd_name, param_name, subresourceRange.baseMipLevel, image_mip_count);\n    }\n\n    if (subresourceRange.levelCount != VK_REMAINING_MIP_LEVELS) {\n        if (subresourceRange.levelCount == 0) {\n            skip |=\n                LogError(image, \"VUID-VkImageSubresourceRange-levelCount-01720\", \"%s: %s.levelCount is 0.\", cmd_name, param_name);\n        } else {\n            const uint64_t necessary_mip_count = uint64_t{subresourceRange.baseMipLevel} + uint64_t{subresourceRange.levelCount};\n\n            if (necessary_mip_count > image_mip_count) {\n                skip |= LogError(image, errorCodes.mip_count_err,\n                                 \"%s: %s.baseMipLevel + .levelCount (= %\" PRIu32 \" + %\" PRIu32 \" = %\" PRIu64\n                                 \") is greater than the mip level count of the image (i.e. greater than %\" PRIu32 \").\",\n                                 cmd_name, param_name, subresourceRange.baseMipLevel, subresourceRange.levelCount,\n                                 necessary_mip_count, image_mip_count);\n            }\n        }\n    }\n\n    // Validate array layers\n    if (subresourceRange.baseArrayLayer >= image_layer_count) {\n        skip |= LogError(image, errorCodes.base_layer_err,\n                         \"%s: %s.baseArrayLayer (= %\" PRIu32\n                         \") is greater or equal to the %s of the image when it was created (i.e. greater or equal to %\" PRIu32 \").\",\n                         cmd_name, param_name, subresourceRange.baseArrayLayer, image_layer_count_var_name, image_layer_count);\n    }\n\n    if (subresourceRange.layerCount != VK_REMAINING_ARRAY_LAYERS) {\n        if (subresourceRange.layerCount == 0) {\n            skip |=\n                LogError(image, \"VUID-VkImageSubresourceRange-layerCount-01721\", \"%s: %s.layerCount is 0.\", cmd_name, param_name);\n        } else {\n            const uint64_t necessary_layer_count =\n                uint64_t{subresourceRange.baseArrayLayer} + uint64_t{subresourceRange.layerCount};\n\n            if (necessary_layer_count > image_layer_count) {\n                skip |= LogError(image, errorCodes.layer_count_err,\n                                 \"%s: %s.baseArrayLayer + .layerCount (= %\" PRIu32 \" + %\" PRIu32 \" = %\" PRIu64\n                                 \") is greater than the %s of the image when it was created (i.e. greater than %\" PRIu32 \").\",\n                                 cmd_name, param_name, subresourceRange.baseArrayLayer, subresourceRange.layerCount,\n                                 necessary_layer_count, image_layer_count_var_name, image_layer_count);\n            }\n        }\n    }\n\n    return skip;\n}\n\nbool CoreChecks::ValidateCreateImageViewSubresourceRange(const IMAGE_STATE *image_state, bool is_imageview_2d_type,\n                                                         const VkImageSubresourceRange &subresourceRange) const {\n    bool is_khr_maintenance1 = IsExtEnabled(device_extensions.vk_khr_maintenance1);\n    bool is_image_slicable = image_state->createInfo.imageType == VK_IMAGE_TYPE_3D &&\n                             (image_state->createInfo.flags & VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT_KHR);\n    bool is_3D_to_2D_map = is_khr_maintenance1 && is_image_slicable && is_imageview_2d_type;\n\n    const auto image_layer_count = is_3D_to_2D_map ? image_state->createInfo.extent.depth : image_state->createInfo.arrayLayers;\n    const auto image_layer_count_var_name = is_3D_to_2D_map ? \"extent.depth\" : \"arrayLayers\";\n\n    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};\n    subresourceRangeErrorCodes.base_mip_err = \"VUID-VkImageViewCreateInfo-subresourceRange-01478\";\n    subresourceRangeErrorCodes.mip_count_err = \"VUID-VkImageViewCreateInfo-subresourceRange-01718\";\n    subresourceRangeErrorCodes.base_layer_err = is_khr_maintenance1 ? (is_3D_to_2D_map ? \"VUID-VkImageViewCreateInfo-image-02724\"\n                                                                                       : \"VUID-VkImageViewCreateInfo-image-01482\")\n                                                                    : \"VUID-VkImageViewCreateInfo-subresourceRange-01480\";\n    subresourceRangeErrorCodes.layer_count_err = is_khr_maintenance1\n                                                     ? (is_3D_to_2D_map ? \"VUID-VkImageViewCreateInfo-subresourceRange-02725\"\n                                                                        : \"VUID-VkImageViewCreateInfo-subresourceRange-01483\")\n                                                     : \"VUID-VkImageViewCreateInfo-subresourceRange-01719\";\n\n    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_layer_count, subresourceRange,\n                                         \"vkCreateImageView\", \"pCreateInfo->subresourceRange\", image_layer_count_var_name,\n                                         image_state->image, subresourceRangeErrorCodes);\n}\n\nbool CoreChecks::ValidateCmdClearColorSubresourceRange(const IMAGE_STATE *image_state,\n                                                       const VkImageSubresourceRange &subresourceRange,\n                                                       const char *param_name) const {\n    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};\n    subresourceRangeErrorCodes.base_mip_err = \"VUID-vkCmdClearColorImage-baseMipLevel-01470\";\n    subresourceRangeErrorCodes.mip_count_err = \"VUID-vkCmdClearColorImage-pRanges-01692\";\n    subresourceRangeErrorCodes.base_layer_err = \"VUID-vkCmdClearColorImage-baseArrayLayer-01472\";\n    subresourceRangeErrorCodes.layer_count_err = \"VUID-vkCmdClearColorImage-pRanges-01693\";\n\n    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,\n                                         \"vkCmdClearColorImage\", param_name, \"arrayLayers\", image_state->image,\n                                         subresourceRangeErrorCodes);\n}\n\nbool CoreChecks::ValidateCmdClearDepthSubresourceRange(const IMAGE_STATE *image_state,\n                                                       const VkImageSubresourceRange &subresourceRange,\n                                                       const char *param_name) const {\n    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};\n    subresourceRangeErrorCodes.base_mip_err = \"VUID-vkCmdClearDepthStencilImage-baseMipLevel-01474\";\n    subresourceRangeErrorCodes.mip_count_err = \"VUID-vkCmdClearDepthStencilImage-pRanges-01694\";\n    subresourceRangeErrorCodes.base_layer_err = \"VUID-vkCmdClearDepthStencilImage-baseArrayLayer-01476\";\n    subresourceRangeErrorCodes.layer_count_err = \"VUID-vkCmdClearDepthStencilImage-pRanges-01695\";\n\n    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,\n                                         \"vkCmdClearDepthStencilImage\", param_name, \"arrayLayers\", image_state->image,\n                                         subresourceRangeErrorCodes);\n}\n\nbool CoreChecks::ValidateImageBarrierSubresourceRange(const IMAGE_STATE *image_state,\n                                                      const VkImageSubresourceRange &subresourceRange, const char *cmd_name,\n                                                      const char *param_name) const {\n    SubresourceRangeErrorCodes subresourceRangeErrorCodes = {};\n    subresourceRangeErrorCodes.base_mip_err = \"VUID-VkImageMemoryBarrier-subresourceRange-01486\";\n    subresourceRangeErrorCodes.mip_count_err = \"VUID-VkImageMemoryBarrier-subresourceRange-01724\";\n    subresourceRangeErrorCodes.base_layer_err = \"VUID-VkImageMemoryBarrier-subresourceRange-01488\";\n    subresourceRangeErrorCodes.layer_count_err = \"VUID-VkImageMemoryBarrier-subresourceRange-01725\";\n\n    return ValidateImageSubresourceRange(image_state->createInfo.mipLevels, image_state->createInfo.arrayLayers, subresourceRange,\n                                         cmd_name, param_name, \"arrayLayers\", image_state->image, subresourceRangeErrorCodes);\n}\n\nbool CoreChecks::ValidateImageViewFormatFeatures(const IMAGE_STATE *image_state, const VkFormat view_format,\n                                                 const VkImageUsageFlags image_usage) const {\n    // Pass in image_usage here instead of extracting it from image_state in case there's a chained VkImageViewUsageCreateInfo\n    bool skip = false;\n\n    VkFormatFeatureFlags tiling_features = VK_FORMAT_FEATURE_FLAG_BITS_MAX_ENUM;\n    const VkImageTiling image_tiling = image_state->createInfo.tiling;\n\n    if (image_state->has_ahb_format == true) {\n        // AHB image view and image share same feature sets\n        tiling_features = image_state->format_features;\n    } else if (image_tiling == VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT) {\n        // Parameter validation should catch if this is used without VK_EXT_image_drm_format_modifier\n        assert(device_extensions.vk_ext_image_drm_format_modifier);\n        VkImageDrmFormatModifierPropertiesEXT drm_format_properties = {VK_STRUCTURE_TYPE_IMAGE_DRM_FORMAT_MODIFIER_PROPERTIES_EXT,\n                                                                       nullptr};\n        DispatchGetImageDrmFormatModifierPropertiesEXT(device, image_state->image, &drm_format_properties);\n\n        VkFormatProperties2 format_properties_2 = {VK_STRUCTURE_TYPE_FORMAT_PROPERTIES_2, nullptr};\n        VkDrmFormatModifierPropertiesListEXT drm_properties_list = {VK_STRUCTURE_TYPE_DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT,\n                                                                    nullptr};\n        format_properties_2.pNext = (void *)&drm_properties_list;\n        DispatchGetPhysicalDeviceFormatProperties2(physical_device, view_format, &format_properties_2);\n\n        for (uint32_t i = 0; i < drm_properties_list.drmFormatModifierCount; i++) {\n            if ((drm_properties_list.pDrmFormatModifierProperties[i].drmFormatModifier & drm_format_properties.drmFormatModifier) !=\n                0) {\n                tiling_features |= drm_properties_list.pDrmFormatModifierProperties[i].drmFormatModifierTilingFeatures;\n            }\n        }\n    } else {\n        VkFormatProperties format_properties = GetPDFormatProperties(view_format);\n        tiling_features = (image_tiling == VK_IMAGE_TILING_LINEAR) ? format_properties.linearTilingFeatures\n                                                                   : format_properties.optimalTilingFeatures;\n    }\n\n    if (tiling_features == 0) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-None-02273\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s has no supported format features on this \"\n                         \"physical device.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    } else if ((image_usage & VK_IMAGE_USAGE_SAMPLED_BIT) && !(tiling_features & VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT)) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-usage-02274\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s does not support usage that includes \"\n                         \"VK_IMAGE_USAGE_SAMPLED_BIT.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    } else if ((image_usage & VK_IMAGE_USAGE_STORAGE_BIT) && !(tiling_features & VK_FORMAT_FEATURE_STORAGE_IMAGE_BIT)) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-usage-02275\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s does not support usage that includes \"\n                         \"VK_IMAGE_USAGE_STORAGE_BIT.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    } else if ((image_usage & VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT) && !(tiling_features & VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT)) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-usage-02276\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s does not support usage that includes \"\n                         \"VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    } else if ((image_usage & VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT) &&\n               !(tiling_features & VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT)) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-usage-02277\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s does not support usage that includes \"\n                         \"VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    } else if ((image_usage & VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT) &&\n               !(tiling_features & (VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT | VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT))) {\n        skip |= LogError(image_state->image, \"VUID-VkImageViewCreateInfo-usage-02652\",\n                         \"vkCreateImageView(): pCreateInfo->format %s with tiling %s does not support usage that includes \"\n                         \"VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT or VK_FORMAT_FEATURE_DEPTH_STENCIL_ATTACHMENT_BIT.\",\n                         string_VkFormat(view_format), string_VkImageTiling(image_tiling));\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCreateImageView(VkDevice device, const VkImageViewCreateInfo *pCreateInfo,\n                                                const VkAllocationCallbacks *pAllocator, VkImageView *pView) const {\n    bool skip = false;\n    const IMAGE_STATE *image_state = GetImageState(pCreateInfo->image);\n    if (image_state) {\n        skip |=\n            ValidateImageUsageFlags(image_state,\n                                    VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_STORAGE_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT |\n                                        VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT | VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT |\n                                        VK_IMAGE_USAGE_TRANSIENT_ATTACHMENT_BIT | VK_IMAGE_USAGE_SHADING_RATE_IMAGE_BIT_NV |\n                                        VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT,\n                                    false, \"VUID-VkImageViewCreateInfo-image-04441\", \"vkCreateImageView()\",\n                                    \"VK_IMAGE_USAGE_[SAMPLED|STORAGE|COLOR_ATTACHMENT|DEPTH_STENCIL_ATTACHMENT|INPUT_ATTACHMENT|\"\n                                    \"TRANSIENT_ATTACHMENT|SHADING_RATE_IMAGE|FRAGMENT_DENSITY_MAP]_BIT\");\n        // If this isn't a sparse image, it needs to have memory backing it at CreateImageView time\n        skip |= ValidateMemoryIsBoundToImage(image_state, \"vkCreateImageView()\", \"VUID-VkImageViewCreateInfo-image-01020\");\n        // Checks imported from image layer\n        skip |= ValidateCreateImageViewSubresourceRange(\n            image_state, pCreateInfo->viewType == VK_IMAGE_VIEW_TYPE_2D || pCreateInfo->viewType == VK_IMAGE_VIEW_TYPE_2D_ARRAY,\n            pCreateInfo->subresourceRange);\n\n        VkImageCreateFlags image_flags = image_state->createInfo.flags;\n        VkFormat image_format = image_state->createInfo.format;\n        VkImageUsageFlags image_usage = image_state->createInfo.usage;\n        VkFormat view_format = pCreateInfo->format;\n        VkImageAspectFlags aspect_mask = pCreateInfo->subresourceRange.aspectMask;\n        VkImageType image_type = image_state->createInfo.imageType;\n        VkImageViewType view_type = pCreateInfo->viewType;\n\n        // If there's a chained VkImageViewUsageCreateInfo struct, modify image_usage to match\n        auto chained_ivuci_struct = lvl_find_in_chain<VkImageViewUsageCreateInfoKHR>(pCreateInfo->pNext);\n        if (chained_ivuci_struct) {\n            if (device_extensions.vk_khr_maintenance2) {\n                if (!device_extensions.vk_ext_separate_stencil_usage) {\n                    if ((image_usage | chained_ivuci_struct->usage) != image_usage) {\n                        skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-pNext-02661\",\n                                         \"vkCreateImageView(): pNext chain includes VkImageViewUsageCreateInfo, usage must not \"\n                                         \"include any bits that were not set in VkImageCreateInfo::usage used to create image\");\n                    }\n                } else {\n                    const auto image_stencil_struct =\n                        lvl_find_in_chain<VkImageStencilUsageCreateInfoEXT>(image_state->createInfo.pNext);\n                    if (image_stencil_struct == nullptr) {\n                        if ((image_usage | chained_ivuci_struct->usage) != image_usage) {\n                            skip |= LogError(\n                                pCreateInfo->image, \"VUID-VkImageViewCreateInfo-pNext-02662\",\n                                \"vkCreateImageView(): pNext chain includes VkImageViewUsageCreateInfo and image was not created \"\n                                \"with a VkImageStencilUsageCreateInfo in pNext of vkImageCreateInfo, usage must not include \"\n                                \"any bits that were not set in VkImageCreateInfo::usage used to create image\");\n                        }\n                    } else {\n                        if ((aspect_mask & VK_IMAGE_ASPECT_STENCIL_BIT) == VK_IMAGE_ASPECT_STENCIL_BIT &&\n                            (image_stencil_struct->stencilUsage | chained_ivuci_struct->usage) !=\n                                image_stencil_struct->stencilUsage) {\n                            skip |= LogError(\n                                pCreateInfo->image, \"VUID-VkImageViewCreateInfo-pNext-02663\",\n                                \"vkCreateImageView(): pNext chain includes VkImageViewUsageCreateInfo, image was created with a \"\n                                \"VkImageStencilUsageCreateInfo in pNext of vkImageCreateInfo, and subResourceRange.aspectMask \"\n                                \"includes VK_IMAGE_ASPECT_STENCIL_BIT, VkImageViewUsageCreateInfo::usage must not include any \"\n                                \"bits that were not set in VkImageStencilUsageCreateInfo::stencilUsage used to create image\");\n                        }\n                        if ((aspect_mask & ~VK_IMAGE_ASPECT_STENCIL_BIT) != 0 &&\n                            (image_usage | chained_ivuci_struct->usage) != image_usage) {\n                            skip |= LogError(\n                                pCreateInfo->image, \"VUID-VkImageViewCreateInfo-pNext-02664\",\n                                \"vkCreateImageView(): pNext chain includes VkImageViewUsageCreateInfo, image was created with a \"\n                                \"VkImageStencilUsageCreateInfo in pNext of vkImageCreateInfo, and subResourceRange.aspectMask \"\n                                \"includes bits other than VK_IMAGE_ASPECT_STENCIL_BIT, VkImageViewUsageCreateInfo::usage must not \"\n                                \"include any bits that were not set in VkImageCreateInfo::usage used to create image\");\n                        }\n                    }\n                }\n            }\n\n            image_usage = chained_ivuci_struct->usage;\n        }\n\n        // Validate VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT state, if view/image formats differ\n        if ((image_flags & VK_IMAGE_CREATE_MUTABLE_FORMAT_BIT) && (image_format != view_format)) {\n            if (FormatIsMultiplane(image_format)) {\n                VkFormat compat_format = FindMultiplaneCompatibleFormat(image_format, aspect_mask);\n                if (view_format != compat_format) {\n                    // View format must match the multiplane compatible format\n                    std::stringstream ss;\n                    ss << \"vkCreateImageView(): ImageView format \" << string_VkFormat(view_format)\n                       << \" is not compatible with plane \" << GetPlaneIndex(aspect_mask) << \" of underlying image format \"\n                       << string_VkFormat(image_format) << \", must be \" << string_VkFormat(compat_format) << \".\";\n                    skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-01586\", \"%s\", ss.str().c_str());\n                }\n            } else {\n                if (!(image_flags & VK_IMAGE_CREATE_BLOCK_TEXEL_VIEW_COMPATIBLE_BIT_KHR)) {\n                    // Format MUST be compatible (in the same format compatibility class) as the format the image was created with\n                    if (FormatCompatibilityClass(image_format) != FormatCompatibilityClass(view_format)) {\n                        const char *error_vuid;\n                        if ((!device_extensions.vk_khr_maintenance2) && (!device_extensions.vk_khr_sampler_ycbcr_conversion)) {\n                            error_vuid = \"VUID-VkImageViewCreateInfo-image-01018\";\n                        } else if ((device_extensions.vk_khr_maintenance2) &&\n                                   (!device_extensions.vk_khr_sampler_ycbcr_conversion)) {\n                            error_vuid = \"VUID-VkImageViewCreateInfo-image-01759\";\n                        } else if ((!device_extensions.vk_khr_maintenance2) &&\n                                   (device_extensions.vk_khr_sampler_ycbcr_conversion)) {\n                            error_vuid = \"VUID-VkImageViewCreateInfo-image-01760\";\n                        } else {\n                            // both enabled\n                            error_vuid = \"VUID-VkImageViewCreateInfo-image-01761\";\n                        }\n                        std::stringstream ss;\n                        ss << \"vkCreateImageView(): ImageView format \" << string_VkFormat(view_format)\n                           << \" is not in the same format compatibility class as \"\n                           << report_data->FormatHandle(pCreateInfo->image).c_str() << \"  format \" << string_VkFormat(image_format)\n                           << \".  Images created with the VK_IMAGE_CREATE_MUTABLE_FORMAT BIT \"\n                           << \"can support ImageViews with differing formats but they must be in the same compatibility class.\";\n                        skip |= LogError(pCreateInfo->image, error_vuid, \"%s\", ss.str().c_str());\n                    }\n                }\n            }\n        } else {\n            // Format MUST be IDENTICAL to the format the image was created with\n            // Unless it is a multi-planar color bit aspect\n            if ((image_format != view_format) &&\n                ((FormatIsMultiplane(image_format) == false) || (aspect_mask != VK_IMAGE_ASPECT_COLOR_BIT))) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion) ? \"VUID-VkImageViewCreateInfo-image-01762\"\n                                                                                       : \"VUID-VkImageViewCreateInfo-image-01019\";\n                std::stringstream ss;\n                ss << \"vkCreateImageView() format \" << string_VkFormat(view_format) << \" differs from \"\n                   << report_data->FormatHandle(pCreateInfo->image).c_str() << \" format \" << string_VkFormat(image_format)\n                   << \".  Formats MUST be IDENTICAL unless VK_IMAGE_CREATE_MUTABLE_FORMAT BIT was set on image creation.\";\n                skip |= LogError(pCreateInfo->image, vuid, \"%s\", ss.str().c_str());\n            }\n        }\n\n        // Validate correct image aspect bits for desired formats and format consistency\n        skip |= ValidateImageAspectMask(image_state->image, image_format, aspect_mask, \"vkCreateImageView()\");\n\n        switch (image_type) {\n            case VK_IMAGE_TYPE_1D:\n                if (view_type != VK_IMAGE_VIEW_TYPE_1D && view_type != VK_IMAGE_VIEW_TYPE_1D_ARRAY) {\n                    skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-subResourceRange-01021\",\n                                     \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                     string_VkImageViewType(view_type), string_VkImageType(image_type));\n                }\n                break;\n            case VK_IMAGE_TYPE_2D:\n                if (view_type != VK_IMAGE_VIEW_TYPE_2D && view_type != VK_IMAGE_VIEW_TYPE_2D_ARRAY) {\n                    if ((view_type == VK_IMAGE_VIEW_TYPE_CUBE || view_type == VK_IMAGE_VIEW_TYPE_CUBE_ARRAY) &&\n                        !(image_flags & VK_IMAGE_CREATE_CUBE_COMPATIBLE_BIT)) {\n                        skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-01003\",\n                                         \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                         string_VkImageViewType(view_type), string_VkImageType(image_type));\n                    } else if (view_type != VK_IMAGE_VIEW_TYPE_CUBE && view_type != VK_IMAGE_VIEW_TYPE_CUBE_ARRAY) {\n                        skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-subResourceRange-01021\",\n                                         \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                         string_VkImageViewType(view_type), string_VkImageType(image_type));\n                    }\n                }\n                break;\n            case VK_IMAGE_TYPE_3D:\n                if (device_extensions.vk_khr_maintenance1) {\n                    if (view_type != VK_IMAGE_VIEW_TYPE_3D) {\n                        if ((view_type == VK_IMAGE_VIEW_TYPE_2D || view_type == VK_IMAGE_VIEW_TYPE_2D_ARRAY)) {\n                            if (!(image_flags & VK_IMAGE_CREATE_2D_ARRAY_COMPATIBLE_BIT_KHR)) {\n                                skip |=\n                                    LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-01005\",\n                                             \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                             string_VkImageViewType(view_type), string_VkImageType(image_type));\n                            } else if ((image_flags & (VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT |\n                                                       VK_IMAGE_CREATE_SPARSE_ALIASED_BIT))) {\n                                skip |= LogError(\n                                    pCreateInfo->image, \"VUID-VkImageViewCreateInfo-subResourceRange-01021\",\n                                    \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s \"\n                                    \"when the VK_IMAGE_CREATE_SPARSE_BINDING_BIT, VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT, or \"\n                                    \"VK_IMAGE_CREATE_SPARSE_ALIASED_BIT flags are enabled.\",\n                                    string_VkImageViewType(view_type), string_VkImageType(image_type));\n                            }\n                        } else {\n                            skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-subResourceRange-01021\",\n                                             \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                             string_VkImageViewType(view_type), string_VkImageType(image_type));\n                        }\n                    }\n                } else {\n                    if (view_type != VK_IMAGE_VIEW_TYPE_3D) {\n                        skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-subResourceRange-01021\",\n                                         \"vkCreateImageView(): pCreateInfo->viewType %s is not compatible with image type %s.\",\n                                         string_VkImageViewType(view_type), string_VkImageType(image_type));\n                    }\n                }\n                break;\n            default:\n                break;\n        }\n\n        // External format checks needed when VK_ANDROID_external_memory_android_hardware_buffer enabled\n        if (device_extensions.vk_android_external_memory_android_hardware_buffer) {\n            skip |= ValidateCreateImageViewANDROID(pCreateInfo);\n        }\n\n        skip |= ValidateImageViewFormatFeatures(image_state, view_format, image_usage);\n\n        if (image_usage & VK_IMAGE_USAGE_SHADING_RATE_IMAGE_BIT_NV) {\n            if (view_type != VK_IMAGE_VIEW_TYPE_2D && view_type != VK_IMAGE_VIEW_TYPE_2D_ARRAY) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-02086\",\n                                 \"vkCreateImageView() If image was created with usage containing \"\n                                 \"VK_IMAGE_USAGE_SHADING_RATE_IMAGE_BIT_NV, viewType must be \"\n                                 \"VK_IMAGE_VIEW_TYPE_2D or VK_IMAGE_VIEW_TYPE_2D_ARRAY.\");\n            }\n            if (view_format != VK_FORMAT_R8_UINT) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-02087\",\n                                 \"vkCreateImageView() If image was created with usage containing \"\n                                 \"VK_IMAGE_USAGE_SHADING_RATE_IMAGE_BIT_NV, format must be VK_FORMAT_R8_UINT.\");\n            }\n        }\n\n        if (pCreateInfo->subresourceRange.layerCount == VK_REMAINING_ARRAY_LAYERS) {\n            if (pCreateInfo->viewType == VK_IMAGE_VIEW_TYPE_CUBE &&\n                image_state->createInfo.arrayLayers - pCreateInfo->subresourceRange.baseArrayLayer != 6) {\n                skip |= LogError(device, \"VUID-VkImageViewCreateInfo-viewType-02962\",\n                                 \"vkCreateImageView(): subresourceRange.layerCount VK_REMAINING_ARRAY_LAYERS=(%d) must be 6\",\n                                 image_state->createInfo.arrayLayers - pCreateInfo->subresourceRange.baseArrayLayer);\n            }\n            if (pCreateInfo->viewType == VK_IMAGE_VIEW_TYPE_CUBE_ARRAY &&\n                ((image_state->createInfo.arrayLayers - pCreateInfo->subresourceRange.baseArrayLayer) % 6) != 0) {\n                skip |= LogError(\n                    device, \"VUID-VkImageViewCreateInfo-viewType-02963\",\n                    \"vkCreateImageView(): subresourceRange.layerCount VK_REMAINING_ARRAY_LAYERS=(%d) must be a multiple of 6\",\n                    image_state->createInfo.arrayLayers - pCreateInfo->subresourceRange.baseArrayLayer);\n            }\n        }\n\n        if (image_usage & VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT) {\n            if (pCreateInfo->subresourceRange.levelCount != 1) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-02571\",\n                                 \"vkCreateImageView(): If image was created with usage containing \"\n                                 \"VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT, subresourceRange.levelCount (%d) must: be 1\",\n                                 pCreateInfo->subresourceRange.levelCount);\n            }\n        }\n        if (pCreateInfo->flags & VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DYNAMIC_BIT_EXT) {\n            if (!enabled_features.fragment_density_map_features.fragmentDensityMapDynamic) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-flags-02572\",\n                                 \"vkCreateImageView(): If the fragmentDensityMapDynamic feature is not enabled, \"\n                                 \"flags must not contain VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DYNAMIC_BIT_EXT\");\n            }\n        } else {\n            if (image_usage & VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT) {\n                if (image_flags & (VK_IMAGE_CREATE_PROTECTED_BIT | VK_IMAGE_CREATE_SPARSE_BINDING_BIT |\n                                   VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT | VK_IMAGE_CREATE_SPARSE_ALIASED_BIT)) {\n                    skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-flags-04116\",\n                                     \"vkCreateImageView(): If image was created with usage containing \"\n                                     \"VK_IMAGE_USAGE_FRAGMENT_DENSITY_MAP_BIT_EXT flags must not contain any of \"\n                                     \"VK_IMAGE_CREATE_PROTECTED_BIT, VK_IMAGE_CREATE_SPARSE_BINDING_BIT, \"\n                                     \"VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT, or VK_IMAGE_CREATE_SPARSE_ALIASED_BIT\");\n                }\n            }\n        }\n\n        if (pCreateInfo->flags & VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DEFERRED_BIT_EXT) {\n            if (!enabled_features.fragment_density_map2_features.fragmentDensityMapDeferred) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-flags-03567\",\n                                 \"vkCreateImageView(): If the fragmentDensityMapDeferred feature is not enabled, \"\n                                 \"flags must not contain VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DEFERRED_BIT_EXT\");\n            }\n            if (pCreateInfo->flags & VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DYNAMIC_BIT_EXT) {\n                skip |=\n                    LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-flags-03568\",\n                             \"vkCreateImageView(): If flags contains VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DEFERRED_BIT_EXT, \"\n                             \"flags must not contain VK_IMAGE_VIEW_CREATE_FRAGMENT_DENSITY_MAP_DYNAMIC_BIT_EXT\");\n            }\n        }\n        if (device_extensions.vk_ext_fragment_density_map_2) {\n            if ((image_flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) && (image_usage & VK_IMAGE_USAGE_SAMPLED_BIT) &&\n                (pCreateInfo->subresourceRange.layerCount >\n                 phys_dev_ext_props.fragment_density_map2_props.maxSubsampledArrayLayers)) {\n                skip |= LogError(pCreateInfo->image, \"VUID-VkImageViewCreateInfo-image-03569\",\n                                 \"vkCreateImageView(): If image was created with flags containing \"\n                                 \"VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT and usage containing VK_IMAGE_USAGE_SAMPLED_BIT \"\n                                 \"subresourceRange.layerCount (%d) must: be less than or equal to maxSubsampledArrayLayers (%d)\",\n                                 pCreateInfo->subresourceRange.layerCount,\n                                 phys_dev_ext_props.fragment_density_map2_props.maxSubsampledArrayLayers);\n            }\n        }\n\n        auto astc_decode_mode = lvl_find_in_chain<VkImageViewASTCDecodeModeEXT>(pCreateInfo->pNext);\n        if ((device_extensions.vk_ext_astc_decode_mode) && (astc_decode_mode != nullptr)) {\n            if ((enabled_features.astc_decode_features.decodeModeSharedExponent == VK_FALSE) &&\n                (astc_decode_mode->decodeMode == VK_FORMAT_E5B9G9R9_UFLOAT_PACK32)) {\n                skip |= LogError(device, \"VUID-VkImageViewASTCDecodeModeEXT-decodeMode-02231\",\n                                 \"vkCreateImageView(): decodeModeSharedExponent is not enabled but \"\n                                 \"VkImageViewASTCDecodeModeEXT::decodeMode is VK_FORMAT_E5B9G9R9_UFLOAT_PACK32.\");\n            }\n        }\n    }\n    return skip;\n}\n\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCmdCopyBufferBounds(const BUFFER_STATE *src_buffer_state, const BUFFER_STATE *dst_buffer_state,\n                                             uint32_t regionCount, const RegionType *pRegions, CopyCommandVersion version) const {\n    bool skip = false;\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdCopyBuffer2KHR()\" : \"vkCmdCopyBuffer()\";\n    const char *vuid;\n\n    VkDeviceSize src_buffer_size = src_buffer_state->createInfo.size;\n    VkDeviceSize dst_buffer_size = dst_buffer_state->createInfo.size;\n    VkDeviceSize src_min = UINT64_MAX;\n    VkDeviceSize src_max = 0;\n    VkDeviceSize dst_min = UINT64_MAX;\n    VkDeviceSize dst_max = 0;\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        src_min = std::min(src_min, pRegions[i].srcOffset);\n        src_max = std::max(src_max, (pRegions[i].srcOffset + pRegions[i].size));\n        dst_min = std::min(dst_min, pRegions[i].dstOffset);\n        dst_max = std::max(dst_max, (pRegions[i].dstOffset + pRegions[i].size));\n\n        // The srcOffset member of each element of pRegions must be less than the size of srcBuffer\n        if (pRegions[i].srcOffset >= src_buffer_size) {\n            vuid = is_2khr ? \"VUID-VkCopyBufferInfo2KHR-srcOffset-00113\" : \"VUID-vkCmdCopyBuffer-srcOffset-00113\";\n            skip |= LogError(src_buffer_state->buffer, vuid,\n                             \"%s: pRegions[%d].srcOffset (%\" PRIuLEAST64 \") is greater than pRegions[%d].size (%\" PRIuLEAST64 \").\",\n                             func_name, i, pRegions[i].srcOffset, i, pRegions[i].size);\n        }\n\n        // The dstOffset member of each element of pRegions must be less than the size of dstBuffer\n        if (pRegions[i].dstOffset >= dst_buffer_size) {\n            vuid = is_2khr ? \"VUID-VkCopyBufferInfo2KHR-dstOffset-00114\" : \"VUID-vkCmdCopyBuffer-dstOffset-00114\";\n            skip |= LogError(dst_buffer_state->buffer, vuid,\n                             \"%s: pRegions[%d].dstOffset (%\" PRIuLEAST64 \") is greater than pRegions[%d].size (%\" PRIuLEAST64 \").\",\n                             func_name, i, pRegions[i].dstOffset, i, pRegions[i].size);\n        }\n\n        // The size member of each element of pRegions must be less than or equal to the size of srcBuffer minus srcOffset\n        if (pRegions[i].size > (src_buffer_size - pRegions[i].srcOffset)) {\n            vuid = is_2khr ? \"VUID-VkCopyBufferInfo2KHR-size-00115\" : \"VUID-vkCmdCopyBuffer-size-00115\";\n            skip |= LogError(src_buffer_state->buffer, vuid,\n                             \"%s: pRegions[%d].size (%\" PRIuLEAST64 \") is greater than the source buffer size (%\" PRIuLEAST64\n                             \") minus pRegions[%d].srcOffset (%\" PRIuLEAST64 \").\",\n                             func_name, i, pRegions[i].size, src_buffer_size, i, pRegions[i].srcOffset);\n        }\n\n        // The size member of each element of pRegions must be less than or equal to the size of dstBuffer minus dstOffset\n        if (pRegions[i].size > (dst_buffer_size - pRegions[i].dstOffset)) {\n            vuid = is_2khr ? \"VUID-VkCopyBufferInfo2KHR-size-00116\" : \"VUID-vkCmdCopyBuffer-size-00116\";\n            skip |= LogError(dst_buffer_state->buffer, vuid,\n                             \"%s: pRegions[%d].size (%\" PRIuLEAST64 \") is greater than the destination buffer size (%\" PRIuLEAST64\n                             \") minus pRegions[%d].dstOffset (%\" PRIuLEAST64 \").\",\n                             func_name, i, pRegions[i].size, dst_buffer_size, i, pRegions[i].dstOffset);\n        }\n    }\n\n    // The union of the source regions, and the union of the destination regions, must not overlap in memory\n    if (src_buffer_state->buffer == dst_buffer_state->buffer) {\n        if (((src_min > dst_min) && (src_min < dst_max)) || ((src_max > dst_min) && (src_max < dst_max))) {\n            vuid = is_2khr ? \"VUID-VkCopyBufferInfo2KHR-pRegions-00117\" : \"VUID-vkCmdCopyBuffer-pRegions-00117\";\n            skip |= LogError(src_buffer_state->buffer, vuid, \"%s: Detected overlap between source and dest regions in memory.\",\n                             func_name);\n        }\n    }\n\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdCopyBuffer(VkCommandBuffer commandBuffer, VkBuffer srcBuffer, VkBuffer dstBuffer,\n                                              uint32_t regionCount, const VkBufferCopy *pRegions) const {\n    const auto cb_node = GetCBState(commandBuffer);\n    const auto src_buffer_state = GetBufferState(srcBuffer);\n    const auto dst_buffer_state = GetBufferState(dstBuffer);\n\n    bool skip = false;\n    skip |= ValidateMemoryIsBoundToBuffer(src_buffer_state, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-srcBuffer-00119\");\n    skip |= ValidateMemoryIsBoundToBuffer(dst_buffer_state, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-dstBuffer-00121\");\n    // Validate that SRC & DST buffers have correct usage flags set\n    skip |=\n        ValidateBufferUsageFlags(src_buffer_state, VK_BUFFER_USAGE_TRANSFER_SRC_BIT, true, \"VUID-vkCmdCopyBuffer-srcBuffer-00118\",\n                                 \"vkCmdCopyBuffer()\", \"VK_BUFFER_USAGE_TRANSFER_SRC_BIT\");\n    skip |=\n        ValidateBufferUsageFlags(dst_buffer_state, VK_BUFFER_USAGE_TRANSFER_DST_BIT, true, \"VUID-vkCmdCopyBuffer-dstBuffer-00120\",\n                                 \"vkCmdCopyBuffer()\", \"VK_BUFFER_USAGE_TRANSFER_DST_BIT\");\n    skip |=\n        ValidateCmdQueueFlags(cb_node, \"vkCmdCopyBuffer()\", VK_QUEUE_TRANSFER_BIT | VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT,\n                              \"VUID-vkCmdCopyBuffer-commandBuffer-cmdpool\");\n    skip |= ValidateCmd(cb_node, CMD_COPYBUFFER, \"vkCmdCopyBuffer()\");\n    skip |= InsideRenderPass(cb_node, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-renderpass\");\n    skip |= ValidateCmdCopyBufferBounds(src_buffer_state, dst_buffer_state, regionCount, pRegions, COPY_COMMAND_VERSION_1);\n    skip |= ValidateProtectedBuffer(cb_node, src_buffer_state, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-commandBuffer-01822\");\n    skip |= ValidateProtectedBuffer(cb_node, dst_buffer_state, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-commandBuffer-01823\");\n    skip |= ValidateUnprotectedBuffer(cb_node, dst_buffer_state, \"vkCmdCopyBuffer()\", \"VUID-vkCmdCopyBuffer-commandBuffer-01824\");\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdCopyBuffer2KHR(VkCommandBuffer commandBuffer,\n                                                  const VkCopyBufferInfo2KHR *pCopyBufferInfos) const {\n    const auto cb_node = GetCBState(commandBuffer);\n    const auto src_buffer_state = GetBufferState(pCopyBufferInfos->srcBuffer);\n    const auto dst_buffer_state = GetBufferState(pCopyBufferInfos->dstBuffer);\n\n    bool skip = false;\n    skip |= ValidateMemoryIsBoundToBuffer(src_buffer_state, \"vkCmdCopyBuffer2KHR()\", \"VUID-VkCopyBufferInfo2KHR-srcBuffer-00119\");\n    skip |= ValidateMemoryIsBoundToBuffer(dst_buffer_state, \"vkCmdCopyBuffer2KHR()\", \"VUID-VkCopyBufferInfo2KHR-dstBuffer-00121\");\n    // Validate that SRC & DST buffers have correct usage flags set\n    skip |= ValidateBufferUsageFlags(src_buffer_state, VK_BUFFER_USAGE_TRANSFER_SRC_BIT, true,\n                                     \"VUID-VkCopyBufferInfo2KHR-srcBuffer-00118\", \"vkCmdCopyBuffer2KHR()\",\n                                     \"VK_BUFFER_USAGE_TRANSFER_SRC_BIT\");\n    skip |= ValidateBufferUsageFlags(dst_buffer_state, VK_BUFFER_USAGE_TRANSFER_DST_BIT, true,\n                                     \"VUID-VkCopyBufferInfo2KHR-dstBuffer-00120\", \"vkCmdCopyBuffer2KHR()\",\n                                     \"VK_BUFFER_USAGE_TRANSFER_DST_BIT\");\n    skip |= ValidateCmdQueueFlags(cb_node, \"vkCmdCopyBuffer2KHR()\",\n                                  VK_QUEUE_TRANSFER_BIT | VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT,\n                                  \"VUID-vkCmdCopyBuffer2KHR-commandBuffer-cmdpool\");\n    skip |= ValidateCmd(cb_node, CMD_COPYBUFFER2KHR, \"vkCmdCopyBuffer2KHR()\");\n    skip |= InsideRenderPass(cb_node, \"vkCmdCopyBuffer2KHR()\", \"VUID-vkCmdCopyBuffer2KHR-renderpass\");\n    skip |= ValidateCmdCopyBufferBounds(src_buffer_state, dst_buffer_state, pCopyBufferInfos->regionCount,\n                                        pCopyBufferInfos->pRegions, COPY_COMMAND_VERSION_2);\n    skip |=\n        ValidateProtectedBuffer(cb_node, src_buffer_state, \"vkCmdCopyBuffer2KHR()\", \"VUID-vkCmdCopyBuffer2KHR-commandBuffer-01822\");\n    skip |=\n        ValidateProtectedBuffer(cb_node, dst_buffer_state, \"vkCmdCopyBuffer2KHR()\", \"VUID-vkCmdCopyBuffer2KHR-commandBuffer-01823\");\n    skip |= ValidateUnprotectedBuffer(cb_node, dst_buffer_state, \"vkCmdCopyBuffer2KHR()\",\n                                      \"VUID-vkCmdCopyBuffer2KHR-commandBuffer-01824\");\n    return skip;\n}\n\nbool CoreChecks::ValidateIdleBuffer(VkBuffer buffer) const {\n    bool skip = false;\n    auto buffer_state = GetBufferState(buffer);\n    if (buffer_state) {\n        if (buffer_state->in_use.load()) {\n            skip |= LogError(buffer, \"VUID-vkDestroyBuffer-buffer-00922\", \"Cannot free %s that is in use by a command buffer.\",\n                             report_data->FormatHandle(buffer).c_str());\n        }\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateDestroyImageView(VkDevice device, VkImageView imageView,\n                                                 const VkAllocationCallbacks *pAllocator) const {\n    const IMAGE_VIEW_STATE *image_view_state = GetImageViewState(imageView);\n    const VulkanTypedHandle obj_struct(imageView, kVulkanObjectTypeImageView);\n\n    bool skip = false;\n    if (image_view_state) {\n        skip |=\n            ValidateObjectNotInUse(image_view_state, obj_struct, \"vkDestroyImageView\", \"VUID-vkDestroyImageView-imageView-01026\");\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateDestroyBuffer(VkDevice device, VkBuffer buffer, const VkAllocationCallbacks *pAllocator) const {\n    auto buffer_state = GetBufferState(buffer);\n\n    bool skip = false;\n    if (buffer_state) {\n        skip |= ValidateIdleBuffer(buffer);\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateDestroyBufferView(VkDevice device, VkBufferView bufferView,\n                                                  const VkAllocationCallbacks *pAllocator) const {\n    auto buffer_view_state = GetBufferViewState(bufferView);\n    const VulkanTypedHandle obj_struct(bufferView, kVulkanObjectTypeBufferView);\n    bool skip = false;\n    if (buffer_view_state) {\n        skip |= ValidateObjectNotInUse(buffer_view_state, obj_struct, \"vkDestroyBufferView\",\n                                       \"VUID-vkDestroyBufferView-bufferView-00936\");\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdFillBuffer(VkCommandBuffer commandBuffer, VkBuffer dstBuffer, VkDeviceSize dstOffset,\n                                              VkDeviceSize size, uint32_t data) const {\n    auto cb_node = GetCBState(commandBuffer);\n    auto buffer_state = GetBufferState(dstBuffer);\n    bool skip = false;\n    skip |= ValidateMemoryIsBoundToBuffer(buffer_state, \"vkCmdFillBuffer()\", \"VUID-vkCmdFillBuffer-dstBuffer-00031\");\n    skip |=\n        ValidateCmdQueueFlags(cb_node, \"vkCmdFillBuffer()\", VK_QUEUE_TRANSFER_BIT | VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT,\n                              \"VUID-vkCmdFillBuffer-commandBuffer-cmdpool\");\n    skip |= ValidateCmd(cb_node, CMD_FILLBUFFER, \"vkCmdFillBuffer()\");\n    // Validate that DST buffer has correct usage flags set\n    skip |= ValidateBufferUsageFlags(buffer_state, VK_BUFFER_USAGE_TRANSFER_DST_BIT, true, \"VUID-vkCmdFillBuffer-dstBuffer-00029\",\n                                     \"vkCmdFillBuffer()\", \"VK_BUFFER_USAGE_TRANSFER_DST_BIT\");\n    skip |= InsideRenderPass(cb_node, \"vkCmdFillBuffer()\", \"VUID-vkCmdFillBuffer-renderpass\");\n\n    skip |= ValidateProtectedBuffer(cb_node, buffer_state, \"vkCmdFillBuffer()\", \"VUID-vkCmdFillBuffer-commandBuffer-01811\");\n    skip |= ValidateUnprotectedBuffer(cb_node, buffer_state, \"vkCmdFillBuffer()\", \"VUID-vkCmdFillBuffer-commandBuffer-01812\");\n\n    if (dstOffset >= buffer_state->createInfo.size) {\n        skip |= LogError(dstBuffer, \"VUID-vkCmdFillBuffer-dstOffset-00024\",\n                         \"vkCmdFillBuffer(): dstOffset (0x%\" PRIxLEAST64\n                         \") is not less than destination buffer (%s) size (0x%\" PRIxLEAST64 \").\",\n                         dstOffset, report_data->FormatHandle(dstBuffer).c_str(), buffer_state->createInfo.size);\n    }\n\n    if ((size != VK_WHOLE_SIZE) && (size > (buffer_state->createInfo.size - dstOffset))) {\n        skip |= LogError(dstBuffer, \"VUID-vkCmdFillBuffer-size-00027\",\n                         \"vkCmdFillBuffer(): size (0x%\" PRIxLEAST64 \") is greater than dstBuffer (%s) size (0x%\" PRIxLEAST64\n                         \") minus dstOffset (0x%\" PRIxLEAST64 \").\",\n                         size, report_data->FormatHandle(dstBuffer).c_str(), buffer_state->createInfo.size, dstOffset);\n    }\n\n    return skip;\n}\n\ntemplate <typename BufferImageCopyRegionType>\nbool CoreChecks::ValidateBufferImageCopyData(const CMD_BUFFER_STATE *cb_node, uint32_t regionCount,\n                                             const BufferImageCopyRegionType *pRegions, const IMAGE_STATE *image_state,\n                                             const char *function, CopyCommandVersion version) const {\n    bool skip = false;\n\n    assert(image_state != nullptr);\n    const VkFormat image_format = image_state->createInfo.format;\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        const VkImageAspectFlags region_aspect_mask = pRegions[i].imageSubresource.aspectMask;\n        if (image_state->createInfo.imageType == VK_IMAGE_TYPE_1D) {\n            if ((pRegions[i].imageOffset.y != 0) || (pRegions[i].imageExtent.height != 1)) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-srcImage-00199\",\n                                 \"%s: pRegion[%d] imageOffset.y is %d and imageExtent.height is %d. For 1D images these must be 0 \"\n                                 \"and 1, respectively.\",\n                                 function, i, pRegions[i].imageOffset.y, pRegions[i].imageExtent.height);\n            }\n        }\n\n        if ((image_state->createInfo.imageType == VK_IMAGE_TYPE_1D) || (image_state->createInfo.imageType == VK_IMAGE_TYPE_2D)) {\n            if ((pRegions[i].imageOffset.z != 0) || (pRegions[i].imageExtent.depth != 1)) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-srcImage-00201\",\n                                 \"%s: pRegion[%d] imageOffset.z is %d and imageExtent.depth is %d. For 1D and 2D images these \"\n                                 \"must be 0 and 1, respectively.\",\n                                 function, i, pRegions[i].imageOffset.z, pRegions[i].imageExtent.depth);\n            }\n        }\n\n        if (image_state->createInfo.imageType == VK_IMAGE_TYPE_3D) {\n            if ((0 != pRegions[i].imageSubresource.baseArrayLayer) || (1 != pRegions[i].imageSubresource.layerCount)) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-baseArrayLayer-00213\",\n                                 \"%s: pRegion[%d] imageSubresource.baseArrayLayer is %d and imageSubresource.layerCount is %d. \"\n                                 \"For 3D images these must be 0 and 1, respectively.\",\n                                 function, i, pRegions[i].imageSubresource.baseArrayLayer, pRegions[i].imageSubresource.layerCount);\n            }\n        }\n\n        // If the the calling command's VkImage parameter's format is not a depth/stencil format,\n        // then bufferOffset must be a multiple of the calling command's VkImage parameter's element size\n        uint32_t element_size = FormatElementSize(image_format, region_aspect_mask);\n\n        // If not depth/stencil and not multi-plane\n        if ((!FormatIsDepthAndStencil(image_format) && !FormatIsMultiplane(image_format)) &&\n            SafeModulo(pRegions[i].bufferOffset, element_size) != 0) {\n            const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                   ? \"VUID-vkCmdCopyBufferToImage-bufferOffset-01558\"\n                                   : \"VUID-vkCmdCopyBufferToImage-bufferOffset-00193\";\n            skip |= LogError(image_state->image, vuid,\n                             \"%s: pRegion[%d] bufferOffset 0x%\" PRIxLEAST64\n                             \" must be a multiple of this format's texel size (%\" PRIu32 \").\",\n                             function, i, pRegions[i].bufferOffset, element_size);\n        }\n\n        //  BufferRowLength must be 0, or greater than or equal to the width member of imageExtent\n        if ((pRegions[i].bufferRowLength != 0) && (pRegions[i].bufferRowLength < pRegions[i].imageExtent.width)) {\n            skip |=\n                LogError(image_state->image, \"VUID-VkBufferImageCopy-bufferRowLength-00195\",\n                         \"%s: pRegion[%d] bufferRowLength (%d) must be zero or greater-than-or-equal-to imageExtent.width (%d).\",\n                         function, i, pRegions[i].bufferRowLength, pRegions[i].imageExtent.width);\n        }\n\n        //  BufferImageHeight must be 0, or greater than or equal to the height member of imageExtent\n        if ((pRegions[i].bufferImageHeight != 0) && (pRegions[i].bufferImageHeight < pRegions[i].imageExtent.height)) {\n            skip |=\n                LogError(image_state->image, \"VUID-VkBufferImageCopy-bufferImageHeight-00196\",\n                         \"%s: pRegion[%d] bufferImageHeight (%d) must be zero or greater-than-or-equal-to imageExtent.height (%d).\",\n                         function, i, pRegions[i].bufferImageHeight, pRegions[i].imageExtent.height);\n        }\n\n        // Calculate adjusted image extent, accounting for multiplane image factors\n        VkExtent3D adjusted_image_extent = GetImageSubresourceExtent(image_state, &pRegions[i].imageSubresource);\n        // imageOffset.x and (imageExtent.width + imageOffset.x) must both be >= 0 and <= image subresource width\n        if ((pRegions[i].imageOffset.x < 0) || (pRegions[i].imageOffset.x > static_cast<int32_t>(adjusted_image_extent.width)) ||\n            ((pRegions[i].imageOffset.x + static_cast<int32_t>(pRegions[i].imageExtent.width)) >\n             static_cast<int32_t>(adjusted_image_extent.width))) {\n            skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-imageOffset-00197\",\n                             \"%s: Both pRegion[%d] imageoffset.x (%d) and (imageExtent.width + imageOffset.x) (%d) must be >= \"\n                             \"zero or <= image subresource width (%d).\",\n                             function, i, pRegions[i].imageOffset.x, (pRegions[i].imageOffset.x + pRegions[i].imageExtent.width),\n                             adjusted_image_extent.width);\n        }\n\n        // imageOffset.y and (imageExtent.height + imageOffset.y) must both be >= 0 and <= image subresource height\n        if ((pRegions[i].imageOffset.y < 0) || (pRegions[i].imageOffset.y > static_cast<int32_t>(adjusted_image_extent.height)) ||\n            ((pRegions[i].imageOffset.y + static_cast<int32_t>(pRegions[i].imageExtent.height)) >\n             static_cast<int32_t>(adjusted_image_extent.height))) {\n            skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-imageOffset-00198\",\n                             \"%s: Both pRegion[%d] imageoffset.y (%d) and (imageExtent.height + imageOffset.y) (%d) must be >= \"\n                             \"zero or <= image subresource height (%d).\",\n                             function, i, pRegions[i].imageOffset.y, (pRegions[i].imageOffset.y + pRegions[i].imageExtent.height),\n                             adjusted_image_extent.height);\n        }\n\n        // imageOffset.z and (imageExtent.depth + imageOffset.z) must both be >= 0 and <= image subresource depth\n        if ((pRegions[i].imageOffset.z < 0) || (pRegions[i].imageOffset.z > static_cast<int32_t>(adjusted_image_extent.depth)) ||\n            ((pRegions[i].imageOffset.z + static_cast<int32_t>(pRegions[i].imageExtent.depth)) >\n             static_cast<int32_t>(adjusted_image_extent.depth))) {\n            skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-imageOffset-00200\",\n                             \"%s: Both pRegion[%d] imageoffset.z (%d) and (imageExtent.depth + imageOffset.z) (%d) must be >= \"\n                             \"zero or <= image subresource depth (%d).\",\n                             function, i, pRegions[i].imageOffset.z, (pRegions[i].imageOffset.z + pRegions[i].imageExtent.depth),\n                             adjusted_image_extent.depth);\n        }\n\n        // subresource aspectMask must have exactly 1 bit set\n        const int num_bits = sizeof(VkFlags) * CHAR_BIT;\n        std::bitset<num_bits> aspect_mask_bits(region_aspect_mask);\n        if (aspect_mask_bits.count() != 1) {\n            skip |= LogError(image_state->image, \"VUID-VkBufferImageCopy-aspectMask-00212\",\n                             \"%s: aspectMasks for imageSubresource in pRegion[%d] must have only a single bit set.\", function, i);\n        }\n\n        // image subresource aspect bit must match format\n        if (!VerifyAspectsPresent(region_aspect_mask, image_format)) {\n            skip |=\n                LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-aspectMask-00211\",\n                         \"%s: pRegion[%d] subresource aspectMask 0x%x specifies aspects that are not present in image format 0x%x.\",\n                         function, i, region_aspect_mask, image_format);\n        }\n\n        // Checks that apply only to compressed images\n        if (FormatIsCompressed(image_format) || FormatIsSinglePlane_422(image_format)) {\n            auto block_size = FormatTexelBlockExtent(image_format);\n\n            //  BufferRowLength must be a multiple of block width\n            if (SafeModulo(pRegions[i].bufferRowLength, block_size.width) != 0) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-bufferRowLength-00203\"\n                                       : \"VUID-vkCmdCopyBufferToImage-bufferRowLength-00203\";\n                skip |=\n                    LogError(image_state->image, vuid,\n                             \"%s: pRegion[%d] bufferRowLength (%d) must be a multiple of the compressed image's texel width (%d)..\",\n                             function, i, pRegions[i].bufferRowLength, block_size.width);\n            }\n\n            //  BufferRowHeight must be a multiple of block height\n            if (SafeModulo(pRegions[i].bufferImageHeight, block_size.height) != 0) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-bufferImageHeight-00204\"\n                                       : \"VUID-vkCmdCopyBufferToImage-bufferImageHeight-00204\";\n                skip |= LogError(\n                    image_state->image, vuid,\n                    \"%s: pRegion[%d] bufferImageHeight (%d) must be a multiple of the compressed image's texel height (%d)..\",\n                    function, i, pRegions[i].bufferImageHeight, block_size.height);\n            }\n\n            //  image offsets must be multiples of block dimensions\n            if ((SafeModulo(pRegions[i].imageOffset.x, block_size.width) != 0) ||\n                (SafeModulo(pRegions[i].imageOffset.y, block_size.height) != 0) ||\n                (SafeModulo(pRegions[i].imageOffset.z, block_size.depth) != 0)) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-imageOffset-00205\"\n                                       : \"VUID-vkCmdCopyBufferToImage-imageOffset-00205\";\n                skip |= LogError(image_state->image, vuid,\n                                 \"%s: pRegion[%d] imageOffset(x,y) (%d, %d) must be multiples of the compressed image's texel \"\n                                 \"width & height (%d, %d)..\",\n                                 function, i, pRegions[i].imageOffset.x, pRegions[i].imageOffset.y, block_size.width,\n                                 block_size.height);\n            }\n\n            // bufferOffset must be a multiple of block size (linear bytes)\n            uint32_t block_size_in_bytes = FormatElementSize(image_format);\n            if (SafeModulo(pRegions[i].bufferOffset, block_size_in_bytes) != 0) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-bufferOffset-00206\"\n                                       : \"VUID-vkCmdCopyBufferToImage-bufferOffset-00206\";\n                skip |= LogError(image_state->image, vuid,\n                                 \"%s: pRegion[%d] bufferOffset (0x%\" PRIxLEAST64\n                                 \") must be a multiple of the compressed image's texel block size (%\" PRIu32 \")..\",\n                                 function, i, pRegions[i].bufferOffset, block_size_in_bytes);\n            }\n\n            // imageExtent width must be a multiple of block width, or extent+offset width must equal subresource width\n            VkExtent3D mip_extent = GetImageSubresourceExtent(image_state, &(pRegions[i].imageSubresource));\n            if ((SafeModulo(pRegions[i].imageExtent.width, block_size.width) != 0) &&\n                (pRegions[i].imageExtent.width + pRegions[i].imageOffset.x != mip_extent.width)) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-imageExtent-00207\"\n                                       : \"VUID-vkCmdCopyBufferToImage-imageExtent-00207\";\n                skip |= LogError(image_state->image, vuid,\n                                 \"%s: pRegion[%d] extent width (%d) must be a multiple of the compressed texture block width \"\n                                 \"(%d), or when added to offset.x (%d) must equal the image subresource width (%d)..\",\n                                 function, i, pRegions[i].imageExtent.width, block_size.width, pRegions[i].imageOffset.x,\n                                 mip_extent.width);\n            }\n\n            // imageExtent height must be a multiple of block height, or extent+offset height must equal subresource height\n            if ((SafeModulo(pRegions[i].imageExtent.height, block_size.height) != 0) &&\n                (pRegions[i].imageExtent.height + pRegions[i].imageOffset.y != mip_extent.height)) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-imageExtent-00208\"\n                                       : \"VUID-vkCmdCopyBufferToImage-imageExtent-00208\";\n                skip |= LogError(image_state->image, vuid,\n                                 \"%s: pRegion[%d] extent height (%d) must be a multiple of the compressed texture block height \"\n                                 \"(%d), or when added to offset.y (%d) must equal the image subresource height (%d)..\",\n                                 function, i, pRegions[i].imageExtent.height, block_size.height, pRegions[i].imageOffset.y,\n                                 mip_extent.height);\n            }\n\n            // imageExtent depth must be a multiple of block depth, or extent+offset depth must equal subresource depth\n            if ((SafeModulo(pRegions[i].imageExtent.depth, block_size.depth) != 0) &&\n                (pRegions[i].imageExtent.depth + pRegions[i].imageOffset.z != mip_extent.depth)) {\n                const char *vuid = (device_extensions.vk_khr_sampler_ycbcr_conversion)\n                                       ? \"VUID-vkCmdCopyBufferToImage-imageExtent-00209\"\n                                       : \"VUID-vkCmdCopyBufferToImage-imageExtent-00209\";\n                skip |= LogError(image_state->image, vuid,\n                                 \"%s: pRegion[%d] extent width (%d) must be a multiple of the compressed texture block depth \"\n                                 \"(%d), or when added to offset.z (%d) must equal the image subresource depth (%d)..\",\n                                 function, i, pRegions[i].imageExtent.depth, block_size.depth, pRegions[i].imageOffset.z,\n                                 mip_extent.depth);\n            }\n        }\n\n        // Checks that apply only to multi-planar format images\n        if (FormatIsMultiplane(image_format)) {\n            // VK_IMAGE_ASPECT_PLANE_2_BIT valid only for image formats with three planes\n            if ((FormatPlaneCount(image_format) < 3) && (region_aspect_mask == VK_IMAGE_ASPECT_PLANE_2_BIT)) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-aspectMask-01560\",\n                                 \"%s: pRegion[%d] subresource aspectMask cannot be VK_IMAGE_ASPECT_PLANE_2_BIT unless image \"\n                                 \"format has three planes.\",\n                                 function, i);\n            }\n\n            // image subresource aspectMask must be VK_IMAGE_ASPECT_PLANE_*_BIT\n            if (0 ==\n                (region_aspect_mask & (VK_IMAGE_ASPECT_PLANE_0_BIT | VK_IMAGE_ASPECT_PLANE_1_BIT | VK_IMAGE_ASPECT_PLANE_2_BIT))) {\n                skip |= LogError(image_state->image, \"VUID-vkCmdCopyBufferToImage-aspectMask-01560\",\n                                 \"%s: pRegion[%d] subresource aspectMask for multi-plane image formats must have a \"\n                                 \"VK_IMAGE_ASPECT_PLANE_*_BIT when copying to or from.\",\n                                 function, i);\n            } else {\n                // Know aspect mask is valid\n                const VkFormat compatible_format = FindMultiplaneCompatibleFormat(image_format, region_aspect_mask);\n                const uint32_t compatible_size = FormatElementSize(compatible_format);\n                if (SafeModulo(pRegions[i].bufferOffset, compatible_size) != 0) {\n                    skip |= LogError(\n                        image_state->image, \"VUID-vkCmdCopyBufferToImage-bufferOffset-01559\",\n                        \"%s: pRegion[%d]->bufferOffset is 0x%\" PRIxLEAST64\n                        \" but must be a multiple of the multi-plane compatible format's texel size (%u) for plane %u (%s).\",\n                        function, i, pRegions[i].bufferOffset, element_size, GetPlaneIndex(region_aspect_mask),\n                        string_VkFormat(compatible_format));\n                }\n            }\n        }\n\n        // Checks depth or stencil aspect are used in graphics queue\n        if ((region_aspect_mask & (VK_IMAGE_ASPECT_DEPTH_BIT | VK_IMAGE_ASPECT_STENCIL_BIT)) != 0) {\n            assert(cb_node != nullptr);\n            const COMMAND_POOL_STATE *command_pool = cb_node->command_pool.get();\n            if (command_pool != nullptr) {\n                const uint32_t queueFamilyIndex = command_pool->queueFamilyIndex;\n                const VkQueueFlags queue_flags = GetPhysicalDeviceState()->queue_family_properties[queueFamilyIndex].queueFlags;\n\n                if ((queue_flags & VK_QUEUE_GRAPHICS_BIT) == 0) {\n                    LogObjectList objlist(cb_node->commandBuffer);\n                    objlist.add(command_pool->commandPool);\n                    // TODO - Label when future headers get merged in from internral MR 4077 fix\n                    skip |=\n                        LogError(image_state->image, \"UNASSIGNED-VkBufferImageCopy-aspectMask\",\n                                 \"%s: pRegion[%d] subresource aspectMask 0x%x specifies VK_IMAGE_ASPECT_DEPTH_BIT or \"\n                                 \"VK_IMAGE_ASPECT_STENCIL_BIT but the command buffer %s was allocated from the command pool %s \"\n                                 \"which was create with queueFamilyIndex %u which doesn't contain the VK_QUEUE_GRAPHICS_BIT flag.\",\n                                 function, i, region_aspect_mask, report_data->FormatHandle(cb_node->commandBuffer).c_str(),\n                                 report_data->FormatHandle(command_pool->commandPool).c_str(), queueFamilyIndex);\n                }\n            }\n        }\n    }\n\n    return skip;\n}\n\ntemplate <typename BufferImageCopyRegionType>\nbool CoreChecks::ValidateImageBounds(const IMAGE_STATE *image_state, const uint32_t regionCount,\n                                     const BufferImageCopyRegionType *pRegions, const char *func_name, const char *msg_code) const {\n    bool skip = false;\n    const VkImageCreateInfo *image_info = &(image_state->createInfo);\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        VkExtent3D extent = pRegions[i].imageExtent;\n        VkOffset3D offset = pRegions[i].imageOffset;\n\n        if (IsExtentSizeZero(&extent))  // Warn on zero area subresource\n        {\n            skip |= LogWarning(image_state->image, kVUID_Core_Image_ZeroAreaSubregion,\n                               \"%s: pRegion[%d] imageExtent of {%1d, %1d, %1d} has zero area\", func_name, i, extent.width,\n                               extent.height, extent.depth);\n        }\n\n        VkExtent3D image_extent = GetImageSubresourceExtent(image_state, &(pRegions[i].imageSubresource));\n\n        // If we're using a compressed format, valid extent is rounded up to multiple of block size (per 18.1)\n        if (FormatIsCompressed(image_info->format) || FormatIsSinglePlane_422(image_state->createInfo.format)) {\n            auto block_extent = FormatTexelBlockExtent(image_info->format);\n            if (image_extent.width % block_extent.width) {\n                image_extent.width += (block_extent.width - (image_extent.width % block_extent.width));\n            }\n            if (image_extent.height % block_extent.height) {\n                image_extent.height += (block_extent.height - (image_extent.height % block_extent.height));\n            }\n            if (image_extent.depth % block_extent.depth) {\n                image_extent.depth += (block_extent.depth - (image_extent.depth % block_extent.depth));\n            }\n        }\n\n        if (0 != ExceedsBounds(&offset, &extent, &image_extent)) {\n            skip |= LogError(image_state->image, msg_code, \"%s: pRegion[%d] exceeds image bounds..\", func_name, i);\n        }\n    }\n\n    return skip;\n}\n\ntemplate <typename BufferImageCopyRegionType>\nbool CoreChecks::ValidateBufferBounds(const IMAGE_STATE *image_state, const BUFFER_STATE *buff_state, uint32_t regionCount,\n                                      const BufferImageCopyRegionType *pRegions, const char *func_name,\n                                      const char *msg_code) const {\n    bool skip = false;\n\n    VkDeviceSize buffer_size = buff_state->createInfo.size;\n\n    for (uint32_t i = 0; i < regionCount; i++) {\n        VkDeviceSize max_buffer_offset =\n            GetBufferSizeFromCopyImage(pRegions[i], image_state->createInfo.format) + pRegions[i].bufferOffset;\n        if (buffer_size < max_buffer_offset) {\n            skip |=\n                LogError(device, msg_code, \"%s: pRegion[%d] exceeds buffer size of %\" PRIu64 \" bytes..\", func_name, i, buffer_size);\n        }\n    }\n\n    return skip;\n}\n\ntemplate <typename BufferImageCopyRegionType>\nbool CoreChecks::ValidateCmdCopyImageToBuffer(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                              VkBuffer dstBuffer, uint32_t regionCount, const BufferImageCopyRegionType *pRegions,\n                                              CopyCommandVersion version) const {\n    const auto cb_node = GetCBState(commandBuffer);\n    const auto src_image_state = GetImageState(srcImage);\n    const auto dst_buffer_state = GetBufferState(dstBuffer);\n\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdCopyImageToBuffer2KHR()\" : \"vkCmdCopyImageToBuffer()\";\n    const CMD_TYPE cmd_type = is_2khr ? CMD_COPYIMAGETOBUFFER2KHR : CMD_COPYIMAGETOBUFFER;\n    const char *vuid;\n\n    bool skip = ValidateBufferImageCopyData(cb_node, regionCount, pRegions, src_image_state, func_name, version);\n\n    // Validate command buffer state\n    skip |= ValidateCmd(cb_node, cmd_type, func_name);\n\n    // Command pool must support graphics, compute, or transfer operations\n    const auto pPool = cb_node->command_pool.get();\n\n    VkQueueFlags queue_flags = GetPhysicalDeviceState()->queue_family_properties[pPool->queueFamilyIndex].queueFlags;\n\n    if (0 == (queue_flags & (VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT))) {\n        vuid =\n            is_2khr ? \"VUID-vkCmdCopyImageToBuffer2KHR-commandBuffer-cmdpool\" : \"VUID-vkCmdCopyImageToBuffer-commandBuffer-cmdpool\";\n        skip |= LogError(cb_node->createInfo.commandPool, vuid,\n                         \"Cannot call %s on a command buffer allocated from a pool without graphics, compute, \"\n                         \"or transfer capabilities.\",\n                         func_name);\n    }\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-pRegions-00182\" : \"VUID-vkCmdCopyImageToBuffer-pRegions-00182\";\n    skip |= ValidateImageBounds(src_image_state, regionCount, pRegions, func_name, vuid);\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-pRegions-00183\" : \"VUID-vkCmdCopyImageToBuffer-pRegions-00183\";\n    skip |= ValidateBufferBounds(src_image_state, dst_buffer_state, regionCount, pRegions, func_name, vuid);\n\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImage-00188\" : \"VUID-vkCmdCopyImageToBuffer-srcImage-00188\";\n    const char *location = is_2khr ? \"vkCmdCopyImageToBuffer2KHR(): srcImage\" : \"vkCmdCopyImageToBuffer(): srcImage\";\n    skip |= ValidateImageSampleCount(src_image_state, VK_SAMPLE_COUNT_1_BIT, location, vuid);\n\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImage-00187\" : \"VUID-vkCmdCopyImageToBuffer-srcImage-00187\";\n    skip |= ValidateMemoryIsBoundToImage(src_image_state, func_name, vuid);\n    vuid = is_2khr ? \"vkCmdCopyImageToBuffer-dstBuffer2KHR-00192\" : \"vkCmdCopyImageToBuffer dstBuffer-00192\";\n    skip |= ValidateMemoryIsBoundToBuffer(dst_buffer_state, func_name, vuid);\n\n    // Validate that SRC image & DST buffer have correct usage flags set\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImage-00186\" : \"VUID-vkCmdCopyImageToBuffer-srcImage-00186\";\n    skip |= ValidateImageUsageFlags(src_image_state, VK_IMAGE_USAGE_TRANSFER_SRC_BIT, true, vuid, func_name,\n                                    \"VK_IMAGE_USAGE_TRANSFER_SRC_BIT\");\n    vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-dstBuffer-00191\" : \"VUID-vkCmdCopyImageToBuffer-dstBuffer-00191\";\n    skip |= ValidateBufferUsageFlags(dst_buffer_state, VK_BUFFER_USAGE_TRANSFER_DST_BIT, true, vuid, func_name,\n                                     \"VK_BUFFER_USAGE_TRANSFER_DST_BIT\");\n    vuid = is_2khr ? \"VUID-vkCmdCopyImageToBuffer2KHR-commandBuffer-01831\" : \"VUID-vkCmdCopyImageToBuffer-commandBuffer-01831\";\n    skip |= ValidateProtectedImage(cb_node, src_image_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyImageToBuffer2KHR-commandBuffer-01832\" : \"VUID-vkCmdCopyImageToBuffer-commandBuffer-01832\";\n    skip |= ValidateProtectedBuffer(cb_node, dst_buffer_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyImageToBuffer2KHR-commandBuffer-01833\" : \"VUID-vkCmdCopyImageToBuffer-commandBuffer-01833\";\n    skip |= ValidateUnprotectedBuffer(cb_node, dst_buffer_state, func_name, vuid);\n\n    // Validation for VK_EXT_fragment_density_map\n    if (src_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n        vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImage-02544\" : \"VUID-vkCmdCopyImageToBuffer-srcImage-02544\";\n        skip |= LogError(cb_node->commandBuffer, vuid,\n                         \"%s: srcImage must not have been created with flags containing \"\n                         \"VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                         func_name);\n    }\n\n    if (device_extensions.vk_khr_maintenance1) {\n        vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImage-01998\" : \"VUID-vkCmdCopyImageToBuffer-srcImage-01998\";\n        skip |= ValidateImageFormatFeatureFlags(src_image_state, VK_FORMAT_FEATURE_TRANSFER_SRC_BIT, func_name, vuid);\n    }\n    vuid = is_2khr ? \"VUID-vkCmdCopyImageToBuffer-renderpass\" : \"VUID-vkCmdCopyImageToBuffer-renderpass\";\n    skip |= InsideRenderPass(cb_node, func_name, vuid);\n    bool hit_error = false;\n\n    const char *src_invalid_layout_vuid = (src_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                                              ? (vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImageLayout-01397\"\n                                                                : \"VUID-vkCmdCopyImageToBuffer-srcImageLayout-01397\")\n                                              : (vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImageLayout-00190\"\n                                                                : \"VUID-vkCmdCopyImageToBuffer-srcImageLayout-00190\");\n\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        skip |= ValidateImageSubresourceLayers(cb_node, &pRegions[i].imageSubresource, func_name, \"imageSubresource\", i);\n        vuid =\n            is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-srcImageLayout-00189\" : \"VUID-vkCmdCopyImageToBuffer-srcImageLayout-00189\";\n        skip |= VerifyImageLayout(cb_node, src_image_state, pRegions[i].imageSubresource, srcImageLayout,\n                                  VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL, func_name, src_invalid_layout_vuid, vuid, &hit_error);\n        vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-imageOffset-01794\" : \"VUID-vkCmdCopyImageToBuffer-imageOffset-01794\";\n        skip |= ValidateCopyBufferImageTransferGranularityRequirements(cb_node, src_image_state, &pRegions[i], i, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-imageSubresource-01703\"\n                       : \"VUID-vkCmdCopyImageToBuffer-imageSubresource-01703\";\n        skip |= ValidateImageMipLevel(cb_node, src_image_state, pRegions[i].imageSubresource.mipLevel, i, func_name,\n                                      \"imageSubresource\", vuid);\n        vuid = is_2khr ? \"VUID-VkCopyImageToBufferInfo2KHR-imageSubresource-01704\"\n                       : \"VUID-vkCmdCopyImageToBuffer-imageSubresource-01704\";\n        skip |= ValidateImageArrayLayerRange(cb_node, src_image_state, pRegions[i].imageSubresource.baseArrayLayer,\n                                             pRegions[i].imageSubresource.layerCount, i, func_name, \"imageSubresource\", vuid);\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdCopyImageToBuffer(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                                     VkBuffer dstBuffer, uint32_t regionCount,\n                                                     const VkBufferImageCopy *pRegions) const {\n    return ValidateCmdCopyImageToBuffer(commandBuffer, srcImage, srcImageLayout, dstBuffer, regionCount, pRegions,\n                                        COPY_COMMAND_VERSION_1);\n}\n\nbool CoreChecks::PreCallValidateCmdCopyImageToBuffer2KHR(VkCommandBuffer commandBuffer,\n                                                         const VkCopyImageToBufferInfo2KHR *pCopyImageToBufferInfo) const {\n    return ValidateCmdCopyImageToBuffer(commandBuffer, pCopyImageToBufferInfo->srcImage, pCopyImageToBufferInfo->srcImageLayout,\n                                        pCopyImageToBufferInfo->dstBuffer, pCopyImageToBufferInfo->regionCount,\n                                        pCopyImageToBufferInfo->pRegions, COPY_COMMAND_VERSION_2);\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyImageToBuffer(VkCommandBuffer commandBuffer, VkImage srcImage, VkImageLayout srcImageLayout,\n                                                   VkBuffer dstBuffer, uint32_t regionCount, const VkBufferImageCopy *pRegions) {\n    StateTracker::PreCallRecordCmdCopyImageToBuffer(commandBuffer, srcImage, srcImageLayout, dstBuffer, regionCount, pRegions);\n\n    auto cb_node = GetCBState(commandBuffer);\n    auto src_image_state = GetImageState(srcImage);\n    // Make sure that all image slices record referenced layout\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *src_image_state, pRegions[i].imageSubresource, srcImageLayout);\n    }\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyImageToBuffer2KHR(VkCommandBuffer commandBuffer,\n                                                       const VkCopyImageToBufferInfo2KHR *pCopyImageToBufferInfo) {\n    StateTracker::PreCallRecordCmdCopyImageToBuffer2KHR(commandBuffer, pCopyImageToBufferInfo);\n\n    auto cb_node = GetCBState(commandBuffer);\n    auto src_image_state = GetImageState(pCopyImageToBufferInfo->srcImage);\n    // Make sure that all image slices record referenced layout\n    for (uint32_t i = 0; i < pCopyImageToBufferInfo->regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *src_image_state, pCopyImageToBufferInfo->pRegions[i].imageSubresource,\n                              pCopyImageToBufferInfo->srcImageLayout);\n    }\n}\n\ntemplate <typename RegionType>\nbool CoreChecks::ValidateCmdCopyBufferToImage(VkCommandBuffer commandBuffer, VkBuffer srcBuffer, VkImage dstImage,\n                                              VkImageLayout dstImageLayout, uint32_t regionCount, const RegionType *pRegions,\n                                              CopyCommandVersion version) const {\n    const auto cb_node = GetCBState(commandBuffer);\n    const auto src_buffer_state = GetBufferState(srcBuffer);\n    const auto dst_image_state = GetImageState(dstImage);\n\n    const bool is_2khr = (version == COPY_COMMAND_VERSION_2);\n    const char *func_name = is_2khr ? \"vkCmdCopyBufferToImage2KHR()\" : \"vkCmdCopyBufferToImage()\";\n    const CMD_TYPE cmd_type = is_2khr ? CMD_COPYBUFFERTOIMAGE2KHR : CMD_COPYBUFFERTOIMAGE;\n    const char *vuid;\n\n    bool skip = ValidateBufferImageCopyData(cb_node, regionCount, pRegions, dst_image_state, func_name, version);\n\n    // Validate command buffer state\n    skip |= ValidateCmd(cb_node, cmd_type, func_name);\n\n    // Command pool must support graphics, compute, or transfer operations\n    const auto pPool = cb_node->command_pool.get();\n    VkQueueFlags queue_flags = GetPhysicalDeviceState()->queue_family_properties[pPool->queueFamilyIndex].queueFlags;\n    if (0 == (queue_flags & (VK_QUEUE_GRAPHICS_BIT | VK_QUEUE_COMPUTE_BIT | VK_QUEUE_TRANSFER_BIT))) {\n        vuid =\n            is_2khr ? \"VUID-vkCmdCopyBufferToImage2KHR-commandBuffer-cmdpool\" : \"VUID-vkCmdCopyBufferToImage-commandBuffer-cmdpool\";\n        skip |= LogError(cb_node->createInfo.commandPool, vuid,\n                         \"Cannot call %s on a command buffer allocated from a pool without graphics, compute, \"\n                         \"or transfer capabilities.\",\n                         func_name);\n    }\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-pRegions-00172\" : \"VUID-vkCmdCopyBufferToImage-pRegions-00172\";\n    skip |= ValidateImageBounds(dst_image_state, regionCount, pRegions, func_name, vuid);\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-pRegions-00171\" : \"VUID-vkCmdCopyBufferToImage-pRegions-00171\";\n    skip |= ValidateBufferBounds(dst_image_state, src_buffer_state, regionCount, pRegions, func_name, vuid);\n\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImage-00179\" : \"VUID-vkCmdCopyBufferToImage-dstImage-00179\";\n    const char *location = is_2khr ? \"vkCmdCopyBufferToImage2KHR(): dstImage\" : \"vkCmdCopyBufferToImage(): dstImage\";\n    skip |= ValidateImageSampleCount(dst_image_state, VK_SAMPLE_COUNT_1_BIT, location, vuid);\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-srcBuffer-00176\" : \"VUID-vkCmdCopyBufferToImage-srcBuffer-00176\";\n    skip |= ValidateMemoryIsBoundToBuffer(src_buffer_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImage-00178\" : \"VUID-vkCmdCopyBufferToImage-dstImage-00178\";\n    skip |= ValidateMemoryIsBoundToImage(dst_image_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-srcBuffer-00174\" : \"VUID-vkCmdCopyBufferToImage-srcBuffer-00174\";\n    skip |= ValidateBufferUsageFlags(src_buffer_state, VK_BUFFER_USAGE_TRANSFER_SRC_BIT, true, vuid, func_name,\n                                     \"VK_BUFFER_USAGE_TRANSFER_SRC_BIT\");\n    vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImage-00177\" : \"VUID-vkCmdCopyBufferToImage-dstImage-00177\";\n    skip |= ValidateImageUsageFlags(dst_image_state, VK_IMAGE_USAGE_TRANSFER_DST_BIT, true, vuid, func_name,\n                                    \"VK_IMAGE_USAGE_TRANSFER_DST_BIT\");\n    vuid = is_2khr ? \"VUID-vkCmdCopyBufferToImage2KHR-commandBuffer-01828\" : \"VUID-vkCmdCopyBufferToImage-commandBuffer-01828\";\n    skip |= ValidateProtectedBuffer(cb_node, src_buffer_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyBufferToImage2KHR-commandBuffer-01829\" : \"VUID-vkCmdCopyBufferToImage-commandBuffer-01829\";\n    skip |= ValidateProtectedImage(cb_node, dst_image_state, func_name, vuid);\n    vuid = is_2khr ? \"VUID-vkCmdCopyBufferToImage-commandBuffer-01830\" : \"VUID-vkCmdCopyBufferToImage-commandBuffer-01830\";\n    skip |= ValidateUnprotectedImage(cb_node, dst_image_state, func_name, vuid);\n\n    // Validation for VK_EXT_fragment_density_map\n    if (dst_image_state->createInfo.flags & VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT) {\n        vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImage-02543\" : \"VUID-vkCmdCopyBufferToImage-dstImage-02543\";\n        skip |= LogError(cb_node->commandBuffer, vuid,\n                         \"%s: dstImage must not have been created with flags containing \"\n                         \"VK_IMAGE_CREATE_SUBSAMPLED_BIT_EXT\",\n                         func_name);\n    }\n\n    if (device_extensions.vk_khr_maintenance1) {\n        vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImage-01997\" : \"VUID-vkCmdCopyBufferToImage-dstImage-01997\";\n        skip |= ValidateImageFormatFeatureFlags(dst_image_state, VK_FORMAT_FEATURE_TRANSFER_DST_BIT, func_name, vuid);\n    }\n    vuid = is_2khr ? \"VUID-vkCmdCopyBufferToImage2KHR-renderpass\" : \"VUID-vkCmdCopyBufferToImage-renderpass\";\n    skip |= InsideRenderPass(cb_node, func_name, vuid);\n    bool hit_error = false;\n\n    const char *dst_invalid_layout_vuid = (dst_image_state->shared_presentable && device_extensions.vk_khr_shared_presentable_image)\n                                              ? (is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImageLayout-01396\"\n                                                         : \"VUID-vkCmdCopyBufferToImage-dstImageLayout-01396\")\n                                              : (is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImageLayout-00181\"\n                                                         : \"VUID-vkCmdCopyBufferToImage-dstImageLayout-00181\");\n\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        skip |= ValidateImageSubresourceLayers(cb_node, &pRegions[i].imageSubresource, func_name, \"imageSubresource\", i);\n        vuid =\n            is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-dstImageLayout-00180\" : \"VUID-vkCmdCopyBufferToImage-dstImageLayout-00180\";\n        skip |= VerifyImageLayout(cb_node, dst_image_state, pRegions[i].imageSubresource, dstImageLayout,\n                                  VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL, func_name, dst_invalid_layout_vuid, vuid, &hit_error);\n        vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-imageOffset-01793\" : \"VUID-vkCmdCopyBufferToImage-imageOffset-01793\";\n        skip |= ValidateCopyBufferImageTransferGranularityRequirements(cb_node, dst_image_state, &pRegions[i], i, func_name, vuid);\n        vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-imageSubresource-01701\"\n                       : \"VUID-vkCmdCopyBufferToImage-imageSubresource-01701\";\n        skip |= ValidateImageMipLevel(cb_node, dst_image_state, pRegions[i].imageSubresource.mipLevel, i, func_name,\n                                      \"imageSubresource\", vuid);\n        vuid = is_2khr ? \"VUID-VkCopyBufferToImageInfo2KHR-imageSubresource-01702\"\n                       : \"VUID-vkCmdCopyBufferToImage-imageSubresource-01702\";\n        skip |= ValidateImageArrayLayerRange(cb_node, dst_image_state, pRegions[i].imageSubresource.baseArrayLayer,\n                                             pRegions[i].imageSubresource.layerCount, i, func_name, \"imageSubresource\", vuid);\n    }\n    return skip;\n}\n\nbool CoreChecks::PreCallValidateCmdCopyBufferToImage(VkCommandBuffer commandBuffer, VkBuffer srcBuffer, VkImage dstImage,\n                                                     VkImageLayout dstImageLayout, uint32_t regionCount,\n                                                     const VkBufferImageCopy *pRegions) const {\n    return ValidateCmdCopyBufferToImage(commandBuffer, srcBuffer, dstImage, dstImageLayout, regionCount, pRegions,\n                                        COPY_COMMAND_VERSION_1);\n}\n\nbool CoreChecks::PreCallValidateCmdCopyBufferToImage2KHR(VkCommandBuffer commandBuffer,\n                                                         const VkCopyBufferToImageInfo2KHR *pCopyBufferToImageInfo) const {\n    return ValidateCmdCopyBufferToImage(commandBuffer, pCopyBufferToImageInfo->srcBuffer, pCopyBufferToImageInfo->dstImage,\n                                        pCopyBufferToImageInfo->dstImageLayout, pCopyBufferToImageInfo->regionCount,\n                                        pCopyBufferToImageInfo->pRegions, COPY_COMMAND_VERSION_2);\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyBufferToImage(VkCommandBuffer commandBuffer, VkBuffer srcBuffer, VkImage dstImage,\n                                                   VkImageLayout dstImageLayout, uint32_t regionCount,\n                                                   const VkBufferImageCopy *pRegions) {\n    StateTracker::PreCallRecordCmdCopyBufferToImage(commandBuffer, srcBuffer, dstImage, dstImageLayout, regionCount, pRegions);\n\n    auto cb_node = GetCBState(commandBuffer);\n    auto dst_image_state = GetImageState(dstImage);\n    // Make sure that all image slices are record referenced layout\n    for (uint32_t i = 0; i < regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *dst_image_state, pRegions[i].imageSubresource, dstImageLayout);\n    }\n}\n\nvoid CoreChecks::PreCallRecordCmdCopyBufferToImage2KHR(VkCommandBuffer commandBuffer,\n                                                       const VkCopyBufferToImageInfo2KHR *pCopyBufferToImageInfo2KHR) {\n    StateTracker::PreCallRecordCmdCopyBufferToImage2KHR(commandBuffer, pCopyBufferToImageInfo2KHR);\n\n    auto cb_node = GetCBState(commandBuffer);\n    auto dst_image_state = GetImageState(pCopyBufferToImageInfo2KHR->dstImage);\n    // Make sure that all image slices are record referenced layout\n    for (uint32_t i = 0; i < pCopyBufferToImageInfo2KHR->regionCount; ++i) {\n        SetImageInitialLayout(cb_node, *dst_image_state, pCopyBufferToImageInfo2KHR->pRegions[i].imageSubresource,\n                              pCopyBufferToImageInfo2KHR->dstImageLayout);\n    }\n}\nbool CoreChecks::PreCallValidateGetImageSubresourceLayout(VkDevice device, VkImage image, const VkImageSubresource *pSubresource,\n                                                          VkSubresourceLayout *pLayout) const {\n    bool skip = false;\n    const VkImageAspectFlags sub_aspect = pSubresource->aspectMask;\n\n    // The aspectMask member of pSubresource must only have a single bit set\n    const int num_bits = sizeof(sub_aspect) * CHAR_BIT;\n    std::bitset<num_bits> aspect_mask_bits(sub_aspect);\n    if (aspect_mask_bits.count() != 1) {\n        skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-aspectMask-00997\",\n                         \"vkGetImageSubresourceLayout(): VkImageSubresource.aspectMask must have exactly 1 bit set.\");\n    }\n\n    const IMAGE_STATE *image_entry = GetImageState(image);\n    if (!image_entry) {\n        return skip;\n    }\n\n    // Image must have been created with tiling equal to VK_IMAGE_TILING_LINEAR\n    if (device_extensions.vk_ext_image_drm_format_modifier) {\n        if ((image_entry->createInfo.tiling != VK_IMAGE_TILING_LINEAR) &&\n            (image_entry->createInfo.tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT)) {\n            skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-image-02270\",\n                             \"vkGetImageSubresourceLayout(): Image must have tiling of VK_IMAGE_TILING_LINEAR or \"\n                             \"VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT.\");\n        }\n    } else {\n        if (image_entry->createInfo.tiling != VK_IMAGE_TILING_LINEAR) {\n            skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-image-00996\",\n                             \"vkGetImageSubresourceLayout(): Image must have tiling of VK_IMAGE_TILING_LINEAR.\");\n        }\n    }\n\n    // mipLevel must be less than the mipLevels specified in VkImageCreateInfo when the image was created\n    if (pSubresource->mipLevel >= image_entry->createInfo.mipLevels) {\n        skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-mipLevel-01716\",\n                         \"vkGetImageSubresourceLayout(): pSubresource.mipLevel (%d) must be less than %d.\", pSubresource->mipLevel,\n                         image_entry->createInfo.mipLevels);\n    }\n\n    // arrayLayer must be less than the arrayLayers specified in VkImageCreateInfo when the image was created\n    if (pSubresource->arrayLayer >= image_entry->createInfo.arrayLayers) {\n        skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-arrayLayer-01717\",\n                         \"vkGetImageSubresourceLayout(): pSubresource.arrayLayer (%d) must be less than %d.\",\n                         pSubresource->arrayLayer, image_entry->createInfo.arrayLayers);\n    }\n\n    // subresource's aspect must be compatible with image's format.\n    const VkFormat img_format = image_entry->createInfo.format;\n    if (image_entry->createInfo.tiling == VK_IMAGE_TILING_LINEAR) {\n        if (FormatIsMultiplane(img_format)) {\n            VkImageAspectFlags allowed_flags = (VK_IMAGE_ASPECT_PLANE_0_BIT_KHR | VK_IMAGE_ASPECT_PLANE_1_BIT_KHR);\n            const char *vuid = \"VUID-vkGetImageSubresourceLayout-format-01581\";  // 2-plane version\n            if (FormatPlaneCount(img_format) > 2u) {\n                allowed_flags |= VK_IMAGE_ASPECT_PLANE_2_BIT_KHR;\n                vuid = \"VUID-vkGetImageSubresourceLayout-format-01582\";  // 3-plane version\n            }\n            if (sub_aspect != (sub_aspect & allowed_flags)) {\n                skip |= LogError(image, vuid,\n                                 \"vkGetImageSubresourceLayout(): For multi-planar images, VkImageSubresource.aspectMask (0x%\" PRIx32\n                                 \") must be a single-plane specifier flag.\",\n                                 sub_aspect);\n            }\n        } else if (FormatIsColor(img_format)) {\n            if (sub_aspect != VK_IMAGE_ASPECT_COLOR_BIT) {\n                skip |= LogError(image, kVUID_Core_DrawState_InvalidImageAspect,\n                                 \"vkGetImageSubresourceLayout(): For color formats, VkImageSubresource.aspectMask must be \"\n                                 \"VK_IMAGE_ASPECT_COLOR.\");\n            }\n        } else if (FormatIsDepthOrStencil(img_format)) {\n            if ((sub_aspect != VK_IMAGE_ASPECT_DEPTH_BIT) && (sub_aspect != VK_IMAGE_ASPECT_STENCIL_BIT)) {\n            }\n        }\n    } else if (image_entry->createInfo.tiling == VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT) {\n        if ((sub_aspect != VK_IMAGE_ASPECT_MEMORY_PLANE_0_BIT_EXT) && (sub_aspect != VK_IMAGE_ASPECT_MEMORY_PLANE_1_BIT_EXT) &&\n            (sub_aspect != VK_IMAGE_ASPECT_MEMORY_PLANE_2_BIT_EXT) && (sub_aspect != VK_IMAGE_ASPECT_MEMORY_PLANE_3_BIT_EXT)) {\n            // TODO: This VU also needs to ensure that the DRM index is in range and valid.\n            skip |= LogError(image, \"VUID-vkGetImageSubresourceLayout-tiling-02271\",\n                             \"vkGetImageSubresourceLayout(): VkImageSubresource.aspectMask must be \"\n                             \"VK_IMAGE_ASPECT_MEMORY_PLANE_i_BIT_EXT.\");\n        }\n    }\n\n    if (device_extensions.vk_android_external_memory_android_hardware_buffer) {\n        skip |= ValidateGetImageSubresourceLayoutANDROID(image);\n    }\n\n    return skip;\n}\n\n// Validates the image is allowed to be protected\nbool CoreChecks::ValidateProtectedImage(const CMD_BUFFER_STATE *cb_state, const IMAGE_STATE *image_state, const char *cmd_name,\n                                        const char *vuid) const {\n    bool skip = false;\n    if ((cb_state->unprotected == true) && (image_state->unprotected == false)) {\n        LogObjectList objlist(cb_state->commandBuffer);\n        objlist.add(image_state->image);\n        skip |= LogError(objlist, vuid, \"%s: command buffer %s is unprotected while image %s is a protected image\", cmd_name,\n                         report_data->FormatHandle(cb_state->commandBuffer).c_str(),\n                         report_data->FormatHandle(image_state->image).c_str());\n    }\n    return skip;\n}\n\n// Validates the image is allowed to be unprotected\nbool CoreChecks::ValidateUnprotectedImage(const CMD_BUFFER_STATE *cb_state, const IMAGE_STATE *image_state, const char *cmd_name,\n                                          const char *vuid) const {\n    bool skip = false;\n    if ((cb_state->unprotected == false) && (image_state->unprotected == true)) {\n        LogObjectList objlist(cb_state->commandBuffer);\n        objlist.add(image_state->image);\n        skip |= LogError(objlist, vuid, \"%s: command buffer %s is protected while image %s is an unprotected image\", cmd_name,\n                         report_data->FormatHandle(cb_state->commandBuffer).c_str(),\n                         report_data->FormatHandle(image_state->image).c_str());\n    }\n    return skip;\n}\n\n// Validates the buffer is allowed to be protected\nbool CoreChecks::ValidateProtectedBuffer(const CMD_BUFFER_STATE *cb_state, const BUFFER_STATE *buffer_state, const char *cmd_name,\n                                         const char *vuid) const {\n    bool skip = false;\n    if ((cb_state->unprotected == true) && (buffer_state->unprotected == false)) {\n        LogObjectList objlist(cb_state->commandBuffer);\n        objlist.add(buffer_state->buffer);\n        skip |= LogError(objlist, vuid, \"%s: command buffer %s is unprotected while buffer %s is a protected buffer\", cmd_name,\n                         report_data->FormatHandle(cb_state->commandBuffer).c_str(),\n                         report_data->FormatHandle(buffer_state->buffer).c_str());\n    }\n    return skip;\n}\n\n// Validates the buffer is allowed to be unprotected\nbool CoreChecks::ValidateUnprotectedBuffer(const CMD_BUFFER_STATE *cb_state, const BUFFER_STATE *buffer_state, const char *cmd_name,\n                                           const char *vuid) const {\n    bool skip = false;\n    if ((cb_state->unprotected == false) && (buffer_state->unprotected == true)) {\n        LogObjectList objlist(cb_state->commandBuffer);\n        objlist.add(buffer_state->buffer);\n        skip |= LogError(objlist, vuid, \"%s: command buffer %s is protected while buffer %s is an unprotected buffer\", cmd_name,\n                         report_data->FormatHandle(cb_state->commandBuffer).c_str(),\n                         report_data->FormatHandle(buffer_state->buffer).c_str());\n    }\n    return skip;\n}\n", "idx": 1, "id": 14410, "msg": "In the original, we walk through every subresource in the range, even if the state is the same as the previous subresource.", "proj": "KhronosGroup-Vulkan-ValidationLayers", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -459,15 +459,11 @@ func (w *Workflow) populate() error {\n \t// so Vars replacement must run before this to resolve the final\n \t// value for those fields.\n \n-\tautovars := map[string]string{\n-\t\t\"ID\":          w.id,\n+\tautovars = map[string]string{\n \t\t\"NAME\":        w.Name,\n \t\t\"ZONE\":        w.Zone,\n \t\t\"PROJECT\":     w.Project,\n \t\t\"GCSPATH\":     w.GCSPath,\n-\t\t\"DATE\":        datestamp,\n-\t\t\"DATETIME\":    datetimestamp,\n-\t\t\"TIMESTAMP\":   timestamp,\n \t\t\"SCRATCHPATH\": fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.scratchPath),\n \t\t\"SOURCESPATH\": fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.sourcesPath),\n \t\t\"LOGSPATH\":    fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.logsPath),", "y": 0, "oldf": "//  Copyright 2017 Google Inc. All Rights Reserved.\n//\n//  Licensed under the Apache License, Version 2.0 (the \"License\");\n//  you may not use this file except in compliance with the License.\n//  You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n\n// Package workflow describes a daisy workflow.\npackage workflow\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"cloud.google.com/go/storage\"\n\t\"github.com/GoogleCloudPlatform/compute-image-tools/daisy/compute\"\n\t\"google.golang.org/api/option\"\n)\n\ntype gcsLogger struct {\n\tclient         *storage.Client\n\tbucket, object string\n\tbuf            *bytes.Buffer\n\tctx            context.Context\n}\n\nfunc (l *gcsLogger) Write(b []byte) (int, error) {\n\tif l.buf == nil {\n\t\tl.buf = new(bytes.Buffer)\n\t}\n\tl.buf.Write(b)\n\twc := l.client.Bucket(l.bucket).Object(l.object).NewWriter(l.ctx)\n\twc.ContentType = \"text/plain\"\n\tn, err := wc.Write(l.buf.Bytes())\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif err := wc.Close(); err != nil {\n\t\treturn 0, err\n\t}\n\treturn n, err\n}\n\ntype refMap struct {\n\tm  map[string]*resource\n\tmx sync.Mutex\n}\n\nfunc (rm *refMap) add(name string, r *resource) {\n\trm.mx.Lock()\n\tdefer rm.mx.Unlock()\n\tif rm.m == nil {\n\t\trm.m = map[string]*resource{}\n\t}\n\trm.m[name] = r\n}\n\nfunc (rm *refMap) del(name string) {\n\trm.mx.Lock()\n\tdefer rm.mx.Unlock()\n\tif rm.m != nil {\n\t\tdelete(rm.m, name)\n\t}\n}\n\nfunc (rm *refMap) get(name string) (*resource, bool) {\n\trm.mx.Lock()\n\tdefer rm.mx.Unlock()\n\tif rm.m == nil {\n\t\treturn nil, false\n\t}\n\tr, ok := rm.m[name]\n\treturn r, ok\n}\n\ntype resource struct {\n\tname, real, link string\n\tnoCleanup        bool\n}\n\ntype step interface {\n\trun(w *Workflow) error\n\tvalidate(w *Workflow) error\n}\n\n// Step is a single daisy workflow step.\ntype Step struct {\n\tname string\n\t// Time to wait for this step to complete (default 10m).\n\t// Must be parsable by https://golang.org/pkg/time/#ParseDuration.\n\tTimeout string\n\ttimeout time.Duration\n\t// Only one of the below fields should exist for each instance of Step.\n\tAttachDisks            *AttachDisks            `json:\",omitempty\"`\n\tCreateDisks            *CreateDisks            `json:\",omitempty\"`\n\tCreateImages           *CreateImages           `json:\",omitempty\"`\n\tCreateInstances        *CreateInstances        `json:\",omitempty\"`\n\tDeleteResources        *DeleteResources        `json:\",omitempty\"`\n\tRunTests               *RunTests               `json:\",omitempty\"`\n\tSubWorkflow            *SubWorkflow            `json:\",omitempty\"`\n\tWaitForInstancesSignal *WaitForInstancesSignal `json:\",omitempty\"`\n\t// Used for unit tests.\n\ttestType step\n}\n\nfunc (s *Step) realStep() (step, error) {\n\tvar result step\n\tmatchCount := 0\n\tif s.AttachDisks != nil {\n\t\tmatchCount++\n\t\tresult = s.AttachDisks\n\t}\n\tif s.CreateDisks != nil {\n\t\tmatchCount++\n\t\tresult = s.CreateDisks\n\t}\n\tif s.CreateImages != nil {\n\t\tmatchCount++\n\t\tresult = s.CreateImages\n\t}\n\tif s.CreateInstances != nil {\n\t\tmatchCount++\n\t\tresult = s.CreateInstances\n\t}\n\tif s.DeleteResources != nil {\n\t\tmatchCount++\n\t\tresult = s.DeleteResources\n\t}\n\tif s.RunTests != nil {\n\t\tmatchCount++\n\t\tresult = s.RunTests\n\t}\n\tif s.SubWorkflow != nil {\n\t\tmatchCount++\n\t\tresult = s.SubWorkflow\n\t}\n\tif s.WaitForInstancesSignal != nil {\n\t\tmatchCount++\n\t\tresult = s.WaitForInstancesSignal\n\t}\n\tif s.testType != nil {\n\t\tmatchCount++\n\t\tresult = s.testType\n\t}\n\n\tif matchCount == 0 {\n\t\treturn nil, errors.New(\"no step type defined\")\n\t}\n\tif matchCount > 1 {\n\t\treturn nil, errors.New(\"multiple step types defined\")\n\t}\n\treturn result, nil\n}\n\nfunc (s *Step) run(w *Workflow) error {\n\trealStep, err := s.realStep()\n\tif err != nil {\n\t\treturn s.wrapRunError(err)\n\t}\n\tvar st string\n\tif t := reflect.TypeOf(realStep); t.Kind() == reflect.Ptr {\n\t\tst = t.Elem().Name()\n\t} else {\n\t\tst = t.Name()\n\t}\n\tw.logger.Printf(\"Running step %q (%s)\", s.name, st)\n\tif err = realStep.run(w); err != nil {\n\t\treturn s.wrapRunError(err)\n\t}\n\tw.logger.Printf(\"Step %q (%s) successfully finished.\", s.name, st)\n\treturn nil\n}\n\nfunc (s *Step) validate(w *Workflow) error {\n\tw.logger.Printf(\"Validating step %q\", s.name)\n\trealStep, err := s.realStep()\n\tif err != nil {\n\t\treturn s.wrapValidateError(err)\n\t}\n\tif err = realStep.validate(w); err != nil {\n\t\treturn s.wrapValidateError(err)\n\t}\n\treturn nil\n}\n\nfunc (s *Step) wrapRunError(e error) error {\n\treturn fmt.Errorf(\"step %q run error: %s\", s.name, e)\n}\n\nfunc (s *Step) wrapValidateError(e error) error {\n\treturn fmt.Errorf(\"step %q validation error: %s\", s.name, e)\n}\n\n// Workflow is a single Daisy workflow workflow.\ntype Workflow struct {\n\t// Populated on New() construction.\n\tCtx    context.Context `json:\"-\"`\n\tCancel chan struct{}   `json:\"-\"`\n\n\t// Workflow template fields.\n\t// Workflow name.\n\tName string\n\t// Project to run in.\n\tProject string\n\t// Zone to run in.\n\tZone string\n\t// GCS Path to use for scratch data and write logs/results to.\n\tGCSPath string\n\t// Path to OAuth credentials file.\n\tOAuthPath string `json:\",omitempty\"`\n\t// Sources used by this workflow, map of destination to source.\n\tSources map[string]string `json:\",omitempty\"`\n\t// Vars defines workflow variables, substitution is done at Workflow run time.\n\tVars  map[string]string `json:\",omitempty\"`\n\tSteps map[string]*Step\n\t// Map of steps to their dependencies.\n\tDependencies map[string][]string\n\n\t// Working fields.\n\tworkflowDir   string\n\tdiskRefs      *refMap\n\tinstanceRefs  *refMap\n\timageRefs     *refMap\n\tparent        *Workflow\n\tbucket        string\n\tscratchPath   string\n\tsourcesPath   string\n\tlogsPath      string\n\toutsPath      string\n\tComputeClient *compute.Client `json:\"-\"`\n\tStorageClient *storage.Client `json:\"-\"`\n\tid            string\n\tlogger        *log.Logger\n}\n\n// Run runs a workflow.\nfunc (w *Workflow) Run() error {\n\tif err := w.validateRequiredFields(); err != nil {\n\t\tclose(w.Cancel)\n\t\treturn fmt.Errorf(\"error validating workflow: %v\", err)\n\t}\n\n\tif err := w.populate(); err != nil {\n\t\tclose(w.Cancel)\n\t\treturn fmt.Errorf(\"error populating workflow: %v\", err)\n\t}\n\n\tw.logger.Print(\"Validating workflow\")\n\tif err := w.validate(); err != nil {\n\t\tw.logger.Printf(\"Error validating workflow: %v\", err)\n\t\tclose(w.Cancel)\n\t\treturn err\n\t}\n\tdefer w.cleanup()\n\tw.logger.Print(\"Uploading sources\")\n\tif err := w.uploadSources(); err != nil {\n\t\tw.logger.Printf(\"Error uploading sources: %v\", err)\n\t\tclose(w.Cancel)\n\t\treturn err\n\t}\n\tw.logger.Print(\"Running workflow\")\n\tif err := w.run(); err != nil {\n\t\tw.logger.Printf(\"Error running workflow: %v\", err)\n\t\tclose(w.Cancel)\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (w *Workflow) String() string {\n\tf := \"{Name:%q Project:%q Zone:%q Bucket:%q OAuthPath:%q Sources:%s Vars:%s Steps:%s Dependencies:%s id:%q}\"\n\treturn fmt.Sprintf(f, w.Name, w.Project, w.Zone, w.bucket, w.OAuthPath, w.Sources, w.Vars, w.Steps, w.Dependencies, w.id)\n}\n\nfunc (w *Workflow) cleanup() {\n\tw.logger.Printf(\"Cleaning ephemeral resources for workflow %q.\", w.Name)\n\tw.cleanupHelper(w.imageRefs, w.deleteImage)\n\tw.cleanupHelper(w.instanceRefs, w.deleteInstance)\n\tw.cleanupHelper(w.diskRefs, w.deleteDisk)\n\n\t// SubWorkflows leave resources around if the workflow fails before the\n\t// SubWorkflows finish. To be safe, try to clean up SubWorkflows again.\n\tfor _, s := range w.Steps {\n\t\tif s.SubWorkflow != nil {\n\t\t\ts.SubWorkflow.workflow.cleanup()\n\t\t}\n\t}\n}\n\nfunc (w *Workflow) cleanupHelper(rm *refMap, deleteFn func(*resource) error) {\n\tvar wg sync.WaitGroup\n\ttoDel := map[string]*resource{}\n\tfor ref, res := range rm.m {\n\t\t// Delete only non-persistent resources.\n\t\tif !res.noCleanup {\n\t\t\ttoDel[ref] = res\n\t\t}\n\t}\n\tfor ref, res := range toDel {\n\t\twg.Add(1)\n\t\tgo func(ref string, r *resource) {\n\t\t\tdefer wg.Done()\n\t\t\tif err := deleteFn(r); err != nil {\n\t\t\t\tfmt.Println(err)\n\t\t\t}\n\t\t}(ref, res)\n\t}\n\twg.Wait()\n}\n\nfunc (w *Workflow) deleteDisk(r *resource) error {\n\tif err := w.ComputeClient.DeleteDisk(w.Project, w.Zone, r.real); err != nil {\n\t\treturn err\n\t}\n\tw.diskRefs.del(r.name)\n\treturn nil\n}\n\nfunc (w *Workflow) deleteImage(r *resource) error {\n\tif err := w.ComputeClient.DeleteImage(w.Project, r.real); err != nil {\n\t\treturn err\n\t}\n\tw.imageRefs.del(r.name)\n\treturn nil\n}\n\nfunc (w *Workflow) deleteInstance(r *resource) error {\n\tif err := w.ComputeClient.DeleteInstance(w.Project, w.Zone, r.real); err != nil {\n\t\treturn err\n\t}\n\tw.instanceRefs.del(r.name)\n\treturn nil\n}\n\nfunc (w *Workflow) genName(n string) string {\n\tprefix := fmt.Sprintf(\"%s-%s\", n, w.Name)\n\tif len(prefix) > 57 {\n\t\tprefix = prefix[0:56]\n\t}\n\tresult := fmt.Sprintf(\"%s-%s\", prefix, w.id)\n\tif len(result) > 64 {\n\t\tresult = result[0:63]\n\t}\n\treturn result\n}\n\nfunc (w *Workflow) getDisk(n string) (*resource, error) {\n\treturn w.getResourceHelper(n, func(name string, wf *Workflow) (*resource, bool) { return wf.diskRefs.get(n) })\n}\n\nfunc (w *Workflow) getImage(n string) (*resource, error) {\n\treturn w.getResourceHelper(n, func(name string, wf *Workflow) (*resource, bool) { return wf.imageRefs.get(n) })\n}\n\nfunc (w *Workflow) getInstance(n string) (*resource, error) {\n\treturn w.getResourceHelper(n, func(name string, wf *Workflow) (*resource, bool) { return wf.instanceRefs.get(n) })\n}\n\nfunc (w *Workflow) getResourceHelper(n string, f func(string, *Workflow) (*resource, bool)) (*resource, error) {\n\tfor cur := w; cur != nil; cur = cur.parent {\n\t\tif r, ok := f(n, cur); ok {\n\t\t\treturn r, nil\n\t\t}\n\t}\n\treturn nil, fmt.Errorf(\"unresolved instance reference %q\", n)\n}\n\nfunc (w *Workflow) nameToDiskLink(n string) string {\n\treturn fmt.Sprintf(\"projects/%s/zones/%s/disks/%s\", w.Project, w.Zone, n)\n}\n\nfunc (w *Workflow) nameToImageLink(n string) string {\n\treturn fmt.Sprintf(\"projects/%s/global/images/%s\", w.Project, n)\n}\n\nfunc (w *Workflow) nameToInstanceLink(n string) string {\n\treturn fmt.Sprintf(\"projects/%s/zones/%s/instances/%s\", w.Project, w.Zone, n)\n}\n\nfunc (w *Workflow) populateStep(step *Step) error {\n\tif step.Timeout == \"\" {\n\t\tstep.Timeout = defaultTimeout\n\t}\n\ttimeout, err := time.ParseDuration(step.Timeout)\n\tif err != nil {\n\t\treturn err\n\t}\n\tstep.timeout = timeout\n\n\t// Recurse on subworkflows.\n\tif step.SubWorkflow == nil {\n\t\treturn nil\n\t}\n\tstep.SubWorkflow.workflow.GCSPath = fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.scratchPath)\n\tstep.SubWorkflow.workflow.Project = w.Project\n\tstep.SubWorkflow.workflow.Zone = w.Zone\n\tstep.SubWorkflow.workflow.OAuthPath = w.OAuthPath\n\tstep.SubWorkflow.workflow.ComputeClient = w.ComputeClient\n\tstep.SubWorkflow.workflow.StorageClient = w.StorageClient\n\tstep.SubWorkflow.workflow.Ctx = w.Ctx\n\tstep.SubWorkflow.workflow.Cancel = w.Cancel\n\tfor k, v := range step.SubWorkflow.Vars {\n\t\tstep.SubWorkflow.workflow.Vars[k] = v\n\t}\n\treturn step.SubWorkflow.workflow.populate()\n}\n\nfunc (w *Workflow) populate() error {\n\tw.id = randString(5)\n\tnow := time.Now()\n\tdatestamp := now.Format(\"20060102\")\n\tdatetimestamp := now.Format(\"20060102150405\")\n\ttimestamp := strconv.FormatInt(now.Unix(), 10)\n\n\t// Do replacement from Vars.\n\tvars := map[string]string{}\n\tfor k, v := range w.Vars {\n\t\tvars[k] = v\n\t}\n\tvar replacements []string\n\tfor k, v := range vars {\n\t\treplacements = append(replacements, fmt.Sprintf(\"${%s}\", k), v)\n\t}\n\tsubstitute(reflect.ValueOf(w).Elem(), strings.NewReplacer(replacements...))\n\n\t// Set up GCS paths.\n\tbkt, p, err := splitGCSPath(w.GCSPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tw.bucket = bkt\n\tw.scratchPath = path.Join(p, fmt.Sprintf(\"daisy-%s-%s-%s\", w.Name, timestamp, w.id))\n\tw.sourcesPath = path.Join(w.scratchPath, \"sources\")\n\tw.logsPath = path.Join(w.scratchPath, \"logs\")\n\tw.outsPath = path.Join(w.scratchPath, \"outs\")\n\n\t// Do replacement for autovars. Autovars pull from workflow fields,\n\t// so Vars replacement must run before this to resolve the final\n\t// value for those fields.\n\n\tautovars := map[string]string{\n\t\t\"ID\":          w.id,\n\t\t\"NAME\":        w.Name,\n\t\t\"ZONE\":        w.Zone,\n\t\t\"PROJECT\":     w.Project,\n\t\t\"GCSPATH\":     w.GCSPath,\n\t\t\"DATE\":        datestamp,\n\t\t\"DATETIME\":    datetimestamp,\n\t\t\"TIMESTAMP\":   timestamp,\n\t\t\"SCRATCHPATH\": fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.scratchPath),\n\t\t\"SOURCESPATH\": fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.sourcesPath),\n\t\t\"LOGSPATH\":    fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.logsPath),\n\t\t\"OUTSPATH\":    fmt.Sprintf(\"gs://%s/%s\", w.bucket, w.outsPath),\n\t}\n\treplacements = []string{}\n\tfor k, v := range autovars {\n\t\treplacements = append(replacements, fmt.Sprintf(\"${%s}\", k), v)\n\t}\n\tsubstitute(reflect.ValueOf(w).Elem(), strings.NewReplacer(replacements...))\n\n\tif w.ComputeClient == nil {\n\t\tw.ComputeClient, err = compute.NewClient(w.Ctx, option.WithServiceAccountFile(w.OAuthPath))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif w.StorageClient == nil {\n\t\tw.StorageClient, err = storage.NewClient(w.Ctx, option.WithServiceAccountFile(w.OAuthPath))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tw.diskRefs = &refMap{m: map[string]*resource{}}\n\tw.imageRefs = &refMap{m: map[string]*resource{}}\n\tw.instanceRefs = &refMap{m: map[string]*resource{}}\n\n\tif w.logger == nil {\n\t\tgcs := &gcsLogger{client: w.StorageClient, bucket: w.bucket, object: path.Join(w.logsPath, \"daisy.log\"), ctx: w.Ctx}\n\t\tname := w.Name\n\t\tfor parent := w.parent; parent != nil; parent = w.parent.parent {\n\t\t\tname = parent.Name + \".\" + name\n\t\t}\n\t\tprefix := fmt.Sprintf(\"[%s]: \", name)\n\t\tflags := log.Ldate | log.Ltime\n\t\tlog.New(os.Stdout, prefix, flags).Println(\"Logs will be streamed to\", \"gs://\"+path.Join(w.bucket, w.logsPath, \"daisy.log\"))\n\t\tw.logger = log.New(io.MultiWriter(os.Stdout, gcs), prefix, flags)\n\t}\n\n\tfor name, s := range w.Steps {\n\t\ts.name = name\n\t\tif err := w.populateStep(s); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Print populates then pretty prints the workflow.\nfunc (w *Workflow) Print() {\n\tw.logger = log.New(ioutil.Discard, \"\", 0)\n\tif err := w.populate(); err != nil {\n\t\tfmt.Println(\"Error running populate:\", err)\n\t}\n\n\tb, err := json.MarshalIndent(w, \"\", \"  \")\n\tif err != nil {\n\t\tfmt.Println(\"Error marshalling workflow for printing:\", err)\n\t}\n\tfmt.Println(string(b))\n}\n\nfunc (w *Workflow) run() error {\n\treturn w.traverseDAG(func(s step) error {\n\t\treturn w.runStep(s.(*Step))\n\t})\n}\n\nfunc (w *Workflow) runStep(s *Step) error {\n\ttimeout := make(chan struct{})\n\tgo func() {\n\t\ttime.Sleep(s.timeout)\n\t\tclose(timeout)\n\t}()\n\n\te := make(chan error)\n\tgo func() {\n\t\te <- s.run(w)\n\t}()\n\n\tselect {\n\tcase err := <-e:\n\t\treturn err\n\tcase <-timeout:\n\t\treturn fmt.Errorf(\"step %q did not stop in specified timeout of %s\", s.name, s.timeout)\n\t}\n}\n\nfunc (w *Workflow) stepDepends(consumer, consumed string) bool {\n\tq := w.Dependencies[consumer]\n\tseen := map[string]bool{}\n\n\tfor i := 0; i < len(q); i++ {\n\t\tname := q[i]\n\t\tif seen[name] {\n\t\t\tcontinue\n\t\t}\n\t\tseen[name] = true\n\t\tif name == consumed {\n\t\t\treturn true\n\t\t}\n\t\tfor _, dep := range w.Dependencies[name] {\n\t\t\tq = append(q, dep)\n\t\t}\n\t}\n\n\treturn false\n}\n\n// Concurrently traverse the DAG, running func f on each step.\n// Return an error if f returns an error on any step.\nfunc (w *Workflow) traverseDAG(f func(step) error) error {\n\t// waiting = steps and the dependencies they are waiting for.\n\t// running = the currently running steps.\n\t// start = map of steps' start channels/semaphores.\n\t// done = map of steps' done channels for signaling step completion.\n\twaiting := map[string][]string{}\n\tvar running []string\n\tstart := map[string]chan error{}\n\tdone := map[string]chan error{}\n\n\t// Setup: channels, copy dependencies.\n\tfor name := range w.Steps {\n\t\twaiting[name] = w.Dependencies[name]\n\t\tstart[name] = make(chan error)\n\t\tdone[name] = make(chan error)\n\t}\n\t// Setup: goroutine for each step. Each waits to be notified to start.\n\tfor name, s := range w.Steps {\n\t\tgo func(name string, s *Step) {\n\t\t\t// Wait for signal, then run the function. Return any errs.\n\t\t\tif err := <-start[name]; err != nil {\n\t\t\t\tdone[name] <- err\n\t\t\t} else if err := f(s); err != nil {\n\t\t\t\tdone[name] <- err\n\t\t\t}\n\t\t\tclose(done[name])\n\t\t}(name, s)\n\t}\n\n\t// Main signaling logic.\n\tfor len(waiting) != 0 || len(running) != 0 {\n\t\t// If we got a Cancel signal, kill all waiting steps.\n\t\t// Let running steps finish.\n\t\tselect {\n\t\tcase <-w.Cancel:\n\t\t\twaiting = map[string][]string{}\n\t\tdefault:\n\t\t}\n\n\t\t// Kick off all steps that aren't waiting for anything.\n\t\tfor name, deps := range waiting {\n\t\t\tif len(deps) == 0 {\n\t\t\t\tdelete(waiting, name)\n\t\t\t\trunning = append(running, name)\n\t\t\t\tclose(start[name])\n\t\t\t}\n\t\t}\n\n\t\t// Sanity check. There should be at least one running step,\n\t\t// but loop back through if there isn't.\n\t\tif len(running) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Get next finished step. Return the step error if it erred.\n\t\tfinished, err := stepsListen(running, done)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Remove finished step from other steps' waiting lists.\n\t\tfor name, deps := range waiting {\n\t\t\twaiting[name] = filter(deps, finished)\n\t\t}\n\n\t\t// Remove finished from currently running list.\n\t\trunning = filter(running, finished)\n\t}\n\treturn nil\n}\n\n// New instantiates a new workflow.\nfunc New(ctx context.Context) *Workflow {\n\tvar w Workflow\n\tw.Ctx = ctx\n\t// We can't use context.WithCancel as we use the context even after cancel for cleanup.\n\tw.Cancel = make(chan struct{})\n\treturn &w\n}\n\n// NewFromFile reads and unmarshals a workflow file.\n// Recursively reads subworkflow steps as well.\nfunc NewFromFile(ctx context.Context, file string) (*Workflow, error) {\n\tw := New(ctx)\n\tdata, err := ioutil.ReadFile(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tw.workflowDir, err = filepath.Abs(filepath.Dir(file))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := json.Unmarshal(data, &w); err != nil {\n\t\t// If this is a syntax error return a useful error.\n\t\tsErr, ok := err.(*json.SyntaxError)\n\t\tif !ok {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// Byte number where the error line starts.\n\t\tstart := bytes.LastIndex(data[:sErr.Offset], []byte(\"\\n\")) + 1\n\t\t// Assume end byte of error line is EOF unless this isn't the last line.\n\t\tend := len(data)\n\t\tif i := bytes.Index(data[start:], []byte(\"\\n\")); i >= 0 {\n\t\t\tend = start + i\n\t\t}\n\n\t\t// Line number of error.\n\t\tline := bytes.Count(data[:start], []byte(\"\\n\")) + 1\n\t\t// Position of error in line (where to place the '^').\n\t\tpos := int(sErr.Offset) - start - 1\n\n\t\treturn nil, fmt.Errorf(\"%s: JSON syntax error in line %d: %s \\n%s\\n%s^\", file, line, err, data[start:end], strings.Repeat(\" \", pos))\n\t}\n\n\tif w.OAuthPath != \"\" && !filepath.IsAbs(w.OAuthPath) {\n\t\tw.OAuthPath = filepath.Join(w.workflowDir, w.OAuthPath)\n\t}\n\n\t// We need to unmarshal any SubWorkflows.\n\tfor name, step := range w.Steps {\n\t\tstep.name = name\n\n\t\tif step.SubWorkflow == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tswPath := step.SubWorkflow.Path\n\t\tif !filepath.IsAbs(swPath) {\n\t\t\tswPath = filepath.Join(w.workflowDir, swPath)\n\t\t}\n\n\t\tsw, err := NewFromFile(w.Ctx, swPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstep.SubWorkflow.workflow = sw\n\t\tsw.parent = w\n\t}\n\n\treturn w, nil\n}\n\n// stepsListen returns the first step that finishes/errs.\nfunc stepsListen(names []string, chans map[string]chan error) (string, error) {\n\tcases := make([]reflect.SelectCase, len(names))\n\tfor i, name := range names {\n\t\tcases[i] = reflect.SelectCase{Dir: reflect.SelectRecv, Chan: reflect.ValueOf(chans[name])}\n\t}\n\tcaseIndex, value, recvOk := reflect.Select(cases)\n\tname := names[caseIndex]\n\tif recvOk {\n\t\t// recvOk -> a step failed, return the error.\n\t\treturn name, value.Interface().(error)\n\t}\n\treturn name, nil\n}\n", "idx": 9, "id": 6435, "msg": "", "proj": "GoogleCloudPlatform-compute-image-tools", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -187,21 +187,7 @@ static void update_annotations (flux_t *h, struct job *job, flux_jobid_t id,\n                           __FUNCTION__, (uintmax_t)id);\n         }\n         if (job->annotations) {\n-            const char *key;\n-            json_t *value;\n-\n-            json_object_foreach (annotations, key, value) {\n-                if (!json_is_null (value)) {\n-                    if (json_object_set (job->annotations, key, value) < 0)\n-                        flux_log (h,\n-                                  LOG_ERR,\n-                                  \"%s: id=%ju update key=%s\",\n-                                  __FUNCTION__, (uintmax_t)id, key);\n-                }\n-                else\n-                    /* not an error if key doesn't exist in job->annotations */\n-                    (void)json_object_del (job->annotations, key);\n-            }\n+            update_recursive (h, job, job->annotations, annotations);\n             /* Special case: if user cleared all entries, assume we no\n              * longer need annotations object */\n             if (!json_object_size (job->annotations))", "y": 0, "oldf": "/************************************************************\\\n * Copyright 2019 Lawrence Livermore National Security, LLC\n * (c.f. AUTHORS, NOTICE.LLNS, COPYING)\n *\n * This file is part of the Flux resource manager framework.\n * For details, see https://github.com/flux-framework.\n *\n * SPDX-License-Identifier: LGPL-3.0\n\\************************************************************/\n\n/* alloc.c - scheduler interface\n *\n * Please refer to RFC27 for scheduler protocol\n *\n * TODO:\n * - implement flow control (credit based?) interface mode\n * - handle post alloc request job priority change\n */\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n#include <jansson.h>\n#include <czmq.h>\n#include <flux/core.h>\n#include <flux/schedutil.h>\n#include <assert.h>\n\n#include \"job.h\"\n#include \"alloc.h\"\n#include \"event.h\"\n#include \"drain.h\"\n\ntypedef enum {\n    SCHED_SINGLE,       // only allow one outstanding sched.alloc request\n    SCHED_UNLIMITED,    // send all sched.alloc requests immediately\n} sched_interface_t;\n\nstruct alloc {\n    struct job_manager *ctx;\n    flux_msg_handler_t **handlers;\n    zlistx_t *queue;\n    sched_interface_t mode;\n    bool ready;\n    bool disable;\n    char *disable_reason;\n    flux_watcher_t *prep;\n    flux_watcher_t *check;\n    flux_watcher_t *idle;\n    unsigned int alloc_pending_count; // for mode=single, max of 1\n    unsigned int free_pending_count;\n};\n\nstatic void clear_annotations (struct job *job)\n{\n    if (job->annotations) {\n        json_decref (job->annotations);\n        job->annotations = NULL;\n    }\n}\n\n/* Initiate teardown.  Clear any alloc/free requests, and clear\n * the alloc->ready flag to stop prep/check from allocating.\n */\nstatic void interface_teardown (struct alloc *alloc, char *s, int errnum)\n{\n    if (alloc->ready) {\n        struct job *job;\n        struct job_manager *ctx = alloc->ctx;\n\n        flux_log (ctx->h, LOG_DEBUG, \"alloc: stop due to %s: %s\",\n                  s, flux_strerror (errnum));\n\n        job = zhashx_first (ctx->active_jobs);\n        while (job) {\n            /* jobs with alloc request pending need to go back in the queue\n             * so they will automatically send alloc again.\n             */\n            if (job->alloc_pending) {\n                bool fwd = job->priority > FLUX_JOB_PRIORITY_DEFAULT ? true\n                                                                     : false;\n\n                assert (job->handle == NULL);\n                if (!(job->handle = zlistx_insert (alloc->queue, job, fwd)))\n                    flux_log_error (ctx->h, \"%s: queue_insert\", __FUNCTION__);\n                job->alloc_pending = 0;\n                job->alloc_queued = 1;\n                clear_annotations (job);\n            }\n            /* jobs with free request pending (much smaller window for this\n             * to be true) need to be picked up again after 'ready'.\n             */\n            job->free_pending = 0;\n            job = zhashx_next (ctx->active_jobs);\n        }\n        alloc->ready = false;\n        alloc->alloc_pending_count = 0;\n        alloc->free_pending_count = 0;\n        drain_check (alloc->ctx->drain);\n    }\n}\n\n/* Handle a sched.free response.\n */\nstatic void free_response_cb (flux_t *h, flux_msg_handler_t *mh,\n                              const flux_msg_t *msg, void *arg)\n{\n    struct job_manager *ctx = arg;\n    flux_jobid_t id = 0;\n    struct job *job;\n\n    if (flux_response_decode (msg, NULL, NULL) < 0)\n        goto teardown;\n    if (flux_msg_unpack (msg, \"{s:I}\", \"id\", &id) < 0)\n        goto teardown;\n    if (!(job = zhashx_lookup (ctx->active_jobs, &id))) {\n        flux_log (h, LOG_ERR, \"sched.free-response: id=%ju not active\",\n                  (uintmax_t)id);\n        errno = EINVAL;\n        goto teardown;\n    }\n    if (!job->has_resources) {\n        flux_log (h, LOG_ERR, \"sched.free-response: id=%ju not allocated\",\n                  (uintmax_t)id);\n        errno = EINVAL;\n        goto teardown;\n    }\n    job->free_pending = 0;\n    ctx->alloc->free_pending_count--;\n    if (event_job_post_pack (ctx->event, job, \"free\", NULL) < 0)\n        goto teardown;\n    return;\nteardown:\n    interface_teardown (ctx->alloc, \"free response error\", errno);\n}\n\n/* Send sched.free request for job.\n * Update flags.\n */\nint free_request (struct alloc *alloc, struct job *job)\n{\n    flux_msg_t *msg;\n\n    if (!(msg = flux_request_encode (\"sched.free\", NULL)))\n        return -1;\n    if (flux_msg_pack (msg, \"{s:I}\", \"id\", job->id) < 0)\n        goto error;\n    if (flux_send (alloc->ctx->h, msg, 0) < 0)\n        goto error;\n    flux_msg_destroy (msg);\n    return 0;\nerror:\n    flux_msg_destroy (msg);\n    return -1;\n}\n\n/* Send sched.cancel request for job.\n*/\nint cancel_request (struct alloc *alloc, struct job *job)\n{\n    flux_future_t *f;\n    flux_t *h = alloc->ctx->h;\n\n    if (!(f = flux_rpc_pack (h,\n                             \"sched.cancel\",\n                             FLUX_NODEID_ANY,\n                             FLUX_RPC_NORESPONSE,\n                             \"{s:I}\",\n                             \"id\",\n                             job->id))) {\n        flux_log_error (h, \"sending sched.cancel id=%ju\", (uintmax_t)job->id);\n        return -1;\n    }\n    flux_future_destroy (f);\n    return 0;\n}\n\nstatic void update_annotations (flux_t *h, struct job *job, flux_jobid_t id,\n                                json_t *annotations)\n{\n    if (annotations) {\n        if (!job->annotations) {\n            if (!(job->annotations = json_object ()))\n                flux_log (h,\n                          LOG_ERR,\n                          \"%s: id=%ju json_object\",\n                          __FUNCTION__, (uintmax_t)id);\n        }\n        if (job->annotations) {\n            const char *key;\n            json_t *value;\n\n            json_object_foreach (annotations, key, value) {\n                if (!json_is_null (value)) {\n                    if (json_object_set (job->annotations, key, value) < 0)\n                        flux_log (h,\n                                  LOG_ERR,\n                                  \"%s: id=%ju update key=%s\",\n                                  __FUNCTION__, (uintmax_t)id, key);\n                }\n                else\n                    /* not an error if key doesn't exist in job->annotations */\n                    (void)json_object_del (job->annotations, key);\n            }\n            /* Special case: if user cleared all entries, assume we no\n             * longer need annotations object */\n            if (!json_object_size (job->annotations))\n                clear_annotations (job);\n        }\n    }\n}\n\n/* Handle a sched.alloc response.\n * Update flags.\n */\nstatic void alloc_response_cb (flux_t *h, flux_msg_handler_t *mh,\n                               const flux_msg_t *msg, void *arg)\n{\n    struct job_manager *ctx = arg;\n    struct alloc *alloc = ctx->alloc;\n    flux_jobid_t id;\n    int type;\n    char *note = NULL;\n    json_t *annotations = NULL;\n    struct job *job;\n\n    if (flux_response_decode (msg, NULL, NULL) < 0)\n        goto teardown; // ENOSYS here if scheduler not loaded/shutting down\n    if (flux_msg_unpack (msg, \"{s:I s:i s?:s s?:o}\",\n                              \"id\", &id,\n                              \"type\", &type,\n                              \"note\", &note,\n                              \"annotations\", &annotations) < 0)\n        goto teardown;\n    if (!(job = zhashx_lookup (ctx->active_jobs, &id))) {\n        flux_log (h, LOG_ERR, \"sched.alloc-response: id=%ju not active\",\n                  (uintmax_t)id);\n        errno = EINVAL;\n        goto teardown;\n    }\n    if (!job->alloc_pending) {\n        flux_log (h, LOG_ERR, \"sched.alloc-response: id=%ju not requested\",\n                  (uintmax_t)id);\n        errno = EINVAL;\n        goto teardown;\n    }\n    switch (type) {\n    case FLUX_SCHED_ALLOC_SUCCESS:\n        alloc->alloc_pending_count--;\n        job->alloc_pending = 0;\n        if (job->has_resources) {\n            flux_log (h,\n                      LOG_ERR,\n                      \"sched.alloc-response: id=%ju already allocated\",\n                      (uintmax_t)id);\n            errno = EEXIST;\n            goto teardown;\n        }\n        update_annotations (h, job, id, annotations);\n        if (job->annotations) {\n            if (event_job_post_pack (ctx->event, job, \"alloc\",\n                                     \"{ s:O }\",\n                                     \"annotations\", job->annotations) < 0)\n                goto teardown;\n        }\n        else {\n            if (event_job_post_pack (ctx->event, job, \"alloc\", NULL) < 0)\n                goto teardown;\n        }\n        break;\n    case FLUX_SCHED_ALLOC_ANNOTATE: // annotation\n        if (!annotations) {\n            errno = EPROTO;\n            goto teardown;\n        }\n        update_annotations (h, job, id, annotations);\n        break;\n    case FLUX_SCHED_ALLOC_DENY: // error\n        alloc->alloc_pending_count--;\n        job->alloc_pending = 0;\n        clear_annotations (job);\n        if (event_job_post_pack (ctx->event, job, \"exception\",\n                                 \"{ s:s s:i s:i s:s }\",\n                                 \"type\", \"alloc\",\n                                 \"severity\", 0,\n                                 \"userid\", FLUX_USERID_UNKNOWN,\n                                 \"note\", note ? note : \"\") < 0)\n            goto teardown;\n        break;\n    case FLUX_SCHED_ALLOC_CANCEL:\n        alloc->alloc_pending_count--;\n        job->alloc_pending = 0;\n        clear_annotations (job);\n        if (event_job_action (ctx->event, job) < 0) {\n            flux_log_error (h,\n                            \"event_job_action id=%ju on alloc cancel\",\n                            (uintmax_t)id);\n            goto teardown;\n        }\n        drain_check (alloc->ctx->drain);\n        break;\n    default:\n        errno = EINVAL;\n        goto teardown;\n    }\n    return;\nteardown:\n    interface_teardown (alloc, \"alloc response error\", errno);\n}\n\n/* Send sched.alloc request for job.\n * Update flags.\n */\nint alloc_request (struct alloc *alloc, struct job *job)\n{\n    flux_msg_t *msg;\n\n    if (!(msg = flux_request_encode (\"sched.alloc\", NULL)))\n        return -1;\n    if (flux_msg_pack (msg, \"{s:I s:i s:i s:f}\",\n                            \"id\", job->id,\n                            \"priority\", job->priority,\n                            \"userid\", job->userid,\n                            \"t_submit\", job->t_submit) < 0)\n        goto error;\n    if (flux_send (alloc->ctx->h, msg, 0) < 0)\n        goto error;\n    flux_msg_destroy (msg);\n    return 0;\nerror:\n    flux_msg_destroy (msg);\n    return -1;\n}\n\n/* sched-hello:\n * Scheduler obtains a list of jobs that have resources allocated.\n */\nstatic void hello_cb (flux_t *h, flux_msg_handler_t *mh,\n                      const flux_msg_t *msg, void *arg)\n{\n    struct job_manager *ctx = arg;\n    struct job *job;\n    json_t *o = NULL;\n    json_t *entry;\n\n    if (flux_request_decode (msg, NULL, NULL) < 0)\n        goto error;\n    flux_log (h, LOG_DEBUG, \"scheduler: hello\");\n    if (!(o = json_array ()))\n        goto nomem;\n    job = zhashx_first (ctx->active_jobs);\n    while (job) {\n        if (job->has_resources) {\n            if (!(entry = json_pack (\"{s:I s:i s:i s:f}\",\n                                     \"id\", job->id,\n                                     \"priority\", job->priority,\n                                     \"userid\", job->userid,\n                                     \"t_submit\", job->t_submit)))\n                goto nomem;\n            if (json_array_append_new (o, entry) < 0) {\n                json_decref (entry);\n                goto nomem;\n            }\n        }\n        job = zhashx_next (ctx->active_jobs);\n    }\n    if (flux_respond_pack (h, msg, \"{s:O}\", \"alloc\", o) < 0)\n        flux_log_error (h, \"%s: flux_respond_pack\", __FUNCTION__);\n    json_decref (o);\n    return;\nnomem:\n    errno = ENOMEM;\nerror:\n    if (flux_respond_error (h, msg, errno, NULL) < 0)\n        flux_log_error (h, \"%s: flux_respond_error\", __FUNCTION__);\n    json_decref (o);\n}\n\n/* sched-ready:\n * Scheduler indicates what style of alloc concurrency is requires,\n * and tells job-manager to start allocations.  job-manager tells\n * scheduler how many jobs are in the queue.\n */\nstatic void ready_cb (flux_t *h, flux_msg_handler_t *mh,\n                      const flux_msg_t *msg, void *arg)\n{\n    struct job_manager *ctx = arg;\n    const char *mode;\n    int count;\n    struct job *job;\n\n    if (flux_request_unpack (msg, NULL, \"{s:s}\", \"mode\", &mode) < 0)\n        goto error;\n    if (!strcmp (mode, \"single\"))\n        ctx->alloc->mode = SCHED_SINGLE;\n    else if (!strcmp (mode, \"unlimited\"))\n        ctx->alloc->mode = SCHED_UNLIMITED;\n    else {\n        errno = EPROTO;\n        goto error;\n    }\n    ctx->alloc->ready = true;\n    flux_log (h, LOG_DEBUG, \"scheduler: ready %s\", mode);\n    count = zlistx_size (ctx->alloc->queue);\n    if (flux_respond_pack (h, msg, \"{s:i}\", \"count\", count) < 0)\n        flux_log_error (h, \"%s: flux_respond_pack\", __FUNCTION__);\n    /* Restart any free requests that might have been interrupted\n     * when scheduler was last unloaded.\n     */\n    job = zhashx_first (ctx->active_jobs);\n    while (job) {\n        /* N.B. first/next are NOT deletion safe but event_job_action()\n         * won't call zhashx_delete() for jobs in FLUX_JOB_CLEANUP state.\n         */\n        if (job->state == FLUX_JOB_CLEANUP && job->has_resources) {\n            if (event_job_action (ctx->event, job) < 0)\n                flux_log_error (h, \"%s: event_job_action\", __FUNCTION__);\n        }\n        job = zhashx_next (ctx->active_jobs);\n    }\n    return;\nerror:\n    if (flux_respond_error (h, msg, errno, NULL) < 0)\n        flux_log_error (h, \"%s: flux_respond_error\", __FUNCTION__);\n}\n\n/* prep:\n * Runs right before reactor calls poll(2).\n * If a job can be scheduled, start idle watcher.\n */\nstatic void prep_cb (flux_reactor_t *r, flux_watcher_t *w,\n                     int revents, void *arg)\n{\n    struct job_manager *ctx = arg;\n    struct alloc *alloc = ctx->alloc;\n\n    if (!alloc->ready || alloc->disable)\n        return;\n    if (alloc->mode == SCHED_SINGLE && alloc->alloc_pending_count > 0)\n        return;\n    if (zlistx_first (alloc->queue))\n        flux_watcher_start (alloc->idle);\n}\n\n/* check:\n * Runs right after reactor calls poll(2).\n * Stop idle watcher, and send next alloc request, if available.\n */\nstatic void check_cb (flux_reactor_t *r, flux_watcher_t *w,\n                      int revents, void *arg)\n{\n    struct job_manager *ctx = arg;\n    struct alloc *alloc = ctx->alloc;\n    struct job *job;\n\n    flux_watcher_stop (alloc->idle);\n    if (!alloc->ready || alloc->disable)\n        return;\n    if (alloc->mode == SCHED_SINGLE && alloc->alloc_pending_count > 0)\n        return;\n    if ((job = zlistx_first (alloc->queue))) {\n        if (alloc_request (alloc, job) < 0) {\n            flux_log_error (ctx->h, \"alloc_request fatal error\");\n            flux_reactor_stop_error (flux_get_reactor (ctx->h));\n            return;\n        }\n        zlistx_delete (alloc->queue, job->handle);\n        job->handle = NULL;\n        job->alloc_pending = 1;\n        job->alloc_queued = 0;\n        alloc->alloc_pending_count++;\n        if ((job->flags & FLUX_JOB_DEBUG))\n            (void)event_job_post_pack (ctx->event, job,\n                                       \"debug.alloc-request\", NULL);\n\n    }\n}\n\n/* called from event_job_action() FLUX_JOB_CLEANUP */\nint alloc_send_free_request (struct alloc *alloc, struct job *job)\n{\n    assert (job->state == FLUX_JOB_CLEANUP);\n    if (!job->free_pending && alloc->ready) {\n        if (free_request (alloc, job) < 0)\n            return -1;\n        job->free_pending = 1;\n        if ((job->flags & FLUX_JOB_DEBUG))\n            (void)event_job_post_pack (alloc->ctx->event, job,\n                                       \"debug.free-request\", NULL);\n        alloc->free_pending_count++;\n    }\n    return 0;\n}\n\n/* called from event_job_action() FLUX_JOB_SCHED */\nint alloc_enqueue_alloc_request (struct alloc *alloc, struct job *job)\n{\n    assert (job->state == FLUX_JOB_SCHED);\n    if (!job->alloc_queued && !job->alloc_pending) {\n        bool fwd = job->priority > FLUX_JOB_PRIORITY_DEFAULT ? true : false;\n        assert (job->handle == NULL);\n        if (!(job->handle = zlistx_insert (alloc->queue, job, fwd)))\n            return -1;\n        job->alloc_queued = 1;\n    }\n    return 0;\n}\n\n/* called from event_job_action() FLUX_JOB_CLEANUP */\nvoid alloc_dequeue_alloc_request (struct alloc *alloc, struct job *job)\n{\n    if (job->alloc_queued) {\n        zlistx_delete (alloc->queue, job->handle);\n        job->handle = NULL;\n        job->alloc_queued = 0;\n    }\n}\n\n/* called from event_job_action() FLUX_JOB_CLEANUP */\nint alloc_cancel_alloc_request (struct alloc *alloc, struct job *job)\n{\n    if (job->alloc_pending) {\n        if (cancel_request (alloc, job) < 0)\n            return -1;\n    }\n    return 0;\n}\n\n/* called from list_handle_request() */\nstruct job *alloc_queue_first (struct alloc *alloc)\n{\n    return zlistx_first (alloc->queue);\n}\n\nstruct job *alloc_queue_next (struct alloc *alloc)\n{\n    return zlistx_next (alloc->queue);\n}\n\n/* called from priority_handle_request() */\nvoid alloc_queue_reorder (struct alloc *alloc, struct job *job)\n{\n    bool fwd = job->priority > FLUX_JOB_PRIORITY_DEFAULT ? true : false;\n\n    zlistx_reorder (alloc->queue, job->handle, fwd);\n}\n\nint alloc_pending_count (struct alloc *alloc)\n{\n    return alloc->alloc_pending_count;\n}\n\n/* Cancel all pending alloc requests in preparation for disabling\n * resource allocation.\n */\nstatic void cancel_all_pending (struct alloc *alloc)\n{\n    if (alloc->alloc_pending_count > 0) {\n        struct job *job;\n\n        job = zhashx_first (alloc->ctx->active_jobs);\n        while (job) {\n            if (job->alloc_pending)\n                cancel_request (alloc, job);\n            job = zhashx_next (alloc->ctx->active_jobs);\n        }\n    }\n}\n\n/* Control resource allocation (query/start/stop).\n * If 'query_only' is true, report allocaction status without altering it.\n * Otherwise update the alloc->disable flag, and for disable only,\n * optionally set alloc->disable_reason.\n *\n * What it means to be administratively disabled:\n * While allocation is disabled, the scheduler can remain loaded and handle\n * requests, but the job manager won't send any more allocation requests.\n * Pending alloc requests are canceled (jobs remain in SCHED state and\n * return to alloc->queue).  The job manager continues to send free requests\n * to the scheduler as jobs relinquish resources.\n *\n * If allocation is adminstratively enabled, but the scheduler is not loaded,\n * the current state is reported as disabled with reason \"Scheduler is offline\".\n */\nstatic void alloc_admin_cb (flux_t *h,\n                            flux_msg_handler_t *mh,\n                            const flux_msg_t *msg,\n                            void *arg)\n{\n    struct job_manager *ctx = arg;\n    struct alloc *alloc = ctx->alloc;\n    int query_only;\n    int enable;\n    const char *reason = NULL;\n\n    if (flux_request_unpack (msg,\n                             NULL,\n                             \"{s:b s:b s?:s}\",\n                             \"query_only\",\n                             &query_only,\n                             \"enable\",\n                             &enable,\n                             \"reason\",\n                             &reason) < 0)\n        goto error;\n    if (!query_only) {\n        if (!enable) {\n            char *cpy = NULL;\n            if (reason && strlen (reason) > 0 && !(cpy = strdup (reason)))\n                goto error;\n            free (alloc->disable_reason);\n            alloc->disable_reason = cpy;\n            cancel_all_pending (alloc);\n        }\n        alloc->disable = enable ? false : true;\n    }\n    if (alloc->disable) { // administratively disabled\n        enable = 0;\n        reason  = alloc->disable_reason;\n    }\n    else if (!alloc->ready) { // scheduler not loaded (waiting for hello)\n        enable = 0;\n        reason = \"Scheduler is offline\";\n    }\n    else { // condtion normal\n        enable = 1;\n        reason = NULL;\n    }\n    if (flux_respond_pack (h,\n                           msg,\n                           \"{s:b s:s s:i s:i s:i s:i}\",\n                           \"enable\",\n                           enable,\n                           \"reason\",\n                           reason ? reason : \"\",\n                           \"queue_length\",\n                           zlistx_size (alloc->queue),\n                           \"alloc_pending\",\n                           alloc->alloc_pending_count,\n                           \"free_pending\",\n                           alloc->free_pending_count,\n                           \"running\",\n                           alloc->ctx->running_jobs) < 0)\n        flux_log_error (h, \"%s: flux_respond\", __FUNCTION__);\n    return;\nerror:\n    if (flux_respond_error (h, msg, errno, NULL) < 0)\n        flux_log_error (h, \"%s: flux_respond_error\", __FUNCTION__);\n}\n\nvoid alloc_ctx_destroy (struct alloc *alloc)\n{\n    if (alloc) {\n        int saved_errno = errno;;\n        flux_msg_handler_delvec (alloc->handlers);\n        flux_watcher_destroy (alloc->prep);\n        flux_watcher_destroy (alloc->check);\n        flux_watcher_destroy (alloc->idle);\n        zlistx_destroy (&alloc->queue);\n        free (alloc->disable_reason);\n        free (alloc);\n        errno = saved_errno;\n    }\n}\n\nstatic const struct flux_msg_handler_spec htab[] = {\n    { FLUX_MSGTYPE_REQUEST,  \"job-manager.sched-hello\", hello_cb, 0},\n    { FLUX_MSGTYPE_REQUEST,  \"job-manager.sched-ready\", ready_cb, 0},\n    { FLUX_MSGTYPE_REQUEST,  \"job-manager.alloc-admin\", alloc_admin_cb, 0},\n    { FLUX_MSGTYPE_RESPONSE, \"sched.alloc\", alloc_response_cb, 0},\n    { FLUX_MSGTYPE_RESPONSE, \"sched.free\", free_response_cb, 0},\n    FLUX_MSGHANDLER_TABLE_END,\n};\n\nstruct alloc *alloc_ctx_create (struct job_manager *ctx)\n{\n    struct alloc *alloc;\n    flux_reactor_t *r = flux_get_reactor (ctx->h);\n\n    if (!(alloc = calloc (1, sizeof (*alloc))))\n        return NULL;\n    alloc->ctx = ctx;\n    if (!(alloc->queue = zlistx_new()))\n        goto error;\n    zlistx_set_destructor (alloc->queue, job_destructor);\n    zlistx_set_comparator (alloc->queue, job_comparator);\n    zlistx_set_duplicator (alloc->queue, job_duplicator);\n\n    if (flux_msg_handler_addvec (ctx->h, htab, ctx, &alloc->handlers) < 0)\n        goto error;\n    alloc->prep = flux_prepare_watcher_create (r, prep_cb, ctx);\n    alloc->check = flux_check_watcher_create (r, check_cb, ctx);\n    alloc->idle = flux_idle_watcher_create (r, NULL, NULL);\n    if (!alloc->prep || !alloc->check || !alloc->idle) {\n        errno = ENOMEM;\n        goto error;\n    }\n    flux_watcher_start (alloc->prep);\n    flux_watcher_start (alloc->check);\n    return alloc;\nerror:\n    alloc_ctx_destroy (alloc);\n    return NULL;\n}\n\n/*\n * vi:tabstop=4 shiftwidth=4 expandtab\n */\n", "idx": 2, "id": 28265, "msg": "", "proj": "flux-framework-flux-core", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -1,8 +1,4 @@\n-import {\n-  variant,\n-  textStyle,\n-  colorStyle,\n-} from '../src'\n+import { variant, textStyle, colorStyle } from '../src'\n import { system, compose } from '@styled-system/core'\n \n const theme = {", "y": 0, "oldf": "import {\n  variant,\n  textStyle,\n  colorStyle,\n} from '../src'\nimport { system, compose } from '@styled-system/core'\n\nconst theme = {\n  colors: {\n    blue: '#07c',\n    black: '#111',\n  },\n}\n\nconst fontSize = system({ fontSize: true })\nconst color = system({ color: true })\n\ntest('variant returns style objects from theme', () => {\n  const buttons = variant({ key: 'buttons' })\n  const a = buttons({\n    theme: {\n      buttons: {\n        primary: {\n          padding: '32px',\n          backgroundColor: 'tomato',\n        },\n      },\n    },\n    variant: 'primary',\n  })\n  expect(a).toEqual({\n    padding: '32px',\n    backgroundColor: 'tomato',\n  })\n})\n\ntest('variant prop can be customized', () => {\n  const buttons = variant({ key: 'buttons', prop: 'type' })\n  const a = buttons({\n    theme: {\n      buttons: {\n        primary: {\n          padding: '32px',\n          backgroundColor: 'tomato',\n        },\n      },\n    },\n    type: 'primary',\n  })\n  expect(a).toEqual({\n    padding: '32px',\n    backgroundColor: 'tomato',\n  })\n})\n\ntest('variant can be composed', () => {\n  const system = compose(\n    variant({ key: 'typography' }),\n    fontSize,\n    color\n  )\n  const result = system({\n    theme: {\n      typography: {\n        primary: {\n          fontSize: '32px',\n          color: '#fff',\n        },\n      },\n    },\n    variant: 'primary',\n    color: '#111',\n  })\n  expect(result).toEqual({\n    fontSize: '32px',\n    color: '#111',\n  })\n})\n\ntest('textStyle prop returns theme.textStyles object', () => {\n  const a = textStyle({\n    theme: {\n      textStyles: {\n        heading: {\n          fontWeight: 'bold',\n          lineHeight: 1.25,\n        },\n      },\n    },\n    textStyle: 'heading',\n  })\n  expect(a).toEqual({\n    fontWeight: 'bold',\n    lineHeight: 1.25,\n  })\n})\n\ntest('colors prop returns theme.colorStyles object', () => {\n  const a = colorStyle({\n    theme: {\n      colorStyles: {\n        dark: {\n          color: '#fff',\n          backgroundColor: '#000',\n        },\n      },\n    },\n    colors: 'dark',\n  })\n  expect(a).toEqual({\n    color: '#fff',\n    backgroundColor: '#000',\n  })\n})\n\n", "idx": 1, "id": 5227, "msg": "", "proj": "styled-system-styled-system", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -23,9 +23,9 @@ namespace OpenTelemetry.Shims.OpenTracing.Tests\n     /// <summary>\n     /// A mock ISpan implementation for unit tests. Sometimes an actual Mock is just easier to deal with than objects created with Moq.\n     /// </summary>\n-    internal class SpanMock : ISpan, IDisposable\n+    internal class SpanMock : TelemetrySpan, IDisposable\n     {\n-        private static readonly ReadOnlyDictionary<string, object> EmptyAttributes = new ReadOnlyDictionary<string, object>(new Dictionary<string, object>());\n+        private Status status;\n \n         public SpanMock(SpanContext spanContext)\n         {", "y": 0, "oldf": "\ufeff// <copyright file=\"SpanMock.cs\" company=\"OpenTelemetry Authors\">\n// Copyright 2018, OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n// </copyright>\nusing System;\nusing System.Collections.Generic;\nusing System.Collections.ObjectModel;\nusing OpenTelemetry.Trace;\n\nnamespace OpenTelemetry.Shims.OpenTracing.Tests\n{\n    /// <summary>\n    /// A mock ISpan implementation for unit tests. Sometimes an actual Mock is just easier to deal with than objects created with Moq.\n    /// </summary>\n    internal class SpanMock : ISpan, IDisposable\n    {\n        private static readonly ReadOnlyDictionary<string, object> EmptyAttributes = new ReadOnlyDictionary<string, object>(new Dictionary<string, object>());\n\n        public SpanMock(SpanContext spanContext)\n        {\n            this.Context = spanContext;\n            this.Events = new List<Event>();\n            this.Links = new List<Link>();\n            this.Attributes = new List<KeyValuePair<string, object>>();\n        }\n\n        public string Name { get; private set; }\n\n        public List<Event> Events { get; }\n\n        public List<Link> Links { get; }\n\n        public List<KeyValuePair<string, object>> Attributes { get; }\n\n        public SpanContext Context { get; private set; }\n\n        public bool IsRecording { get; private set; }\n\n        public Status Status { get; set; }\n\n        public bool HasEnded { get; private set; }\n\n        public void AddEvent(string name)\n        {\n            this.Events.Add(new Event(name));\n        }\n\n        public void AddEvent(Event newEvent)\n        {\n            this.Events.Add(newEvent);\n        }\n\n        public void End()\n        {\n            this.HasEnded = true;\n        }\n\n        public void End(DateTimeOffset endTimestamp)\n        {\n            this.End();\n        }\n\n        public void SetAttribute(string key, object value)\n        {\n            this.Attributes.Add(new KeyValuePair<string, object>(key, value));\n        }\n\n        public void SetAttribute(string key, long value)\n        {\n            this.SetAttribute(key, (object)value);\n        }\n\n        public void SetAttribute(string key, bool value)\n        {\n            this.SetAttribute(key, (object)value);\n        }\n\n        public void SetAttribute(string key, double value)\n        {\n            this.SetAttribute(key, (object)value);\n        }\n\n        public void UpdateName(string name)\n        {\n            this.Name = name;\n        }\n\n        public void Dispose()\n        {\n        }\n    }\n}\n", "idx": 1, "id": 13019, "msg": "", "proj": "open-telemetry-opentelemetry-dotnet", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -1,18 +1,23 @@\n #define BOOST_TEST_MODULE TQTcpServerTest\n #include <QTest>\n-#include <boost/smart_ptr.hpp>\n+\n+#ifndef Q_MOC_RUN\n+  #include <boost/smart_ptr.hpp>\n+#endif\n #include <iostream>\n \n #include <QTcpServer>\n #include <QTcpSocket>\n #include <QHostAddress>\n \n-#include \"thrift/protocol/TBinaryProtocol.h\"\n-#include \"thrift/async/TAsyncProcessor.h\"\n-#include \"thrift/qt/TQTcpServer.h\"\n-#include \"thrift/qt/TQIODeviceTransport.h\"\n-\n-#include \"gen-cpp/ParentService.h\"\n+#ifndef Q_MOC_RUN\n+  #include \"thrift/protocol/TBinaryProtocol.h\"\n+  #include \"thrift/async/TAsyncProcessor.h\"\n+  #include \"thrift/qt/TQTcpServer.h\"\n+  #include \"thrift/qt/TQIODeviceTransport.h\"\n+  \n+  #include \"gen-cpp/ParentService.h\"\n+#endif\n \n using namespace apache::thrift;\n ", "y": 1, "oldf": "#define BOOST_TEST_MODULE TQTcpServerTest\n#include <QTest>\n#include <boost/smart_ptr.hpp>\n#include <iostream>\n\n#include <QTcpServer>\n#include <QTcpSocket>\n#include <QHostAddress>\n\n#include \"thrift/protocol/TBinaryProtocol.h\"\n#include \"thrift/async/TAsyncProcessor.h\"\n#include \"thrift/qt/TQTcpServer.h\"\n#include \"thrift/qt/TQIODeviceTransport.h\"\n\n#include \"gen-cpp/ParentService.h\"\n\nusing namespace apache::thrift;\n\nstruct AsyncHandler : public test::ParentServiceCobSvIf {\n  std::vector<std::string> strings;\n  virtual void addString(tcxx::function<void()> cob, const std::string& s) {\n    strings.push_back(s);\n    cob();\n  }\n  virtual void getStrings(tcxx::function<void(std::vector<std::string> const& _return)> cob) {\n    cob(strings);\n  }\n\n  // Overrides not used in this test\n  virtual void incrementGeneration(tcxx::function<void(int32_t const& _return)> cob) {}\n  virtual void getGeneration(tcxx::function<void(int32_t const& _return)> cob) {}\n  virtual void getDataWait(tcxx::function<void(std::string const& _return)> cob,\n                           const int32_t length) {}\n  virtual void onewayWait(tcxx::function<void()> cob) {}\n  virtual void exceptionWait(\n      tcxx::function<void()> cob,\n      tcxx::function<void(::apache::thrift::TDelayedException* _throw)> /* exn_cob */,\n      const std::string& message) {}\n  virtual void unexpectedExceptionWait(tcxx::function<void()> cob, const std::string& message) {}\n};\n\nclass TQTcpServerTest : public QObject {\n  void init() {\n    // setup server\n    serverSocket.reset(new QTcpServer);\n    server.reset(new async::TQTcpServer(serverSocket,\n                                        boost::make_shared<test::ParentServiceAsyncProcessor>(\n                                            boost::make_shared<AsyncHandler>()),\n                                        boost::make_shared<protocol::TBinaryProtocolFactory>()));\n    QVERIFY(serverSocket->listen(QHostAddress::LocalHost));\n    int port = serverSocket->serverPort();\n    QVERIFY(port > 0);\n\n    // setup client\n    socket.reset(new QTcpSocket);\n    client.reset(new test::ParentServiceClient(boost::make_shared<protocol::TBinaryProtocol>(\n        boost::make_shared<transport::TQIODeviceTransport>(socket))));\n    socket->connectToHost(QHostAddress::LocalHost, port);\n    QVERIFY(socket->waitForConnected());\n  }\n\n  void cleanup() {\n    socket->close();\n    serverSocket->close();\n  }\n\n  void test_communicate() {\n    client->addString(\"foo\");\n    client->addString(\"bar\");\n\n    std::vector<std::string> reply;\n    client->getStrings(reply);\n    QCOMPARE(reply[0], \"foo\");\n    QCOMPARE(reply[1], \"foo\");\n  }\n\n  boost::shared_ptr<QTcpServer> serverSocket;\n  boost::shared_ptr<async::TQTcpServer> server;\n  boost::shared_ptr<QTcpSocket> socket;\n  boost::shared_ptr<test::ParentServiceClient> client;\n};\n\n#if (QT_VERSION >= QT_VERSION_CHECK(5, 0, 0))\nQTEST_GUILESS_MAIN(TQTcpServerTest);\n#else\n#undef QT_GUI_LIB\nQTEST_MAIN(TQTcpServerTest);\n#endif\n#include \"TQTcpServerTest.moc\"\n", "idx": 1, "id": 11309, "msg": "Please put it to #ifdef below as reordering seems OK here.", "proj": "apache-thrift", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -426,14 +426,14 @@ describe('Core_validate', () => {\n     }, 200);\n   });\n \n-  it('should add class name `htInvalid` to an cell that does not validate - when we trigger validateCell', function(done) {\n+  it('should add class name `htInvalid` to an cell that does not validate - when we trigger validateCell', function (done) {\n     var onAfterValidate = jasmine.createSpy('onAfterValidate');\n     var hot = handsontable({\n       data: Handsontable.helper.createSpreadsheetData(2, 2),\n       validator(value, cb) {\n         cb(false);\n       },\n-      afterValidate: onAfterValidate\n+      afterValidate: onAfterValidate,\n     });\n \n     expect(this.$container.find('td:not(.htInvalid)').length).toEqual(4);", "y": 0, "oldf": "describe('Core_validate', () => {\n  var id = 'testContainer';\n\n  beforeEach(function() {\n    this.$container = $(`<div id=\"${id}\"></div>`).appendTo('body');\n  });\n\n  afterEach(function() {\n    if (this.$container) {\n      destroy();\n      this.$container.remove();\n    }\n  });\n\n  var arrayOfObjects = function() {\n    return [\n      {id: 1, name: 'Ted', lastName: 'Right'},\n      {id: 2, name: 'Frank', lastName: 'Honest'},\n      {id: 3, name: 'Joan', lastName: 'Well'},\n      {id: 4, name: 'Sid', lastName: 'Strong'},\n      {id: 5, name: 'Jane', lastName: 'Neat'},\n      {id: 6, name: 'Chuck', lastName: 'Jackson'},\n      {id: 7, name: 'Meg', lastName: 'Jansen'},\n      {id: 8, name: 'Rob', lastName: 'Norris'},\n      {id: 9, name: 'Sean', lastName: 'O\\'Hara'},\n      {id: 10, name: 'Eve', lastName: 'Branson'}\n    ];\n  };\n\n  it('should call beforeValidate', () => {\n    var fired = null;\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id', type: 'numeric'},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      beforeValidate() {\n        fired = true;\n      }\n    });\n    setDataAtCell(2, 0, 'test');\n\n    expect(fired).toEqual(true);\n  });\n\n  it('should call beforeValidate when columns is a function', () => {\n    var fired = null;\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = {};\n\n        if (column === 0) {\n          colMeta.data = 'id';\n          colMeta.type = 'numeric';\n\n        } else if (column === 1) {\n          colMeta.data = 'name';\n\n        } else if (column === 2) {\n          colMeta.data = 'lastName';\n\n        } else {\n          colMeta = null;\n        }\n\n        return colMeta;\n      },\n      beforeValidate() {\n        fired = true;\n      }\n    });\n    setDataAtCell(2, 0, 'test');\n\n    expect(fired).toBe(true);\n  });\n\n  it('should call afterValidate', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id', type: 'numeric'},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 'test');\n\n    setTimeout(() => {\n      expect(onAfterValidate.calls.count()).toBe(1);\n      done();\n    }, 200);\n  });\n\n  it('should call afterValidate when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = {};\n\n        if (column === 0) {\n          colMeta.data = 'id';\n          colMeta.type = 'numeric';\n\n        } else if (column === 1) {\n          colMeta.data = 'name';\n\n        } else if (column === 2) {\n          colMeta.data = 'lastName';\n\n        } else {\n          colMeta = null;\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 'test');\n\n    setTimeout(() => {\n      expect(onAfterValidate.calls.count()).toBe(1);\n      done();\n    }, 200);\n  });\n\n  it('beforeValidate can manipulate value', (done) => {\n    var result = null;\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    onAfterValidate.and.callFake((valid, value) => {\n      result = value;\n    });\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id', type: 'numeric'},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      beforeValidate(value) {\n        value = 999;\n        return value;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(result).toBe(999);\n      done();\n    }, 200);\n  });\n\n  it('beforeValidate can manipulate value when columns is a function', (done) => {\n    var result = null;\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    onAfterValidate.and.callFake((valid, value) => {\n      result = value;\n    });\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = {};\n\n        if (column === 0) {\n          colMeta.data = 'id';\n          colMeta.type = 'numeric';\n\n        } else if (column === 1) {\n          colMeta.data = 'name';\n\n        } else if (column === 2) {\n          colMeta.data = 'lastName';\n\n        } else {\n          colMeta = null;\n        }\n\n        return colMeta;\n      },\n      beforeValidate(value) {\n        value = 999;\n        return value;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(result).toBe(999);\n      done();\n    }, 200);\n  });\n\n  it('should be able to define custom validator function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id',\n          validator(value, cb) {\n            cb(true);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(onAfterValidate).toHaveBeenCalledWith(true, 123, 2, 'id', undefined, undefined);\n      done();\n    }, 200);\n  });\n\n  it('should be able to define custom validator function when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {\n            data: 'id',\n            validator(value, cb) {\n              cb(true);\n            }\n          };\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(onAfterValidate).toHaveBeenCalledWith(true, 123, 2, 'id', undefined, undefined);\n      done();\n    }, 200);\n  });\n\n  it('should be able to define custom validator RegExp', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id', validator: /^\\d+$/ },\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    setDataAtCell(2, 0, 'test');\n\n    setTimeout(() => {\n      expect(onAfterValidate).toHaveBeenCalledWith(false, 'test', 2, 'id', undefined, undefined);\n      done();\n    }, 200);\n  });\n\n  it('should be able to define custom validator RegExp when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {data: 'id', validator: /^\\d+$/};\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 'test');\n\n    setTimeout(() => {\n      expect(onAfterValidate).toHaveBeenCalledWith(false, 'test', 2, 'id', undefined, undefined);\n      done();\n    }, 200);\n  });\n\n  it('this in validator should point to cellProperties', (done) => {\n    var result = null;\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {\n          data: 'id',\n          validator(value, cb) {\n            result = this;\n            cb(true);\n          }\n        },\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(result.instance).toEqual(getInstance());\n      done();\n    }, 200);\n  });\n\n  it('this in validator should point to cellProperties when columns is a function', (done) => {\n    var result = null;\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {\n            data: 'id',\n            validator(value, cb) {\n              result = this;\n              cb(true);\n            }\n          };\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(2, 0, 123);\n\n    setTimeout(() => {\n      expect(result.instance).toEqual(getInstance());\n      done();\n    }, 200);\n  });\n\n  it('should not throw error after calling validateCells without first argument', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callb) {\n        if (value == 'B1') {\n          callb(false);\n        } else {\n          callb(true);\n        }\n      },\n      afterValidate: onAfterValidate\n    });\n\n    expect(hot.validateCells).not.toThrow();\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(1);\n      expect(spec().$container.find('td:not(.htInvalid)').length).toEqual(3);\n      done();\n    }, 200);\n  });\n\n  it('should add class name `htInvalid` to an cell that does not validate - on validateCells', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callb) {\n        if (value == 'B1') {\n          callb(false);\n        } else {\n          callb(true);\n        }\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(() => {\n      hot.render();\n    });\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(1);\n      expect(spec().$container.find('td:not(.htInvalid)').length).toEqual(3);\n      done();\n    }, 200);\n  });\n\n  it('should add class name `htInvalid` to an cell that does not validate - when we trigger validateCell', function(done) {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, cb) {\n        cb(false);\n      },\n      afterValidate: onAfterValidate\n    });\n\n    expect(this.$container.find('td:not(.htInvalid)').length).toEqual(4);\n\n    hot.validateCell(hot.getDataAtCell(1, 1), hot.getCellMeta(1, 1), () => {});\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(1);\n      expect(spec().$container.find('td:not(.htInvalid)').length).toEqual(3);\n      done();\n    }, 200);\n  });\n\n  it('should remove class name `htInvalid` from an cell that does validate - when we change validator rules', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var isValid = false;\n    var validator = function() {\n      return isValid;\n    };\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, cb) {\n        cb(validator());\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(() => {});\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(4);\n      expect(spec().$container.find('td:not(.htInvalid)').length).toEqual(0);\n\n      isValid = true;\n      onAfterValidate.calls.reset();\n      hot.validateCell(hot.getDataAtCell(1, 1), hot.getCellMeta(1, 1), () => {});\n    }, 200);\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(3);\n      expect(spec().$container.find('td:not(.htInvalid)').length).toEqual(1);\n      done();\n    }, 400);\n  });\n\n  it('should add class name `htInvalid` to an cell that does not validate - on edit', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callb) {\n        if (value == 'test') {\n          callb(false);\n        } else {\n          callb(true);\n        }\n      },\n      afterValidate: onAfterValidate\n    });\n\n    setDataAtCell(0, 0, 'test');\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(1);\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(true);\n      done();\n    }, 200);\n  });\n\n  it('should add class name `htInvalid` to a cell without removing other classes', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var validator = jasmine.createSpy('validator');\n\n    validator.and.callFake((value, callb) => {\n      if (value == 123) {\n        callb(false);\n      } else {\n        callb(true);\n      }\n    });\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      type: 'numeric',\n      validator,\n      afterValidate: onAfterValidate\n    });\n    setDataAtCell(0, 0, 123);\n\n    setTimeout(() => {\n      expect(validator.calls.count()).toEqual(1);\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(true);\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htNumeric')).toEqual(true);\n      onAfterValidate.calls.reset();\n      setDataAtCell(0, 0, 124);\n    }, 200);\n\n    setTimeout(() => {\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(false);\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htNumeric')).toEqual(true);\n      done();\n    }, 400);\n  });\n\n  it('should add class name `htInvalid` to an cell that does not validate - after validateCells', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      afterValidate: onAfterValidate\n    });\n\n    setDataAtCell(0, 0, 'test');\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(0);\n      updateSettings({\n        validator(value, callb) {\n          if (value == 'test') {\n            callb(false);\n          } else {\n            callb(true);\n          }\n        }\n      });\n\n      onAfterValidate.calls.reset();\n\n      hot.validateCells(() => {});\n    }, 200);\n\n    setTimeout(() => {\n      expect(spec().$container.find('td.htInvalid').length).toEqual(1);\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(true);\n      done();\n    }, 400);\n  });\n\n  it('should remove class name `htInvalid` when cell is edited to validate', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callb) {\n        if (value == 'A1') {\n          callb(false);\n        } else {\n          callb(true);\n        }\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(() => {\n      hot.render();\n    });\n\n    setTimeout(() => {\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(true);\n      onAfterValidate.calls.reset();\n      setDataAtCell(0, 0, 'test');\n    }, 200);\n\n    setTimeout(() => {\n      expect(spec().$container.find('tr:eq(0) td:eq(0)').hasClass('htInvalid')).toEqual(false);\n      done();\n    }, 400);\n  });\n\n  it('should call callback with first argument as `true` if all cells are valid', (done) => {\n    var onValidate = jasmine.createSpy('onValidate');\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callback) {\n        callback(true);\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(onValidate);\n\n    setTimeout(() => {\n      expect(onValidate).toHaveBeenCalledWith(true);\n      done();\n    }, 200);\n  });\n\n  it('should call callback with first argument as `false` if one of cells is invalid', (done) => {\n    var onValidate = jasmine.createSpy('onValidate');\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(2, 2),\n      validator(value, callback) {\n        callback(false);\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(onValidate);\n\n    setTimeout(() => {\n      expect(onValidate).toHaveBeenCalledWith(false);\n      done();\n    }, 200);\n  });\n\n  it('should not allow for changes where data is invalid (multiple changes, async)', (done) => {\n    var validatedChanges;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callb) {\n        setTimeout(() => {\n          if (value === 'fail') {\n            callb(false);\n          } else {\n            callb(true);\n          }\n        }, 10);\n      },\n      afterChange(changes, source) {\n        if (source !== 'loadData') {\n          validatedChanges = changes;\n        }\n      }\n    });\n\n    populateFromArray(0, 0, [\n      ['A1-new'],\n      ['fail'],\n      ['A3-new']\n    ]);\n\n    setTimeout(() => {\n      expect(validatedChanges.length).toEqual(2);\n      expect(validatedChanges[0]).toEqual([0, 0, 'A1', 'A1-new']);\n      expect(validatedChanges[1]).toEqual([2, 0, 'A3', 'A3-new']);\n      expect(getDataAtCell(0, 0)).toEqual('A1-new');\n      expect(getDataAtCell(1, 0)).toEqual('A2');\n      expect(getDataAtCell(2, 0)).toEqual('A3-new');\n      expect(getCellMeta(0, 0).valid).toBe(true);\n      expect(getCellMeta(1, 0).valid).toBe(true);\n      expect(getCellMeta(2, 0).valid).toBe(true);\n      done();\n    }, 200);\n  });\n\n  it('should call beforeChange exactly once after cell value edit and validator is synchronous', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var onBeforeChange = jasmine.createSpy('onBeforeChange');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        callback(true);\n      },\n      beforeChange: onBeforeChange,\n      afterValidate: onAfterValidate\n    });\n\n    expect(onBeforeChange.calls.count()).toEqual(0);\n\n    hot.setDataAtCell(0, 0, 10);\n\n    setTimeout(() => {\n      expect(onBeforeChange.calls.count()).toEqual(1);\n      done();\n    }, 200);\n  });\n\n  it('should call beforeChange exactly once after cell value edit and validator is asynchronous', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var onBeforeChange = jasmine.createSpy('onBeforeChange');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          callback(true);\n        }, 10);\n      },\n      beforeChange: onBeforeChange,\n      afterValidate: onAfterValidate\n    });\n\n    expect(onBeforeChange.calls.count()).toEqual(0);\n\n    hot.setDataAtCell(0, 0, 10);\n\n    setTimeout(() => {\n      expect(onBeforeChange.calls.count()).toEqual(1);\n      done();\n    }, 200);\n  });\n\n  it('should call afterChange exactly once after cell value edit and validator is synchronous', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var onAfterChange = jasmine.createSpy('onAfterChange');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        callback(true);\n      },\n      afterChange: onAfterChange,\n      afterValidate: onAfterValidate\n    });\n\n    expect(onAfterChange.calls.count()).toEqual(1); // loadData\n\n    hot.setDataAtCell(0, 0, 10);\n\n    setTimeout(() => {\n      expect(onAfterChange.calls.count()).toEqual(2);\n      done();\n    }, 200);\n  });\n\n  it('should call afterChange exactly once after cell value edit and validator is asynchronous', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var onAfterChange = jasmine.createSpy('onAfterChange');\n    var hot = handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          callback(true);\n        }, 10);\n      },\n      afterChange: onAfterChange,\n      afterValidate: onAfterValidate\n    });\n\n    expect(onAfterChange.calls.count()).toEqual(1); // loadData\n\n    hot.setDataAtCell(0, 0, 10);\n\n    setTimeout(() => {\n      expect(onAfterChange.calls.count()).toEqual(2);\n      done();\n    }, 200);\n  });\n\n  it('edited cell should stay on screen until value is validated', (done) => {\n    var isEditorVisibleBeforeChange;\n    var isEditorVisibleAfterChange;\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n    var onAfterChange = jasmine.createSpy('onAfterChange');\n\n    onAfterValidate.and.callFake(() => {\n      isEditorVisibleBeforeChange = isEditorVisible();\n    });\n    onAfterChange.and.callFake(() => {\n      isEditorVisibleAfterChange = isEditorVisible();\n    });\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      afterValidate: onAfterValidate,\n      afterChange: onAfterChange,\n      validator(value, callback) {\n        setTimeout(() => {\n          callback(true);\n        }, 100);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n    document.activeElement.value = 'Ted';\n\n    onAfterValidate.calls.reset();\n    onAfterChange.calls.reset();\n\n    keyDown('enter');\n\n    expect(document.activeElement.nodeName).toEqual('TEXTAREA');\n\n    setTimeout(() => {\n      expect(isEditorVisibleBeforeChange).toBe(true);\n      expect(isEditorVisibleAfterChange).toBe(true);\n      expect(isEditorVisible()).toBe(false);\n      done();\n    }, 200);\n  });\n\n  it('should validate edited cell after selecting another cell', (done) => {\n    var validated = false;\n    var validatedValue;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          validated = true;\n          validatedValue = value;\n          callback(true);\n        }, 100);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    selectCell(0, 1);\n\n    setTimeout(() => {\n      expect(validatedValue).toEqual('Ted');\n      done();\n    }, 200);\n  });\n\n  it('should leave the new value in editor if it does not validate (async validation), after hitting ENTER', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          validated = true;\n          validationResult = value.length == 2;\n          callback(validationResult);\n        }, 100);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    keyDown('enter');\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(document.activeElement.value).toEqual('Ted');\n      done();\n    }, 200);\n  });\n\n  it('should leave the new value in editor if it does not validate (sync validation), after hitting ENTER', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        validated = true;\n        validationResult = value.length == 2;\n        callback(validationResult);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    keyDown('enter');\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(document.activeElement.value).toEqual('Ted');\n      done();\n    }, 200);\n  });\n\n  it('should leave the new value in editor if it does not validate (async validation), after selecting another cell', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          setTimeout(() => {\n            validated = true;\n          }, 0);\n\n          validationResult = value.length == 2;\n          callback(validationResult);\n        }, 100);\n      }\n    });\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    selectCell(1, 0);\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(document.activeElement.value).toEqual('Ted');\n      done();\n    }, 200);\n  });\n\n  it('should leave the new value in editor if it does not validate (sync validation), after selecting another cell', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        validationResult = value.length == 2;\n        callback(validationResult);\n\n        /* Setting this variable has to be async, because we are not interested in when the validation happens, but when\n         the callback is being called. Since internally all the callbacks are processed asynchronously (even if they are\n         synchronous) end of validator function is not the equivalent of whole validation routine end.\n         If it still sounds weird, take a look at HandsontableTextEditorClass.prototype.finishEditing method.\n         */\n\n        setTimeout(() => {\n          validated = true;\n        }, 0);\n      }\n    });\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    selectCell(1, 0);\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(document.activeElement.value).toEqual('Ted');\n      done();\n    }, 200);\n  });\n\n  it('should remove htInvalid class properly after cancelling change, when physical indexes are not equal to visual indexes', (done) => {\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      columnSorting: {\n        column: 0,\n        sortOrder: false\n      },\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          callback(value.length === 2);\n        }, 100);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    keyDown('enter');\n\n    setTimeout(() => {\n      const $cell = $(getCell(0, 0));\n      expect($cell.hasClass('htInvalid')).toEqual(false);\n      done();\n    }, 200);\n  });\n\n  it('should close the editor and save the new value if validation fails and allowInvalid is set to \"true\"', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: true,\n      validator(value, callback) {\n        setTimeout(() => {\n\n          validated = true;\n          validationResult = value.length == 2;\n          callback(validationResult);\n        }, 100);\n      }\n    });\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'Ted';\n\n    selectCell(1, 0);\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(getDataAtCell(0, 0)).toEqual('Ted');\n      expect(getCell(0, 0).className).toMatch(/htInvalid/);\n      done();\n    }, 200);\n  });\n\n  it('should close the editor and save the new value after double clicking on a cell, if the previously edited cell validated correctly', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n\n          validated = true;\n          validationResult = value.length == 2;\n          callback(validationResult);\n        }, 100);\n      }\n    });\n\n    selectCell(0, 0);\n    keyDown('enter');\n\n    var editor = $('.handsontableInputHolder');\n    expect(editor.is(':visible')).toBe(true);\n\n    document.activeElement.value = 'AA';\n\n    expect(document.activeElement.value).toEqual('AA');\n\n    var cell = $(getCell(1, 0));\n    var clicks = 0;\n\n    setTimeout(() => {\n      mouseDown(cell);\n      mouseUp(cell);\n      clicks++;\n    }, 0);\n\n    setTimeout(() => {\n      mouseDown(cell);\n      mouseUp(cell);\n      clicks++;\n    }, 100);\n\n    setTimeout(() => {\n      expect(editor.is(':visible')).toBe(false);\n      expect(validationResult).toBe(true);\n      expect(getDataAtCell(0, 0)).toEqual('AA');\n      done();\n    }, 300);\n  });\n\n  it('should close the editor and restore the original value after double clicking on a cell, if the previously edited cell have not validated', (done) => {\n    var validated = false;\n    var validationResult;\n\n    handsontable({\n      data: Handsontable.helper.createSpreadsheetData(5, 2),\n      allowInvalid: false,\n      validator(value, callback) {\n        setTimeout(() => {\n          validated = true;\n          validationResult = value.length == 2;\n          callback(validationResult);\n        }, 100);\n      }\n    });\n    selectCell(0, 0);\n    keyDown('enter');\n\n    document.activeElement.value = 'AAA';\n\n    expect(document.activeElement.value).toEqual('AAA');\n\n    var cell = $(getCell(1, 0));\n    var clicks = 0;\n\n    setTimeout(() => {\n      mouseDown(cell);\n      mouseUp(cell);\n      clicks++;\n    }, 0);\n\n    setTimeout(() => {\n      mouseDown(cell);\n      mouseUp(cell);\n      clicks++;\n    }, 100);\n\n    setTimeout(() => {\n      expect(validationResult).toBe(false);\n      expect(getDataAtCell(0, 0)).toEqual('A1');\n      done();\n    }, 300);\n  });\n\n  it('should listen to key changes after cell is corrected (allowInvalid: false)', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      allowInvalid: false,\n      columns: [\n        {data: 'id',\n          type: 'numeric',\n          validator(val, cb) {\n            cb(parseInt(val, 10) > 100);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n    selectCell(2, 0);\n\n    keyDownUp('enter');\n    document.activeElement.value = '99';\n\n    onAfterValidate.calls.reset();\n\n    keyDownUp('enter'); // should be ignored\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(true);\n      document.activeElement.value = '999';\n\n      onAfterValidate.calls.reset();\n      keyDownUp('enter'); // should be accepted\n    }, 200);\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(false);\n      expect(getSelected()).toEqual([3, 0, 3, 0]);\n\n      keyDownUp('arrow_up');\n      expect(getSelected()).toEqual([2, 0, 2, 0]);\n      done();\n    }, 400);\n  });\n\n  it('should allow keyboard movement when cell is being validated (move DOWN)', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      allowInvalid: false,\n      columns: [\n        {data: 'id',\n          type: 'numeric',\n          validator(val, cb) {\n            setTimeout(() => {\n              cb(parseInt(val, 10) > 100);\n            }, 100);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n    selectCell(2, 0);\n\n    keyDownUp('enter');\n    document.activeElement.value = '999';\n    keyDownUp('enter');\n\n    expect(getSelected()).toEqual([3, 0, 3, 0]);\n\n    keyDownUp('arrow_down');\n    keyDownUp('arrow_down');\n    expect(isEditorVisible()).toBe(true);\n    expect(getSelected()).toEqual([5, 0, 5, 0]);\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(false);\n      expect(getSelected()).toEqual([5, 0, 5, 0]); // only enterMove and first arrow_down is performed\n      done();\n    }, 200);\n  });\n\n  it('should not allow keyboard movement until cell is validated (move UP)', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      allowInvalid: false,\n      columns: [\n        {data: 'id',\n          type: 'numeric',\n          validator(val, cb) {\n            setTimeout(() => {\n              cb(parseInt(val, 10) > 100);\n            }, 100);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(2, 0);\n\n    keyDownUp('enter');\n    document.activeElement.value = '999';\n    keyDownUp('enter');\n\n    expect(getSelected()).toEqual([3, 0, 3, 0]);\n\n    keyDownUp('arrow_up');\n    keyDownUp('arrow_up');\n    expect(isEditorVisible()).toBe(true);\n    expect(getSelected()).toEqual([1, 0, 1, 0]);\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(false);\n      expect(getSelected()).toEqual([1, 0, 1, 0]);\n      done();\n    }, 200);\n  });\n\n  it('should not allow keyboard movement until cell is validated (move RIGHT)', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      allowInvalid: false,\n      columns: [\n        {data: 'id',\n          type: 'numeric',\n          validator(val, cb) {\n            setTimeout(() => {\n              cb(parseInt(val, 10) > 100);\n            }, 100);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(2, 0);\n\n    keyDownUp('enter');\n    document.activeElement.value = '999';\n    keyDownUp('enter'); // should be accepted but only after 100 ms\n    expect(getSelected()).toEqual([3, 0, 3, 0]);\n\n    keyDownUp('arrow_right');\n    keyDownUp('arrow_right');\n    expect(isEditorVisible()).toBe(true);\n    expect(getSelected()).toEqual([3, 2, 3, 2]);\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(false);\n      expect(getSelected()).toEqual([3, 2, 3, 2]);\n      done();\n    }, 200);\n  });\n\n  it('should not allow keyboard movement until cell is validated (move LEFT)', function(done) {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    hot = handsontable({\n      data: arrayOfObjects(),\n      allowInvalid: false,\n      columns: [\n        {data: 'name'},\n        {data: 'lastName'},\n        {data: 'id',\n          type: 'numeric',\n          validator(val, cb) {\n            setTimeout(() => {\n              cb(parseInt(val, 10) > 100);\n            }, 100);\n          }}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(2, 2);\n\n    keyDownUp('enter');\n    document.activeElement.value = '999';\n    keyDownUp('enter'); // should be accepted but only after 100 ms\n    expect(getSelected()).toEqual([3, 2, 3, 2]);\n\n    this.$container.simulate('keydown', {keyCode: Handsontable.helper.KEY_CODES.ARROW_LEFT});\n    this.$container.simulate('keyup', {keyCode: Handsontable.helper.KEY_CODES.ARROW_LEFT});\n    this.$container.simulate('keydown', {keyCode: Handsontable.helper.KEY_CODES.ARROW_LEFT});\n    this.$container.simulate('keyup', {keyCode: Handsontable.helper.KEY_CODES.ARROW_LEFT});\n\n    expect(isEditorVisible()).toBe(true);\n    expect(getSelected()).toEqual([3, 0, 3, 0]);\n\n    setTimeout(() => {\n      expect(isEditorVisible()).toBe(false);\n      expect(getSelected()).toEqual([3, 0, 3, 0]);\n      done();\n    }, 200);\n  });\n\n  it('should not validate cell if editing has been canceled', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id'},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(0, 0);\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n    keyDownUp(Handsontable.helper.KEY_CODES.ESCAPE); // cancel editing\n\n    setTimeout(() => {\n      expect(onAfterValidate).not.toHaveBeenCalled();\n      done();\n    }, 100);\n  });\n\n  it('should not validate cell if editing has been canceled when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {data: 'id'};\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(0, 0);\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n    keyDownUp(Handsontable.helper.KEY_CODES.ESCAPE); // cancel editing\n\n    setTimeout(() => {\n      expect(onAfterValidate).not.toHaveBeenCalled();\n      done();\n    }, 100);\n  });\n\n  it('should leave cell invalid if editing has been canceled', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {data: 'id',\n          validator(value, cb) {\n            cb(false);\n          }},\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    setDataAtCell(0, 0, 'foo');\n\n    setTimeout(() => {\n      expect(getCellMeta(0, 0).valid).toBe(false);\n\n      selectCell(0, 0);\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n      keyDownUp(Handsontable.helper.KEY_CODES.ESCAPE); // cancel editing\n\n      expect(getCellMeta(0, 0).valid).toBe(false);\n      done();\n    }, 200);\n  });\n\n  it('should leave cell invalid if editing has been canceled when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {\n            data: 'id',\n            validator(value, cb) {\n              cb(false);\n            }\n          };\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n\n    setDataAtCell(0, 0, 'foo');\n\n    setTimeout(() => {\n      expect(getCellMeta(0, 0).valid).toBe(false);\n\n      selectCell(0, 0);\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n      keyDownUp(Handsontable.helper.KEY_CODES.ESCAPE); // cancel editing\n\n      expect(getCellMeta(0, 0).valid).toBe(false);\n      done();\n    }, 200);\n  });\n\n  it('should open an appropriate editor after cell value is valid again', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    var hot = handsontable({\n      data: arrayOfObjects(),\n      columns: [\n        {\n          data: 'id',\n          validator(value, cb) {\n            cb(value == parseInt(value, 10));\n          },\n          allowInvalid: false\n        },\n        {data: 'name'},\n        {data: 'lastName'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(0, 0);\n\n    var activeEditor = hot.getActiveEditor();\n\n    expect(activeEditor.row).toEqual(0);\n    expect(activeEditor.col).toEqual(0);\n\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n    activeEditor.setValue('foo');\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // save changes, close editor\n\n    setTimeout(() => {\n      onAfterValidate.calls.reset();\n      activeEditor = hot.getActiveEditor();\n\n      expect(activeEditor.isOpened()).toBe(true); // value is invalid, so editor stays opened\n      expect(activeEditor.row).toEqual(0);\n      expect(activeEditor.col).toEqual(0);\n\n      activeEditor.setValue(2);\n\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // save changes and move to cell below (row: 1, col: \u015b0)\n    }, 200);\n\n    setTimeout(() => {\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n\n      activeEditor = hot.getActiveEditor();\n      expect(activeEditor.row).toEqual(1);\n      expect(activeEditor.col).toEqual(0);\n      done();\n    }, 400);\n  });\n\n  it('should open an appropriate editor after cell value is valid again when columns is a function', (done) => {\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    var hot = handsontable({\n      data: arrayOfObjects(),\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {\n            data: 'id',\n            validator(value, cb) {\n              cb(value == parseInt(value, 10));\n            },\n            allowInvalid: false\n          };\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n\n        } else if (column === 2) {\n          colMeta = {data: 'lastName'};\n\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n\n    selectCell(0, 0);\n\n    var activeEditor = hot.getActiveEditor();\n\n    expect(activeEditor.row).toEqual(0);\n    expect(activeEditor.col).toEqual(0);\n\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n    activeEditor.setValue('foo');\n    keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // save changes, close editor\n\n    setTimeout(() => {\n      onAfterValidate.calls.reset();\n      activeEditor = hot.getActiveEditor();\n\n      expect(activeEditor.isOpened()).toBe(true); // value is invalid, so editor stays opened\n      expect(activeEditor.row).toEqual(0);\n      expect(activeEditor.col).toEqual(0);\n\n      activeEditor.setValue(2);\n\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // save changes and move to cell below (row: 1, col: \u015b0)\n    }, 200);\n\n    setTimeout(() => {\n      keyDownUp(Handsontable.helper.KEY_CODES.ENTER); // open editor\n\n      activeEditor = hot.getActiveEditor();\n      expect(activeEditor.row).toEqual(1);\n      expect(activeEditor.col).toEqual(0);\n      done();\n    }, 400);\n  });\n\n  it('should call the validation callback only once, when using the validateCells method on a mixed set of data', (done) => {\n    var onValidate = jasmine.createSpy('onValidate');\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    var hot = handsontable({\n      data: [\n        {id: 'sth', name: 'Steve'},\n        {id: 'sth else', name: 'Bob'}\n      ],\n      columns: [\n        {\n          data: 'id',\n          validator(value, cb) {\n            cb(value == parseInt(value, 10));\n          }\n        },\n        {data: 'name'}\n      ],\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(onValidate);\n\n    setTimeout(() => {\n      expect(onValidate).toHaveBeenCalledWith(false);\n      expect(onValidate.calls.count()).toEqual(1);\n      done();\n    }, 200);\n  });\n\n  it('should call the validation callback only once, when using the validateCells method on a mixed set of data and when columns is a function', (done) => {\n    var onValidate = jasmine.createSpy('onValidate');\n    var onAfterValidate = jasmine.createSpy('onAfterValidate');\n\n    var hot = handsontable({\n      data: [\n        {id: 'sth', name: 'Steve'},\n        {id: 'sth else', name: 'Bob'}\n      ],\n      columns(column) {\n        var colMeta = null;\n\n        if (column === 0) {\n          colMeta = {\n            data: 'id',\n            validator(value, cb) {\n              cb(value == parseInt(value, 10));\n            }\n          };\n\n        } else if (column === 1) {\n          colMeta = {data: 'name'};\n        }\n\n        return colMeta;\n      },\n      afterValidate: onAfterValidate\n    });\n\n    hot.validateCells(onValidate);\n\n    setTimeout(() => {\n      expect(onValidate).toHaveBeenCalledWith(false);\n      expect(onValidate.calls.count()).toEqual(1);\n      done();\n    }, 200);\n  });\n});\n", "idx": 16, "id": 14504, "msg": "", "proj": "handsontable-handsontable", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -499,6 +499,7 @@ func (c *client) RegisterNkeyUser(user *NkeyUser) {\n \n \tc.mu.Lock()\n \tdefer c.mu.Unlock()\n+\tc.user = user\n \n \t// Assign permissions.\n \tif user.Permissions == nil {", "y": 0, "oldf": "// Copyright 2012-2018 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"crypto/tls\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand\"\n\t\"net\"\n\t\"regexp\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/nats-io/jwt\"\n)\n\n// Type of client connection.\nconst (\n\t// CLIENT is an end user.\n\tCLIENT = iota\n\t// ROUTER is another router in the cluster.\n\tROUTER\n\t// GATEWAY is a link between 2 clusters.\n\tGATEWAY\n)\n\nconst (\n\t// ClientProtoZero is the original Client protocol from 2009.\n\t// http://nats.io/documentation/internals/nats-protocol/\n\tClientProtoZero = iota\n\t// ClientProtoInfo signals a client can receive more then the original INFO block.\n\t// This can be used to update clients on other cluster members, etc.\n\tClientProtoInfo\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\nconst (\n\t// Scratch buffer size for the processMsg() calls.\n\tmsgScratchSize  = 1024\n\tmsgHeadProto    = \"RMSG \"\n\tmsgHeadProtoLen = len(msgHeadProto)\n)\n\n// For controlling dynamic buffer sizes.\nconst (\n\tstartBufSize   = 512   // For INFO/CONNECT block\n\tminBufSize     = 64    // Smallest to shrink to for PING/PONG\n\tmaxBufSize     = 65536 // 64k\n\tshortsToShrink = 2\n)\n\n// Represent client booleans with a bitmask\ntype clientFlag byte\n\n// Some client state represented as flags\nconst (\n\tconnectReceived   clientFlag = 1 << iota // The CONNECT proto has been received\n\tinfoReceived                             // The INFO protocol has been received\n\tfirstPongSent                            // The first PONG has been sent\n\thandshakeComplete                        // For TLS clients, indicate that the handshake is complete\n\tclearConnection                          // Marks that clearConnection has already been called.\n\tflushOutbound                            // Marks client as having a flushOutbound call in progress.\n\tnoReconnect                              // Indicate that on close, this connection should not attempt a reconnect\n)\n\n// set the flag (would be equivalent to set the boolean to true)\nfunc (cf *clientFlag) set(c clientFlag) {\n\t*cf |= c\n}\n\n// clear the flag (would be equivalent to set the boolean to false)\nfunc (cf *clientFlag) clear(c clientFlag) {\n\t*cf &= ^c\n}\n\n// isSet returns true if the flag is set, false otherwise\nfunc (cf clientFlag) isSet(c clientFlag) bool {\n\treturn cf&c != 0\n}\n\n// setIfNotSet will set the flag `c` only if that flag was not already\n// set and return true to indicate that the flag has been set. Returns\n// false otherwise.\nfunc (cf *clientFlag) setIfNotSet(c clientFlag) bool {\n\tif *cf&c == 0 {\n\t\t*cf |= c\n\t\treturn true\n\t}\n\treturn false\n}\n\n// ClosedState is the reason client was closed. This will\n// be passed into calls to clearConnection, but will only\n// be stored in ConnInfo for monitoring.\ntype ClosedState int\n\nconst (\n\tClientClosed = ClosedState(iota + 1)\n\tAuthenticationTimeout\n\tAuthenticationViolation\n\tTLSHandshakeError\n\tSlowConsumerPendingBytes\n\tSlowConsumerWriteDeadline\n\tWriteError\n\tReadError\n\tParseError\n\tStaleConnection\n\tProtocolViolation\n\tBadClientProtocolVersion\n\tWrongPort\n\tMaxAccountConnectionsExceeded\n\tMaxConnectionsExceeded\n\tMaxPayloadExceeded\n\tMaxControlLineExceeded\n\tMaxSubscriptionsExceeded\n\tDuplicateRoute\n\tRouteRemoved\n\tServerShutdown\n\tAuthenticationExpired\n\tWrongGateway\n)\n\ntype client struct {\n\t// Here first because of use of atomics, and memory alignment.\n\tstats\n\tmpay   int32\n\tmsubs  int\n\tmu     sync.Mutex\n\ttyp    int\n\tcid    uint64\n\topts   clientOpts\n\tstart  time.Time\n\tnonce  []byte\n\tnc     net.Conn\n\tncs    string\n\tout    outbound\n\tsrv    *Server\n\tacc    *Account\n\tsubs   map[string]*subscription\n\tperms  *permissions\n\tmperms *msgDeny\n\tdarray []string\n\tin     readCache\n\tpcd    map[*client]struct{}\n\tatmr   *time.Timer\n\tping   pinfo\n\tmsgb   [msgScratchSize]byte\n\tlast   time.Time\n\tparseState\n\n\trtt      time.Duration\n\trttStart time.Time\n\n\troute *route\n\n\tgw *gateway\n\n\tdebug bool\n\ttrace bool\n\techo  bool\n\n\tflags clientFlag // Compact booleans into a single field. Size will be increased when needed.\n}\n\n// Struct for PING initiation from the server.\ntype pinfo struct {\n\ttmr *time.Timer\n\tout int\n}\n\n// outbound holds pending data for a socket.\ntype outbound struct {\n\tp   []byte        // Primary write buffer\n\ts   []byte        // Secondary for use post flush\n\tnb  net.Buffers   // net.Buffers for writev IO\n\tsz  int           // limit size per []byte, uses variable BufSize constants, start, min, max.\n\tsws int           // Number of short writes, used for dyanmic resizing.\n\tpb  int64         // Total pending/queued bytes.\n\tpm  int64         // Total pending/queued messages.\n\tsg  *sync.Cond    // Flusher conditional for signaling.\n\tfsp int           // Flush signals that are pending from readLoop's pcd.\n\tmp  int64         // snapshot of max pending.\n\twdl time.Duration // Snapshot fo write deadline.\n\tlft time.Duration // Last flush time.\n}\n\ntype perm struct {\n\tallow *Sublist\n\tdeny  *Sublist\n}\ntype permissions struct {\n\tsub    perm\n\tpub    perm\n\tpcache map[string]bool\n}\n\n// msgDeny is used when a user permission for subscriptions has a deny\n// clause but a subscription could be made that is of broader scope.\n// e.g. deny = \"foo\", but user subscribes to \"*\". That subscription should\n// succeed but no message sent on foo should be delivered.\ntype msgDeny struct {\n\tdeny   *Sublist\n\tdcache map[string]bool\n}\n\n// routeTarget collects information regarding routes and queue groups for\n// sending information to a remote.\ntype routeTarget struct {\n\tsub *subscription\n\tqs  []byte\n\t_qs [32]byte\n}\n\nconst (\n\tmaxResultCacheSize   = 512\n\tmaxDenyPermCacheSize = 256\n\tmaxPermCacheSize     = 128\n\tpruneSize            = 32\n\trouteTargetInit      = 8\n)\n\n// Used in readloop to cache hot subject lookups and group statistics.\ntype readCache struct {\n\t// These are for clients who are bound to a single account.\n\tgenid   uint64\n\tresults map[string]*SublistResult\n\n\t// This is for routes and gateways to have their own L1 as well that is account aware.\n\tpacache map[string]*perAccountCache\n\n\t// This is for when we deliver messages across a route. We use this structure\n\t// to make sure to only send one message and properly scope to queues as needed.\n\trts []routeTarget\n\n\tprand *rand.Rand\n\tmsgs  int\n\tbytes int\n\tsubs  int\n\trsz   int // Read buffer size\n\tsrs   int // Short reads, used for dynamic buffer resizing.\n}\n\nconst (\n\tmaxPerAccountCacheSize   = 32768\n\tprunePerAccountCacheSize = 512\n)\n\n// perAccountCache is for L1 semantics for inbound messages from a route or gateway to mimic the performance of clients.\ntype perAccountCache struct {\n\tacc     *Account\n\tresults *SublistResult\n\tgenid   uint64\n}\n\nfunc (c *client) String() (id string) {\n\treturn c.ncs\n}\n\nfunc (c *client) GetOpts() *clientOpts {\n\treturn &c.opts\n}\n\n// GetTLSConnectionState returns the TLS ConnectionState if TLS is enabled, nil\n// otherwise. Implements the ClientAuth interface.\nfunc (c *client) GetTLSConnectionState() *tls.ConnectionState {\n\ttc, ok := c.nc.(*tls.Conn)\n\tif !ok {\n\t\treturn nil\n\t}\n\tstate := tc.ConnectionState()\n\treturn &state\n}\n\n// This is the main subscription struct that indicates\n// interest in published messages.\n// FIXME(dlc) - This is getting bloated for normal subs, need\n// to optionally have an opts section for non-normal stuff.\ntype subscription struct {\n\tclient  *client\n\tim      *streamImport   // This is for import stream support.\n\tshadow  []*subscription // This is to track shadowed accounts.\n\tsubject []byte\n\tqueue   []byte\n\tsid     []byte\n\tnm      int64\n\tmax     int64\n\tqw      int32\n}\n\ntype clientOpts struct {\n\tEcho          bool   `json:\"echo\"`\n\tVerbose       bool   `json:\"verbose\"`\n\tPedantic      bool   `json:\"pedantic\"`\n\tTLSRequired   bool   `json:\"tls_required\"`\n\tNkey          string `json:\"nkey,omitempty\"`\n\tJWT           string `json:\"jwt,omitempty\"`\n\tSig           string `json:\"sig,omitempty\"`\n\tAuthorization string `json:\"auth_token,omitempty\"`\n\tUsername      string `json:\"user,omitempty\"`\n\tPassword      string `json:\"pass,omitempty\"`\n\tName          string `json:\"name\"`\n\tLang          string `json:\"lang\"`\n\tVersion       string `json:\"version\"`\n\tProtocol      int    `json:\"protocol\"`\n\tAccount       string `json:\"account,omitempty\"`\n\tAccountNew    bool   `json:\"new_account,omitempty\"`\n\n\t// Routes only\n\tImport *SubjectPermission `json:\"import,omitempty\"`\n\tExport *SubjectPermission `json:\"export,omitempty\"`\n}\n\nvar defaultOpts = clientOpts{Verbose: true, Pedantic: true, Echo: true}\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// Lock should be held\nfunc (c *client) initClient() {\n\ts := c.srv\n\tc.cid = atomic.AddUint64(&s.gcid, 1)\n\n\t// Outbound data structure setup\n\tc.out.sz = startBufSize\n\tc.out.sg = sync.NewCond(&c.mu)\n\topts := s.getOpts()\n\t// Snapshots to avoid mutex access in fast paths.\n\tc.out.wdl = opts.WriteDeadline\n\tc.out.mp = opts.MaxPending\n\n\tc.subs = make(map[string]*subscription)\n\tc.echo = true\n\n\tc.debug = (atomic.LoadInt32(&c.srv.logging.debug) != 0)\n\tc.trace = (atomic.LoadInt32(&c.srv.logging.trace) != 0)\n\n\t// This is a scratch buffer used for processMsg()\n\t// The msg header starts with \"RMSG \", which can be used\n\t// for both local and routes.\n\t// in bytes that is [82 77 83 71 32].\n\tc.msgb = [msgScratchSize]byte{82, 77, 83, 71, 32}\n\n\t// This is to track pending clients that have data to be flushed\n\t// after we process inbound msgs from our own connection.\n\tc.pcd = make(map[*client]struct{})\n\n\t// snapshot the string version of the connection\n\tconn := \"-\"\n\tif ip, ok := c.nc.(*net.TCPConn); ok {\n\t\taddr := ip.RemoteAddr().(*net.TCPAddr)\n\t\tconn = fmt.Sprintf(\"%s:%d\", addr.IP, addr.Port)\n\t}\n\n\tswitch c.typ {\n\tcase CLIENT:\n\t\tc.ncs = fmt.Sprintf(\"%s - cid:%d\", conn, c.cid)\n\tcase ROUTER:\n\t\tc.ncs = fmt.Sprintf(\"%s - rid:%d\", conn, c.cid)\n\tcase GATEWAY:\n\t\tc.ncs = fmt.Sprintf(\"%s - gid:%d\", conn, c.cid)\n\t}\n}\n\n// Helper function to report errors.\nfunc (c *client) reportErrRegisterAccount(acc *Account, err error) {\n\tif err == ErrTooManyAccountConnections {\n\t\tc.maxAccountConnExceeded()\n\t\treturn\n\t}\n\tc.Errorf(\"Problem registering with account [%s]\", acc.Name)\n\tc.sendErr(\"Failed Account Registration\")\n}\n\n// RegisterWithAccount will register the given user with a specific\n// account. This will change the subject namespace.\nfunc (c *client) registerWithAccount(acc *Account) error {\n\tif acc == nil || acc.sl == nil {\n\t\treturn ErrBadAccount\n\t}\n\t// If we were previously register, usually to $G, do accounting here to remove.\n\tif c.acc != nil {\n\t\tif prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\n\t\t\tc.srv.decActiveAccounts()\n\t\t}\n\t}\n\t// Check if we have a max connections violation\n\tif acc.MaxClientsReached() {\n\t\treturn ErrTooManyAccountConnections\n\t}\n\n\t// Add in new one.\n\tif prev := acc.addClient(c); prev == 0 && c.srv != nil {\n\t\tc.srv.incActiveAccounts()\n\t}\n\n\tc.mu.Lock()\n\tc.acc = acc\n\tc.applyAccountLimits()\n\tc.mu.Unlock()\n\n\treturn nil\n}\n\n// Apply account limits\n// Lock is held on entry.\n// FIXME(dlc) - Should server be able to override here?\nfunc (c *client) applyAccountLimits() {\n\tif c.acc == nil {\n\t\treturn\n\t}\n\t// Set here, check for more details below. Only set if non-zero.\n\tif c.acc.msubs > 0 {\n\t\tc.msubs = c.acc.msubs\n\t}\n\tif c.acc.mpay > 0 {\n\t\tc.mpay = c.acc.mpay\n\t}\n\n\topts := c.srv.getOpts()\n\n\t// We check here if the server has an option set that is lower than the account limit.\n\tif c.mpay != 0 && opts.MaxPayload != 0 && int32(opts.MaxPayload) < c.acc.mpay {\n\t\tc.Errorf(\"Max Payload set to %d from server config which overrides %d from account claims\", opts.MaxPayload, c.acc.mpay)\n\t\tc.mpay = int32(opts.MaxPayload)\n\t}\n\n\t// We check here if the server has an option set that is lower than the account limit.\n\tif c.msubs != 0 && opts.MaxSubs != 0 && opts.MaxSubs < c.acc.msubs {\n\t\tc.Errorf(\"Max Subscriptions set to %d from server config which overrides %d from account claims\", opts.MaxSubs, c.acc.msubs)\n\t\tc.msubs = opts.MaxSubs\n\t}\n\n\tif c.msubs > 0 && len(c.subs) > c.msubs {\n\t\tgo func() {\n\t\t\tc.maxSubsExceeded()\n\t\t\ttime.Sleep(20 * time.Millisecond)\n\t\t\tc.closeConnection(MaxSubscriptionsExceeded)\n\t\t}()\n\t}\n}\n\n// RegisterUser allows auth to call back into a new client\n// with the authenticated user. This is used to map\n// any permissions into the client and setup accounts.\nfunc (c *client) RegisterUser(user *User) {\n\t// Register with proper account and sublist.\n\tif user.Account != nil {\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Assign permissions.\n\tif user.Permissions == nil {\n\t\t// Reset perms to nil in case client previously had them.\n\t\tc.perms = nil\n\t\tc.mperms = nil\n\t\treturn\n\t}\n\n\tc.setPermissions(user.Permissions)\n}\n\n// RegisterNkey allows auth to call back into a new nkey\n// client with the authenticated user. This is used to map\n// any permissions into the client and setup accounts.\nfunc (c *client) RegisterNkeyUser(user *NkeyUser) {\n\t// Register with proper account and sublist.\n\tif user.Account != nil {\n\t\tif err := c.registerWithAccount(user.Account); err != nil {\n\t\t\tc.reportErrRegisterAccount(user.Account, err)\n\t\t\treturn\n\t\t}\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Assign permissions.\n\tif user.Permissions == nil {\n\t\t// Reset perms to nil in case client previously had them.\n\t\tc.perms = nil\n\t\tc.mperms = nil\n\t\treturn\n\t}\n\n\tc.setPermissions(user.Permissions)\n}\n\n// Initializes client.perms structure.\n// Lock is held on entry.\nfunc (c *client) setPermissions(perms *Permissions) {\n\tif perms == nil {\n\t\treturn\n\t}\n\tc.perms = &permissions{}\n\tc.perms.pcache = make(map[string]bool)\n\n\t// Loop over publish permissions\n\tif perms.Publish != nil {\n\t\tif len(perms.Publish.Allow) > 0 {\n\t\t\tc.perms.pub.allow = NewSublist()\n\t\t}\n\t\tfor _, pubSubject := range perms.Publish.Allow {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tc.perms.pub.allow.Insert(sub)\n\t\t}\n\t\tif len(perms.Publish.Deny) > 0 {\n\t\t\tc.perms.pub.deny = NewSublist()\n\t\t}\n\t\tfor _, pubSubject := range perms.Publish.Deny {\n\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n\t\t\tc.perms.pub.deny.Insert(sub)\n\t\t}\n\t}\n\n\t// Loop over subscribe permissions\n\tif perms.Subscribe != nil {\n\t\tif len(perms.Subscribe.Allow) > 0 {\n\t\t\tc.perms.sub.allow = NewSublist()\n\t\t}\n\t\tfor _, subSubject := range perms.Subscribe.Allow {\n\t\t\tsub := &subscription{subject: []byte(subSubject)}\n\t\t\tc.perms.sub.allow.Insert(sub)\n\t\t}\n\t\tif len(perms.Subscribe.Deny) > 0 {\n\t\t\tc.perms.sub.deny = NewSublist()\n\t\t\t// Also hold onto this array for later.\n\t\t\tc.darray = perms.Subscribe.Deny\n\t\t}\n\t\tfor _, subSubject := range perms.Subscribe.Deny {\n\t\t\tsub := &subscription{subject: []byte(subSubject)}\n\t\t\tc.perms.sub.deny.Insert(sub)\n\t\t}\n\t}\n}\n\n// Check to see if we have an expiration for the user JWT via base claims.\n// FIXME(dlc) - Clear on connect with new JWT.\nfunc (c *client) checkExpiration(claims *jwt.ClaimsData) {\n\tif claims.Expires == 0 {\n\t\treturn\n\t}\n\ttn := time.Now().Unix()\n\tif claims.Expires < tn {\n\t\treturn\n\t}\n\texpiresAt := time.Duration(claims.Expires - tn)\n\tc.setExpirationTimer(expiresAt * time.Second)\n}\n\n// This will load up the deny structure used for filtering delivered\n// messages based on a deny clause for subscriptions.\n// Lock should be held.\nfunc (c *client) loadMsgDenyFilter() {\n\tc.mperms = &msgDeny{NewSublist(), make(map[string]bool)}\n\tfor _, sub := range c.darray {\n\t\tc.mperms.deny.Insert(&subscription{subject: []byte(sub)})\n\t}\n}\n\n// writeLoop is the main socket write functionality.\n// Runs in its own Go routine.\nfunc (c *client) writeLoop() {\n\tdefer c.srv.grWG.Done()\n\n\t// Used to check that we did flush from last wake up.\n\twaitOk := true\n\n\t// Main loop. Will wait to be signaled and then will use\n\t// buffered outbound structure for efficient writev to the underlying socket.\n\tfor {\n\t\tc.mu.Lock()\n\t\tif waitOk && (c.out.pb == 0 || c.out.fsp > 0) && len(c.out.nb) == 0 && !c.flags.isSet(clearConnection) {\n\t\t\t// Wait on pending data.\n\t\t\tc.out.sg.Wait()\n\t\t}\n\t\t// Flush data\n\t\twaitOk = c.flushOutbound()\n\t\tisClosed := c.flags.isSet(clearConnection)\n\t\tc.mu.Unlock()\n\n\t\tif isClosed {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// readLoop is the main socket read functionality.\n// Runs in its own Go routine.\nfunc (c *client) readLoop() {\n\t// Grab the connection off the client, it will be cleared on a close.\n\t// We check for that after the loop, but want to avoid a nil dereference\n\tc.mu.Lock()\n\tnc := c.nc\n\ts := c.srv\n\tc.in.rsz = startBufSize\n\tdefer s.grWG.Done()\n\tc.mu.Unlock()\n\n\tif nc == nil {\n\t\treturn\n\t}\n\n\t// Start read buffer.\n\n\tb := make([]byte, c.in.rsz)\n\n\tfor {\n\t\tn, err := nc.Read(b)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tc.closeConnection(ClientClosed)\n\t\t\t} else {\n\t\t\t\tc.closeConnection(ReadError)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Grab for updates for last activity.\n\t\tlast := time.Now()\n\n\t\t// Clear inbound stats cache\n\t\tc.in.msgs = 0\n\t\tc.in.bytes = 0\n\t\tc.in.subs = 0\n\n\t\t// Main call into parser for inbound data. This will generate callouts\n\t\t// to process messages, etc.\n\t\tif err := c.parse(b[:n]); err != nil {\n\t\t\t// handled inline\n\t\t\tif err != ErrMaxPayload && err != ErrAuthentication {\n\t\t\t\tc.Errorf(\"%s\", err.Error())\n\t\t\t\tc.closeConnection(ProtocolViolation)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Updates stats for client and server that were collected\n\t\t// from parsing through the buffer.\n\t\tif c.in.msgs > 0 {\n\t\t\tatomic.AddInt64(&c.inMsgs, int64(c.in.msgs))\n\t\t\tatomic.AddInt64(&c.inBytes, int64(c.in.bytes))\n\t\t\tatomic.AddInt64(&s.inMsgs, int64(c.in.msgs))\n\t\t\tatomic.AddInt64(&s.inBytes, int64(c.in.bytes))\n\t\t}\n\n\t\t// Budget to spend in place flushing outbound data.\n\t\t// Client will be checked on several fronts to see\n\t\t// if applicable. Routes will never wait in place.\n\t\tbudget := 500 * time.Microsecond\n\t\tif c.typ == ROUTER {\n\t\t\tbudget = 0\n\t\t}\n\n\t\t// Check pending clients for flush.\n\t\tfor cp := range c.pcd {\n\t\t\t// Queue up a flush for those in the set\n\t\t\tcp.mu.Lock()\n\t\t\t// Update last activity for message delivery\n\t\t\tcp.last = last\n\t\t\tcp.out.fsp--\n\t\t\tif budget > 0 && cp.flushOutbound() {\n\t\t\t\tbudget -= cp.out.lft\n\t\t\t} else {\n\t\t\t\tcp.flushSignal()\n\t\t\t}\n\t\t\tcp.mu.Unlock()\n\t\t\tdelete(c.pcd, cp)\n\t\t}\n\n\t\t// Update activity, check read buffer size.\n\t\tc.mu.Lock()\n\t\tnc := c.nc\n\n\t\t// Activity based on interest changes or data/msgs.\n\t\tif c.in.msgs > 0 || c.in.subs > 0 {\n\t\t\tc.last = last\n\t\t}\n\n\t\tif n >= cap(b) {\n\t\t\tc.in.srs = 0\n\t\t} else if n < cap(b)/2 { // divide by 2 b/c we want less than what we would shrink to.\n\t\t\tc.in.srs++\n\t\t}\n\n\t\t// Update read buffer size as/if needed.\n\t\tif n >= cap(b) && cap(b) < maxBufSize {\n\t\t\t// Grow\n\t\t\tc.in.rsz = cap(b) * 2\n\t\t\tb = make([]byte, c.in.rsz)\n\t\t} else if n < cap(b) && cap(b) > minBufSize && c.in.srs > shortsToShrink {\n\t\t\t// Shrink, for now don't accelerate, ping/pong will eventually sort it out.\n\t\t\tc.in.rsz = cap(b) / 2\n\t\t\tb = make([]byte, c.in.rsz)\n\t\t}\n\t\tc.mu.Unlock()\n\n\t\t// Check to see if we got closed, e.g. slow consumer\n\t\tif nc == nil {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// collapsePtoNB will place primary onto nb buffer as needed in prep for WriteTo.\n// This will return a copy on purpose.\nfunc (c *client) collapsePtoNB() net.Buffers {\n\tif c.out.p != nil {\n\t\tp := c.out.p\n\t\tc.out.p = nil\n\t\treturn append(c.out.nb, p)\n\t}\n\treturn c.out.nb\n}\n\n// This will handle the fixup needed on a partial write.\n// Assume pending has been already calculated correctly.\nfunc (c *client) handlePartialWrite(pnb net.Buffers) {\n\tnb := c.collapsePtoNB()\n\t// The partial needs to be first, so append nb to pnb\n\tc.out.nb = append(pnb, nb...)\n}\n\n// flushOutbound will flush outbound buffer to a client.\n// Will return if data was attempted to be written.\n// Lock must be held\nfunc (c *client) flushOutbound() bool {\n\tif c.flags.isSet(flushOutbound) {\n\t\treturn false\n\t}\n\tc.flags.set(flushOutbound)\n\tdefer c.flags.clear(flushOutbound)\n\n\t// Check for nothing to do.\n\tif c.nc == nil || c.srv == nil || c.out.pb == 0 {\n\t\treturn true // true because no need to queue a signal.\n\t}\n\n\t// Snapshot opts\n\tsrv := c.srv\n\n\t// Place primary on nb, assign primary to secondary, nil out nb and secondary.\n\tnb := c.collapsePtoNB()\n\tc.out.p, c.out.nb, c.out.s = c.out.s, nil, nil\n\n\t// For selecting primary replacement.\n\tcnb := nb\n\n\t// In case it goes away after releasing the lock.\n\tnc := c.nc\n\tattempted := c.out.pb\n\tapm := c.out.pm\n\n\t// Do NOT hold lock during actual IO\n\tc.mu.Unlock()\n\n\t// flush here\n\tnow := time.Now()\n\t// FIXME(dlc) - writev will do multiple IOs past 1024 on\n\t// most platforms, need to account for that with deadline?\n\tnc.SetWriteDeadline(now.Add(c.out.wdl))\n\t// Actual write to the socket.\n\tn, err := nb.WriteTo(nc)\n\tnc.SetWriteDeadline(time.Time{})\n\tlft := time.Since(now)\n\n\t// Re-acquire client lock\n\tc.mu.Lock()\n\n\t// Update flush time statistics\n\tc.out.lft = lft\n\n\t// Subtract from pending bytes and messages.\n\tc.out.pb -= n\n\tc.out.pm -= apm // FIXME(dlc) - this will not be accurate.\n\n\t// Check for partial writes\n\tif n != attempted && n > 0 {\n\t\tc.handlePartialWrite(nb)\n\t} else if n >= int64(c.out.sz) {\n\t\tc.out.sws = 0\n\t}\n\n\tif err != nil {\n\t\tif n == 0 {\n\t\t\tc.out.pb -= attempted\n\t\t}\n\t\tif ne, ok := err.(net.Error); ok && ne.Timeout() {\n\t\t\tatomic.AddInt64(&srv.slowConsumers, 1)\n\t\t\tc.clearConnection(SlowConsumerWriteDeadline)\n\t\t\tc.Noticef(\"Slow Consumer Detected: WriteDeadline of %v Exceeded\", c.out.wdl)\n\t\t} else {\n\t\t\tc.clearConnection(WriteError)\n\t\t\tc.Debugf(\"Error flushing: %v\", err)\n\t\t}\n\t\treturn true\n\t}\n\n\t// Adjust based on what we wrote plus any pending.\n\tpt := int(n + c.out.pb)\n\n\t// Adjust sz as needed downward, keeping power of 2.\n\t// We do this at a slower rate, hence the pt*4.\n\tif pt < c.out.sz && c.out.sz > minBufSize {\n\t\tc.out.sws++\n\t\tif c.out.sws > shortsToShrink {\n\t\t\tc.out.sz >>= 1\n\t\t}\n\t}\n\t// Adjust sz as needed upward, keeping power of 2.\n\tif pt > c.out.sz && c.out.sz < maxBufSize {\n\t\tc.out.sz <<= 1\n\t}\n\n\t// Check to see if we can reuse buffers.\n\tif len(cnb) > 0 {\n\t\toldp := cnb[0][:0]\n\t\tif cap(oldp) >= c.out.sz {\n\t\t\t// Replace primary or secondary if they are nil, reusing same buffer.\n\t\t\tif c.out.p == nil {\n\t\t\t\tc.out.p = oldp\n\t\t\t} else if c.out.s == nil || cap(c.out.s) < c.out.sz {\n\t\t\t\tc.out.s = oldp\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\n// flushSignal will use server to queue the flush IO operation to a pool of flushers.\n// Lock must be held.\nfunc (c *client) flushSignal() {\n\tc.out.sg.Signal()\n}\n\nfunc (c *client) traceMsg(msg []byte) {\n\tif !c.trace {\n\t\treturn\n\t}\n\t// FIXME(dlc), allow limits to printable payload\n\tc.Tracef(\"<<- MSG_PAYLOAD: [%s]\", string(msg[:len(msg)-LEN_CR_LF]))\n}\n\nfunc (c *client) traceInOp(op string, arg []byte) {\n\tc.traceOp(\"<<- %s\", op, arg)\n}\n\nfunc (c *client) traceOutOp(op string, arg []byte) {\n\tc.traceOp(\"->> %s\", op, arg)\n}\n\nfunc (c *client) traceOp(format, op string, arg []byte) {\n\tif !c.trace {\n\t\treturn\n\t}\n\n\topa := []interface{}{}\n\tif op != \"\" {\n\t\topa = append(opa, op)\n\t}\n\tif arg != nil {\n\t\topa = append(opa, string(arg))\n\t}\n\tc.Tracef(format, opa)\n}\n\n// Process the information messages from Clients and other Routes.\nfunc (c *client) processInfo(arg []byte) error {\n\tinfo := Info{}\n\tif err := json.Unmarshal(arg, &info); err != nil {\n\t\treturn err\n\t}\n\tswitch c.typ {\n\tcase ROUTER:\n\t\tc.processRouteInfo(&info)\n\tcase GATEWAY:\n\t\tc.processGatewayInfo(&info)\n\t}\n\treturn nil\n}\n\nfunc (c *client) processErr(errStr string) {\n\tswitch c.typ {\n\tcase CLIENT:\n\t\tc.Errorf(\"Client Error %s\", errStr)\n\tcase ROUTER:\n\t\tc.Errorf(\"Route Error %s\", errStr)\n\tcase GATEWAY:\n\t\tc.Errorf(\"Gateway Error %s\", errStr)\n\t}\n\tc.closeConnection(ParseError)\n}\n\n// Password pattern matcher.\nvar passPat = regexp.MustCompile(`\"?\\s*pass\\S*?\"?\\s*[:=]\\s*\"?(([^\",\\r\\n}])*)`)\n\n// removePassFromTrace removes any notion of passwords from trace\n// messages for logging.\nfunc removePassFromTrace(arg []byte) []byte {\n\tif !bytes.Contains(arg, []byte(`pass`)) {\n\t\treturn arg\n\t}\n\t// Take a copy of the connect proto just for the trace message.\n\tvar _arg [4096]byte\n\tbuf := append(_arg[:0], arg...)\n\n\tm := passPat.FindAllSubmatchIndex(buf, -1)\n\tif len(m) == 0 {\n\t\treturn arg\n\t}\n\n\tredactedPass := []byte(\"[REDACTED]\")\n\tfor _, i := range m {\n\t\tif len(i) < 4 {\n\t\t\tcontinue\n\t\t}\n\t\tstart := i[2]\n\t\tend := i[3]\n\n\t\t// Replace password substring.\n\t\tbuf = append(buf[:start], append(redactedPass, buf[end:]...)...)\n\t\tbreak\n\t}\n\treturn buf\n}\n\nfunc (c *client) processConnect(arg []byte) error {\n\tif c.trace {\n\t\tc.traceInOp(\"CONNECT\", removePassFromTrace(arg))\n\t}\n\n\tc.mu.Lock()\n\t// If we can't stop the timer because the callback is in progress...\n\tif !c.clearAuthTimer() {\n\t\t// wait for it to finish and handle sending the failure back to\n\t\t// the client.\n\t\tfor c.nc != nil {\n\t\t\tc.mu.Unlock()\n\t\t\ttime.Sleep(25 * time.Millisecond)\n\t\t\tc.mu.Lock()\n\t\t}\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\tc.last = time.Now()\n\ttyp := c.typ\n\tsrv := c.srv\n\t// Moved unmarshalling of clients' Options under the lock.\n\t// The client has already been added to the server map, so it is possible\n\t// that other routines lookup the client, and access its options under\n\t// the client's lock, so unmarshalling the options outside of the lock\n\t// would cause data RACEs.\n\tif err := json.Unmarshal(arg, &c.opts); err != nil {\n\t\tc.mu.Unlock()\n\t\treturn err\n\t}\n\t// Indicate that the CONNECT protocol has been received, and that the\n\t// server now knows which protocol this client supports.\n\tc.flags.set(connectReceived)\n\t// Capture these under lock\n\tc.echo = c.opts.Echo\n\tproto := c.opts.Protocol\n\tverbose := c.opts.Verbose\n\tlang := c.opts.Lang\n\taccount := c.opts.Account\n\taccountNew := c.opts.AccountNew\n\tc.mu.Unlock()\n\n\tif srv != nil {\n\t\t// Applicable to clients only:\n\t\t// As soon as c.opts is unmarshalled and if the proto is at\n\t\t// least ClientProtoInfo, we need to increment the following counter.\n\t\t// This is decremented when client is removed from the server's\n\t\t// clients map.\n\t\tif typ == CLIENT && proto >= ClientProtoInfo {\n\t\t\tsrv.mu.Lock()\n\t\t\tsrv.cproto++\n\t\t\tsrv.mu.Unlock()\n\t\t}\n\n\t\t// Check for Auth\n\t\tif ok := srv.checkAuthentication(c); !ok {\n\t\t\tc.authViolation()\n\t\t\treturn ErrAuthentication\n\t\t}\n\n\t\t// Check for Account designation\n\t\tif account != \"\" {\n\t\t\tvar acc *Account\n\t\t\tvar wasNew bool\n\t\t\tif !srv.NewAccountsAllowed() {\n\t\t\t\tacc = srv.LookupAccount(account)\n\t\t\t\tif acc == nil {\n\t\t\t\t\tc.Errorf(ErrMissingAccount.Error())\n\t\t\t\t\tc.sendErr(\"Account Not Found\")\n\t\t\t\t\treturn ErrMissingAccount\n\t\t\t\t} else if accountNew {\n\t\t\t\t\tc.Errorf(ErrAccountExists.Error())\n\t\t\t\t\tc.sendErr(ErrAccountExists.Error())\n\t\t\t\t\treturn ErrAccountExists\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// We can create this one on the fly.\n\t\t\t\tacc, wasNew = srv.LookupOrRegisterAccount(account)\n\t\t\t\tif accountNew && !wasNew {\n\t\t\t\t\tc.Errorf(ErrAccountExists.Error())\n\t\t\t\t\tc.sendErr(ErrAccountExists.Error())\n\t\t\t\t\treturn ErrAccountExists\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we are here we can register ourselves with the new account.\n\t\t\tif err := c.registerWithAccount(acc); err != nil {\n\t\t\t\tc.reportErrRegisterAccount(acc, err)\n\t\t\t\treturn ErrBadAccount\n\t\t\t}\n\t\t} else if c.acc == nil {\n\t\t\t// By default register with the global account.\n\t\t\tc.registerWithAccount(srv.gacc)\n\t\t}\n\n\t}\n\n\tswitch typ {\n\tcase CLIENT:\n\t\t// Check client protocol request if it exists.\n\t\tif proto < ClientProtoZero || proto > ClientProtoInfo {\n\t\t\tc.sendErr(ErrBadClientProtocol.Error())\n\t\t\tc.closeConnection(BadClientProtocolVersion)\n\t\t\treturn ErrBadClientProtocol\n\t\t}\n\t\tif verbose {\n\t\t\tc.sendOK()\n\t\t}\n\tcase ROUTER:\n\t\t// Delegate the rest of processing to the route\n\t\treturn c.processRouteConnect(srv, arg, lang)\n\tcase GATEWAY:\n\t\t// Delegate the rest of processing to the gateway\n\t\treturn c.processGatewayConnect(arg)\n\t}\n\treturn nil\n}\n\nfunc (c *client) sendErrAndErr(err string) {\n\tc.sendErr(err)\n\tc.Errorf(err)\n}\n\nfunc (c *client) sendErrAndDebug(err string) {\n\tc.sendErr(err)\n\tc.Debugf(err)\n}\n\nfunc (c *client) authTimeout() {\n\tc.sendErrAndDebug(\"Authentication Timeout\")\n\tc.closeConnection(AuthenticationTimeout)\n}\n\nfunc (c *client) authExpired() {\n\tc.sendErrAndDebug(\"User Authentication Expired\")\n\tc.closeConnection(AuthenticationExpired)\n}\n\nfunc (c *client) accountAuthExpired() {\n\tc.sendErrAndDebug(\"Account Authentication Expired\")\n\tc.closeConnection(AuthenticationExpired)\n}\n\nfunc (c *client) authViolation() {\n\tvar hasTrustedNkeys, hasNkeys, hasUsers bool\n\tif s := c.srv; s != nil {\n\t\ts.mu.Lock()\n\t\thasTrustedNkeys = len(s.trustedNkeys) > 0\n\t\thasNkeys = s.nkeys != nil\n\t\thasUsers = s.users != nil\n\t\ts.mu.Unlock()\n\t}\n\tif hasTrustedNkeys {\n\t\tc.Errorf(\"%v\", ErrAuthentication)\n\t} else if hasNkeys {\n\t\tc.Errorf(\"%s - Nkey %q\",\n\t\t\tErrAuthentication.Error(),\n\t\t\tc.opts.Nkey)\n\t} else if hasUsers {\n\t\tc.Errorf(\"%s - User %q\",\n\t\t\tErrAuthentication.Error(),\n\t\t\tc.opts.Username)\n\t} else {\n\t\tc.Errorf(ErrAuthentication.Error())\n\t}\n\tc.sendErr(\"Authorization Violation\")\n\tc.closeConnection(AuthenticationViolation)\n}\n\nfunc (c *client) maxAccountConnExceeded() {\n\tc.sendErrAndErr(ErrTooManyAccountConnections.Error())\n\tc.closeConnection(MaxAccountConnectionsExceeded)\n}\n\nfunc (c *client) maxConnExceeded() {\n\tc.sendErrAndErr(ErrTooManyConnections.Error())\n\tc.closeConnection(MaxConnectionsExceeded)\n}\n\nfunc (c *client) maxSubsExceeded() {\n\tc.sendErrAndErr(ErrTooManySubs.Error())\n}\n\nfunc (c *client) maxPayloadViolation(sz int, max int32) {\n\tc.Errorf(\"%s: %d vs %d\", ErrMaxPayload.Error(), sz, max)\n\tc.sendErr(\"Maximum Payload Violation\")\n\tc.closeConnection(MaxPayloadExceeded)\n}\n\n// queueOutbound queues data for client/route connections.\n// Return if the data is referenced or not. If referenced, the caller\n// should not reuse the `data` array.\n// Lock should be held.\nfunc (c *client) queueOutbound(data []byte) bool {\n\t// Assume data will not be referenced\n\treferenced := false\n\t// Add to pending bytes total.\n\tc.out.pb += int64(len(data))\n\n\t// Check for slow consumer via pending bytes limit.\n\t// ok to return here, client is going away.\n\tif c.out.pb > c.out.mp {\n\t\tc.clearConnection(SlowConsumerPendingBytes)\n\t\tatomic.AddInt64(&c.srv.slowConsumers, 1)\n\t\tc.Noticef(\"Slow Consumer Detected: MaxPending of %d Exceeded\", c.out.mp)\n\t\treturn referenced\n\t}\n\n\tif c.out.p == nil && len(data) < maxBufSize {\n\t\tif c.out.sz == 0 {\n\t\t\tc.out.sz = startBufSize\n\t\t}\n\t\tif c.out.s != nil && cap(c.out.s) >= c.out.sz {\n\t\t\tc.out.p = c.out.s\n\t\t\tc.out.s = nil\n\t\t} else {\n\t\t\t// FIXME(dlc) - make power of 2 if less than maxBufSize?\n\t\t\tc.out.p = make([]byte, 0, c.out.sz)\n\t\t}\n\t}\n\t// Determine if we copy or reference\n\tavailable := cap(c.out.p) - len(c.out.p)\n\tif len(data) > available {\n\t\t// We can fit into existing primary, but message will fit in next one\n\t\t// we allocate or utilize from the secondary. So copy what we can.\n\t\tif available > 0 && len(data) < c.out.sz {\n\t\t\tc.out.p = append(c.out.p, data[:available]...)\n\t\t\tdata = data[available:]\n\t\t}\n\t\t// Put the primary on the nb if it has a payload\n\t\tif len(c.out.p) > 0 {\n\t\t\tc.out.nb = append(c.out.nb, c.out.p)\n\t\t\tc.out.p = nil\n\t\t}\n\t\t// Check for a big message, and if found place directly on nb\n\t\t// FIXME(dlc) - do we need signaling of ownership here if we want len(data) < maxBufSize\n\t\tif len(data) > maxBufSize {\n\t\t\tc.out.nb = append(c.out.nb, data)\n\t\t\treferenced = true\n\t\t} else {\n\t\t\t// We will copy to primary.\n\t\t\tif c.out.p == nil {\n\t\t\t\t// Grow here\n\t\t\t\tif (c.out.sz << 1) <= maxBufSize {\n\t\t\t\t\tc.out.sz <<= 1\n\t\t\t\t}\n\t\t\t\tif len(data) > c.out.sz {\n\t\t\t\t\tc.out.p = make([]byte, 0, len(data))\n\t\t\t\t} else {\n\t\t\t\t\tif c.out.s != nil && cap(c.out.s) >= c.out.sz { // TODO(dlc) - Size mismatch?\n\t\t\t\t\t\tc.out.p = c.out.s\n\t\t\t\t\t\tc.out.s = nil\n\t\t\t\t\t} else {\n\t\t\t\t\t\tc.out.p = make([]byte, 0, c.out.sz)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.out.p = append(c.out.p, data...)\n\t\t}\n\t} else {\n\t\tc.out.p = append(c.out.p, data...)\n\t}\n\treturn referenced\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendProto(info []byte, doFlush bool) {\n\tif c.nc == nil {\n\t\treturn\n\t}\n\tc.queueOutbound(info)\n\tif !(doFlush && c.flushOutbound()) {\n\t\tc.flushSignal()\n\t}\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendPong() {\n\tc.traceOutOp(\"PONG\", nil)\n\tc.sendProto([]byte(\"PONG\\r\\n\"), true)\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendPing() {\n\tc.rttStart = time.Now()\n\tc.ping.out++\n\tc.traceOutOp(\"PING\", nil)\n\tc.sendProto([]byte(\"PING\\r\\n\"), true)\n}\n\n// Generates the INFO to be sent to the client with the client ID included.\n// info arg will be copied since passed by value.\n// Assume lock is held.\nfunc (c *client) generateClientInfoJSON(info Info) []byte {\n\tinfo.CID = c.cid\n\t// Generate the info json\n\tb, _ := json.Marshal(info)\n\tpcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\n\treturn bytes.Join(pcs, []byte(\" \"))\n}\n\n// Assume the lock is held upon entry.\nfunc (c *client) sendInfo(info []byte) {\n\tc.sendProto(info, true)\n}\n\nfunc (c *client) sendErr(err string) {\n\tc.mu.Lock()\n\tc.traceOutOp(\"-ERR\", []byte(err))\n\tc.sendProto([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", err)), true)\n\tc.mu.Unlock()\n}\n\nfunc (c *client) sendOK() {\n\tc.mu.Lock()\n\tc.traceOutOp(\"OK\", nil)\n\t// Can not autoflush this one, needs to be async.\n\tc.sendProto([]byte(\"+OK\\r\\n\"), false)\n\t// FIXME(dlc) - ??\n\tc.pcd[c] = needFlush\n\tc.mu.Unlock()\n}\n\nfunc (c *client) processPing() {\n\tc.mu.Lock()\n\tc.traceInOp(\"PING\", nil)\n\tif c.nc == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tc.sendPong()\n\n\t// If not a CLIENT, we are done\n\tif c.typ != CLIENT {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\t// The CONNECT should have been received, but make sure it\n\t// is so before proceeding\n\tif !c.flags.isSet(connectReceived) {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\t// If we are here, the CONNECT has been received so we know\n\t// if this client supports async INFO or not.\n\tvar (\n\t\tcheckClusterChange bool\n\t\tsrv                = c.srv\n\t)\n\t// For older clients, just flip the firstPongSent flag if not already\n\t// set and we are done.\n\tif c.opts.Protocol < ClientProtoInfo || srv == nil {\n\t\tc.flags.setIfNotSet(firstPongSent)\n\t} else {\n\t\t// This is a client that supports async INFO protocols.\n\t\t// If this is the first PING (so firstPongSent is not set yet),\n\t\t// we will need to check if there was a change in cluster topology.\n\t\tcheckClusterChange = !c.flags.isSet(firstPongSent)\n\t}\n\tc.mu.Unlock()\n\n\tif checkClusterChange {\n\t\tsrv.mu.Lock()\n\t\tc.mu.Lock()\n\t\t// Now that we are under both locks, we can flip the flag.\n\t\t// This prevents sendAsyncInfoToClients() and and code here\n\t\t// to send a double INFO protocol.\n\t\tc.flags.set(firstPongSent)\n\t\t// If there was a cluster update since this client was created,\n\t\t// send an updated INFO protocol now.\n\t\tif srv.lastCURLsUpdate >= c.start.UnixNano() {\n\t\t\tc.sendInfo(c.generateClientInfoJSON(srv.copyInfo()))\n\t\t}\n\t\tc.mu.Unlock()\n\t\tsrv.mu.Unlock()\n\t}\n}\n\nfunc (c *client) processPong() {\n\tc.traceInOp(\"PONG\", nil)\n\tc.mu.Lock()\n\tc.ping.out = 0\n\tc.rtt = time.Since(c.rttStart)\n\tsrv := c.srv\n\treorderGWs := c.typ == GATEWAY && c.gw.outbound\n\tc.mu.Unlock()\n\tif reorderGWs {\n\t\tsrv.gateway.orderOutboundConnections()\n\t}\n}\n\nfunc (c *client) processPub(trace bool, arg []byte) error {\n\tif trace {\n\t\tc.traceInOp(\"PUB\", arg)\n\t}\n\n\t// Unroll splitArgs to avoid runtime/heap issues\n\ta := [MAX_PUB_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\n\tc.pa.arg = arg\n\tswitch len(args) {\n\tcase 2:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = nil\n\t\tc.pa.size = parseSize(args[1])\n\t\tc.pa.szb = args[1]\n\tcase 3:\n\t\tc.pa.subject = args[0]\n\t\tc.pa.reply = args[1]\n\t\tc.pa.size = parseSize(args[2])\n\t\tc.pa.szb = args[2]\n\tdefault:\n\t\treturn fmt.Errorf(\"processPub Parse Error: '%s'\", arg)\n\t}\n\tif c.pa.size < 0 {\n\t\treturn fmt.Errorf(\"processPub Bad or Missing Size: '%s'\", arg)\n\t}\n\tmaxPayload := atomic.LoadInt32(&c.mpay)\n\tif maxPayload > 0 && int32(c.pa.size) > maxPayload {\n\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\n\t\treturn ErrMaxPayload\n\t}\n\n\tif c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\n\t\tc.sendErr(\"Invalid Publish Subject\")\n\t}\n\treturn nil\n}\n\nfunc splitArg(arg []byte) [][]byte {\n\ta := [MAX_MSG_ARGS][]byte{}\n\targs := a[:0]\n\tstart := -1\n\tfor i, b := range arg {\n\t\tswitch b {\n\t\tcase ' ', '\\t', '\\r', '\\n':\n\t\t\tif start >= 0 {\n\t\t\t\targs = append(args, arg[start:i])\n\t\t\t\tstart = -1\n\t\t\t}\n\t\tdefault:\n\t\t\tif start < 0 {\n\t\t\t\tstart = i\n\t\t\t}\n\t\t}\n\t}\n\tif start >= 0 {\n\t\targs = append(args, arg[start:])\n\t}\n\treturn args\n}\n\nfunc (c *client) processSub(argo []byte) (err error) {\n\tc.traceInOp(\"SUB\", argo)\n\n\t// Indicate activity.\n\tc.in.subs++\n\n\t// Copy so we do not reference a potentially large buffer\n\t// FIXME(dlc) - make more efficient.\n\targ := make([]byte, len(argo))\n\tcopy(arg, argo)\n\targs := splitArg(arg)\n\tsub := &subscription{client: c}\n\tswitch len(args) {\n\tcase 2:\n\t\tsub.subject = args[0]\n\t\tsub.queue = nil\n\t\tsub.sid = args[1]\n\tcase 3:\n\t\tsub.subject = args[0]\n\t\tsub.queue = args[1]\n\t\tsub.sid = args[2]\n\tdefault:\n\t\treturn fmt.Errorf(\"processSub Parse Error: '%s'\", arg)\n\t}\n\n\tc.mu.Lock()\n\tif c.nc == nil {\n\t\tc.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Grab connection type.\n\tctype := c.typ\n\n\t// Check permissions if applicable.\n\tif ctype == ROUTER {\n\t\tif !c.canExport(string(sub.subject)) {\n\t\t\tc.mu.Unlock()\n\t\t\treturn nil\n\t\t}\n\t} else if !c.canSubscribe(string(sub.subject)) {\n\t\tc.mu.Unlock()\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject))\n\t\tc.Errorf(\"Subscription Violation - User %q, Subject %q, SID %s\",\n\t\t\tc.opts.Username, sub.subject, sub.sid)\n\t\treturn nil\n\t}\n\n\t// Check if we have a maximum on the number of subscriptions.\n\tif c.msubs > 0 && len(c.subs) >= c.msubs {\n\t\tc.mu.Unlock()\n\t\tc.maxSubsExceeded()\n\t\treturn nil\n\t}\n\n\tsid := string(sub.sid)\n\tacc := c.acc\n\n\t// Subscribe here.\n\tif c.subs[sid] == nil {\n\t\tc.subs[sid] = sub\n\t\tif acc != nil && acc.sl != nil {\n\t\t\terr = acc.sl.Insert(sub)\n\t\t\tif err != nil {\n\t\t\t\tdelete(c.subs, sid)\n\t\t\t}\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\tif err != nil {\n\t\tc.sendErr(\"Invalid Subject\")\n\t\treturn nil\n\t} else if c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\tif acc != nil {\n\t\tif err := c.addShadowSubscriptions(acc, sub); err != nil {\n\t\t\tc.Errorf(err.Error())\n\t\t}\n\t\t// If we are routing and this is a local sub, add to the route map for the associated account.\n\t\tif ctype == CLIENT {\n\t\t\tc.srv.updateRouteSubscriptionMap(acc, sub, 1)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// If the client's account has stream imports and there are matches for\n// this subscription's subject, then add shadow subscriptions in\n// other accounts that can export this subject.\nfunc (c *client) addShadowSubscriptions(acc *Account, sub *subscription) error {\n\tif acc == nil {\n\t\treturn ErrMissingAccount\n\t}\n\n\tvar (\n\t\trims   [32]*streamImport\n\t\tims    = rims[:0]\n\t\trfroms [32]*streamImport\n\t\tfroms  = rfroms[:0]\n\t\ttokens []string\n\t\ttsa    [32]string\n\t\thasWC  bool\n\t)\n\n\tacc.mu.RLock()\n\t// Loop over the import subjects. We have 3 scenarios. If we exact\n\t// match or we know the proposed subject is a strict subset of the\n\t// import we can subscribe to the subscription's subject directly.\n\t// The third scenario is where the proposed subject has a wildcard\n\t// and may not be an exact subset, but is a match. Therefore we have to\n\t// subscribe to the import subject, not the subscription's subject.\n\tfor _, im := range acc.imports.streams {\n\t\tif im.invalid {\n\t\t\tcontinue\n\t\t}\n\t\tsubj := string(sub.subject)\n\t\tif subj == im.prefix+im.from {\n\t\t\tims = append(ims, im)\n\t\t\tcontinue\n\t\t}\n\t\tif tokens == nil {\n\t\t\ttokens = tsa[:0]\n\t\t\tstart := 0\n\t\t\tfor i := 0; i < len(subj); i++ {\n\t\t\t\t// This is not perfect, but the test below will\n\t\t\t\t// be more exact, this is just to trigger the\n\t\t\t\t// additional test.\n\t\t\t\tif subj[i] == pwc || subj[i] == fwc {\n\t\t\t\t\thasWC = true\n\t\t\t\t} else if subj[i] == btsep {\n\t\t\t\t\ttokens = append(tokens, subj[start:i])\n\t\t\t\t\tstart = i + 1\n\t\t\t\t}\n\t\t\t}\n\t\t\ttokens = append(tokens, subj[start:])\n\t\t}\n\t\tif isSubsetMatch(tokens, im.prefix+im.from) {\n\t\t\tims = append(ims, im)\n\t\t} else if hasWC {\n\t\t\tif subjectIsSubsetMatch(im.prefix+im.from, subj) {\n\t\t\t\tfroms = append(froms, im)\n\t\t\t}\n\t\t}\n\t}\n\tacc.mu.RUnlock()\n\n\tvar shadow []*subscription\n\n\tif len(ims) > 0 || len(froms) > 0 {\n\t\tshadow = make([]*subscription, 0, len(ims)+len(froms))\n\t}\n\n\t// Now walk through collected importMaps\n\tfor _, im := range ims {\n\t\t// We will create a shadow subscription.\n\t\tnsub, err := c.addShadowSub(sub, im, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tshadow = append(shadow, nsub)\n\t}\n\t// Now walk through importMaps that we need to subscribe\n\t// exactly to the from property.\n\tfor _, im := range froms {\n\t\t// We will create a shadow subscription.\n\t\tnsub, err := c.addShadowSub(sub, im, true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tshadow = append(shadow, nsub)\n\t}\n\n\tif shadow != nil {\n\t\tc.mu.Lock()\n\t\tsub.shadow = shadow\n\t\tc.mu.Unlock()\n\t}\n\n\treturn nil\n}\n\n// Add in the shadow subscription.\nfunc (c *client) addShadowSub(sub *subscription, im *streamImport, useFrom bool) (*subscription, error) {\n\tnsub := *sub // copy\n\tnsub.im = im\n\tif useFrom {\n\t\tnsub.subject = []byte(im.from)\n\t} else if im.prefix != \"\" {\n\t\t// redo subject here to match subject in the publisher account space.\n\t\t// Just remove prefix from what they gave us. That maps into other space.\n\t\tnsub.subject = sub.subject[len(im.prefix):]\n\t}\n\n\tc.Debugf(\"Creating import subscription on %q from account %q\", nsub.subject, im.acc.Name)\n\n\tif err := im.acc.sl.Insert(&nsub); err != nil {\n\t\terrs := fmt.Sprintf(\"Could not add shadow import subscription for account %q\", im.acc.Name)\n\t\tc.Debugf(errs)\n\t\treturn nil, fmt.Errorf(errs)\n\t}\n\n\t// Update our route map here.\n\tc.srv.updateRouteSubscriptionMap(im.acc, &nsub, 1)\n\treturn &nsub, nil\n}\n\n// canSubscribe determines if the client is authorized to subscribe to the\n// given subject. Assumes caller is holding lock.\nfunc (c *client) canSubscribe(subject string) bool {\n\tif c.perms == nil {\n\t\treturn true\n\t}\n\n\tallowed := true\n\n\t// Check allow list. If no allow list that means all are allowed. Deny can overrule.\n\tif c.perms.sub.allow != nil {\n\t\tr := c.perms.sub.allow.Match(subject)\n\t\tallowed = len(r.psubs) != 0\n\t}\n\t// If we have a deny list and we think we are allowed, check that as well.\n\tif allowed && c.perms.sub.deny != nil {\n\t\tr := c.perms.sub.deny.Match(subject)\n\t\tallowed = len(r.psubs) == 0\n\n\t\t// We use the actual subscription to signal us to spin up the deny mperms\n\t\t// and cache. We check if the subject is a wildcard that contains any of\n\t\t// the deny clauses.\n\t\t// FIXME(dlc) - We could be smarter and track when these go away and remove.\n\t\tif allowed && c.mperms == nil && subjectHasWildcard(subject) {\n\t\t\t// Whip through the deny array and check if this wildcard subject is within scope.\n\t\t\tfor _, sub := range c.darray {\n\t\t\t\ttokens := strings.Split(sub, tsep)\n\t\t\t\tif isSubsetMatch(tokens, sub) {\n\t\t\t\t\tc.loadMsgDenyFilter()\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn allowed\n}\n\n// Low level unsubscribe for a given client.\nfunc (c *client) unsubscribe(acc *Account, sub *subscription, force bool) {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tif !force && sub.max > 0 && sub.nm < sub.max {\n\t\tc.Debugf(\n\t\t\t\"Deferring actual UNSUB(%s): %d max, %d received\",\n\t\t\tstring(sub.subject), sub.max, sub.nm)\n\t\treturn\n\t}\n\tc.traceOp(\"<-> %s\", \"DELSUB\", sub.sid)\n\n\tdelete(c.subs, string(sub.sid))\n\tif c.typ != CLIENT {\n\t\tc.removeReplySubTimeout(sub)\n\t}\n\n\tif acc != nil {\n\t\tacc.sl.Remove(sub)\n\t}\n\n\t// Check to see if we have shadow subscriptions.\n\tfor _, nsub := range sub.shadow {\n\t\tif err := nsub.im.acc.sl.Remove(nsub); err != nil {\n\t\t\tc.Debugf(\"Could not remove shadow import subscription for account %q\", nsub.im.acc.Name)\n\t\t} else if c.typ == CLIENT && c.srv != nil {\n\t\t\tc.srv.updateRouteSubscriptionMap(nsub.im.acc, nsub, -1)\n\t\t}\n\t}\n\tsub.shadow = nil\n}\n\nfunc (c *client) processUnsub(arg []byte) error {\n\tc.traceInOp(\"UNSUB\", arg)\n\targs := splitArg(arg)\n\tvar sid []byte\n\tmax := -1\n\n\tswitch len(args) {\n\tcase 1:\n\t\tsid = args[0]\n\tcase 2:\n\t\tsid = args[0]\n\t\tmax = parseSize(args[1])\n\tdefault:\n\t\treturn fmt.Errorf(\"processUnsub Parse Error: '%s'\", arg)\n\t}\n\t// Indicate activity.\n\tc.in.subs++\n\n\tvar sub *subscription\n\n\tunsub := false\n\tok := false\n\n\tc.mu.Lock()\n\n\t// Grab connection type.\n\tctype := c.typ\n\tvar acc *Account\n\n\tif sub, ok = c.subs[string(sid)]; ok {\n\t\tacc = c.acc\n\t\tif max > 0 {\n\t\t\tsub.max = int64(max)\n\t\t} else {\n\t\t\t// Clear it here to override\n\t\t\tsub.max = 0\n\t\t\tunsub = true\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\tif unsub {\n\t\tc.unsubscribe(acc, sub, false)\n\t\tif acc != nil && ctype == CLIENT {\n\t\t\tc.srv.updateRouteSubscriptionMap(acc, sub, -1)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// checkDenySub will check if we are allowed to deliver this message in the\n// presence of deny clauses for subscriptions. Deny clauses will not prevent\n// larger scoped wildcard subscriptions, so we need to check at delivery time.\n// Lock should be held.\nfunc (c *client) checkDenySub(subject string) bool {\n\tif denied, ok := c.mperms.dcache[subject]; ok {\n\t\treturn denied\n\t} else if r := c.mperms.deny.Match(subject); len(r.psubs) != 0 {\n\t\tc.mperms.dcache[subject] = true\n\t\treturn true\n\t} else {\n\t\tc.mperms.dcache[subject] = false\n\t}\n\tif len(c.mperms.dcache) > maxDenyPermCacheSize {\n\t\tc.pruneDenyCache()\n\t}\n\treturn false\n}\n\nfunc (c *client) msgHeader(mh []byte, sub *subscription, reply []byte) []byte {\n\tif len(sub.sid) > 0 {\n\t\tmh = append(mh, sub.sid...)\n\t\tmh = append(mh, ' ')\n\t}\n\tif reply != nil {\n\t\tmh = append(mh, reply...)\n\t\tmh = append(mh, ' ')\n\t}\n\tmh = append(mh, c.pa.szb...)\n\tmh = append(mh, _CRLF_...)\n\treturn mh\n}\n\n// Used to treat maps as efficient set\nvar needFlush = struct{}{}\n\nfunc (c *client) deliverMsg(sub *subscription, mh, msg []byte) bool {\n\tif sub.client == nil {\n\t\treturn false\n\t}\n\tclient := sub.client\n\tclient.mu.Lock()\n\n\t// Check echo\n\tif c == client && !client.echo {\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Check if we have a subscribe deny clause. This will trigger us to check the subject\n\t// for a match against the denied subjects.\n\tif client.mperms != nil && client.checkDenySub(string(c.pa.subject)) {\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\tsrv := client.srv\n\n\tsub.nm++\n\t// Check if we should auto-unsubscribe.\n\tif sub.max > 0 {\n\t\tif client.typ == ROUTER && sub.nm >= sub.max {\n\t\t\t// The only router based messages that we will see here are remoteReplies.\n\t\t\t// We handle these slightly differently.\n\t\t\tdefer client.removeReplySub(sub)\n\t\t} else {\n\t\t\t// For routing..\n\t\t\tshouldForward := client.typ == CLIENT && client.srv != nil\n\t\t\t// If we are at the exact number, unsubscribe but\n\t\t\t// still process the message in hand, otherwise\n\t\t\t// unsubscribe and drop message on the floor.\n\t\t\tif sub.nm == sub.max {\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit of %d reached for sid '%s'\", sub.max, string(sub.sid))\n\t\t\t\t// Due to defer, reverse the code order so that execution\n\t\t\t\t// is consistent with other cases where we unsubscribe.\n\t\t\t\tif shouldForward {\n\t\t\t\t\tdefer srv.updateRouteSubscriptionMap(client.acc, sub, -1)\n\t\t\t\t}\n\t\t\t\tdefer client.unsubscribe(client.acc, sub, true)\n\t\t\t} else if sub.nm > sub.max {\n\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit [%d] exceeded\", sub.max)\n\t\t\t\tclient.mu.Unlock()\n\t\t\t\tclient.unsubscribe(client.acc, sub, true)\n\t\t\t\tif shouldForward {\n\t\t\t\t\tsrv.updateRouteSubscriptionMap(client.acc, sub, -1)\n\t\t\t\t}\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check for closed connection\n\tif client.nc == nil {\n\t\tclient.mu.Unlock()\n\t\treturn false\n\t}\n\n\t// Update statistics\n\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tmsgSize := int64(len(msg) - LEN_CR_LF)\n\n\t// No atomic needed since accessed under client lock.\n\t// Monitor is reading those also under client's lock.\n\tclient.outMsgs++\n\tclient.outBytes += msgSize\n\n\tatomic.AddInt64(&srv.outMsgs, 1)\n\tatomic.AddInt64(&srv.outBytes, msgSize)\n\n\t// Queue to outbound buffer\n\tclient.queueOutbound(mh)\n\tclient.queueOutbound(msg)\n\n\tclient.out.pm++\n\n\t// Check outbound threshold and queue IO flush if needed.\n\tif client.out.pm > 1 && client.out.pb > maxBufSize*2 {\n\t\tclient.flushSignal()\n\t}\n\n\tif c.trace {\n\t\tclient.traceOutOp(string(mh[:len(mh)-LEN_CR_LF]), nil)\n\t}\n\n\t// Increment the flush pending signals if we are setting for the first time.\n\tif _, ok := c.pcd[client]; !ok {\n\t\tclient.out.fsp++\n\t}\n\tclient.mu.Unlock()\n\n\t// Remember for when we return to the top of the loop.\n\tc.pcd[client] = needFlush\n\n\treturn true\n}\n\n// pruneDenyCache will prune the deny cache via randomly\n// deleting items. Doing so pruneSize items at a time.\n// Lock must be held for this one since it is shared under\n// deliverMsg.\nfunc (c *client) pruneDenyCache() {\n\tr := 0\n\tfor subject := range c.mperms.dcache {\n\t\tdelete(c.mperms.dcache, subject)\n\t\tif r++; r > pruneSize {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// prunePubPermsCache will prune the cache via randomly\n// deleting items. Doing so pruneSize items at a time.\nfunc (c *client) prunePubPermsCache() {\n\tr := 0\n\tfor subject := range c.perms.pcache {\n\t\tdelete(c.perms.pcache, subject)\n\t\tif r++; r > pruneSize {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// pubAllowed checks on publish permissioning.\nfunc (c *client) pubAllowed(subject string) bool {\n\tif c.perms == nil || (c.perms.pub.allow == nil && c.perms.pub.deny == nil) {\n\t\treturn true\n\t}\n\t// Check if published subject is allowed if we have permissions in place.\n\tallowed, ok := c.perms.pcache[subject]\n\tif ok {\n\t\treturn allowed\n\t}\n\t// Cache miss, check allow then deny as needed.\n\tif c.perms.pub.allow != nil {\n\t\tr := c.perms.pub.allow.Match(subject)\n\t\tallowed = len(r.psubs) != 0\n\t} else {\n\t\t// No entries means all are allowed. Deny will overrule as needed.\n\t\tallowed = true\n\t}\n\t// If we have a deny list and are currently allowed, check that as well.\n\tif allowed && c.perms.pub.deny != nil {\n\t\tr := c.perms.pub.deny.Match(subject)\n\t\tallowed = len(r.psubs) == 0\n\t}\n\t// Update our cache here.\n\tc.perms.pcache[string(subject)] = allowed\n\t// Prune if needed.\n\tif len(c.perms.pcache) > maxPermCacheSize {\n\t\tc.prunePubPermsCache()\n\t}\n\treturn allowed\n}\n\n// Used to mimic client like replies.\nconst (\n\treplyPrefix    = \"_R_.\"\n\treplyPrefixLen = len(replyPrefix)\n\tdigits         = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n\tbase           = 62\n)\n\n// newServiceReply is used when rewriting replies that cross account boundaries.\n// These will look like _R_.XXXXXXXX.\nfunc (c *client) newServiceReply() []byte {\n\t// Check to see if we have our own rand yet. Global rand\n\t// has contention with lots of clients, etc.\n\tif c.in.prand == nil {\n\t\tc.in.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\n\tvar b = [15]byte{'_', 'R', '_', '.'}\n\trn := c.in.prand.Int63()\n\tfor i, l := replyPrefixLen, rn; i < len(b); i++ {\n\t\tb[i] = digits[l%base]\n\t\tl /= base\n\t}\n\treturn b[:]\n}\n\n// Test whether a reply subject is a service import reply.\nfunc isServiceReply(reply []byte) bool {\n\treturn len(reply) > 3 && string(reply[:4]) == replyPrefix\n}\n\n// This will decide to call the client code or router code.\nfunc (c *client) processInboundMsg(msg []byte) {\n\tswitch c.typ {\n\tcase CLIENT:\n\t\tc.processInboundClientMsg(msg)\n\tcase ROUTER:\n\t\tc.processInboundRoutedMsg(msg)\n\tcase GATEWAY:\n\t\tc.processInboundGatewayMsg(msg)\n\t}\n}\n\n// processInboundClientMsg is called to process an inbound msg from a client.\nfunc (c *client) processInboundClientMsg(msg []byte) {\n\t// Update statistics\n\t// The msg includes the CR_LF, so pull back out for accounting.\n\tc.in.msgs++\n\tc.in.bytes += len(msg) - LEN_CR_LF\n\n\tif c.trace {\n\t\tc.traceMsg(msg)\n\t}\n\n\t// Check pub permissions\n\tif c.perms != nil && (c.perms.pub.allow != nil || c.perms.pub.deny != nil) && !c.pubAllowed(string(c.pa.subject)) {\n\t\tc.pubPermissionViolation(c.pa.subject)\n\t\treturn\n\t}\n\n\t// Now check for reserved replies. These are used for service imports.\n\tif isServiceReply(c.pa.reply) {\n\t\tc.replySubjectViolation(c.pa.reply)\n\t\treturn\n\t}\n\n\tif c.opts.Verbose {\n\t\tc.sendOK()\n\t}\n\n\t// Mostly under testing scenarios.\n\tif c.srv == nil || c.acc == nil {\n\t\treturn\n\t}\n\n\t// Match the subscriptions. We will use our own L1 map if\n\t// it's still valid, avoiding contention on the shared sublist.\n\tvar r *SublistResult\n\tvar ok bool\n\n\tgenid := atomic.LoadUint64(&c.acc.sl.genid)\n\tif genid == c.in.genid && c.in.results != nil {\n\t\tr, ok = c.in.results[string(c.pa.subject)]\n\t} else {\n\t\t// Reset our L1 completely.\n\t\tc.in.results = make(map[string]*SublistResult)\n\t\tc.in.genid = genid\n\t}\n\n\t// Go back to the sublist data structure.\n\tif !ok {\n\t\tr = c.acc.sl.Match(string(c.pa.subject))\n\t\tc.in.results[string(c.pa.subject)] = r\n\t\t// Prune the results cache. Keeps us from unbounded growth. Random delete.\n\t\tif len(c.in.results) > maxResultCacheSize {\n\t\t\tn := 0\n\t\t\tfor subject := range c.in.results {\n\t\t\t\tdelete(c.in.results, subject)\n\t\t\t\tif n++; n > pruneSize {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check to see if we need to map/route to another account.\n\tif c.acc.imports.services != nil {\n\t\tc.checkForImportServices(c.acc, msg)\n\t}\n\n\tvar qa [16][]byte\n\tqueues := qa[:0]\n\n\t// Check for no interest, short circuit if so.\n\t// This is the fanout scale.\n\tif len(r.psubs)+len(r.qsubs) > 0 {\n\t\tvar qnames *[][]byte\n\t\t// If we have queue subs in this cluster, then if we run in gateway\n\t\t// mode and the remote gateways have queue subs, then we need to\n\t\t// collect the queue groups this message was sent to so that we\n\t\t// exclude them when sending to gateways.\n\t\tif len(r.qsubs) > 0 && c.srv.gateway.enabled && atomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\n\t\t\tqnames = &queues\n\t\t}\n\t\tc.processMsgResults(c.acc, r, msg, c.pa.subject, c.pa.reply, qnames)\n\t}\n\n\t// Now deal with gateways\n\tif c.srv.gateway.enabled {\n\t\tc.sendMsgToGateways(msg, queues)\n\t}\n}\n\n// This checks and process import services by doing the mapping and sending the\n// message onward if applicable.\nfunc (c *client) checkForImportServices(acc *Account, msg []byte) {\n\tif acc == nil || acc.imports.services == nil {\n\t\treturn\n\t}\n\tacc.mu.RLock()\n\trm := acc.imports.services[string(c.pa.subject)]\n\tinvalid := rm != nil && rm.invalid\n\tacc.mu.RUnlock()\n\t// Get the results from the other account for the mapped \"to\" subject.\n\t// If we have been marked invalid simply return here.\n\tif rm != nil && !invalid && rm.acc != nil && rm.acc.sl != nil {\n\t\tvar nrr []byte\n\t\tif rm.ae {\n\t\t\tacc.removeServiceImport(rm.from)\n\t\t}\n\t\tif c.pa.reply != nil {\n\t\t\t// We want to remap this to provide anonymity.\n\t\t\tnrr = c.newServiceReply()\n\t\t\trm.acc.addImplicitServiceImport(acc, string(nrr), string(c.pa.reply), true, nil)\n\t\t}\n\t\t// FIXME(dlc) - Do L1 cache trick from above.\n\t\trr := rm.acc.sl.Match(rm.to)\n\t\tc.processMsgResults(rm.acc, rr, msg, []byte(rm.to), nrr, nil)\n\t}\n}\n\nfunc (c *client) addSubToRouteTargets(sub *subscription) {\n\tif c.in.rts == nil {\n\t\tc.in.rts = make([]routeTarget, 0, routeTargetInit)\n\t}\n\n\tfor i := range c.in.rts {\n\t\trt := &c.in.rts[i]\n\t\tif rt.sub.client == sub.client {\n\t\t\tif sub.queue != nil {\n\t\t\t\trt.qs = append(rt.qs, sub.queue...)\n\t\t\t\trt.qs = append(rt.qs, ' ')\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\t// If we are here we do not have the sub yet in our list\n\t// If we have to grow do so here.\n\tif len(c.in.rts) == cap(c.in.rts) {\n\t\tc.in.rts = append(c.in.rts, routeTarget{})\n\t}\n\n\tvar rt *routeTarget\n\n\tlrts := len(c.in.rts)\n\tc.in.rts = c.in.rts[:lrts+1]\n\trt = &c.in.rts[lrts]\n\trt.sub = sub\n\trt.qs = rt._qs[:0]\n\tif sub.queue != nil {\n\t\trt.qs = append(rt.qs, sub.queue...)\n\t\trt.qs = append(rt.qs, ' ')\n\t}\n}\n\n// This processes the sublist results for a given message.\nfunc (c *client) processMsgResults(acc *Account, r *SublistResult, msg, subject, reply []byte, queues *[][]byte) {\n\t// msg header for clients.\n\tmsgh := c.msgb[1:msgHeadProtoLen]\n\tmsgh = append(msgh, subject...)\n\tmsgh = append(msgh, ' ')\n\tsi := len(msgh)\n\n\t// For sending messages across routes. Reset it if we have one.\n\t// We reuse this data structure.\n\tif c.in.rts != nil {\n\t\tc.in.rts = c.in.rts[:0]\n\t}\n\n\t// Loop over all normal subscriptions that match.\n\tfor _, sub := range r.psubs {\n\t\t// Check if this is a send to a ROUTER. We now process\n\t\t// these after everything else.\n\t\tif sub.client.typ == ROUTER {\n\t\t\tif c.typ == ROUTER {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc.addSubToRouteTargets(sub)\n\t\t\tcontinue\n\t\t} else if sub.client.typ == GATEWAY {\n\t\t\t// Never send to gateway from here.\n\t\t\tcontinue\n\t\t}\n\t\t// Check for stream import mapped subs. These apply to local subs only.\n\t\tif sub.im != nil && sub.im.prefix != \"\" {\n\t\t\t// Redo the subject here on the fly.\n\t\t\tmsgh = c.msgb[1:msgHeadProtoLen]\n\t\t\tmsgh = append(msgh, sub.im.prefix...)\n\t\t\tmsgh = append(msgh, c.pa.subject...)\n\t\t\tmsgh = append(msgh, ' ')\n\t\t\tsi = len(msgh)\n\t\t}\n\t\t// Normal delivery\n\t\tmh := c.msgHeader(msgh[:si], sub, reply)\n\t\tc.deliverMsg(sub, mh, msg)\n\t}\n\n\t// If we are sourced from a route we need to have direct filtered queues.\n\tif c.typ == ROUTER && c.pa.queues == nil {\n\t\treturn\n\t}\n\n\t// Set these up to optionally filter based on the queue lists.\n\t// This is for messages received from routes which will have directed\n\t// guidance on which queue groups we should deliver to.\n\tqf := c.pa.queues\n\n\t// For gateway connections, we still want to send messages to routes\n\t// even if there is no queue filters.\n\tif c.typ == GATEWAY && qf == nil {\n\t\tgoto sendToRoutes\n\t}\n\n\t// Check to see if we have our own rand yet. Global rand\n\t// has contention with lots of clients, etc.\n\tif c.in.prand == nil {\n\t\tc.in.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\n\t// Process queue subs\n\n\tfor i := 0; i < len(r.qsubs); i++ {\n\t\tqsubs := r.qsubs[i]\n\t\t// If we have a filter check that here. We could make this a map or someting more\n\t\t// complex but linear search since we expect queues to be small. Should be faster\n\t\t// and more cache friendly.\n\t\tif qf != nil && len(qsubs) > 0 {\n\t\t\ttqn := qsubs[0].queue\n\t\t\tfor _, qn := range qf {\n\t\t\t\tif bytes.Equal(qn, tqn) {\n\t\t\t\t\tgoto selectQSub\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\tselectQSub:\n\n\t\t// We will hold onto remote qsubs when we are coming from a route\n\t\t// just in case we can no longer do local delivery.\n\t\tvar rsub *subscription\n\n\t\t// Find a subscription that is able to deliver this message\n\t\t// starting at a random index.\n\t\tstartIndex := c.in.prand.Intn(len(qsubs))\n\t\tfor i := 0; i < len(qsubs); i++ {\n\t\t\tindex := (startIndex + i) % len(qsubs)\n\t\t\tsub := qsubs[index]\n\t\t\tif sub == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Potentially sending to a remote sub across a route.\n\t\t\tif sub.client.typ == ROUTER {\n\t\t\t\tif c.typ == ROUTER {\n\t\t\t\t\t// We just came from a route, so skip and prefer local subs.\n\t\t\t\t\t// Keep our first rsub in case all else fails.\n\t\t\t\t\tif rsub == nil {\n\t\t\t\t\t\trsub = sub\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\tc.addSubToRouteTargets(sub)\n\t\t\t\t\tif queues != nil {\n\t\t\t\t\t\t*queues = append(*queues, sub.queue)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// Check for mapped subs\n\t\t\tif sub.im != nil && sub.im.prefix != \"\" {\n\t\t\t\t// Redo the subject here on the fly.\n\t\t\t\tmsgh = c.msgb[1:msgHeadProtoLen]\n\t\t\t\tmsgh = append(msgh, sub.im.prefix...)\n\t\t\t\tmsgh = append(msgh, c.pa.subject...)\n\t\t\t\tmsgh = append(msgh, ' ')\n\t\t\t\tsi = len(msgh)\n\t\t\t}\n\n\t\t\tmh := c.msgHeader(msgh[:si], sub, reply)\n\t\t\tif c.deliverMsg(sub, mh, msg) {\n\t\t\t\t// Clear rsub\n\t\t\t\trsub = nil\n\t\t\t\tif queues != nil {\n\t\t\t\t\t*queues = append(*queues, sub.queue)\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif rsub != nil {\n\t\t\t// If we are here we tried to deliver to a local qsub\n\t\t\t// but failed. So we will send it to a remote.\n\t\t\tc.addSubToRouteTargets(rsub)\n\t\t\tif queues != nil {\n\t\t\t\t*queues = append(*queues, rsub.queue)\n\t\t\t}\n\t\t}\n\t}\n\nsendToRoutes:\n\n\t// If no messages for routes return here.\n\tif len(c.in.rts) == 0 {\n\t\treturn\n\t}\n\n\t// We address by index to avoid struct copy. We have inline structs for memory\n\t// layout and cache coherency.\n\tfor i := range c.in.rts {\n\t\trt := &c.in.rts[i]\n\n\t\tmh := c.msgb[:msgHeadProtoLen]\n\t\tmh = append(mh, acc.Name...)\n\t\tmh = append(mh, ' ')\n\t\tmh = append(mh, subject...)\n\t\tmh = append(mh, ' ')\n\n\t\tif len(rt.qs) > 0 {\n\t\t\tif reply != nil {\n\t\t\t\tmh = append(mh, \"+ \"...) // Signal that there is a reply.\n\t\t\t\tmh = append(mh, reply...)\n\t\t\t\tmh = append(mh, ' ')\n\t\t\t} else {\n\t\t\t\tmh = append(mh, \"| \"...) // Only queues\n\t\t\t}\n\t\t\tmh = append(mh, rt.qs...)\n\t\t} else if reply != nil {\n\t\t\tmh = append(mh, reply...)\n\t\t\tmh = append(mh, ' ')\n\t\t}\n\t\tmh = append(mh, c.pa.szb...)\n\t\tmh = append(mh, _CRLF_...)\n\t\tc.deliverMsg(rt.sub, mh, msg)\n\t}\n}\n\nfunc (c *client) pubPermissionViolation(subject []byte) {\n\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish to %q\", subject))\n\tc.Errorf(\"Publish Violation - User %q, Subject %q\", c.opts.Username, subject)\n}\n\nfunc (c *client) replySubjectViolation(reply []byte) {\n\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish with Reply of %q\", reply))\n\tc.Errorf(\"Publish Violation - User %q, Reply %q\", c.opts.Username, reply)\n}\n\nfunc (c *client) processPingTimer() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tc.ping.tmr = nil\n\t// Check if connection is still opened\n\tif c.nc == nil {\n\t\treturn\n\t}\n\n\tc.Debugf(\"%s Ping Timer\", c.typeString())\n\n\t// If we have had activity within the PingInterval no\n\t// need to send a ping.\n\tif delta := time.Since(c.last); delta < c.srv.getOpts().PingInterval {\n\t\tc.Debugf(\"Delaying PING due to activity %v ago\", delta.Round(time.Second))\n\t} else {\n\t\t// Check for violation\n\t\tif c.ping.out+1 > c.srv.getOpts().MaxPingsOut {\n\t\t\tc.Debugf(\"Stale Client Connection - Closing\")\n\t\t\tc.sendProto([]byte(fmt.Sprintf(\"-ERR '%s'\\r\\n\", \"Stale Connection\")), true)\n\t\t\tc.clearConnection(StaleConnection)\n\t\t\treturn\n\t\t}\n\t\t// Send PING\n\t\tc.sendPing()\n\t}\n\n\t// Reset to fire again.\n\tc.setPingTimer()\n}\n\n// Lock should be held\nfunc (c *client) setPingTimer() {\n\tif c.srv == nil {\n\t\treturn\n\t}\n\td := c.srv.getOpts().PingInterval\n\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\n}\n\n// Lock should be held\nfunc (c *client) clearPingTimer() {\n\tif c.ping.tmr == nil {\n\t\treturn\n\t}\n\tc.ping.tmr.Stop()\n\tc.ping.tmr = nil\n}\n\n// Lock should be held\nfunc (c *client) setAuthTimer(d time.Duration) {\n\tc.atmr = time.AfterFunc(d, c.authTimeout)\n}\n\n// Lock should be held\nfunc (c *client) clearAuthTimer() bool {\n\tif c.atmr == nil {\n\t\treturn true\n\t}\n\tstopped := c.atmr.Stop()\n\tc.atmr = nil\n\treturn stopped\n}\n\n// We may reuse atmr for expiring user jwts,\n// so check connectReceived.\nfunc (c *client) awaitingAuth() bool {\n\tc.mu.Lock()\n\tauthSet := !c.flags.isSet(connectReceived) && c.atmr != nil\n\tc.mu.Unlock()\n\treturn authSet\n}\n\n// This will set the atmr for the JWT expiration time.\n// We will lock on entry.\nfunc (c *client) setExpirationTimer(d time.Duration) {\n\tc.mu.Lock()\n\tc.atmr = time.AfterFunc(d, c.authExpired)\n\tc.mu.Unlock()\n}\n\n// Lock should be held\nfunc (c *client) clearConnection(reason ClosedState) {\n\tif c.flags.isSet(clearConnection) {\n\t\treturn\n\t}\n\tc.flags.set(clearConnection)\n\n\tnc := c.nc\n\tif nc == nil || c.srv == nil {\n\t\treturn\n\t}\n\t// Flush any pending.\n\tc.flushOutbound()\n\n\t// Clear outbound here.\n\tc.out.sg.Broadcast()\n\n\t// With TLS, Close() is sending an alert (that is doing a write).\n\t// Need to set a deadline otherwise the server could block there\n\t// if the peer is not reading from socket.\n\tif c.flags.isSet(handshakeComplete) {\n\t\tnc.SetWriteDeadline(time.Now().Add(c.out.wdl))\n\t}\n\tnc.Close()\n\t// Do this always to also kick out any IO writes.\n\tnc.SetWriteDeadline(time.Time{})\n\n\t// Save off the connection if its a client.\n\tif c.typ == CLIENT && c.srv != nil {\n\t\tgo c.srv.saveClosedClient(c, nc, reason)\n\t}\n}\n\nfunc (c *client) typeString() string {\n\tswitch c.typ {\n\tcase CLIENT:\n\t\treturn \"Client\"\n\tcase ROUTER:\n\t\treturn \"Router\"\n\tcase GATEWAY:\n\t\treturn \"Gateway\"\n\t}\n\treturn \"Unknown Type\"\n}\n\n// processSubsOnConfigReload removes any subscriptions the client has that are no\n// longer authorized, and check for imports (accounts) due to a config reload.\nfunc (c *client) processSubsOnConfigReload(awcsti map[string]struct{}) {\n\tc.mu.Lock()\n\tvar (\n\t\tcheckPerms = c.perms != nil\n\t\tcheckAcc   = c.acc != nil\n\t\tacc        = c.acc\n\t)\n\tif !checkPerms && !checkAcc {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\tvar (\n\t\t_subs    [32]*subscription\n\t\tsubs     = _subs[:0]\n\t\t_removed [32]*subscription\n\t\tremoved  = _removed[:0]\n\t\tsrv      = c.srv\n\t\tuserInfo = c.opts.Nkey\n\t)\n\tif userInfo == \"\" {\n\t\tuserInfo = c.opts.Username\n\t\tif userInfo == \"\" {\n\t\t\tuserInfo = fmt.Sprintf(\"%v\", c.cid)\n\t\t}\n\t}\n\tif checkAcc {\n\t\t// We actually only want to check if stream imports have changed.\n\t\tif _, ok := awcsti[acc.Name]; !ok {\n\t\t\tcheckAcc = false\n\t\t}\n\t}\n\t// We will clear any mperms we have here. It will rebuild on the fly with canSubscribe,\n\t// so we do that here as we collect them. We will check result down below.\n\tc.mperms = nil\n\t// Collect client's subs under the lock\n\tfor _, sub := range c.subs {\n\t\t// Just checking to rebuild mperms under the lock, will collect removed though here.\n\t\t// Only collect under subs array of canSubscribe and checkAcc true.\n\t\tif !c.canSubscribe(string(sub.subject)) {\n\t\t\tremoved = append(removed, sub)\n\t\t} else if checkAcc {\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t}\n\tc.mu.Unlock()\n\n\t// This list is all subs who are allowed and we need to check accounts.\n\tfor _, sub := range subs {\n\t\tc.mu.Lock()\n\t\toldShadows := sub.shadow\n\t\tsub.shadow = nil\n\t\tc.mu.Unlock()\n\t\tc.addShadowSubscriptions(acc, sub)\n\t\tfor _, nsub := range oldShadows {\n\t\t\tnsub.im.acc.sl.Remove(nsub)\n\t\t}\n\t}\n\n\t// Unsubscribe all that need to be removed and report back to client and logs.\n\tfor _, sub := range removed {\n\t\tc.unsubscribe(acc, sub, true)\n\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q (sid %q)\",\n\t\t\tsub.subject, sub.sid))\n\t\tsrv.Noticef(\"Removed sub %q (sid %q) for user %q - not authorized\",\n\t\t\tsub.subject, sub.sid, userInfo)\n\t}\n}\n\n// Allows us to count up all the queue subscribers during close.\ntype qsub struct {\n\tsub *subscription\n\tn   int32\n}\n\nfunc (c *client) closeConnection(reason ClosedState) {\n\tc.mu.Lock()\n\tif c.nc == nil {\n\t\tc.mu.Unlock()\n\t\treturn\n\t}\n\n\t// Be consistent with the creation: for routes and gateways,\n\t// we use Noticef on create, so use that too for delete.\n\tif c.typ == ROUTER || c.typ == GATEWAY {\n\t\tc.Noticef(\"%s connection closed\", c.typeString())\n\t} else {\n\t\tc.Debugf(\"%s connection closed\", c.typeString())\n\t}\n\n\tc.clearAuthTimer()\n\tc.clearPingTimer()\n\tc.clearConnection(reason)\n\tc.nc = nil\n\n\tvar (\n\t\tretryImplicit bool\n\t\tconnectURLs   []string\n\t\tgwName        string\n\t\tgwIsOutbound  bool\n\t\tgwCfg         *gatewayCfg\n\t\tctype         = c.typ\n\t\tsrv           = c.srv\n\t\tnoReconnect   = c.flags.isSet(noReconnect)\n\t\tacc           = c.acc\n\t)\n\n\t// Snapshot for use if we are a client connection.\n\t// FIXME(dlc) - we can just stub in a new one for client\n\t// and reference existing one.\n\tvar subs []*subscription\n\tif ctype == CLIENT {\n\t\tsubs = make([]*subscription, 0, len(c.subs))\n\t\tfor _, sub := range c.subs {\n\t\t\t// Auto-unsubscribe subscriptions must be unsubscribed forcibly.\n\t\t\tsub.max = 0\n\t\t\tsubs = append(subs, sub)\n\t\t}\n\t}\n\n\tif c.route != nil {\n\t\tif !noReconnect {\n\t\t\tretryImplicit = c.route.retry\n\t\t}\n\t\tconnectURLs = c.route.connectURLs\n\t}\n\tif ctype == GATEWAY {\n\t\tgwName = c.gw.name\n\t\tgwIsOutbound = c.gw.outbound\n\t\tgwCfg = c.gw.cfg\n\t}\n\n\tc.mu.Unlock()\n\n\t// Remove clients subscriptions.\n\tif ctype == CLIENT {\n\t\tacc.sl.RemoveBatch(subs)\n\t} else {\n\t\tgo c.removeRemoteSubs()\n\t}\n\n\tif srv != nil {\n\t\t// This is a route that disconnected, but we are not in lame duck mode...\n\t\tif len(connectURLs) > 0 && !srv.isLameDuckMode() {\n\t\t\t// Unless disabled, possibly update the server's INFO protocol\n\t\t\t// and send to clients that know how to handle async INFOs.\n\t\t\tif !srv.getOpts().Cluster.NoAdvertise {\n\t\t\t\tsrv.removeClientConnectURLsAndSendINFOToClients(connectURLs)\n\t\t\t}\n\t\t}\n\n\t\t// Unregister\n\t\tsrv.removeClient(c)\n\n\t\t// Update remote subscriptions.\n\t\tif acc != nil && ctype == CLIENT {\n\t\t\tqsubs := map[string]*qsub{}\n\t\t\tfor _, sub := range subs {\n\t\t\t\tif sub.queue == nil {\n\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\n\t\t\t\t} else {\n\t\t\t\t\t// We handle queue subscribers special in case we\n\t\t\t\t\t// have a bunch we can just send one update to the\n\t\t\t\t\t// connected routes.\n\t\t\t\t\tkey := string(sub.subject) + \" \" + string(sub.queue)\n\t\t\t\t\tif esub, ok := qsubs[key]; ok {\n\t\t\t\t\t\tesub.n++\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqsubs[key] = &qsub{sub, 1}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Process any qsubs here.\n\t\t\tfor _, esub := range qsubs {\n\t\t\t\tsrv.updateRouteSubscriptionMap(acc, esub.sub, -(esub.n))\n\t\t\t}\n\t\t\tif prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\n\t\t\t\tc.srv.mu.Lock()\n\t\t\t\tc.srv.activeAccounts--\n\t\t\t\tc.srv.mu.Unlock()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Don't reconnect connections that have been marked with\n\t// the no reconnect flag.\n\tif noReconnect {\n\t\treturn\n\t}\n\n\t// Check for a solicited route. If it was, start up a reconnect unless\n\t// we are already connected to the other end.\n\tif c.isSolicitedRoute() || retryImplicit {\n\t\t// Capture these under lock\n\t\tc.mu.Lock()\n\t\trid := c.route.remoteID\n\t\trtype := c.route.routeType\n\t\trurl := c.route.url\n\t\tc.mu.Unlock()\n\n\t\tsrv.mu.Lock()\n\t\tdefer srv.mu.Unlock()\n\n\t\t// It is possible that the server is being shutdown.\n\t\t// If so, don't try to reconnect\n\t\tif !srv.running {\n\t\t\treturn\n\t\t}\n\n\t\tif rid != \"\" && srv.remotes[rid] != nil {\n\t\t\tc.srv.Debugf(\"Not attempting reconnect for solicited route, already connected to \\\"%s\\\"\", rid)\n\t\t\treturn\n\t\t} else if rid == srv.info.ID {\n\t\t\tc.srv.Debugf(\"Detected route to self, ignoring \\\"%s\\\"\", rurl)\n\t\t\treturn\n\t\t} else if rtype != Implicit || retryImplicit {\n\t\t\tc.srv.Debugf(\"Attempting reconnect for solicited route \\\"%s\\\"\", rurl)\n\t\t\t// Keep track of this go-routine so we can wait for it on\n\t\t\t// server shutdown.\n\t\t\tsrv.startGoRoutine(func() { srv.reConnectToRoute(rurl, rtype) })\n\t\t}\n\t} else if srv != nil && ctype == GATEWAY && gwIsOutbound {\n\t\tif gwCfg != nil {\n\t\t\tsrv.Debugf(\"Attempting reconnect for gateway %q\", gwName)\n\t\t\t// Run this as a go routine since we may be called within\n\t\t\t// the solicitGateway itself if there was an error during\n\t\t\t// the creation of the gateway connection.\n\t\t\tsrv.startGoRoutine(func() { srv.reconnectGateway(gwCfg) })\n\t\t} else {\n\t\t\tsrv.Debugf(\"Gateway %q not in configuration, not attempting reconnect\", gwName)\n\t\t}\n\t}\n}\n\n// Set the noReconnect flag. This is used before a call to closeConnection()\n// to prevent the connection to reconnect (routes, gateways).\nfunc (c *client) setNoReconnect() {\n\tc.mu.Lock()\n\tc.flags.set(noReconnect)\n\tc.mu.Unlock()\n}\n\n// Returns the client's RTT value with the protection of the client's lock.\nfunc (c *client) getRTTValue() time.Duration {\n\tc.mu.Lock()\n\trtt := c.rtt\n\tc.mu.Unlock()\n\treturn rtt\n}\n\n// This function is used by ROUTER and GATEWAY connections to\n// look for a subject on a given account (since these type of\n// connections are not bound to a specific account).\n// If the c.pa.subject is found in the cache, the cached result\n// is returned, otherwse, we match the account's sublist and update\n// the cache. The cache is pruned if reaching a certain size.\nfunc (c *client) getAccAndResultFromCache() (*Account, *SublistResult) {\n\tvar (\n\t\tacc *Account\n\t\tpac *perAccountCache\n\t\tr   *SublistResult\n\t\tok  bool\n\t)\n\t// Check our cache.\n\tif pac, ok = c.in.pacache[string(c.pa.pacache)]; ok {\n\t\t// Check the genid to see if it's still valid.\n\t\tif genid := atomic.LoadUint64(&pac.acc.sl.genid); genid != pac.genid {\n\t\t\tok = false\n\t\t\tdelete(c.in.pacache, string(c.pa.pacache))\n\t\t} else {\n\t\t\tacc = pac.acc\n\t\t\tr = pac.results\n\t\t}\n\t}\n\n\tif !ok {\n\t\t// Match correct account and sublist.\n\t\tacc = c.srv.LookupAccount(string(c.pa.account))\n\t\tif acc == nil {\n\t\t\treturn nil, nil\n\t\t}\n\n\t\t// Match against the account sublist.\n\t\tr = acc.sl.Match(string(c.pa.subject))\n\n\t\t// Store in our cache\n\t\tc.in.pacache[string(c.pa.pacache)] = &perAccountCache{acc, r, atomic.LoadUint64(&acc.sl.genid)}\n\n\t\t// Check if we need to prune.\n\t\tif len(c.in.pacache) > maxPerAccountCacheSize {\n\t\t\tc.prunePerAccountCache()\n\t\t}\n\t}\n\treturn acc, r\n}\n\n// prunePerAccountCache will prune off a random number of cache entries.\nfunc (c *client) prunePerAccountCache() {\n\tn := 0\n\tfor cacheKey := range c.in.pacache {\n\t\tdelete(c.in.pacache, cacheKey)\n\t\tif n++; n > prunePerAccountCacheSize {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// Logging functionality scoped to a client or route.\n\nfunc (c *client) Errorf(format string, v ...interface{}) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Errorf(format, v...)\n}\n\nfunc (c *client) Debugf(format string, v ...interface{}) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Debugf(format, v...)\n}\n\nfunc (c *client) Noticef(format string, v ...interface{}) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Noticef(format, v...)\n}\n\nfunc (c *client) Tracef(format string, v ...interface{}) {\n\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n\tc.srv.Tracef(format, v...)\n}\n", "idx": 5, "id": 8374, "msg": "", "proj": "nats-io-nats-server", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -90,15 +90,24 @@ class ObserveChanges extends BasePlugin {\n       const actions = {\n         add: (patch) => {\n           if (isNaN(patch.col)) {\n-            this.hot.runHooks('afterCreateRow', patch.row, 1, sourceName);\n+            const visualRow = patch.row;\n+\n+            this.rowIndexMapper.insertIndexes(visualRow, 1);\n+            this.hot.runHooks('afterCreateRow', visualRow, 1, sourceName);\n+\n           } else {\n-            this.hot.runHooks('afterCreateCol', patch.col, 1, sourceName);\n+            const visualColumn = patch.col;\n+\n+            this.columnIndexMapper.insertIndexes(visualColumn, 1);\n+            this.hot.runHooks('afterCreateCol', visualColumn, 1, sourceName);\n           }\n         },\n         remove: (patch) => {\n           if (isNaN(patch.col)) {\n+            this.rowIndexMapper.removeIndexes([patch.row]);\n             this.hot.runHooks('afterRemoveRow', patch.row, 1, sourceName);\n           } else {\n+            this.columnIndexMapper.removeIndexes([patch.col]);\n             this.hot.runHooks('afterRemoveCol', patch.col, 1, sourceName);\n           }\n         },", "y": 1, "oldf": "import BasePlugin from './../_base';\nimport DataObserver from './dataObserver';\nimport { arrayEach } from './../../helpers/array';\nimport { registerPlugin } from './../../plugins';\n\n// Handsontable.hooks.register('afterChangesObserved');\n\n/**\n * @plugin ObserveChanges\n *\n * @description\n * This plugin allows to observe data source changes. By default, the plugin is declared as `undefined`, which makes it\n * disabled. Enabling this plugin switches the table into one-way data binding where changes are applied into the data\n * source (outside from the table) will be automatically reflected in the table.\n *\n * ```js\n * // as a boolean\n * observeChanges: true,\n * ```\n *\n * To configure this plugin see {@link Options#observeChanges}.\n */\nclass ObserveChanges extends BasePlugin {\n  constructor(hotInstance) {\n    super(hotInstance);\n    /**\n     * Instance of {@link DataObserver}.\n     *\n     * @private\n     * @type {DataObserver}\n     */\n    this.observer = null;\n  }\n\n  /**\n   * Checks if the plugin is enabled in the handsontable settings. This method is executed in {@link Hooks#beforeInit}\n   * hook and if it returns `true` than the {@link ObserveChanges#enablePlugin} method is called.\n   *\n   * @returns {Boolean}\n   */\n  isEnabled() {\n    return this.hot.getSettings().observeChanges;\n  }\n\n  /**\n   * Enables the plugin functionality for this Handsontable instance.\n   */\n  enablePlugin() {\n    if (this.enabled) {\n      return;\n    }\n    if (!this.observer) {\n      this.observer = new DataObserver(this.hot.getSourceData());\n      this._exposePublicApi();\n    }\n\n    this.observer.addLocalHook('change', patches => this.onDataChange(patches));\n    this.addHook('afterCreateRow', () => this.onAfterTableAlter());\n    this.addHook('afterRemoveRow', () => this.onAfterTableAlter());\n    this.addHook('afterCreateCol', () => this.onAfterTableAlter());\n    this.addHook('afterRemoveCol', () => this.onAfterTableAlter());\n    this.addHook('afterChange', (changes, source) => this.onAfterTableAlter(source));\n    this.addHook('afterLoadData', firstRun => this.onAfterLoadData(firstRun));\n\n    super.enablePlugin();\n  }\n\n  /**\n   * Disables the plugin functionality for this Handsontable instance.\n   */\n  disablePlugin() {\n    if (this.observer) {\n      this.observer.destroy();\n      this.observer = null;\n      this._deletePublicApi();\n    }\n\n    super.disablePlugin();\n  }\n\n  /**\n   * Data change observer.\n   *\n   * @private\n   * @param {Array} patches An array of objects which every item defines coordinates where data was changed.\n   */\n  onDataChange(patches) {\n    if (!this.observer.isPaused()) {\n      const sourceName = `${this.pluginName}.change`;\n      const actions = {\n        add: (patch) => {\n          if (isNaN(patch.col)) {\n            this.hot.runHooks('afterCreateRow', patch.row, 1, sourceName);\n          } else {\n            this.hot.runHooks('afterCreateCol', patch.col, 1, sourceName);\n          }\n        },\n        remove: (patch) => {\n          if (isNaN(patch.col)) {\n            this.hot.runHooks('afterRemoveRow', patch.row, 1, sourceName);\n          } else {\n            this.hot.runHooks('afterRemoveCol', patch.col, 1, sourceName);\n          }\n        },\n        replace: (patch) => {\n          this.hot.runHooks('afterChange', [[patch.row, patch.col, null, patch.value]], sourceName);\n        },\n      };\n\n      arrayEach(patches, (patch) => {\n        if (actions[patch.op]) {\n          actions[patch.op](patch);\n        }\n      });\n      this.hot.render();\n    }\n\n    this.hot.runHooks('afterChangesObserved');\n  }\n\n  /**\n   * On after table alter listener. Prevents infinity loop between internal and external data changing.\n   *\n   * @private\n   * @param source\n   */\n  onAfterTableAlter(source) {\n    if (source !== 'loadData') {\n      this.observer.pause();\n      this.hot.addHookOnce('afterChangesObserved', () => this.observer.resume());\n    }\n  }\n\n  /**\n   * On after load data listener.\n   *\n   * @private\n   * @param {Boolean} firstRun `true` if event was fired first time.\n   */\n  onAfterLoadData(firstRun) {\n    if (!firstRun) {\n      this.observer.setObservedData(this.hot.getSourceData());\n    }\n  }\n\n  /**\n   * Destroys the plugin instance.\n   */\n  destroy() {\n    if (this.observer) {\n      this.observer.destroy();\n      this._deletePublicApi();\n    }\n    super.destroy();\n  }\n\n  /**\n   * Expose plugins methods to the core.\n   *\n   * @private\n   */\n  _exposePublicApi() {\n    const hot = this.hot;\n\n    hot.pauseObservingChanges = () => this.observer.pause();\n    hot.resumeObservingChanges = () => this.observer.resume();\n    hot.isPausedObservingChanges = () => this.observer.isPaused();\n  }\n\n  /**\n   * Deletes all previously exposed methods.\n   *\n   * @private\n   */\n  _deletePublicApi() {\n    const hot = this.hot;\n\n    delete hot.pauseObservingChanges;\n    delete hot.resumeObservingChanges;\n    delete hot.isPausedObservingChanges;\n  }\n}\n\nexport default ObserveChanges;\n\nregisterPlugin('observeChanges', ObserveChanges);\n", "idx": 1, "id": 15207, "msg": "For `afterCreateCol` we cache the `const visualColumn = patch.col` variable. Should we do the same here?", "proj": "handsontable-handsontable", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -4857,7 +4857,7 @@ func (m *SdkVolumeDeleteRequest) Reset()         { *m = SdkVolumeDeleteRequest{}\n func (m *SdkVolumeDeleteRequest) String() string { return proto.CompactTextString(m) }\n func (*SdkVolumeDeleteRequest) ProtoMessage()    {}\n func (*SdkVolumeDeleteRequest) Descriptor() ([]byte, []int) {\n-\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{73}\n+\treturn fileDescriptor_api_feccc90d7cea2234, []int{73}\n }\n func (m *SdkVolumeDeleteRequest) XXX_Unmarshal(b []byte) error {\n \treturn xxx_messageInfo_SdkVolumeDeleteRequest.Unmarshal(m, b)", "y": 0, "oldf": "// Code generated by protoc-gen-go. DO NOT EDIT.\n// source: api/api.proto\n\npackage api\n\nimport proto \"github.com/golang/protobuf/proto\"\nimport fmt \"fmt\"\nimport math \"math\"\nimport timestamp \"github.com/golang/protobuf/ptypes/timestamp\"\nimport _ \"google.golang.org/genproto/googleapis/api/annotations\"\n\nimport (\n\tcontext \"golang.org/x/net/context\"\n\tgrpc \"google.golang.org/grpc\"\n)\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package\n\ntype Status int32\n\nconst (\n\tStatus_STATUS_NONE                     Status = 0\n\tStatus_STATUS_INIT                     Status = 1\n\tStatus_STATUS_OK                       Status = 2\n\tStatus_STATUS_OFFLINE                  Status = 3\n\tStatus_STATUS_ERROR                    Status = 4\n\tStatus_STATUS_NOT_IN_QUORUM            Status = 5\n\tStatus_STATUS_DECOMMISSION             Status = 6\n\tStatus_STATUS_MAINTENANCE              Status = 7\n\tStatus_STATUS_STORAGE_DOWN             Status = 8\n\tStatus_STATUS_STORAGE_DEGRADED         Status = 9\n\tStatus_STATUS_NEEDS_REBOOT             Status = 10\n\tStatus_STATUS_STORAGE_REBALANCE        Status = 11\n\tStatus_STATUS_STORAGE_DRIVE_REPLACE    Status = 12\n\tStatus_STATUS_NOT_IN_QUORUM_NO_STORAGE Status = 13\n\t// Add statuses before MAX and update the number for MAX\n\tStatus_STATUS_MAX Status = 14\n)\n\nvar Status_name = map[int32]string{\n\t0:  \"STATUS_NONE\",\n\t1:  \"STATUS_INIT\",\n\t2:  \"STATUS_OK\",\n\t3:  \"STATUS_OFFLINE\",\n\t4:  \"STATUS_ERROR\",\n\t5:  \"STATUS_NOT_IN_QUORUM\",\n\t6:  \"STATUS_DECOMMISSION\",\n\t7:  \"STATUS_MAINTENANCE\",\n\t8:  \"STATUS_STORAGE_DOWN\",\n\t9:  \"STATUS_STORAGE_DEGRADED\",\n\t10: \"STATUS_NEEDS_REBOOT\",\n\t11: \"STATUS_STORAGE_REBALANCE\",\n\t12: \"STATUS_STORAGE_DRIVE_REPLACE\",\n\t13: \"STATUS_NOT_IN_QUORUM_NO_STORAGE\",\n\t14: \"STATUS_MAX\",\n}\nvar Status_value = map[string]int32{\n\t\"STATUS_NONE\":                     0,\n\t\"STATUS_INIT\":                     1,\n\t\"STATUS_OK\":                       2,\n\t\"STATUS_OFFLINE\":                  3,\n\t\"STATUS_ERROR\":                    4,\n\t\"STATUS_NOT_IN_QUORUM\":            5,\n\t\"STATUS_DECOMMISSION\":             6,\n\t\"STATUS_MAINTENANCE\":              7,\n\t\"STATUS_STORAGE_DOWN\":             8,\n\t\"STATUS_STORAGE_DEGRADED\":         9,\n\t\"STATUS_NEEDS_REBOOT\":             10,\n\t\"STATUS_STORAGE_REBALANCE\":        11,\n\t\"STATUS_STORAGE_DRIVE_REPLACE\":    12,\n\t\"STATUS_NOT_IN_QUORUM_NO_STORAGE\": 13,\n\t\"STATUS_MAX\":                      14,\n}\n\nfunc (x Status) String() string {\n\treturn proto.EnumName(Status_name, int32(x))\n}\nfunc (Status) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{0}\n}\n\ntype DriverType int32\n\nconst (\n\tDriverType_DRIVER_TYPE_NONE      DriverType = 0\n\tDriverType_DRIVER_TYPE_FILE      DriverType = 1\n\tDriverType_DRIVER_TYPE_BLOCK     DriverType = 2\n\tDriverType_DRIVER_TYPE_OBJECT    DriverType = 3\n\tDriverType_DRIVER_TYPE_CLUSTERED DriverType = 4\n\tDriverType_DRIVER_TYPE_GRAPH     DriverType = 5\n)\n\nvar DriverType_name = map[int32]string{\n\t0: \"DRIVER_TYPE_NONE\",\n\t1: \"DRIVER_TYPE_FILE\",\n\t2: \"DRIVER_TYPE_BLOCK\",\n\t3: \"DRIVER_TYPE_OBJECT\",\n\t4: \"DRIVER_TYPE_CLUSTERED\",\n\t5: \"DRIVER_TYPE_GRAPH\",\n}\nvar DriverType_value = map[string]int32{\n\t\"DRIVER_TYPE_NONE\":      0,\n\t\"DRIVER_TYPE_FILE\":      1,\n\t\"DRIVER_TYPE_BLOCK\":     2,\n\t\"DRIVER_TYPE_OBJECT\":    3,\n\t\"DRIVER_TYPE_CLUSTERED\": 4,\n\t\"DRIVER_TYPE_GRAPH\":     5,\n}\n\nfunc (x DriverType) String() string {\n\treturn proto.EnumName(DriverType_name, int32(x))\n}\nfunc (DriverType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{1}\n}\n\ntype FSType int32\n\nconst (\n\tFSType_FS_TYPE_NONE  FSType = 0\n\tFSType_FS_TYPE_BTRFS FSType = 1\n\tFSType_FS_TYPE_EXT4  FSType = 2\n\tFSType_FS_TYPE_FUSE  FSType = 3\n\tFSType_FS_TYPE_NFS   FSType = 4\n\tFSType_FS_TYPE_VFS   FSType = 5\n\tFSType_FS_TYPE_XFS   FSType = 6\n\tFSType_FS_TYPE_ZFS   FSType = 7\n)\n\nvar FSType_name = map[int32]string{\n\t0: \"FS_TYPE_NONE\",\n\t1: \"FS_TYPE_BTRFS\",\n\t2: \"FS_TYPE_EXT4\",\n\t3: \"FS_TYPE_FUSE\",\n\t4: \"FS_TYPE_NFS\",\n\t5: \"FS_TYPE_VFS\",\n\t6: \"FS_TYPE_XFS\",\n\t7: \"FS_TYPE_ZFS\",\n}\nvar FSType_value = map[string]int32{\n\t\"FS_TYPE_NONE\":  0,\n\t\"FS_TYPE_BTRFS\": 1,\n\t\"FS_TYPE_EXT4\":  2,\n\t\"FS_TYPE_FUSE\":  3,\n\t\"FS_TYPE_NFS\":   4,\n\t\"FS_TYPE_VFS\":   5,\n\t\"FS_TYPE_XFS\":   6,\n\t\"FS_TYPE_ZFS\":   7,\n}\n\nfunc (x FSType) String() string {\n\treturn proto.EnumName(FSType_name, int32(x))\n}\nfunc (FSType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{2}\n}\n\ntype GraphDriverChangeType int32\n\nconst (\n\tGraphDriverChangeType_GRAPH_DRIVER_CHANGE_TYPE_NONE     GraphDriverChangeType = 0\n\tGraphDriverChangeType_GRAPH_DRIVER_CHANGE_TYPE_MODIFIED GraphDriverChangeType = 1\n\tGraphDriverChangeType_GRAPH_DRIVER_CHANGE_TYPE_ADDED    GraphDriverChangeType = 2\n\tGraphDriverChangeType_GRAPH_DRIVER_CHANGE_TYPE_DELETED  GraphDriverChangeType = 3\n)\n\nvar GraphDriverChangeType_name = map[int32]string{\n\t0: \"GRAPH_DRIVER_CHANGE_TYPE_NONE\",\n\t1: \"GRAPH_DRIVER_CHANGE_TYPE_MODIFIED\",\n\t2: \"GRAPH_DRIVER_CHANGE_TYPE_ADDED\",\n\t3: \"GRAPH_DRIVER_CHANGE_TYPE_DELETED\",\n}\nvar GraphDriverChangeType_value = map[string]int32{\n\t\"GRAPH_DRIVER_CHANGE_TYPE_NONE\":     0,\n\t\"GRAPH_DRIVER_CHANGE_TYPE_MODIFIED\": 1,\n\t\"GRAPH_DRIVER_CHANGE_TYPE_ADDED\":    2,\n\t\"GRAPH_DRIVER_CHANGE_TYPE_DELETED\":  3,\n}\n\nfunc (x GraphDriverChangeType) String() string {\n\treturn proto.EnumName(GraphDriverChangeType_name, int32(x))\n}\nfunc (GraphDriverChangeType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{3}\n}\n\ntype SeverityType int32\n\nconst (\n\tSeverityType_SEVERITY_TYPE_NONE    SeverityType = 0\n\tSeverityType_SEVERITY_TYPE_ALARM   SeverityType = 1\n\tSeverityType_SEVERITY_TYPE_WARNING SeverityType = 2\n\tSeverityType_SEVERITY_TYPE_NOTIFY  SeverityType = 3\n)\n\nvar SeverityType_name = map[int32]string{\n\t0: \"SEVERITY_TYPE_NONE\",\n\t1: \"SEVERITY_TYPE_ALARM\",\n\t2: \"SEVERITY_TYPE_WARNING\",\n\t3: \"SEVERITY_TYPE_NOTIFY\",\n}\nvar SeverityType_value = map[string]int32{\n\t\"SEVERITY_TYPE_NONE\":    0,\n\t\"SEVERITY_TYPE_ALARM\":   1,\n\t\"SEVERITY_TYPE_WARNING\": 2,\n\t\"SEVERITY_TYPE_NOTIFY\":  3,\n}\n\nfunc (x SeverityType) String() string {\n\treturn proto.EnumName(SeverityType_name, int32(x))\n}\nfunc (SeverityType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{4}\n}\n\ntype ResourceType int32\n\nconst (\n\tResourceType_RESOURCE_TYPE_NONE    ResourceType = 0\n\tResourceType_RESOURCE_TYPE_VOLUME  ResourceType = 1\n\tResourceType_RESOURCE_TYPE_NODE    ResourceType = 2\n\tResourceType_RESOURCE_TYPE_CLUSTER ResourceType = 3\n\tResourceType_RESOURCE_TYPE_DRIVE   ResourceType = 4\n)\n\nvar ResourceType_name = map[int32]string{\n\t0: \"RESOURCE_TYPE_NONE\",\n\t1: \"RESOURCE_TYPE_VOLUME\",\n\t2: \"RESOURCE_TYPE_NODE\",\n\t3: \"RESOURCE_TYPE_CLUSTER\",\n\t4: \"RESOURCE_TYPE_DRIVE\",\n}\nvar ResourceType_value = map[string]int32{\n\t\"RESOURCE_TYPE_NONE\":    0,\n\t\"RESOURCE_TYPE_VOLUME\":  1,\n\t\"RESOURCE_TYPE_NODE\":    2,\n\t\"RESOURCE_TYPE_CLUSTER\": 3,\n\t\"RESOURCE_TYPE_DRIVE\":   4,\n}\n\nfunc (x ResourceType) String() string {\n\treturn proto.EnumName(ResourceType_name, int32(x))\n}\nfunc (ResourceType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{5}\n}\n\ntype AlertActionType int32\n\nconst (\n\tAlertActionType_ALERT_ACTION_TYPE_NONE   AlertActionType = 0\n\tAlertActionType_ALERT_ACTION_TYPE_DELETE AlertActionType = 1\n\tAlertActionType_ALERT_ACTION_TYPE_CREATE AlertActionType = 2\n\tAlertActionType_ALERT_ACTION_TYPE_UPDATE AlertActionType = 3\n)\n\nvar AlertActionType_name = map[int32]string{\n\t0: \"ALERT_ACTION_TYPE_NONE\",\n\t1: \"ALERT_ACTION_TYPE_DELETE\",\n\t2: \"ALERT_ACTION_TYPE_CREATE\",\n\t3: \"ALERT_ACTION_TYPE_UPDATE\",\n}\nvar AlertActionType_value = map[string]int32{\n\t\"ALERT_ACTION_TYPE_NONE\":   0,\n\t\"ALERT_ACTION_TYPE_DELETE\": 1,\n\t\"ALERT_ACTION_TYPE_CREATE\": 2,\n\t\"ALERT_ACTION_TYPE_UPDATE\": 3,\n}\n\nfunc (x AlertActionType) String() string {\n\treturn proto.EnumName(AlertActionType_name, int32(x))\n}\nfunc (AlertActionType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{6}\n}\n\ntype VolumeActionParam int32\n\nconst (\n\tVolumeActionParam_VOLUME_ACTION_PARAM_NONE VolumeActionParam = 0\n\t// Maps to the boolean value false\n\tVolumeActionParam_VOLUME_ACTION_PARAM_OFF VolumeActionParam = 1\n\t// Maps to the boolean value true.\n\tVolumeActionParam_VOLUME_ACTION_PARAM_ON VolumeActionParam = 2\n)\n\nvar VolumeActionParam_name = map[int32]string{\n\t0: \"VOLUME_ACTION_PARAM_NONE\",\n\t1: \"VOLUME_ACTION_PARAM_OFF\",\n\t2: \"VOLUME_ACTION_PARAM_ON\",\n}\nvar VolumeActionParam_value = map[string]int32{\n\t\"VOLUME_ACTION_PARAM_NONE\": 0,\n\t\"VOLUME_ACTION_PARAM_OFF\":  1,\n\t\"VOLUME_ACTION_PARAM_ON\":   2,\n}\n\nfunc (x VolumeActionParam) String() string {\n\treturn proto.EnumName(VolumeActionParam_name, int32(x))\n}\nfunc (VolumeActionParam) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{7}\n}\n\ntype CosType int32\n\nconst (\n\tCosType_NONE   CosType = 0\n\tCosType_LOW    CosType = 1\n\tCosType_MEDIUM CosType = 2\n\tCosType_HIGH   CosType = 3\n)\n\nvar CosType_name = map[int32]string{\n\t0: \"NONE\",\n\t1: \"LOW\",\n\t2: \"MEDIUM\",\n\t3: \"HIGH\",\n}\nvar CosType_value = map[string]int32{\n\t\"NONE\":   0,\n\t\"LOW\":    1,\n\t\"MEDIUM\": 2,\n\t\"HIGH\":   3,\n}\n\nfunc (x CosType) String() string {\n\treturn proto.EnumName(CosType_name, int32(x))\n}\nfunc (CosType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{8}\n}\n\ntype IoProfile int32\n\nconst (\n\tIoProfile_IO_PROFILE_SEQUENTIAL IoProfile = 0\n\tIoProfile_IO_PROFILE_RANDOM     IoProfile = 1\n\tIoProfile_IO_PROFILE_DB         IoProfile = 2\n\tIoProfile_IO_PROFILE_DB_REMOTE  IoProfile = 3\n\tIoProfile_IO_PROFILE_CMS        IoProfile = 4\n)\n\nvar IoProfile_name = map[int32]string{\n\t0: \"IO_PROFILE_SEQUENTIAL\",\n\t1: \"IO_PROFILE_RANDOM\",\n\t2: \"IO_PROFILE_DB\",\n\t3: \"IO_PROFILE_DB_REMOTE\",\n\t4: \"IO_PROFILE_CMS\",\n}\nvar IoProfile_value = map[string]int32{\n\t\"IO_PROFILE_SEQUENTIAL\": 0,\n\t\"IO_PROFILE_RANDOM\":     1,\n\t\"IO_PROFILE_DB\":         2,\n\t\"IO_PROFILE_DB_REMOTE\":  3,\n\t\"IO_PROFILE_CMS\":        4,\n}\n\nfunc (x IoProfile) String() string {\n\treturn proto.EnumName(IoProfile_name, int32(x))\n}\nfunc (IoProfile) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{9}\n}\n\n// VolumeState represents the state of a volume.\ntype VolumeState int32\n\nconst (\n\tVolumeState_VOLUME_STATE_NONE VolumeState = 0\n\t// Volume is transitioning to new state\n\tVolumeState_VOLUME_STATE_PENDING VolumeState = 1\n\t// Volume is ready to be assigned to a container\n\tVolumeState_VOLUME_STATE_AVAILABLE VolumeState = 2\n\t// Volume is attached to container\n\tVolumeState_VOLUME_STATE_ATTACHED VolumeState = 3\n\t// Volume is detached but associated with a container\n\tVolumeState_VOLUME_STATE_DETACHED VolumeState = 4\n\t// Volume detach is in progress\n\tVolumeState_VOLUME_STATE_DETATCHING VolumeState = 5\n\t// Volume is in error state\n\tVolumeState_VOLUME_STATE_ERROR VolumeState = 6\n\t// Volume is deleted, it will remain in this state\n\t// while resources are asynchronously reclaimed\n\tVolumeState_VOLUME_STATE_DELETED VolumeState = 7\n\t// Volume is trying to be detached\n\tVolumeState_VOLUME_STATE_TRY_DETACHING VolumeState = 8\n\t// Volume is undergoing restore\n\tVolumeState_VOLUME_STATE_RESTORE VolumeState = 9\n)\n\nvar VolumeState_name = map[int32]string{\n\t0: \"VOLUME_STATE_NONE\",\n\t1: \"VOLUME_STATE_PENDING\",\n\t2: \"VOLUME_STATE_AVAILABLE\",\n\t3: \"VOLUME_STATE_ATTACHED\",\n\t4: \"VOLUME_STATE_DETACHED\",\n\t5: \"VOLUME_STATE_DETATCHING\",\n\t6: \"VOLUME_STATE_ERROR\",\n\t7: \"VOLUME_STATE_DELETED\",\n\t8: \"VOLUME_STATE_TRY_DETACHING\",\n\t9: \"VOLUME_STATE_RESTORE\",\n}\nvar VolumeState_value = map[string]int32{\n\t\"VOLUME_STATE_NONE\":          0,\n\t\"VOLUME_STATE_PENDING\":       1,\n\t\"VOLUME_STATE_AVAILABLE\":     2,\n\t\"VOLUME_STATE_ATTACHED\":      3,\n\t\"VOLUME_STATE_DETACHED\":      4,\n\t\"VOLUME_STATE_DETATCHING\":    5,\n\t\"VOLUME_STATE_ERROR\":         6,\n\t\"VOLUME_STATE_DELETED\":       7,\n\t\"VOLUME_STATE_TRY_DETACHING\": 8,\n\t\"VOLUME_STATE_RESTORE\":       9,\n}\n\nfunc (x VolumeState) String() string {\n\treturn proto.EnumName(VolumeState_name, int32(x))\n}\nfunc (VolumeState) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{10}\n}\n\n// VolumeStatus represents a health status for a volume.\ntype VolumeStatus int32\n\nconst (\n\tVolumeStatus_VOLUME_STATUS_NONE VolumeStatus = 0\n\t// Volume is not present\n\tVolumeStatus_VOLUME_STATUS_NOT_PRESENT VolumeStatus = 1\n\t// Volume is healthy\n\tVolumeStatus_VOLUME_STATUS_UP VolumeStatus = 2\n\t// Volume is in fail mode\n\tVolumeStatus_VOLUME_STATUS_DOWN VolumeStatus = 3\n\t// Volume is up but with degraded performance\n\t// In a RAID group, this may indicate a problem with one or more drives\n\tVolumeStatus_VOLUME_STATUS_DEGRADED VolumeStatus = 4\n)\n\nvar VolumeStatus_name = map[int32]string{\n\t0: \"VOLUME_STATUS_NONE\",\n\t1: \"VOLUME_STATUS_NOT_PRESENT\",\n\t2: \"VOLUME_STATUS_UP\",\n\t3: \"VOLUME_STATUS_DOWN\",\n\t4: \"VOLUME_STATUS_DEGRADED\",\n}\nvar VolumeStatus_value = map[string]int32{\n\t\"VOLUME_STATUS_NONE\":        0,\n\t\"VOLUME_STATUS_NOT_PRESENT\": 1,\n\t\"VOLUME_STATUS_UP\":          2,\n\t\"VOLUME_STATUS_DOWN\":        3,\n\t\"VOLUME_STATUS_DEGRADED\":    4,\n}\n\nfunc (x VolumeStatus) String() string {\n\treturn proto.EnumName(VolumeStatus_name, int32(x))\n}\nfunc (VolumeStatus) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{11}\n}\n\ntype StorageMedium int32\n\nconst (\n\t// Magnetic spinning disk.\n\tStorageMedium_STORAGE_MEDIUM_MAGNETIC StorageMedium = 0\n\t// SSD disk\n\tStorageMedium_STORAGE_MEDIUM_SSD StorageMedium = 1\n\t// NVME disk\n\tStorageMedium_STORAGE_MEDIUM_NVME StorageMedium = 2\n)\n\nvar StorageMedium_name = map[int32]string{\n\t0: \"STORAGE_MEDIUM_MAGNETIC\",\n\t1: \"STORAGE_MEDIUM_SSD\",\n\t2: \"STORAGE_MEDIUM_NVME\",\n}\nvar StorageMedium_value = map[string]int32{\n\t\"STORAGE_MEDIUM_MAGNETIC\": 0,\n\t\"STORAGE_MEDIUM_SSD\":      1,\n\t\"STORAGE_MEDIUM_NVME\":     2,\n}\n\nfunc (x StorageMedium) String() string {\n\treturn proto.EnumName(StorageMedium_name, int32(x))\n}\nfunc (StorageMedium) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{12}\n}\n\ntype ClusterNotify int32\n\nconst (\n\t// Node is down\n\tClusterNotify_CLUSTER_NOTIFY_DOWN ClusterNotify = 0\n)\n\nvar ClusterNotify_name = map[int32]string{\n\t0: \"CLUSTER_NOTIFY_DOWN\",\n}\nvar ClusterNotify_value = map[string]int32{\n\t\"CLUSTER_NOTIFY_DOWN\": 0,\n}\n\nfunc (x ClusterNotify) String() string {\n\treturn proto.EnumName(ClusterNotify_name, int32(x))\n}\nfunc (ClusterNotify) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{13}\n}\n\ntype AttachState int32\n\nconst (\n\t// Attached and available externally\n\tAttachState_ATTACH_STATE_EXTERNAL AttachState = 0\n\t// Attached but only available internally\n\tAttachState_ATTACH_STATE_INTERNAL AttachState = 1\n\t// Switching from External to Internal\n\tAttachState_ATTACH_STATE_INTERNAL_SWITCH AttachState = 2\n)\n\nvar AttachState_name = map[int32]string{\n\t0: \"ATTACH_STATE_EXTERNAL\",\n\t1: \"ATTACH_STATE_INTERNAL\",\n\t2: \"ATTACH_STATE_INTERNAL_SWITCH\",\n}\nvar AttachState_value = map[string]int32{\n\t\"ATTACH_STATE_EXTERNAL\":        0,\n\t\"ATTACH_STATE_INTERNAL\":        1,\n\t\"ATTACH_STATE_INTERNAL_SWITCH\": 2,\n}\n\nfunc (x AttachState) String() string {\n\treturn proto.EnumName(AttachState_name, int32(x))\n}\nfunc (AttachState) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{14}\n}\n\ntype OperationFlags int32\n\nconst (\n\tOperationFlags_OP_FLAGS_UNKNOWN OperationFlags = 0\n\tOperationFlags_OP_FLAGS_NONE    OperationFlags = 1\n\t// Perform a force_detach during detach operation\n\tOperationFlags_OP_FLAGS_DETACH_FORCE OperationFlags = 2\n)\n\nvar OperationFlags_name = map[int32]string{\n\t0: \"OP_FLAGS_UNKNOWN\",\n\t1: \"OP_FLAGS_NONE\",\n\t2: \"OP_FLAGS_DETACH_FORCE\",\n}\nvar OperationFlags_value = map[string]int32{\n\t\"OP_FLAGS_UNKNOWN\":      0,\n\t\"OP_FLAGS_NONE\":         1,\n\t\"OP_FLAGS_DETACH_FORCE\": 2,\n}\n\nfunc (x OperationFlags) String() string {\n\treturn proto.EnumName(OperationFlags_name, int32(x))\n}\nfunc (OperationFlags) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{15}\n}\n\ntype SdkCloudBackupOpType int32\n\nconst (\n\tSdkCloudBackupOpType_SdkCloudBackupOpTypeUnknown   SdkCloudBackupOpType = 0\n\tSdkCloudBackupOpType_SdkCloudBackupOpTypeBackupOp  SdkCloudBackupOpType = 1\n\tSdkCloudBackupOpType_SdkCloudBackupOpTypeRestoreOp SdkCloudBackupOpType = 2\n)\n\nvar SdkCloudBackupOpType_name = map[int32]string{\n\t0: \"SdkCloudBackupOpTypeUnknown\",\n\t1: \"SdkCloudBackupOpTypeBackupOp\",\n\t2: \"SdkCloudBackupOpTypeRestoreOp\",\n}\nvar SdkCloudBackupOpType_value = map[string]int32{\n\t\"SdkCloudBackupOpTypeUnknown\":   0,\n\t\"SdkCloudBackupOpTypeBackupOp\":  1,\n\t\"SdkCloudBackupOpTypeRestoreOp\": 2,\n}\n\nfunc (x SdkCloudBackupOpType) String() string {\n\treturn proto.EnumName(SdkCloudBackupOpType_name, int32(x))\n}\nfunc (SdkCloudBackupOpType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{16}\n}\n\ntype SdkCloudBackupStatusType int32\n\nconst (\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeUnknown    SdkCloudBackupStatusType = 0\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeNotStarted SdkCloudBackupStatusType = 1\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeDone       SdkCloudBackupStatusType = 2\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeAborted    SdkCloudBackupStatusType = 3\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypePaused     SdkCloudBackupStatusType = 4\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeStopped    SdkCloudBackupStatusType = 5\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeActive     SdkCloudBackupStatusType = 6\n\tSdkCloudBackupStatusType_SdkCloudBackupStatusTypeFailed     SdkCloudBackupStatusType = 7\n)\n\nvar SdkCloudBackupStatusType_name = map[int32]string{\n\t0: \"SdkCloudBackupStatusTypeUnknown\",\n\t1: \"SdkCloudBackupStatusTypeNotStarted\",\n\t2: \"SdkCloudBackupStatusTypeDone\",\n\t3: \"SdkCloudBackupStatusTypeAborted\",\n\t4: \"SdkCloudBackupStatusTypePaused\",\n\t5: \"SdkCloudBackupStatusTypeStopped\",\n\t6: \"SdkCloudBackupStatusTypeActive\",\n\t7: \"SdkCloudBackupStatusTypeFailed\",\n}\nvar SdkCloudBackupStatusType_value = map[string]int32{\n\t\"SdkCloudBackupStatusTypeUnknown\":    0,\n\t\"SdkCloudBackupStatusTypeNotStarted\": 1,\n\t\"SdkCloudBackupStatusTypeDone\":       2,\n\t\"SdkCloudBackupStatusTypeAborted\":    3,\n\t\"SdkCloudBackupStatusTypePaused\":     4,\n\t\"SdkCloudBackupStatusTypeStopped\":    5,\n\t\"SdkCloudBackupStatusTypeActive\":     6,\n\t\"SdkCloudBackupStatusTypeFailed\":     7,\n}\n\nfunc (x SdkCloudBackupStatusType) String() string {\n\treturn proto.EnumName(SdkCloudBackupStatusType_name, int32(x))\n}\nfunc (SdkCloudBackupStatusType) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{17}\n}\n\ntype SdkCloudBackupRequestedState int32\n\nconst (\n\tSdkCloudBackupRequestedState_SdkCloudBackupRequestedStateUnknown SdkCloudBackupRequestedState = 0\n\tSdkCloudBackupRequestedState_SdkCloudBackupRequestedStatePause   SdkCloudBackupRequestedState = 1\n\tSdkCloudBackupRequestedState_SdkCloudBackupRequestedStateResume  SdkCloudBackupRequestedState = 2\n\tSdkCloudBackupRequestedState_SdkCloudBackupRequestedStateStop    SdkCloudBackupRequestedState = 3\n)\n\nvar SdkCloudBackupRequestedState_name = map[int32]string{\n\t0: \"SdkCloudBackupRequestedStateUnknown\",\n\t1: \"SdkCloudBackupRequestedStatePause\",\n\t2: \"SdkCloudBackupRequestedStateResume\",\n\t3: \"SdkCloudBackupRequestedStateStop\",\n}\nvar SdkCloudBackupRequestedState_value = map[string]int32{\n\t\"SdkCloudBackupRequestedStateUnknown\": 0,\n\t\"SdkCloudBackupRequestedStatePause\":   1,\n\t\"SdkCloudBackupRequestedStateResume\":  2,\n\t\"SdkCloudBackupRequestedStateStop\":    3,\n}\n\nfunc (x SdkCloudBackupRequestedState) String() string {\n\treturn proto.EnumName(SdkCloudBackupRequestedState_name, int32(x))\n}\nfunc (SdkCloudBackupRequestedState) EnumDescriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{18}\n}\n\n// StorageResource groups properties of a storage device.\n// swagger:model\ntype StorageResource struct {\n\t// Id is the LUN identifier.\n\tId string `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Path device path for this storage resource.\n\tPath string `protobuf:\"bytes,2,opt,name=path,proto3\" json:\"path,omitempty\"`\n\t// Storage medium.\n\tMedium StorageMedium `protobuf:\"varint,3,opt,name=medium,proto3,enum=openstorage.api.StorageMedium\" json:\"medium,omitempty\"`\n\t// True if this device is online.\n\tOnline bool `protobuf:\"varint,4,opt,name=online,proto3\" json:\"online,omitempty\"`\n\t// IOPS\n\tIops uint64 `protobuf:\"varint,5,opt,name=iops,proto3\" json:\"iops,omitempty\"`\n\t// SeqWrite\n\tSeqWrite float64 `protobuf:\"fixed64,6,opt,name=seq_write,json=seqWrite,proto3\" json:\"seq_write,omitempty\"`\n\t// SeqRead\n\tSeqRead float64 `protobuf:\"fixed64,7,opt,name=seq_read,json=seqRead,proto3\" json:\"seq_read,omitempty\"`\n\t// RandRW\n\tRandRW float64 `protobuf:\"fixed64,8,opt,name=randRW,proto3\" json:\"randRW,omitempty\"`\n\t// Total size in bytes.\n\tSize uint64 `protobuf:\"varint,9,opt,name=size,proto3\" json:\"size,omitempty\"`\n\t// Physical Bytes used.\n\tUsed uint64 `protobuf:\"varint,10,opt,name=used,proto3\" json:\"used,omitempty\"`\n\t// True if this device is rotational.\n\tRotationSpeed string `protobuf:\"bytes,11,opt,name=rotation_speed,json=rotationSpeed,proto3\" json:\"rotation_speed,omitempty\"`\n\t// Timestamp of last time this device was scanned.\n\tLastScan *timestamp.Timestamp `protobuf:\"bytes,12,opt,name=last_scan,json=lastScan,proto3\" json:\"last_scan,omitempty\"`\n\t// True if dedicated for metadata.\n\tMetadata             bool     `protobuf:\"varint,13,opt,name=metadata,proto3\" json:\"metadata,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *StorageResource) Reset()         { *m = StorageResource{} }\nfunc (m *StorageResource) String() string { return proto.CompactTextString(m) }\nfunc (*StorageResource) ProtoMessage()    {}\nfunc (*StorageResource) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{0}\n}\nfunc (m *StorageResource) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_StorageResource.Unmarshal(m, b)\n}\nfunc (m *StorageResource) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_StorageResource.Marshal(b, m, deterministic)\n}\nfunc (dst *StorageResource) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_StorageResource.Merge(dst, src)\n}\nfunc (m *StorageResource) XXX_Size() int {\n\treturn xxx_messageInfo_StorageResource.Size(m)\n}\nfunc (m *StorageResource) XXX_DiscardUnknown() {\n\txxx_messageInfo_StorageResource.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_StorageResource proto.InternalMessageInfo\n\nfunc (m *StorageResource) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageResource) GetPath() string {\n\tif m != nil {\n\t\treturn m.Path\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageResource) GetMedium() StorageMedium {\n\tif m != nil {\n\t\treturn m.Medium\n\t}\n\treturn StorageMedium_STORAGE_MEDIUM_MAGNETIC\n}\n\nfunc (m *StorageResource) GetOnline() bool {\n\tif m != nil {\n\t\treturn m.Online\n\t}\n\treturn false\n}\n\nfunc (m *StorageResource) GetIops() uint64 {\n\tif m != nil {\n\t\treturn m.Iops\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetSeqWrite() float64 {\n\tif m != nil {\n\t\treturn m.SeqWrite\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetSeqRead() float64 {\n\tif m != nil {\n\t\treturn m.SeqRead\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetRandRW() float64 {\n\tif m != nil {\n\t\treturn m.RandRW\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetSize() uint64 {\n\tif m != nil {\n\t\treturn m.Size\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetUsed() uint64 {\n\tif m != nil {\n\t\treturn m.Used\n\t}\n\treturn 0\n}\n\nfunc (m *StorageResource) GetRotationSpeed() string {\n\tif m != nil {\n\t\treturn m.RotationSpeed\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageResource) GetLastScan() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.LastScan\n\t}\n\treturn nil\n}\n\nfunc (m *StorageResource) GetMetadata() bool {\n\tif m != nil {\n\t\treturn m.Metadata\n\t}\n\treturn false\n}\n\n// StoragePool groups different storage devices based on their CosType\ntype StoragePool struct {\n\t// ID pool ID\n\tID int32 `protobuf:\"varint,1,opt,name=ID,proto3\" json:\"ID,omitempty\"`\n\t// Cos reflects the capabilities of this drive pool\n\tCos CosType `protobuf:\"varint,2,opt,name=Cos,proto3,enum=openstorage.api.CosType\" json:\"Cos,omitempty\"`\n\t// Medium underlying storage type\n\tMedium StorageMedium `protobuf:\"varint,3,opt,name=Medium,proto3,enum=openstorage.api.StorageMedium\" json:\"Medium,omitempty\"`\n\t// RaidLevel storage raid level\n\tRaidLevel string `protobuf:\"bytes,4,opt,name=RaidLevel,proto3\" json:\"RaidLevel,omitempty\"`\n\t// TotalSize of the pool\n\tTotalSize uint64 `protobuf:\"varint,7,opt,name=TotalSize,proto3\" json:\"TotalSize,omitempty\"`\n\t// Used size of the pool\n\tUsed uint64 `protobuf:\"varint,8,opt,name=Used,proto3\" json:\"Used,omitempty\"`\n\t// Labels is a list of user defined name-value pairs\n\tLabels               map[string]string `protobuf:\"bytes,9,rep,name=labels,proto3\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *StoragePool) Reset()         { *m = StoragePool{} }\nfunc (m *StoragePool) String() string { return proto.CompactTextString(m) }\nfunc (*StoragePool) ProtoMessage()    {}\nfunc (*StoragePool) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{1}\n}\nfunc (m *StoragePool) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_StoragePool.Unmarshal(m, b)\n}\nfunc (m *StoragePool) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_StoragePool.Marshal(b, m, deterministic)\n}\nfunc (dst *StoragePool) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_StoragePool.Merge(dst, src)\n}\nfunc (m *StoragePool) XXX_Size() int {\n\treturn xxx_messageInfo_StoragePool.Size(m)\n}\nfunc (m *StoragePool) XXX_DiscardUnknown() {\n\txxx_messageInfo_StoragePool.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_StoragePool proto.InternalMessageInfo\n\nfunc (m *StoragePool) GetID() int32 {\n\tif m != nil {\n\t\treturn m.ID\n\t}\n\treturn 0\n}\n\nfunc (m *StoragePool) GetCos() CosType {\n\tif m != nil {\n\t\treturn m.Cos\n\t}\n\treturn CosType_NONE\n}\n\nfunc (m *StoragePool) GetMedium() StorageMedium {\n\tif m != nil {\n\t\treturn m.Medium\n\t}\n\treturn StorageMedium_STORAGE_MEDIUM_MAGNETIC\n}\n\nfunc (m *StoragePool) GetRaidLevel() string {\n\tif m != nil {\n\t\treturn m.RaidLevel\n\t}\n\treturn \"\"\n}\n\nfunc (m *StoragePool) GetTotalSize() uint64 {\n\tif m != nil {\n\t\treturn m.TotalSize\n\t}\n\treturn 0\n}\n\nfunc (m *StoragePool) GetUsed() uint64 {\n\tif m != nil {\n\t\treturn m.Used\n\t}\n\treturn 0\n}\n\nfunc (m *StoragePool) GetLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.Labels\n\t}\n\treturn nil\n}\n\n// VolumeLocator is a structure that is attached to a volume\n// and is used to carry opaque metadata.\n// swagger:model\ntype VolumeLocator struct {\n\t// User friendly identifier\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// A set of name-value pairs that acts as search filters\n\tVolumeLabels         map[string]string `protobuf:\"bytes,2,rep,name=volume_labels,json=volumeLabels,proto3\" json:\"volume_labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *VolumeLocator) Reset()         { *m = VolumeLocator{} }\nfunc (m *VolumeLocator) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeLocator) ProtoMessage()    {}\nfunc (*VolumeLocator) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{2}\n}\nfunc (m *VolumeLocator) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeLocator.Unmarshal(m, b)\n}\nfunc (m *VolumeLocator) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeLocator.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeLocator) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeLocator.Merge(dst, src)\n}\nfunc (m *VolumeLocator) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeLocator.Size(m)\n}\nfunc (m *VolumeLocator) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeLocator.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeLocator proto.InternalMessageInfo\n\nfunc (m *VolumeLocator) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeLocator) GetVolumeLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.VolumeLabels\n\t}\n\treturn nil\n}\n\n// Source is a structure that can be given to a volume\n// to seed the volume with data.\n// swagger:model\ntype Source struct {\n\t// A volume id, if specified will create a clone of the parent.\n\tParent string `protobuf:\"bytes,1,opt,name=parent,proto3\" json:\"parent,omitempty\"`\n\t// Seed will seed the volume from the specified URI\n\t// Any additional config for the source comes from the labels in the spec\n\tSeed                 string   `protobuf:\"bytes,2,opt,name=seed,proto3\" json:\"seed,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *Source) Reset()         { *m = Source{} }\nfunc (m *Source) String() string { return proto.CompactTextString(m) }\nfunc (*Source) ProtoMessage()    {}\nfunc (*Source) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{3}\n}\nfunc (m *Source) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Source.Unmarshal(m, b)\n}\nfunc (m *Source) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Source.Marshal(b, m, deterministic)\n}\nfunc (dst *Source) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Source.Merge(dst, src)\n}\nfunc (m *Source) XXX_Size() int {\n\treturn xxx_messageInfo_Source.Size(m)\n}\nfunc (m *Source) XXX_DiscardUnknown() {\n\txxx_messageInfo_Source.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Source proto.InternalMessageInfo\n\nfunc (m *Source) GetParent() string {\n\tif m != nil {\n\t\treturn m.Parent\n\t}\n\treturn \"\"\n}\n\nfunc (m *Source) GetSeed() string {\n\tif m != nil {\n\t\treturn m.Seed\n\t}\n\treturn \"\"\n}\n\n// Group represents VolumeGroup / namespace\n// All volumes in the same group share this object.\n// swagger:model\ntype Group struct {\n\t// Id common identifier across volumes that have the same group.\n\tId                   string   `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *Group) Reset()         { *m = Group{} }\nfunc (m *Group) String() string { return proto.CompactTextString(m) }\nfunc (*Group) ProtoMessage()    {}\nfunc (*Group) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{4}\n}\nfunc (m *Group) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Group.Unmarshal(m, b)\n}\nfunc (m *Group) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Group.Marshal(b, m, deterministic)\n}\nfunc (dst *Group) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Group.Merge(dst, src)\n}\nfunc (m *Group) XXX_Size() int {\n\treturn xxx_messageInfo_Group.Size(m)\n}\nfunc (m *Group) XXX_DiscardUnknown() {\n\txxx_messageInfo_Group.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Group proto.InternalMessageInfo\n\nfunc (m *Group) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\n// VolumeSpec has the properties needed to create a volume.\n// swagger:model\ntype VolumeSpec struct {\n\t// Ephemeral storage\n\tEphemeral bool `protobuf:\"varint,1,opt,name=ephemeral,proto3\" json:\"ephemeral,omitempty\"`\n\t// Size specifies the thin provisioned volume size.\n\tSize uint64 `protobuf:\"varint,2,opt,name=size,proto3\" json:\"size,omitempty\"`\n\t// Format specifies the filesystem for this volume.\n\tFormat FSType `protobuf:\"varint,3,opt,name=format,proto3,enum=openstorage.api.FSType\" json:\"format,omitempty\"`\n\t// BlockSize for the filesystem.\n\tBlockSize int64 `protobuf:\"varint,4,opt,name=block_size,json=blockSize,proto3\" json:\"block_size,omitempty\"`\n\t// HaLevel specifies the number of copies of data.\n\tHaLevel int64 `protobuf:\"varint,5,opt,name=ha_level,json=haLevel,proto3\" json:\"ha_level,omitempty\"`\n\t// Cos specifies the relative class of service.\n\tCos CosType `protobuf:\"varint,6,opt,name=cos,proto3,enum=openstorage.api.CosType\" json:\"cos,omitempty\"`\n\t// IoProfile provides a hint about application using this volume.\n\tIoProfile IoProfile `protobuf:\"varint,7,opt,name=io_profile,json=ioProfile,proto3,enum=openstorage.api.IoProfile\" json:\"io_profile,omitempty\"`\n\t// Dedupe specifies if the volume data is to be de-duplicated.\n\tDedupe bool `protobuf:\"varint,8,opt,name=dedupe,proto3\" json:\"dedupe,omitempty\"`\n\t// SnapshotInterval in minutes, set to 0 to disable snapshots\n\tSnapshotInterval uint32 `protobuf:\"varint,9,opt,name=snapshot_interval,json=snapshotInterval,proto3\" json:\"snapshot_interval,omitempty\"`\n\t// VolumeLabels configuration labels\n\tVolumeLabels map[string]string `protobuf:\"bytes,10,rep,name=volume_labels,json=volumeLabels,proto3\" json:\"volume_labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\t// Shared is true if this volume can be remotely accessed.\n\tShared bool `protobuf:\"varint,11,opt,name=shared,proto3\" json:\"shared,omitempty\"`\n\t// ReplicaSet is the desired set of nodes for the volume data.\n\tReplicaSet *ReplicaSet `protobuf:\"bytes,12,opt,name=replica_set,json=replicaSet,proto3\" json:\"replica_set,omitempty\"`\n\t// Aggregatiokn level Specifies the number of parts the volume can be aggregated from.\n\tAggregationLevel uint32 `protobuf:\"varint,13,opt,name=aggregation_level,json=aggregationLevel,proto3\" json:\"aggregation_level,omitempty\"`\n\t// Encrypted is true if this volume will be cryptographically secured.\n\tEncrypted bool `protobuf:\"varint,14,opt,name=encrypted,proto3\" json:\"encrypted,omitempty\"`\n\t// Passphrase for an encrypted volume\n\tPassphrase string `protobuf:\"bytes,15,opt,name=passphrase,proto3\" json:\"passphrase,omitempty\"`\n\t// SnapshotSchedule a well known string that specifies when snapshots should be taken.\n\tSnapshotSchedule string `protobuf:\"bytes,16,opt,name=snapshot_schedule,json=snapshotSchedule,proto3\" json:\"snapshot_schedule,omitempty\"`\n\t// Scale allows autocreation of volumes.\n\tScale uint32 `protobuf:\"varint,17,opt,name=scale,proto3\" json:\"scale,omitempty\"`\n\t// Sticky volumes cannot be deleted until the flag is removed.\n\tSticky bool `protobuf:\"varint,18,opt,name=sticky,proto3\" json:\"sticky,omitempty\"`\n\t// Group identifies a consistency group\n\tGroup *Group `protobuf:\"bytes,21,opt,name=group,proto3\" json:\"group,omitempty\"`\n\t// GroupEnforced is true if consistency group creation is enforced.\n\tGroupEnforced bool `protobuf:\"varint,22,opt,name=group_enforced,json=groupEnforced,proto3\" json:\"group_enforced,omitempty\"`\n\t// Compressed is true if this volume is to be compressed.\n\tCompressed bool `protobuf:\"varint,23,opt,name=compressed,proto3\" json:\"compressed,omitempty\"`\n\t// Cascaded is true if this volume can be populated on any node from an external source.\n\tCascaded bool `protobuf:\"varint,24,opt,name=cascaded,proto3\" json:\"cascaded,omitempty\"`\n\t// Journal is true if data for the volume goes into the journal.\n\tJournal bool `protobuf:\"varint,25,opt,name=journal,proto3\" json:\"journal,omitempty\"`\n\t// Sharedv4 is true if this volume can be accessed via sharedv4.\n\tSharedv4             bool     `protobuf:\"varint,26,opt,name=sharedv4,proto3\" json:\"sharedv4,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *VolumeSpec) Reset()         { *m = VolumeSpec{} }\nfunc (m *VolumeSpec) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeSpec) ProtoMessage()    {}\nfunc (*VolumeSpec) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{5}\n}\nfunc (m *VolumeSpec) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeSpec.Unmarshal(m, b)\n}\nfunc (m *VolumeSpec) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeSpec.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeSpec) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeSpec.Merge(dst, src)\n}\nfunc (m *VolumeSpec) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeSpec.Size(m)\n}\nfunc (m *VolumeSpec) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeSpec.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeSpec proto.InternalMessageInfo\n\nfunc (m *VolumeSpec) GetEphemeral() bool {\n\tif m != nil {\n\t\treturn m.Ephemeral\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetSize() uint64 {\n\tif m != nil {\n\t\treturn m.Size\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetFormat() FSType {\n\tif m != nil {\n\t\treturn m.Format\n\t}\n\treturn FSType_FS_TYPE_NONE\n}\n\nfunc (m *VolumeSpec) GetBlockSize() int64 {\n\tif m != nil {\n\t\treturn m.BlockSize\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetHaLevel() int64 {\n\tif m != nil {\n\t\treturn m.HaLevel\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetCos() CosType {\n\tif m != nil {\n\t\treturn m.Cos\n\t}\n\treturn CosType_NONE\n}\n\nfunc (m *VolumeSpec) GetIoProfile() IoProfile {\n\tif m != nil {\n\t\treturn m.IoProfile\n\t}\n\treturn IoProfile_IO_PROFILE_SEQUENTIAL\n}\n\nfunc (m *VolumeSpec) GetDedupe() bool {\n\tif m != nil {\n\t\treturn m.Dedupe\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetSnapshotInterval() uint32 {\n\tif m != nil {\n\t\treturn m.SnapshotInterval\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetVolumeLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.VolumeLabels\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSpec) GetShared() bool {\n\tif m != nil {\n\t\treturn m.Shared\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetReplicaSet() *ReplicaSet {\n\tif m != nil {\n\t\treturn m.ReplicaSet\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSpec) GetAggregationLevel() uint32 {\n\tif m != nil {\n\t\treturn m.AggregationLevel\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetEncrypted() bool {\n\tif m != nil {\n\t\treturn m.Encrypted\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetPassphrase() string {\n\tif m != nil {\n\t\treturn m.Passphrase\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeSpec) GetSnapshotSchedule() string {\n\tif m != nil {\n\t\treturn m.SnapshotSchedule\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeSpec) GetScale() uint32 {\n\tif m != nil {\n\t\treturn m.Scale\n\t}\n\treturn 0\n}\n\nfunc (m *VolumeSpec) GetSticky() bool {\n\tif m != nil {\n\t\treturn m.Sticky\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetGroup() *Group {\n\tif m != nil {\n\t\treturn m.Group\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSpec) GetGroupEnforced() bool {\n\tif m != nil {\n\t\treturn m.GroupEnforced\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetCompressed() bool {\n\tif m != nil {\n\t\treturn m.Compressed\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetCascaded() bool {\n\tif m != nil {\n\t\treturn m.Cascaded\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetJournal() bool {\n\tif m != nil {\n\t\treturn m.Journal\n\t}\n\treturn false\n}\n\nfunc (m *VolumeSpec) GetSharedv4() bool {\n\tif m != nil {\n\t\treturn m.Sharedv4\n\t}\n\treturn false\n}\n\n// ReplicaSet set of machine IDs (nodes) to which part of this volume is erasure\n// coded - for clustered storage arrays\n// swagger:model\ntype ReplicaSet struct {\n\tNodes                []string `protobuf:\"bytes,1,rep,name=nodes,proto3\" json:\"nodes,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *ReplicaSet) Reset()         { *m = ReplicaSet{} }\nfunc (m *ReplicaSet) String() string { return proto.CompactTextString(m) }\nfunc (*ReplicaSet) ProtoMessage()    {}\nfunc (*ReplicaSet) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{6}\n}\nfunc (m *ReplicaSet) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_ReplicaSet.Unmarshal(m, b)\n}\nfunc (m *ReplicaSet) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_ReplicaSet.Marshal(b, m, deterministic)\n}\nfunc (dst *ReplicaSet) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_ReplicaSet.Merge(dst, src)\n}\nfunc (m *ReplicaSet) XXX_Size() int {\n\treturn xxx_messageInfo_ReplicaSet.Size(m)\n}\nfunc (m *ReplicaSet) XXX_DiscardUnknown() {\n\txxx_messageInfo_ReplicaSet.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_ReplicaSet proto.InternalMessageInfo\n\nfunc (m *ReplicaSet) GetNodes() []string {\n\tif m != nil {\n\t\treturn m.Nodes\n\t}\n\treturn nil\n}\n\n// RuntimeStateMap is a list of name value mapping of driver specific runtime\n// information.\n// swagger:model\ntype RuntimeStateMap struct {\n\tRuntimeState         map[string]string `protobuf:\"bytes,1,rep,name=runtime_state,json=runtimeState,proto3\" json:\"runtime_state,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *RuntimeStateMap) Reset()         { *m = RuntimeStateMap{} }\nfunc (m *RuntimeStateMap) String() string { return proto.CompactTextString(m) }\nfunc (*RuntimeStateMap) ProtoMessage()    {}\nfunc (*RuntimeStateMap) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{7}\n}\nfunc (m *RuntimeStateMap) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_RuntimeStateMap.Unmarshal(m, b)\n}\nfunc (m *RuntimeStateMap) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_RuntimeStateMap.Marshal(b, m, deterministic)\n}\nfunc (dst *RuntimeStateMap) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_RuntimeStateMap.Merge(dst, src)\n}\nfunc (m *RuntimeStateMap) XXX_Size() int {\n\treturn xxx_messageInfo_RuntimeStateMap.Size(m)\n}\nfunc (m *RuntimeStateMap) XXX_DiscardUnknown() {\n\txxx_messageInfo_RuntimeStateMap.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_RuntimeStateMap proto.InternalMessageInfo\n\nfunc (m *RuntimeStateMap) GetRuntimeState() map[string]string {\n\tif m != nil {\n\t\treturn m.RuntimeState\n\t}\n\treturn nil\n}\n\n// Volume represents an abstract storage volume.\n// Volume represents an abstract storage volume.\n// swagger:model\ntype Volume struct {\n\t// Self referential volume ID.\n\tId string `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Source specified seed data for the volume.\n\tSource *Source `protobuf:\"bytes,2,opt,name=source,proto3\" json:\"source,omitempty\"`\n\t// Group volumes in the same group have the same group id.\n\tGroup *Group `protobuf:\"bytes,3,opt,name=group,proto3\" json:\"group,omitempty\"`\n\t// Readonly is true if this volume is to be mounted with readonly access.\n\tReadonly bool `protobuf:\"varint,4,opt,name=readonly,proto3\" json:\"readonly,omitempty\"`\n\t// User specified locator\n\tLocator *VolumeLocator `protobuf:\"bytes,5,opt,name=locator,proto3\" json:\"locator,omitempty\"`\n\t// Volume creation time\n\tCtime *timestamp.Timestamp `protobuf:\"bytes,6,opt,name=ctime,proto3\" json:\"ctime,omitempty\"`\n\t// User specified VolumeSpec\n\tSpec *VolumeSpec `protobuf:\"bytes,7,opt,name=spec,proto3\" json:\"spec,omitempty\"`\n\t// Usage is bytes consumed by vtheis volume.\n\tUsage uint64 `protobuf:\"varint,8,opt,name=usage,proto3\" json:\"usage,omitempty\"`\n\t// LastScan is the time when an integrity check was run.\n\tLastScan *timestamp.Timestamp `protobuf:\"bytes,9,opt,name=last_scan,json=lastScan,proto3\" json:\"last_scan,omitempty\"`\n\t// Format specifies the filesytem for this volume.\n\tFormat FSType `protobuf:\"varint,10,opt,name=format,proto3,enum=openstorage.api.FSType\" json:\"format,omitempty\"`\n\t// Status is the availability status of this volume.\n\tStatus VolumeStatus `protobuf:\"varint,11,opt,name=status,proto3,enum=openstorage.api.VolumeStatus\" json:\"status,omitempty\"`\n\t// State is the current runtime state of this volume.\n\tState VolumeState `protobuf:\"varint,12,opt,name=state,proto3,enum=openstorage.api.VolumeState\" json:\"state,omitempty\"`\n\t// AttachedOn is the node instance identifier for clustered systems.\n\tAttachedOn string `protobuf:\"bytes,13,opt,name=attached_on,json=attachedOn,proto3\" json:\"attached_on,omitempty\"`\n\t// AttachedState shows whether the device is attached for internal or external use.\n\tAttachedState AttachState `protobuf:\"varint,14,opt,name=attached_state,json=attachedState,proto3,enum=openstorage.api.AttachState\" json:\"attached_state,omitempty\"`\n\t// DevicePath is the device exported by block device implementations.\n\tDevicePath string `protobuf:\"bytes,15,opt,name=device_path,json=devicePath,proto3\" json:\"device_path,omitempty\"`\n\t// SecureDevicePath is the device path for an encrypted volume.\n\tSecureDevicePath string `protobuf:\"bytes,16,opt,name=secure_device_path,json=secureDevicePath,proto3\" json:\"secure_device_path,omitempty\"`\n\t// AttachPath is the mounted path in the host namespace.\n\tAttachPath []string `protobuf:\"bytes,17,rep,name=attach_path,json=attachPath,proto3\" json:\"attach_path,omitempty\"`\n\t// AttachInfo is a list of name value mappings that provides attach information.\n\tAttachInfo map[string]string `protobuf:\"bytes,18,rep,name=attach_info,json=attachInfo,proto3\" json:\"attach_info,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\t// ReplicatSets storage for this volumefor clustered storage arrays.\n\tReplicaSets []*ReplicaSet `protobuf:\"bytes,19,rep,name=replica_sets,json=replicaSets,proto3\" json:\"replica_sets,omitempty\"`\n\t// RuntimeState is a lst of name value mapping of driver specific runtime\n\t// information.\n\tRuntimeState []*RuntimeStateMap `protobuf:\"bytes,20,rep,name=runtime_state,json=runtimeState,proto3\" json:\"runtime_state,omitempty\"`\n\t// Error is the Last recorded error.\n\tError string `protobuf:\"bytes,21,opt,name=error,proto3\" json:\"error,omitempty\"`\n\t// VolumeConsumers are entities that consume this volume\n\tVolumeConsumers      []*VolumeConsumer `protobuf:\"bytes,22,rep,name=volume_consumers,json=volumeConsumers,proto3\" json:\"volume_consumers,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *Volume) Reset()         { *m = Volume{} }\nfunc (m *Volume) String() string { return proto.CompactTextString(m) }\nfunc (*Volume) ProtoMessage()    {}\nfunc (*Volume) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{8}\n}\nfunc (m *Volume) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Volume.Unmarshal(m, b)\n}\nfunc (m *Volume) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Volume.Marshal(b, m, deterministic)\n}\nfunc (dst *Volume) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Volume.Merge(dst, src)\n}\nfunc (m *Volume) XXX_Size() int {\n\treturn xxx_messageInfo_Volume.Size(m)\n}\nfunc (m *Volume) XXX_DiscardUnknown() {\n\txxx_messageInfo_Volume.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Volume proto.InternalMessageInfo\n\nfunc (m *Volume) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *Volume) GetSource() *Source {\n\tif m != nil {\n\t\treturn m.Source\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetGroup() *Group {\n\tif m != nil {\n\t\treturn m.Group\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetReadonly() bool {\n\tif m != nil {\n\t\treturn m.Readonly\n\t}\n\treturn false\n}\n\nfunc (m *Volume) GetLocator() *VolumeLocator {\n\tif m != nil {\n\t\treturn m.Locator\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetCtime() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.Ctime\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetSpec() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Spec\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetUsage() uint64 {\n\tif m != nil {\n\t\treturn m.Usage\n\t}\n\treturn 0\n}\n\nfunc (m *Volume) GetLastScan() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.LastScan\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetFormat() FSType {\n\tif m != nil {\n\t\treturn m.Format\n\t}\n\treturn FSType_FS_TYPE_NONE\n}\n\nfunc (m *Volume) GetStatus() VolumeStatus {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn VolumeStatus_VOLUME_STATUS_NONE\n}\n\nfunc (m *Volume) GetState() VolumeState {\n\tif m != nil {\n\t\treturn m.State\n\t}\n\treturn VolumeState_VOLUME_STATE_NONE\n}\n\nfunc (m *Volume) GetAttachedOn() string {\n\tif m != nil {\n\t\treturn m.AttachedOn\n\t}\n\treturn \"\"\n}\n\nfunc (m *Volume) GetAttachedState() AttachState {\n\tif m != nil {\n\t\treturn m.AttachedState\n\t}\n\treturn AttachState_ATTACH_STATE_EXTERNAL\n}\n\nfunc (m *Volume) GetDevicePath() string {\n\tif m != nil {\n\t\treturn m.DevicePath\n\t}\n\treturn \"\"\n}\n\nfunc (m *Volume) GetSecureDevicePath() string {\n\tif m != nil {\n\t\treturn m.SecureDevicePath\n\t}\n\treturn \"\"\n}\n\nfunc (m *Volume) GetAttachPath() []string {\n\tif m != nil {\n\t\treturn m.AttachPath\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetAttachInfo() map[string]string {\n\tif m != nil {\n\t\treturn m.AttachInfo\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetReplicaSets() []*ReplicaSet {\n\tif m != nil {\n\t\treturn m.ReplicaSets\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetRuntimeState() []*RuntimeStateMap {\n\tif m != nil {\n\t\treturn m.RuntimeState\n\t}\n\treturn nil\n}\n\nfunc (m *Volume) GetError() string {\n\tif m != nil {\n\t\treturn m.Error\n\t}\n\treturn \"\"\n}\n\nfunc (m *Volume) GetVolumeConsumers() []*VolumeConsumer {\n\tif m != nil {\n\t\treturn m.VolumeConsumers\n\t}\n\treturn nil\n}\n\n// Stats is a structure that represents last collected stats for a volume\n// swagger:model\ntype Stats struct {\n\t// Reads completed successfully\n\tReads uint64 `protobuf:\"varint,1,opt,name=reads,proto3\" json:\"reads,omitempty\"`\n\t// Time spent in reads in ms\n\tReadMs    uint64 `protobuf:\"varint,2,opt,name=read_ms,json=readMs,proto3\" json:\"read_ms,omitempty\"`\n\tReadBytes uint64 `protobuf:\"varint,3,opt,name=read_bytes,json=readBytes,proto3\" json:\"read_bytes,omitempty\"`\n\t// Writes completed successfully\n\tWrites uint64 `protobuf:\"varint,4,opt,name=writes,proto3\" json:\"writes,omitempty\"`\n\t// Time spent in writes in ms\n\tWriteMs    uint64 `protobuf:\"varint,5,opt,name=write_ms,json=writeMs,proto3\" json:\"write_ms,omitempty\"`\n\tWriteBytes uint64 `protobuf:\"varint,6,opt,name=write_bytes,json=writeBytes,proto3\" json:\"write_bytes,omitempty\"`\n\t// IOs curently in progress\n\tIoProgress uint64 `protobuf:\"varint,7,opt,name=io_progress,json=ioProgress,proto3\" json:\"io_progress,omitempty\"`\n\t// Time spent doing IOs ms\n\tIoMs uint64 `protobuf:\"varint,8,opt,name=io_ms,json=ioMs,proto3\" json:\"io_ms,omitempty\"`\n\t// BytesUsed\n\tBytesUsed uint64 `protobuf:\"varint,9,opt,name=bytes_used,json=bytesUsed,proto3\" json:\"bytes_used,omitempty\"`\n\t// Interval in ms during which stats were collected\n\tIntervalMs           uint64   `protobuf:\"varint,10,opt,name=interval_ms,json=intervalMs,proto3\" json:\"interval_ms,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *Stats) Reset()         { *m = Stats{} }\nfunc (m *Stats) String() string { return proto.CompactTextString(m) }\nfunc (*Stats) ProtoMessage()    {}\nfunc (*Stats) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{9}\n}\nfunc (m *Stats) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Stats.Unmarshal(m, b)\n}\nfunc (m *Stats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Stats.Marshal(b, m, deterministic)\n}\nfunc (dst *Stats) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Stats.Merge(dst, src)\n}\nfunc (m *Stats) XXX_Size() int {\n\treturn xxx_messageInfo_Stats.Size(m)\n}\nfunc (m *Stats) XXX_DiscardUnknown() {\n\txxx_messageInfo_Stats.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Stats proto.InternalMessageInfo\n\nfunc (m *Stats) GetReads() uint64 {\n\tif m != nil {\n\t\treturn m.Reads\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetReadMs() uint64 {\n\tif m != nil {\n\t\treturn m.ReadMs\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetReadBytes() uint64 {\n\tif m != nil {\n\t\treturn m.ReadBytes\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetWrites() uint64 {\n\tif m != nil {\n\t\treturn m.Writes\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetWriteMs() uint64 {\n\tif m != nil {\n\t\treturn m.WriteMs\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetWriteBytes() uint64 {\n\tif m != nil {\n\t\treturn m.WriteBytes\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetIoProgress() uint64 {\n\tif m != nil {\n\t\treturn m.IoProgress\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetIoMs() uint64 {\n\tif m != nil {\n\t\treturn m.IoMs\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetBytesUsed() uint64 {\n\tif m != nil {\n\t\treturn m.BytesUsed\n\t}\n\treturn 0\n}\n\nfunc (m *Stats) GetIntervalMs() uint64 {\n\tif m != nil {\n\t\treturn m.IntervalMs\n\t}\n\treturn 0\n}\n\n// Alert is a structure that represents an alert object\n// swagger:model\ntype Alert struct {\n\t// Id for Alert\n\tId int64 `protobuf:\"varint,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Severity of the Alert\n\tSeverity SeverityType `protobuf:\"varint,2,opt,name=severity,proto3,enum=openstorage.api.SeverityType\" json:\"severity,omitempty\"`\n\t// AlertType user defined alert type\n\tAlertType int64 `protobuf:\"varint,3,opt,name=alert_type,json=alertType,proto3\" json:\"alert_type,omitempty\"`\n\t// Message describing the Alert\n\tMessage string `protobuf:\"bytes,4,opt,name=message,proto3\" json:\"message,omitempty\"`\n\t// Timestamp when Alert occured\n\tTimestamp *timestamp.Timestamp `protobuf:\"bytes,5,opt,name=timestamp,proto3\" json:\"timestamp,omitempty\"`\n\t// ResourceId where Alert occured\n\tResourceId string `protobuf:\"bytes,6,opt,name=resource_id,json=resourceId,proto3\" json:\"resource_id,omitempty\"`\n\t// Resource where Alert occured\n\tResource ResourceType `protobuf:\"varint,7,opt,name=resource,proto3,enum=openstorage.api.ResourceType\" json:\"resource,omitempty\"`\n\t// Cleared Flag\n\tCleared bool `protobuf:\"varint,8,opt,name=cleared,proto3\" json:\"cleared,omitempty\"`\n\t// TTL in seconds for this Alert\n\tTtl uint64 `protobuf:\"varint,9,opt,name=ttl,proto3\" json:\"ttl,omitempty\"`\n\t// UniqueTag helps identify a unique alert for a given resouce\n\tUniqueTag            string   `protobuf:\"bytes,10,opt,name=unique_tag,json=uniqueTag,proto3\" json:\"unique_tag,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *Alert) Reset()         { *m = Alert{} }\nfunc (m *Alert) String() string { return proto.CompactTextString(m) }\nfunc (*Alert) ProtoMessage()    {}\nfunc (*Alert) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{10}\n}\nfunc (m *Alert) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Alert.Unmarshal(m, b)\n}\nfunc (m *Alert) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Alert.Marshal(b, m, deterministic)\n}\nfunc (dst *Alert) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Alert.Merge(dst, src)\n}\nfunc (m *Alert) XXX_Size() int {\n\treturn xxx_messageInfo_Alert.Size(m)\n}\nfunc (m *Alert) XXX_DiscardUnknown() {\n\txxx_messageInfo_Alert.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Alert proto.InternalMessageInfo\n\nfunc (m *Alert) GetId() int64 {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn 0\n}\n\nfunc (m *Alert) GetSeverity() SeverityType {\n\tif m != nil {\n\t\treturn m.Severity\n\t}\n\treturn SeverityType_SEVERITY_TYPE_NONE\n}\n\nfunc (m *Alert) GetAlertType() int64 {\n\tif m != nil {\n\t\treturn m.AlertType\n\t}\n\treturn 0\n}\n\nfunc (m *Alert) GetMessage() string {\n\tif m != nil {\n\t\treturn m.Message\n\t}\n\treturn \"\"\n}\n\nfunc (m *Alert) GetTimestamp() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.Timestamp\n\t}\n\treturn nil\n}\n\nfunc (m *Alert) GetResourceId() string {\n\tif m != nil {\n\t\treturn m.ResourceId\n\t}\n\treturn \"\"\n}\n\nfunc (m *Alert) GetResource() ResourceType {\n\tif m != nil {\n\t\treturn m.Resource\n\t}\n\treturn ResourceType_RESOURCE_TYPE_NONE\n}\n\nfunc (m *Alert) GetCleared() bool {\n\tif m != nil {\n\t\treturn m.Cleared\n\t}\n\treturn false\n}\n\nfunc (m *Alert) GetTtl() uint64 {\n\tif m != nil {\n\t\treturn m.Ttl\n\t}\n\treturn 0\n}\n\nfunc (m *Alert) GetUniqueTag() string {\n\tif m != nil {\n\t\treturn m.UniqueTag\n\t}\n\treturn \"\"\n}\n\n// Alerts is an array of Alert objects\n// swagger:model\ntype Alerts struct {\n\tAlert                []*Alert `protobuf:\"bytes,1,rep,name=alert,proto3\" json:\"alert,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *Alerts) Reset()         { *m = Alerts{} }\nfunc (m *Alerts) String() string { return proto.CompactTextString(m) }\nfunc (*Alerts) ProtoMessage()    {}\nfunc (*Alerts) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{11}\n}\nfunc (m *Alerts) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_Alerts.Unmarshal(m, b)\n}\nfunc (m *Alerts) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_Alerts.Marshal(b, m, deterministic)\n}\nfunc (dst *Alerts) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_Alerts.Merge(dst, src)\n}\nfunc (m *Alerts) XXX_Size() int {\n\treturn xxx_messageInfo_Alerts.Size(m)\n}\nfunc (m *Alerts) XXX_DiscardUnknown() {\n\txxx_messageInfo_Alerts.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_Alerts proto.InternalMessageInfo\n\nfunc (m *Alerts) GetAlert() []*Alert {\n\tif m != nil {\n\t\treturn m.Alert\n\t}\n\treturn nil\n}\n\n// ObjectstoreInfo is a structure that has current objectstore info\n// swagger:model\ntype ObjectstoreInfo struct {\n\t// UUID of objectstore\n\tUuid string `protobuf:\"bytes,1,opt,name=uuid,proto3\" json:\"uuid,omitempty\"`\n\t// VolumeID of volume used by object store\n\tVolumeId string `protobuf:\"bytes,2,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Enable/Disable created objectstore\n\tEnabled bool `protobuf:\"varint,3,opt,name=enabled,proto3\" json:\"enabled,omitempty\"`\n\t// Status of objectstore running/failed\n\tStatus string `protobuf:\"bytes,4,opt,name=status,proto3\" json:\"status,omitempty\"`\n\t// Action being taken on this objectstore\n\tAction int64 `protobuf:\"varint,5,opt,name=action,proto3\" json:\"action,omitempty\"`\n\t// AccessKey for login into objectstore\n\tAccessKey string `protobuf:\"bytes,6,opt,name=access_key,json=accessKey,proto3\" json:\"access_key,omitempty\"`\n\t// SecretKey for login into objectstore\n\tSecretKey string `protobuf:\"bytes,7,opt,name=secret_key,json=secretKey,proto3\" json:\"secret_key,omitempty\"`\n\t// Endpoints for accessing objectstore\n\tEndpoints []string `protobuf:\"bytes,8,rep,name=endpoints,proto3\" json:\"endpoints,omitempty\"`\n\t// CurrentEndpoint on which objectstore server is accessible\n\tCurrentEndPoint string `protobuf:\"bytes,9,opt,name=current_endPoint,json=currentEndPoint,proto3\" json:\"current_endPoint,omitempty\"`\n\t// AccessPort is objectstore server port\n\tAccessPort int64 `protobuf:\"varint,10,opt,name=access_port,json=accessPort,proto3\" json:\"access_port,omitempty\"`\n\t// Region for this objectstore\n\tRegion               string   `protobuf:\"bytes,11,opt,name=region,proto3\" json:\"region,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *ObjectstoreInfo) Reset()         { *m = ObjectstoreInfo{} }\nfunc (m *ObjectstoreInfo) String() string { return proto.CompactTextString(m) }\nfunc (*ObjectstoreInfo) ProtoMessage()    {}\nfunc (*ObjectstoreInfo) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{12}\n}\nfunc (m *ObjectstoreInfo) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_ObjectstoreInfo.Unmarshal(m, b)\n}\nfunc (m *ObjectstoreInfo) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_ObjectstoreInfo.Marshal(b, m, deterministic)\n}\nfunc (dst *ObjectstoreInfo) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_ObjectstoreInfo.Merge(dst, src)\n}\nfunc (m *ObjectstoreInfo) XXX_Size() int {\n\treturn xxx_messageInfo_ObjectstoreInfo.Size(m)\n}\nfunc (m *ObjectstoreInfo) XXX_DiscardUnknown() {\n\txxx_messageInfo_ObjectstoreInfo.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_ObjectstoreInfo proto.InternalMessageInfo\n\nfunc (m *ObjectstoreInfo) GetUuid() string {\n\tif m != nil {\n\t\treturn m.Uuid\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetEnabled() bool {\n\tif m != nil {\n\t\treturn m.Enabled\n\t}\n\treturn false\n}\n\nfunc (m *ObjectstoreInfo) GetStatus() string {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetAction() int64 {\n\tif m != nil {\n\t\treturn m.Action\n\t}\n\treturn 0\n}\n\nfunc (m *ObjectstoreInfo) GetAccessKey() string {\n\tif m != nil {\n\t\treturn m.AccessKey\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetSecretKey() string {\n\tif m != nil {\n\t\treturn m.SecretKey\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetEndpoints() []string {\n\tif m != nil {\n\t\treturn m.Endpoints\n\t}\n\treturn nil\n}\n\nfunc (m *ObjectstoreInfo) GetCurrentEndPoint() string {\n\tif m != nil {\n\t\treturn m.CurrentEndPoint\n\t}\n\treturn \"\"\n}\n\nfunc (m *ObjectstoreInfo) GetAccessPort() int64 {\n\tif m != nil {\n\t\treturn m.AccessPort\n\t}\n\treturn 0\n}\n\nfunc (m *ObjectstoreInfo) GetRegion() string {\n\tif m != nil {\n\t\treturn m.Region\n\t}\n\treturn \"\"\n}\n\n// VolumeCreateRequest is a structure that has the locator, source and spec\n// to create a volume\n// swagger:model\ntype VolumeCreateRequest struct {\n\t// User specified volume name and labels\n\tLocator *VolumeLocator `protobuf:\"bytes,1,opt,name=locator,proto3\" json:\"locator,omitempty\"`\n\t// Source to create volume\n\tSource *Source `protobuf:\"bytes,2,opt,name=source,proto3\" json:\"source,omitempty\"`\n\t// The storage spec for the volume\n\tSpec                 *VolumeSpec `protobuf:\"bytes,3,opt,name=spec,proto3\" json:\"spec,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}    `json:\"-\"`\n\tXXX_unrecognized     []byte      `json:\"-\"`\n\tXXX_sizecache        int32       `json:\"-\"`\n}\n\nfunc (m *VolumeCreateRequest) Reset()         { *m = VolumeCreateRequest{} }\nfunc (m *VolumeCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeCreateRequest) ProtoMessage()    {}\nfunc (*VolumeCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{13}\n}\nfunc (m *VolumeCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeCreateRequest.Unmarshal(m, b)\n}\nfunc (m *VolumeCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeCreateRequest.Merge(dst, src)\n}\nfunc (m *VolumeCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeCreateRequest.Size(m)\n}\nfunc (m *VolumeCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeCreateRequest proto.InternalMessageInfo\n\nfunc (m *VolumeCreateRequest) GetLocator() *VolumeLocator {\n\tif m != nil {\n\t\treturn m.Locator\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeCreateRequest) GetSource() *Source {\n\tif m != nil {\n\t\treturn m.Source\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeCreateRequest) GetSpec() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Spec\n\t}\n\treturn nil\n}\n\n// VolumeResponse is a structure that wraps an error.\n// swagger:response volumeResponse\ntype VolumeResponse struct {\n\t// Error message\n\t//\n\t// in: body\n\t// Required: true\n\tError                string   `protobuf:\"bytes,1,opt,name=error,proto3\" json:\"error,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *VolumeResponse) Reset()         { *m = VolumeResponse{} }\nfunc (m *VolumeResponse) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeResponse) ProtoMessage()    {}\nfunc (*VolumeResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{14}\n}\nfunc (m *VolumeResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeResponse.Unmarshal(m, b)\n}\nfunc (m *VolumeResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeResponse.Merge(dst, src)\n}\nfunc (m *VolumeResponse) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeResponse.Size(m)\n}\nfunc (m *VolumeResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeResponse proto.InternalMessageInfo\n\nfunc (m *VolumeResponse) GetError() string {\n\tif m != nil {\n\t\treturn m.Error\n\t}\n\treturn \"\"\n}\n\n// VolumeCreateResponse\n// swagger:response volumeCreateResponse\ntype VolumeCreateResponse struct {\n\t// ID of the newly created volume\n\t//\n\t// in: body\n\t// Required: true\n\tId string `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Volume Response\n\t//\n\t// in: body\n\t// Required: true\n\tVolumeResponse       *VolumeResponse `protobuf:\"bytes,2,opt,name=volume_response,json=volumeResponse,proto3\" json:\"volume_response,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}        `json:\"-\"`\n\tXXX_unrecognized     []byte          `json:\"-\"`\n\tXXX_sizecache        int32           `json:\"-\"`\n}\n\nfunc (m *VolumeCreateResponse) Reset()         { *m = VolumeCreateResponse{} }\nfunc (m *VolumeCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeCreateResponse) ProtoMessage()    {}\nfunc (*VolumeCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{15}\n}\nfunc (m *VolumeCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeCreateResponse.Unmarshal(m, b)\n}\nfunc (m *VolumeCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeCreateResponse.Merge(dst, src)\n}\nfunc (m *VolumeCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeCreateResponse.Size(m)\n}\nfunc (m *VolumeCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeCreateResponse proto.InternalMessageInfo\n\nfunc (m *VolumeCreateResponse) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeCreateResponse) GetVolumeResponse() *VolumeResponse {\n\tif m != nil {\n\t\treturn m.VolumeResponse\n\t}\n\treturn nil\n}\n\n// VolumeStateAction specifies desired actions.\n// swagger:model\ntype VolumeStateAction struct {\n\t// Attach or Detach volume\n\tAttach VolumeActionParam `protobuf:\"varint,1,opt,name=attach,proto3,enum=openstorage.api.VolumeActionParam\" json:\"attach,omitempty\"`\n\t// Mount or unmount volume\n\tMount VolumeActionParam `protobuf:\"varint,2,opt,name=mount,proto3,enum=openstorage.api.VolumeActionParam\" json:\"mount,omitempty\"`\n\t// MountPath Path where the device is mounted\n\tMountPath string `protobuf:\"bytes,3,opt,name=mount_path,json=mountPath,proto3\" json:\"mount_path,omitempty\"`\n\t// DevicePath Path returned in attach\n\tDevicePath           string   `protobuf:\"bytes,4,opt,name=device_path,json=devicePath,proto3\" json:\"device_path,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *VolumeStateAction) Reset()         { *m = VolumeStateAction{} }\nfunc (m *VolumeStateAction) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeStateAction) ProtoMessage()    {}\nfunc (*VolumeStateAction) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{16}\n}\nfunc (m *VolumeStateAction) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeStateAction.Unmarshal(m, b)\n}\nfunc (m *VolumeStateAction) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeStateAction.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeStateAction) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeStateAction.Merge(dst, src)\n}\nfunc (m *VolumeStateAction) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeStateAction.Size(m)\n}\nfunc (m *VolumeStateAction) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeStateAction.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeStateAction proto.InternalMessageInfo\n\nfunc (m *VolumeStateAction) GetAttach() VolumeActionParam {\n\tif m != nil {\n\t\treturn m.Attach\n\t}\n\treturn VolumeActionParam_VOLUME_ACTION_PARAM_NONE\n}\n\nfunc (m *VolumeStateAction) GetMount() VolumeActionParam {\n\tif m != nil {\n\t\treturn m.Mount\n\t}\n\treturn VolumeActionParam_VOLUME_ACTION_PARAM_NONE\n}\n\nfunc (m *VolumeStateAction) GetMountPath() string {\n\tif m != nil {\n\t\treturn m.MountPath\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeStateAction) GetDevicePath() string {\n\tif m != nil {\n\t\treturn m.DevicePath\n\t}\n\treturn \"\"\n}\n\n// VolumeSet specifies a request to update a volume.\n// swagger:model\ntype VolumeSetRequest struct {\n\t// User specified volume name and labels\n\tLocator *VolumeLocator `protobuf:\"bytes,1,opt,name=locator,proto3\" json:\"locator,omitempty\"`\n\t// The storage spec for the volume\n\tSpec *VolumeSpec `protobuf:\"bytes,2,opt,name=spec,proto3\" json:\"spec,omitempty\"`\n\t// State modification on this volume.\n\tAction *VolumeStateAction `protobuf:\"bytes,3,opt,name=action,proto3\" json:\"action,omitempty\"`\n\t// additional options\n\t// required for the Set operation.\n\tOptions              map[string]string `protobuf:\"bytes,4,rep,name=options,proto3\" json:\"options,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *VolumeSetRequest) Reset()         { *m = VolumeSetRequest{} }\nfunc (m *VolumeSetRequest) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeSetRequest) ProtoMessage()    {}\nfunc (*VolumeSetRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{17}\n}\nfunc (m *VolumeSetRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeSetRequest.Unmarshal(m, b)\n}\nfunc (m *VolumeSetRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeSetRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeSetRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeSetRequest.Merge(dst, src)\n}\nfunc (m *VolumeSetRequest) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeSetRequest.Size(m)\n}\nfunc (m *VolumeSetRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeSetRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeSetRequest proto.InternalMessageInfo\n\nfunc (m *VolumeSetRequest) GetLocator() *VolumeLocator {\n\tif m != nil {\n\t\treturn m.Locator\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSetRequest) GetSpec() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Spec\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSetRequest) GetAction() *VolumeStateAction {\n\tif m != nil {\n\t\treturn m.Action\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSetRequest) GetOptions() map[string]string {\n\tif m != nil {\n\t\treturn m.Options\n\t}\n\treturn nil\n}\n\n// VolumeSetResponse\n// swagger:response volumeSetResponse\ntype VolumeSetResponse struct {\n\t// Volume\n\t//\n\t// in: body\n\t// Required: true\n\tVolume *Volume `protobuf:\"bytes,1,opt,name=volume,proto3\" json:\"volume,omitempty\"`\n\t// VolumeResponse\n\t//\n\t// in: body\n\t// Required: true\n\tVolumeResponse       *VolumeResponse `protobuf:\"bytes,2,opt,name=volume_response,json=volumeResponse,proto3\" json:\"volume_response,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}        `json:\"-\"`\n\tXXX_unrecognized     []byte          `json:\"-\"`\n\tXXX_sizecache        int32           `json:\"-\"`\n}\n\nfunc (m *VolumeSetResponse) Reset()         { *m = VolumeSetResponse{} }\nfunc (m *VolumeSetResponse) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeSetResponse) ProtoMessage()    {}\nfunc (*VolumeSetResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{18}\n}\nfunc (m *VolumeSetResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeSetResponse.Unmarshal(m, b)\n}\nfunc (m *VolumeSetResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeSetResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeSetResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeSetResponse.Merge(dst, src)\n}\nfunc (m *VolumeSetResponse) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeSetResponse.Size(m)\n}\nfunc (m *VolumeSetResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeSetResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeSetResponse proto.InternalMessageInfo\n\nfunc (m *VolumeSetResponse) GetVolume() *Volume {\n\tif m != nil {\n\t\treturn m.Volume\n\t}\n\treturn nil\n}\n\nfunc (m *VolumeSetResponse) GetVolumeResponse() *VolumeResponse {\n\tif m != nil {\n\t\treturn m.VolumeResponse\n\t}\n\treturn nil\n}\n\n// SnapCreateRequest specifies a request to create a snapshot of given volume.\n// swagger:parameters snapVolume\ntype SnapCreateRequest struct {\n\t// volume id\n\tId                   string         `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\tLocator              *VolumeLocator `protobuf:\"bytes,2,opt,name=locator,proto3\" json:\"locator,omitempty\"`\n\tReadonly             bool           `protobuf:\"varint,3,opt,name=readonly,proto3\" json:\"readonly,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}       `json:\"-\"`\n\tXXX_unrecognized     []byte         `json:\"-\"`\n\tXXX_sizecache        int32          `json:\"-\"`\n}\n\nfunc (m *SnapCreateRequest) Reset()         { *m = SnapCreateRequest{} }\nfunc (m *SnapCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SnapCreateRequest) ProtoMessage()    {}\nfunc (*SnapCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{19}\n}\nfunc (m *SnapCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SnapCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SnapCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SnapCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SnapCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SnapCreateRequest.Merge(dst, src)\n}\nfunc (m *SnapCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SnapCreateRequest.Size(m)\n}\nfunc (m *SnapCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SnapCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SnapCreateRequest proto.InternalMessageInfo\n\nfunc (m *SnapCreateRequest) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *SnapCreateRequest) GetLocator() *VolumeLocator {\n\tif m != nil {\n\t\treturn m.Locator\n\t}\n\treturn nil\n}\n\nfunc (m *SnapCreateRequest) GetReadonly() bool {\n\tif m != nil {\n\t\treturn m.Readonly\n\t}\n\treturn false\n}\n\n// SnapCreateRequest specifies a response that get's returned when creating a snapshot.\n// swagger:response snapCreateResponse\ntype SnapCreateResponse struct {\n\t// VolumeCreateResponse\n\t//\n\t// in: body\n\t// Required: true\n\tVolumeCreateResponse *VolumeCreateResponse `protobuf:\"bytes,1,opt,name=volume_create_response,json=volumeCreateResponse,proto3\" json:\"volume_create_response,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}              `json:\"-\"`\n\tXXX_unrecognized     []byte                `json:\"-\"`\n\tXXX_sizecache        int32                 `json:\"-\"`\n}\n\nfunc (m *SnapCreateResponse) Reset()         { *m = SnapCreateResponse{} }\nfunc (m *SnapCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SnapCreateResponse) ProtoMessage()    {}\nfunc (*SnapCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{20}\n}\nfunc (m *SnapCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SnapCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SnapCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SnapCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SnapCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SnapCreateResponse.Merge(dst, src)\n}\nfunc (m *SnapCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SnapCreateResponse.Size(m)\n}\nfunc (m *SnapCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SnapCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SnapCreateResponse proto.InternalMessageInfo\n\nfunc (m *SnapCreateResponse) GetVolumeCreateResponse() *VolumeCreateResponse {\n\tif m != nil {\n\t\treturn m.VolumeCreateResponse\n\t}\n\treturn nil\n}\n\n// VolumeInfo\n// swagger:model\ntype VolumeInfo struct {\n\tVolumeId             string      `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tPath                 string      `protobuf:\"bytes,2,opt,name=path,proto3\" json:\"path,omitempty\"`\n\tStorage              *VolumeSpec `protobuf:\"bytes,3,opt,name=storage,proto3\" json:\"storage,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}    `json:\"-\"`\n\tXXX_unrecognized     []byte      `json:\"-\"`\n\tXXX_sizecache        int32       `json:\"-\"`\n}\n\nfunc (m *VolumeInfo) Reset()         { *m = VolumeInfo{} }\nfunc (m *VolumeInfo) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeInfo) ProtoMessage()    {}\nfunc (*VolumeInfo) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{21}\n}\nfunc (m *VolumeInfo) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeInfo.Unmarshal(m, b)\n}\nfunc (m *VolumeInfo) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeInfo.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeInfo) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeInfo.Merge(dst, src)\n}\nfunc (m *VolumeInfo) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeInfo.Size(m)\n}\nfunc (m *VolumeInfo) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeInfo.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeInfo proto.InternalMessageInfo\n\nfunc (m *VolumeInfo) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeInfo) GetPath() string {\n\tif m != nil {\n\t\treturn m.Path\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeInfo) GetStorage() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Storage\n\t}\n\treturn nil\n}\n\n// VolumeConsumer identifies a consumer for a Volume. An example of a VolumeConsumer\n// would be a Pod in Kubernetes who has mounted the PersistentVolumeClaim for the\n// Volume\n// swagger: model\ntype VolumeConsumer struct {\n\t// Name is the name of the volume consumer\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Namespace is the namespace of the volume consumer\n\tNamespace string `protobuf:\"bytes,2,opt,name=namespace,proto3\" json:\"namespace,omitempty\"`\n\t// Type is the type of the consumer. E.g a Kubernetes pod\n\tType string `protobuf:\"bytes,3,opt,name=type,proto3\" json:\"type,omitempty\"`\n\t// NodeID is the identifier of the node on which the consumer is running. This\n\t// identifier would be from the perspective of the container runtime or\n\t// orchestrator under which the volume consumer resides. For example, NodeID\n\t//  can be name of a minion in Kubernetes.\n\tNodeId string `protobuf:\"bytes,4,opt,name=node_id,json=nodeId,proto3\" json:\"node_id,omitempty\"`\n\t// OwnerName is the name of the entity who owns this volume consumer\n\tOwnerName string `protobuf:\"bytes,5,opt,name=owner_name,json=ownerName,proto3\" json:\"owner_name,omitempty\"`\n\t// OwnerType is the type of the entity who owns this volume consumer. The type would\n\t// be from the perspective of the container runtime or the orchestrator under which\n\t// the volume consumer resides. For e.g OwnerType can be a Deployment in Kubernetes.\n\tOwnerType            string   `protobuf:\"bytes,6,opt,name=owner_type,json=ownerType,proto3\" json:\"owner_type,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *VolumeConsumer) Reset()         { *m = VolumeConsumer{} }\nfunc (m *VolumeConsumer) String() string { return proto.CompactTextString(m) }\nfunc (*VolumeConsumer) ProtoMessage()    {}\nfunc (*VolumeConsumer) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{22}\n}\nfunc (m *VolumeConsumer) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_VolumeConsumer.Unmarshal(m, b)\n}\nfunc (m *VolumeConsumer) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_VolumeConsumer.Marshal(b, m, deterministic)\n}\nfunc (dst *VolumeConsumer) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_VolumeConsumer.Merge(dst, src)\n}\nfunc (m *VolumeConsumer) XXX_Size() int {\n\treturn xxx_messageInfo_VolumeConsumer.Size(m)\n}\nfunc (m *VolumeConsumer) XXX_DiscardUnknown() {\n\txxx_messageInfo_VolumeConsumer.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_VolumeConsumer proto.InternalMessageInfo\n\nfunc (m *VolumeConsumer) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeConsumer) GetNamespace() string {\n\tif m != nil {\n\t\treturn m.Namespace\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeConsumer) GetType() string {\n\tif m != nil {\n\t\treturn m.Type\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeConsumer) GetNodeId() string {\n\tif m != nil {\n\t\treturn m.NodeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeConsumer) GetOwnerName() string {\n\tif m != nil {\n\t\treturn m.OwnerName\n\t}\n\treturn \"\"\n}\n\nfunc (m *VolumeConsumer) GetOwnerType() string {\n\tif m != nil {\n\t\treturn m.OwnerType\n\t}\n\treturn \"\"\n}\n\n// GraphDriverChanges represent a list of changes between the filesystem layers\n// specified by the ID and Parent.  // Parent may be an empty string, in which\n// case there is no parent.\n// Where the Path is the filesystem path within the layered filesystem\n// swagger:model\ntype GraphDriverChanges struct {\n\tPath                 string                `protobuf:\"bytes,1,opt,name=path,proto3\" json:\"path,omitempty\"`\n\tKind                 GraphDriverChangeType `protobuf:\"varint,2,opt,name=kind,proto3,enum=openstorage.api.GraphDriverChangeType\" json:\"kind,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}              `json:\"-\"`\n\tXXX_unrecognized     []byte                `json:\"-\"`\n\tXXX_sizecache        int32                 `json:\"-\"`\n}\n\nfunc (m *GraphDriverChanges) Reset()         { *m = GraphDriverChanges{} }\nfunc (m *GraphDriverChanges) String() string { return proto.CompactTextString(m) }\nfunc (*GraphDriverChanges) ProtoMessage()    {}\nfunc (*GraphDriverChanges) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{23}\n}\nfunc (m *GraphDriverChanges) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_GraphDriverChanges.Unmarshal(m, b)\n}\nfunc (m *GraphDriverChanges) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_GraphDriverChanges.Marshal(b, m, deterministic)\n}\nfunc (dst *GraphDriverChanges) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_GraphDriverChanges.Merge(dst, src)\n}\nfunc (m *GraphDriverChanges) XXX_Size() int {\n\treturn xxx_messageInfo_GraphDriverChanges.Size(m)\n}\nfunc (m *GraphDriverChanges) XXX_DiscardUnknown() {\n\txxx_messageInfo_GraphDriverChanges.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_GraphDriverChanges proto.InternalMessageInfo\n\nfunc (m *GraphDriverChanges) GetPath() string {\n\tif m != nil {\n\t\treturn m.Path\n\t}\n\treturn \"\"\n}\n\nfunc (m *GraphDriverChanges) GetKind() GraphDriverChangeType {\n\tif m != nil {\n\t\treturn m.Kind\n\t}\n\treturn GraphDriverChangeType_GRAPH_DRIVER_CHANGE_TYPE_NONE\n}\n\n// ClusterResponse specifies a response that gets returned when requesting the cluster\n// swagger:response clusterResponse\ntype ClusterResponse struct {\n\t// Error code\n\t//\n\t// in: body\n\tError                string   `protobuf:\"bytes,1,opt,name=error,proto3\" json:\"error,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *ClusterResponse) Reset()         { *m = ClusterResponse{} }\nfunc (m *ClusterResponse) String() string { return proto.CompactTextString(m) }\nfunc (*ClusterResponse) ProtoMessage()    {}\nfunc (*ClusterResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{24}\n}\nfunc (m *ClusterResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_ClusterResponse.Unmarshal(m, b)\n}\nfunc (m *ClusterResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_ClusterResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *ClusterResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_ClusterResponse.Merge(dst, src)\n}\nfunc (m *ClusterResponse) XXX_Size() int {\n\treturn xxx_messageInfo_ClusterResponse.Size(m)\n}\nfunc (m *ClusterResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_ClusterResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_ClusterResponse proto.InternalMessageInfo\n\nfunc (m *ClusterResponse) GetError() string {\n\tif m != nil {\n\t\treturn m.Error\n\t}\n\treturn \"\"\n}\n\n// Active Request\n// swagger:model\ntype ActiveRequest struct {\n\tReqestKV             map[int64]string `protobuf:\"bytes,1,rep,name=ReqestKV,proto3\" json:\"ReqestKV,omitempty\" protobuf_key:\"varint,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}         `json:\"-\"`\n\tXXX_unrecognized     []byte           `json:\"-\"`\n\tXXX_sizecache        int32            `json:\"-\"`\n}\n\nfunc (m *ActiveRequest) Reset()         { *m = ActiveRequest{} }\nfunc (m *ActiveRequest) String() string { return proto.CompactTextString(m) }\nfunc (*ActiveRequest) ProtoMessage()    {}\nfunc (*ActiveRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{25}\n}\nfunc (m *ActiveRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_ActiveRequest.Unmarshal(m, b)\n}\nfunc (m *ActiveRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_ActiveRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *ActiveRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_ActiveRequest.Merge(dst, src)\n}\nfunc (m *ActiveRequest) XXX_Size() int {\n\treturn xxx_messageInfo_ActiveRequest.Size(m)\n}\nfunc (m *ActiveRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_ActiveRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_ActiveRequest proto.InternalMessageInfo\n\nfunc (m *ActiveRequest) GetReqestKV() map[int64]string {\n\tif m != nil {\n\t\treturn m.ReqestKV\n\t}\n\treturn nil\n}\n\n// Active Requests\n// swagger:model\ntype ActiveRequests struct {\n\tRequestCount         int64            `protobuf:\"varint,1,opt,name=RequestCount,proto3\" json:\"RequestCount,omitempty\"`\n\tActiveRequest        []*ActiveRequest `protobuf:\"bytes,2,rep,name=ActiveRequest,proto3\" json:\"ActiveRequest,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}         `json:\"-\"`\n\tXXX_unrecognized     []byte           `json:\"-\"`\n\tXXX_sizecache        int32            `json:\"-\"`\n}\n\nfunc (m *ActiveRequests) Reset()         { *m = ActiveRequests{} }\nfunc (m *ActiveRequests) String() string { return proto.CompactTextString(m) }\nfunc (*ActiveRequests) ProtoMessage()    {}\nfunc (*ActiveRequests) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{26}\n}\nfunc (m *ActiveRequests) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_ActiveRequests.Unmarshal(m, b)\n}\nfunc (m *ActiveRequests) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_ActiveRequests.Marshal(b, m, deterministic)\n}\nfunc (dst *ActiveRequests) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_ActiveRequests.Merge(dst, src)\n}\nfunc (m *ActiveRequests) XXX_Size() int {\n\treturn xxx_messageInfo_ActiveRequests.Size(m)\n}\nfunc (m *ActiveRequests) XXX_DiscardUnknown() {\n\txxx_messageInfo_ActiveRequests.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_ActiveRequests proto.InternalMessageInfo\n\nfunc (m *ActiveRequests) GetRequestCount() int64 {\n\tif m != nil {\n\t\treturn m.RequestCount\n\t}\n\treturn 0\n}\n\nfunc (m *ActiveRequests) GetActiveRequest() []*ActiveRequest {\n\tif m != nil {\n\t\treturn m.ActiveRequest\n\t}\n\treturn nil\n}\n\n// GroupSnapCreateRequest specifies a request to create a snapshot of given group.\n// swagger:model\ntype GroupSnapCreateRequest struct {\n\tId                   string            `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\tLabels               map[string]string `protobuf:\"bytes,2,rep,name=Labels,proto3\" json:\"Labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *GroupSnapCreateRequest) Reset()         { *m = GroupSnapCreateRequest{} }\nfunc (m *GroupSnapCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*GroupSnapCreateRequest) ProtoMessage()    {}\nfunc (*GroupSnapCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{27}\n}\nfunc (m *GroupSnapCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_GroupSnapCreateRequest.Unmarshal(m, b)\n}\nfunc (m *GroupSnapCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_GroupSnapCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *GroupSnapCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_GroupSnapCreateRequest.Merge(dst, src)\n}\nfunc (m *GroupSnapCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_GroupSnapCreateRequest.Size(m)\n}\nfunc (m *GroupSnapCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_GroupSnapCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_GroupSnapCreateRequest proto.InternalMessageInfo\n\nfunc (m *GroupSnapCreateRequest) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *GroupSnapCreateRequest) GetLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.Labels\n\t}\n\treturn nil\n}\n\n// GroupSnapCreateRequest specifies a response that get's returned when creating a group snapshot.\n// swagger:response groupSnapCreateResponse\ntype GroupSnapCreateResponse struct {\n\t// Created snapshots\n\t//\n\t// in: body\n\t// Required: true\n\tSnapshots map[string]*SnapCreateResponse `protobuf:\"bytes,1,rep,name=snapshots,proto3\" json:\"snapshots,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\t// Error message\n\t//\n\t// in: body\n\t// Required: true\n\tError                string   `protobuf:\"bytes,2,opt,name=error,proto3\" json:\"error,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *GroupSnapCreateResponse) Reset()         { *m = GroupSnapCreateResponse{} }\nfunc (m *GroupSnapCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*GroupSnapCreateResponse) ProtoMessage()    {}\nfunc (*GroupSnapCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{28}\n}\nfunc (m *GroupSnapCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_GroupSnapCreateResponse.Unmarshal(m, b)\n}\nfunc (m *GroupSnapCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_GroupSnapCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *GroupSnapCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_GroupSnapCreateResponse.Merge(dst, src)\n}\nfunc (m *GroupSnapCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_GroupSnapCreateResponse.Size(m)\n}\nfunc (m *GroupSnapCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_GroupSnapCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_GroupSnapCreateResponse proto.InternalMessageInfo\n\nfunc (m *GroupSnapCreateResponse) GetSnapshots() map[string]*SnapCreateResponse {\n\tif m != nil {\n\t\treturn m.Snapshots\n\t}\n\treturn nil\n}\n\nfunc (m *GroupSnapCreateResponse) GetError() string {\n\tif m != nil {\n\t\treturn m.Error\n\t}\n\treturn \"\"\n}\n\n// StorageNode describes the state of the node\ntype StorageNode struct {\n\t// Id of the node\n\tId string `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Cpu usage of the node\n\tCpu float64 `protobuf:\"fixed64,2,opt,name=cpu,proto3\" json:\"cpu,omitempty\"`\n\t// Total memory of the node\n\tMemTotal uint64 `protobuf:\"varint,3,opt,name=mem_total,json=memTotal,proto3\" json:\"mem_total,omitempty\"`\n\t// Used memory of the node\n\tMemUsed uint64 `protobuf:\"varint,4,opt,name=mem_used,json=memUsed,proto3\" json:\"mem_used,omitempty\"`\n\t// Free memory of the node\n\tMemFree uint64 `protobuf:\"varint,5,opt,name=mem_free,json=memFree,proto3\" json:\"mem_free,omitempty\"`\n\t// Average load (percentage)\n\tAvgLoad int64 `protobuf:\"varint,6,opt,name=avg_load,json=avgLoad,proto3\" json:\"avg_load,omitempty\"`\n\t// Node status\n\tStatus Status `protobuf:\"varint,7,opt,name=status,proto3,enum=openstorage.api.Status\" json:\"status,omitempty\"`\n\t// List of disks on the node\n\tDisks map[string]*StorageResource `protobuf:\"bytes,9,rep,name=disks,proto3\" json:\"disks,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\t// List of storage pools this node supports\n\tPools []*StoragePool `protobuf:\"bytes,10,rep,name=pools,proto3\" json:\"pools,omitempty\"`\n\t// Management IP\n\tMgmtIp string `protobuf:\"bytes,11,opt,name=mgmt_ip,json=mgmtIp,proto3\" json:\"mgmt_ip,omitempty\"`\n\t// Data IP\n\tDataIp string `protobuf:\"bytes,12,opt,name=data_ip,json=dataIp,proto3\" json:\"data_ip,omitempty\"`\n\t// Hostname of the node\n\tHostname string `protobuf:\"bytes,15,opt,name=hostname,proto3\" json:\"hostname,omitempty\"`\n\t// User defined labels for the node\n\tNodeLabels           map[string]string `protobuf:\"bytes,16,rep,name=node_labels,json=nodeLabels,proto3\" json:\"node_labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *StorageNode) Reset()         { *m = StorageNode{} }\nfunc (m *StorageNode) String() string { return proto.CompactTextString(m) }\nfunc (*StorageNode) ProtoMessage()    {}\nfunc (*StorageNode) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{29}\n}\nfunc (m *StorageNode) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_StorageNode.Unmarshal(m, b)\n}\nfunc (m *StorageNode) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_StorageNode.Marshal(b, m, deterministic)\n}\nfunc (dst *StorageNode) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_StorageNode.Merge(dst, src)\n}\nfunc (m *StorageNode) XXX_Size() int {\n\treturn xxx_messageInfo_StorageNode.Size(m)\n}\nfunc (m *StorageNode) XXX_DiscardUnknown() {\n\txxx_messageInfo_StorageNode.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_StorageNode proto.InternalMessageInfo\n\nfunc (m *StorageNode) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageNode) GetCpu() float64 {\n\tif m != nil {\n\t\treturn m.Cpu\n\t}\n\treturn 0\n}\n\nfunc (m *StorageNode) GetMemTotal() uint64 {\n\tif m != nil {\n\t\treturn m.MemTotal\n\t}\n\treturn 0\n}\n\nfunc (m *StorageNode) GetMemUsed() uint64 {\n\tif m != nil {\n\t\treturn m.MemUsed\n\t}\n\treturn 0\n}\n\nfunc (m *StorageNode) GetMemFree() uint64 {\n\tif m != nil {\n\t\treturn m.MemFree\n\t}\n\treturn 0\n}\n\nfunc (m *StorageNode) GetAvgLoad() int64 {\n\tif m != nil {\n\t\treturn m.AvgLoad\n\t}\n\treturn 0\n}\n\nfunc (m *StorageNode) GetStatus() Status {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn Status_STATUS_NONE\n}\n\nfunc (m *StorageNode) GetDisks() map[string]*StorageResource {\n\tif m != nil {\n\t\treturn m.Disks\n\t}\n\treturn nil\n}\n\nfunc (m *StorageNode) GetPools() []*StoragePool {\n\tif m != nil {\n\t\treturn m.Pools\n\t}\n\treturn nil\n}\n\nfunc (m *StorageNode) GetMgmtIp() string {\n\tif m != nil {\n\t\treturn m.MgmtIp\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageNode) GetDataIp() string {\n\tif m != nil {\n\t\treturn m.DataIp\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageNode) GetHostname() string {\n\tif m != nil {\n\t\treturn m.Hostname\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageNode) GetNodeLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.NodeLabels\n\t}\n\treturn nil\n}\n\n// StorageCluster represents the state of the cluster\ntype StorageCluster struct {\n\t// Status of the cluster\n\tStatus Status `protobuf:\"varint,1,opt,name=status,proto3,enum=openstorage.api.Status\" json:\"status,omitempty\"`\n\t// Id of the cluster\n\tId string `protobuf:\"bytes,2,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// NodeId is the id of the node servicing these requests\n\tNodeId string `protobuf:\"bytes,3,opt,name=node_id,json=nodeId,proto3\" json:\"node_id,omitempty\"`\n\t// Nodes are a list of all the nodes on the cluster\n\tNodes                []*StorageNode `protobuf:\"bytes,4,rep,name=nodes,proto3\" json:\"nodes,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}       `json:\"-\"`\n\tXXX_unrecognized     []byte         `json:\"-\"`\n\tXXX_sizecache        int32          `json:\"-\"`\n}\n\nfunc (m *StorageCluster) Reset()         { *m = StorageCluster{} }\nfunc (m *StorageCluster) String() string { return proto.CompactTextString(m) }\nfunc (*StorageCluster) ProtoMessage()    {}\nfunc (*StorageCluster) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{30}\n}\nfunc (m *StorageCluster) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_StorageCluster.Unmarshal(m, b)\n}\nfunc (m *StorageCluster) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_StorageCluster.Marshal(b, m, deterministic)\n}\nfunc (dst *StorageCluster) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_StorageCluster.Merge(dst, src)\n}\nfunc (m *StorageCluster) XXX_Size() int {\n\treturn xxx_messageInfo_StorageCluster.Size(m)\n}\nfunc (m *StorageCluster) XXX_DiscardUnknown() {\n\txxx_messageInfo_StorageCluster.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_StorageCluster proto.InternalMessageInfo\n\nfunc (m *StorageCluster) GetStatus() Status {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn Status_STATUS_NONE\n}\n\nfunc (m *StorageCluster) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageCluster) GetNodeId() string {\n\tif m != nil {\n\t\treturn m.NodeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *StorageCluster) GetNodes() []*StorageNode {\n\tif m != nil {\n\t\treturn m.Nodes\n\t}\n\treturn nil\n}\n\ntype SdkSchedulePolicyCreateRequest struct {\n\t// Schedule Policy\n\tSchedulePolicy       *SdkSchedulePolicy `protobuf:\"bytes,1,opt,name=SchedulePolicy,proto3\" json:\"SchedulePolicy,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}           `json:\"-\"`\n\tXXX_unrecognized     []byte             `json:\"-\"`\n\tXXX_sizecache        int32              `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyCreateRequest) Reset()         { *m = SdkSchedulePolicyCreateRequest{} }\nfunc (m *SdkSchedulePolicyCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyCreateRequest) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{31}\n}\nfunc (m *SdkSchedulePolicyCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyCreateRequest.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateRequest.Size(m)\n}\nfunc (m *SdkSchedulePolicyCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyCreateRequest proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyCreateRequest) GetSchedulePolicy() *SdkSchedulePolicy {\n\tif m != nil {\n\t\treturn m.SchedulePolicy\n\t}\n\treturn nil\n}\n\ntype SdkSchedulePolicyCreateResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyCreateResponse) Reset()         { *m = SdkSchedulePolicyCreateResponse{} }\nfunc (m *SdkSchedulePolicyCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyCreateResponse) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{32}\n}\nfunc (m *SdkSchedulePolicyCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyCreateResponse.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyCreateResponse.Size(m)\n}\nfunc (m *SdkSchedulePolicyCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyCreateResponse proto.InternalMessageInfo\n\ntype SdkSchedulePolicyUpdateRequest struct {\n\t// Schedule Policy\n\tSchedulePolicy       *SdkSchedulePolicy `protobuf:\"bytes,1,opt,name=SchedulePolicy,proto3\" json:\"SchedulePolicy,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}           `json:\"-\"`\n\tXXX_unrecognized     []byte             `json:\"-\"`\n\tXXX_sizecache        int32              `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyUpdateRequest) Reset()         { *m = SdkSchedulePolicyUpdateRequest{} }\nfunc (m *SdkSchedulePolicyUpdateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyUpdateRequest) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyUpdateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{33}\n}\nfunc (m *SdkSchedulePolicyUpdateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyUpdateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyUpdateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyUpdateRequest.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyUpdateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateRequest.Size(m)\n}\nfunc (m *SdkSchedulePolicyUpdateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyUpdateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyUpdateRequest proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyUpdateRequest) GetSchedulePolicy() *SdkSchedulePolicy {\n\tif m != nil {\n\t\treturn m.SchedulePolicy\n\t}\n\treturn nil\n}\n\ntype SdkSchedulePolicyUpdateResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyUpdateResponse) Reset()         { *m = SdkSchedulePolicyUpdateResponse{} }\nfunc (m *SdkSchedulePolicyUpdateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyUpdateResponse) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyUpdateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{34}\n}\nfunc (m *SdkSchedulePolicyUpdateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyUpdateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyUpdateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyUpdateResponse.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyUpdateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyUpdateResponse.Size(m)\n}\nfunc (m *SdkSchedulePolicyUpdateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyUpdateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyUpdateResponse proto.InternalMessageInfo\n\ntype SdkSchedulePolicyEnumerateRequest struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyEnumerateRequest) Reset()         { *m = SdkSchedulePolicyEnumerateRequest{} }\nfunc (m *SdkSchedulePolicyEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{35}\n}\nfunc (m *SdkSchedulePolicyEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateRequest.Size(m)\n}\nfunc (m *SdkSchedulePolicyEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyEnumerateRequest proto.InternalMessageInfo\n\ntype SdkSchedulePolicyEnumerateResponse struct {\n\t// List of Schedule Policy\n\tPolicies             []*SdkSchedulePolicy `protobuf:\"bytes,1,rep,name=policies,proto3\" json:\"policies,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}             `json:\"-\"`\n\tXXX_unrecognized     []byte               `json:\"-\"`\n\tXXX_sizecache        int32                `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyEnumerateResponse) Reset()         { *m = SdkSchedulePolicyEnumerateResponse{} }\nfunc (m *SdkSchedulePolicyEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{36}\n}\nfunc (m *SdkSchedulePolicyEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyEnumerateResponse.Size(m)\n}\nfunc (m *SdkSchedulePolicyEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyEnumerateResponse) GetPolicies() []*SdkSchedulePolicy {\n\tif m != nil {\n\t\treturn m.Policies\n\t}\n\treturn nil\n}\n\ntype SdkSchedulePolicyInspectRequest struct {\n\t// Name of the schedule Policy\n\tName                 string   `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyInspectRequest) Reset()         { *m = SdkSchedulePolicyInspectRequest{} }\nfunc (m *SdkSchedulePolicyInspectRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyInspectRequest) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyInspectRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{37}\n}\nfunc (m *SdkSchedulePolicyInspectRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectRequest.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyInspectRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyInspectRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyInspectRequest.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyInspectRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectRequest.Size(m)\n}\nfunc (m *SdkSchedulePolicyInspectRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyInspectRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyInspectRequest proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyInspectRequest) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\ntype SdkSchedulePolicyInspectResponse struct {\n\t// List of Schedule Policy\n\tPolicy               *SdkSchedulePolicy `protobuf:\"bytes,1,opt,name=policy,proto3\" json:\"policy,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}           `json:\"-\"`\n\tXXX_unrecognized     []byte             `json:\"-\"`\n\tXXX_sizecache        int32              `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyInspectResponse) Reset()         { *m = SdkSchedulePolicyInspectResponse{} }\nfunc (m *SdkSchedulePolicyInspectResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyInspectResponse) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyInspectResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{38}\n}\nfunc (m *SdkSchedulePolicyInspectResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectResponse.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyInspectResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyInspectResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyInspectResponse.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyInspectResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyInspectResponse.Size(m)\n}\nfunc (m *SdkSchedulePolicyInspectResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyInspectResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyInspectResponse proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyInspectResponse) GetPolicy() *SdkSchedulePolicy {\n\tif m != nil {\n\t\treturn m.Policy\n\t}\n\treturn nil\n}\n\ntype SdkSchedulePolicyDeleteRequest struct {\n\t// Name of the schedule policy\n\tName                 string   `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyDeleteRequest) Reset()         { *m = SdkSchedulePolicyDeleteRequest{} }\nfunc (m *SdkSchedulePolicyDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyDeleteRequest) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{39}\n}\nfunc (m *SdkSchedulePolicyDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteRequest.Size(m)\n}\nfunc (m *SdkSchedulePolicyDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicyDeleteRequest) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\ntype SdkSchedulePolicyDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicyDeleteResponse) Reset()         { *m = SdkSchedulePolicyDeleteResponse{} }\nfunc (m *SdkSchedulePolicyDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicyDeleteResponse) ProtoMessage()    {}\nfunc (*SdkSchedulePolicyDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{40}\n}\nfunc (m *SdkSchedulePolicyDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicyDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicyDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicyDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicyDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicyDeleteResponse.Size(m)\n}\nfunc (m *SdkSchedulePolicyDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicyDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicyDeleteResponse proto.InternalMessageInfo\n\ntype SdkSchedulePolicy struct {\n\t// Name of the schedule policy\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Schedule string in yaml\n\tSchedule             string   `protobuf:\"bytes,2,opt,name=schedule,proto3\" json:\"schedule,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkSchedulePolicy) Reset()         { *m = SdkSchedulePolicy{} }\nfunc (m *SdkSchedulePolicy) String() string { return proto.CompactTextString(m) }\nfunc (*SdkSchedulePolicy) ProtoMessage()    {}\nfunc (*SdkSchedulePolicy) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{41}\n}\nfunc (m *SdkSchedulePolicy) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkSchedulePolicy.Unmarshal(m, b)\n}\nfunc (m *SdkSchedulePolicy) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkSchedulePolicy.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkSchedulePolicy) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkSchedulePolicy.Merge(dst, src)\n}\nfunc (m *SdkSchedulePolicy) XXX_Size() int {\n\treturn xxx_messageInfo_SdkSchedulePolicy.Size(m)\n}\nfunc (m *SdkSchedulePolicy) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkSchedulePolicy.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkSchedulePolicy proto.InternalMessageInfo\n\nfunc (m *SdkSchedulePolicy) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkSchedulePolicy) GetSchedule() string {\n\tif m != nil {\n\t\treturn m.Schedule\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialCreateAzureRequest struct {\n\t// Azure Credential\n\tCredential           *AzureCredential `protobuf:\"bytes,1,opt,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}         `json:\"-\"`\n\tXXX_unrecognized     []byte           `json:\"-\"`\n\tXXX_sizecache        int32            `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateAzureRequest) Reset()         { *m = SdkCredentialCreateAzureRequest{} }\nfunc (m *SdkCredentialCreateAzureRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateAzureRequest) ProtoMessage()    {}\nfunc (*SdkCredentialCreateAzureRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{42}\n}\nfunc (m *SdkCredentialCreateAzureRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateAzureRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateAzureRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateAzureRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateAzureRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureRequest.Size(m)\n}\nfunc (m *SdkCredentialCreateAzureRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateAzureRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateAzureRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateAzureRequest) GetCredential() *AzureCredential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialCreateAzureResponse struct {\n\t// Id of the credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateAzureResponse) Reset()         { *m = SdkCredentialCreateAzureResponse{} }\nfunc (m *SdkCredentialCreateAzureResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateAzureResponse) ProtoMessage()    {}\nfunc (*SdkCredentialCreateAzureResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{43}\n}\nfunc (m *SdkCredentialCreateAzureResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateAzureResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateAzureResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateAzureResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateAzureResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateAzureResponse.Size(m)\n}\nfunc (m *SdkCredentialCreateAzureResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateAzureResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateAzureResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateAzureResponse) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialCreateGoogleRequest struct {\n\t// Google Credential\n\tCredential           *GoogleCredential `protobuf:\"bytes,1,opt,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateGoogleRequest) Reset()         { *m = SdkCredentialCreateGoogleRequest{} }\nfunc (m *SdkCredentialCreateGoogleRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateGoogleRequest) ProtoMessage()    {}\nfunc (*SdkCredentialCreateGoogleRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{44}\n}\nfunc (m *SdkCredentialCreateGoogleRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateGoogleRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateGoogleRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateGoogleRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateGoogleRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleRequest.Size(m)\n}\nfunc (m *SdkCredentialCreateGoogleRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateGoogleRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateGoogleRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateGoogleRequest) GetCredential() *GoogleCredential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialCreateGoogleResponse struct {\n\t// Id of the credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateGoogleResponse) Reset()         { *m = SdkCredentialCreateGoogleResponse{} }\nfunc (m *SdkCredentialCreateGoogleResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateGoogleResponse) ProtoMessage()    {}\nfunc (*SdkCredentialCreateGoogleResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{45}\n}\nfunc (m *SdkCredentialCreateGoogleResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateGoogleResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateGoogleResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateGoogleResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateGoogleResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateGoogleResponse.Size(m)\n}\nfunc (m *SdkCredentialCreateGoogleResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateGoogleResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateGoogleResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateGoogleResponse) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialCreateAWSRequest struct {\n\t// AWS S3 Credential\n\tCredential           *S3Credential `protobuf:\"bytes,1,opt,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}      `json:\"-\"`\n\tXXX_unrecognized     []byte        `json:\"-\"`\n\tXXX_sizecache        int32         `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateAWSRequest) Reset()         { *m = SdkCredentialCreateAWSRequest{} }\nfunc (m *SdkCredentialCreateAWSRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateAWSRequest) ProtoMessage()    {}\nfunc (*SdkCredentialCreateAWSRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{46}\n}\nfunc (m *SdkCredentialCreateAWSRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateAWSRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateAWSRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateAWSRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateAWSRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSRequest.Size(m)\n}\nfunc (m *SdkCredentialCreateAWSRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateAWSRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateAWSRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateAWSRequest) GetCredential() *S3Credential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialCreateAWSResponse struct {\n\t// Id of the credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialCreateAWSResponse) Reset()         { *m = SdkCredentialCreateAWSResponse{} }\nfunc (m *SdkCredentialCreateAWSResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialCreateAWSResponse) ProtoMessage()    {}\nfunc (*SdkCredentialCreateAWSResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{47}\n}\nfunc (m *SdkCredentialCreateAWSResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialCreateAWSResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialCreateAWSResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialCreateAWSResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialCreateAWSResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialCreateAWSResponse.Size(m)\n}\nfunc (m *SdkCredentialCreateAWSResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialCreateAWSResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialCreateAWSResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialCreateAWSResponse) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype S3Credential struct {\n\t// Id of the credentials\n\tCredentialId string `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\t// Access key\n\tAccessKey string `protobuf:\"bytes,2,opt,name=access_key,json=accessKey,proto3\" json:\"access_key,omitempty\"`\n\t// Secret key\n\tSecretKey string `protobuf:\"bytes,3,opt,name=secret_key,json=secretKey,proto3\" json:\"secret_key,omitempty\"`\n\t// Endpoint\n\tEndpoint string `protobuf:\"bytes,4,opt,name=endpoint,proto3\" json:\"endpoint,omitempty\"`\n\t// Region\n\tRegion               string   `protobuf:\"bytes,5,opt,name=region,proto3\" json:\"region,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *S3Credential) Reset()         { *m = S3Credential{} }\nfunc (m *S3Credential) String() string { return proto.CompactTextString(m) }\nfunc (*S3Credential) ProtoMessage()    {}\nfunc (*S3Credential) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{48}\n}\nfunc (m *S3Credential) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_S3Credential.Unmarshal(m, b)\n}\nfunc (m *S3Credential) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_S3Credential.Marshal(b, m, deterministic)\n}\nfunc (dst *S3Credential) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_S3Credential.Merge(dst, src)\n}\nfunc (m *S3Credential) XXX_Size() int {\n\treturn xxx_messageInfo_S3Credential.Size(m)\n}\nfunc (m *S3Credential) XXX_DiscardUnknown() {\n\txxx_messageInfo_S3Credential.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_S3Credential proto.InternalMessageInfo\n\nfunc (m *S3Credential) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\nfunc (m *S3Credential) GetAccessKey() string {\n\tif m != nil {\n\t\treturn m.AccessKey\n\t}\n\treturn \"\"\n}\n\nfunc (m *S3Credential) GetSecretKey() string {\n\tif m != nil {\n\t\treturn m.SecretKey\n\t}\n\treturn \"\"\n}\n\nfunc (m *S3Credential) GetEndpoint() string {\n\tif m != nil {\n\t\treturn m.Endpoint\n\t}\n\treturn \"\"\n}\n\nfunc (m *S3Credential) GetRegion() string {\n\tif m != nil {\n\t\treturn m.Region\n\t}\n\treturn \"\"\n}\n\ntype AzureCredential struct {\n\t// Cred Type\n\tCredentialId string `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\t// Account name\n\tAccountName string `protobuf:\"bytes,2,opt,name=account_name,json=accountName,proto3\" json:\"account_name,omitempty\"`\n\t// Account key\n\tAccountKey           string   `protobuf:\"bytes,3,opt,name=account_key,json=accountKey,proto3\" json:\"account_key,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *AzureCredential) Reset()         { *m = AzureCredential{} }\nfunc (m *AzureCredential) String() string { return proto.CompactTextString(m) }\nfunc (*AzureCredential) ProtoMessage()    {}\nfunc (*AzureCredential) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{49}\n}\nfunc (m *AzureCredential) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_AzureCredential.Unmarshal(m, b)\n}\nfunc (m *AzureCredential) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_AzureCredential.Marshal(b, m, deterministic)\n}\nfunc (dst *AzureCredential) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_AzureCredential.Merge(dst, src)\n}\nfunc (m *AzureCredential) XXX_Size() int {\n\treturn xxx_messageInfo_AzureCredential.Size(m)\n}\nfunc (m *AzureCredential) XXX_DiscardUnknown() {\n\txxx_messageInfo_AzureCredential.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_AzureCredential proto.InternalMessageInfo\n\nfunc (m *AzureCredential) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\nfunc (m *AzureCredential) GetAccountName() string {\n\tif m != nil {\n\t\treturn m.AccountName\n\t}\n\treturn \"\"\n}\n\nfunc (m *AzureCredential) GetAccountKey() string {\n\tif m != nil {\n\t\treturn m.AccountKey\n\t}\n\treturn \"\"\n}\n\ntype GoogleCredential struct {\n\tCredentialId string `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\t// Project ID\n\tProjectId string `protobuf:\"bytes,2,opt,name=project_id,json=projectId,proto3\" json:\"project_id,omitempty\"`\n\t// JSON Key\n\tJsonKey              string   `protobuf:\"bytes,3,opt,name=json_key,json=jsonKey,proto3\" json:\"json_key,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *GoogleCredential) Reset()         { *m = GoogleCredential{} }\nfunc (m *GoogleCredential) String() string { return proto.CompactTextString(m) }\nfunc (*GoogleCredential) ProtoMessage()    {}\nfunc (*GoogleCredential) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{50}\n}\nfunc (m *GoogleCredential) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_GoogleCredential.Unmarshal(m, b)\n}\nfunc (m *GoogleCredential) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_GoogleCredential.Marshal(b, m, deterministic)\n}\nfunc (dst *GoogleCredential) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_GoogleCredential.Merge(dst, src)\n}\nfunc (m *GoogleCredential) XXX_Size() int {\n\treturn xxx_messageInfo_GoogleCredential.Size(m)\n}\nfunc (m *GoogleCredential) XXX_DiscardUnknown() {\n\txxx_messageInfo_GoogleCredential.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_GoogleCredential proto.InternalMessageInfo\n\nfunc (m *GoogleCredential) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\nfunc (m *GoogleCredential) GetProjectId() string {\n\tif m != nil {\n\t\treturn m.ProjectId\n\t}\n\treturn \"\"\n}\n\nfunc (m *GoogleCredential) GetJsonKey() string {\n\tif m != nil {\n\t\treturn m.JsonKey\n\t}\n\treturn \"\"\n}\n\n// should enumerate accept anything?\ntype SdkCredentialEnumerateAWSRequest struct {\n\t// Id of the credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateAWSRequest) Reset()         { *m = SdkCredentialEnumerateAWSRequest{} }\nfunc (m *SdkCredentialEnumerateAWSRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateAWSRequest) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateAWSRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{51}\n}\nfunc (m *SdkCredentialEnumerateAWSRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateAWSRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateAWSRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateAWSRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateAWSRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSRequest.Size(m)\n}\nfunc (m *SdkCredentialEnumerateAWSRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateAWSRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateAWSRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateAWSRequest) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialEnumerateAWSResponse struct {\n\t// Array of Credentials for AWS\n\tCredential           []*S3Credential `protobuf:\"bytes,1,rep,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}        `json:\"-\"`\n\tXXX_unrecognized     []byte          `json:\"-\"`\n\tXXX_sizecache        int32           `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateAWSResponse) Reset()         { *m = SdkCredentialEnumerateAWSResponse{} }\nfunc (m *SdkCredentialEnumerateAWSResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateAWSResponse) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateAWSResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{52}\n}\nfunc (m *SdkCredentialEnumerateAWSResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateAWSResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateAWSResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateAWSResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateAWSResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAWSResponse.Size(m)\n}\nfunc (m *SdkCredentialEnumerateAWSResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateAWSResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateAWSResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateAWSResponse) GetCredential() []*S3Credential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialEnumerateAzureRequest struct {\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateAzureRequest) Reset()         { *m = SdkCredentialEnumerateAzureRequest{} }\nfunc (m *SdkCredentialEnumerateAzureRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateAzureRequest) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateAzureRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{53}\n}\nfunc (m *SdkCredentialEnumerateAzureRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateAzureRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateAzureRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateAzureRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateAzureRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureRequest.Size(m)\n}\nfunc (m *SdkCredentialEnumerateAzureRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateAzureRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateAzureRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateAzureRequest) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialEnumerateAzureResponse struct {\n\t// List of Credentials for Azure\n\tCredential           []*AzureCredential `protobuf:\"bytes,1,rep,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}           `json:\"-\"`\n\tXXX_unrecognized     []byte             `json:\"-\"`\n\tXXX_sizecache        int32              `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateAzureResponse) Reset()         { *m = SdkCredentialEnumerateAzureResponse{} }\nfunc (m *SdkCredentialEnumerateAzureResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateAzureResponse) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateAzureResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{54}\n}\nfunc (m *SdkCredentialEnumerateAzureResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateAzureResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateAzureResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateAzureResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateAzureResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateAzureResponse.Size(m)\n}\nfunc (m *SdkCredentialEnumerateAzureResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateAzureResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateAzureResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateAzureResponse) GetCredential() []*AzureCredential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialEnumerateGoogleRequest struct {\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateGoogleRequest) Reset()         { *m = SdkCredentialEnumerateGoogleRequest{} }\nfunc (m *SdkCredentialEnumerateGoogleRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateGoogleRequest) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateGoogleRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{55}\n}\nfunc (m *SdkCredentialEnumerateGoogleRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateGoogleRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateGoogleRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateGoogleRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateGoogleRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleRequest.Size(m)\n}\nfunc (m *SdkCredentialEnumerateGoogleRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateGoogleRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateGoogleRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateGoogleRequest) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialEnumerateGoogleResponse struct {\n\t// List of Credentials for Google\n\tCredential           []*GoogleCredential `protobuf:\"bytes,1,rep,name=credential,proto3\" json:\"credential,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}            `json:\"-\"`\n\tXXX_unrecognized     []byte              `json:\"-\"`\n\tXXX_sizecache        int32               `json:\"-\"`\n}\n\nfunc (m *SdkCredentialEnumerateGoogleResponse) Reset()         { *m = SdkCredentialEnumerateGoogleResponse{} }\nfunc (m *SdkCredentialEnumerateGoogleResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialEnumerateGoogleResponse) ProtoMessage()    {}\nfunc (*SdkCredentialEnumerateGoogleResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{56}\n}\nfunc (m *SdkCredentialEnumerateGoogleResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialEnumerateGoogleResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialEnumerateGoogleResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialEnumerateGoogleResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialEnumerateGoogleResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialEnumerateGoogleResponse.Size(m)\n}\nfunc (m *SdkCredentialEnumerateGoogleResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialEnumerateGoogleResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialEnumerateGoogleResponse proto.InternalMessageInfo\n\nfunc (m *SdkCredentialEnumerateGoogleResponse) GetCredential() []*GoogleCredential {\n\tif m != nil {\n\t\treturn m.Credential\n\t}\n\treturn nil\n}\n\ntype SdkCredentialDeleteRequest struct {\n\t// ID for credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialDeleteRequest) Reset()         { *m = SdkCredentialDeleteRequest{} }\nfunc (m *SdkCredentialDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialDeleteRequest) ProtoMessage()    {}\nfunc (*SdkCredentialDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{57}\n}\nfunc (m *SdkCredentialDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialDeleteRequest.Size(m)\n}\nfunc (m *SdkCredentialDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialDeleteRequest) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialDeleteResponse) Reset()         { *m = SdkCredentialDeleteResponse{} }\nfunc (m *SdkCredentialDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialDeleteResponse) ProtoMessage()    {}\nfunc (*SdkCredentialDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{58}\n}\nfunc (m *SdkCredentialDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialDeleteResponse.Size(m)\n}\nfunc (m *SdkCredentialDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialDeleteResponse proto.InternalMessageInfo\n\ntype SdkCredentialValidateRequest struct {\n\t// Id of the credentials\n\tCredentialId         string   `protobuf:\"bytes,1,opt,name=credential_id,json=credentialId,proto3\" json:\"credential_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialValidateRequest) Reset()         { *m = SdkCredentialValidateRequest{} }\nfunc (m *SdkCredentialValidateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialValidateRequest) ProtoMessage()    {}\nfunc (*SdkCredentialValidateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{59}\n}\nfunc (m *SdkCredentialValidateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialValidateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialValidateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialValidateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialValidateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialValidateRequest.Merge(dst, src)\n}\nfunc (m *SdkCredentialValidateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialValidateRequest.Size(m)\n}\nfunc (m *SdkCredentialValidateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialValidateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialValidateRequest proto.InternalMessageInfo\n\nfunc (m *SdkCredentialValidateRequest) GetCredentialId() string {\n\tif m != nil {\n\t\treturn m.CredentialId\n\t}\n\treturn \"\"\n}\n\ntype SdkCredentialValidateResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCredentialValidateResponse) Reset()         { *m = SdkCredentialValidateResponse{} }\nfunc (m *SdkCredentialValidateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCredentialValidateResponse) ProtoMessage()    {}\nfunc (*SdkCredentialValidateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{60}\n}\nfunc (m *SdkCredentialValidateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCredentialValidateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCredentialValidateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCredentialValidateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCredentialValidateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCredentialValidateResponse.Merge(dst, src)\n}\nfunc (m *SdkCredentialValidateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCredentialValidateResponse.Size(m)\n}\nfunc (m *SdkCredentialValidateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCredentialValidateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCredentialValidateResponse proto.InternalMessageInfo\n\ntype SdkVolumeMountRequest struct {\n\t// Id of the volume\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Mount path for mounting the volume.\n\tMountPath string `protobuf:\"bytes,2,opt,name=mount_path,json=mountPath,proto3\" json:\"mount_path,omitempty\"`\n\t// Additional options\n\tOptions              map[string]string `protobuf:\"bytes,3,rep,name=options,proto3\" json:\"options,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkVolumeMountRequest) Reset()         { *m = SdkVolumeMountRequest{} }\nfunc (m *SdkVolumeMountRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeMountRequest) ProtoMessage()    {}\nfunc (*SdkVolumeMountRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{61}\n}\nfunc (m *SdkVolumeMountRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeMountRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeMountRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeMountRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeMountRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeMountRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeMountRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeMountRequest.Size(m)\n}\nfunc (m *SdkVolumeMountRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeMountRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeMountRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeMountRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeMountRequest) GetMountPath() string {\n\tif m != nil {\n\t\treturn m.MountPath\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeMountRequest) GetOptions() map[string]string {\n\tif m != nil {\n\t\treturn m.Options\n\t}\n\treturn nil\n}\n\ntype SdkVolumeMountResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeMountResponse) Reset()         { *m = SdkVolumeMountResponse{} }\nfunc (m *SdkVolumeMountResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeMountResponse) ProtoMessage()    {}\nfunc (*SdkVolumeMountResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{62}\n}\nfunc (m *SdkVolumeMountResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeMountResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeMountResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeMountResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeMountResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeMountResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeMountResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeMountResponse.Size(m)\n}\nfunc (m *SdkVolumeMountResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeMountResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeMountResponse proto.InternalMessageInfo\n\ntype SdkVolumeUnmountRequest struct {\n\t// Id of volume\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// MountPath for device\n\tMountPath string `protobuf:\"bytes,2,opt,name=mount_path,json=mountPath,proto3\" json:\"mount_path,omitempty\"`\n\t// Options to unmount device\n\tOptions              map[string]string `protobuf:\"bytes,3,rep,name=options,proto3\" json:\"options,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkVolumeUnmountRequest) Reset()         { *m = SdkVolumeUnmountRequest{} }\nfunc (m *SdkVolumeUnmountRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeUnmountRequest) ProtoMessage()    {}\nfunc (*SdkVolumeUnmountRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{63}\n}\nfunc (m *SdkVolumeUnmountRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeUnmountRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeUnmountRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeUnmountRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeUnmountRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeUnmountRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeUnmountRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeUnmountRequest.Size(m)\n}\nfunc (m *SdkVolumeUnmountRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeUnmountRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeUnmountRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeUnmountRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeUnmountRequest) GetMountPath() string {\n\tif m != nil {\n\t\treturn m.MountPath\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeUnmountRequest) GetOptions() map[string]string {\n\tif m != nil {\n\t\treturn m.Options\n\t}\n\treturn nil\n}\n\ntype SdkVolumeUnmountResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeUnmountResponse) Reset()         { *m = SdkVolumeUnmountResponse{} }\nfunc (m *SdkVolumeUnmountResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeUnmountResponse) ProtoMessage()    {}\nfunc (*SdkVolumeUnmountResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{64}\n}\nfunc (m *SdkVolumeUnmountResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeUnmountResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeUnmountResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeUnmountResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeUnmountResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeUnmountResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeUnmountResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeUnmountResponse.Size(m)\n}\nfunc (m *SdkVolumeUnmountResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeUnmountResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeUnmountResponse proto.InternalMessageInfo\n\ntype SdkVolumeAttachRequest struct {\n\t// Id of volume\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Options for attaching volume, right now only passphrase options is supported\n\tOptions              map[string]string `protobuf:\"bytes,2,rep,name=options,proto3\" json:\"options,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkVolumeAttachRequest) Reset()         { *m = SdkVolumeAttachRequest{} }\nfunc (m *SdkVolumeAttachRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeAttachRequest) ProtoMessage()    {}\nfunc (*SdkVolumeAttachRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{65}\n}\nfunc (m *SdkVolumeAttachRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeAttachRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeAttachRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeAttachRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeAttachRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeAttachRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeAttachRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeAttachRequest.Size(m)\n}\nfunc (m *SdkVolumeAttachRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeAttachRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeAttachRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeAttachRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeAttachRequest) GetOptions() map[string]string {\n\tif m != nil {\n\t\treturn m.Options\n\t}\n\treturn nil\n}\n\ntype SdkVolumeAttachResponse struct {\n\t// Device path where device is exported\n\tDevicePath           string   `protobuf:\"bytes,1,opt,name=device_path,json=devicePath,proto3\" json:\"device_path,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeAttachResponse) Reset()         { *m = SdkVolumeAttachResponse{} }\nfunc (m *SdkVolumeAttachResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeAttachResponse) ProtoMessage()    {}\nfunc (*SdkVolumeAttachResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{66}\n}\nfunc (m *SdkVolumeAttachResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeAttachResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeAttachResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeAttachResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeAttachResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeAttachResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeAttachResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeAttachResponse.Size(m)\n}\nfunc (m *SdkVolumeAttachResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeAttachResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeAttachResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeAttachResponse) GetDevicePath() string {\n\tif m != nil {\n\t\treturn m.DevicePath\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeDetachRequest struct {\n\t// Id of the volume\n\tVolumeId             string   `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeDetachRequest) Reset()         { *m = SdkVolumeDetachRequest{} }\nfunc (m *SdkVolumeDetachRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeDetachRequest) ProtoMessage()    {}\nfunc (*SdkVolumeDetachRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{67}\n}\nfunc (m *SdkVolumeDetachRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeDetachRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeDetachRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeDetachRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeDetachRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeDetachRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeDetachRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeDetachRequest.Size(m)\n}\nfunc (m *SdkVolumeDetachRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeDetachRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeDetachRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeDetachRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeDetachResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeDetachResponse) Reset()         { *m = SdkVolumeDetachResponse{} }\nfunc (m *SdkVolumeDetachResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeDetachResponse) ProtoMessage()    {}\nfunc (*SdkVolumeDetachResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{68}\n}\nfunc (m *SdkVolumeDetachResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeDetachResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeDetachResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeDetachResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeDetachResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeDetachResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeDetachResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeDetachResponse.Size(m)\n}\nfunc (m *SdkVolumeDetachResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeDetachResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeDetachResponse proto.InternalMessageInfo\n\ntype SdkVolumeCreateRequest struct {\n\t// Unique name of the volume. This will be used for idempotency.\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Volume specification\n\tSpec                 *VolumeSpec `protobuf:\"bytes,2,opt,name=spec,proto3\" json:\"spec,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}    `json:\"-\"`\n\tXXX_unrecognized     []byte      `json:\"-\"`\n\tXXX_sizecache        int32       `json:\"-\"`\n}\n\nfunc (m *SdkVolumeCreateRequest) Reset()         { *m = SdkVolumeCreateRequest{} }\nfunc (m *SdkVolumeCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeCreateRequest) ProtoMessage()    {}\nfunc (*SdkVolumeCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{69}\n}\nfunc (m *SdkVolumeCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeCreateRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeCreateRequest.Size(m)\n}\nfunc (m *SdkVolumeCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeCreateRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeCreateRequest) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeCreateRequest) GetSpec() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Spec\n\t}\n\treturn nil\n}\n\ntype SdkVolumeCreateResponse struct {\n\t// Id of new volume\n\tVolumeId             string   `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeCreateResponse) Reset()         { *m = SdkVolumeCreateResponse{} }\nfunc (m *SdkVolumeCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeCreateResponse) ProtoMessage()    {}\nfunc (*SdkVolumeCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{70}\n}\nfunc (m *SdkVolumeCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeCreateResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeCreateResponse.Size(m)\n}\nfunc (m *SdkVolumeCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeCreateResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeCreateResponse) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeCloneRequest struct {\n\t// Unique name of the volume. This will be used for idempotency.\n\tName string `protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"`\n\t// Parent volume id, if specified will create a new volume as a clone of the parent.\n\tParentId string `protobuf:\"bytes,2,opt,name=parent_id,json=parentId,proto3\" json:\"parent_id,omitempty\"`\n\t// Volume specification\n\tSpec                 *VolumeSpec `protobuf:\"bytes,3,opt,name=spec,proto3\" json:\"spec,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}    `json:\"-\"`\n\tXXX_unrecognized     []byte      `json:\"-\"`\n\tXXX_sizecache        int32       `json:\"-\"`\n}\n\nfunc (m *SdkVolumeCloneRequest) Reset()         { *m = SdkVolumeCloneRequest{} }\nfunc (m *SdkVolumeCloneRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeCloneRequest) ProtoMessage()    {}\nfunc (*SdkVolumeCloneRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{71}\n}\nfunc (m *SdkVolumeCloneRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeCloneRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeCloneRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeCloneRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeCloneRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeCloneRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeCloneRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeCloneRequest.Size(m)\n}\nfunc (m *SdkVolumeCloneRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeCloneRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeCloneRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeCloneRequest) GetName() string {\n\tif m != nil {\n\t\treturn m.Name\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeCloneRequest) GetParentId() string {\n\tif m != nil {\n\t\treturn m.ParentId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeCloneRequest) GetSpec() *VolumeSpec {\n\tif m != nil {\n\t\treturn m.Spec\n\t}\n\treturn nil\n}\n\ntype SdkVolumeCloneResponse struct {\n\t// Id of new volume\n\tVolumeId             string   `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeCloneResponse) Reset()         { *m = SdkVolumeCloneResponse{} }\nfunc (m *SdkVolumeCloneResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeCloneResponse) ProtoMessage()    {}\nfunc (*SdkVolumeCloneResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{72}\n}\nfunc (m *SdkVolumeCloneResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeCloneResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeCloneResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeCloneResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeCloneResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeCloneResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeCloneResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeCloneResponse.Size(m)\n}\nfunc (m *SdkVolumeCloneResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeCloneResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeCloneResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeCloneResponse) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeDeleteRequest struct {\n\t// Id of volume to delete\n\tVolumeId             string   `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeDeleteRequest) Reset()         { *m = SdkVolumeDeleteRequest{} }\nfunc (m *SdkVolumeDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeDeleteRequest) ProtoMessage()    {}\nfunc (*SdkVolumeDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{73}\n}\nfunc (m *SdkVolumeDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeDeleteRequest.Size(m)\n}\nfunc (m *SdkVolumeDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeDeleteRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeDeleteResponse) Reset()         { *m = SdkVolumeDeleteResponse{} }\nfunc (m *SdkVolumeDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeDeleteResponse) ProtoMessage()    {}\nfunc (*SdkVolumeDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{74}\n}\nfunc (m *SdkVolumeDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeDeleteResponse.Size(m)\n}\nfunc (m *SdkVolumeDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeDeleteResponse proto.InternalMessageInfo\n\ntype SdkVolumeInspectRequest struct {\n\t// Id of volume to inspect\n\tVolumeId             string   `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeInspectRequest) Reset()         { *m = SdkVolumeInspectRequest{} }\nfunc (m *SdkVolumeInspectRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeInspectRequest) ProtoMessage()    {}\nfunc (*SdkVolumeInspectRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{75}\n}\nfunc (m *SdkVolumeInspectRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeInspectRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeInspectRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeInspectRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeInspectRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeInspectRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeInspectRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeInspectRequest.Size(m)\n}\nfunc (m *SdkVolumeInspectRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeInspectRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeInspectRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeInspectRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeInspectResponse struct {\n\t// Information about the volume\n\tVolume               *Volume  `protobuf:\"bytes,1,opt,name=volume,proto3\" json:\"volume,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeInspectResponse) Reset()         { *m = SdkVolumeInspectResponse{} }\nfunc (m *SdkVolumeInspectResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeInspectResponse) ProtoMessage()    {}\nfunc (*SdkVolumeInspectResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{76}\n}\nfunc (m *SdkVolumeInspectResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeInspectResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeInspectResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeInspectResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeInspectResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeInspectResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeInspectResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeInspectResponse.Size(m)\n}\nfunc (m *SdkVolumeInspectResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeInspectResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeInspectResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeInspectResponse) GetVolume() *Volume {\n\tif m != nil {\n\t\treturn m.Volume\n\t}\n\treturn nil\n}\n\ntype SdkVolumeEnumerateRequest struct {\n\t// Volumes to match to this locator.\n\t// If not provided, all volumes will be returned.\n\tLocator              *VolumeLocator `protobuf:\"bytes,1,opt,name=locator,proto3\" json:\"locator,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}       `json:\"-\"`\n\tXXX_unrecognized     []byte         `json:\"-\"`\n\tXXX_sizecache        int32          `json:\"-\"`\n}\n\nfunc (m *SdkVolumeEnumerateRequest) Reset()         { *m = SdkVolumeEnumerateRequest{} }\nfunc (m *SdkVolumeEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkVolumeEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{77}\n}\nfunc (m *SdkVolumeEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeEnumerateRequest.Size(m)\n}\nfunc (m *SdkVolumeEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeEnumerateRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeEnumerateRequest) GetLocator() *VolumeLocator {\n\tif m != nil {\n\t\treturn m.Locator\n\t}\n\treturn nil\n}\n\ntype SdkVolumeEnumerateResponse struct {\n\t// List of volumes matching label\n\tVolumes              []*Volume `protobuf:\"bytes,1,rep,name=volumes,proto3\" json:\"volumes,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}  `json:\"-\"`\n\tXXX_unrecognized     []byte    `json:\"-\"`\n\tXXX_sizecache        int32     `json:\"-\"`\n}\n\nfunc (m *SdkVolumeEnumerateResponse) Reset()         { *m = SdkVolumeEnumerateResponse{} }\nfunc (m *SdkVolumeEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkVolumeEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{78}\n}\nfunc (m *SdkVolumeEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeEnumerateResponse.Size(m)\n}\nfunc (m *SdkVolumeEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeEnumerateResponse) GetVolumes() []*Volume {\n\tif m != nil {\n\t\treturn m.Volumes\n\t}\n\treturn nil\n}\n\ntype SdkVolumeSnapshotCreateRequest struct {\n\t// Id of volume to take the snapshot from\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Labels to apply to snapshot\n\tLabels               map[string]string `protobuf:\"bytes,2,rep,name=labels,proto3\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotCreateRequest) Reset()         { *m = SdkVolumeSnapshotCreateRequest{} }\nfunc (m *SdkVolumeSnapshotCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotCreateRequest) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{79}\n}\nfunc (m *SdkVolumeSnapshotCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotCreateRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateRequest.Size(m)\n}\nfunc (m *SdkVolumeSnapshotCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotCreateRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeSnapshotCreateRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeSnapshotCreateRequest) GetLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.Labels\n\t}\n\treturn nil\n}\n\ntype SdkVolumeSnapshotCreateResponse struct {\n\t// Id of immutable snapshot\n\tSnapshotId           string   `protobuf:\"bytes,1,opt,name=snapshot_id,json=snapshotId,proto3\" json:\"snapshot_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotCreateResponse) Reset()         { *m = SdkVolumeSnapshotCreateResponse{} }\nfunc (m *SdkVolumeSnapshotCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotCreateResponse) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{80}\n}\nfunc (m *SdkVolumeSnapshotCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotCreateResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotCreateResponse.Size(m)\n}\nfunc (m *SdkVolumeSnapshotCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotCreateResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeSnapshotCreateResponse) GetSnapshotId() string {\n\tif m != nil {\n\t\treturn m.SnapshotId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeSnapshotRestoreRequest struct {\n\t// Id of volume\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Snapshot id to apply to `volume_id`\n\tSnapshotId           string   `protobuf:\"bytes,2,opt,name=snapshot_id,json=snapshotId,proto3\" json:\"snapshot_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotRestoreRequest) Reset()         { *m = SdkVolumeSnapshotRestoreRequest{} }\nfunc (m *SdkVolumeSnapshotRestoreRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotRestoreRequest) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotRestoreRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{81}\n}\nfunc (m *SdkVolumeSnapshotRestoreRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotRestoreRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotRestoreRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotRestoreRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotRestoreRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreRequest.Size(m)\n}\nfunc (m *SdkVolumeSnapshotRestoreRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotRestoreRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotRestoreRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeSnapshotRestoreRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeSnapshotRestoreRequest) GetSnapshotId() string {\n\tif m != nil {\n\t\treturn m.SnapshotId\n\t}\n\treturn \"\"\n}\n\ntype SdkVolumeSnapshotRestoreResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotRestoreResponse) Reset()         { *m = SdkVolumeSnapshotRestoreResponse{} }\nfunc (m *SdkVolumeSnapshotRestoreResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotRestoreResponse) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotRestoreResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{82}\n}\nfunc (m *SdkVolumeSnapshotRestoreResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotRestoreResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotRestoreResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotRestoreResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotRestoreResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotRestoreResponse.Size(m)\n}\nfunc (m *SdkVolumeSnapshotRestoreResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotRestoreResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotRestoreResponse proto.InternalMessageInfo\n\ntype SdkVolumeSnapshotEnumerateRequest struct {\n\t// Id of volume\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// Labels from snapshot\n\tLabels               map[string]string `protobuf:\"bytes,2,rep,name=labels,proto3\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}          `json:\"-\"`\n\tXXX_unrecognized     []byte            `json:\"-\"`\n\tXXX_sizecache        int32             `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotEnumerateRequest) Reset()         { *m = SdkVolumeSnapshotEnumerateRequest{} }\nfunc (m *SdkVolumeSnapshotEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{83}\n}\nfunc (m *SdkVolumeSnapshotEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateRequest.Size(m)\n}\nfunc (m *SdkVolumeSnapshotEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotEnumerateRequest proto.InternalMessageInfo\n\nfunc (m *SdkVolumeSnapshotEnumerateRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkVolumeSnapshotEnumerateRequest) GetLabels() map[string]string {\n\tif m != nil {\n\t\treturn m.Labels\n\t}\n\treturn nil\n}\n\ntype SdkVolumeSnapshotEnumerateResponse struct {\n\t// List of immutable snapshots\n\tSnapshots            []*Volume `protobuf:\"bytes,1,rep,name=snapshots,proto3\" json:\"snapshots,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}  `json:\"-\"`\n\tXXX_unrecognized     []byte    `json:\"-\"`\n\tXXX_sizecache        int32     `json:\"-\"`\n}\n\nfunc (m *SdkVolumeSnapshotEnumerateResponse) Reset()         { *m = SdkVolumeSnapshotEnumerateResponse{} }\nfunc (m *SdkVolumeSnapshotEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkVolumeSnapshotEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkVolumeSnapshotEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{84}\n}\nfunc (m *SdkVolumeSnapshotEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkVolumeSnapshotEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkVolumeSnapshotEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkVolumeSnapshotEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkVolumeSnapshotEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkVolumeSnapshotEnumerateResponse.Size(m)\n}\nfunc (m *SdkVolumeSnapshotEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkVolumeSnapshotEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkVolumeSnapshotEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkVolumeSnapshotEnumerateResponse) GetSnapshots() []*Volume {\n\tif m != nil {\n\t\treturn m.Snapshots\n\t}\n\treturn nil\n}\n\ntype SdkClusterEnumerateRequest struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterEnumerateRequest) Reset()         { *m = SdkClusterEnumerateRequest{} }\nfunc (m *SdkClusterEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkClusterEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{85}\n}\nfunc (m *SdkClusterEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkClusterEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkClusterEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterEnumerateRequest.Size(m)\n}\nfunc (m *SdkClusterEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterEnumerateRequest proto.InternalMessageInfo\n\ntype SdkClusterEnumerateResponse struct {\n\t// Cluster information\n\tCluster              *StorageCluster `protobuf:\"bytes,1,opt,name=cluster,proto3\" json:\"cluster,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}        `json:\"-\"`\n\tXXX_unrecognized     []byte          `json:\"-\"`\n\tXXX_sizecache        int32           `json:\"-\"`\n}\n\nfunc (m *SdkClusterEnumerateResponse) Reset()         { *m = SdkClusterEnumerateResponse{} }\nfunc (m *SdkClusterEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkClusterEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{86}\n}\nfunc (m *SdkClusterEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkClusterEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkClusterEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterEnumerateResponse.Size(m)\n}\nfunc (m *SdkClusterEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkClusterEnumerateResponse) GetCluster() *StorageCluster {\n\tif m != nil {\n\t\treturn m.Cluster\n\t}\n\treturn nil\n}\n\ntype SdkClusterInspectRequest struct {\n\t// Id of node to inspect (required)\n\tNodeId               string   `protobuf:\"bytes,1,opt,name=node_id,json=nodeId,proto3\" json:\"node_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterInspectRequest) Reset()         { *m = SdkClusterInspectRequest{} }\nfunc (m *SdkClusterInspectRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterInspectRequest) ProtoMessage()    {}\nfunc (*SdkClusterInspectRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{87}\n}\nfunc (m *SdkClusterInspectRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterInspectRequest.Unmarshal(m, b)\n}\nfunc (m *SdkClusterInspectRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterInspectRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterInspectRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterInspectRequest.Merge(dst, src)\n}\nfunc (m *SdkClusterInspectRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterInspectRequest.Size(m)\n}\nfunc (m *SdkClusterInspectRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterInspectRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterInspectRequest proto.InternalMessageInfo\n\nfunc (m *SdkClusterInspectRequest) GetNodeId() string {\n\tif m != nil {\n\t\treturn m.NodeId\n\t}\n\treturn \"\"\n}\n\ntype SdkClusterInspectResponse struct {\n\t// Node information\n\tNode                 *StorageNode `protobuf:\"bytes,1,opt,name=node,proto3\" json:\"node,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}     `json:\"-\"`\n\tXXX_unrecognized     []byte       `json:\"-\"`\n\tXXX_sizecache        int32        `json:\"-\"`\n}\n\nfunc (m *SdkClusterInspectResponse) Reset()         { *m = SdkClusterInspectResponse{} }\nfunc (m *SdkClusterInspectResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterInspectResponse) ProtoMessage()    {}\nfunc (*SdkClusterInspectResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{88}\n}\nfunc (m *SdkClusterInspectResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterInspectResponse.Unmarshal(m, b)\n}\nfunc (m *SdkClusterInspectResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterInspectResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterInspectResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterInspectResponse.Merge(dst, src)\n}\nfunc (m *SdkClusterInspectResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterInspectResponse.Size(m)\n}\nfunc (m *SdkClusterInspectResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterInspectResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterInspectResponse proto.InternalMessageInfo\n\nfunc (m *SdkClusterInspectResponse) GetNode() *StorageNode {\n\tif m != nil {\n\t\treturn m.Node\n\t}\n\treturn nil\n}\n\ntype SdkClusterAlertEnumerateRequest struct {\n\t// Start time of alerts (required)\n\tTimeStart *timestamp.Timestamp `protobuf:\"bytes,1,opt,name=time_start,json=timeStart,proto3\" json:\"time_start,omitempty\"`\n\t// End time of alerts (required)\n\tTimeEnd *timestamp.Timestamp `protobuf:\"bytes,2,opt,name=time_end,json=timeEnd,proto3\" json:\"time_end,omitempty\"`\n\t// Type of resource (required)\n\tResource             ResourceType `protobuf:\"varint,3,opt,name=resource,proto3,enum=openstorage.api.ResourceType\" json:\"resource,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}     `json:\"-\"`\n\tXXX_unrecognized     []byte       `json:\"-\"`\n\tXXX_sizecache        int32        `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertEnumerateRequest) Reset()         { *m = SdkClusterAlertEnumerateRequest{} }\nfunc (m *SdkClusterAlertEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkClusterAlertEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{89}\n}\nfunc (m *SdkClusterAlertEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateRequest.Size(m)\n}\nfunc (m *SdkClusterAlertEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertEnumerateRequest proto.InternalMessageInfo\n\nfunc (m *SdkClusterAlertEnumerateRequest) GetTimeStart() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.TimeStart\n\t}\n\treturn nil\n}\n\nfunc (m *SdkClusterAlertEnumerateRequest) GetTimeEnd() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.TimeEnd\n\t}\n\treturn nil\n}\n\nfunc (m *SdkClusterAlertEnumerateRequest) GetResource() ResourceType {\n\tif m != nil {\n\t\treturn m.Resource\n\t}\n\treturn ResourceType_RESOURCE_TYPE_NONE\n}\n\ntype SdkClusterAlertEnumerateResponse struct {\n\t// Information on the alerts requested\n\tAlerts               *Alerts  `protobuf:\"bytes,1,opt,name=alerts,proto3\" json:\"alerts,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertEnumerateResponse) Reset()         { *m = SdkClusterAlertEnumerateResponse{} }\nfunc (m *SdkClusterAlertEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkClusterAlertEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{90}\n}\nfunc (m *SdkClusterAlertEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertEnumerateResponse.Size(m)\n}\nfunc (m *SdkClusterAlertEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkClusterAlertEnumerateResponse) GetAlerts() *Alerts {\n\tif m != nil {\n\t\treturn m.Alerts\n\t}\n\treturn nil\n}\n\ntype SdkClusterAlertClearRequest struct {\n\t// Type of resource (required)\n\tResource ResourceType `protobuf:\"varint,1,opt,name=resource,proto3,enum=openstorage.api.ResourceType\" json:\"resource,omitempty\"`\n\t// Id of alert as returned by ClusterEnumerateAlertResponse (required)\n\tAlertId              int64    `protobuf:\"varint,2,opt,name=alert_id,json=alertId,proto3\" json:\"alert_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertClearRequest) Reset()         { *m = SdkClusterAlertClearRequest{} }\nfunc (m *SdkClusterAlertClearRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertClearRequest) ProtoMessage()    {}\nfunc (*SdkClusterAlertClearRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{91}\n}\nfunc (m *SdkClusterAlertClearRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertClearRequest.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertClearRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertClearRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertClearRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertClearRequest.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertClearRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertClearRequest.Size(m)\n}\nfunc (m *SdkClusterAlertClearRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertClearRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertClearRequest proto.InternalMessageInfo\n\nfunc (m *SdkClusterAlertClearRequest) GetResource() ResourceType {\n\tif m != nil {\n\t\treturn m.Resource\n\t}\n\treturn ResourceType_RESOURCE_TYPE_NONE\n}\n\nfunc (m *SdkClusterAlertClearRequest) GetAlertId() int64 {\n\tif m != nil {\n\t\treturn m.AlertId\n\t}\n\treturn 0\n}\n\ntype SdkClusterAlertClearResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertClearResponse) Reset()         { *m = SdkClusterAlertClearResponse{} }\nfunc (m *SdkClusterAlertClearResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertClearResponse) ProtoMessage()    {}\nfunc (*SdkClusterAlertClearResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{92}\n}\nfunc (m *SdkClusterAlertClearResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertClearResponse.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertClearResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertClearResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertClearResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertClearResponse.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertClearResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertClearResponse.Size(m)\n}\nfunc (m *SdkClusterAlertClearResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertClearResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertClearResponse proto.InternalMessageInfo\n\ntype SdkClusterAlertDeleteRequest struct {\n\t// Type of resource (required)\n\tResource ResourceType `protobuf:\"varint,1,opt,name=resource,proto3,enum=openstorage.api.ResourceType\" json:\"resource,omitempty\"`\n\t// Id of alert as returned by ClusterEnumerateAlertResponse (required)\n\tAlertId              int64    `protobuf:\"varint,2,opt,name=alert_id,json=alertId,proto3\" json:\"alert_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertDeleteRequest) Reset()         { *m = SdkClusterAlertDeleteRequest{} }\nfunc (m *SdkClusterAlertDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertDeleteRequest) ProtoMessage()    {}\nfunc (*SdkClusterAlertDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{93}\n}\nfunc (m *SdkClusterAlertDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteRequest.Size(m)\n}\nfunc (m *SdkClusterAlertDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkClusterAlertDeleteRequest) GetResource() ResourceType {\n\tif m != nil {\n\t\treturn m.Resource\n\t}\n\treturn ResourceType_RESOURCE_TYPE_NONE\n}\n\nfunc (m *SdkClusterAlertDeleteRequest) GetAlertId() int64 {\n\tif m != nil {\n\t\treturn m.AlertId\n\t}\n\treturn 0\n}\n\ntype SdkClusterAlertDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkClusterAlertDeleteResponse) Reset()         { *m = SdkClusterAlertDeleteResponse{} }\nfunc (m *SdkClusterAlertDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkClusterAlertDeleteResponse) ProtoMessage()    {}\nfunc (*SdkClusterAlertDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{94}\n}\nfunc (m *SdkClusterAlertDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkClusterAlertDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkClusterAlertDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkClusterAlertDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkClusterAlertDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkClusterAlertDeleteResponse.Size(m)\n}\nfunc (m *SdkClusterAlertDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkClusterAlertDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkClusterAlertDeleteResponse proto.InternalMessageInfo\n\ntype SdkObjectstoreInspectRequest struct {\n\t// ObjecstoreID to query objestore status\n\tObjectstoreId        string   `protobuf:\"bytes,1,opt,name=objectstore_id,json=objectstoreId,proto3\" json:\"objectstore_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreInspectRequest) Reset()         { *m = SdkObjectstoreInspectRequest{} }\nfunc (m *SdkObjectstoreInspectRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreInspectRequest) ProtoMessage()    {}\nfunc (*SdkObjectstoreInspectRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{95}\n}\nfunc (m *SdkObjectstoreInspectRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreInspectRequest.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreInspectRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreInspectRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreInspectRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreInspectRequest.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreInspectRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreInspectRequest.Size(m)\n}\nfunc (m *SdkObjectstoreInspectRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreInspectRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreInspectRequest proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreInspectRequest) GetObjectstoreId() string {\n\tif m != nil {\n\t\treturn m.ObjectstoreId\n\t}\n\treturn \"\"\n}\n\ntype SdkObjectstoreInspectResponse struct {\n\t// Objectstore status\n\tObjectstoreStatus    *ObjectstoreInfo `protobuf:\"bytes,1,opt,name=objectstore_status,json=objectstoreStatus,proto3\" json:\"objectstore_status,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}         `json:\"-\"`\n\tXXX_unrecognized     []byte           `json:\"-\"`\n\tXXX_sizecache        int32            `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreInspectResponse) Reset()         { *m = SdkObjectstoreInspectResponse{} }\nfunc (m *SdkObjectstoreInspectResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreInspectResponse) ProtoMessage()    {}\nfunc (*SdkObjectstoreInspectResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{96}\n}\nfunc (m *SdkObjectstoreInspectResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreInspectResponse.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreInspectResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreInspectResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreInspectResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreInspectResponse.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreInspectResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreInspectResponse.Size(m)\n}\nfunc (m *SdkObjectstoreInspectResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreInspectResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreInspectResponse proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreInspectResponse) GetObjectstoreStatus() *ObjectstoreInfo {\n\tif m != nil {\n\t\treturn m.ObjectstoreStatus\n\t}\n\treturn nil\n}\n\ntype SdkObjectstoreCreateRequest struct {\n\t// Volume on which objectstore will be running\n\tVolumeName           string   `protobuf:\"bytes,1,opt,name=volume_name,json=volumeName,proto3\" json:\"volume_name,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreCreateRequest) Reset()         { *m = SdkObjectstoreCreateRequest{} }\nfunc (m *SdkObjectstoreCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreCreateRequest) ProtoMessage()    {}\nfunc (*SdkObjectstoreCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{97}\n}\nfunc (m *SdkObjectstoreCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreCreateRequest.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreCreateRequest.Size(m)\n}\nfunc (m *SdkObjectstoreCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreCreateRequest proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreCreateRequest) GetVolumeName() string {\n\tif m != nil {\n\t\treturn m.VolumeName\n\t}\n\treturn \"\"\n}\n\ntype SdkObjectstoreCreateResponse struct {\n\t// Created objecstore status\n\tObjectstoreStatus    *ObjectstoreInfo `protobuf:\"bytes,1,opt,name=objectstore_status,json=objectstoreStatus,proto3\" json:\"objectstore_status,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}         `json:\"-\"`\n\tXXX_unrecognized     []byte           `json:\"-\"`\n\tXXX_sizecache        int32            `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreCreateResponse) Reset()         { *m = SdkObjectstoreCreateResponse{} }\nfunc (m *SdkObjectstoreCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreCreateResponse) ProtoMessage()    {}\nfunc (*SdkObjectstoreCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{98}\n}\nfunc (m *SdkObjectstoreCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreCreateResponse.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreCreateResponse.Size(m)\n}\nfunc (m *SdkObjectstoreCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreCreateResponse proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreCreateResponse) GetObjectstoreStatus() *ObjectstoreInfo {\n\tif m != nil {\n\t\treturn m.ObjectstoreStatus\n\t}\n\treturn nil\n}\n\ntype SdkObjectstoreDeleteRequest struct {\n\t// Objectstore ID to delete\n\tObjectstoreId        string   `protobuf:\"bytes,1,opt,name=objectstore_id,json=objectstoreId,proto3\" json:\"objectstore_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreDeleteRequest) Reset()         { *m = SdkObjectstoreDeleteRequest{} }\nfunc (m *SdkObjectstoreDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreDeleteRequest) ProtoMessage()    {}\nfunc (*SdkObjectstoreDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{99}\n}\nfunc (m *SdkObjectstoreDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteRequest.Size(m)\n}\nfunc (m *SdkObjectstoreDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreDeleteRequest) GetObjectstoreId() string {\n\tif m != nil {\n\t\treturn m.ObjectstoreId\n\t}\n\treturn \"\"\n}\n\ntype SdkObjectstoreDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreDeleteResponse) Reset()         { *m = SdkObjectstoreDeleteResponse{} }\nfunc (m *SdkObjectstoreDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreDeleteResponse) ProtoMessage()    {}\nfunc (*SdkObjectstoreDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{100}\n}\nfunc (m *SdkObjectstoreDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreDeleteResponse.Size(m)\n}\nfunc (m *SdkObjectstoreDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreDeleteResponse proto.InternalMessageInfo\n\ntype SdkObjectstoreUpdateRequest struct {\n\t// Objectstore Id to update\n\tObjectstoreId string `protobuf:\"bytes,1,opt,name=objectstore_id,json=objectstoreId,proto3\" json:\"objectstore_id,omitempty\"`\n\t// enable/disable objectstore\n\tEnable               bool     `protobuf:\"varint,2,opt,name=enable,proto3\" json:\"enable,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreUpdateRequest) Reset()         { *m = SdkObjectstoreUpdateRequest{} }\nfunc (m *SdkObjectstoreUpdateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreUpdateRequest) ProtoMessage()    {}\nfunc (*SdkObjectstoreUpdateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{101}\n}\nfunc (m *SdkObjectstoreUpdateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreUpdateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreUpdateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreUpdateRequest.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreUpdateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateRequest.Size(m)\n}\nfunc (m *SdkObjectstoreUpdateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreUpdateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreUpdateRequest proto.InternalMessageInfo\n\nfunc (m *SdkObjectstoreUpdateRequest) GetObjectstoreId() string {\n\tif m != nil {\n\t\treturn m.ObjectstoreId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkObjectstoreUpdateRequest) GetEnable() bool {\n\tif m != nil {\n\t\treturn m.Enable\n\t}\n\treturn false\n}\n\ntype SdkObjectstoreUpdateResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkObjectstoreUpdateResponse) Reset()         { *m = SdkObjectstoreUpdateResponse{} }\nfunc (m *SdkObjectstoreUpdateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkObjectstoreUpdateResponse) ProtoMessage()    {}\nfunc (*SdkObjectstoreUpdateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{102}\n}\nfunc (m *SdkObjectstoreUpdateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkObjectstoreUpdateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkObjectstoreUpdateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkObjectstoreUpdateResponse.Merge(dst, src)\n}\nfunc (m *SdkObjectstoreUpdateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkObjectstoreUpdateResponse.Size(m)\n}\nfunc (m *SdkObjectstoreUpdateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkObjectstoreUpdateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkObjectstoreUpdateResponse proto.InternalMessageInfo\n\ntype SdkCloudBackupCreateRequest struct {\n\t// VolumeID of the volume for which cloudbackup is requested\n\tVolumeId string `protobuf:\"bytes,1,opt,name=volume_id,json=volumeId,proto3\" json:\"volume_id,omitempty\"`\n\t// CredentialUUID is cloud credential to be used for backup\n\tCredentialUuid string `protobuf:\"bytes,2,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\t// Full indicates if full backup is desired even though incremental is possible\n\tFull                 bool     `protobuf:\"varint,3,opt,name=full,proto3\" json:\"full,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupCreateRequest) Reset()         { *m = SdkCloudBackupCreateRequest{} }\nfunc (m *SdkCloudBackupCreateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupCreateRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupCreateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{103}\n}\nfunc (m *SdkCloudBackupCreateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupCreateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupCreateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupCreateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupCreateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupCreateRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupCreateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupCreateRequest.Size(m)\n}\nfunc (m *SdkCloudBackupCreateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupCreateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupCreateRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupCreateRequest) GetVolumeId() string {\n\tif m != nil {\n\t\treturn m.VolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupCreateRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupCreateRequest) GetFull() bool {\n\tif m != nil {\n\t\treturn m.Full\n\t}\n\treturn false\n}\n\ntype SdkCloudBackupCreateResponse struct {\n\t// Id of the backup created\n\tBackupId             string   `protobuf:\"bytes,1,opt,name=backup_id,json=backupId,proto3\" json:\"backup_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupCreateResponse) Reset()         { *m = SdkCloudBackupCreateResponse{} }\nfunc (m *SdkCloudBackupCreateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupCreateResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupCreateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{104}\n}\nfunc (m *SdkCloudBackupCreateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupCreateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupCreateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupCreateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupCreateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupCreateResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupCreateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupCreateResponse.Size(m)\n}\nfunc (m *SdkCloudBackupCreateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupCreateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupCreateResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupCreateResponse) GetBackupId() string {\n\tif m != nil {\n\t\treturn m.BackupId\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupRestoreRequest struct {\n\t// Backup ID being restored\n\tBackupId string `protobuf:\"bytes,1,opt,name=backup_id,json=backupId,proto3\" json:\"backup_id,omitempty\"`\n\t// Optional volume Name of the new volume to be created\n\t// in the cluster for restoring the cloudbackup\n\tRestoreVolumeName string `protobuf:\"bytes,2,opt,name=restore_volume_name,json=restoreVolumeName,proto3\" json:\"restore_volume_name,omitempty\"`\n\t// The credential to be used for restore operation\n\tCredentialUuid string `protobuf:\"bytes,3,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\t// Optional for provisioning restore\n\t// volume (ResoreVolumeName should not be specified)\n\tNodeId               string   `protobuf:\"bytes,4,opt,name=node_id,json=nodeId,proto3\" json:\"node_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupRestoreRequest) Reset()         { *m = SdkCloudBackupRestoreRequest{} }\nfunc (m *SdkCloudBackupRestoreRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupRestoreRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupRestoreRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{105}\n}\nfunc (m *SdkCloudBackupRestoreRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupRestoreRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupRestoreRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupRestoreRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupRestoreRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreRequest.Size(m)\n}\nfunc (m *SdkCloudBackupRestoreRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupRestoreRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupRestoreRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupRestoreRequest) GetBackupId() string {\n\tif m != nil {\n\t\treturn m.BackupId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupRestoreRequest) GetRestoreVolumeName() string {\n\tif m != nil {\n\t\treturn m.RestoreVolumeName\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupRestoreRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupRestoreRequest) GetNodeId() string {\n\tif m != nil {\n\t\treturn m.NodeId\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupRestoreResponse struct {\n\t// VolumeID to which the backup is being restored\n\tRestoreVolumeId      string   `protobuf:\"bytes,1,opt,name=restore_volume_id,json=restoreVolumeId,proto3\" json:\"restore_volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupRestoreResponse) Reset()         { *m = SdkCloudBackupRestoreResponse{} }\nfunc (m *SdkCloudBackupRestoreResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupRestoreResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupRestoreResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{106}\n}\nfunc (m *SdkCloudBackupRestoreResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupRestoreResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupRestoreResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupRestoreResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupRestoreResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupRestoreResponse.Size(m)\n}\nfunc (m *SdkCloudBackupRestoreResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupRestoreResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupRestoreResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupRestoreResponse) GetRestoreVolumeId() string {\n\tif m != nil {\n\t\treturn m.RestoreVolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupDeleteRequest struct {\n\t// ID is the ID of the cloud backup\n\tBackupId string `protobuf:\"bytes,1,opt,name=backup_id,json=backupId,proto3\" json:\"backup_id,omitempty\"`\n\t// CredentialUUID is the credential for cloud to be used for the request\n\tCredentialUuid string `protobuf:\"bytes,2,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\t// Force Delete cloudbackup even if there are dependencies\n\tForce                bool     `protobuf:\"varint,3,opt,name=force,proto3\" json:\"force,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupDeleteRequest) Reset()         { *m = SdkCloudBackupDeleteRequest{} }\nfunc (m *SdkCloudBackupDeleteRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupDeleteRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupDeleteRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{107}\n}\nfunc (m *SdkCloudBackupDeleteRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupDeleteRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupDeleteRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupDeleteRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupDeleteRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteRequest.Size(m)\n}\nfunc (m *SdkCloudBackupDeleteRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupDeleteRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupDeleteRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupDeleteRequest) GetBackupId() string {\n\tif m != nil {\n\t\treturn m.BackupId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupDeleteRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupDeleteRequest) GetForce() bool {\n\tif m != nil {\n\t\treturn m.Force\n\t}\n\treturn false\n}\n\ntype SdkCloudBackupDeleteResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupDeleteResponse) Reset()         { *m = SdkCloudBackupDeleteResponse{} }\nfunc (m *SdkCloudBackupDeleteResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupDeleteResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupDeleteResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{108}\n}\nfunc (m *SdkCloudBackupDeleteResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupDeleteResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupDeleteResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupDeleteResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupDeleteResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteResponse.Size(m)\n}\nfunc (m *SdkCloudBackupDeleteResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupDeleteResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupDeleteResponse proto.InternalMessageInfo\n\ntype SdkCloudBackupDeleteAllRequest struct {\n\t// id of the volume for the request\n\tSrcVolumeId string `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// CredentialUUID is the credential for cloud to be used for the request\n\tCredentialUuid       string   `protobuf:\"bytes,2,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupDeleteAllRequest) Reset()         { *m = SdkCloudBackupDeleteAllRequest{} }\nfunc (m *SdkCloudBackupDeleteAllRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupDeleteAllRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupDeleteAllRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{109}\n}\nfunc (m *SdkCloudBackupDeleteAllRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupDeleteAllRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupDeleteAllRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupDeleteAllRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupDeleteAllRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllRequest.Size(m)\n}\nfunc (m *SdkCloudBackupDeleteAllRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupDeleteAllRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupDeleteAllRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupDeleteAllRequest) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupDeleteAllRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupDeleteAllResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupDeleteAllResponse) Reset()         { *m = SdkCloudBackupDeleteAllResponse{} }\nfunc (m *SdkCloudBackupDeleteAllResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupDeleteAllResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupDeleteAllResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{110}\n}\nfunc (m *SdkCloudBackupDeleteAllResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupDeleteAllResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupDeleteAllResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupDeleteAllResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupDeleteAllResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupDeleteAllResponse.Size(m)\n}\nfunc (m *SdkCloudBackupDeleteAllResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupDeleteAllResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupDeleteAllResponse proto.InternalMessageInfo\n\ntype SdkCloudBackupEnumerateRequest struct {\n\t// Optional source id of the volume for the request\n\tSrcVolumeId string `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// ClusterID is the optional clusterID for the request\n\tClusterId string `protobuf:\"bytes,2,opt,name=cluster_id,json=clusterId,proto3\" json:\"cluster_id,omitempty\"`\n\t// CredentialUUID is the credential for cloud to be used for the request\n\tCredentialUuid string `protobuf:\"bytes,3,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\t// All if set to true, backups for all clusters in the cloud are processed\n\tAll                  bool     `protobuf:\"varint,4,opt,name=all,proto3\" json:\"all,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupEnumerateRequest) Reset()         { *m = SdkCloudBackupEnumerateRequest{} }\nfunc (m *SdkCloudBackupEnumerateRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupEnumerateRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupEnumerateRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{111}\n}\nfunc (m *SdkCloudBackupEnumerateRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupEnumerateRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupEnumerateRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupEnumerateRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupEnumerateRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateRequest.Size(m)\n}\nfunc (m *SdkCloudBackupEnumerateRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupEnumerateRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupEnumerateRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupEnumerateRequest) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupEnumerateRequest) GetClusterId() string {\n\tif m != nil {\n\t\treturn m.ClusterId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupEnumerateRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupEnumerateRequest) GetAll() bool {\n\tif m != nil {\n\t\treturn m.All\n\t}\n\treturn false\n}\n\ntype SdkCloudBackupInfo struct {\n\t// ID is the ID of the cloud backup\n\tId string `protobuf:\"bytes,1,opt,name=id,proto3\" json:\"id,omitempty\"`\n\t// Source volumeID of the backup\n\tSrcVolumeId string `protobuf:\"bytes,2,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// Name of the sourceVolume of the backup\n\tSrcVolumeName string `protobuf:\"bytes,3,opt,name=src_volume_name,json=srcVolumeName,proto3\" json:\"src_volume_name,omitempty\"`\n\t// Timestamp is the timestamp at which the source volume\n\t// was backed up to cloud\n\tTimestamp *timestamp.Timestamp `protobuf:\"bytes,4,opt,name=timestamp,proto3\" json:\"timestamp,omitempty\"`\n\t// Metadata associated with the backup\n\tMetadata map[string]string `protobuf:\"bytes,5,rep,name=metadata,proto3\" json:\"metadata,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\t// Status indicates the status of the backup\n\tStatus               SdkCloudBackupStatusType `protobuf:\"varint,6,opt,name=status,proto3,enum=openstorage.api.SdkCloudBackupStatusType\" json:\"status,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}                 `json:\"-\"`\n\tXXX_unrecognized     []byte                   `json:\"-\"`\n\tXXX_sizecache        int32                    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupInfo) Reset()         { *m = SdkCloudBackupInfo{} }\nfunc (m *SdkCloudBackupInfo) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupInfo) ProtoMessage()    {}\nfunc (*SdkCloudBackupInfo) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{112}\n}\nfunc (m *SdkCloudBackupInfo) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupInfo.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupInfo) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupInfo.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupInfo) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupInfo.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupInfo) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupInfo.Size(m)\n}\nfunc (m *SdkCloudBackupInfo) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupInfo.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupInfo proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupInfo) GetId() string {\n\tif m != nil {\n\t\treturn m.Id\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupInfo) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupInfo) GetSrcVolumeName() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeName\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupInfo) GetTimestamp() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.Timestamp\n\t}\n\treturn nil\n}\n\nfunc (m *SdkCloudBackupInfo) GetMetadata() map[string]string {\n\tif m != nil {\n\t\treturn m.Metadata\n\t}\n\treturn nil\n}\n\nfunc (m *SdkCloudBackupInfo) GetStatus() SdkCloudBackupStatusType {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn SdkCloudBackupStatusType_SdkCloudBackupStatusTypeUnknown\n}\n\ntype SdkCloudBackupEnumerateResponse struct {\n\tBackups              []*SdkCloudBackupInfo `protobuf:\"bytes,1,rep,name=backups,proto3\" json:\"backups,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}              `json:\"-\"`\n\tXXX_unrecognized     []byte                `json:\"-\"`\n\tXXX_sizecache        int32                 `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupEnumerateResponse) Reset()         { *m = SdkCloudBackupEnumerateResponse{} }\nfunc (m *SdkCloudBackupEnumerateResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupEnumerateResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupEnumerateResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{113}\n}\nfunc (m *SdkCloudBackupEnumerateResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupEnumerateResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupEnumerateResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupEnumerateResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupEnumerateResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupEnumerateResponse.Size(m)\n}\nfunc (m *SdkCloudBackupEnumerateResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupEnumerateResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupEnumerateResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupEnumerateResponse) GetBackups() []*SdkCloudBackupInfo {\n\tif m != nil {\n\t\treturn m.Backups\n\t}\n\treturn nil\n}\n\ntype SdkCloudBackupStatus struct {\n\t// ID is the ID for the operation\n\tBackupId string `protobuf:\"bytes,1,opt,name=backup_id,json=backupId,proto3\" json:\"backup_id,omitempty\"`\n\t// OpType indicates if this is a backup or restore\n\tOptype SdkCloudBackupOpType `protobuf:\"varint,2,opt,name=optype,proto3,enum=openstorage.api.SdkCloudBackupOpType\" json:\"optype,omitempty\"`\n\t// State indicates if the op is currently active/done/failed\n\tStatus SdkCloudBackupStatusType `protobuf:\"varint,3,opt,name=status,proto3,enum=openstorage.api.SdkCloudBackupStatusType\" json:\"status,omitempty\"`\n\t// BytesDone indicates total Bytes uploaded/downloaded\n\tBytesDone uint64 `protobuf:\"varint,4,opt,name=bytes_done,json=bytesDone,proto3\" json:\"bytes_done,omitempty\"`\n\t// StartTime indicates Op's start time\n\tStartTime *timestamp.Timestamp `protobuf:\"bytes,5,opt,name=start_time,json=startTime,proto3\" json:\"start_time,omitempty\"`\n\t// CompletedTime indicates Op's completed time\n\tCompletedTime *timestamp.Timestamp `protobuf:\"bytes,6,opt,name=completed_time,json=completedTime,proto3\" json:\"completed_time,omitempty\"`\n\t// NodeID is the ID of the node where this Op is active\n\tNodeId               string   `protobuf:\"bytes,7,opt,name=node_id,json=nodeId,proto3\" json:\"node_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupStatus) Reset()         { *m = SdkCloudBackupStatus{} }\nfunc (m *SdkCloudBackupStatus) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupStatus) ProtoMessage()    {}\nfunc (*SdkCloudBackupStatus) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{114}\n}\nfunc (m *SdkCloudBackupStatus) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupStatus.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupStatus) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupStatus.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupStatus) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupStatus.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupStatus) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupStatus.Size(m)\n}\nfunc (m *SdkCloudBackupStatus) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupStatus.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupStatus proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupStatus) GetBackupId() string {\n\tif m != nil {\n\t\treturn m.BackupId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupStatus) GetOptype() SdkCloudBackupOpType {\n\tif m != nil {\n\t\treturn m.Optype\n\t}\n\treturn SdkCloudBackupOpType_SdkCloudBackupOpTypeUnknown\n}\n\nfunc (m *SdkCloudBackupStatus) GetStatus() SdkCloudBackupStatusType {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn SdkCloudBackupStatusType_SdkCloudBackupStatusTypeUnknown\n}\n\nfunc (m *SdkCloudBackupStatus) GetBytesDone() uint64 {\n\tif m != nil {\n\t\treturn m.BytesDone\n\t}\n\treturn 0\n}\n\nfunc (m *SdkCloudBackupStatus) GetStartTime() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.StartTime\n\t}\n\treturn nil\n}\n\nfunc (m *SdkCloudBackupStatus) GetCompletedTime() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.CompletedTime\n\t}\n\treturn nil\n}\n\nfunc (m *SdkCloudBackupStatus) GetNodeId() string {\n\tif m != nil {\n\t\treturn m.NodeId\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupStatusRequest struct {\n\t// SrcVolumeID optional volumeID to list status of backup/restore\n\tSrcVolumeId string `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// Local indicates if only those backups/restores that are\n\t// active on current node must be returned\n\tLocal                bool     `protobuf:\"varint,2,opt,name=local,proto3\" json:\"local,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupStatusRequest) Reset()         { *m = SdkCloudBackupStatusRequest{} }\nfunc (m *SdkCloudBackupStatusRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupStatusRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupStatusRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{115}\n}\nfunc (m *SdkCloudBackupStatusRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupStatusRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupStatusRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupStatusRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupStatusRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupStatusRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupStatusRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupStatusRequest.Size(m)\n}\nfunc (m *SdkCloudBackupStatusRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupStatusRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupStatusRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupStatusRequest) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupStatusRequest) GetLocal() bool {\n\tif m != nil {\n\t\treturn m.Local\n\t}\n\treturn false\n}\n\ntype SdkCloudBackupStatusResponse struct {\n\t// statuses is list of currently active/failed/done backup/restores\n\tStatuses             map[string]*SdkCloudBackupStatus `protobuf:\"bytes,1,rep,name=statuses,proto3\" json:\"statuses,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"`\n\tXXX_NoUnkeyedLiteral struct{}                         `json:\"-\"`\n\tXXX_unrecognized     []byte                           `json:\"-\"`\n\tXXX_sizecache        int32                            `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupStatusResponse) Reset()         { *m = SdkCloudBackupStatusResponse{} }\nfunc (m *SdkCloudBackupStatusResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupStatusResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupStatusResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{116}\n}\nfunc (m *SdkCloudBackupStatusResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupStatusResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupStatusResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupStatusResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupStatusResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupStatusResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupStatusResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupStatusResponse.Size(m)\n}\nfunc (m *SdkCloudBackupStatusResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupStatusResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupStatusResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupStatusResponse) GetStatuses() map[string]*SdkCloudBackupStatus {\n\tif m != nil {\n\t\treturn m.Statuses\n\t}\n\treturn nil\n}\n\ntype SdkCloudBackupCatalogRequest struct {\n\t// Id of the backup\n\tBackupId string `protobuf:\"bytes,1,opt,name=backup_id,json=backupId,proto3\" json:\"backup_id,omitempty\"`\n\t// is the credential for cloud\n\tCredentialUuid       string   `protobuf:\"bytes,2,opt,name=credential_uuid,json=credentialUuid,proto3\" json:\"credential_uuid,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupCatalogRequest) Reset()         { *m = SdkCloudBackupCatalogRequest{} }\nfunc (m *SdkCloudBackupCatalogRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupCatalogRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupCatalogRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{117}\n}\nfunc (m *SdkCloudBackupCatalogRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupCatalogRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupCatalogRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupCatalogRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupCatalogRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogRequest.Size(m)\n}\nfunc (m *SdkCloudBackupCatalogRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupCatalogRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupCatalogRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupCatalogRequest) GetBackupId() string {\n\tif m != nil {\n\t\treturn m.BackupId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupCatalogRequest) GetCredentialUuid() string {\n\tif m != nil {\n\t\treturn m.CredentialUuid\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupCatalogResponse struct {\n\t// Contents is listing of backup contents\n\tContents             []string `protobuf:\"bytes,1,rep,name=contents,proto3\" json:\"contents,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupCatalogResponse) Reset()         { *m = SdkCloudBackupCatalogResponse{} }\nfunc (m *SdkCloudBackupCatalogResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupCatalogResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupCatalogResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{118}\n}\nfunc (m *SdkCloudBackupCatalogResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupCatalogResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupCatalogResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupCatalogResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupCatalogResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupCatalogResponse.Size(m)\n}\nfunc (m *SdkCloudBackupCatalogResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupCatalogResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupCatalogResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupCatalogResponse) GetContents() []string {\n\tif m != nil {\n\t\treturn m.Contents\n\t}\n\treturn nil\n}\n\ntype SdkCloudBackupHistoryItem struct {\n\t// SrcVolumeID is volume ID which was backedup\n\tSrcVolumeId string `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// TimeStamp is the time at which either backup completed/failed\n\tTimestamp *timestamp.Timestamp `protobuf:\"bytes,2,opt,name=timestamp,proto3\" json:\"timestamp,omitempty\"`\n\t// Status indicates whether backup was completed/failed\n\tStatus               SdkCloudBackupStatusType `protobuf:\"varint,3,opt,name=status,proto3,enum=openstorage.api.SdkCloudBackupStatusType\" json:\"status,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}                 `json:\"-\"`\n\tXXX_unrecognized     []byte                   `json:\"-\"`\n\tXXX_sizecache        int32                    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupHistoryItem) Reset()         { *m = SdkCloudBackupHistoryItem{} }\nfunc (m *SdkCloudBackupHistoryItem) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupHistoryItem) ProtoMessage()    {}\nfunc (*SdkCloudBackupHistoryItem) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{119}\n}\nfunc (m *SdkCloudBackupHistoryItem) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryItem.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupHistoryItem) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryItem.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupHistoryItem) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupHistoryItem.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupHistoryItem) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryItem.Size(m)\n}\nfunc (m *SdkCloudBackupHistoryItem) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupHistoryItem.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupHistoryItem proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupHistoryItem) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupHistoryItem) GetTimestamp() *timestamp.Timestamp {\n\tif m != nil {\n\t\treturn m.Timestamp\n\t}\n\treturn nil\n}\n\nfunc (m *SdkCloudBackupHistoryItem) GetStatus() SdkCloudBackupStatusType {\n\tif m != nil {\n\t\treturn m.Status\n\t}\n\treturn SdkCloudBackupStatusType_SdkCloudBackupStatusTypeUnknown\n}\n\ntype SdkCloudBackupHistoryRequest struct {\n\t// volumeID for which history of backup/restore is being requested\n\t// (optional) If not provided, it will return the history for all volumes.\n\tSrcVolumeId          string   `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupHistoryRequest) Reset()         { *m = SdkCloudBackupHistoryRequest{} }\nfunc (m *SdkCloudBackupHistoryRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupHistoryRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupHistoryRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{120}\n}\nfunc (m *SdkCloudBackupHistoryRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupHistoryRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupHistoryRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupHistoryRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupHistoryRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryRequest.Size(m)\n}\nfunc (m *SdkCloudBackupHistoryRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupHistoryRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupHistoryRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupHistoryRequest) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\ntype SdkCloudBackupHistoryResponse struct {\n\t// HistoryList is list of past backup/restores in the cluster\n\tHistoryList          []*SdkCloudBackupHistoryItem `protobuf:\"bytes,1,rep,name=history_list,json=historyList,proto3\" json:\"history_list,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}                     `json:\"-\"`\n\tXXX_unrecognized     []byte                       `json:\"-\"`\n\tXXX_sizecache        int32                        `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupHistoryResponse) Reset()         { *m = SdkCloudBackupHistoryResponse{} }\nfunc (m *SdkCloudBackupHistoryResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupHistoryResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupHistoryResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{121}\n}\nfunc (m *SdkCloudBackupHistoryResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupHistoryResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupHistoryResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupHistoryResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupHistoryResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupHistoryResponse.Size(m)\n}\nfunc (m *SdkCloudBackupHistoryResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupHistoryResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupHistoryResponse proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupHistoryResponse) GetHistoryList() []*SdkCloudBackupHistoryItem {\n\tif m != nil {\n\t\treturn m.HistoryList\n\t}\n\treturn nil\n}\n\ntype SdkCloudBackupStateChangeRequest struct {\n\t// SrcVolumeID is volume ID on which backup/restore\n\t// state change is being requested\n\tSrcVolumeId string `protobuf:\"bytes,1,opt,name=src_volume_id,json=srcVolumeId,proto3\" json:\"src_volume_id,omitempty\"`\n\t// RequestedState is desired state of the op\n\t// can be pause/resume/stop\n\tRequestedState       SdkCloudBackupRequestedState `protobuf:\"varint,2,opt,name=requested_state,json=requestedState,proto3,enum=openstorage.api.SdkCloudBackupRequestedState\" json:\"requested_state,omitempty\"`\n\tXXX_NoUnkeyedLiteral struct{}                     `json:\"-\"`\n\tXXX_unrecognized     []byte                       `json:\"-\"`\n\tXXX_sizecache        int32                        `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupStateChangeRequest) Reset()         { *m = SdkCloudBackupStateChangeRequest{} }\nfunc (m *SdkCloudBackupStateChangeRequest) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupStateChangeRequest) ProtoMessage()    {}\nfunc (*SdkCloudBackupStateChangeRequest) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{122}\n}\nfunc (m *SdkCloudBackupStateChangeRequest) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeRequest.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupStateChangeRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeRequest.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupStateChangeRequest) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupStateChangeRequest.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupStateChangeRequest) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeRequest.Size(m)\n}\nfunc (m *SdkCloudBackupStateChangeRequest) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupStateChangeRequest.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupStateChangeRequest proto.InternalMessageInfo\n\nfunc (m *SdkCloudBackupStateChangeRequest) GetSrcVolumeId() string {\n\tif m != nil {\n\t\treturn m.SrcVolumeId\n\t}\n\treturn \"\"\n}\n\nfunc (m *SdkCloudBackupStateChangeRequest) GetRequestedState() SdkCloudBackupRequestedState {\n\tif m != nil {\n\t\treturn m.RequestedState\n\t}\n\treturn SdkCloudBackupRequestedState_SdkCloudBackupRequestedStateUnknown\n}\n\ntype SdkCloudBackupStateChangeResponse struct {\n\tXXX_NoUnkeyedLiteral struct{} `json:\"-\"`\n\tXXX_unrecognized     []byte   `json:\"-\"`\n\tXXX_sizecache        int32    `json:\"-\"`\n}\n\nfunc (m *SdkCloudBackupStateChangeResponse) Reset()         { *m = SdkCloudBackupStateChangeResponse{} }\nfunc (m *SdkCloudBackupStateChangeResponse) String() string { return proto.CompactTextString(m) }\nfunc (*SdkCloudBackupStateChangeResponse) ProtoMessage()    {}\nfunc (*SdkCloudBackupStateChangeResponse) Descriptor() ([]byte, []int) {\n\treturn fileDescriptor_api_4634bdf1f2f590eb, []int{123}\n}\nfunc (m *SdkCloudBackupStateChangeResponse) XXX_Unmarshal(b []byte) error {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeResponse.Unmarshal(m, b)\n}\nfunc (m *SdkCloudBackupStateChangeResponse) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeResponse.Marshal(b, m, deterministic)\n}\nfunc (dst *SdkCloudBackupStateChangeResponse) XXX_Merge(src proto.Message) {\n\txxx_messageInfo_SdkCloudBackupStateChangeResponse.Merge(dst, src)\n}\nfunc (m *SdkCloudBackupStateChangeResponse) XXX_Size() int {\n\treturn xxx_messageInfo_SdkCloudBackupStateChangeResponse.Size(m)\n}\nfunc (m *SdkCloudBackupStateChangeResponse) XXX_DiscardUnknown() {\n\txxx_messageInfo_SdkCloudBackupStateChangeResponse.DiscardUnknown(m)\n}\n\nvar xxx_messageInfo_SdkCloudBackupStateChangeResponse proto.InternalMessageInfo\n\nfunc init() {\n\tproto.RegisterType((*StorageResource)(nil), \"openstorage.api.StorageResource\")\n\tproto.RegisterType((*StoragePool)(nil), \"openstorage.api.StoragePool\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.StoragePool.LabelsEntry\")\n\tproto.RegisterType((*VolumeLocator)(nil), \"openstorage.api.VolumeLocator\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.VolumeLocator.VolumeLabelsEntry\")\n\tproto.RegisterType((*Source)(nil), \"openstorage.api.Source\")\n\tproto.RegisterType((*Group)(nil), \"openstorage.api.Group\")\n\tproto.RegisterType((*VolumeSpec)(nil), \"openstorage.api.VolumeSpec\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.VolumeSpec.VolumeLabelsEntry\")\n\tproto.RegisterType((*ReplicaSet)(nil), \"openstorage.api.ReplicaSet\")\n\tproto.RegisterType((*RuntimeStateMap)(nil), \"openstorage.api.RuntimeStateMap\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.RuntimeStateMap.RuntimeStateEntry\")\n\tproto.RegisterType((*Volume)(nil), \"openstorage.api.Volume\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.Volume.AttachInfoEntry\")\n\tproto.RegisterType((*Stats)(nil), \"openstorage.api.Stats\")\n\tproto.RegisterType((*Alert)(nil), \"openstorage.api.Alert\")\n\tproto.RegisterType((*Alerts)(nil), \"openstorage.api.Alerts\")\n\tproto.RegisterType((*ObjectstoreInfo)(nil), \"openstorage.api.ObjectstoreInfo\")\n\tproto.RegisterType((*VolumeCreateRequest)(nil), \"openstorage.api.VolumeCreateRequest\")\n\tproto.RegisterType((*VolumeResponse)(nil), \"openstorage.api.VolumeResponse\")\n\tproto.RegisterType((*VolumeCreateResponse)(nil), \"openstorage.api.VolumeCreateResponse\")\n\tproto.RegisterType((*VolumeStateAction)(nil), \"openstorage.api.VolumeStateAction\")\n\tproto.RegisterType((*VolumeSetRequest)(nil), \"openstorage.api.VolumeSetRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.VolumeSetRequest.OptionsEntry\")\n\tproto.RegisterType((*VolumeSetResponse)(nil), \"openstorage.api.VolumeSetResponse\")\n\tproto.RegisterType((*SnapCreateRequest)(nil), \"openstorage.api.SnapCreateRequest\")\n\tproto.RegisterType((*SnapCreateResponse)(nil), \"openstorage.api.SnapCreateResponse\")\n\tproto.RegisterType((*VolumeInfo)(nil), \"openstorage.api.VolumeInfo\")\n\tproto.RegisterType((*VolumeConsumer)(nil), \"openstorage.api.VolumeConsumer\")\n\tproto.RegisterType((*GraphDriverChanges)(nil), \"openstorage.api.GraphDriverChanges\")\n\tproto.RegisterType((*ClusterResponse)(nil), \"openstorage.api.ClusterResponse\")\n\tproto.RegisterType((*ActiveRequest)(nil), \"openstorage.api.ActiveRequest\")\n\tproto.RegisterMapType((map[int64]string)(nil), \"openstorage.api.ActiveRequest.ReqestKVEntry\")\n\tproto.RegisterType((*ActiveRequests)(nil), \"openstorage.api.ActiveRequests\")\n\tproto.RegisterType((*GroupSnapCreateRequest)(nil), \"openstorage.api.GroupSnapCreateRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.GroupSnapCreateRequest.LabelsEntry\")\n\tproto.RegisterType((*GroupSnapCreateResponse)(nil), \"openstorage.api.GroupSnapCreateResponse\")\n\tproto.RegisterMapType((map[string]*SnapCreateResponse)(nil), \"openstorage.api.GroupSnapCreateResponse.SnapshotsEntry\")\n\tproto.RegisterType((*StorageNode)(nil), \"openstorage.api.StorageNode\")\n\tproto.RegisterMapType((map[string]*StorageResource)(nil), \"openstorage.api.StorageNode.DisksEntry\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.StorageNode.NodeLabelsEntry\")\n\tproto.RegisterType((*StorageCluster)(nil), \"openstorage.api.StorageCluster\")\n\tproto.RegisterType((*SdkSchedulePolicyCreateRequest)(nil), \"openstorage.api.SdkSchedulePolicyCreateRequest\")\n\tproto.RegisterType((*SdkSchedulePolicyCreateResponse)(nil), \"openstorage.api.SdkSchedulePolicyCreateResponse\")\n\tproto.RegisterType((*SdkSchedulePolicyUpdateRequest)(nil), \"openstorage.api.SdkSchedulePolicyUpdateRequest\")\n\tproto.RegisterType((*SdkSchedulePolicyUpdateResponse)(nil), \"openstorage.api.SdkSchedulePolicyUpdateResponse\")\n\tproto.RegisterType((*SdkSchedulePolicyEnumerateRequest)(nil), \"openstorage.api.SdkSchedulePolicyEnumerateRequest\")\n\tproto.RegisterType((*SdkSchedulePolicyEnumerateResponse)(nil), \"openstorage.api.SdkSchedulePolicyEnumerateResponse\")\n\tproto.RegisterType((*SdkSchedulePolicyInspectRequest)(nil), \"openstorage.api.SdkSchedulePolicyInspectRequest\")\n\tproto.RegisterType((*SdkSchedulePolicyInspectResponse)(nil), \"openstorage.api.SdkSchedulePolicyInspectResponse\")\n\tproto.RegisterType((*SdkSchedulePolicyDeleteRequest)(nil), \"openstorage.api.SdkSchedulePolicyDeleteRequest\")\n\tproto.RegisterType((*SdkSchedulePolicyDeleteResponse)(nil), \"openstorage.api.SdkSchedulePolicyDeleteResponse\")\n\tproto.RegisterType((*SdkSchedulePolicy)(nil), \"openstorage.api.SdkSchedulePolicy\")\n\tproto.RegisterType((*SdkCredentialCreateAzureRequest)(nil), \"openstorage.api.SdkCredentialCreateAzureRequest\")\n\tproto.RegisterType((*SdkCredentialCreateAzureResponse)(nil), \"openstorage.api.SdkCredentialCreateAzureResponse\")\n\tproto.RegisterType((*SdkCredentialCreateGoogleRequest)(nil), \"openstorage.api.SdkCredentialCreateGoogleRequest\")\n\tproto.RegisterType((*SdkCredentialCreateGoogleResponse)(nil), \"openstorage.api.SdkCredentialCreateGoogleResponse\")\n\tproto.RegisterType((*SdkCredentialCreateAWSRequest)(nil), \"openstorage.api.SdkCredentialCreateAWSRequest\")\n\tproto.RegisterType((*SdkCredentialCreateAWSResponse)(nil), \"openstorage.api.SdkCredentialCreateAWSResponse\")\n\tproto.RegisterType((*S3Credential)(nil), \"openstorage.api.S3Credential\")\n\tproto.RegisterType((*AzureCredential)(nil), \"openstorage.api.AzureCredential\")\n\tproto.RegisterType((*GoogleCredential)(nil), \"openstorage.api.GoogleCredential\")\n\tproto.RegisterType((*SdkCredentialEnumerateAWSRequest)(nil), \"openstorage.api.SdkCredentialEnumerateAWSRequest\")\n\tproto.RegisterType((*SdkCredentialEnumerateAWSResponse)(nil), \"openstorage.api.SdkCredentialEnumerateAWSResponse\")\n\tproto.RegisterType((*SdkCredentialEnumerateAzureRequest)(nil), \"openstorage.api.SdkCredentialEnumerateAzureRequest\")\n\tproto.RegisterType((*SdkCredentialEnumerateAzureResponse)(nil), \"openstorage.api.SdkCredentialEnumerateAzureResponse\")\n\tproto.RegisterType((*SdkCredentialEnumerateGoogleRequest)(nil), \"openstorage.api.SdkCredentialEnumerateGoogleRequest\")\n\tproto.RegisterType((*SdkCredentialEnumerateGoogleResponse)(nil), \"openstorage.api.SdkCredentialEnumerateGoogleResponse\")\n\tproto.RegisterType((*SdkCredentialDeleteRequest)(nil), \"openstorage.api.SdkCredentialDeleteRequest\")\n\tproto.RegisterType((*SdkCredentialDeleteResponse)(nil), \"openstorage.api.SdkCredentialDeleteResponse\")\n\tproto.RegisterType((*SdkCredentialValidateRequest)(nil), \"openstorage.api.SdkCredentialValidateRequest\")\n\tproto.RegisterType((*SdkCredentialValidateResponse)(nil), \"openstorage.api.SdkCredentialValidateResponse\")\n\tproto.RegisterType((*SdkVolumeMountRequest)(nil), \"openstorage.api.SdkVolumeMountRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkVolumeMountRequest.OptionsEntry\")\n\tproto.RegisterType((*SdkVolumeMountResponse)(nil), \"openstorage.api.SdkVolumeMountResponse\")\n\tproto.RegisterType((*SdkVolumeUnmountRequest)(nil), \"openstorage.api.SdkVolumeUnmountRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkVolumeUnmountRequest.OptionsEntry\")\n\tproto.RegisterType((*SdkVolumeUnmountResponse)(nil), \"openstorage.api.SdkVolumeUnmountResponse\")\n\tproto.RegisterType((*SdkVolumeAttachRequest)(nil), \"openstorage.api.SdkVolumeAttachRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkVolumeAttachRequest.OptionsEntry\")\n\tproto.RegisterType((*SdkVolumeAttachResponse)(nil), \"openstorage.api.SdkVolumeAttachResponse\")\n\tproto.RegisterType((*SdkVolumeDetachRequest)(nil), \"openstorage.api.SdkVolumeDetachRequest\")\n\tproto.RegisterType((*SdkVolumeDetachResponse)(nil), \"openstorage.api.SdkVolumeDetachResponse\")\n\tproto.RegisterType((*SdkVolumeCreateRequest)(nil), \"openstorage.api.SdkVolumeCreateRequest\")\n\tproto.RegisterType((*SdkVolumeCreateResponse)(nil), \"openstorage.api.SdkVolumeCreateResponse\")\n\tproto.RegisterType((*SdkVolumeCloneRequest)(nil), \"openstorage.api.SdkVolumeCloneRequest\")\n\tproto.RegisterType((*SdkVolumeCloneResponse)(nil), \"openstorage.api.SdkVolumeCloneResponse\")\n\tproto.RegisterType((*SdkVolumeDeleteRequest)(nil), \"openstorage.api.SdkVolumeDeleteRequest\")\n\tproto.RegisterType((*SdkVolumeDeleteResponse)(nil), \"openstorage.api.SdkVolumeDeleteResponse\")\n\tproto.RegisterType((*SdkVolumeInspectRequest)(nil), \"openstorage.api.SdkVolumeInspectRequest\")\n\tproto.RegisterType((*SdkVolumeInspectResponse)(nil), \"openstorage.api.SdkVolumeInspectResponse\")\n\tproto.RegisterType((*SdkVolumeEnumerateRequest)(nil), \"openstorage.api.SdkVolumeEnumerateRequest\")\n\tproto.RegisterType((*SdkVolumeEnumerateResponse)(nil), \"openstorage.api.SdkVolumeEnumerateResponse\")\n\tproto.RegisterType((*SdkVolumeSnapshotCreateRequest)(nil), \"openstorage.api.SdkVolumeSnapshotCreateRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkVolumeSnapshotCreateRequest.LabelsEntry\")\n\tproto.RegisterType((*SdkVolumeSnapshotCreateResponse)(nil), \"openstorage.api.SdkVolumeSnapshotCreateResponse\")\n\tproto.RegisterType((*SdkVolumeSnapshotRestoreRequest)(nil), \"openstorage.api.SdkVolumeSnapshotRestoreRequest\")\n\tproto.RegisterType((*SdkVolumeSnapshotRestoreResponse)(nil), \"openstorage.api.SdkVolumeSnapshotRestoreResponse\")\n\tproto.RegisterType((*SdkVolumeSnapshotEnumerateRequest)(nil), \"openstorage.api.SdkVolumeSnapshotEnumerateRequest\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkVolumeSnapshotEnumerateRequest.LabelsEntry\")\n\tproto.RegisterType((*SdkVolumeSnapshotEnumerateResponse)(nil), \"openstorage.api.SdkVolumeSnapshotEnumerateResponse\")\n\tproto.RegisterType((*SdkClusterEnumerateRequest)(nil), \"openstorage.api.SdkClusterEnumerateRequest\")\n\tproto.RegisterType((*SdkClusterEnumerateResponse)(nil), \"openstorage.api.SdkClusterEnumerateResponse\")\n\tproto.RegisterType((*SdkClusterInspectRequest)(nil), \"openstorage.api.SdkClusterInspectRequest\")\n\tproto.RegisterType((*SdkClusterInspectResponse)(nil), \"openstorage.api.SdkClusterInspectResponse\")\n\tproto.RegisterType((*SdkClusterAlertEnumerateRequest)(nil), \"openstorage.api.SdkClusterAlertEnumerateRequest\")\n\tproto.RegisterType((*SdkClusterAlertEnumerateResponse)(nil), \"openstorage.api.SdkClusterAlertEnumerateResponse\")\n\tproto.RegisterType((*SdkClusterAlertClearRequest)(nil), \"openstorage.api.SdkClusterAlertClearRequest\")\n\tproto.RegisterType((*SdkClusterAlertClearResponse)(nil), \"openstorage.api.SdkClusterAlertClearResponse\")\n\tproto.RegisterType((*SdkClusterAlertDeleteRequest)(nil), \"openstorage.api.SdkClusterAlertDeleteRequest\")\n\tproto.RegisterType((*SdkClusterAlertDeleteResponse)(nil), \"openstorage.api.SdkClusterAlertDeleteResponse\")\n\tproto.RegisterType((*SdkObjectstoreInspectRequest)(nil), \"openstorage.api.SdkObjectstoreInspectRequest\")\n\tproto.RegisterType((*SdkObjectstoreInspectResponse)(nil), \"openstorage.api.SdkObjectstoreInspectResponse\")\n\tproto.RegisterType((*SdkObjectstoreCreateRequest)(nil), \"openstorage.api.SdkObjectstoreCreateRequest\")\n\tproto.RegisterType((*SdkObjectstoreCreateResponse)(nil), \"openstorage.api.SdkObjectstoreCreateResponse\")\n\tproto.RegisterType((*SdkObjectstoreDeleteRequest)(nil), \"openstorage.api.SdkObjectstoreDeleteRequest\")\n\tproto.RegisterType((*SdkObjectstoreDeleteResponse)(nil), \"openstorage.api.SdkObjectstoreDeleteResponse\")\n\tproto.RegisterType((*SdkObjectstoreUpdateRequest)(nil), \"openstorage.api.SdkObjectstoreUpdateRequest\")\n\tproto.RegisterType((*SdkObjectstoreUpdateResponse)(nil), \"openstorage.api.SdkObjectstoreUpdateResponse\")\n\tproto.RegisterType((*SdkCloudBackupCreateRequest)(nil), \"openstorage.api.SdkCloudBackupCreateRequest\")\n\tproto.RegisterType((*SdkCloudBackupCreateResponse)(nil), \"openstorage.api.SdkCloudBackupCreateResponse\")\n\tproto.RegisterType((*SdkCloudBackupRestoreRequest)(nil), \"openstorage.api.SdkCloudBackupRestoreRequest\")\n\tproto.RegisterType((*SdkCloudBackupRestoreResponse)(nil), \"openstorage.api.SdkCloudBackupRestoreResponse\")\n\tproto.RegisterType((*SdkCloudBackupDeleteRequest)(nil), \"openstorage.api.SdkCloudBackupDeleteRequest\")\n\tproto.RegisterType((*SdkCloudBackupDeleteResponse)(nil), \"openstorage.api.SdkCloudBackupDeleteResponse\")\n\tproto.RegisterType((*SdkCloudBackupDeleteAllRequest)(nil), \"openstorage.api.SdkCloudBackupDeleteAllRequest\")\n\tproto.RegisterType((*SdkCloudBackupDeleteAllResponse)(nil), \"openstorage.api.SdkCloudBackupDeleteAllResponse\")\n\tproto.RegisterType((*SdkCloudBackupEnumerateRequest)(nil), \"openstorage.api.SdkCloudBackupEnumerateRequest\")\n\tproto.RegisterType((*SdkCloudBackupInfo)(nil), \"openstorage.api.SdkCloudBackupInfo\")\n\tproto.RegisterMapType((map[string]string)(nil), \"openstorage.api.SdkCloudBackupInfo.MetadataEntry\")\n\tproto.RegisterType((*SdkCloudBackupEnumerateResponse)(nil), \"openstorage.api.SdkCloudBackupEnumerateResponse\")\n\tproto.RegisterType((*SdkCloudBackupStatus)(nil), \"openstorage.api.SdkCloudBackupStatus\")\n\tproto.RegisterType((*SdkCloudBackupStatusRequest)(nil), \"openstorage.api.SdkCloudBackupStatusRequest\")\n\tproto.RegisterType((*SdkCloudBackupStatusResponse)(nil), \"openstorage.api.SdkCloudBackupStatusResponse\")\n\tproto.RegisterMapType((map[string]*SdkCloudBackupStatus)(nil), \"openstorage.api.SdkCloudBackupStatusResponse.StatusesEntry\")\n\tproto.RegisterType((*SdkCloudBackupCatalogRequest)(nil), \"openstorage.api.SdkCloudBackupCatalogRequest\")\n\tproto.RegisterType((*SdkCloudBackupCatalogResponse)(nil), \"openstorage.api.SdkCloudBackupCatalogResponse\")\n\tproto.RegisterType((*SdkCloudBackupHistoryItem)(nil), \"openstorage.api.SdkCloudBackupHistoryItem\")\n\tproto.RegisterType((*SdkCloudBackupHistoryRequest)(nil), \"openstorage.api.SdkCloudBackupHistoryRequest\")\n\tproto.RegisterType((*SdkCloudBackupHistoryResponse)(nil), \"openstorage.api.SdkCloudBackupHistoryResponse\")\n\tproto.RegisterType((*SdkCloudBackupStateChangeRequest)(nil), \"openstorage.api.SdkCloudBackupStateChangeRequest\")\n\tproto.RegisterType((*SdkCloudBackupStateChangeResponse)(nil), \"openstorage.api.SdkCloudBackupStateChangeResponse\")\n\tproto.RegisterEnum(\"openstorage.api.Status\", Status_name, Status_value)\n\tproto.RegisterEnum(\"openstorage.api.DriverType\", DriverType_name, DriverType_value)\n\tproto.RegisterEnum(\"openstorage.api.FSType\", FSType_name, FSType_value)\n\tproto.RegisterEnum(\"openstorage.api.GraphDriverChangeType\", GraphDriverChangeType_name, GraphDriverChangeType_value)\n\tproto.RegisterEnum(\"openstorage.api.SeverityType\", SeverityType_name, SeverityType_value)\n\tproto.RegisterEnum(\"openstorage.api.ResourceType\", ResourceType_name, ResourceType_value)\n\tproto.RegisterEnum(\"openstorage.api.AlertActionType\", AlertActionType_name, AlertActionType_value)\n\tproto.RegisterEnum(\"openstorage.api.VolumeActionParam\", VolumeActionParam_name, VolumeActionParam_value)\n\tproto.RegisterEnum(\"openstorage.api.CosType\", CosType_name, CosType_value)\n\tproto.RegisterEnum(\"openstorage.api.IoProfile\", IoProfile_name, IoProfile_value)\n\tproto.RegisterEnum(\"openstorage.api.VolumeState\", VolumeState_name, VolumeState_value)\n\tproto.RegisterEnum(\"openstorage.api.VolumeStatus\", VolumeStatus_name, VolumeStatus_value)\n\tproto.RegisterEnum(\"openstorage.api.StorageMedium\", StorageMedium_name, StorageMedium_value)\n\tproto.RegisterEnum(\"openstorage.api.ClusterNotify\", ClusterNotify_name, ClusterNotify_value)\n\tproto.RegisterEnum(\"openstorage.api.AttachState\", AttachState_name, AttachState_value)\n\tproto.RegisterEnum(\"openstorage.api.OperationFlags\", OperationFlags_name, OperationFlags_value)\n\tproto.RegisterEnum(\"openstorage.api.SdkCloudBackupOpType\", SdkCloudBackupOpType_name, SdkCloudBackupOpType_value)\n\tproto.RegisterEnum(\"openstorage.api.SdkCloudBackupStatusType\", SdkCloudBackupStatusType_name, SdkCloudBackupStatusType_value)\n\tproto.RegisterEnum(\"openstorage.api.SdkCloudBackupRequestedState\", SdkCloudBackupRequestedState_name, SdkCloudBackupRequestedState_value)\n}\n\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ context.Context\nvar _ grpc.ClientConn\n\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the grpc package it is being compiled against.\nconst _ = grpc.SupportPackageIsVersion4\n\n// OpenStorageClusterClient is the client API for OpenStorageCluster service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageClusterClient interface {\n\t// Enumerate lists all the nodes in the cluster.\n\tEnumerate(ctx context.Context, in *SdkClusterEnumerateRequest, opts ...grpc.CallOption) (*SdkClusterEnumerateResponse, error)\n\t// Inspect the node given a UUID.\n\tInspect(ctx context.Context, in *SdkClusterInspectRequest, opts ...grpc.CallOption) (*SdkClusterInspectResponse, error)\n\t// Get a list of alerts from the storage cluster\n\tAlertEnumerate(ctx context.Context, in *SdkClusterAlertEnumerateRequest, opts ...grpc.CallOption) (*SdkClusterAlertEnumerateResponse, error)\n\t// Clear the alert for a given resource\n\tAlertClear(ctx context.Context, in *SdkClusterAlertClearRequest, opts ...grpc.CallOption) (*SdkClusterAlertClearResponse, error)\n\t// Erases an alert for a given resource\n\tAlertDelete(ctx context.Context, in *SdkClusterAlertDeleteRequest, opts ...grpc.CallOption) (*SdkClusterAlertDeleteResponse, error)\n}\n\ntype openStorageClusterClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageClusterClient(cc *grpc.ClientConn) OpenStorageClusterClient {\n\treturn &openStorageClusterClient{cc}\n}\n\nfunc (c *openStorageClusterClient) Enumerate(ctx context.Context, in *SdkClusterEnumerateRequest, opts ...grpc.CallOption) (*SdkClusterEnumerateResponse, error) {\n\tout := new(SdkClusterEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCluster/Enumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageClusterClient) Inspect(ctx context.Context, in *SdkClusterInspectRequest, opts ...grpc.CallOption) (*SdkClusterInspectResponse, error) {\n\tout := new(SdkClusterInspectResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCluster/Inspect\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageClusterClient) AlertEnumerate(ctx context.Context, in *SdkClusterAlertEnumerateRequest, opts ...grpc.CallOption) (*SdkClusterAlertEnumerateResponse, error) {\n\tout := new(SdkClusterAlertEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCluster/AlertEnumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageClusterClient) AlertClear(ctx context.Context, in *SdkClusterAlertClearRequest, opts ...grpc.CallOption) (*SdkClusterAlertClearResponse, error) {\n\tout := new(SdkClusterAlertClearResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCluster/AlertClear\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageClusterClient) AlertDelete(ctx context.Context, in *SdkClusterAlertDeleteRequest, opts ...grpc.CallOption) (*SdkClusterAlertDeleteResponse, error) {\n\tout := new(SdkClusterAlertDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCluster/AlertDelete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageClusterServer is the server API for OpenStorageCluster service.\ntype OpenStorageClusterServer interface {\n\t// Enumerate lists all the nodes in the cluster.\n\tEnumerate(context.Context, *SdkClusterEnumerateRequest) (*SdkClusterEnumerateResponse, error)\n\t// Inspect the node given a UUID.\n\tInspect(context.Context, *SdkClusterInspectRequest) (*SdkClusterInspectResponse, error)\n\t// Get a list of alerts from the storage cluster\n\tAlertEnumerate(context.Context, *SdkClusterAlertEnumerateRequest) (*SdkClusterAlertEnumerateResponse, error)\n\t// Clear the alert for a given resource\n\tAlertClear(context.Context, *SdkClusterAlertClearRequest) (*SdkClusterAlertClearResponse, error)\n\t// Erases an alert for a given resource\n\tAlertDelete(context.Context, *SdkClusterAlertDeleteRequest) (*SdkClusterAlertDeleteResponse, error)\n}\n\nfunc RegisterOpenStorageClusterServer(s *grpc.Server, srv OpenStorageClusterServer) {\n\ts.RegisterService(&_OpenStorageCluster_serviceDesc, srv)\n}\n\nfunc _OpenStorageCluster_Enumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkClusterEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageClusterServer).Enumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCluster/Enumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageClusterServer).Enumerate(ctx, req.(*SdkClusterEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCluster_Inspect_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkClusterInspectRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageClusterServer).Inspect(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCluster/Inspect\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageClusterServer).Inspect(ctx, req.(*SdkClusterInspectRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCluster_AlertEnumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkClusterAlertEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageClusterServer).AlertEnumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCluster/AlertEnumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageClusterServer).AlertEnumerate(ctx, req.(*SdkClusterAlertEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCluster_AlertClear_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkClusterAlertClearRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageClusterServer).AlertClear(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCluster/AlertClear\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageClusterServer).AlertClear(ctx, req.(*SdkClusterAlertClearRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCluster_AlertDelete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkClusterAlertDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageClusterServer).AlertDelete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCluster/AlertDelete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageClusterServer).AlertDelete(ctx, req.(*SdkClusterAlertDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageCluster_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageCluster\",\n\tHandlerType: (*OpenStorageClusterServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Enumerate\",\n\t\t\tHandler:    _OpenStorageCluster_Enumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Inspect\",\n\t\t\tHandler:    _OpenStorageCluster_Inspect_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"AlertEnumerate\",\n\t\t\tHandler:    _OpenStorageCluster_AlertEnumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"AlertClear\",\n\t\t\tHandler:    _OpenStorageCluster_AlertClear_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"AlertDelete\",\n\t\t\tHandler:    _OpenStorageCluster_AlertDelete_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\n// OpenStorageVolumeClient is the client API for OpenStorageVolume service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageVolumeClient interface {\n\t// Creates a new volume\n\tCreate(ctx context.Context, in *SdkVolumeCreateRequest, opts ...grpc.CallOption) (*SdkVolumeCreateResponse, error)\n\t// Clone creates a new volume cloned from an existing volume\n\tClone(ctx context.Context, in *SdkVolumeCloneRequest, opts ...grpc.CallOption) (*SdkVolumeCloneResponse, error)\n\t// Delete a volume\n\tDelete(ctx context.Context, in *SdkVolumeDeleteRequest, opts ...grpc.CallOption) (*SdkVolumeDeleteResponse, error)\n\t// Get information on a volume\n\tInspect(ctx context.Context, in *SdkVolumeInspectRequest, opts ...grpc.CallOption) (*SdkVolumeInspectResponse, error)\n\t// Get a list of volumes\n\tEnumerate(ctx context.Context, in *SdkVolumeEnumerateRequest, opts ...grpc.CallOption) (*SdkVolumeEnumerateResponse, error)\n\t// Create a snapshot of a volume. This creates an immutable (read-only),\n\t// point-in-time snapshot of a volume.\n\tSnapshotCreate(ctx context.Context, in *SdkVolumeSnapshotCreateRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotCreateResponse, error)\n\t// Restores a volume to a specified snapshot\n\tSnapshotRestore(ctx context.Context, in *SdkVolumeSnapshotRestoreRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotRestoreResponse, error)\n\t// List the number of snapshots for a specific volume\n\tSnapshotEnumerate(ctx context.Context, in *SdkVolumeSnapshotEnumerateRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotEnumerateResponse, error)\n\t// Attach device to host\n\tAttach(ctx context.Context, in *SdkVolumeAttachRequest, opts ...grpc.CallOption) (*SdkVolumeAttachResponse, error)\n\t// Detaches the volume from the node.\n\tDetach(ctx context.Context, in *SdkVolumeDetachRequest, opts ...grpc.CallOption) (*SdkVolumeDetachResponse, error)\n\t// Attaches the volume to a node.\n\tMount(ctx context.Context, in *SdkVolumeMountRequest, opts ...grpc.CallOption) (*SdkVolumeMountResponse, error)\n\t// Unmount volume at specified path\n\tUnmount(ctx context.Context, in *SdkVolumeUnmountRequest, opts ...grpc.CallOption) (*SdkVolumeUnmountResponse, error)\n}\n\ntype openStorageVolumeClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageVolumeClient(cc *grpc.ClientConn) OpenStorageVolumeClient {\n\treturn &openStorageVolumeClient{cc}\n}\n\nfunc (c *openStorageVolumeClient) Create(ctx context.Context, in *SdkVolumeCreateRequest, opts ...grpc.CallOption) (*SdkVolumeCreateResponse, error) {\n\tout := new(SdkVolumeCreateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Create\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Clone(ctx context.Context, in *SdkVolumeCloneRequest, opts ...grpc.CallOption) (*SdkVolumeCloneResponse, error) {\n\tout := new(SdkVolumeCloneResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Clone\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Delete(ctx context.Context, in *SdkVolumeDeleteRequest, opts ...grpc.CallOption) (*SdkVolumeDeleteResponse, error) {\n\tout := new(SdkVolumeDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Delete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Inspect(ctx context.Context, in *SdkVolumeInspectRequest, opts ...grpc.CallOption) (*SdkVolumeInspectResponse, error) {\n\tout := new(SdkVolumeInspectResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Inspect\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Enumerate(ctx context.Context, in *SdkVolumeEnumerateRequest, opts ...grpc.CallOption) (*SdkVolumeEnumerateResponse, error) {\n\tout := new(SdkVolumeEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Enumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) SnapshotCreate(ctx context.Context, in *SdkVolumeSnapshotCreateRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotCreateResponse, error) {\n\tout := new(SdkVolumeSnapshotCreateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/SnapshotCreate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) SnapshotRestore(ctx context.Context, in *SdkVolumeSnapshotRestoreRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotRestoreResponse, error) {\n\tout := new(SdkVolumeSnapshotRestoreResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/SnapshotRestore\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) SnapshotEnumerate(ctx context.Context, in *SdkVolumeSnapshotEnumerateRequest, opts ...grpc.CallOption) (*SdkVolumeSnapshotEnumerateResponse, error) {\n\tout := new(SdkVolumeSnapshotEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/SnapshotEnumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Attach(ctx context.Context, in *SdkVolumeAttachRequest, opts ...grpc.CallOption) (*SdkVolumeAttachResponse, error) {\n\tout := new(SdkVolumeAttachResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Attach\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Detach(ctx context.Context, in *SdkVolumeDetachRequest, opts ...grpc.CallOption) (*SdkVolumeDetachResponse, error) {\n\tout := new(SdkVolumeDetachResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Detach\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Mount(ctx context.Context, in *SdkVolumeMountRequest, opts ...grpc.CallOption) (*SdkVolumeMountResponse, error) {\n\tout := new(SdkVolumeMountResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Mount\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageVolumeClient) Unmount(ctx context.Context, in *SdkVolumeUnmountRequest, opts ...grpc.CallOption) (*SdkVolumeUnmountResponse, error) {\n\tout := new(SdkVolumeUnmountResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageVolume/Unmount\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageVolumeServer is the server API for OpenStorageVolume service.\ntype OpenStorageVolumeServer interface {\n\t// Creates a new volume\n\tCreate(context.Context, *SdkVolumeCreateRequest) (*SdkVolumeCreateResponse, error)\n\t// Clone creates a new volume cloned from an existing volume\n\tClone(context.Context, *SdkVolumeCloneRequest) (*SdkVolumeCloneResponse, error)\n\t// Delete a volume\n\tDelete(context.Context, *SdkVolumeDeleteRequest) (*SdkVolumeDeleteResponse, error)\n\t// Get information on a volume\n\tInspect(context.Context, *SdkVolumeInspectRequest) (*SdkVolumeInspectResponse, error)\n\t// Get a list of volumes\n\tEnumerate(context.Context, *SdkVolumeEnumerateRequest) (*SdkVolumeEnumerateResponse, error)\n\t// Create a snapshot of a volume. This creates an immutable (read-only),\n\t// point-in-time snapshot of a volume.\n\tSnapshotCreate(context.Context, *SdkVolumeSnapshotCreateRequest) (*SdkVolumeSnapshotCreateResponse, error)\n\t// Restores a volume to a specified snapshot\n\tSnapshotRestore(context.Context, *SdkVolumeSnapshotRestoreRequest) (*SdkVolumeSnapshotRestoreResponse, error)\n\t// List the number of snapshots for a specific volume\n\tSnapshotEnumerate(context.Context, *SdkVolumeSnapshotEnumerateRequest) (*SdkVolumeSnapshotEnumerateResponse, error)\n\t// Attach device to host\n\tAttach(context.Context, *SdkVolumeAttachRequest) (*SdkVolumeAttachResponse, error)\n\t// Detaches the volume from the node.\n\tDetach(context.Context, *SdkVolumeDetachRequest) (*SdkVolumeDetachResponse, error)\n\t// Attaches the volume to a node.\n\tMount(context.Context, *SdkVolumeMountRequest) (*SdkVolumeMountResponse, error)\n\t// Unmount volume at specified path\n\tUnmount(context.Context, *SdkVolumeUnmountRequest) (*SdkVolumeUnmountResponse, error)\n}\n\nfunc RegisterOpenStorageVolumeServer(s *grpc.Server, srv OpenStorageVolumeServer) {\n\ts.RegisterService(&_OpenStorageVolume_serviceDesc, srv)\n}\n\nfunc _OpenStorageVolume_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeCreateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Create(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Create\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Create(ctx, req.(*SdkVolumeCreateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Clone_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeCloneRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Clone(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Clone\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Clone(ctx, req.(*SdkVolumeCloneRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Delete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Delete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Delete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Delete(ctx, req.(*SdkVolumeDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Inspect_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeInspectRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Inspect(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Inspect\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Inspect(ctx, req.(*SdkVolumeInspectRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Enumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Enumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Enumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Enumerate(ctx, req.(*SdkVolumeEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_SnapshotCreate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeSnapshotCreateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotCreate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/SnapshotCreate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotCreate(ctx, req.(*SdkVolumeSnapshotCreateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_SnapshotRestore_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeSnapshotRestoreRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotRestore(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/SnapshotRestore\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotRestore(ctx, req.(*SdkVolumeSnapshotRestoreRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_SnapshotEnumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeSnapshotEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotEnumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/SnapshotEnumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).SnapshotEnumerate(ctx, req.(*SdkVolumeSnapshotEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Attach_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeAttachRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Attach(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Attach\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Attach(ctx, req.(*SdkVolumeAttachRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Detach_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeDetachRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Detach(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Detach\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Detach(ctx, req.(*SdkVolumeDetachRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Mount_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeMountRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Mount(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Mount\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Mount(ctx, req.(*SdkVolumeMountRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageVolume_Unmount_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkVolumeUnmountRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageVolumeServer).Unmount(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageVolume/Unmount\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageVolumeServer).Unmount(ctx, req.(*SdkVolumeUnmountRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageVolume_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageVolume\",\n\tHandlerType: (*OpenStorageVolumeServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Create\",\n\t\t\tHandler:    _OpenStorageVolume_Create_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Clone\",\n\t\t\tHandler:    _OpenStorageVolume_Clone_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Delete\",\n\t\t\tHandler:    _OpenStorageVolume_Delete_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Inspect\",\n\t\t\tHandler:    _OpenStorageVolume_Inspect_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Enumerate\",\n\t\t\tHandler:    _OpenStorageVolume_Enumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"SnapshotCreate\",\n\t\t\tHandler:    _OpenStorageVolume_SnapshotCreate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"SnapshotRestore\",\n\t\t\tHandler:    _OpenStorageVolume_SnapshotRestore_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"SnapshotEnumerate\",\n\t\t\tHandler:    _OpenStorageVolume_SnapshotEnumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Attach\",\n\t\t\tHandler:    _OpenStorageVolume_Attach_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Detach\",\n\t\t\tHandler:    _OpenStorageVolume_Detach_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Mount\",\n\t\t\tHandler:    _OpenStorageVolume_Mount_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Unmount\",\n\t\t\tHandler:    _OpenStorageVolume_Unmount_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\n// OpenStorageObjectstoreClient is the client API for OpenStorageObjectstore service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageObjectstoreClient interface {\n\t// Inspect returns current status of objectstore\n\tInspect(ctx context.Context, in *SdkObjectstoreInspectRequest, opts ...grpc.CallOption) (*SdkObjectstoreInspectResponse, error)\n\t// Creates objectstore on specified volume\n\tCreate(ctx context.Context, in *SdkObjectstoreCreateRequest, opts ...grpc.CallOption) (*SdkObjectstoreCreateResponse, error)\n\t// Deletes objectstore by id\n\tDelete(ctx context.Context, in *SdkObjectstoreDeleteRequest, opts ...grpc.CallOption) (*SdkObjectstoreDeleteResponse, error)\n\t// Updates provided objectstore status\n\tUpdate(ctx context.Context, in *SdkObjectstoreUpdateRequest, opts ...grpc.CallOption) (*SdkObjectstoreUpdateResponse, error)\n}\n\ntype openStorageObjectstoreClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageObjectstoreClient(cc *grpc.ClientConn) OpenStorageObjectstoreClient {\n\treturn &openStorageObjectstoreClient{cc}\n}\n\nfunc (c *openStorageObjectstoreClient) Inspect(ctx context.Context, in *SdkObjectstoreInspectRequest, opts ...grpc.CallOption) (*SdkObjectstoreInspectResponse, error) {\n\tout := new(SdkObjectstoreInspectResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageObjectstore/Inspect\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageObjectstoreClient) Create(ctx context.Context, in *SdkObjectstoreCreateRequest, opts ...grpc.CallOption) (*SdkObjectstoreCreateResponse, error) {\n\tout := new(SdkObjectstoreCreateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageObjectstore/Create\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageObjectstoreClient) Delete(ctx context.Context, in *SdkObjectstoreDeleteRequest, opts ...grpc.CallOption) (*SdkObjectstoreDeleteResponse, error) {\n\tout := new(SdkObjectstoreDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageObjectstore/Delete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageObjectstoreClient) Update(ctx context.Context, in *SdkObjectstoreUpdateRequest, opts ...grpc.CallOption) (*SdkObjectstoreUpdateResponse, error) {\n\tout := new(SdkObjectstoreUpdateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageObjectstore/Update\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageObjectstoreServer is the server API for OpenStorageObjectstore service.\ntype OpenStorageObjectstoreServer interface {\n\t// Inspect returns current status of objectstore\n\tInspect(context.Context, *SdkObjectstoreInspectRequest) (*SdkObjectstoreInspectResponse, error)\n\t// Creates objectstore on specified volume\n\tCreate(context.Context, *SdkObjectstoreCreateRequest) (*SdkObjectstoreCreateResponse, error)\n\t// Deletes objectstore by id\n\tDelete(context.Context, *SdkObjectstoreDeleteRequest) (*SdkObjectstoreDeleteResponse, error)\n\t// Updates provided objectstore status\n\tUpdate(context.Context, *SdkObjectstoreUpdateRequest) (*SdkObjectstoreUpdateResponse, error)\n}\n\nfunc RegisterOpenStorageObjectstoreServer(s *grpc.Server, srv OpenStorageObjectstoreServer) {\n\ts.RegisterService(&_OpenStorageObjectstore_serviceDesc, srv)\n}\n\nfunc _OpenStorageObjectstore_Inspect_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkObjectstoreInspectRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageObjectstoreServer).Inspect(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageObjectstore/Inspect\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageObjectstoreServer).Inspect(ctx, req.(*SdkObjectstoreInspectRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageObjectstore_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkObjectstoreCreateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageObjectstoreServer).Create(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageObjectstore/Create\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageObjectstoreServer).Create(ctx, req.(*SdkObjectstoreCreateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageObjectstore_Delete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkObjectstoreDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageObjectstoreServer).Delete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageObjectstore/Delete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageObjectstoreServer).Delete(ctx, req.(*SdkObjectstoreDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageObjectstore_Update_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkObjectstoreUpdateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageObjectstoreServer).Update(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageObjectstore/Update\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageObjectstoreServer).Update(ctx, req.(*SdkObjectstoreUpdateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageObjectstore_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageObjectstore\",\n\tHandlerType: (*OpenStorageObjectstoreServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Inspect\",\n\t\t\tHandler:    _OpenStorageObjectstore_Inspect_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Create\",\n\t\t\tHandler:    _OpenStorageObjectstore_Create_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Delete\",\n\t\t\tHandler:    _OpenStorageObjectstore_Delete_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Update\",\n\t\t\tHandler:    _OpenStorageObjectstore_Update_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\n// OpenStorageCredentialsClient is the client API for OpenStorageCredentials service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageCredentialsClient interface {\n\t// Create credential for AWS S3 and if valid ,\n\t// returns a unique identifier\n\tCreateForAWS(ctx context.Context, in *SdkCredentialCreateAWSRequest, opts ...grpc.CallOption) (*SdkCredentialCreateAWSResponse, error)\n\t// Create credential for Azure and if valid ,\n\t// returns a unique identifier\n\tCreateForAzure(ctx context.Context, in *SdkCredentialCreateAzureRequest, opts ...grpc.CallOption) (*SdkCredentialCreateAzureResponse, error)\n\t// Create credential for Google and if valid ,\n\t// returns a unique identifier\n\tCreateForGoogle(ctx context.Context, in *SdkCredentialCreateGoogleRequest, opts ...grpc.CallOption) (*SdkCredentialCreateGoogleResponse, error)\n\t// EnumerateForAWS lists the configured AWS credentials\n\tEnumerateForAWS(ctx context.Context, in *SdkCredentialEnumerateAWSRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateAWSResponse, error)\n\t// EnumerateForAzure lists the configured Azure credentials\n\tEnumerateForAzure(ctx context.Context, in *SdkCredentialEnumerateAzureRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateAzureResponse, error)\n\t// EnumerateForGoogle lists the configured Google credentials\n\tEnumerateForGoogle(ctx context.Context, in *SdkCredentialEnumerateGoogleRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateGoogleResponse, error)\n\t// Delete a specified credential\n\tDelete(ctx context.Context, in *SdkCredentialDeleteRequest, opts ...grpc.CallOption) (*SdkCredentialDeleteResponse, error)\n\t// Validate a specified credential\n\tValidate(ctx context.Context, in *SdkCredentialValidateRequest, opts ...grpc.CallOption) (*SdkCredentialValidateResponse, error)\n}\n\ntype openStorageCredentialsClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageCredentialsClient(cc *grpc.ClientConn) OpenStorageCredentialsClient {\n\treturn &openStorageCredentialsClient{cc}\n}\n\nfunc (c *openStorageCredentialsClient) CreateForAWS(ctx context.Context, in *SdkCredentialCreateAWSRequest, opts ...grpc.CallOption) (*SdkCredentialCreateAWSResponse, error) {\n\tout := new(SdkCredentialCreateAWSResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/CreateForAWS\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) CreateForAzure(ctx context.Context, in *SdkCredentialCreateAzureRequest, opts ...grpc.CallOption) (*SdkCredentialCreateAzureResponse, error) {\n\tout := new(SdkCredentialCreateAzureResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/CreateForAzure\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) CreateForGoogle(ctx context.Context, in *SdkCredentialCreateGoogleRequest, opts ...grpc.CallOption) (*SdkCredentialCreateGoogleResponse, error) {\n\tout := new(SdkCredentialCreateGoogleResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/CreateForGoogle\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) EnumerateForAWS(ctx context.Context, in *SdkCredentialEnumerateAWSRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateAWSResponse, error) {\n\tout := new(SdkCredentialEnumerateAWSResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/EnumerateForAWS\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) EnumerateForAzure(ctx context.Context, in *SdkCredentialEnumerateAzureRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateAzureResponse, error) {\n\tout := new(SdkCredentialEnumerateAzureResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/EnumerateForAzure\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) EnumerateForGoogle(ctx context.Context, in *SdkCredentialEnumerateGoogleRequest, opts ...grpc.CallOption) (*SdkCredentialEnumerateGoogleResponse, error) {\n\tout := new(SdkCredentialEnumerateGoogleResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/EnumerateForGoogle\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) Delete(ctx context.Context, in *SdkCredentialDeleteRequest, opts ...grpc.CallOption) (*SdkCredentialDeleteResponse, error) {\n\tout := new(SdkCredentialDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/Delete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCredentialsClient) Validate(ctx context.Context, in *SdkCredentialValidateRequest, opts ...grpc.CallOption) (*SdkCredentialValidateResponse, error) {\n\tout := new(SdkCredentialValidateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCredentials/Validate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageCredentialsServer is the server API for OpenStorageCredentials service.\ntype OpenStorageCredentialsServer interface {\n\t// Create credential for AWS S3 and if valid ,\n\t// returns a unique identifier\n\tCreateForAWS(context.Context, *SdkCredentialCreateAWSRequest) (*SdkCredentialCreateAWSResponse, error)\n\t// Create credential for Azure and if valid ,\n\t// returns a unique identifier\n\tCreateForAzure(context.Context, *SdkCredentialCreateAzureRequest) (*SdkCredentialCreateAzureResponse, error)\n\t// Create credential for Google and if valid ,\n\t// returns a unique identifier\n\tCreateForGoogle(context.Context, *SdkCredentialCreateGoogleRequest) (*SdkCredentialCreateGoogleResponse, error)\n\t// EnumerateForAWS lists the configured AWS credentials\n\tEnumerateForAWS(context.Context, *SdkCredentialEnumerateAWSRequest) (*SdkCredentialEnumerateAWSResponse, error)\n\t// EnumerateForAzure lists the configured Azure credentials\n\tEnumerateForAzure(context.Context, *SdkCredentialEnumerateAzureRequest) (*SdkCredentialEnumerateAzureResponse, error)\n\t// EnumerateForGoogle lists the configured Google credentials\n\tEnumerateForGoogle(context.Context, *SdkCredentialEnumerateGoogleRequest) (*SdkCredentialEnumerateGoogleResponse, error)\n\t// Delete a specified credential\n\tDelete(context.Context, *SdkCredentialDeleteRequest) (*SdkCredentialDeleteResponse, error)\n\t// Validate a specified credential\n\tValidate(context.Context, *SdkCredentialValidateRequest) (*SdkCredentialValidateResponse, error)\n}\n\nfunc RegisterOpenStorageCredentialsServer(s *grpc.Server, srv OpenStorageCredentialsServer) {\n\ts.RegisterService(&_OpenStorageCredentials_serviceDesc, srv)\n}\n\nfunc _OpenStorageCredentials_CreateForAWS_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialCreateAWSRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForAWS(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/CreateForAWS\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForAWS(ctx, req.(*SdkCredentialCreateAWSRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_CreateForAzure_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialCreateAzureRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForAzure(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/CreateForAzure\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForAzure(ctx, req.(*SdkCredentialCreateAzureRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_CreateForGoogle_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialCreateGoogleRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForGoogle(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/CreateForGoogle\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).CreateForGoogle(ctx, req.(*SdkCredentialCreateGoogleRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_EnumerateForAWS_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialEnumerateAWSRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForAWS(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/EnumerateForAWS\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForAWS(ctx, req.(*SdkCredentialEnumerateAWSRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_EnumerateForAzure_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialEnumerateAzureRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForAzure(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/EnumerateForAzure\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForAzure(ctx, req.(*SdkCredentialEnumerateAzureRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_EnumerateForGoogle_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialEnumerateGoogleRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForGoogle(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/EnumerateForGoogle\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).EnumerateForGoogle(ctx, req.(*SdkCredentialEnumerateGoogleRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_Delete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).Delete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/Delete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).Delete(ctx, req.(*SdkCredentialDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCredentials_Validate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCredentialValidateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCredentialsServer).Validate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCredentials/Validate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCredentialsServer).Validate(ctx, req.(*SdkCredentialValidateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageCredentials_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageCredentials\",\n\tHandlerType: (*OpenStorageCredentialsServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"CreateForAWS\",\n\t\t\tHandler:    _OpenStorageCredentials_CreateForAWS_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"CreateForAzure\",\n\t\t\tHandler:    _OpenStorageCredentials_CreateForAzure_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"CreateForGoogle\",\n\t\t\tHandler:    _OpenStorageCredentials_CreateForGoogle_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"EnumerateForAWS\",\n\t\t\tHandler:    _OpenStorageCredentials_EnumerateForAWS_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"EnumerateForAzure\",\n\t\t\tHandler:    _OpenStorageCredentials_EnumerateForAzure_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"EnumerateForGoogle\",\n\t\t\tHandler:    _OpenStorageCredentials_EnumerateForGoogle_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Delete\",\n\t\t\tHandler:    _OpenStorageCredentials_Delete_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Validate\",\n\t\t\tHandler:    _OpenStorageCredentials_Validate_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\n// OpenStorageSchedulePolicyClient is the client API for OpenStorageSchedulePolicy service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageSchedulePolicyClient interface {\n\t// Create Schedule Policy for snapshots\n\tCreate(ctx context.Context, in *SdkSchedulePolicyCreateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyCreateResponse, error)\n\t// Update Schedule Policy\n\tUpdate(ctx context.Context, in *SdkSchedulePolicyUpdateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyUpdateResponse, error)\n\tEnumerate(ctx context.Context, in *SdkSchedulePolicyEnumerateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyEnumerateResponse, error)\n\t// Inspect Schedule Policy\n\tInspect(ctx context.Context, in *SdkSchedulePolicyInspectRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyInspectResponse, error)\n\t// Delete Schedule Policy\n\tDelete(ctx context.Context, in *SdkSchedulePolicyDeleteRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyDeleteResponse, error)\n}\n\ntype openStorageSchedulePolicyClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageSchedulePolicyClient(cc *grpc.ClientConn) OpenStorageSchedulePolicyClient {\n\treturn &openStorageSchedulePolicyClient{cc}\n}\n\nfunc (c *openStorageSchedulePolicyClient) Create(ctx context.Context, in *SdkSchedulePolicyCreateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyCreateResponse, error) {\n\tout := new(SdkSchedulePolicyCreateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageSchedulePolicy/Create\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageSchedulePolicyClient) Update(ctx context.Context, in *SdkSchedulePolicyUpdateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyUpdateResponse, error) {\n\tout := new(SdkSchedulePolicyUpdateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageSchedulePolicy/Update\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageSchedulePolicyClient) Enumerate(ctx context.Context, in *SdkSchedulePolicyEnumerateRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyEnumerateResponse, error) {\n\tout := new(SdkSchedulePolicyEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageSchedulePolicy/Enumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageSchedulePolicyClient) Inspect(ctx context.Context, in *SdkSchedulePolicyInspectRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyInspectResponse, error) {\n\tout := new(SdkSchedulePolicyInspectResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageSchedulePolicy/Inspect\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageSchedulePolicyClient) Delete(ctx context.Context, in *SdkSchedulePolicyDeleteRequest, opts ...grpc.CallOption) (*SdkSchedulePolicyDeleteResponse, error) {\n\tout := new(SdkSchedulePolicyDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageSchedulePolicy/Delete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageSchedulePolicyServer is the server API for OpenStorageSchedulePolicy service.\ntype OpenStorageSchedulePolicyServer interface {\n\t// Create Schedule Policy for snapshots\n\tCreate(context.Context, *SdkSchedulePolicyCreateRequest) (*SdkSchedulePolicyCreateResponse, error)\n\t// Update Schedule Policy\n\tUpdate(context.Context, *SdkSchedulePolicyUpdateRequest) (*SdkSchedulePolicyUpdateResponse, error)\n\tEnumerate(context.Context, *SdkSchedulePolicyEnumerateRequest) (*SdkSchedulePolicyEnumerateResponse, error)\n\t// Inspect Schedule Policy\n\tInspect(context.Context, *SdkSchedulePolicyInspectRequest) (*SdkSchedulePolicyInspectResponse, error)\n\t// Delete Schedule Policy\n\tDelete(context.Context, *SdkSchedulePolicyDeleteRequest) (*SdkSchedulePolicyDeleteResponse, error)\n}\n\nfunc RegisterOpenStorageSchedulePolicyServer(s *grpc.Server, srv OpenStorageSchedulePolicyServer) {\n\ts.RegisterService(&_OpenStorageSchedulePolicy_serviceDesc, srv)\n}\n\nfunc _OpenStorageSchedulePolicy_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkSchedulePolicyCreateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Create(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageSchedulePolicy/Create\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Create(ctx, req.(*SdkSchedulePolicyCreateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageSchedulePolicy_Update_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkSchedulePolicyUpdateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Update(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageSchedulePolicy/Update\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Update(ctx, req.(*SdkSchedulePolicyUpdateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageSchedulePolicy_Enumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkSchedulePolicyEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Enumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageSchedulePolicy/Enumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Enumerate(ctx, req.(*SdkSchedulePolicyEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageSchedulePolicy_Inspect_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkSchedulePolicyInspectRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Inspect(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageSchedulePolicy/Inspect\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Inspect(ctx, req.(*SdkSchedulePolicyInspectRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageSchedulePolicy_Delete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkSchedulePolicyDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Delete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageSchedulePolicy/Delete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageSchedulePolicyServer).Delete(ctx, req.(*SdkSchedulePolicyDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageSchedulePolicy_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageSchedulePolicy\",\n\tHandlerType: (*OpenStorageSchedulePolicyServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Create\",\n\t\t\tHandler:    _OpenStorageSchedulePolicy_Create_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Update\",\n\t\t\tHandler:    _OpenStorageSchedulePolicy_Update_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Enumerate\",\n\t\t\tHandler:    _OpenStorageSchedulePolicy_Enumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Inspect\",\n\t\t\tHandler:    _OpenStorageSchedulePolicy_Inspect_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Delete\",\n\t\t\tHandler:    _OpenStorageSchedulePolicy_Delete_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\n// OpenStorageCloudBackupClient is the client API for OpenStorageCloudBackup service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype OpenStorageCloudBackupClient interface {\n\t// Create\n\tCreate(ctx context.Context, in *SdkCloudBackupCreateRequest, opts ...grpc.CallOption) (*SdkCloudBackupCreateResponse, error)\n\t// Restore\n\tRestore(ctx context.Context, in *SdkCloudBackupRestoreRequest, opts ...grpc.CallOption) (*SdkCloudBackupRestoreResponse, error)\n\t// Delete\n\tDelete(ctx context.Context, in *SdkCloudBackupDeleteRequest, opts ...grpc.CallOption) (*SdkCloudBackupDeleteResponse, error)\n\t// DeleteAll\n\tDeleteAll(ctx context.Context, in *SdkCloudBackupDeleteAllRequest, opts ...grpc.CallOption) (*SdkCloudBackupDeleteAllResponse, error)\n\t// Enumerate\n\tEnumerate(ctx context.Context, in *SdkCloudBackupEnumerateRequest, opts ...grpc.CallOption) (*SdkCloudBackupEnumerateResponse, error)\n\t// Status\n\tStatus(ctx context.Context, in *SdkCloudBackupStatusRequest, opts ...grpc.CallOption) (*SdkCloudBackupStatusResponse, error)\n\t// Catalog\n\tCatalog(ctx context.Context, in *SdkCloudBackupCatalogRequest, opts ...grpc.CallOption) (*SdkCloudBackupCatalogResponse, error)\n\t// History\n\tHistory(ctx context.Context, in *SdkCloudBackupHistoryRequest, opts ...grpc.CallOption) (*SdkCloudBackupHistoryResponse, error)\n\t// StateChange\n\tStateChange(ctx context.Context, in *SdkCloudBackupStateChangeRequest, opts ...grpc.CallOption) (*SdkCloudBackupStateChangeResponse, error)\n}\n\ntype openStorageCloudBackupClient struct {\n\tcc *grpc.ClientConn\n}\n\nfunc NewOpenStorageCloudBackupClient(cc *grpc.ClientConn) OpenStorageCloudBackupClient {\n\treturn &openStorageCloudBackupClient{cc}\n}\n\nfunc (c *openStorageCloudBackupClient) Create(ctx context.Context, in *SdkCloudBackupCreateRequest, opts ...grpc.CallOption) (*SdkCloudBackupCreateResponse, error) {\n\tout := new(SdkCloudBackupCreateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Create\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) Restore(ctx context.Context, in *SdkCloudBackupRestoreRequest, opts ...grpc.CallOption) (*SdkCloudBackupRestoreResponse, error) {\n\tout := new(SdkCloudBackupRestoreResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Restore\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) Delete(ctx context.Context, in *SdkCloudBackupDeleteRequest, opts ...grpc.CallOption) (*SdkCloudBackupDeleteResponse, error) {\n\tout := new(SdkCloudBackupDeleteResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Delete\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) DeleteAll(ctx context.Context, in *SdkCloudBackupDeleteAllRequest, opts ...grpc.CallOption) (*SdkCloudBackupDeleteAllResponse, error) {\n\tout := new(SdkCloudBackupDeleteAllResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/DeleteAll\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) Enumerate(ctx context.Context, in *SdkCloudBackupEnumerateRequest, opts ...grpc.CallOption) (*SdkCloudBackupEnumerateResponse, error) {\n\tout := new(SdkCloudBackupEnumerateResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Enumerate\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) Status(ctx context.Context, in *SdkCloudBackupStatusRequest, opts ...grpc.CallOption) (*SdkCloudBackupStatusResponse, error) {\n\tout := new(SdkCloudBackupStatusResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Status\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) Catalog(ctx context.Context, in *SdkCloudBackupCatalogRequest, opts ...grpc.CallOption) (*SdkCloudBackupCatalogResponse, error) {\n\tout := new(SdkCloudBackupCatalogResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/Catalog\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) History(ctx context.Context, in *SdkCloudBackupHistoryRequest, opts ...grpc.CallOption) (*SdkCloudBackupHistoryResponse, error) {\n\tout := new(SdkCloudBackupHistoryResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/History\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\nfunc (c *openStorageCloudBackupClient) StateChange(ctx context.Context, in *SdkCloudBackupStateChangeRequest, opts ...grpc.CallOption) (*SdkCloudBackupStateChangeResponse, error) {\n\tout := new(SdkCloudBackupStateChangeResponse)\n\terr := c.cc.Invoke(ctx, \"/openstorage.api.OpenStorageCloudBackup/StateChange\", in, out, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn out, nil\n}\n\n// OpenStorageCloudBackupServer is the server API for OpenStorageCloudBackup service.\ntype OpenStorageCloudBackupServer interface {\n\t// Create\n\tCreate(context.Context, *SdkCloudBackupCreateRequest) (*SdkCloudBackupCreateResponse, error)\n\t// Restore\n\tRestore(context.Context, *SdkCloudBackupRestoreRequest) (*SdkCloudBackupRestoreResponse, error)\n\t// Delete\n\tDelete(context.Context, *SdkCloudBackupDeleteRequest) (*SdkCloudBackupDeleteResponse, error)\n\t// DeleteAll\n\tDeleteAll(context.Context, *SdkCloudBackupDeleteAllRequest) (*SdkCloudBackupDeleteAllResponse, error)\n\t// Enumerate\n\tEnumerate(context.Context, *SdkCloudBackupEnumerateRequest) (*SdkCloudBackupEnumerateResponse, error)\n\t// Status\n\tStatus(context.Context, *SdkCloudBackupStatusRequest) (*SdkCloudBackupStatusResponse, error)\n\t// Catalog\n\tCatalog(context.Context, *SdkCloudBackupCatalogRequest) (*SdkCloudBackupCatalogResponse, error)\n\t// History\n\tHistory(context.Context, *SdkCloudBackupHistoryRequest) (*SdkCloudBackupHistoryResponse, error)\n\t// StateChange\n\tStateChange(context.Context, *SdkCloudBackupStateChangeRequest) (*SdkCloudBackupStateChangeResponse, error)\n}\n\nfunc RegisterOpenStorageCloudBackupServer(s *grpc.Server, srv OpenStorageCloudBackupServer) {\n\ts.RegisterService(&_OpenStorageCloudBackup_serviceDesc, srv)\n}\n\nfunc _OpenStorageCloudBackup_Create_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupCreateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Create(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Create\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Create(ctx, req.(*SdkCloudBackupCreateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_Restore_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupRestoreRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Restore(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Restore\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Restore(ctx, req.(*SdkCloudBackupRestoreRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_Delete_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupDeleteRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Delete(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Delete\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Delete(ctx, req.(*SdkCloudBackupDeleteRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_DeleteAll_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupDeleteAllRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).DeleteAll(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/DeleteAll\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).DeleteAll(ctx, req.(*SdkCloudBackupDeleteAllRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_Enumerate_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupEnumerateRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Enumerate(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Enumerate\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Enumerate(ctx, req.(*SdkCloudBackupEnumerateRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_Status_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupStatusRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Status(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Status\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Status(ctx, req.(*SdkCloudBackupStatusRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_Catalog_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupCatalogRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).Catalog(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/Catalog\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).Catalog(ctx, req.(*SdkCloudBackupCatalogRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_History_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupHistoryRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).History(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/History\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).History(ctx, req.(*SdkCloudBackupHistoryRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nfunc _OpenStorageCloudBackup_StateChange_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n\tin := new(SdkCloudBackupStateChangeRequest)\n\tif err := dec(in); err != nil {\n\t\treturn nil, err\n\t}\n\tif interceptor == nil {\n\t\treturn srv.(OpenStorageCloudBackupServer).StateChange(ctx, in)\n\t}\n\tinfo := &grpc.UnaryServerInfo{\n\t\tServer:     srv,\n\t\tFullMethod: \"/openstorage.api.OpenStorageCloudBackup/StateChange\",\n\t}\n\thandler := func(ctx context.Context, req interface{}) (interface{}, error) {\n\t\treturn srv.(OpenStorageCloudBackupServer).StateChange(ctx, req.(*SdkCloudBackupStateChangeRequest))\n\t}\n\treturn interceptor(ctx, in, info, handler)\n}\n\nvar _OpenStorageCloudBackup_serviceDesc = grpc.ServiceDesc{\n\tServiceName: \"openstorage.api.OpenStorageCloudBackup\",\n\tHandlerType: (*OpenStorageCloudBackupServer)(nil),\n\tMethods: []grpc.MethodDesc{\n\t\t{\n\t\t\tMethodName: \"Create\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Create_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Restore\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Restore_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Delete\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Delete_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"DeleteAll\",\n\t\t\tHandler:    _OpenStorageCloudBackup_DeleteAll_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Enumerate\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Enumerate_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Status\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Status_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"Catalog\",\n\t\t\tHandler:    _OpenStorageCloudBackup_Catalog_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"History\",\n\t\t\tHandler:    _OpenStorageCloudBackup_History_Handler,\n\t\t},\n\t\t{\n\t\t\tMethodName: \"StateChange\",\n\t\t\tHandler:    _OpenStorageCloudBackup_StateChange_Handler,\n\t\t},\n\t},\n\tStreams:  []grpc.StreamDesc{},\n\tMetadata: \"api/api.proto\",\n}\n\nfunc init() { proto.RegisterFile(\"api/api.proto\", fileDescriptor_api_4634bdf1f2f590eb) }\n\nvar fileDescriptor_api_4634bdf1f2f590eb = []byte{\n\t// 6804 bytes of a gzipped FileDescriptorProto\n\t0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xbc, 0x7c, 0x5b, 0x8c, 0x1b, 0xc9,\n\t0x75, 0xf6, 0x36, 0x39, 0xbc, 0x9d, 0xb9, 0xf5, 0x94, 0xa4, 0x11, 0x45, 0x8d, 0xa4, 0x51, 0xcb,\n\t0xba, 0x2c, 0x57, 0x3b, 0xa3, 0x9d, 0x5d, 0xad, 0x77, 0xa5, 0x7f, 0xf7, 0x37, 0x35, 0xe4, 0x48,\n\t0x5c, 0xcd, 0x90, 0xe3, 0x26, 0x47, 0xda, 0xb5, 0x7f, 0x9b, 0x7f, 0x8b, 0x2c, 0x8d, 0xb8, 0x22,\n\t0xd9, 0x54, 0x77, 0x73, 0x16, 0xb3, 0xfa, 0xe5, 0x1f, 0x09, 0x10, 0x38, 0x30, 0x7c, 0x49, 0xe2,\n\t0x38, 0x81, 0x91, 0xc4, 0x41, 0x1e, 0x92, 0x17, 0xc3, 0x40, 0xf2, 0x1c, 0x03, 0x46, 0x80, 0xbc,\n\t0x04, 0x88, 0x9d, 0x07, 0x3f, 0xe4, 0x21, 0x40, 0x1e, 0x72, 0x79, 0x08, 0x12, 0xe4, 0x29, 0x2f,\n\t0x7e, 0x0b, 0xea, 0xd2, 0xcd, 0xaa, 0xbe, 0x90, 0x4d, 0x5b, 0x9b, 0x17, 0x89, 0x75, 0xea, 0x54,\n\t0x9d, 0xaf, 0x4e, 0x9d, 0x3a, 0x75, 0xaa, 0xfa, 0xd4, 0xc0, 0xa2, 0x31, 0xec, 0x6e, 0x1a, 0xc3,\n\t0xee, 0xc6, 0xd0, 0x32, 0x1d, 0x13, 0x2d, 0x9b, 0x43, 0x3c, 0xb0, 0x1d, 0xd3, 0x32, 0x0e, 0xf1,\n\t0x86, 0x31, 0xec, 0x16, 0x2e, 0x1c, 0x9a, 0xe6, 0x61, 0x0f, 0x6f, 0xd2, 0xea, 0x47, 0xa3, 0xc7,\n\t0x9b, 0x4e, 0xb7, 0x8f, 0x6d, 0xc7, 0xe8, 0x0f, 0x59, 0x8b, 0xc2, 0x1a, 0x67, 0xa0, 0xfd, 0x0c,\n\t0x06, 0xa6, 0x63, 0x38, 0x5d, 0x73, 0x60, 0xb3, 0x5a, 0xed, 0x5b, 0x49, 0x58, 0x6e, 0xb0, 0xee,\n\t0x74, 0x6c, 0x9b, 0x23, 0xab, 0x8d, 0xd1, 0x12, 0x24, 0xba, 0x9d, 0xbc, 0xb2, 0xae, 0x5c, 0xcb,\n\t0xe9, 0x89, 0x6e, 0x07, 0x21, 0x98, 0x1b, 0x1a, 0xce, 0x93, 0x7c, 0x82, 0x52, 0xe8, 0x6f, 0xf4,\n\t0x36, 0xa4, 0xfb, 0xb8, 0xd3, 0x1d, 0xf5, 0xf3, 0xc9, 0x75, 0xe5, 0xda, 0xd2, 0xd6, 0xf9, 0x0d,\n\t0x1f, 0xb0, 0x0d, 0xde, 0xeb, 0x1e, 0xe5, 0xd2, 0x39, 0x37, 0x5a, 0x85, 0xb4, 0x39, 0xe8, 0x75,\n\t0x07, 0x38, 0x3f, 0xb7, 0xae, 0x5c, 0xcb, 0xea, 0xbc, 0x44, 0x64, 0x74, 0xcd, 0xa1, 0x9d, 0x4f,\n\t0xad, 0x2b, 0xd7, 0xe6, 0x74, 0xfa, 0x1b, 0x9d, 0x85, 0x9c, 0x8d, 0x9f, 0xb5, 0x3e, 0xb1, 0xba,\n\t0x0e, 0xce, 0xa7, 0xd7, 0x95, 0x6b, 0x8a, 0x9e, 0xb5, 0xf1, 0xb3, 0x87, 0xa4, 0x8c, 0xce, 0x00,\n\t0xf9, 0xdd, 0xb2, 0xb0, 0xd1, 0xc9, 0x67, 0x68, 0x5d, 0xc6, 0xc6, 0xcf, 0x74, 0x6c, 0x74, 0x88,\n\t0x0c, 0xcb, 0x18, 0x74, 0xf4, 0x87, 0xf9, 0x2c, 0xad, 0xe0, 0x25, 0x22, 0xc3, 0xee, 0x7e, 0x8a,\n\t0xf3, 0x39, 0x26, 0x83, 0xfc, 0x26, 0xb4, 0x91, 0x8d, 0x3b, 0x79, 0x60, 0x34, 0xf2, 0x1b, 0x5d,\n\t0x86, 0x25, 0x8b, 0xab, 0xa9, 0x65, 0x0f, 0x31, 0xee, 0xe4, 0xe7, 0xe9, 0xc8, 0x17, 0x5d, 0x6a,\n\t0x83, 0x10, 0xd1, 0xe7, 0x21, 0xd7, 0x33, 0x6c, 0xa7, 0x65, 0xb7, 0x8d, 0x41, 0x7e, 0x61, 0x5d,\n\t0xb9, 0x36, 0xbf, 0x55, 0xd8, 0x60, 0xca, 0xde, 0x70, 0x67, 0x63, 0xa3, 0xe9, 0xce, 0x86, 0x9e,\n\t0x25, 0xcc, 0x8d, 0xb6, 0x31, 0x40, 0x05, 0xc8, 0xf6, 0xb1, 0x63, 0x74, 0x0c, 0xc7, 0xc8, 0x2f,\n\t0x52, 0x2d, 0x78, 0x65, 0xed, 0x67, 0x09, 0x98, 0xe7, 0x9a, 0xdb, 0x37, 0xcd, 0x1e, 0x99, 0x8b,\n\t0x6a, 0x99, 0xce, 0x45, 0x4a, 0x4f, 0x54, 0xcb, 0xa8, 0x08, 0xc9, 0x6d, 0xd3, 0xa6, 0x53, 0xb1,\n\t0xb4, 0x95, 0x0f, 0x28, 0x7d, 0xdb, 0xb4, 0x9b, 0xc7, 0x43, 0xac, 0x13, 0x26, 0x32, 0x47, 0x7b,\n\t0x33, 0xcd, 0x11, 0xfb, 0x1f, 0xad, 0x41, 0x4e, 0x37, 0xba, 0x9d, 0x5d, 0x7c, 0x84, 0x7b, 0x74,\n\t0x9a, 0x72, 0xfa, 0x98, 0x40, 0x6a, 0x9b, 0xa6, 0x63, 0xf4, 0x1a, 0x44, 0x95, 0x19, 0xaa, 0xb6,\n\t0x31, 0x81, 0xe8, 0xf3, 0x80, 0xe8, 0x33, 0xcb, 0xf4, 0x49, 0x7e, 0xa3, 0x2f, 0x40, 0xba, 0x67,\n\t0x3c, 0xc2, 0x3d, 0x3b, 0x9f, 0x5b, 0x4f, 0x5e, 0x9b, 0xdf, 0xba, 0x16, 0x85, 0x83, 0x8c, 0x78,\n\t0x63, 0x97, 0xb2, 0x56, 0x06, 0x8e, 0x75, 0xac, 0xf3, 0x76, 0x85, 0x77, 0x61, 0x5e, 0x20, 0x23,\n\t0x15, 0x92, 0x4f, 0xf1, 0x31, 0xb7, 0x50, 0xf2, 0x13, 0x9d, 0x84, 0xd4, 0x91, 0xd1, 0x1b, 0x61,\n\t0x6e, 0xa3, 0xac, 0x70, 0x2b, 0xf1, 0x8e, 0xa2, 0xfd, 0xa5, 0x02, 0x8b, 0x0f, 0xcc, 0xde, 0xa8,\n\t0x8f, 0x77, 0xcd, 0xb6, 0xe1, 0x98, 0x16, 0x81, 0x38, 0x30, 0xfa, 0x98, 0x37, 0xa7, 0xbf, 0xd1,\n\t0x01, 0x2c, 0x1e, 0x51, 0xa6, 0x16, 0x47, 0x9a, 0xa0, 0x48, 0x6f, 0x04, 0x90, 0x4a, 0x5d, 0xb9,\n\t0x25, 0x01, 0xf1, 0xc2, 0x91, 0x40, 0x2a, 0xfc, 0x6f, 0x58, 0x09, 0xb0, 0xcc, 0x84, 0xfe, 0x2d,\n\t0x48, 0x37, 0xd8, 0xa2, 0x5c, 0x85, 0xf4, 0xd0, 0xb0, 0xf0, 0xc0, 0xe1, 0x0d, 0x79, 0x89, 0x1a,\n\t0x35, 0x31, 0x51, 0xbe, 0x38, 0xc9, 0x6f, 0xed, 0x34, 0xa4, 0xee, 0x5a, 0xe6, 0x68, 0xe8, 0x5f,\n\t0xc9, 0xda, 0x4f, 0x33, 0x00, 0x0c, 0x50, 0x63, 0x88, 0xdb, 0x64, 0x2a, 0xf1, 0xf0, 0x09, 0xee,\n\t0x63, 0xcb, 0xe8, 0x51, 0xae, 0xac, 0x3e, 0x26, 0x78, 0xcb, 0x25, 0x21, 0x2c, 0x97, 0x4d, 0x48,\n\t0x3f, 0x36, 0xad, 0xbe, 0xe1, 0x70, 0x93, 0x3a, 0x1d, 0x50, 0xd0, 0x4e, 0x83, 0x1a, 0x20, 0x67,\n\t0x43, 0xe7, 0x00, 0x1e, 0xf5, 0xcc, 0xf6, 0xd3, 0x16, 0xed, 0x8a, 0x18, 0x53, 0x52, 0xcf, 0x51,\n\t0x0a, 0x35, 0x97, 0x33, 0x90, 0x7d, 0x62, 0xb4, 0x7a, 0xd4, 0xd2, 0x52, 0xb4, 0x32, 0xf3, 0xc4,\n\t0x60, 0x76, 0x56, 0x84, 0x64, 0xdb, 0xb4, 0xe9, 0xba, 0x9f, 0x68, 0xe9, 0x6d, 0xd3, 0x46, 0xef,\n\t0x02, 0x74, 0xcd, 0xd6, 0xd0, 0x32, 0x1f, 0x77, 0x7b, 0xcc, 0x28, 0x97, 0xb6, 0x0a, 0x81, 0x26,\n\t0x55, 0x73, 0x9f, 0x71, 0xe8, 0xb9, 0xae, 0xfb, 0x93, 0xe8, 0xb5, 0x83, 0x3b, 0xa3, 0x21, 0xa6,\n\t0x26, 0x9b, 0xd5, 0x79, 0x09, 0xbd, 0x06, 0x2b, 0xf6, 0xc0, 0x18, 0xda, 0x4f, 0x4c, 0xa7, 0xd5,\n\t0x1d, 0x38, 0xd8, 0x3a, 0x32, 0x7a, 0xd4, 0x73, 0x2c, 0xea, 0xaa, 0x5b, 0x51, 0xe5, 0x74, 0xa4,\n\t0xfb, 0xcd, 0x07, 0xa8, 0xf9, 0xbc, 0x1e, 0x61, 0x3e, 0x44, 0xf9, 0xd3, 0x6c, 0x87, 0x00, 0xb3,\n\t0x9f, 0x18, 0x16, 0xf7, 0x3e, 0x59, 0x9d, 0x97, 0xd0, 0xff, 0x82, 0x79, 0x0b, 0x0f, 0x7b, 0xdd,\n\t0xb6, 0xd1, 0xb2, 0xb1, 0xc3, 0x1d, 0xcf, 0xd9, 0x80, 0x24, 0x9d, 0xf1, 0x34, 0xb0, 0xa3, 0x83,\n\t0xe5, 0xfd, 0x26, 0xc3, 0x32, 0x0e, 0x0f, 0x2d, 0x7c, 0xc8, 0xdc, 0x1b, 0xd3, 0xfc, 0x22, 0x1b,\n\t0x96, 0x50, 0xe1, 0x2d, 0x75, 0x3c, 0x68, 0x5b, 0xc7, 0x43, 0x07, 0x77, 0xf2, 0x4b, 0xdc, 0x3e,\n\t0x5c, 0x02, 0x3a, 0x0f, 0x30, 0x34, 0x6c, 0x7b, 0xf8, 0xc4, 0x32, 0x6c, 0x9c, 0x5f, 0xa6, 0x46,\n\t0x26, 0x50, 0x24, 0x0d, 0xda, 0xed, 0x27, 0xb8, 0x33, 0xea, 0xe1, 0xbc, 0x4a, 0xd9, 0x3c, 0x0d,\n\t0x36, 0x38, 0x9d, 0x2c, 0x01, 0xbb, 0x6d, 0xf4, 0x70, 0x7e, 0x85, 0x62, 0x61, 0x05, 0xaa, 0x03,\n\t0xa7, 0xdb, 0x7e, 0x7a, 0x9c, 0x47, 0x5c, 0x07, 0xb4, 0x84, 0xae, 0x43, 0xea, 0x90, 0x18, 0x78,\n\t0xfe, 0x14, 0x1d, 0xfd, 0x6a, 0x60, 0xf4, 0xd4, 0xfc, 0x75, 0xc6, 0x44, 0xfc, 0x39, 0xfd, 0xd1,\n\t0xc2, 0x83, 0xc7, 0xa6, 0xd5, 0xc6, 0x9d, 0xfc, 0x2a, 0xed, 0x6d, 0x91, 0x52, 0x2b, 0x9c, 0x48,\n\t0xc6, 0xd3, 0x36, 0xfb, 0x43, 0x0b, 0xdb, 0xc4, 0x81, 0x9d, 0xa6, 0x2c, 0x02, 0x85, 0xb8, 0xed,\n\t0xb6, 0x61, 0xb7, 0x8d, 0x0e, 0xee, 0xe4, 0xf3, 0xcc, 0x6d, 0xbb, 0x65, 0x94, 0x87, 0xcc, 0xc7,\n\t0xe6, 0xc8, 0x1a, 0x18, 0xbd, 0xfc, 0x19, 0x5a, 0xe5, 0x16, 0x49, 0x2b, 0x36, 0x71, 0x47, 0x6f,\n\t0xe5, 0x0b, 0xac, 0x95, 0x5b, 0xfe, 0xd5, 0xdd, 0x83, 0x06, 0x30, 0x9e, 0x67, 0xc2, 0x37, 0x30,\n\t0x3b, 0xd8, 0xce, 0x2b, 0xeb, 0x49, 0xc2, 0x47, 0x0b, 0xda, 0x0f, 0x15, 0x58, 0xd6, 0x47, 0x03,\n\t0x12, 0x16, 0x34, 0x1c, 0xc3, 0xc1, 0x7b, 0xc6, 0x10, 0x3d, 0x84, 0x45, 0x8b, 0x91, 0x5a, 0x36,\n\t0xa1, 0xd1, 0x16, 0xf3, 0x5b, 0x5b, 0x41, 0x2b, 0x92, 0x1b, 0x4a, 0x65, 0x6e, 0xb4, 0x96, 0x40,\n\t0x22, 0x23, 0x0a, 0xb0, 0xcc, 0x34, 0xa2, 0x7f, 0xce, 0x42, 0x9a, 0xe9, 0x24, 0x10, 0x86, 0x6c,\n\t0x42, 0x9a, 0x05, 0x28, 0xb4, 0xd5, 0x7c, 0x88, 0xef, 0x61, 0xae, 0x52, 0xe7, 0x6c, 0x63, 0x2b,\n\t0x49, 0xc6, 0xb1, 0x92, 0x02, 0x64, 0x49, 0x30, 0x61, 0x0e, 0x7a, 0xc7, 0x3c, 0x36, 0xf1, 0xca,\n\t0xe8, 0x1d, 0xc8, 0xf4, 0x98, 0xcb, 0xa7, 0x5e, 0x6a, 0x3e, 0x64, 0x2b, 0x95, 0x36, 0x06, 0xdd,\n\t0x65, 0x47, 0x37, 0x20, 0xd5, 0x26, 0xea, 0xa0, 0x7e, 0x6c, 0x72, 0x80, 0xc0, 0x18, 0xd1, 0x26,\n\t0xcc, 0xd9, 0x43, 0xdc, 0xa6, 0x5e, 0x2c, 0x6c, 0x61, 0x8f, 0x5d, 0x88, 0x4e, 0x19, 0x89, 0x32,\n\t0x47, 0xb6, 0x71, 0x88, 0xf9, 0x9e, 0xcb, 0x0a, 0x72, 0x74, 0x92, 0x9b, 0x21, 0x3a, 0x19, 0xbb,\n\t0x78, 0x88, 0xe7, 0xe2, 0x6f, 0x92, 0x45, 0x6a, 0x38, 0x23, 0x9b, 0x3a, 0xaa, 0xa5, 0xad, 0x73,\n\t0x51, 0x90, 0x29, 0x93, 0xce, 0x99, 0xd1, 0x16, 0xa4, 0x98, 0xed, 0x2d, 0xd0, 0x56, 0x6b, 0x13,\n\t0x5a, 0x61, 0x9d, 0xb1, 0xa2, 0x0b, 0x30, 0x6f, 0x38, 0x8e, 0x41, 0x9c, 0x46, 0xcb, 0x1c, 0x50,\n\t0xbf, 0x95, 0xd3, 0xc1, 0x25, 0xd5, 0x07, 0x68, 0x1b, 0x96, 0x3c, 0x06, 0xd6, 0xfb, 0x52, 0x44,\n\t0xef, 0x25, 0xca, 0xc6, 0x7a, 0x5f, 0x74, 0xdb, 0x34, 0x5c, 0x29, 0x1d, 0x7c, 0xd4, 0x6d, 0xe3,\n\t0x16, 0x0d, 0x7b, 0xb9, 0x67, 0x63, 0xa4, 0x7d, 0x12, 0xfc, 0x5e, 0x07, 0x64, 0xe3, 0xf6, 0xc8,\n\t0xc2, 0x2d, 0x91, 0xcf, 0x75, 0x6d, 0xb4, 0xa6, 0x3c, 0xe6, 0xf6, 0x40, 0x33, 0xb6, 0x15, 0xba,\n\t0x38, 0x39, 0x68, 0xca, 0x70, 0xcf, 0x63, 0xe8, 0x0e, 0x1e, 0x9b, 0x79, 0x44, 0xd7, 0xe2, 0xd5,\n\t0x08, 0x7d, 0x70, 0xe0, 0xd5, 0xc1, 0x63, 0x93, 0x2d, 0x40, 0xde, 0x13, 0x21, 0xa0, 0xf7, 0x61,\n\t0x41, 0xd8, 0x1b, 0xec, 0xfc, 0x09, 0xda, 0xd5, 0xc4, 0xcd, 0x61, 0x7e, 0xbc, 0x39, 0xd8, 0xa8,\n\t0xe2, 0xf7, 0x0b, 0x27, 0x69, 0x07, 0xeb, 0xd3, 0xfc, 0x82, 0xec, 0x05, 0x88, 0x45, 0x62, 0xcb,\n\t0x32, 0x2d, 0xea, 0x9e, 0x73, 0x3a, 0x2b, 0xa0, 0x0f, 0x40, 0xe5, 0x9b, 0x64, 0xdb, 0x1c, 0xd8,\n\t0xa3, 0x3e, 0xb6, 0xec, 0xfc, 0x2a, 0xed, 0xff, 0x42, 0xc4, 0x58, 0xb7, 0x39, 0x9f, 0xbe, 0x7c,\n\t0x24, 0x95, 0xed, 0xc2, 0x7b, 0xb0, 0xec, 0xd3, 0xc3, 0x4c, 0x5e, 0xe6, 0x8f, 0x13, 0x90, 0x22,\n\t0x50, 0x6d, 0xc2, 0x43, 0x56, 0xb9, 0x4d, 0xdb, 0xcd, 0xe9, 0xac, 0x80, 0x4e, 0x43, 0x86, 0xfc,\n\t0x68, 0xf5, 0x6d, 0x1e, 0xfd, 0xa4, 0x49, 0x71, 0xcf, 0x26, 0xe1, 0x0c, 0xad, 0x78, 0x74, 0xec,\n\t0x60, 0x9b, 0xfa, 0x95, 0x39, 0x3d, 0x47, 0x28, 0x77, 0x08, 0x81, 0xec, 0x57, 0xf4, 0xb4, 0x62,\n\t0x53, 0x0f, 0x32, 0xa7, 0xf3, 0x12, 0x09, 0x73, 0xe8, 0x2f, 0xd2, 0x21, 0x3b, 0xe1, 0x64, 0x68,\n\t0x79, 0xcf, 0x26, 0xd6, 0xc1, 0xaa, 0x58, 0x97, 0x69, 0x5a, 0x0b, 0x94, 0xc4, 0xfa, 0xbc, 0x00,\n\t0xf3, 0x2c, 0xb6, 0x39, 0x24, 0xfb, 0x10, 0x8f, 0xb8, 0x81, 0x06, 0x30, 0x94, 0x82, 0x4e, 0x40,\n\t0xaa, 0x6b, 0x92, 0x9e, 0xb3, 0xee, 0xd9, 0x89, 0x01, 0xa5, 0x1d, 0xb6, 0xe8, 0xe9, 0x86, 0x9d,\n\t0x78, 0x72, 0x94, 0x42, 0x43, 0x72, 0xd2, 0x29, 0x0f, 0x5e, 0x48, 0x4b, 0xe0, 0x9d, 0x72, 0xd2,\n\t0x9e, 0xad, 0xfd, 0x47, 0x02, 0x52, 0xa5, 0x1e, 0xb6, 0x1c, 0xc1, 0x0d, 0x27, 0xa9, 0x1b, 0x7e,\n\t0x97, 0x1c, 0xbc, 0x8e, 0xb0, 0xd5, 0x75, 0x8e, 0xf9, 0x31, 0x24, 0xb8, 0xe0, 0x1b, 0x9c, 0x81,\n\t0xfa, 0x09, 0x8f, 0x9d, 0x80, 0x32, 0x48, 0x9f, 0x2d, 0xe7, 0x78, 0x88, 0xa9, 0xf6, 0x92, 0x7a,\n\t0x8e, 0x52, 0x08, 0x23, 0xd9, 0x44, 0xfb, 0xd8, 0xa6, 0xae, 0x8c, 0x9d, 0x3a, 0xdc, 0x22, 0x7a,\n\t0x07, 0x72, 0xde, 0xb1, 0x96, 0x7b, 0xe0, 0x49, 0xce, 0x6c, 0xcc, 0x4c, 0x06, 0x6a, 0xf1, 0x73,\n\t0x6d, 0xab, 0xdb, 0xa1, 0xea, 0xcd, 0x91, 0x80, 0x88, 0x91, 0xaa, 0x74, 0x38, 0x6e, 0x89, 0x07,\n\t0x8e, 0xe7, 0x42, 0x96, 0x0b, 0x63, 0x60, 0xc3, 0x71, 0xd9, 0x09, 0xde, 0x76, 0x0f, 0xd3, 0x10,\n\t0x8d, 0xc5, 0x8e, 0x6e, 0x91, 0xd8, 0xa2, 0xe3, 0xf4, 0xb8, 0xda, 0xc9, 0x4f, 0x32, 0xf4, 0xd1,\n\t0xa0, 0xfb, 0x6c, 0x84, 0x5b, 0x8e, 0x71, 0x48, 0xf5, 0x9d, 0xd3, 0x73, 0x8c, 0xd2, 0x34, 0x0e,\n\t0xb5, 0xb7, 0x21, 0x4d, 0xb5, 0x6d, 0x93, 0x4d, 0x8b, 0x6a, 0x84, 0x6f, 0xc9, 0xc1, 0x4d, 0x8b,\n\t0xf2, 0xe9, 0x8c, 0x49, 0xfb, 0xdb, 0x04, 0x2c, 0xd7, 0x1f, 0x7d, 0x8c, 0xdb, 0x0e, 0x61, 0xc1,\n\t0xd4, 0x09, 0x90, 0x23, 0xed, 0xc8, 0xdb, 0x39, 0xe9, 0x6f, 0x72, 0x94, 0xe6, 0x6b, 0xaf, 0xeb,\n\t0x1e, 0x15, 0xb2, 0x8c, 0x50, 0xa5, 0xc1, 0x0b, 0x1e, 0x18, 0x8f, 0x7a, 0xb8, 0x43, 0xe7, 0x24,\n\t0xab, 0xbb, 0x45, 0x16, 0x7f, 0x51, 0xd7, 0xce, 0x26, 0xc4, 0xf5, 0xdd, 0xab, 0x90, 0x36, 0xda,\n\t0x24, 0x4e, 0xe4, 0x41, 0x3b, 0x2f, 0xd1, 0x09, 0x6e, 0xb7, 0xb1, 0x6d, 0xb7, 0xc8, 0x52, 0x64,\n\t0xca, 0xce, 0x31, 0xca, 0x7d, 0x4c, 0xe7, 0xdf, 0xc6, 0x6d, 0x0b, 0x3b, 0xb4, 0x3a, 0xc3, 0xaa,\n\t0x19, 0x85, 0x54, 0xd3, 0x70, 0xb3, 0x33, 0x34, 0xbb, 0x03, 0x87, 0x18, 0x33, 0x71, 0x93, 0x63,\n\t0x02, 0x7a, 0x15, 0xd4, 0xf6, 0xc8, 0x22, 0x67, 0x9e, 0x16, 0x1e, 0x74, 0xf6, 0x09, 0x91, 0x2a,\n\t0x38, 0xa7, 0x2f, 0x73, 0x7a, 0x85, 0x93, 0xa9, 0xc7, 0x65, 0x30, 0x86, 0xa6, 0xc5, 0xf6, 0xb1,\n\t0xa4, 0xce, 0x91, 0xed, 0x9b, 0x96, 0x43, 0x6f, 0x08, 0xf0, 0x21, 0xc1, 0xcf, 0x4e, 0xf6, 0xbc,\n\t0xa4, 0xfd, 0x85, 0x02, 0x27, 0xb8, 0xeb, 0xb1, 0x30, 0xd9, 0x19, 0xf0, 0xb3, 0x11, 0xb6, 0x1d,\n\t0x71, 0xff, 0x57, 0x66, 0xdb, 0xff, 0x67, 0x0e, 0x5a, 0xdc, 0xed, 0x3f, 0x19, 0x73, 0xfb, 0xd7,\n\t0xae, 0xc0, 0x12, 0xa3, 0xe9, 0xd8, 0x1e, 0x9a, 0x03, 0x5b, 0x70, 0xbf, 0x8a, 0xe0, 0x7e, 0xb5,\n\t0x21, 0x9c, 0x94, 0x87, 0xc6, 0xb9, 0xfd, 0x61, 0xd6, 0x3d, 0xe0, 0xde, 0xb6, 0x65, 0x71, 0x16,\n\t0x0e, 0x3d, 0xca, 0x4b, 0xbb, 0x3d, 0xe9, 0x4b, 0x47, 0x52, 0x59, 0xfb, 0x1b, 0xc5, 0x8d, 0x6f,\n\t0xe9, 0xb6, 0x50, 0x62, 0x36, 0x72, 0x0b, 0xd2, 0x6c, 0xc7, 0xa2, 0x32, 0x97, 0xb6, 0xb4, 0x88,\n\t0x6e, 0x19, 0xfb, 0xbe, 0x61, 0x19, 0x7d, 0x9d, 0xb7, 0x40, 0xef, 0x40, 0xaa, 0x6f, 0x8e, 0x06,\n\t0x0e, 0x77, 0x3c, 0x71, 0x9a, 0xb2, 0x06, 0xc4, 0xf4, 0xe8, 0x0f, 0xb6, 0x07, 0x27, 0x99, 0xe9,\n\t0x51, 0x8a, 0xbb, 0x47, 0x8b, 0x5b, 0xf9, 0x9c, 0x7f, 0xcb, 0xd7, 0x7e, 0x92, 0x00, 0x95, 0x8f,\n\t0x05, 0x3b, 0x2f, 0xc3, 0x2c, 0xd8, 0x2c, 0x27, 0xe2, 0x06, 0x79, 0xb7, 0xbc, 0x15, 0xc7, 0x0c,\n\t0x43, 0x9b, 0x14, 0x2e, 0xb1, 0xf1, 0x7b, 0xab, 0xf2, 0x1e, 0x64, 0xcc, 0x21, 0xbd, 0xf4, 0xcb,\n\t0xcf, 0x51, 0xa7, 0xb2, 0x11, 0xd5, 0xd8, 0x1b, 0xda, 0x46, 0x9d, 0x35, 0x60, 0x21, 0x86, 0xdb,\n\t0xbc, 0x70, 0x0b, 0x16, 0xc4, 0x8a, 0x99, 0xf6, 0xdc, 0x6f, 0x8f, 0xad, 0x81, 0x88, 0xe1, 0xd6,\n\t0xb7, 0x09, 0x69, 0x66, 0x35, 0x5c, 0x83, 0xa7, 0xa3, 0x8c, 0x8c, 0xb3, 0xbd, 0x44, 0xf3, 0x3c,\n\t0x86, 0x95, 0xc6, 0xc0, 0x18, 0xca, 0x2b, 0xdd, 0xbf, 0x1a, 0x84, 0x29, 0x4e, 0xcc, 0x36, 0xc5,\n\t0xe2, 0x79, 0x22, 0x29, 0x9f, 0x27, 0xb4, 0x67, 0x80, 0x44, 0xd1, 0x5c, 0x17, 0x5f, 0x86, 0x55,\n\t0x37, 0x40, 0xa2, 0x15, 0xe3, 0x11, 0x32, 0xdd, 0x5c, 0x8e, 0x0a, 0x93, 0xa4, 0x6e, 0xf4, 0x93,\n\t0x47, 0x21, 0x54, 0xcd, 0x71, 0x6f, 0x7e, 0xe8, 0x1e, 0x21, 0xed, 0x07, 0x8a, 0x6f, 0x3f, 0x08,\n\t0xbb, 0xef, 0xbd, 0x09, 0x19, 0x2e, 0x38, 0x8e, 0x67, 0x72, 0x79, 0xb5, 0x1f, 0x29, 0xae, 0x77,\n\t0x72, 0x63, 0xb7, 0xd0, 0xeb, 0xb7, 0x35, 0xc8, 0x91, 0xff, 0xed, 0xa1, 0xd1, 0x76, 0x2d, 0x67,\n\t0x4c, 0x20, 0x2d, 0xbc, 0x80, 0x21, 0xa7, 0xd3, 0xdf, 0x24, 0x42, 0x23, 0xc7, 0x5b, 0x02, 0x9f,\n\t0x6f, 0x4d, 0xa4, 0x58, 0xed, 0x90, 0x85, 0x6e, 0x7e, 0x32, 0xc0, 0x56, 0x8b, 0x0a, 0x49, 0xb1,\n\t0xbe, 0x28, 0xa5, 0x46, 0x24, 0x79, 0xd5, 0xb4, 0xc7, 0xb4, 0x50, 0x4d, 0x36, 0x77, 0xad, 0x03,\n\t0xe8, 0xae, 0x65, 0x0c, 0x9f, 0x94, 0xad, 0xee, 0x11, 0xb6, 0xb6, 0x9f, 0x18, 0x83, 0x43, 0x6c,\n\t0x7b, 0x0a, 0x51, 0x04, 0x85, 0xdc, 0x82, 0xb9, 0xa7, 0xdd, 0x41, 0x87, 0x7b, 0xa2, 0x2b, 0x21,\n\t0x67, 0x4b, 0x5f, 0x37, 0x34, 0x78, 0xa0, 0x6d, 0xb4, 0xab, 0xb0, 0xbc, 0xdd, 0x1b, 0xd9, 0x0e,\n\t0xb6, 0xa6, 0xf8, 0xec, 0xdf, 0x53, 0x60, 0x91, 0x2c, 0xe6, 0x23, 0xcf, 0x3e, 0xef, 0x41, 0x56,\n\t0xc7, 0xcf, 0xb0, 0xed, 0xdc, 0x7f, 0xc0, 0x23, 0x84, 0xeb, 0xc1, 0x08, 0x41, 0x6c, 0xb1, 0xe1,\n\t0xb2, 0xb3, 0xa5, 0xec, 0xb5, 0x2e, 0xdc, 0x86, 0x45, 0xa9, 0x4a, 0x5c, 0xcc, 0xc9, 0x69, 0x8b,\n\t0xf9, 0x53, 0x58, 0x92, 0xa4, 0xd8, 0x48, 0x83, 0x05, 0xfe, 0x7b, 0x9b, 0x7a, 0x68, 0xd6, 0x8d,\n\t0x44, 0x43, 0x65, 0xdf, 0x68, 0xf8, 0x2d, 0xeb, 0xf9, 0xc9, 0x23, 0xd0, 0xe5, 0x46, 0xda, 0x9f,\n\t0x2b, 0xb0, 0x4a, 0x4f, 0xee, 0xd3, 0x57, 0xef, 0x7d, 0x48, 0xef, 0x8a, 0xf7, 0xb9, 0x6f, 0x86,\n\t0x5f, 0x01, 0x04, 0x3a, 0x92, 0x2f, 0xa1, 0x77, 0x7f, 0xe5, 0x4b, 0xe8, 0x7f, 0x53, 0xe0, 0x74,\n\t0x40, 0x12, 0x9f, 0xf9, 0x03, 0xc8, 0xb9, 0xb7, 0x61, 0x36, 0x9f, 0xd2, 0xcf, 0x4f, 0x87, 0xc9,\n\t0x1a, 0x6f, 0x34, 0xdc, 0x96, 0x0c, 0xea, 0xb8, 0xa7, 0xb1, 0x41, 0x25, 0x04, 0x83, 0x2a, 0x18,\n\t0xb0, 0x24, 0x37, 0x09, 0x19, 0xc6, 0xbb, 0xe2, 0x30, 0xe6, 0xb7, 0x2e, 0x05, 0x23, 0x96, 0x00,\n\t0x0e, 0x71, 0xac, 0xbf, 0x98, 0xf3, 0xbe, 0x60, 0xd4, 0xcc, 0x4e, 0x30, 0xbe, 0x50, 0x21, 0xd9,\n\t0x1e, 0x8e, 0x68, 0xe7, 0x8a, 0x4e, 0x7e, 0x12, 0x67, 0xd4, 0xc7, 0xfd, 0x96, 0x63, 0x3a, 0x46,\n\t0x8f, 0x9f, 0xa9, 0xb2, 0x7d, 0xdc, 0xa7, 0x1f, 0x15, 0xc8, 0xd1, 0x89, 0x54, 0xd2, 0x63, 0x0c,\n\t0x3b, 0x54, 0x65, 0xfa, 0xb8, 0x4f, 0x0f, 0x31, 0xbc, 0xea, 0xb1, 0x85, 0xb1, 0x7b, 0xaa, 0xea,\n\t0xe3, 0xfe, 0x8e, 0x85, 0xe9, 0xbd, 0xb2, 0x71, 0x74, 0xd8, 0xea, 0x99, 0x06, 0x8b, 0xf9, 0x93,\n\t0x7a, 0xc6, 0x38, 0x3a, 0xdc, 0x35, 0x0d, 0x76, 0x8d, 0xc4, 0x62, 0xda, 0x4c, 0xc4, 0xfd, 0x86,\n\t0xef, 0xa2, 0xe2, 0x3d, 0x48, 0x75, 0xba, 0xf6, 0x53, 0xf7, 0xeb, 0xc5, 0xd5, 0xa8, 0xaf, 0x17,\n\t0x64, 0xb4, 0x1b, 0x65, 0xc2, 0xc9, 0x26, 0x83, 0xb5, 0x42, 0x5b, 0x90, 0x1a, 0x9a, 0xa6, 0x77,\n\t0x27, 0xbc, 0x36, 0xe9, 0xe3, 0x87, 0xce, 0x58, 0x89, 0x77, 0xeb, 0x1f, 0xf6, 0x9d, 0x56, 0x77,\n\t0xe8, 0x06, 0xa8, 0xa4, 0x58, 0x1d, 0x92, 0x8a, 0x8e, 0xe1, 0x18, 0xa4, 0x62, 0x81, 0x55, 0x90,\n\t0x62, 0x95, 0xde, 0x5e, 0x3d, 0x31, 0x6d, 0x87, 0x3a, 0x3d, 0x76, 0x61, 0xe1, 0x95, 0xd1, 0x1e,\n\t0xcc, 0x53, 0x5f, 0xc9, 0xef, 0xa6, 0xd5, 0x08, 0xb7, 0x21, 0x0e, 0x83, 0xfc, 0x23, 0xae, 0x01,\n\t0x18, 0x78, 0x84, 0xc2, 0x97, 0x00, 0xc6, 0xa3, 0x0c, 0xb1, 0x9f, 0xb7, 0x65, 0xfb, 0x59, 0x8f,\n\t0x12, 0xe4, 0x9e, 0xaa, 0x04, 0xe3, 0x21, 0xe7, 0x7a, 0x9f, 0xe8, 0x99, 0xd6, 0xd9, 0x0f, 0x14,\n\t0x58, 0xe2, 0xbd, 0x73, 0x07, 0x2b, 0x4c, 0xb7, 0x12, 0x6f, 0xba, 0x99, 0xbd, 0x26, 0x3c, 0x7b,\n\t0x15, 0x76, 0x9a, 0xa4, 0xb4, 0xd3, 0x6c, 0xb9, 0xd7, 0xad, 0x73, 0x93, 0x27, 0x96, 0x0c, 0xc8,\n\t0xbd, 0x8c, 0xed, 0xc1, 0xf9, 0x46, 0xe7, 0xa9, 0x7b, 0xeb, 0xbd, 0x6f, 0xf6, 0xba, 0xed, 0x63,\n\t0xd9, 0x85, 0x7d, 0x00, 0x4b, 0x72, 0x35, 0xdf, 0xfc, 0x83, 0x01, 0x5f, 0xa0, 0x23, 0xdd, 0xd7,\n\t0x52, 0xbb, 0x08, 0x17, 0x22, 0xa5, 0xf1, 0xb0, 0x20, 0x0c, 0xd0, 0xc1, 0xb0, 0xf3, 0x3f, 0x08,\n\t0xc8, 0x95, 0xc6, 0x01, 0x5d, 0x82, 0x8b, 0x01, 0x96, 0xca, 0x80, 0x44, 0x0e, 0x63, 0x4c, 0x5a,\n\t0x07, 0xb4, 0x49, 0x4c, 0xdc, 0xb3, 0xbe, 0x0f, 0xd9, 0x21, 0xa9, 0xea, 0x62, 0xd7, 0xb1, 0xc6,\n\t0xc1, 0xec, 0xb5, 0xd1, 0x6e, 0x86, 0xa0, 0xad, 0x0e, 0x48, 0x38, 0xee, 0x9d, 0x00, 0x42, 0x82,\n\t0x19, 0xed, 0xab, 0xb0, 0x1e, 0xdd, 0x8c, 0x43, 0xbb, 0x05, 0xe9, 0xe1, 0xac, 0xca, 0xe4, 0x2d,\n\t0xb4, 0xb7, 0x42, 0xa6, 0xac, 0x8c, 0x7b, 0x78, 0x3c, 0x65, 0x61, 0xa8, 0xc2, 0x54, 0xef, 0xb6,\n\t0xe2, 0xaa, 0xdf, 0x86, 0x95, 0x00, 0x4b, 0x68, 0xb8, 0x56, 0x80, 0xac, 0xf7, 0x41, 0x87, 0x5f,\n\t0x26, 0xb8, 0x65, 0xad, 0x4d, 0xe5, 0x6c, 0x5b, 0xb8, 0x83, 0x07, 0x4e, 0xd7, 0xe8, 0x31, 0x7b,\n\t0x2b, 0x7d, 0x3a, 0xb2, 0x3c, 0x78, 0x5f, 0x00, 0x68, 0x7b, 0xf5, 0x5c, 0x01, 0x41, 0x2f, 0x41,\n\t0x9b, 0x8c, 0xfb, 0xd1, 0x85, 0x36, 0xda, 0x5d, 0xaa, 0xe2, 0x08, 0x21, 0x5c, 0xc5, 0x97, 0x60,\n\t0x71, 0xdc, 0x62, 0x1c, 0xe6, 0x2e, 0x8c, 0x89, 0xd5, 0x8e, 0x86, 0x43, 0x3b, 0xba, 0x4b, 0x6f,\n\t0x96, 0x5c, 0xb8, 0xa5, 0x10, 0xb8, 0x17, 0x83, 0x3b, 0x34, 0x6d, 0x13, 0x81, 0xf7, 0x1e, 0x35,\n\t0xea, 0x28, 0x31, 0xb3, 0x00, 0xfe, 0x2a, 0x9c, 0x0b, 0x1b, 0xf9, 0xc3, 0x86, 0x8b, 0xf6, 0xbd,\n\t0x10, 0xb4, 0x21, 0x17, 0x74, 0x6f, 0x46, 0x20, 0xad, 0x50, 0xe3, 0x0a, 0xed, 0x7f, 0x16, 0x98,\n\t0x7f, 0xaa, 0xc0, 0x82, 0x28, 0x23, 0x56, 0x2b, 0xdf, 0xf5, 0x51, 0x62, 0xf2, 0xf5, 0x51, 0xd2,\n\t0x7f, 0x7d, 0x54, 0x80, 0xac, 0x7b, 0x5b, 0xc4, 0xcf, 0x04, 0x5e, 0x59, 0xb8, 0xf0, 0x49, 0x49,\n\t0x17, 0x3e, 0x9f, 0xc2, 0xb2, 0xcf, 0xce, 0xe2, 0x21, 0xbd, 0x08, 0x0b, 0x46, 0xbb, 0x4d, 0x2f,\n\t0x14, 0xe8, 0xea, 0x60, 0x58, 0xe7, 0x39, 0x8d, 0x9e, 0x34, 0xd8, 0x25, 0x14, 0x65, 0x19, 0xc3,\n\t0x05, 0x4e, 0xba, 0x8f, 0xc9, 0x21, 0x50, 0xf5, 0x1b, 0x4d, 0x6c, 0x35, 0x0d, 0x2d, 0xf3, 0x63,\n\t0xdc, 0x76, 0xc6, 0xb7, 0x79, 0x39, 0x4e, 0xa9, 0xd2, 0xb0, 0xe8, 0x63, 0xdb, 0x1c, 0x08, 0x52,\n\t0x33, 0xa4, 0x4c, 0x44, 0xfa, 0xd7, 0x8d, 0xe7, 0x33, 0x05, 0x03, 0x8a, 0x35, 0xbf, 0x8f, 0x7c,\n\t0x06, 0x2d, 0x77, 0xc4, 0x2d, 0xc5, 0x6f, 0x8a, 0xc9, 0xd9, 0x4c, 0xb1, 0x4a, 0x9d, 0x7c, 0x98,\n\t0x0c, 0xd1, 0x99, 0xc4, 0x82, 0x7b, 0x08, 0x97, 0x26, 0x76, 0xc5, 0x01, 0x7f, 0x21, 0x04, 0xf0,\n\t0x6c, 0x8e, 0xe9, 0x83, 0x28, 0x41, 0xb2, 0x4b, 0x89, 0x05, 0xba, 0x0b, 0x9f, 0x9b, 0xdc, 0x17,\n\t0x47, 0x5d, 0x0a, 0x41, 0x3d, 0xa3, 0x7f, 0x2a, 0x41, 0x41, 0x12, 0x25, 0x6f, 0x27, 0xb1, 0xd0,\n\t0x9e, 0x83, 0xb3, 0xa1, 0x5d, 0x78, 0x7b, 0xcb, 0x9a, 0x54, 0xfd, 0xc0, 0xe8, 0x75, 0xc5, 0x28,\n\t0x23, 0x96, 0x8c, 0x0b, 0x3e, 0xe7, 0x37, 0xee, 0x84, 0x4b, 0xf9, 0x47, 0x05, 0x4e, 0x35, 0x3a,\n\t0x4f, 0xd9, 0x8d, 0xc3, 0x1e, 0x59, 0x68, 0x6e, 0xff, 0x13, 0x2f, 0x3c, 0xe4, 0xcb, 0xc1, 0x84,\n\t0xff, 0x72, 0x70, 0x6f, 0x7c, 0x7f, 0x96, 0x8c, 0x38, 0x46, 0x86, 0x0a, 0xfd, 0x0c, 0x2e, 0xd1,\n\t0xf2, 0xb0, 0xea, 0x17, 0xc5, 0x87, 0xfe, 0x4f, 0x0a, 0x9c, 0xf6, 0xaa, 0x0e, 0x06, 0xfd, 0x97,\n\t0x35, 0xf8, 0xba, 0x7f, 0xf0, 0x37, 0xa3, 0x07, 0x2f, 0x8b, 0xfd, 0x0c, 0x86, 0x5f, 0x80, 0x7c,\n\t0x50, 0x18, 0x57, 0xc0, 0x5f, 0x29, 0x82, 0x6e, 0xd8, 0xc7, 0xc1, 0x58, 0xe3, 0xaf, 0x8d, 0x07,\n\t0xc8, 0x2e, 0x09, 0xde, 0x8a, 0x1e, 0xa0, 0xd4, 0xed, 0x67, 0x30, 0xbe, 0x5b, 0xc2, 0x1c, 0xba,\n\t0xb2, 0xf8, 0x2a, 0xf7, 0xdd, 0x50, 0x2b, 0x81, 0x1b, 0xea, 0x9b, 0xc2, 0xf0, 0xcb, 0x38, 0xee,\n\t0xf0, 0xb5, 0x33, 0x82, 0x48, 0xb7, 0x19, 0xd7, 0xe8, 0x57, 0x84, 0x1e, 0xe5, 0x43, 0x4a, 0x58,\n\t0x50, 0x38, 0xeb, 0x95, 0xb6, 0xf6, 0xb6, 0x20, 0xd9, 0x77, 0x27, 0x32, 0x11, 0xf1, 0xb1, 0xb0,\n\t0xc6, 0xb7, 0x7b, 0xe6, 0x60, 0x22, 0xaa, 0xb3, 0x90, 0x63, 0x89, 0x72, 0xc2, 0x87, 0x2f, 0x46,\n\t0xa8, 0x76, 0x66, 0xff, 0xd6, 0x22, 0xea, 0x98, 0x8b, 0x8e, 0x83, 0x58, 0x9e, 0x1a, 0xd1, 0xb5,\n\t0xce, 0x30, 0x35, 0x92, 0x3b, 0x15, 0x75, 0xe7, 0x3b, 0x92, 0x4c, 0xec, 0xf2, 0xbe, 0xb0, 0x80,\n\t0xfc, 0x67, 0x92, 0x59, 0xaf, 0xe2, 0xb5, 0x03, 0x38, 0xe3, 0x75, 0xe6, 0x3f, 0xa2, 0xfd, 0xf2,\n\t0xdf, 0x46, 0xb4, 0x3a, 0xdd, 0x8c, 0x02, 0xdd, 0x72, 0x94, 0x6f, 0x40, 0x86, 0x89, 0x77, 0xcf,\n\t0x74, 0x91, 0x30, 0x5d, 0x3e, 0xed, 0xa7, 0x0a, 0x0d, 0x6a, 0xf9, 0x6c, 0xf2, 0xeb, 0x2f, 0xd9,\n\t0xa0, 0x27, 0x7a, 0x88, 0x86, 0x97, 0xbf, 0xca, 0x1c, 0xc4, 0xed, 0x68, 0x07, 0x11, 0xda, 0xfb,\n\t0xcb, 0x4e, 0x69, 0xbd, 0x43, 0x8f, 0x58, 0xe1, 0x02, 0xc7, 0xde, 0x62, 0x9c, 0xbd, 0xe8, 0x8e,\n\t0x08, 0xbc, 0xbc, 0xc5, 0x8e, 0xd6, 0x0a, 0xe9, 0x43, 0xc7, 0xf4, 0x23, 0x72, 0x2c, 0x9d, 0xf8,\n\t0x04, 0x24, 0x02, 0x02, 0x34, 0x1a, 0x6a, 0x46, 0x08, 0xe0, 0x56, 0xfc, 0x73, 0x85, 0x86, 0x91,\n\t0x32, 0x53, 0xc0, 0x92, 0x26, 0xe2, 0x78, 0xe0, 0x9b, 0x9b, 0xf7, 0xa7, 0xcf, 0x8d, 0x5f, 0xc0,\n\t0xcb, 0x9e, 0x9e, 0x2f, 0xd3, 0xb8, 0x35, 0x52, 0x26, 0x9f, 0xa1, 0x9b, 0xc1, 0x6b, 0xdf, 0x48,\n\t0x4b, 0x1e, 0x73, 0x6a, 0x6b, 0x2c, 0x52, 0x63, 0x97, 0x5b, 0x81, 0x7b, 0x91, 0x0f, 0x59, 0x10,\n\t0x16, 0xa8, 0xe5, 0x32, 0xdf, 0x85, 0x4c, 0x9b, 0xd5, 0xf1, 0x35, 0x79, 0x21, 0xea, 0xce, 0xca,\n\t0xfd, 0x3c, 0xe1, 0xf2, 0x6b, 0x6f, 0x52, 0xc7, 0xc1, 0xc9, 0x3e, 0x8f, 0x23, 0xdc, 0x90, 0x29,\n\t0xe2, 0x0d, 0x99, 0xb6, 0x47, 0x1d, 0x84, 0xbf, 0x11, 0x07, 0x73, 0x03, 0xe6, 0x08, 0x1b, 0x47,\n\t0x32, 0xf9, 0xf6, 0x8c, 0x72, 0x6a, 0x3f, 0x53, 0xd8, 0xdd, 0x02, 0xeb, 0x8f, 0xe6, 0x41, 0x04,\n\t0x8c, 0xe5, 0x5d, 0x00, 0x37, 0x7d, 0xc9, 0x72, 0x78, 0xdf, 0x53, 0x53, 0x45, 0x1a, 0x84, 0x19,\n\t0xdd, 0x84, 0x2c, 0x6d, 0x8a, 0xf9, 0x57, 0x9d, 0xc9, 0x0d, 0x33, 0x84, 0xb7, 0x32, 0x90, 0x13,\n\t0x48, 0x92, 0x33, 0x25, 0x90, 0x68, 0x0d, 0x76, 0x1c, 0x0b, 0x1f, 0xcf, 0xd8, 0x2b, 0xd3, 0x54,\n\t0x0f, 0x3b, 0xd2, 0x2b, 0xb3, 0xc4, 0x11, 0x9d, 0xb3, 0x69, 0xb6, 0x68, 0x03, 0xb4, 0x6e, 0xbb,\n\t0x87, 0x0d, 0x6b, 0xac, 0xa0, 0x31, 0x5c, 0x65, 0xb6, 0x7c, 0x97, 0x33, 0x90, 0x65, 0xe9, 0x3b,\n\t0x7c, 0xc1, 0x27, 0xf5, 0x0c, 0x2d, 0x57, 0x3b, 0xda, 0x79, 0x16, 0xde, 0x07, 0x85, 0x7a, 0x5f,\n\t0x1f, 0xfd, 0xf5, 0xf2, 0x3e, 0xf8, 0xd9, 0xa0, 0xe2, 0xe7, 0x85, 0x10, 0xa9, 0x1c, 0x56, 0x85,\n\t0xc2, 0x92, 0x12, 0x68, 0x24, 0xcb, 0xbe, 0x0c, 0x4b, 0xe6, 0xb8, 0x72, 0x6c, 0xe0, 0x8b, 0x02,\n\t0xb5, 0xda, 0xd1, 0x86, 0x54, 0x4e, 0x58, 0x37, 0x7c, 0x12, 0xeb, 0x80, 0xc4, 0x7e, 0x84, 0x0b,\n\t0xe9, 0xb0, 0x03, 0xa6, 0x2f, 0xa1, 0x47, 0x5f, 0x11, 0xda, 0xb2, 0xcb, 0x6a, 0xed, 0x7d, 0x3a,\n\t0xc9, 0x02, 0xa3, 0xbc, 0x9d, 0x5d, 0x80, 0x79, 0xee, 0x32, 0x85, 0x80, 0x08, 0x18, 0xa9, 0x66,\n\t0xf4, 0xb1, 0x66, 0xfa, 0x07, 0xee, 0xdb, 0x3f, 0x5e, 0x3a, 0xe0, 0xb2, 0x1f, 0xb0, 0x3c, 0xff,\n\t0x31, 0x15, 0x7d, 0xde, 0x0f, 0xdb, 0x37, 0x9f, 0xff, 0xc7, 0x2f, 0x45, 0xbe, 0xca, 0x8e, 0x27,\n\t0x05, 0xad, 0x42, 0x9a, 0x25, 0x40, 0x51, 0x7b, 0xca, 0xea, 0xbc, 0x14, 0x94, 0xee, 0xbb, 0xba,\n\t0xfe, 0x84, 0xaf, 0x3c, 0x73, 0xd4, 0xb9, 0x63, 0xb4, 0x9f, 0x8e, 0x86, 0x33, 0xc4, 0x18, 0x57,\n\t0x61, 0x59, 0x38, 0xff, 0xd2, 0xfc, 0x2d, 0xb6, 0xb1, 0x2c, 0x8d, 0xc9, 0x07, 0x23, 0xf6, 0x18,\n\t0xeb, 0xf1, 0xa8, 0xd7, 0xe3, 0x29, 0x05, 0xf4, 0xb7, 0x76, 0x9b, 0xaf, 0xae, 0x80, 0xe0, 0x71,\n\t0x70, 0xfa, 0x88, 0xd2, 0x05, 0xc9, 0x8c, 0x50, 0xed, 0x68, 0x3f, 0x52, 0xfc, 0xad, 0x83, 0x71,\n\t0x40, 0x64, 0x6b, 0xb4, 0x01, 0x27, 0x2c, 0xc6, 0xde, 0x12, 0x2d, 0x8e, 0x61, 0x5f, 0xe1, 0x55,\n\t0x0f, 0x3c, 0xc3, 0x0b, 0x1b, 0x67, 0x32, 0x74, 0x9c, 0x51, 0x1f, 0xf8, 0xb5, 0xfb, 0x7c, 0x51,\n\t0x07, 0xe1, 0xf2, 0xd1, 0x16, 0x61, 0xc5, 0x07, 0xc9, 0xc3, 0xbd, 0x2c, 0x01, 0xa2, 0x67, 0x09,\n\t0xdf, 0x94, 0x05, 0xc2, 0xf3, 0xe8, 0xa1, 0xc7, 0x9e, 0xb2, 0x93, 0x90, 0xa2, 0x4f, 0x0c, 0xf8,\n\t0x9c, 0xb1, 0x82, 0xe7, 0x32, 0x03, 0xa2, 0xb9, 0x35, 0xf5, 0xd9, 0x4d, 0xac, 0xbf, 0xbe, 0xd4,\n\t0xeb, 0xb9, 0xe8, 0x34, 0x58, 0xb4, 0xad, 0x76, 0x60, 0x90, 0xf3, 0xb6, 0xd5, 0x7e, 0x30, 0xab,\n\t0x5d, 0xf1, 0xef, 0x03, 0xe1, 0xe2, 0x38, 0xa2, 0x1f, 0x28, 0x7e, 0x48, 0x81, 0xed, 0x37, 0x0e,\n\t0xa4, 0x73, 0x00, 0x3c, 0xaa, 0x10, 0xae, 0x2f, 0x39, 0x25, 0x1c, 0x71, 0xb8, 0x85, 0xa8, 0x90,\n\t0x34, 0x7a, 0x3d, 0x9e, 0xab, 0x4f, 0x7e, 0x6a, 0xbf, 0x48, 0x00, 0x92, 0x01, 0xd2, 0x64, 0x17,\n\t0xff, 0x17, 0xe8, 0x00, 0xc8, 0x44, 0x10, 0xe4, 0x15, 0x58, 0x16, 0x78, 0xa8, 0x4d, 0x33, 0x14,\n\t0x8b, 0x1e, 0x17, 0xb5, 0x67, 0x29, 0x33, 0x75, 0x6e, 0x96, 0xcc, 0xd4, 0x3d, 0xe1, 0x15, 0x60,\n\t0x8a, 0xc6, 0x7f, 0x6f, 0x84, 0xc5, 0xae, 0xbe, 0xc1, 0x6c, 0xec, 0xf1, 0x36, 0x3c, 0x9d, 0xc3,\n\t0xed, 0x02, 0x95, 0xbc, 0xef, 0x9c, 0xec, 0xc5, 0xd4, 0xab, 0x53, 0x3a, 0x63, 0x7e, 0x99, 0x25,\n\t0xf2, 0xb3, 0x86, 0x85, 0xdb, 0xb0, 0x28, 0xf5, 0x3e, 0x53, 0xd4, 0xfb, 0x7f, 0xfd, 0xf6, 0x13,\n\t0x0c, 0x65, 0xde, 0x83, 0x0c, 0x5b, 0x3c, 0x6e, 0xc0, 0x7b, 0x29, 0xc6, 0x80, 0x75, 0xb7, 0x8d,\n\t0xf6, 0xef, 0x09, 0x38, 0x19, 0x36, 0x86, 0xc9, 0xab, 0xf4, 0x3d, 0x48, 0x9b, 0x43, 0x9a, 0xec,\n\t0xc3, 0x32, 0x75, 0x2e, 0x4f, 0x91, 0x59, 0x1f, 0x32, 0x9d, 0xb0, 0x46, 0x82, 0x5a, 0x93, 0xbf,\n\t0xa4, 0x5a, 0xc7, 0xa9, 0xd8, 0x1d, 0x93, 0x3f, 0x7b, 0x75, 0x53, 0xb1, 0xcb, 0xe6, 0x80, 0x04,\n\t0xe5, 0x40, 0x83, 0xd5, 0x16, 0x7d, 0x26, 0x12, 0x23, 0xb9, 0x99, 0x72, 0x93, 0x32, 0x2a, 0xc1,\n\t0x52, 0xdb, 0xec, 0x0f, 0xc9, 0x3a, 0xed, 0xb4, 0x62, 0xbe, 0x32, 0x59, 0xf4, 0x5a, 0xd0, 0x2e,\n\t0x04, 0x37, 0x9b, 0x91, 0xdc, 0xec, 0x43, 0xbf, 0x67, 0xe4, 0x9f, 0xc9, 0x67, 0x58, 0xe8, 0x27,\n\t0x21, 0x45, 0xce, 0xf4, 0x3d, 0xbe, 0x8d, 0xb2, 0x82, 0xf6, 0x0f, 0x81, 0xfd, 0xc6, 0xed, 0x99,\n\t0x9b, 0xc9, 0x43, 0xc8, 0x32, 0xcd, 0x79, 0x47, 0xfc, 0xdb, 0xb1, 0x94, 0x3e, 0x4e, 0x8a, 0xe1,\n\t0xad, 0xf9, 0x12, 0x71, 0x3b, 0x2b, 0x3c, 0x82, 0x45, 0xa9, 0x2a, 0xc4, 0xbe, 0x6f, 0xcb, 0xb9,\n\t0x0b, 0x97, 0xe3, 0x09, 0x16, 0x96, 0x41, 0x27, 0xb0, 0x15, 0x1b, 0x8e, 0xd1, 0x33, 0x0f, 0x5f,\n\t0xea, 0x8e, 0xa2, 0xdd, 0xf6, 0xef, 0x81, 0x9e, 0x14, 0xae, 0xc3, 0x02, 0x64, 0xdb, 0xe6, 0xc0,\n\t0xc1, 0x03, 0xc7, 0x7d, 0x0d, 0xe6, 0x95, 0xb5, 0x1f, 0x2b, 0xfc, 0x58, 0xe6, 0xb5, 0xbe, 0xd7,\n\t0x25, 0x43, 0x3c, 0xae, 0x3a, 0xb8, 0x1f, 0x6b, 0x62, 0x25, 0xa7, 0x97, 0x98, 0xc5, 0xe9, 0xfd,\n\t0xea, 0xcb, 0x49, 0xbb, 0xe3, 0xd7, 0x30, 0x47, 0x3f, 0x83, 0x65, 0x6a, 0x03, 0xbf, 0xfe, 0xbc,\n\t0x3e, 0xb8, 0xfe, 0xf6, 0x60, 0xe1, 0x09, 0x23, 0xb5, 0x7a, 0x5d, 0xdb, 0x4d, 0xc6, 0x2f, 0x4e,\n\t0x41, 0x2b, 0xe8, 0x51, 0x9f, 0xe7, 0xed, 0x77, 0xbb, 0xb6, 0x43, 0x76, 0xce, 0xf5, 0xe0, 0xc0,\n\t0x30, 0x4b, 0x0c, 0x9c, 0x65, 0x49, 0x3d, 0x80, 0x65, 0x8b, 0xb1, 0x7b, 0x0f, 0x9c, 0x98, 0x5b,\n\t0x7b, 0x7d, 0x0a, 0x34, 0xdd, 0x6d, 0xc5, 0x5e, 0x3c, 0x2d, 0x59, 0x52, 0x99, 0x67, 0x5d, 0x44,\n\t0xe1, 0x63, 0x4a, 0x29, 0xfe, 0x67, 0x02, 0xd2, 0xdc, 0xe5, 0x2e, 0xc3, 0x7c, 0xa3, 0x59, 0x6a,\n\t0x1e, 0x34, 0x5a, 0xb5, 0x7a, 0xad, 0xa2, 0xbe, 0x22, 0x10, 0xaa, 0xb5, 0x6a, 0x53, 0x55, 0xd0,\n\t0x22, 0xe4, 0x38, 0xa1, 0x7e, 0x5f, 0x4d, 0x20, 0x04, 0x4b, 0x6e, 0x71, 0x67, 0x67, 0xb7, 0x5a,\n\t0xab, 0xa8, 0x49, 0xa4, 0xc2, 0x02, 0xa7, 0x55, 0x74, 0xbd, 0xae, 0xab, 0x73, 0x28, 0x0f, 0x27,\n\t0xbd, 0x6e, 0x9b, 0xad, 0x6a, 0xad, 0xf5, 0xc5, 0x83, 0xba, 0x7e, 0xb0, 0xa7, 0xa6, 0xd0, 0x69,\n\t0x38, 0xc1, 0x6b, 0xca, 0x95, 0xed, 0xfa, 0xde, 0x5e, 0xb5, 0xd1, 0xa8, 0xd6, 0x6b, 0x6a, 0x1a,\n\t0xad, 0x02, 0xe2, 0x15, 0x7b, 0xa5, 0x6a, 0xad, 0x59, 0xa9, 0x95, 0x6a, 0xdb, 0x15, 0x35, 0x23,\n\t0x34, 0x68, 0x34, 0xeb, 0x7a, 0xe9, 0x6e, 0xa5, 0x55, 0xae, 0x3f, 0xac, 0xa9, 0x59, 0x74, 0x16,\n\t0x4e, 0xfb, 0x2b, 0x2a, 0x77, 0xf5, 0x52, 0xb9, 0x52, 0x56, 0x73, 0x42, 0xab, 0x5a, 0xa5, 0x52,\n\t0x6e, 0xb4, 0xf4, 0xca, 0x9d, 0x7a, 0xbd, 0xa9, 0x02, 0x5a, 0x83, 0xbc, 0xaf, 0x95, 0x5e, 0xb9,\n\t0x53, 0xda, 0xa5, 0xc2, 0xe6, 0xd1, 0x3a, 0xac, 0xf9, 0xfb, 0xd4, 0xab, 0x0f, 0x08, 0xcf, 0xfe,\n\t0x6e, 0x69, 0xbb, 0xa2, 0x2e, 0xa0, 0x4b, 0x70, 0x21, 0x6c, 0x64, 0xad, 0x5a, 0xdd, 0x6d, 0xa2,\n\t0x2e, 0xa2, 0x25, 0x00, 0x6f, 0x2c, 0x1f, 0xaa, 0x4b, 0xc5, 0xef, 0x2b, 0x00, 0x2c, 0x85, 0x94,\n\t0xbe, 0x8f, 0x39, 0x09, 0x2a, 0xed, 0x56, 0x6f, 0x35, 0x3f, 0xda, 0xaf, 0xb8, 0x9a, 0xf7, 0x51,\n\t0x77, 0xaa, 0xbb, 0x15, 0x55, 0x41, 0xa7, 0x60, 0x45, 0xa4, 0xde, 0xd9, 0xad, 0x6f, 0x93, 0x69,\n\t0x58, 0x05, 0x24, 0x92, 0xeb, 0x77, 0x3e, 0xa8, 0x6c, 0x37, 0xd5, 0x24, 0x3a, 0x03, 0xa7, 0x44,\n\t0xfa, 0xf6, 0xee, 0x41, 0xa3, 0x59, 0xd1, 0x2b, 0x65, 0x75, 0xce, 0xdf, 0xd3, 0x5d, 0xbd, 0xb4,\n\t0x7f, 0x4f, 0x4d, 0x15, 0xbf, 0xa7, 0x40, 0x9a, 0x3d, 0x04, 0x24, 0xf3, 0xb8, 0xd3, 0x90, 0x30,\n\t0xad, 0xc0, 0xa2, 0x4b, 0xb9, 0xd3, 0xd4, 0x77, 0x1a, 0xaa, 0x22, 0x32, 0x55, 0x3e, 0x6c, 0xbe,\n\t0xa5, 0x26, 0x44, 0xca, 0xce, 0x41, 0x83, 0x18, 0xc4, 0x32, 0xcc, 0x7b, 0x1d, 0xed, 0x34, 0xd4,\n\t0x39, 0x91, 0xf0, 0x60, 0xa7, 0xa1, 0xa6, 0x44, 0xc2, 0x87, 0x3b, 0x0d, 0x35, 0x2d, 0x12, 0xbe,\n\t0xb4, 0xd3, 0x50, 0x33, 0xc5, 0x1f, 0x2a, 0x70, 0x2a, 0x34, 0xf7, 0x16, 0x5d, 0x84, 0x73, 0x14,\n\t0x7c, 0x8b, 0x0f, 0x67, 0xfb, 0x5e, 0xa9, 0x76, 0xb7, 0x22, 0xe1, 0xbe, 0x0c, 0x17, 0x23, 0x59,\n\t0xf6, 0xea, 0xe5, 0xea, 0x4e, 0xb5, 0x52, 0x56, 0x15, 0xa4, 0xc1, 0xf9, 0x48, 0xb6, 0x52, 0x99,\n\t0x58, 0x52, 0x02, 0x7d, 0x0e, 0xd6, 0x23, 0x79, 0xca, 0x95, 0xdd, 0x4a, 0xb3, 0x52, 0x56, 0x93,\n\t0x45, 0x07, 0x16, 0xc4, 0xb7, 0x52, 0xd4, 0x9a, 0x2b, 0x0f, 0x2a, 0x7a, 0xb5, 0xf9, 0x91, 0x04,\n\t0x8c, 0xd8, 0xa5, 0x44, 0x2f, 0xed, 0x96, 0xf4, 0x3d, 0x55, 0x21, 0x13, 0x27, 0x57, 0x3c, 0x2c,\n\t0xe9, 0xb5, 0x6a, 0xed, 0xae, 0x9a, 0xa0, 0x8b, 0xc9, 0xd7, 0x57, 0xb3, 0xba, 0xf3, 0x91, 0x9a,\n\t0x2c, 0x7e, 0x53, 0x81, 0x05, 0xf1, 0x36, 0x85, 0x88, 0xd5, 0x2b, 0x8d, 0xfa, 0x81, 0xbe, 0x2d,\n\t0xeb, 0x23, 0x0f, 0x27, 0x65, 0xfa, 0x83, 0xfa, 0xee, 0xc1, 0x1e, 0xb1, 0xaf, 0x90, 0x16, 0xe5,\n\t0x8a, 0x9a, 0x20, 0x78, 0x64, 0x3a, 0x37, 0x25, 0x35, 0x49, 0xc6, 0x20, 0x57, 0x51, 0xcd, 0xa8,\n\t0x73, 0xc5, 0xaf, 0x2b, 0xb0, 0x4c, 0x6f, 0x67, 0xd8, 0xbb, 0x05, 0x8a, 0xa8, 0x00, 0xab, 0xa5,\n\t0xdd, 0x8a, 0xde, 0x6c, 0x95, 0xb6, 0x9b, 0xd5, 0x7a, 0x4d, 0x42, 0xb5, 0x06, 0xf9, 0x60, 0x1d,\n\t0xd3, 0xa9, 0xaa, 0x84, 0xd7, 0x6e, 0xeb, 0x95, 0x52, 0x93, 0xe0, 0x0b, 0xad, 0x3d, 0xd8, 0x2f,\n\t0x93, 0xda, 0x64, 0xf1, 0x63, 0xf7, 0x89, 0x82, 0xf0, 0x82, 0x84, 0x34, 0x61, 0xc3, 0x76, 0xdb,\n\t0xec, 0x97, 0xf4, 0xd2, 0x9e, 0x0b, 0xe6, 0x2c, 0x9c, 0x0e, 0xab, 0xad, 0xef, 0xec, 0xa8, 0x0a,\n\t0x19, 0x45, 0x68, 0x65, 0x4d, 0x4d, 0x14, 0xb7, 0x20, 0xc3, 0xff, 0x86, 0x01, 0xca, 0xc2, 0x1c,\n\t0xef, 0x2d, 0x03, 0xc9, 0xdd, 0xfa, 0x43, 0x55, 0x41, 0x00, 0xe9, 0xbd, 0x4a, 0xb9, 0x7a, 0xb0,\n\t0xa7, 0x26, 0x48, 0xf5, 0xbd, 0xea, 0xdd, 0x7b, 0x6a, 0xb2, 0xf8, 0x35, 0xc8, 0x79, 0x7f, 0xc4,\n\t0x80, 0xa8, 0xba, 0x5a, 0x6f, 0xed, 0xeb, 0x75, 0xb2, 0xe4, 0x5b, 0x8d, 0xca, 0x17, 0x0f, 0x2a,\n\t0xb5, 0x66, 0xb5, 0xb4, 0xab, 0xbe, 0x42, 0xd6, 0xac, 0x50, 0xa5, 0x97, 0x6a, 0xe5, 0x3a, 0x31,\n\t0x96, 0x15, 0x58, 0x14, 0xc8, 0xe5, 0x3b, 0xcc, 0x48, 0x24, 0x52, 0x4b, 0xaf, 0xec, 0xd5, 0x89,\n\t0x2e, 0x88, 0xc7, 0x16, 0x6a, 0xb6, 0xf7, 0x1a, 0xea, 0x5c, 0xf1, 0xfb, 0x09, 0x98, 0x17, 0xde,\n\t0x99, 0x10, 0x39, 0x7c, 0x7c, 0xc4, 0x6f, 0x89, 0x66, 0x23, 0x91, 0xf7, 0x2b, 0xb5, 0x32, 0xb1,\n\t0x49, 0x51, 0x21, 0xac, 0xa6, 0xf4, 0xa0, 0x54, 0xdd, 0x2d, 0xdd, 0xd9, 0xe5, 0xa6, 0x23, 0xd7,\n\t0x35, 0x9b, 0xa5, 0xed, 0x7b, 0x64, 0x99, 0x04, 0xaa, 0xca, 0x15, 0x5e, 0x35, 0x27, 0xe8, 0x7f,\n\t0x5c, 0xd5, 0xdc, 0xbe, 0x47, 0xc4, 0xa5, 0x88, 0x95, 0x4a, 0x95, 0x6c, 0x9f, 0x49, 0x07, 0x00,\n\t0xba, 0x0b, 0x32, 0x83, 0xce, 0x43, 0x41, 0xaa, 0x69, 0xea, 0x1f, 0x71, 0x69, 0xa4, 0xc7, 0x6c,\n\t0xa0, 0xa5, 0x5e, 0x21, 0xee, 0xbb, 0xa2, 0xe6, 0x8a, 0xdf, 0x51, 0x60, 0x41, 0x7c, 0xe8, 0xec,\n\t0x13, 0x3e, 0xde, 0x2a, 0xcf, 0xc1, 0x19, 0x3f, 0xbd, 0xd9, 0xda, 0xd7, 0x2b, 0x8d, 0x4a, 0x8d,\n\t0x6c, 0x9c, 0x27, 0x41, 0x95, 0xab, 0x0f, 0xf6, 0x99, 0xe3, 0x96, 0xa9, 0x74, 0x37, 0x4b, 0xfa,\n\t0x14, 0x4a, 0xb7, 0x47, 0xbe, 0x99, 0xcd, 0x15, 0xbf, 0x42, 0xe2, 0x5d, 0xe1, 0x0f, 0xbc, 0xb0,\n\t0xad, 0x8f, 0xed, 0x4f, 0xcc, 0xb8, 0x5a, 0x7b, 0xa5, 0xbb, 0xb5, 0x4a, 0xb3, 0xba, 0xad, 0xbe,\n\t0xc2, 0x36, 0x52, 0xa9, 0xb2, 0xd1, 0x20, 0xce, 0x8e, 0x6e, 0x89, 0x12, 0xbd, 0xf6, 0x60, 0xaf,\n\t0xa2, 0x26, 0x8a, 0xd7, 0x60, 0x91, 0x5f, 0xad, 0xd6, 0x4c, 0xa7, 0xfb, 0xf8, 0x98, 0x70, 0xf2,\n\t0xd5, 0xce, 0x5d, 0x0d, 0x03, 0xf9, 0x4a, 0x11, 0xc3, 0xbc, 0xf0, 0xdc, 0x9a, 0xcc, 0x26, 0x9b,\n\t0x5b, 0x77, 0x56, 0x3e, 0x6c, 0x56, 0xf4, 0x1a, 0x35, 0x5c, 0x7f, 0x15, 0xd9, 0xd1, 0x69, 0x95,\n\t0x42, 0xf6, 0xd8, 0xd0, 0xaa, 0x56, 0xe3, 0x61, 0xb5, 0xb9, 0x7d, 0x4f, 0x4d, 0x14, 0x9b, 0xb0,\n\t0x54, 0x1f, 0x92, 0x23, 0x67, 0xd7, 0x1c, 0xec, 0xf4, 0x8c, 0x43, 0x9b, 0xe8, 0xb2, 0xbe, 0xdf,\n\t0xda, 0xd9, 0x2d, 0xdd, 0x6d, 0xb4, 0x0e, 0x6a, 0xf7, 0x6b, 0x14, 0x0e, 0x59, 0x06, 0x1e, 0x95,\n\t0xce, 0x09, 0x75, 0xa3, 0x1e, 0x89, 0x4d, 0x77, 0x6b, 0xa7, 0xae, 0x6f, 0x93, 0x61, 0xfe, 0x3f,\n\t0xff, 0xa9, 0x93, 0x9d, 0x10, 0xd1, 0x05, 0xff, 0x01, 0x89, 0xd1, 0x0f, 0x06, 0x4f, 0x07, 0xe6,\n\t0x27, 0x03, 0xf5, 0x15, 0x1a, 0x14, 0x84, 0x30, 0xb8, 0xbf, 0x55, 0x85, 0xec, 0x48, 0xa1, 0x87,\n\t0x4f, 0x76, 0x49, 0x55, 0x1f, 0xaa, 0x89, 0xe2, 0x4f, 0x12, 0xfc, 0xc3, 0x4b, 0x48, 0x48, 0x4c,\n\t0x83, 0x8a, 0x88, 0xba, 0x31, 0x8c, 0x2b, 0x2c, 0x8d, 0x2a, 0x8c, 0xa9, 0x66, 0x3a, 0xf4, 0xd3,\n\t0x07, 0xee, 0x30, 0xfd, 0x46, 0xf1, 0x91, 0x73, 0xaa, 0x9a, 0x98, 0x24, 0xae, 0xf4, 0xc8, 0xa4,\n\t0xdd, 0x24, 0xc9, 0xde, 0x18, 0xc5, 0xb4, 0x6f, 0x8c, 0x6c, 0xdc, 0x51, 0xe7, 0x26, 0x75, 0xd4,\n\t0x70, 0xcc, 0xe1, 0x10, 0x77, 0xd4, 0xd4, 0xa4, 0x8e, 0xd8, 0x7b, 0x10, 0x35, 0x3d, 0x89, 0x67,\n\t0xc7, 0xe8, 0xf6, 0x70, 0x47, 0xcd, 0x14, 0x7f, 0x1c, 0x72, 0xbf, 0x29, 0xc6, 0xbe, 0xe8, 0x2a,\n\t0xcb, 0xd9, 0x8a, 0xaa, 0x1f, 0x6b, 0xf2, 0xb2, 0x3f, 0x48, 0x96, 0x19, 0xe9, 0xf0, 0x54, 0x25,\n\t0xa8, 0x70, 0x5f, 0xec, 0x8d, 0xed, 0x51, 0x1f, 0xb3, 0x08, 0x61, 0x12, 0x1f, 0xd1, 0x84, 0x9a,\n\t0xdc, 0xfa, 0xfb, 0x14, 0xa0, 0xfa, 0x10, 0x0f, 0x7c, 0x69, 0xed, 0xcf, 0x21, 0xe7, 0x5d, 0xb0,\n\t0xa0, 0xd7, 0xc2, 0x83, 0xff, 0xd0, 0x6f, 0x84, 0x85, 0xeb, 0xf1, 0x98, 0xf9, 0x9d, 0xdf, 0x89,\n\t0x5f, 0xff, 0xf9, 0xbf, 0x7c, 0x37, 0xb1, 0x88, 0xe6, 0x37, 0x8f, 0xde, 0xd8, 0xe4, 0x97, 0x74,\n\t0xe8, 0xff, 0x43, 0x86, 0x7f, 0xe1, 0x40, 0xaf, 0x4e, 0xe8, 0x4d, 0xfe, 0x98, 0x52, 0x28, 0xc6,\n\t0x61, 0xe5, 0x62, 0xcf, 0x51, 0xb1, 0xa7, 0xd1, 0x29, 0x41, 0xec, 0xe6, 0x73, 0x7e, 0x53, 0xf1,\n\t0x02, 0xfd, 0x91, 0x02, 0x4b, 0xf2, 0xf7, 0x32, 0x74, 0x63, 0x42, 0xef, 0xa1, 0x9f, 0x0a, 0x0b,\n\t0x6f, 0xcc, 0xd0, 0x82, 0xc3, 0xba, 0x42, 0x61, 0xad, 0x6b, 0x67, 0x45, 0x58, 0xf4, 0x6b, 0xd3,\n\t0x26, 0x76, 0x99, 0x6f, 0x29, 0x45, 0xf4, 0x2d, 0x05, 0x60, 0xfc, 0x15, 0x0c, 0x5d, 0x9f, 0x26,\n\t0x49, 0xfc, 0x42, 0x57, 0x78, 0x3d, 0x26, 0x37, 0xc7, 0xa4, 0x51, 0x4c, 0x6b, 0xda, 0xe9, 0x20,\n\t0x26, 0xfa, 0x1c, 0x9d, 0xe0, 0xf9, 0x1d, 0x05, 0xe6, 0x85, 0xef, 0x5f, 0x68, 0xaa, 0x08, 0xe9,\n\t0x1a, 0xbc, 0xb0, 0x11, 0x97, 0x9d, 0x43, 0xfa, 0x1c, 0x85, 0x74, 0xbe, 0xb8, 0x16, 0x84, 0xf4,\n\t0xdc, 0xfd, 0x56, 0xf7, 0x62, 0xeb, 0xef, 0x16, 0x60, 0x45, 0xb0, 0x6c, 0xfe, 0x57, 0x5f, 0x2c,\n\t0x48, 0xb3, 0xaf, 0x17, 0xe8, 0x6a, 0xf4, 0xa7, 0x7c, 0xe9, 0xc3, 0x4a, 0xe1, 0xda, 0x74, 0x46,\n\t0x0e, 0xec, 0x14, 0x05, 0xb6, 0xac, 0x01, 0x01, 0xc6, 0x0e, 0xda, 0x44, 0x3d, 0x23, 0x48, 0xd1,\n\t0x6c, 0x1e, 0x74, 0x65, 0x42, 0x4f, 0x42, 0xa6, 0x51, 0xe1, 0xea, 0x54, 0x3e, 0x2e, 0xf0, 0x2c,\n\t0x15, 0x78, 0x4a, 0x53, 0xc7, 0x02, 0x37, 0xdb, 0x84, 0x83, 0x88, 0x7d, 0x0e, 0x69, 0x3e, 0x1f,\n\t0x13, 0xfa, 0x93, 0x67, 0xe2, 0xda, 0x74, 0x46, 0x2e, 0xf9, 0x3c, 0x95, 0x9c, 0x2f, 0xae, 0x0a,\n\t0x92, 0x9f, 0x7b, 0x77, 0x0b, 0x2f, 0xd0, 0xd7, 0xc6, 0x6b, 0x78, 0x42, 0xa7, 0xbe, 0x25, 0xfc,\n\t0x6a, 0x0c, 0x4e, 0x59, 0x3e, 0x8a, 0x92, 0xff, 0x1b, 0x8a, 0xe8, 0xc1, 0x8a, 0xd1, 0x1d, 0x07,\n\t0xd6, 0xed, 0x6b, 0xb1, 0x78, 0x39, 0x8c, 0x0b, 0x14, 0xc6, 0x19, 0xed, 0xa4, 0x00, 0x43, 0x5a,\n\t0xaa, 0xdf, 0x55, 0xc6, 0x4f, 0xe2, 0xb8, 0xe1, 0x6d, 0xce, 0x98, 0xdf, 0x53, 0xb8, 0x11, 0xbf,\n\t0x81, 0xac, 0x1d, 0xed, 0x84, 0x00, 0xcb, 0x4d, 0xf2, 0x20, 0xa8, 0x7e, 0xa0, 0xc0, 0xb2, 0x2f,\n\t0x6b, 0x06, 0xc5, 0x90, 0x22, 0x7f, 0xb9, 0x0b, 0xf7, 0x70, 0x93, 0x53, 0x72, 0x24, 0x0f, 0xe7,\n\t0x03, 0xb6, 0xc9, 0xbf, 0x9e, 0x11, 0x80, 0x7f, 0xa6, 0xb0, 0xd7, 0xd3, 0x52, 0x72, 0x0b, 0xda,\n\t0x9a, 0x3d, 0xfb, 0xa6, 0xf0, 0xe6, 0x4c, 0x6d, 0x38, 0xcc, 0x6b, 0x14, 0xa6, 0xa6, 0x9d, 0x0b,\n\t0x83, 0x29, 0xcd, 0xef, 0x31, 0xa4, 0x59, 0xec, 0x39, 0x69, 0x91, 0x49, 0x79, 0x9d, 0x93, 0x16,\n\t0x99, 0x9c, 0x94, 0xa9, 0xad, 0x51, 0x18, 0xab, 0xda, 0x8a, 0x00, 0x83, 0xfd, 0xa9, 0x02, 0x2e,\n\t0x9a, 0x65, 0x54, 0x4e, 0x5e, 0xdf, 0x31, 0x45, 0xfb, 0x92, 0x33, 0xc3, 0x44, 0x77, 0xb0, 0x2b,\n\t0x7a, 0x04, 0x29, 0x9a, 0x1e, 0x3c, 0xc9, 0xa3, 0x89, 0xa9, 0xca, 0x93, 0x3c, 0x9a, 0x9c, 0x67,\n\t0x1c, 0xe6, 0xd1, 0x68, 0x22, 0x2e, 0x11, 0xfb, 0x02, 0x32, 0x3c, 0x2d, 0x77, 0x92, 0x53, 0x91,\n\t0xd3, 0x84, 0x27, 0x39, 0x15, 0x7f, 0x8e, 0x2f, 0x0f, 0x0b, 0x34, 0x24, 0x08, 0x1f, 0x0d, 0x5c,\n\t0xf1, 0x5b, 0x3f, 0x9b, 0x83, 0x55, 0x61, 0x47, 0x11, 0xbe, 0xd4, 0x93, 0x1d, 0xd0, 0xf3, 0x77,\n\t0xa1, 0xbb, 0x5f, 0x64, 0x12, 0x48, 0xf8, 0xee, 0x17, 0x9d, 0xec, 0xe1, 0xda, 0x26, 0x5a, 0x27,\n\t0x20, 0x85, 0xcc, 0x82, 0xcd, 0xe7, 0x72, 0xf2, 0xc1, 0x0b, 0xf4, 0x6b, 0x8a, 0xb7, 0xd9, 0x5d,\n\t0x9f, 0x22, 0x44, 0x76, 0x38, 0xaf, 0xc7, 0xe4, 0xe6, 0x88, 0x0a, 0x14, 0xd1, 0x49, 0x6d, 0xd9,\n\t0x87, 0x88, 0x4c, 0xd9, 0x77, 0x14, 0x6f, 0x17, 0x9a, 0x86, 0x41, 0xde, 0x8a, 0x5e, 0x8f, 0xc9,\n\t0x2d, 0x6b, 0xa5, 0x38, 0x5d, 0x2b, 0xbf, 0xa9, 0x40, 0x9a, 0x65, 0x56, 0x4c, 0x45, 0x24, 0xa5,\n\t0x77, 0x4c, 0x45, 0xe4, 0x4b, 0xd7, 0xb8, 0x48, 0x11, 0x9d, 0xd5, 0x56, 0xfd, 0x88, 0x46, 0x94,\n\t0x8f, 0x18, 0xd4, 0xbf, 0xe6, 0x24, 0x83, 0x1a, 0xbf, 0x3c, 0xb0, 0x89, 0xde, 0x16, 0x98, 0x9a,\n\t0x77, 0x4c, 0xab, 0xf4, 0xb0, 0x81, 0xc2, 0x83, 0xa4, 0xc8, 0x87, 0x5a, 0x85, 0xcd, 0xd8, 0xfc,\n\t0x61, 0x7b, 0xc6, 0xf8, 0x83, 0x90, 0xbd, 0x69, 0x7c, 0x62, 0x93, 0x99, 0xfc, 0x7d, 0x05, 0x96,\n\t0xc6, 0x88, 0x3e, 0x1d, 0x45, 0x6d, 0x19, 0x93, 0xde, 0xe6, 0x45, 0x04, 0xc5, 0x93, 0x1e, 0xda,\n\t0x69, 0xeb, 0x14, 0x57, 0x41, 0x3b, 0x15, 0xc0, 0x45, 0xd8, 0x08, 0xb2, 0x3f, 0x50, 0x60, 0xd9,\n\t0x43, 0xc6, 0x1e, 0xa2, 0xa0, 0x58, 0x82, 0xa4, 0x57, 0x33, 0x85, 0xad, 0x59, 0x9a, 0x84, 0x4d,\n\t0xb2, 0x08, 0x8e, 0x7d, 0xae, 0x22, 0xe8, 0xbe, 0xa7, 0xc0, 0xb2, 0xb7, 0xc3, 0xf0, 0xc9, 0x9c,\n\t0x82, 0x2e, 0xe4, 0xdd, 0xd4, 0x34, 0x74, 0x61, 0x2f, 0xa4, 0x5c, 0x67, 0x8a, 0xc2, 0xa6, 0x94,\n\t0x1c, 0x72, 0x56, 0x24, 0x5c, 0x74, 0x4a, 0xdf, 0x8c, 0x2b, 0x46, 0x9c, 0xd5, 0xb7, 0x66, 0x6b,\n\t0x14, 0x7a, 0x08, 0xf3, 0x4f, 0x2c, 0xfa, 0x13, 0x05, 0x90, 0x88, 0x8f, 0x4f, 0x6c, 0x5c, 0x59,\n\t0xf2, 0xdc, 0xde, 0x9c, 0xb1, 0x55, 0x58, 0x94, 0x19, 0x9c, 0x5e, 0x12, 0xdd, 0xb9, 0xde, 0xed,\n\t0xb5, 0xc9, 0x12, 0x64, 0xe7, 0x76, 0x3d, 0x1e, 0x33, 0x47, 0xb1, 0x41, 0x51, 0x5c, 0x2b, 0x5e,\n\t0xf1, 0xa3, 0xe8, 0x50, 0xbe, 0xcd, 0xe7, 0xd2, 0xd3, 0xa6, 0x17, 0xe8, 0xdb, 0x0a, 0x64, 0xdd,\n\t0xb7, 0x4b, 0x11, 0x67, 0xb1, 0xa8, 0x87, 0x52, 0x85, 0x8d, 0xb8, 0xec, 0xee, 0x7b, 0x6a, 0x8a,\n\t0xed, 0x9c, 0x96, 0xf7, 0x63, 0x3b, 0xe2, 0x9c, 0xc4, 0xcf, 0xfd, 0x57, 0x0a, 0xce, 0x08, 0x7e,\n\t0xce, 0xf7, 0x04, 0xf8, 0x1b, 0xe3, 0x6d, 0x6a, 0x73, 0xfa, 0x3b, 0xe5, 0x18, 0xa1, 0xf1, 0xc4,\n\t0x17, 0xe9, 0xd2, 0x1e, 0xef, 0x3e, 0x2b, 0x66, 0x4f, 0x9f, 0xc9, 0x6a, 0xfd, 0xed, 0xf1, 0xee,\n\t0x10, 0x03, 0x8c, 0xbc, 0x41, 0xdc, 0x88, 0xdf, 0x40, 0x3e, 0xc9, 0x6a, 0x67, 0x82, 0x60, 0xc6,\n\t0xdb, 0x04, 0xfa, 0x2d, 0xe9, 0x2c, 0xb3, 0x35, 0x5d, 0x4a, 0xbc, 0x20, 0x78, 0xca, 0xfb, 0x76,\n\t0x77, 0x5b, 0x47, 0x21, 0x9a, 0x22, 0x6a, 0xf2, 0xe2, 0x9d, 0x18, 0xc3, 0xf6, 0x85, 0x3c, 0x6f,\n\t0xcc, 0xd0, 0x42, 0x76, 0xb4, 0x28, 0x4c, 0x53, 0xcf, 0x07, 0x46, 0x1f, 0x53, 0xb3, 0x77, 0x17,\n\t0x63, 0x8c, 0xa9, 0x93, 0x17, 0xe4, 0x8d, 0xf8, 0x0d, 0x64, 0x40, 0xc5, 0x68, 0x40, 0x5b, 0x7f,\n\t0x0d, 0xf2, 0xf6, 0x3e, 0xbe, 0x8a, 0x9b, 0x1a, 0x9a, 0x45, 0x65, 0x79, 0x16, 0x5e, 0x8f, 0xc9,\n\t0x1d, 0x16, 0x9a, 0xb5, 0x09, 0x1b, 0xcb, 0x06, 0x21, 0x66, 0xf5, 0x4d, 0x05, 0x32, 0xee, 0xe1,\n\t0x6f, 0xfa, 0xf7, 0x7d, 0xe9, 0xe4, 0xb7, 0x11, 0x97, 0x3d, 0xfc, 0x12, 0xc9, 0x83, 0x21, 0x1e,\n\t0xf9, 0xbe, 0x31, 0x25, 0x52, 0x8c, 0xca, 0xa2, 0x9c, 0xaa, 0x12, 0xdf, 0xc4, 0x71, 0x8f, 0x55,\n\t0x3c, 0xeb, 0xc7, 0xf2, 0xdc, 0xcb, 0x9c, 0x79, 0x81, 0x7e, 0x57, 0x81, 0x9c, 0x97, 0xa1, 0x18,\n\t0x6e, 0x4e, 0x13, 0x52, 0x27, 0x0b, 0x37, 0xe2, 0x37, 0x08, 0xf3, 0x04, 0x22, 0x2a, 0xe6, 0xe3,\n\t0x8d, 0x5e, 0x8f, 0xe8, 0xe8, 0xeb, 0x92, 0x27, 0x98, 0x06, 0x2b, 0xe0, 0x06, 0x6e, 0xc4, 0x6f,\n\t0xc0, 0x61, 0x9d, 0xa6, 0xb0, 0x56, 0x90, 0xdf, 0x7e, 0x48, 0x54, 0xe3, 0x26, 0x6b, 0x5c, 0x8f,\n\t0x99, 0x3e, 0x15, 0x6f, 0xb6, 0xe4, 0x64, 0x2b, 0x77, 0xef, 0x43, 0x57, 0xfc, 0x7a, 0x61, 0xe9,\n\t0x3a, 0x9b, 0xcf, 0xa5, 0x9c, 0x96, 0x17, 0x04, 0x57, 0x86, 0x67, 0x2b, 0x4d, 0x35, 0x6a, 0x39,\n\t0x77, 0x6a, 0xaa, 0x51, 0xfb, 0x92, 0xa0, 0xb4, 0xd7, 0x28, 0xb4, 0xcb, 0xe8, 0x92, 0x1f, 0x5a,\n\t0x9b, 0x31, 0x4a, 0x06, 0xf5, 0x7d, 0x05, 0x32, 0x3c, 0x7f, 0x67, 0x2a, 0x2e, 0x39, 0xe3, 0x68,\n\t0x2a, 0x2e, 0x5f, 0x72, 0x91, 0xb6, 0x49, 0x71, 0xbd, 0x8a, 0xae, 0xfa, 0x71, 0xf1, 0x94, 0xa1,\n\t0x80, 0xce, 0xfe, 0x50, 0x81, 0x79, 0x21, 0x21, 0x07, 0xbd, 0x11, 0x63, 0x8a, 0xe4, 0xe4, 0xa2,\n\t0x88, 0xe8, 0x74, 0x62, 0xbe, 0x8f, 0xff, 0xb6, 0x5b, 0x9e, 0x5a, 0xdc, 0xa6, 0xcc, 0xb7, 0x94,\n\t0xe2, 0x9d, 0x35, 0x38, 0xd1, 0x36, 0xfb, 0x7e, 0x01, 0xfb, 0xca, 0x97, 0x92, 0xc6, 0xb0, 0xfb,\n\t0x28, 0x4d, 0x33, 0xc2, 0xde, 0xfc, 0xef, 0x00, 0x00, 0x00, 0xff, 0xff, 0xcc, 0x31, 0x70, 0xa0,\n\t0xc9, 0x64, 0x00, 0x00,\n}\n", "idx": 156, "id": 7147, "msg": "", "proj": "libopenstorage-openstorage", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -54,8 +54,8 @@ void handle_keyboard_key(struct libinput_event *event,\n \tstruct libinput_event_keyboard *kbevent =\n \t\tlibinput_event_get_keyboard_event(event);\n \tstruct wlr_event_keyboard_key wlr_event = { 0 };\n-\twlr_event.time_sec = libinput_event_keyboard_get_time(kbevent);\n \twlr_event.time_usec = libinput_event_keyboard_get_time_usec(kbevent);\n+\twlr_event.time_msec = (uint32_t)(wlr_event.time_usec / 1000);\n \twlr_event.keycode = libinput_event_keyboard_get_key(kbevent);\n \tenum libinput_key_state state = \n \t\tlibinput_event_keyboard_get_key_state(kbevent);", "y": 1, "oldf": "#include <stdlib.h>\n#include <assert.h>\n#include <libinput.h>\n#include <wlr/backend/session.h>\n#include <wlr/types/wlr_input_device.h>\n#include <wlr/interfaces/wlr_keyboard.h>\n#include <wlr/util/log.h>\n#include \"backend/libinput.h\"\n\nstruct wlr_libinput_keyboard {\n\tstruct wlr_keyboard wlr_keyboard;\n\tstruct libinput_device *libinput_dev;\n};\n\nstatic void wlr_libinput_keyboard_set_leds(struct wlr_keyboard *wlr_kb, uint32_t leds) {\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb = (struct wlr_libinput_keyboard *)wlr_kb;\n\tlibinput_device_led_update(wlr_libinput_kb->libinput_dev, leds);\n}\n\nstatic void wlr_libinput_keyboard_destroy(struct wlr_keyboard *wlr_kb) {\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb =\n\t\t(struct wlr_libinput_keyboard *)wlr_kb;\n\tlibinput_device_unref(wlr_libinput_kb->libinput_dev);\n}\n\nstruct wlr_keyboard_impl impl = {\n\t.destroy = wlr_libinput_keyboard_destroy,\n\t.led_update = wlr_libinput_keyboard_set_leds\n};\n\nstruct wlr_keyboard *wlr_libinput_keyboard_create(\n\t\tstruct libinput_device *libinput_dev) {\n\tassert(libinput_dev);\n\tstruct wlr_libinput_keyboard *wlr_libinput_kb;\n\tif (!(wlr_libinput_kb= calloc(1, sizeof(struct wlr_libinput_keyboard)))) {\n\t\treturn NULL;\n\t}\n\twlr_libinput_kb->libinput_dev = libinput_dev;\n\tlibinput_device_ref(libinput_dev);\n\tlibinput_device_led_update(libinput_dev, 0);\n\tstruct wlr_keyboard *wlr_kb = &wlr_libinput_kb->wlr_keyboard;\n\twlr_keyboard_init(wlr_kb, &impl);\n\treturn wlr_kb;\n}\n\nvoid handle_keyboard_key(struct libinput_event *event,\n\t\tstruct libinput_device *libinput_dev) {\n\tstruct wlr_input_device *wlr_dev =\n\t\tget_appropriate_device(WLR_INPUT_DEVICE_KEYBOARD, libinput_dev);\n\tif (!wlr_dev) {\n\t\twlr_log(L_DEBUG, \"Got a keyboard event for a device with no keyboards?\");\n\t\treturn;\n\t}\n\tstruct libinput_event_keyboard *kbevent =\n\t\tlibinput_event_get_keyboard_event(event);\n\tstruct wlr_event_keyboard_key wlr_event = { 0 };\n\twlr_event.time_sec = libinput_event_keyboard_get_time(kbevent);\n\twlr_event.time_usec = libinput_event_keyboard_get_time_usec(kbevent);\n\twlr_event.keycode = libinput_event_keyboard_get_key(kbevent);\n\tenum libinput_key_state state = \n\t\tlibinput_event_keyboard_get_key_state(kbevent);\n\tswitch (state) {\n\tcase LIBINPUT_KEY_STATE_RELEASED:\n\t\twlr_event.state = WLR_KEY_RELEASED;\n\t\tbreak;\n\tcase LIBINPUT_KEY_STATE_PRESSED:\n\t\twlr_event.state = WLR_KEY_PRESSED;\n\t\tbreak;\n\t}\n\twlr_event.update_state = true;\n\twlr_keyboard_notify_key(wlr_dev->keyboard, &wlr_event);\n}\n", "idx": 1, "id": 8920, "msg": "So we don't have to do this everywhere can you write a little helper function to convert?", "proj": "swaywm-wlroots", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -109,6 +109,8 @@ func NewStateDBAdapter(\n \t\tsuicideSnapshot:    make(map[int]deleteAccount),\n \t\tpreimages:          make(preimageMap),\n \t\tpreimageSnapshot:   make(map[int]preimageMap),\n+\t\taccessList:         newAccessList(),\n+\t\taccessListSnapshot: make(map[int]*accessList),\n \t\tnotFixTopicCopyBug: notFixTopicCopyBug,\n \t\tasyncContractTrie:  asyncContractTrie,\n \t}", "y": 0, "oldf": "// Copyright (c) 2019 IoTeX Foundation\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage evm\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"sort\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/zap\"\n\n\t\"github.com/iotexproject/go-pkgs/hash\"\n\t\"github.com/iotexproject/iotex-address/address\"\n\t\"github.com/iotexproject/iotex-proto/golang/iotextypes\"\n\n\t\"github.com/iotexproject/iotex-core/action\"\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\taccountutil \"github.com/iotexproject/iotex-core/action/protocol/account/util\"\n\t\"github.com/iotexproject/iotex-core/db/trie\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/state\"\n)\n\ntype (\n\t// deleteAccount records the account/contract to be deleted\n\tdeleteAccount map[hash.Hash160]struct{}\n\n\t// contractMap records the contracts being changed\n\tcontractMap map[hash.Hash160]Contract\n\n\t// preimageMap records the preimage of hash reported by VM\n\tpreimageMap map[common.Hash]SerializableBytes\n\n\t// GetBlockHash gets block hash by height\n\tGetBlockHash func(uint64) (hash.Hash256, error)\n\n\t// DepositGas deposits gas\n\tDepositGas func(context.Context, protocol.StateManager, *big.Int) (*action.TransactionLog, error)\n\n\t// StateDBAdapter represents the state db adapter for evm to access iotx blockchain\n\tStateDBAdapter struct {\n\t\tsm                  protocol.StateManager\n\t\tlogs                []*action.Log\n\t\ttransactionLogs     []*action.TransactionLog\n\t\terr                 error\n\t\tblockHeight         uint64\n\t\texecutionHash       hash.Hash256\n\t\trefund              uint64\n\t\tcachedContract      contractMap\n\t\tcontractSnapshot    map[int]contractMap   // snapshots of contracts\n\t\tsuicided            deleteAccount         // account/contract calling Suicide\n\t\tsuicideSnapshot     map[int]deleteAccount // snapshots of suicide accounts\n\t\tpreimages           preimageMap\n\t\tpreimageSnapshot    map[int]preimageMap\n\t\tnotFixTopicCopyBug  bool\n\t\tasyncContractTrie   bool\n\t\tsortCachedContracts bool\n\t\tusePendingNonce     bool\n\t}\n)\n\n// StateDBAdapterOption set StateDBAdapter construction param\ntype StateDBAdapterOption func(*StateDBAdapter) error\n\n// SortCachedContractsOption set sort cached contracts as true\nfunc SortCachedContractsOption() StateDBAdapterOption {\n\treturn func(adapter *StateDBAdapter) error {\n\t\tadapter.sortCachedContracts = true\n\t\treturn nil\n\t}\n}\n\n// UsePendingNonceOption set sort cached contracts as true\nfunc UsePendingNonceOption() StateDBAdapterOption {\n\treturn func(adapter *StateDBAdapter) error {\n\t\tadapter.usePendingNonce = true\n\t\treturn nil\n\t}\n}\n\n// NewStateDBAdapter creates a new state db with iotex blockchain\nfunc NewStateDBAdapter(\n\tsm protocol.StateManager,\n\tblockHeight uint64,\n\tnotFixTopicCopyBug bool,\n\tasyncContractTrie bool,\n\texecutionHash hash.Hash256,\n\topts ...StateDBAdapterOption,\n) *StateDBAdapter {\n\ts := &StateDBAdapter{\n\t\tsm:                 sm,\n\t\tlogs:               []*action.Log{},\n\t\terr:                nil,\n\t\tblockHeight:        blockHeight,\n\t\texecutionHash:      executionHash,\n\t\tcachedContract:     make(contractMap),\n\t\tcontractSnapshot:   make(map[int]contractMap),\n\t\tsuicided:           make(deleteAccount),\n\t\tsuicideSnapshot:    make(map[int]deleteAccount),\n\t\tpreimages:          make(preimageMap),\n\t\tpreimageSnapshot:   make(map[int]preimageMap),\n\t\tnotFixTopicCopyBug: notFixTopicCopyBug,\n\t\tasyncContractTrie:  asyncContractTrie,\n\t}\n\tfor _, opt := range opts {\n\t\tif err := opt(s); err != nil {\n\t\t\tlog.L().Panic(\"failed to execute stateDB creation option\")\n\t\t}\n\t}\n\treturn s\n}\n\nfunc (stateDB *StateDBAdapter) logError(err error) {\n\tif stateDB.err == nil {\n\t\tstateDB.err = err\n\t}\n}\n\n// Error returns the first stored error during evm contract execution\nfunc (stateDB *StateDBAdapter) Error() error {\n\treturn stateDB.err\n}\n\n// CreateAccount creates an account in iotx blockchain\nfunc (stateDB *StateDBAdapter) CreateAccount(evmAddr common.Address) {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn\n\t}\n\t_, err = accountutil.LoadOrCreateAccount(stateDB.sm, addr.String())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to create account.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tlog.L().Debug(\"Called CreateAccount.\", log.Hex(\"addrHash\", evmAddr[:]))\n}\n\n// SubBalance subtracts balance from account\nfunc (stateDB *StateDBAdapter) SubBalance(evmAddr common.Address, amount *big.Int) {\n\tif amount.Cmp(big.NewInt(int64(0))) == 0 {\n\t\treturn\n\t}\n\t// stateDB.GetBalance(evmAddr)\n\tlog.L().Debug(fmt.Sprintf(\"SubBalance %v from %s\", amount, evmAddr.Hex()))\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn\n\t}\n\tstate, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to sub balance.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tif err := state.SubBalance(amount); err != nil {\n\t\tlog.L().Error(\"Failed to sub balance.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tif err := accountutil.StoreAccount(stateDB.sm, addr, state); err != nil {\n\t\tlog.L().Error(\"Failed to update pending account changes to trie.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t}\n}\n\n// AddBalance adds balance to account\nfunc (stateDB *StateDBAdapter) AddBalance(evmAddr common.Address, amount *big.Int) {\n\tif amount.Cmp(big.NewInt(int64(0))) == 0 {\n\t\treturn\n\t}\n\t// stateDB.GetBalance(evmAddr)\n\tlog.L().Debug(fmt.Sprintf(\"AddBalance %v to %s\", amount, evmAddr.Hex()))\n\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn\n\t}\n\tvar state *state.Account\n\taddrHash := hash.BytesToHash160(evmAddr[:])\n\tif contract, ok := stateDB.cachedContract[addrHash]; ok {\n\t\tstate = contract.SelfState()\n\t} else {\n\t\tstate, err = accountutil.LoadOrCreateAccount(stateDB.sm, addr.String())\n\t\tif err != nil {\n\t\t\tlog.L().Error(\"Failed to add balance.\", log.Hex(\"addrHash\", evmAddr[:]))\n\t\t\tstateDB.logError(err)\n\t\t\treturn\n\t\t}\n\t}\n\tif err := state.AddBalance(amount); err != nil {\n\t\tlog.L().Error(\"Failed to add balance.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tif err := accountutil.StoreAccount(stateDB.sm, addr, state); err != nil {\n\t\tlog.L().Error(\"Failed to update pending account changes to trie.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t}\n}\n\n// GetBalance gets the balance of account\nfunc (stateDB *StateDBAdapter) GetBalance(evmAddr common.Address) *big.Int {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn big.NewInt(0)\n\t}\n\tstate, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get balance.\", zap.Error(err))\n\t\treturn big.NewInt(0)\n\t}\n\tlog.L().Debug(fmt.Sprintf(\"Balance of %s is %v\", evmAddr.Hex(), state.Balance))\n\n\treturn state.Balance\n}\n\n// InitNonce returns the init nonce of an account\nfunc (stateDB *StateDBAdapter) InitNonce() uint64 {\n\tif stateDB.usePendingNonce {\n\t\treturn 1\n\t}\n\treturn 0\n}\n\n// GetNonce gets the nonce of account\nfunc (stateDB *StateDBAdapter) GetNonce(evmAddr common.Address) uint64 {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn 0\n\t}\n\tnonce := uint64(0)\n\tstate, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get nonce.\", zap.Error(err))\n\t\t// stateDB.logError(err)\n\t} else {\n\t\tnonce = state.Nonce\n\t}\n\tif stateDB.usePendingNonce {\n\t\tnonce++\n\t}\n\tlog.L().Debug(\"Called GetNonce.\",\n\t\tzap.String(\"address\", addr.String()),\n\t\tzap.Uint64(\"nonce\", nonce))\n\n\treturn nonce\n}\n\n// SetNonce sets the nonce of account\nfunc (stateDB *StateDBAdapter) SetNonce(evmAddr common.Address, nonce uint64) {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn\n\t}\n\ts, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to set nonce.\", zap.Error(err))\n\t\t// stateDB.logError(err)\n\t\treturn\n\t}\n\tif stateDB.usePendingNonce {\n\t\tif nonce == 0 {\n\t\t\tpanic(\"invalid nonce zero\")\n\t\t}\n\t\tnonce--\n\t}\n\tlog.L().Debug(\"Called SetNonce.\",\n\t\tzap.String(\"address\", addr.String()),\n\t\tzap.Uint64(\"nonce\", nonce))\n\ts.Nonce = nonce\n\tif err := accountutil.StoreAccount(stateDB.sm, addr, s); err != nil {\n\t\tlog.L().Error(\"Failed to set nonce.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t}\n}\n\n// SubRefund subtracts refund\nfunc (stateDB *StateDBAdapter) SubRefund(gas uint64) {\n\tlog.L().Debug(\"Called SubRefund.\", zap.Uint64(\"gas\", gas))\n\t// stateDB.journal.append(refundChange{prev: self.refund})\n\tif gas > stateDB.refund {\n\t\tpanic(\"Refund counter not enough!\")\n\t}\n\tstateDB.refund -= gas\n}\n\n// AddRefund adds refund\nfunc (stateDB *StateDBAdapter) AddRefund(gas uint64) {\n\tlog.L().Debug(\"Called AddRefund.\", zap.Uint64(\"gas\", gas))\n\t// stateDB.journal.append(refundChange{prev: self.refund})\n\tstateDB.refund += gas\n}\n\n// GetRefund gets refund\nfunc (stateDB *StateDBAdapter) GetRefund() uint64 {\n\tlog.L().Debug(\"Called GetRefund.\")\n\treturn stateDB.refund\n}\n\n// Suicide kills the contract\nfunc (stateDB *StateDBAdapter) Suicide(evmAddr common.Address) bool {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn false\n\t}\n\tif !stateDB.Exist(evmAddr) {\n\t\tlog.L().Debug(\"Account does not exist.\", zap.String(\"address\", addr.String()))\n\t\treturn false\n\t}\n\ts, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\tlog.L().Debug(\"Failed to get account.\", zap.String(\"address\", addr.String()))\n\t\treturn false\n\t}\n\t// clears the account balance\n\ts.Balance = nil\n\ts.Balance = big.NewInt(0)\n\taddrHash := hash.BytesToHash160(evmAddr.Bytes())\n\tif _, err := stateDB.sm.PutState(s, protocol.LegacyKeyOption(addrHash)); err != nil {\n\t\tlog.L().Error(\"Failed to kill contract.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn false\n\t}\n\t// mark it as deleted\n\tstateDB.suicided[addrHash] = struct{}{}\n\treturn true\n}\n\n// HasSuicided returns whether the contract has been killed\nfunc (stateDB *StateDBAdapter) HasSuicided(evmAddr common.Address) bool {\n\taddrHash := hash.BytesToHash160(evmAddr.Bytes())\n\t_, ok := stateDB.suicided[addrHash]\n\treturn ok\n}\n\n// Exist checks the existence of an address\nfunc (stateDB *StateDBAdapter) Exist(evmAddr common.Address) bool {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn false\n\t}\n\tlog.L().Debug(\"Check existence.\", zap.String(\"address\", addr.String()), log.Hex(\"addrHash\", evmAddr[:]))\n\taddrHash := hash.BytesToHash160(addr.Bytes())\n\tif _, ok := stateDB.cachedContract[addrHash]; ok {\n\t\treturn true\n\t}\n\trecorded, err := accountutil.Recorded(stateDB.sm, addr)\n\tif !recorded || err != nil {\n\t\tlog.L().Debug(\"Account does not exist.\", zap.String(\"address\", addr.String()))\n\t\treturn false\n\t}\n\treturn true\n}\n\n// Empty returns true if the the contract is empty\nfunc (stateDB *StateDBAdapter) Empty(evmAddr common.Address) bool {\n\taddr, err := address.FromBytes(evmAddr.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn true\n\t}\n\tlog.L().Debug(\"Check whether the contract is empty.\")\n\ts, err := stateDB.AccountState(addr.String())\n\tif err != nil {\n\t\treturn true\n\t}\n\t// TODO: delete hash.ZeroHash256\n\treturn s.Nonce == 0 &&\n\t\ts.Balance.Sign() == 0 &&\n\t\t(len(s.CodeHash) == 0 || bytes.Equal(s.CodeHash, hash.ZeroHash256[:]))\n}\n\n// RevertToSnapshot reverts the state factory to the state at a given snapshot\nfunc (stateDB *StateDBAdapter) RevertToSnapshot(snapshot int) {\n\tif err := stateDB.sm.Revert(snapshot); err != nil {\n\t\terr := errors.New(\"unexpected error: state manager's Revert() failed\")\n\t\tlog.L().Error(\"Failed to revert to snapshot.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tds, ok := stateDB.suicideSnapshot[snapshot]\n\tif !ok {\n\t\t// this should not happen, b/c we save the suicide accounts on a successful return of Snapshot(), but check anyway\n\t\tlog.L().Error(\"Failed to get snapshot.\", zap.Int(\"snapshot\", snapshot))\n\t\treturn\n\t}\n\t// restore the suicide accounts\n\tstateDB.suicided = nil\n\tstateDB.suicided = ds\n\t// restore modified contracts\n\tstateDB.cachedContract = nil\n\tstateDB.cachedContract = stateDB.contractSnapshot[snapshot]\n\tif stateDB.sortCachedContracts {\n\t\tfor _, addr := range stateDB.cachedContractAddrs() {\n\t\t\tc := stateDB.cachedContract[addr]\n\t\t\tif err := c.LoadRoot(); err != nil {\n\t\t\t\tlog.L().Error(\"Failed to load root for contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor addr, c := range stateDB.cachedContract {\n\t\t\tif err := c.LoadRoot(); err != nil {\n\t\t\t\tlog.L().Error(\"Failed to load root for contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// restore preimages\n\tstateDB.preimages = nil\n\tstateDB.preimages = stateDB.preimageSnapshot[snapshot]\n}\n\nfunc (stateDB *StateDBAdapter) cachedContractAddrs() []hash.Hash160 {\n\taddrs := make([]hash.Hash160, 0, len(stateDB.cachedContract))\n\tfor addr := range stateDB.cachedContract {\n\t\taddrs = append(addrs, addr)\n\t}\n\tsort.Slice(addrs, func(i, j int) bool { return bytes.Compare(addrs[i][:], addrs[j][:]) < 0 })\n\treturn addrs\n}\n\n// Snapshot returns the snapshot id\nfunc (stateDB *StateDBAdapter) Snapshot() int {\n\tsn := stateDB.sm.Snapshot()\n\tif _, ok := stateDB.suicideSnapshot[sn]; ok {\n\t\terr := errors.New(\"unexpected error: duplicate snapshot version\")\n\t\tlog.L().Error(\"Failed to snapshot.\", zap.Error(err))\n\t\t// stateDB.err = err\n\t\treturn sn\n\t}\n\t// save a copy of current suicide accounts\n\tsa := make(deleteAccount)\n\tfor k, v := range stateDB.suicided {\n\t\tsa[k] = v\n\t}\n\tstateDB.suicideSnapshot[sn] = sa\n\t// save a copy of modified contracts\n\tc := make(contractMap)\n\tif stateDB.sortCachedContracts {\n\t\tfor _, addr := range stateDB.cachedContractAddrs() {\n\t\t\tc[addr] = stateDB.cachedContract[addr].Snapshot()\n\t\t}\n\t} else {\n\t\tfor addr := range stateDB.cachedContract {\n\t\t\tc[addr] = stateDB.cachedContract[addr].Snapshot()\n\t\t}\n\t}\n\tstateDB.contractSnapshot[sn] = c\n\t// save a copy of preimages\n\tp := make(preimageMap)\n\tfor k, v := range stateDB.preimages {\n\t\tp[k] = v\n\t}\n\tstateDB.preimageSnapshot[sn] = p\n\treturn sn\n}\n\n// AddLog adds log whose transaction amount is larger than 0\nfunc (stateDB *StateDBAdapter) AddLog(evmLog *types.Log) {\n\tlog.L().Debug(\"Called AddLog.\", zap.Any(\"log\", evmLog))\n\taddr, err := address.FromBytes(evmLog.Address.Bytes())\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to convert evm address.\", zap.Error(err))\n\t\treturn\n\t}\n\tvar topics []hash.Hash256\n\tfor _, evmTopic := range evmLog.Topics {\n\t\tvar topic hash.Hash256\n\t\tcopy(topic[:], evmTopic.Bytes())\n\t\ttopics = append(topics, topic)\n\t}\n\tif topics[0] == inContractTransfer {\n\t\tif len(topics) != 3 {\n\t\t\tpanic(\"Invalid in contract transfer topics\")\n\t\t}\n\t\tif amount, zero := new(big.Int).SetBytes(evmLog.Data), big.NewInt(0); amount.Cmp(zero) == 1 {\n\t\t\tfrom, _ := address.FromBytes(topics[1][12:])\n\t\t\tto, _ := address.FromBytes(topics[2][12:])\n\t\t\tstateDB.transactionLogs = append(stateDB.transactionLogs, &action.TransactionLog{\n\t\t\t\tType:      iotextypes.TransactionLogType_IN_CONTRACT_TRANSFER,\n\t\t\t\tSender:    from.String(),\n\t\t\t\tRecipient: to.String(),\n\t\t\t\tAmount:    amount,\n\t\t\t})\n\t\t}\n\t\treturn\n\t}\n\n\tstateDB.logs = append(stateDB.logs, &action.Log{\n\t\tAddress:            addr.String(),\n\t\tTopics:             topics,\n\t\tData:               evmLog.Data,\n\t\tBlockHeight:        stateDB.blockHeight,\n\t\tActionHash:         stateDB.executionHash,\n\t\tNotFixTopicCopyBug: stateDB.notFixTopicCopyBug,\n\t})\n}\n\n// Logs returns the logs\nfunc (stateDB *StateDBAdapter) Logs() []*action.Log {\n\treturn stateDB.logs\n}\n\n// TransactionLogs returns the transaction logs\nfunc (stateDB *StateDBAdapter) TransactionLogs() []*action.TransactionLog {\n\treturn stateDB.transactionLogs\n}\n\n// AddPreimage adds the preimage of a hash\nfunc (stateDB *StateDBAdapter) AddPreimage(hash common.Hash, preimage []byte) {\n\tif _, ok := stateDB.preimages[hash]; !ok {\n\t\tb := make([]byte, len(preimage))\n\t\tcopy(b, preimage)\n\t\tstateDB.preimages[hash] = b\n\t}\n}\n\n// ForEachStorage loops each storage\nfunc (stateDB *StateDBAdapter) ForEachStorage(addr common.Address, cb func(common.Hash, common.Hash) bool) error {\n\tctt, err := stateDB.getContract(hash.BytesToHash160(addr[:]))\n\tif err != nil {\n\t\t// stateDB.err = err\n\t\treturn err\n\t}\n\titer, err := ctt.Iterator()\n\tif err != nil {\n\t\t// stateDB.err = err\n\t\treturn err\n\t}\n\n\tfor {\n\t\tkey, value, err := iter.Next()\n\t\tif err == trie.ErrEndOfIterator {\n\t\t\t// hit the end of the iterator, exit now\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tckey := common.Hash{}\n\t\tcopy(ckey[:], key[:])\n\t\tcvalue := common.Hash{}\n\t\tcopy(cvalue[:], value[:])\n\t\tif !cb(ckey, cvalue) {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn nil\n}\n\n// AccountState returns an account state\nfunc (stateDB *StateDBAdapter) AccountState(encodedAddr string) (*state.Account, error) {\n\taddr, err := address.FromString(encodedAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get public key hash from encoded address\")\n\t}\n\taddrHash := hash.BytesToHash160(addr.Bytes())\n\tif contract, ok := stateDB.cachedContract[addrHash]; ok {\n\t\treturn contract.SelfState(), nil\n\t}\n\treturn accountutil.LoadAccount(stateDB.sm, addrHash)\n}\n\n//======================================\n// Contract functions\n//======================================\n\n// GetCodeHash returns contract's code hash\nfunc (stateDB *StateDBAdapter) GetCodeHash(evmAddr common.Address) common.Hash {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tcodeHash := common.Hash{}\n\tif contract, ok := stateDB.cachedContract[addr]; ok {\n\t\tcopy(codeHash[:], contract.SelfState().CodeHash)\n\t\treturn codeHash\n\t}\n\taccount, err := accountutil.LoadAccount(stateDB.sm, addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get code hash.\", zap.Error(err))\n\t\t// TODO (zhi) not all err should be logged\n\t\t// stateDB.logError(err)\n\t\treturn codeHash\n\t}\n\tcopy(codeHash[:], account.CodeHash)\n\treturn codeHash\n}\n\n// GetCode returns contract's code\nfunc (stateDB *StateDBAdapter) GetCode(evmAddr common.Address) []byte {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tif contract, ok := stateDB.cachedContract[addr]; ok {\n\t\tcode, err := contract.GetCode()\n\t\tif err != nil {\n\t\t\tlog.L().Error(\"Failed to get code hash.\", zap.Error(err))\n\t\t\treturn nil\n\t\t}\n\t\treturn code\n\t}\n\taccount, err := accountutil.LoadAccount(stateDB.sm, addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to load account state for address.\", log.Hex(\"addrHash\", addr[:]))\n\t\treturn nil\n\t}\n\tvar code SerializableBytes\n\tif _, err = stateDB.sm.State(&code, protocol.NamespaceOption(CodeKVNameSpace), protocol.KeyOption(account.CodeHash[:])); err != nil {\n\t\t// TODO: Suppress the as it's too much now\n\t\t//log.L().Error(\"Failed to get code from trie.\", zap.Error(err))\n\t\treturn nil\n\t}\n\treturn code[:]\n}\n\n// GetCodeSize gets the code size saved in hash\nfunc (stateDB *StateDBAdapter) GetCodeSize(evmAddr common.Address) int {\n\tcode := stateDB.GetCode(evmAddr)\n\tif code == nil {\n\t\treturn 0\n\t}\n\tlog.L().Debug(\"Called GetCodeSize.\", log.Hex(\"addrHash\", evmAddr[:]))\n\treturn len(code)\n}\n\n// SetCode sets contract's code\nfunc (stateDB *StateDBAdapter) SetCode(evmAddr common.Address, code []byte) {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tcontract, err := stateDB.getContract(addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tcontract.SetCode(hash.Hash256b(code), code)\n}\n\n// GetCommittedState gets committed state\nfunc (stateDB *StateDBAdapter) GetCommittedState(evmAddr common.Address, k common.Hash) common.Hash {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tcontract, err := stateDB.getContract(addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\tstateDB.logError(err)\n\t\treturn common.Hash{}\n\t}\n\tv, err := contract.GetCommittedState(hash.BytesToHash256(k[:]))\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get committed state.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn common.Hash{}\n\t}\n\treturn common.BytesToHash(v)\n}\n\n// GetState gets state\nfunc (stateDB *StateDBAdapter) GetState(evmAddr common.Address, k common.Hash) common.Hash {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tcontract, err := stateDB.getContract(addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\tstateDB.logError(err)\n\t\treturn common.Hash{}\n\t}\n\tv, err := contract.GetState(hash.BytesToHash256(k[:]))\n\tif err != nil {\n\t\tlog.L().Debug(\"Failed to get state.\", zap.Error(err))\n\t\tstateDB.logError(err)\n\t\treturn common.Hash{}\n\t}\n\treturn common.BytesToHash(v)\n}\n\n// SetState sets state\nfunc (stateDB *StateDBAdapter) SetState(evmAddr common.Address, k, v common.Hash) {\n\taddr := hash.BytesToHash160(evmAddr[:])\n\tcontract, err := stateDB.getContract(addr)\n\tif err != nil {\n\t\tlog.L().Error(\"Failed to get contract.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n\tlog.L().Debug(\"Called SetState\", log.Hex(\"addrHash\", evmAddr[:]), log.Hex(\"k\", k[:]))\n\tif err := contract.SetState(hash.BytesToHash256(k[:]), v[:]); err != nil {\n\t\tlog.L().Error(\"Failed to set state.\", zap.Error(err), log.Hex(\"addrHash\", addr[:]))\n\t\tstateDB.logError(err)\n\t\treturn\n\t}\n}\n\n// CommitContracts commits contract code to db and update pending contract account changes to trie\nfunc (stateDB *StateDBAdapter) CommitContracts() error {\n\taddrStrs := make([]string, 0)\n\tfor addr := range stateDB.cachedContract {\n\t\taddrStrs = append(addrStrs, hex.EncodeToString(addr[:]))\n\t}\n\tsort.Strings(addrStrs)\n\n\tfor _, addrStr := range addrStrs {\n\t\tvar addr hash.Hash160\n\t\taddrBytes, err := hex.DecodeString(addrStr)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to decode address hash\")\n\t\t}\n\t\tcopy(addr[:], addrBytes)\n\t\tif _, ok := stateDB.suicided[addr]; ok {\n\t\t\t// no need to update a suicide account/contract\n\t\t\tcontinue\n\t\t}\n\t\tcontract := stateDB.cachedContract[addr]\n\t\tif err := contract.Commit(); err != nil {\n\t\t\tstateDB.logError(err)\n\t\t\treturn errors.Wrap(err, \"failed to commit contract\")\n\t\t}\n\t\tstate := contract.SelfState()\n\t\t// store the account (with new storage trie root) into account trie\n\t\tif _, err := stateDB.sm.PutState(state, protocol.LegacyKeyOption(addr)); err != nil {\n\t\t\tstateDB.logError(err)\n\t\t\treturn errors.Wrap(err, \"failed to update pending account changes to trie\")\n\t\t}\n\t}\n\t// delete suicided accounts/contract\n\taddrStrs = make([]string, 0)\n\tfor addr := range stateDB.suicided {\n\t\taddrStrs = append(addrStrs, hex.EncodeToString(addr[:]))\n\t}\n\tsort.Strings(addrStrs)\n\n\tfor _, addrStr := range addrStrs {\n\t\tvar addr hash.Hash160\n\t\taddrBytes, err := hex.DecodeString(addrStr)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to decode address hash\")\n\t\t}\n\t\tcopy(addr[:], addrBytes)\n\t\tif _, err := stateDB.sm.DelState(protocol.LegacyKeyOption(addr)); err != nil {\n\t\t\tstateDB.logError(err)\n\t\t\treturn errors.Wrapf(err, \"failed to delete suicide account/contract %x\", addr[:])\n\t\t}\n\t}\n\t// write preimages to DB\n\taddrStrs = make([]string, 0)\n\tfor addr := range stateDB.preimages {\n\t\taddrStrs = append(addrStrs, hex.EncodeToString(addr[:]))\n\t}\n\tsort.Strings(addrStrs)\n\n\tfor _, addrStr := range addrStrs {\n\t\tvar k common.Hash\n\t\taddrBytes, err := hex.DecodeString(addrStr)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to decode address hash\")\n\t\t}\n\t\tcopy(k[:], addrBytes)\n\t\tv := stateDB.preimages[k]\n\t\th := make([]byte, len(k))\n\t\tcopy(h, k[:])\n\t\tstateDB.sm.PutState(v, protocol.NamespaceOption(PreimageKVNameSpace), protocol.KeyOption(h))\n\t}\n\treturn nil\n}\n\n// getContract returns the contract of addr\nfunc (stateDB *StateDBAdapter) getContract(addr hash.Hash160) (Contract, error) {\n\tif contract, ok := stateDB.cachedContract[addr]; ok {\n\t\treturn contract, nil\n\t}\n\treturn stateDB.getNewContract(addr)\n}\n\nfunc (stateDB *StateDBAdapter) getNewContract(addr hash.Hash160) (Contract, error) {\n\taccount, err := accountutil.LoadAccount(stateDB.sm, addr)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to load account state for address %x\", addr)\n\t}\n\tcontract, err := newContract(addr, account, stateDB.sm, stateDB.asyncContractTrie)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to create storage trie for new contract %x\", addr)\n\t}\n\t// add to contract cache\n\tstateDB.cachedContract[addr] = contract\n\treturn contract, nil\n}\n\n// clear clears local changes\nfunc (stateDB *StateDBAdapter) clear() {\n\tstateDB.cachedContract = nil\n\tstateDB.contractSnapshot = nil\n\tstateDB.suicided = nil\n\tstateDB.suicideSnapshot = nil\n\tstateDB.preimages = nil\n\tstateDB.preimageSnapshot = nil\n\tstateDB.cachedContract = make(contractMap)\n\tstateDB.contractSnapshot = make(map[int]contractMap)\n\tstateDB.suicided = make(deleteAccount)\n\tstateDB.suicideSnapshot = make(map[int]deleteAccount)\n\tstateDB.preimages = make(preimageMap)\n\tstateDB.preimageSnapshot = make(map[int]preimageMap)\n}\n", "idx": 2, "id": 23715, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -40,7 +40,7 @@ func createTestMinerWith(pledge int64,\n \tmsg := types.NewMessage(minerOwnerAddr, address.StorageMarketAddress, nonce, types.NewAttoFILFromFIL(collateral), \"createMiner\", pdata)\n \n \tresult, err := th.ApplyTestMessage(stateTree, vms, msg, types.NewBlockHeight(0))\n-\tassert.NoError(t, err)\n+\trequire.NoError(t, err)\n \n \taddr, err := address.NewFromBytes(result.Receipt.Return[0])\n \tassert.NoError(t, err)", "y": 1, "oldf": "package miner_test\n\nimport (\n\t\"context\"\n\t\"math/big\"\n\t\"testing\"\n\n\tpeer \"github.com/libp2p/go-libp2p-peer\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/filecoin-project/go-filecoin/actor\"\n\t\"github.com/filecoin-project/go-filecoin/actor/builtin\"\n\t. \"github.com/filecoin-project/go-filecoin/actor/builtin/miner\"\n\t\"github.com/filecoin-project/go-filecoin/address\"\n\t\"github.com/filecoin-project/go-filecoin/consensus\"\n\t\"github.com/filecoin-project/go-filecoin/core\"\n\t\"github.com/filecoin-project/go-filecoin/state\"\n\tth \"github.com/filecoin-project/go-filecoin/testhelpers\"\n\ttf \"github.com/filecoin-project/go-filecoin/testhelpers/testflags\"\n\t\"github.com/filecoin-project/go-filecoin/types\"\n\t\"github.com/filecoin-project/go-filecoin/vm\"\n)\n\nfunc createTestMiner(t *testing.T, st state.Tree, vms vm.StorageMap, minerOwnerAddr address.Address, key []byte, pid peer.ID) address.Address {\n\treturn createTestMinerWith(100, 100, t, st, vms, minerOwnerAddr, key, pid)\n}\n\nfunc createTestMinerWith(pledge int64,\n\tcollateral uint64,\n\tt *testing.T,\n\tstateTree state.Tree,\n\tvms vm.StorageMap,\n\tminerOwnerAddr address.Address,\n\tkey []byte,\n\tpeerId peer.ID,\n) address.Address {\n\tpdata := actor.MustConvertParams(big.NewInt(pledge), key, peerId)\n\tnonce := core.MustGetNonce(stateTree, address.TestAddress)\n\tmsg := types.NewMessage(minerOwnerAddr, address.StorageMarketAddress, nonce, types.NewAttoFILFromFIL(collateral), \"createMiner\", pdata)\n\n\tresult, err := th.ApplyTestMessage(stateTree, vms, msg, types.NewBlockHeight(0))\n\tassert.NoError(t, err)\n\n\taddr, err := address.NewFromBytes(result.Receipt.Return[0])\n\tassert.NoError(t, err)\n\treturn addr\n}\n\nfunc TestAskFunctions(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tst, vms := core.CreateStorages(ctx, t)\n\n\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"abcd123\"), th.RequireRandomPeerID(t))\n\n\t// make an ask, and then make sure it all looks good\n\tpdata := actor.MustConvertParams(types.NewAttoFILFromFIL(5), big.NewInt(1500))\n\tmsg := types.NewMessage(address.TestAddress, minerAddr, 1, nil, \"addAsk\", pdata)\n\n\t_, err := th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(1))\n\tassert.NoError(t, err)\n\n\tpdata = actor.MustConvertParams(big.NewInt(0))\n\tmsg = types.NewMessage(address.TestAddress, minerAddr, 2, types.NewZeroAttoFIL(), \"getAsk\", pdata)\n\tresult, err := th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(2))\n\tassert.NoError(t, err)\n\n\tvar ask Ask\n\terr = actor.UnmarshalStorage(result.Receipt.Return[0], &ask)\n\trequire.NoError(t, err)\n\tassert.Equal(t, types.NewBlockHeight(1501), ask.Expiry)\n\n\tminer, err := st.GetActor(ctx, minerAddr)\n\tassert.NoError(t, err)\n\n\tvar minerStorage State\n\tbuiltin.RequireReadState(t, vms, minerAddr, miner, &minerStorage)\n\tassert.Equal(t, 1, len(minerStorage.Asks))\n\tassert.Equal(t, uint64(1), minerStorage.NextAskID.Uint64())\n\n\t// Look for an ask that doesn't exist\n\tpdata = actor.MustConvertParams(big.NewInt(3453))\n\tmsg = types.NewMessage(address.TestAddress, minerAddr, 2, types.NewZeroAttoFIL(), \"getAsk\", pdata)\n\tresult, err = th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(2))\n\tassert.Equal(t, Errors[ErrAskNotFound], result.ExecutionError)\n\tassert.NoError(t, err)\n\n\t// make another ask!\n\tpdata = actor.MustConvertParams(types.NewAttoFILFromFIL(110), big.NewInt(200))\n\tmsg = types.NewMessage(address.TestAddress, minerAddr, 3, nil, \"addAsk\", pdata)\n\tresult, err = th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(3))\n\tassert.NoError(t, err)\n\tassert.Equal(t, big.NewInt(1), big.NewInt(0).SetBytes(result.Receipt.Return[0]))\n\n\tpdata = actor.MustConvertParams(big.NewInt(1))\n\tmsg = types.NewMessage(address.TestAddress, minerAddr, 4, types.NewZeroAttoFIL(), \"getAsk\", pdata)\n\tresult, err = th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(4))\n\tassert.NoError(t, err)\n\n\tvar ask2 Ask\n\terr = actor.UnmarshalStorage(result.Receipt.Return[0], &ask2)\n\trequire.NoError(t, err)\n\tassert.Equal(t, types.NewBlockHeight(203), ask2.Expiry)\n\tassert.Equal(t, uint64(1), ask2.ID.Uint64())\n\n\tmsg = types.NewMessage(address.TestAddress, minerAddr, 5, types.NewZeroAttoFIL(), \"getAsks\", nil)\n\tresult, err = th.ApplyTestMessage(st, vms, msg, types.NewBlockHeight(4))\n\tassert.NoError(t, err)\n\tassert.NoError(t, result.ExecutionError)\n\n\tvar askids []uint64\n\trequire.NoError(t, actor.UnmarshalStorage(result.Receipt.Return[0], &askids))\n\tassert.Len(t, askids, 2)\n}\n\nfunc TestGetKey(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tst, vms := core.CreateStorages(ctx, t)\n\n\tsignature := []byte(\"my public key\")\n\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, signature, th.RequireRandomPeerID(t))\n\n\t// retrieve key\n\tresult := callQueryMethodSuccess(\"getKey\", ctx, t, st, vms, address.TestAddress, minerAddr)\n\tassert.Equal(t, result[0], signature)\n}\n\nfunc TestCBOREncodeState(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tstate := NewState(address.TestAddress, []byte{}, big.NewInt(1), th.RequireRandomPeerID(t), types.NewZeroAttoFIL())\n\n\tstate.SectorCommitments[\"1\"] = types.Commitments{\n\t\tCommD:     types.CommD{},\n\t\tCommR:     types.CommR{},\n\t\tCommRStar: types.CommRStar{},\n\t}\n\n\t_, err := actor.MarshalStorage(state)\n\tassert.NoError(t, err)\n\n}\n\nfunc TestPeerIdGetterAndSetter(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tt.Run(\"successfully retrieves and updates peer ID\", func(t *testing.T) {\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tst, vms := core.CreateStorages(ctx, t)\n\n\t\torigPid := th.RequireRandomPeerID(t)\n\t\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"my public key\"), origPid)\n\n\t\t// retrieve peer ID\n\t\tresultA := callQueryMethodSuccess(\"getPeerID\", ctx, t, st, vms, address.TestAddress, minerAddr)\n\t\tpid, err := peer.IDFromBytes(resultA[0])\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, peer.IDB58Encode(origPid), peer.IDB58Encode(pid))\n\n\t\t// update peer ID\n\t\tnewPid := th.RequireRandomPeerID(t)\n\t\tupdatePeerIdSuccess(ctx, t, st, vms, address.TestAddress, minerAddr, newPid)\n\n\t\t// retrieve peer ID\n\t\tresultB := callQueryMethodSuccess(\"getPeerID\", ctx, t, st, vms, address.TestAddress, minerAddr)\n\t\tpid, err = peer.IDFromBytes(resultB[0])\n\t\trequire.NoError(t, err)\n\n\t\trequire.Equal(t, peer.IDB58Encode(newPid), peer.IDB58Encode(pid))\n\t})\n\n\tt.Run(\"authorization failure while updating peer ID\", func(t *testing.T) {\n\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tst, vms := core.CreateStorages(ctx, t)\n\n\t\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"other public key\"), th.RequireRandomPeerID(t))\n\n\t\t// update peer ID and expect authorization failure (TestAddress2 doesn't owner miner)\n\t\tupdatePeerIdMsg := types.NewMessage(\n\t\t\taddress.TestAddress2,\n\t\t\tminerAddr,\n\t\t\tcore.MustGetNonce(st, address.TestAddress2),\n\t\t\ttypes.NewAttoFILFromFIL(0),\n\t\t\t\"updatePeerID\",\n\t\t\tactor.MustConvertParams(th.RequireRandomPeerID(t)))\n\n\t\tapplyMsgResult, err := th.ApplyTestMessage(st, vms, updatePeerIdMsg, types.NewBlockHeight(0))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, Errors[ErrCallerUnauthorized], applyMsgResult.ExecutionError)\n\t\trequire.NotEqual(t, uint8(0), applyMsgResult.Receipt.ExitCode)\n\t})\n}\n\nfunc TestMinerGetPledge(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tt.Run(\"GetPledge returns pledged sectors, 0, nil when successful\", func(t *testing.T) {\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tst, vms := core.CreateStorages(ctx, t)\n\n\t\tminerAddr := createTestMinerWith(120, 240, t, st, vms, address.TestAddress,\n\t\t\t[]byte(\"my public key\"), th.RequireRandomPeerID(t))\n\n\t\t// retrieve power (trivial result for no proven sectors)\n\t\tresult := callQueryMethodSuccess(\"getPledge\", ctx, t, st, vms, address.TestAddress, minerAddr)[0][0]\n\n\t\trequire.Equal(t, 120, int(result))\n\t})\n}\n\nfunc TestMinerGetPower(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tt.Run(\"GetPower returns proven sectors, 0, nil when successful\", func(t *testing.T) {\n\t\tctx, cancel := context.WithCancel(context.Background())\n\t\tdefer cancel()\n\n\t\tst, vms := core.CreateStorages(ctx, t)\n\n\t\tminerAddr := createTestMinerWith(120, 240, t, st, vms, address.TestAddress,\n\t\t\t[]byte(\"my public key\"), th.RequireRandomPeerID(t))\n\n\t\t// retrieve power (trivial result for no proven sectors)\n\t\tresult := callQueryMethodSuccess(\"getPower\", ctx, t, st, vms, address.TestAddress, minerAddr)\n\t\trequire.Equal(t, []byte{}, result[0])\n\t})\n}\n\nfunc updatePeerIdSuccess(ctx context.Context, t *testing.T, st state.Tree, vms vm.StorageMap, fromAddr address.Address, minerAddr address.Address, newPid peer.ID) {\n\tupdatePeerIdMsg := types.NewMessage(\n\t\tfromAddr,\n\t\tminerAddr,\n\t\tcore.MustGetNonce(st, fromAddr),\n\t\ttypes.NewAttoFILFromFIL(0),\n\t\t\"updatePeerID\",\n\t\tactor.MustConvertParams(newPid))\n\n\tapplyMsgResult, err := th.ApplyTestMessage(st, vms, updatePeerIdMsg, types.NewBlockHeight(0))\n\trequire.NoError(t, err)\n\trequire.NoError(t, applyMsgResult.ExecutionError)\n\trequire.Equal(t, uint8(0), applyMsgResult.Receipt.ExitCode)\n}\n\nfunc callQueryMethodSuccess(method string,\n\tctx context.Context,\n\tt *testing.T, st state.Tree,\n\tvms vm.StorageMap,\n\tfromAddr address.Address,\n\tminerAddr address.Address) [][]byte {\n\tres, code, err := consensus.CallQueryMethod(ctx, st, vms, minerAddr, method, []byte{}, fromAddr, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, uint8(0), code)\n\treturn res\n}\n\nfunc TestMinerCommitSector(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx := context.Background()\n\tst, vms := core.CreateStorages(ctx, t)\n\n\torigPid := th.RequireRandomPeerID(t)\n\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"my public key\"), origPid)\n\n\tcommR := th.MakeCommitment()\n\tcommRStar := th.MakeCommitment()\n\tcommD := th.MakeCommitment()\n\n\tres, err := th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 3, \"commitSector\", nil, uint64(1), commD, commR, commRStar, th.MakeRandomBytes(types.TwoPoRepProofPartitions.ProofLen()))\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\t// check that the proving period matches\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 3, \"getProvingPeriodStart\", nil)\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\t// blockheight was 3\n\trequire.Equal(t, types.NewBlockHeight(3), types.NewBlockHeightFromBytes(res.Receipt.Return[0]))\n\n\t// fail because commR already exists\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 4, \"commitSector\", nil, uint64(1), commD, commR, commRStar, th.MakeRandomBytes(types.TwoPoRepProofPartitions.ProofLen()))\n\trequire.NoError(t, err)\n\trequire.EqualError(t, res.ExecutionError, \"sector already committed\")\n\trequire.Equal(t, uint8(0x23), res.Receipt.ExitCode)\n}\n\nfunc TestMinerSubmitPoSt(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx := context.Background()\n\tst, vms := core.CreateStorages(ctx, t)\n\n\tancestors := th.RequireTipSetChain(t, 10)\n\n\torigPid := th.RequireRandomPeerID(t)\n\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"my public key\"), origPid)\n\n\t// add a sector\n\tres, err := th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 3, \"commitSector\", ancestors, uint64(1), th.MakeCommitment(), th.MakeCommitment(), th.MakeCommitment(), th.MakeRandomBytes(types.TwoPoRepProofPartitions.ProofLen()))\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\t// add another sector\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 4, \"commitSector\", ancestors, uint64(2), th.MakeCommitment(), th.MakeCommitment(), th.MakeCommitment(), th.MakeRandomBytes(types.TwoPoRepProofPartitions.ProofLen()))\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\t// submit post\n\tproof := th.MakeRandomPoSTProofForTest()\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 8, \"submitPoSt\", ancestors, []types.PoStProof{proof})\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\t// check that the proving period is now the next one\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 9, \"getProvingPeriodStart\", ancestors)\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, types.NewBlockHeightFromBytes(res.Receipt.Return[0]), types.NewBlockHeight(20003))\n\n\t// fail to submit inside the proving period\n\tproof = th.MakeRandomPoSTProofForTest()\n\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 40008, \"submitPoSt\", ancestors, []types.PoStProof{proof})\n\trequire.NoError(t, err)\n\trequire.EqualError(t, res.ExecutionError, \"submitted PoSt late, need to pay a fee\")\n}\n\nfunc TestVerifyPIP(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx := context.Background()\n\tst, vms := core.CreateStorages(ctx, t)\n\n\tancestors := th.RequireTipSetChain(t, 10)\n\n\torigPid := th.RequireRandomPeerID(t)\n\tminerAddr := createTestMiner(t, st, vms, address.TestAddress, []byte(\"my public key\"), origPid)\n\n\tsectorId := uint64(1)\n\tcommD := th.MakeCommitment()\n\n\t// add a sector\n\tres, err := th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, 3, \"commitSector\", ancestors, sectorId, commD, th.MakeCommitment(), th.MakeCommitment(), th.MakeRandomBytes(types.TwoPoRepProofPartitions.ProofLen()))\n\trequire.NoError(t, err)\n\trequire.NoError(t, res.ExecutionError)\n\trequire.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\trunVerifyPIP := func(t *testing.T, bh uint64, commP []byte, sectorId uint64, proof []byte) error {\n\t\tres, err := th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, bh, \"verifyPieceInclusion\", ancestors, commP, sectorId, proof)\n\t\trequire.NoError(t, err)\n\n\t\treturn res.ExecutionError\n\t}\n\n\tcommP := th.MakeCommitment()\n\n\t// TODO: This is a fake pip form by concatenating commP and commD.\n\t// It will need to be generated correctly once real verification is implemented\n\t// see https://github.com/filecoin-project/go-filecoin/issues/2629\n\tpip := []byte{}\n\tpip = append(pip, commP[:]...)\n\tpip = append(pip, commD[:]...)\n\n\tt.Run(\"PIP is invalid if miner has not submitted any proofs\", func(t *testing.T) {\n\t\terr := runVerifyPIP(t, 3, commP, sectorId, pip)\n\n\t\tassert.Error(t, err)\n\t\tassert.Equal(t, \"proofs out of date\", err.Error())\n\t})\n\n\tt.Run(\"After submitting a PoSt\", func(t *testing.T) {\n\t\t// submit a post\n\t\tproof := th.MakeRandomPoSTProofForTest()\n\t\tblockheightOfPoSt := uint64(8)\n\t\tres, err = th.CreateAndApplyTestMessage(t, st, vms, minerAddr, 0, blockheightOfPoSt, \"submitPoSt\", ancestors, []types.PoStProof{proof})\n\t\tassert.NoError(t, err)\n\t\tassert.NoError(t, res.ExecutionError)\n\t\tassert.Equal(t, uint8(0), res.Receipt.ExitCode)\n\n\t\tt.Run(\"Valid PIP returns true\", func(t *testing.T) {\n\t\t\terr := runVerifyPIP(t, 3, commP, sectorId, pip)\n\n\t\t\tassert.NoError(t, err)\n\t\t})\n\n\t\tt.Run(\"PIP is invalid if miner hasn't committed sector\", func(t *testing.T) {\n\t\t\twrongSectorId := sectorId + 1\n\t\t\terr := runVerifyPIP(t, 3, commP, wrongSectorId, pip)\n\n\t\t\trequire.Error(t, err)\n\t\t\tassert.Equal(t, \"sector not committed\", err.Error())\n\t\t})\n\n\t\tt.Run(\"PIP is valid if miner's PoSts are before the end of the grace period\", func(t *testing.T) {\n\t\t\tblockHeight := blockheightOfPoSt + PieceInclusionGracePeriodBlocks - 1\n\t\t\terr := runVerifyPIP(t, blockHeight, commP, sectorId, pip)\n\n\t\t\tassert.NoError(t, err)\n\t\t})\n\n\t\tt.Run(\"PIP is valid if miner's PoSts are at the very end of the grace period\", func(t *testing.T) {\n\t\t\tblockHeight := blockheightOfPoSt + PieceInclusionGracePeriodBlocks\n\t\t\terr := runVerifyPIP(t, blockHeight, commP, sectorId, pip)\n\n\t\t\tassert.NoError(t, err)\n\t\t})\n\n\t\tt.Run(\"PIP is invalid if miner's PoSts are after the end of the grace period\", func(t *testing.T) {\n\t\t\tblockHeight := blockheightOfPoSt + PieceInclusionGracePeriodBlocks + 1\n\t\t\terr := runVerifyPIP(t, blockHeight, commP, sectorId, pip)\n\n\t\t\trequire.Error(t, err)\n\t\t\tassert.Equal(t, \"proofs out of date\", err.Error())\n\t\t})\n\n\t\tt.Run(\"PIP is invalid if proof is invalid\", func(t *testing.T) {\n\t\t\twrongPIP := append([]byte{pip[0] + 1}, pip[1:]...)\n\t\t\terr := runVerifyPIP(t, 3, commP, sectorId, wrongPIP)\n\n\t\t\trequire.Error(t, err)\n\t\t\tassert.Equal(t, \"invalid inclusion proof\", err.Error())\n\t\t})\n\n\t\tt.Run(\"Malformed PIP is a validation error\", func(t *testing.T) {\n\t\t\twrongPIP := pip[1:]\n\t\t\terr := runVerifyPIP(t, 3, commP, sectorId, wrongPIP)\n\n\t\t\tassert.Error(t, err)\n\t\t\tassert.Equal(t, \"malformed inclusion proof\", err.Error())\n\t\t})\n\t})\n}\n\nfunc TestGetProofsMode(t *testing.T) {\n\tctx := context.Background()\n\tst, vms := core.CreateStorages(ctx, t)\n\n\tgasTracker := vm.NewGasTracker()\n\tgasTracker.MsgGasLimit = 99999\n\n\tt.Run(\"in TestMode\", func(t *testing.T) {\n\t\tvmCtx := vm.NewVMContext(vm.NewContextParams{\n\t\t\tFrom:        &actor.Actor{},\n\t\t\tTo:          &actor.Actor{},\n\t\t\tMessage:     &types.Message{},\n\t\t\tState:       state.NewCachedStateTree(st),\n\t\t\tStorageMap:  vms,\n\t\t\tGasTracker:  gasTracker,\n\t\t\tBlockHeight: types.NewBlockHeight(0),\n\t\t\tAncestors:   []types.TipSet{},\n\t\t})\n\n\t\trequire.NoError(t, consensus.SetupDefaultActors(ctx, st, vms, types.TestProofsMode))\n\n\t\tmode, err := GetProofsMode(vmCtx)\n\t\trequire.NoError(t, err)\n\t\tassert.Equal(t, types.TestProofsMode, mode)\n\t})\n\n\tt.Run(\"in LiveMode\", func(t *testing.T) {\n\t\tvmCtx := vm.NewVMContext(vm.NewContextParams{\n\t\t\tFrom:        &actor.Actor{},\n\t\t\tTo:          &actor.Actor{},\n\t\t\tMessage:     &types.Message{},\n\t\t\tState:       state.NewCachedStateTree(st),\n\t\t\tStorageMap:  vms,\n\t\t\tGasTracker:  gasTracker,\n\t\t\tBlockHeight: types.NewBlockHeight(0),\n\t\t\tAncestors:   []types.TipSet{},\n\t\t})\n\n\t\trequire.NoError(t, consensus.SetupDefaultActors(ctx, st, vms, types.LiveProofsMode))\n\n\t\tmode, err := GetProofsMode(vmCtx)\n\t\trequire.NoError(t, err)\n\t\tassert.Equal(t, types.LiveProofsMode, mode)\n\t})\n}\n\nfunc parseAbiBoolean(bytes []byte) bool {\n\treturn bytes[0] == 1\n}\n", "idx": 1, "id": 18972, "msg": "`assert` doesn't halt the test on failure, so `result.Receipt.Return[0]` results in an index out of bounds error.", "proj": "filecoin-project-venus", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -31,7 +31,9 @@ import (\n )\n \n var (\n-\tgsRegex = regexp.MustCompile(`^gs://([a-z0-9][-_.a-z0-9]*)/(.+)$`)\n+\tbucket      = `([a-z0-9][-_.a-z0-9]*)`\n+\tbucketRegex = regexp.MustCompile(fmt.Sprintf(`^gs://%s/(.*)$`, bucket))\n+\tgsRegex     = regexp.MustCompile(fmt.Sprintf(`^gs://%s/(.+)$`, bucket))\n )\n \n // StorageClient implements domain.StorageClientInterface. It implements main Storage functions", "y": 1, "oldf": "//  Copyright 2019 Google Inc. All Rights Reserved.\n//\n//  Licensed under the Apache License, Version 2.0 (the \"License\");\n//  you may not use this file except in compliance with the License.\n//  You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n\npackage storageutils\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"cloud.google.com/go/storage\"\n\t\"github.com/GoogleCloudPlatform/compute-image-tools/cli_tools/common/domain\"\n\t\"github.com/GoogleCloudPlatform/compute-image-tools/cli_tools/common/utils/logging\"\n\t\"google.golang.org/api/iterator\"\n)\n\nvar (\n\tgsRegex = regexp.MustCompile(`^gs://([a-z0-9][-_.a-z0-9]*)/(.+)$`)\n)\n\n// StorageClient implements domain.StorageClientInterface. It implements main Storage functions\n// used by image import features.\ntype StorageClient struct {\n\tObjectDeleter commondomain.StorageObjectDeleterInterface\n\tStorageClient *storage.Client\n\tLogger        logging.LoggerInterface\n\tCtx           context.Context\n\tOic           commondomain.ObjectIteratorCreatorInterface\n}\n\n// NewStorageClient creates a StorageClient\nfunc NewStorageClient(ctx context.Context, client *storage.Client,\n\tlogger logging.LoggerInterface) (*StorageClient, error) {\n\tsc := &StorageClient{StorageClient: client, Ctx: ctx,\n\t\tOic: &ObjectIteratorCreator{ctx: ctx, sc: client}, Logger: logger}\n\n\tsc.ObjectDeleter = &StorageObjectDeleter{sc}\n\treturn sc, nil\n}\n\n// CreateBucket creates a GCS bucket\nfunc (sc *StorageClient) CreateBucket(\n\tbucketName string, project string, attrs *storage.BucketAttrs) error {\n\treturn sc.StorageClient.Bucket(bucketName).Create(sc.Ctx, project, attrs)\n}\n\n// Buckets returns a bucket iterator for all buckets within a project\nfunc (sc *StorageClient) Buckets(projectID string) *storage.BucketIterator {\n\treturn sc.StorageClient.Buckets(sc.Ctx, projectID)\n}\n\n// GetBucketAttrs returns bucket attributes for given bucket\nfunc (sc *StorageClient) GetBucketAttrs(bucket string) (*storage.BucketAttrs, error) {\n\treturn sc.StorageClient.Bucket(bucket).Attrs(sc.Ctx)\n}\n\n// GetObjectReader creates a new Reader to read the contents of the object.\nfunc (sc *StorageClient) GetObjectReader(bucket string, objectPath string) (io.ReadCloser, error) {\n\treturn sc.GetBucket(bucket).Object(objectPath).NewReader(sc.Ctx)\n}\n\n// GetBucket returns a BucketHandle, which provides operations on the named bucket.\nfunc (sc *StorageClient) GetBucket(bucket string) *storage.BucketHandle {\n\treturn sc.StorageClient.Bucket(bucket)\n}\n\n// GetObjects returns object iterator for given bucket and path\nfunc (sc *StorageClient) GetObjects(bucket string, objectPath string) commondomain.ObjectIteratorInterface {\n\treturn sc.Oic.CreateObjectIterator(bucket, objectPath)\n}\n\n// DeleteObject deletes GCS object in given bucket and object path\nfunc (sc *StorageClient) DeleteObject(bucket string, objectPath string) error {\n\treturn sc.ObjectDeleter.DeleteObject(bucket, objectPath)\n}\n\n// DeleteGcsPath deletes a GCS path, including files\nfunc (sc *StorageClient) DeleteGcsPath(gcsPath string) error {\n\tbucketName, objectPath, err := SplitGCSPath(gcsPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlog.Printf(\"Deleting content of: %v\", gcsPath)\n\n\tit := sc.GetObjects(bucketName, objectPath)\n\n\tfor {\n\t\tattrs, err := it.Next()\n\t\tif err == iterator.Done {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsc.Logger.Log(fmt.Sprintf(\"Deleting gs://%v/%v\\n\", bucketName, attrs.Name))\n\n\t\tif err := sc.DeleteObject(bucketName, attrs.Name); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// FindGcsFile finds a file in a GCS directory path for given file extension. File extension can\n// be a file name as well.\nfunc (sc *StorageClient) FindGcsFile(gcsDirectoryPath string, fileExtension string) (*storage.ObjectHandle, error) {\n\n\tbucketName, objectPath, err := SplitGCSPath(gcsDirectoryPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tit := sc.GetObjects(bucketName, objectPath)\n\tfor {\n\t\tattrs, err := it.Next()\n\t\tif err == iterator.Done {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif !strings.HasSuffix(attrs.Name, fileExtension) {\n\t\t\tcontinue\n\t\t}\n\t\tsc.Logger.Log(fmt.Sprintf(\"Found gs://%v/%v\\n\", bucketName, attrs.Name))\n\n\t\treturn sc.GetBucket(bucketName).Object(attrs.Name), nil\n\t}\n\treturn nil, fmt.Errorf(\n\t\t\"path %v doesn't contain a file with %v extension\", gcsDirectoryPath, fileExtension)\n}\n\n// GetGcsFileContent returns content of a GCS object as byte array\nfunc (sc *StorageClient) GetGcsFileContent(gcsObject *storage.ObjectHandle) ([]byte, error) {\n\treader, err := gcsObject.NewReader(sc.Ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn ioutil.ReadAll(reader)\n}\n\n// WriteToGCS writes content from a reader to destination bucket and path\nfunc (sc *StorageClient) WriteToGCS(\n\tdestinationBucketName string, destinationObjectPath string, reader io.Reader) error {\n\tdestinationBucket := sc.GetBucket(destinationBucketName)\n\tfileWriter := destinationBucket.Object(destinationObjectPath).NewWriter(sc.Ctx)\n\n\tif _, err := io.Copy(fileWriter, reader); err != nil {\n\t\treturn err\n\t}\n\n\treturn fileWriter.Close()\n}\n\n// Close closes the Client.\n//\n// Close need not be called at program exit.\nfunc (sc *StorageClient) Close() error {\n\treturn sc.StorageClient.Close()\n}\n\n// SplitGCSPath splits GCS path into bucket and object path portions\nfunc SplitGCSPath(p string) (string, string, error) {\n\tmatches := gsRegex.FindStringSubmatch(p)\n\tif matches != nil {\n\t\treturn matches[1], matches[2], nil\n\t}\n\n\treturn \"\", \"\", fmt.Errorf(\"%q is not a valid GCS path\", p)\n}\n\n// HTTPClient implements domain.HTTPClientInterface which abstracts HTTP functionality used by\n// image import features.\ntype HTTPClient struct {\n\thttpClient *http.Client\n}\n\n// Get executes HTTP GET request for given URL\nfunc (hc *HTTPClient) Get(url string) (resp *http.Response, err error) {\n\treturn hc.httpClient.Get(url)\n}\n\n// StorageObjectDeleter is responsible for deleting object\ntype StorageObjectDeleter struct {\n\tsc *StorageClient\n}\n\n// DeleteObject deletes GCS object in given bucket and path\nfunc (sod *StorageObjectDeleter) DeleteObject(bucket string, objectPath string) error {\n\treturn sod.sc.GetBucket(bucket).Object(objectPath).Delete(sod.sc.Ctx)\n}\n", "idx": 1, "id": 8743, "msg": "Won't you need to make the '/' optional?", "proj": "GoogleCloudPlatform-compute-image-tools", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -21,11 +21,14 @@\n package http\n \n import (\n+\t\"context\"\n \t\"net\"\n \t\"net/http\"\n \t\"sync\"\n \t\"time\"\n \n+\t\"golang.org/x/net/proxy\"\n+\n \t\"go.uber.org/yarpc/api/peer\"\n \t\"go.uber.org/yarpc/api/transport\"\n \tintsync \"go.uber.org/yarpc/internal/sync\"", "y": 1, "oldf": "// Copyright (c) 2017 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage http\n\nimport (\n\t\"net\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n\n\t\"go.uber.org/yarpc/api/peer\"\n\t\"go.uber.org/yarpc/api/transport\"\n\tintsync \"go.uber.org/yarpc/internal/sync\"\n\t\"go.uber.org/yarpc/peer/hostport\"\n\n\t\"github.com/opentracing/opentracing-go\"\n)\n\ntype transportConfig struct {\n\tkeepAlive           time.Duration\n\tmaxIdleConnsPerHost int\n\ttracer              opentracing.Tracer\n}\n\nvar defaultTransportConfig = transportConfig{\n\tkeepAlive:           30 * time.Second,\n\tmaxIdleConnsPerHost: 2,\n}\n\n// TransportOption customizes the behavior of an HTTP transport.\ntype TransportOption func(*transportConfig)\n\nfunc (TransportOption) httpOption() {}\n\n// KeepAlive specifies the keep-alive period for the network connection. If\n// zero, keep-alives are disabled.\n//\n// Defaults to 30 seconds.\nfunc KeepAlive(t time.Duration) TransportOption {\n\treturn func(c *transportConfig) {\n\t\tc.keepAlive = t\n\t}\n}\n\n// MaxIdleConnsPerHost specifies the number of idle (keep-alive) HTTP\n// connections that will be maintained per host.\n// Existing idle connections will be used instead of creating new HTTP\n// connections.\n//\n// Defaults to 2 connections.\nfunc MaxIdleConnsPerHost(i int) TransportOption {\n\treturn func(c *transportConfig) {\n\t\tc.maxIdleConnsPerHost = i\n\t}\n}\n\n// Tracer configures a tracer for the transport and all its inbounds and\n// outbounds.\nfunc Tracer(tracer opentracing.Tracer) TransportOption {\n\treturn func(c *transportConfig) {\n\t\tc.tracer = tracer\n\t}\n}\n\n// NewTransport creates a new HTTP transport for managing peers and sending requests\nfunc NewTransport(opts ...TransportOption) *Transport {\n\tcfg := defaultTransportConfig\n\tcfg.tracer = opentracing.GlobalTracer()\n\tfor _, o := range opts {\n\t\to(&cfg)\n\t}\n\n\treturn &Transport{\n\t\tonce:   intsync.Once(),\n\t\tclient: buildClient(&cfg),\n\t\tpeers:  make(map[string]*hostport.Peer),\n\t\ttracer: cfg.tracer,\n\t}\n}\n\nfunc buildClient(cfg *transportConfig) *http.Client {\n\treturn &http.Client{\n\t\tTransport: &http.Transport{\n\t\t\t// options lifted from https://golang.org/src/net/http/transport.go\n\t\t\tProxy: http.ProxyFromEnvironment,\n\t\t\tDial: (&net.Dialer{\n\t\t\t\tTimeout:   30 * time.Second,\n\t\t\t\tKeepAlive: cfg.keepAlive,\n\t\t\t}).Dial,\n\t\t\tTLSHandshakeTimeout:   10 * time.Second,\n\t\t\tExpectContinueTimeout: 1 * time.Second,\n\t\t\tMaxIdleConnsPerHost:   cfg.maxIdleConnsPerHost,\n\t\t},\n\t}\n}\n\n// Transport keeps track of HTTP peers and the associated HTTP client. It\n// allows using a single HTTP client to make requests to multiple YARPC\n// services and pooling the resources needed therein.\ntype Transport struct {\n\tlock sync.Mutex\n\tonce intsync.LifecycleOnce\n\n\tclient *http.Client\n\tpeers  map[string]*hostport.Peer\n\n\ttracer opentracing.Tracer\n}\n\nvar _ transport.Transport = (*Transport)(nil)\n\n// Start starts the HTTP transport.\nfunc (a *Transport) Start() error {\n\treturn a.once.Start(func() error {\n\t\treturn nil // Nothing to do\n\t})\n}\n\n// Stop stops the HTTP transport.\nfunc (a *Transport) Stop() error {\n\treturn a.once.Stop(func() error {\n\t\treturn nil // Nothing to do\n\t})\n}\n\n// IsRunning returns whether the HTTP transport is running.\nfunc (a *Transport) IsRunning() bool {\n\treturn a.once.IsRunning()\n}\n\n// RetainPeer gets or creates a Peer for the specified peer.Subscriber (usually a peer.Chooser)\nfunc (a *Transport) RetainPeer(pid peer.Identifier, sub peer.Subscriber) (peer.Peer, error) {\n\ta.lock.Lock()\n\tdefer a.lock.Unlock()\n\n\thppid, ok := pid.(hostport.PeerIdentifier)\n\tif !ok {\n\t\treturn nil, peer.ErrInvalidPeerType{\n\t\t\tExpectedType:   \"hostport.PeerIdentifier\",\n\t\t\tPeerIdentifier: pid,\n\t\t}\n\t}\n\n\tp := a.getOrCreatePeer(hppid)\n\tp.Subscribe(sub)\n\treturn p, nil\n}\n\n// **NOTE** should only be called while the lock write mutex is acquired\nfunc (a *Transport) getOrCreatePeer(pid hostport.PeerIdentifier) *hostport.Peer {\n\tif p, ok := a.peers[pid.Identifier()]; ok {\n\t\treturn p\n\t}\n\n\tp := hostport.NewPeer(pid, a)\n\tp.SetStatus(peer.Available)\n\n\ta.peers[p.Identifier()] = p\n\n\treturn p\n}\n\n// ReleasePeer releases a peer from the peer.Subscriber and removes that peer from the Transport if nothing is listening to it\nfunc (a *Transport) ReleasePeer(pid peer.Identifier, sub peer.Subscriber) error {\n\ta.lock.Lock()\n\tdefer a.lock.Unlock()\n\n\tp, ok := a.peers[pid.Identifier()]\n\tif !ok {\n\t\treturn peer.ErrTransportHasNoReferenceToPeer{\n\t\t\tTransportName:  \"http.Transport\",\n\t\t\tPeerIdentifier: pid.Identifier(),\n\t\t}\n\t}\n\n\tif err := p.Unsubscribe(sub); err != nil {\n\t\treturn err\n\t}\n\n\tif p.NumSubscribers() == 0 {\n\t\tdelete(a.peers, pid.Identifier())\n\t}\n\n\treturn nil\n}\n", "idx": 1, "id": 14078, "msg": "import order: internal packages, then external packages", "proj": "yarpc-yarpc-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -21,9 +21,9 @@ import org.apache.tuweni.bytes.Bytes32;\n \n public class StorageEntriesCollector<V> implements TrieIterator.LeafHandler<V> {\n \n-  private final Bytes32 startKeyHash;\n-  private final int limit;\n-  private final Map<Bytes32, V> values = new TreeMap<>();\n+  protected final Bytes32 startKeyHash;\n+  protected int limit = 0;\n+  protected final Map<Bytes32, V> values = new TreeMap<>();\n \n   public StorageEntriesCollector(final Bytes32 startKeyHash, final int limit) {\n     this.startKeyHash = startKeyHash;", "y": 1, "oldf": "/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n */\npackage org.hyperledger.besu.ethereum.trie;\n\nimport java.util.Map;\nimport java.util.TreeMap;\n\nimport org.apache.tuweni.bytes.Bytes32;\n\npublic class StorageEntriesCollector<V> implements TrieIterator.LeafHandler<V> {\n\n  private final Bytes32 startKeyHash;\n  private final int limit;\n  private final Map<Bytes32, V> values = new TreeMap<>();\n\n  public StorageEntriesCollector(final Bytes32 startKeyHash, final int limit) {\n    this.startKeyHash = startKeyHash;\n    this.limit = limit;\n  }\n\n  public static <V> Map<Bytes32, V> collectEntries(\n      final Node<V> root, final Bytes32 startKeyHash, final int limit) {\n    final StorageEntriesCollector<V> entriesCollector =\n        new StorageEntriesCollector<>(startKeyHash, limit);\n    final TrieIterator<V> visitor = new TrieIterator<>(entriesCollector, false);\n    root.accept(visitor, CompactEncoding.bytesToPath(startKeyHash));\n    return entriesCollector.getValues();\n  }\n\n  private boolean limitReached() {\n    return limit <= values.size();\n  }\n\n  @Override\n  public TrieIterator.State onLeaf(final Bytes32 keyHash, final Node<V> node) {\n    if (keyHash.compareTo(startKeyHash) >= 0) {\n      node.getValue().ifPresent(value -> values.put(keyHash, value));\n    }\n    return limitReached() ? TrieIterator.State.STOP : TrieIterator.State.CONTINUE;\n  }\n\n  public Map<Bytes32, V> getValues() {\n    return values;\n  }\n}\n", "idx": 1, "id": 26147, "msg": "Why initialize and remove final?", "proj": "hyperledger-besu", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -172,6 +172,7 @@ type Config struct {\n \tIptablesLockFilePath               string        `config:\"file;/run/xtables.lock\"`\n \tIptablesLockTimeoutSecs            time.Duration `config:\"seconds;0\"`\n \tIptablesLockProbeIntervalMillis    time.Duration `config:\"millis;50\"`\n+\tIptablesFeatureDetectOverride      string        `config:\"string;;local\"`\n \tIpsetsRefreshInterval              time.Duration `config:\"seconds;10\"`\n \tMaxIpsetSize                       int           `config:\"int;1048576;non-zero\"`\n \tXDPRefreshInterval                 time.Duration `config:\"seconds;90\"`", "y": 1, "oldf": "// Copyright (c) 2020 Tigera, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\n\t\"github.com/projectcalico/libcalico-go/lib/apiconfig\"\n\t\"github.com/projectcalico/libcalico-go/lib/names\"\n\t\"github.com/projectcalico/libcalico-go/lib/numorstring\"\n\n\t\"github.com/projectcalico/felix/idalloc\"\n)\n\nvar (\n\t// RegexpIfaceElemRegexp matches an individual element in the overall interface list;\n\t// assumes the value represents a regular expression and is marked by '/' at the start\n\t// and end and cannot have spaces\n\tRegexpIfaceElemRegexp = regexp.MustCompile(`^\\/[^\\s]+\\/$`)\n\t// NonRegexpIfaceElemRegexp matches an individual element in the overall interface list;\n\t// assumes the value is between 1-15 chars long and only be alphanumeric or - or _\n\tNonRegexpIfaceElemRegexp = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}$`)\n\tIfaceListRegexp          = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}(,[a-zA-Z0-9_-]{1,15})*$`)\n\tAuthorityRegexp          = regexp.MustCompile(`^[^:/]+:\\d+$`)\n\tHostnameRegexp           = regexp.MustCompile(`^[a-zA-Z0-9_.-]+$`)\n\tStringRegexp             = regexp.MustCompile(`^.*$`)\n\tIfaceParamRegexp         = regexp.MustCompile(`^[a-zA-Z0-9:._+-]{1,15}$`)\n\t// Hostname  have to be valid ipv4, ipv6 or strings up to 64 characters.\n\tHostAddressRegexp = regexp.MustCompile(`^[a-zA-Z0-9:._+-]{1,64}$`)\n)\n\nconst (\n\tmaxUint = ^uint(0)\n\tmaxInt  = int(maxUint >> 1)\n\tminInt  = -maxInt - 1\n)\n\n// Source of a config value.  Values from higher-numbered sources override\n// those from lower-numbered sources.  Note: some parameters (such as those\n// needed to connect to the datastore) can only be set from a local source.\ntype Source uint8\n\nconst (\n\tDefault = iota\n\tDatastoreGlobal\n\tDatastorePerHost\n\tConfigFile\n\tEnvironmentVariable\n\tInternalOverride\n)\n\nvar SourcesInDescendingOrder = []Source{InternalOverride, EnvironmentVariable, ConfigFile, DatastorePerHost, DatastoreGlobal}\n\nfunc (source Source) String() string {\n\tswitch source {\n\tcase Default:\n\t\treturn \"<default>\"\n\tcase DatastoreGlobal:\n\t\treturn \"datastore (global)\"\n\tcase DatastorePerHost:\n\t\treturn \"datastore (per-host)\"\n\tcase ConfigFile:\n\t\treturn \"config file\"\n\tcase EnvironmentVariable:\n\t\treturn \"environment variable\"\n\tcase InternalOverride:\n\t\treturn \"internal override\"\n\t}\n\treturn fmt.Sprintf(\"<unknown(%v)>\", uint8(source))\n}\n\nfunc (source Source) Local() bool {\n\tswitch source {\n\tcase Default, ConfigFile, EnvironmentVariable, InternalOverride:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// Config contains the best, parsed config values loaded from the various sources.\n// We use tags to control the parsing and validation.\ntype Config struct {\n\t// Configuration parameters.\n\tUseInternalDataplaneDriver bool   `config:\"bool;true\"`\n\tDataplaneDriver            string `config:\"file(must-exist,executable);calico-iptables-plugin;non-zero,die-on-fail,skip-default-validation\"`\n\n\t// Wireguard configuration\n\tWireguardEnabled             bool   `config:\"bool;false\"`\n\tWireguardListeningPort       int    `config:\"int;51820\"`\n\tWireguardRoutingRulePriority int    `config:\"int;99\"`\n\tWireguardInterfaceName       string `config:\"iface-param;wireguard.cali;non-zero\"`\n\tWireguardMTU                 int    `config:\"int;1420;non-zero\"`\n\n\tBPFEnabled                         bool           `config:\"bool;false\"`\n\tBPFDisableUnprivileged             bool           `config:\"bool;true\"`\n\tBPFLogLevel                        string         `config:\"oneof(off,info,debug);off;non-zero\"`\n\tBPFDataIfacePattern                *regexp.Regexp `config:\"regexp;^(en.*|eth.*|tunl0$)\"`\n\tBPFConnectTimeLoadBalancingEnabled bool           `config:\"bool;true\"`\n\tBPFExternalServiceMode             string         `config:\"oneof(tunnel,dsr);tunnel;non-zero\"`\n\tBPFKubeProxyIptablesCleanupEnabled bool           `config:\"bool;true\"`\n\tBPFKubeProxyMinSyncPeriod          time.Duration  `config:\"seconds;1\"`\n\tBPFKubeProxyEndpointSlicesEnabled  bool           `config:\"bool;false\"`\n\n\t// DebugBPFCgroupV2 controls the cgroup v2 path that we apply the connect-time load balancer to.  Most distros\n\t// are configured for cgroup v1, which prevents all but hte root cgroup v2 from working so this is only useful\n\t// for development right now.\n\tDebugBPFCgroupV2 string `config:\"string;;local\"`\n\t// DebugBPFMapRepinEnabled can be used to prevent Felix from repinning its BPF maps at startup.  This is useful for\n\t// testing with multiple Felix instances running on one host.\n\tDebugBPFMapRepinEnabled bool `config:\"bool;true;local\"`\n\n\tDatastoreType string `config:\"oneof(kubernetes,etcdv3);etcdv3;non-zero,die-on-fail,local\"`\n\n\tFelixHostname string `config:\"hostname;;local,non-zero\"`\n\n\tEtcdAddr      string   `config:\"authority;127.0.0.1:2379;local\"`\n\tEtcdScheme    string   `config:\"oneof(http,https);http;local\"`\n\tEtcdKeyFile   string   `config:\"file(must-exist);;local\"`\n\tEtcdCertFile  string   `config:\"file(must-exist);;local\"`\n\tEtcdCaFile    string   `config:\"file(must-exist);;local\"`\n\tEtcdEndpoints []string `config:\"endpoint-list;;local\"`\n\n\tTyphaAddr           string        `config:\"authority;;local\"`\n\tTyphaK8sServiceName string        `config:\"string;;local\"`\n\tTyphaK8sNamespace   string        `config:\"string;kube-system;non-zero,local\"`\n\tTyphaReadTimeout    time.Duration `config:\"seconds;30;local\"`\n\tTyphaWriteTimeout   time.Duration `config:\"seconds;10;local\"`\n\n\t// Client-side TLS config for Felix's communication with Typha.  If any of these are\n\t// specified, they _all_ must be - except that either TyphaCN or TyphaURISAN may be left\n\t// unset.  Felix will then initiate a secure (TLS) connection to Typha.  Typha must present\n\t// a certificate signed by a CA in TyphaCAFile, and with CN matching TyphaCN or URI SAN\n\t// matching TyphaURISAN.\n\tTyphaKeyFile  string `config:\"file(must-exist);;local\"`\n\tTyphaCertFile string `config:\"file(must-exist);;local\"`\n\tTyphaCAFile   string `config:\"file(must-exist);;local\"`\n\tTyphaCN       string `config:\"string;;local\"`\n\tTyphaURISAN   string `config:\"string;;local\"`\n\n\tIpv6Support bool `config:\"bool;true\"`\n\n\tIptablesBackend                    string        `config:\"oneof(legacy,nft,auto);legacy\"`\n\tRouteRefreshInterval               time.Duration `config:\"seconds;90\"`\n\tDeviceRouteSourceAddress           net.IP        `config:\"ipv4;\"`\n\tDeviceRouteProtocol                int           `config:\"int;3\"`\n\tRemoveExternalRoutes               bool          `config:\"bool;true\"`\n\tIptablesRefreshInterval            time.Duration `config:\"seconds;90\"`\n\tIptablesPostWriteCheckIntervalSecs time.Duration `config:\"seconds;1\"`\n\tIptablesLockFilePath               string        `config:\"file;/run/xtables.lock\"`\n\tIptablesLockTimeoutSecs            time.Duration `config:\"seconds;0\"`\n\tIptablesLockProbeIntervalMillis    time.Duration `config:\"millis;50\"`\n\tIpsetsRefreshInterval              time.Duration `config:\"seconds;10\"`\n\tMaxIpsetSize                       int           `config:\"int;1048576;non-zero\"`\n\tXDPRefreshInterval                 time.Duration `config:\"seconds;90\"`\n\n\tPolicySyncPathPrefix string `config:\"file;;\"`\n\n\tNetlinkTimeoutSecs time.Duration `config:\"seconds;10\"`\n\n\tMetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"`\n\tMetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"`\n\n\tOpenstackRegion string `config:\"region;;die-on-fail\"`\n\n\tInterfacePrefix  string           `config:\"iface-list;cali;non-zero,die-on-fail\"`\n\tInterfaceExclude []*regexp.Regexp `config:\"iface-list-regexp;kube-ipvs0\"`\n\n\tChainInsertMode             string `config:\"oneof(insert,append);insert;non-zero,die-on-fail\"`\n\tDefaultEndpointToHostAction string `config:\"oneof(DROP,RETURN,ACCEPT);DROP;non-zero,die-on-fail\"`\n\tIptablesFilterAllowAction   string `config:\"oneof(ACCEPT,RETURN);ACCEPT;non-zero,die-on-fail\"`\n\tIptablesMangleAllowAction   string `config:\"oneof(ACCEPT,RETURN);ACCEPT;non-zero,die-on-fail\"`\n\tLogPrefix                   string `config:\"string;calico-packet\"`\n\n\tLogFilePath string `config:\"file;/var/log/calico/felix.log;die-on-fail\"`\n\n\tLogSeverityFile   string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\tLogSeverityScreen string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\tLogSeveritySys    string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\n\tVXLANEnabled        bool   `config:\"bool;false\"`\n\tVXLANPort           int    `config:\"int;4789\"`\n\tVXLANVNI            int    `config:\"int;4096\"`\n\tVXLANMTU            int    `config:\"int;1410;non-zero\"`\n\tIPv4VXLANTunnelAddr net.IP `config:\"ipv4;\"`\n\tVXLANTunnelMACAddr  string `config:\"string;\"`\n\n\tIpInIpEnabled    bool   `config:\"bool;false\"`\n\tIpInIpMtu        int    `config:\"int;1440;non-zero\"`\n\tIpInIpTunnelAddr net.IP `config:\"ipv4;\"`\n\n\tAWSSrcDstCheck string `config:\"oneof(DoNothing,Enable,Disable);DoNothing;non-zero\"`\n\n\tReportingIntervalSecs time.Duration `config:\"seconds;30\"`\n\tReportingTTLSecs      time.Duration `config:\"seconds;90\"`\n\n\tEndpointReportingEnabled   bool          `config:\"bool;false\"`\n\tEndpointReportingDelaySecs time.Duration `config:\"seconds;1\"`\n\n\tIptablesMarkMask uint32 `config:\"mark-bitmask;0xffff0000;non-zero,die-on-fail\"`\n\n\tDisableConntrackInvalidCheck bool `config:\"bool;false\"`\n\n\tHealthEnabled                   bool   `config:\"bool;false\"`\n\tHealthPort                      int    `config:\"int(0,65535);9099\"`\n\tHealthHost                      string `config:\"host-address;localhost\"`\n\tPrometheusMetricsEnabled        bool   `config:\"bool;false\"`\n\tPrometheusMetricsHost           string `config:\"host-address;\"`\n\tPrometheusMetricsPort           int    `config:\"int(0,65535);9091\"`\n\tPrometheusGoMetricsEnabled      bool   `config:\"bool;true\"`\n\tPrometheusProcessMetricsEnabled bool   `config:\"bool;true\"`\n\n\tFailsafeInboundHostPorts  []ProtoPort `config:\"port-list;tcp:22,udp:68,tcp:179,tcp:2379,tcp:2380,tcp:5473,tcp:6443,tcp:6666,tcp:6667;die-on-fail\"`\n\tFailsafeOutboundHostPorts []ProtoPort `config:\"port-list;udp:53,udp:67,tcp:179,tcp:2379,tcp:2380,tcp:5473,tcp:6443,tcp:6666,tcp:6667;die-on-fail\"`\n\n\tKubeNodePortRanges []numorstring.Port `config:\"portrange-list;30000:32767\"`\n\tNATPortRange       numorstring.Port   `config:\"portrange;\"`\n\tNATOutgoingAddress net.IP             `config:\"ipv4;\"`\n\n\tUsageReportingEnabled          bool          `config:\"bool;true\"`\n\tUsageReportingInitialDelaySecs time.Duration `config:\"seconds;300\"`\n\tUsageReportingIntervalSecs     time.Duration `config:\"seconds;86400\"`\n\tClusterGUID                    string        `config:\"string;baddecaf\"`\n\tClusterType                    string        `config:\"string;\"`\n\tCalicoVersion                  string        `config:\"string;\"`\n\n\tExternalNodesCIDRList []string `config:\"cidr-list;;die-on-fail\"`\n\n\tDebugMemoryProfilePath          string        `config:\"file;;\"`\n\tDebugCPUProfilePath             string        `config:\"file;/tmp/felix-cpu-<timestamp>.pprof;\"`\n\tDebugDisableLogDropping         bool          `config:\"bool;false\"`\n\tDebugSimulateCalcGraphHangAfter time.Duration `config:\"seconds;0\"`\n\tDebugSimulateDataplaneHangAfter time.Duration `config:\"seconds;0\"`\n\tDebugPanicAfter                 time.Duration `config:\"seconds;0\"`\n\tDebugSimulateDataRace           bool          `config:\"bool;false\"`\n\n\t// Configure where Felix gets its routing information.\n\t// - workloadIPs: use workload endpoints to construct routes.\n\t// - calicoIPAM: use IPAM data to contruct routes.\n\tRouteSource string `config:\"oneof(WorkloadIPs,CalicoIPAM);CalicoIPAM\"`\n\n\tRouteTableRange idalloc.IndexRange `config:\"route-table-range;1-250;die-on-fail\"`\n\n\tIptablesNATOutgoingInterfaceFilter string `config:\"iface-param;\"`\n\n\tSidecarAccelerationEnabled bool `config:\"bool;false\"`\n\tXDPEnabled                 bool `config:\"bool;true\"`\n\tGenericXDPEnabled          bool `config:\"bool;false\"`\n\n\tVariant string `config:\"string;Calico\"`\n\n\t// State tracking.\n\n\t// internalOverrides contains our highest priority config source, generated from internal constraints\n\t// such as kernel version support.\n\tinternalOverrides map[string]string\n\t// sourceToRawConfig maps each source to the set of config that was give to us via UpdateFrom.\n\tsourceToRawConfig map[Source]map[string]string\n\t// rawValues maps keys to the current highest-priority raw value.\n\trawValues map[string]string\n\t// Err holds the most recent error from a config update.\n\tErr error\n\n\tloadClientConfigFromEnvironment func() (*apiconfig.CalicoAPIConfig, error)\n\n\tuseNodeResourceUpdates bool\n}\n\n// Copy makes a copy of the object.  Internal state is deep copied but config parameters are only shallow copied.\n// This saves work since updates to the copy will trigger the config params to be recalculated.\nfunc (config *Config) Copy() *Config {\n\t// Start by shallow-copying the object.\n\tcp := *config\n\n\t// Copy the internal state over as a deep copy.\n\tcp.internalOverrides = map[string]string{}\n\tfor k, v := range config.internalOverrides {\n\t\tcp.internalOverrides[k] = v\n\t}\n\n\tcp.sourceToRawConfig = map[Source]map[string]string{}\n\tfor k, v := range config.sourceToRawConfig {\n\t\tcp.sourceToRawConfig[k] = map[string]string{}\n\t\tfor k2, v2 := range v {\n\t\t\tcp.sourceToRawConfig[k][k2] = v2\n\t\t}\n\t}\n\n\tcp.rawValues = map[string]string{}\n\tfor k, v := range config.rawValues {\n\t\tcp.rawValues[k] = v\n\t}\n\n\treturn &cp\n}\n\ntype ProtoPort struct {\n\tProtocol string\n\tPort     uint16\n}\n\n// Load parses and merges the rawData from one particular source into this config object.\n// If there is a config value already loaded from a higher-priority source, then\n// the new value will be ignored (after validation).\nfunc (config *Config) UpdateFrom(rawData map[string]string, source Source) (changed bool, err error) {\n\tlog.Infof(\"Merging in config from %v: %v\", source, rawData)\n\t// Defensively take a copy of the raw data, in case we've been handed\n\t// a mutable map by mistake.\n\trawDataCopy := make(map[string]string)\n\tfor k, v := range rawData {\n\t\tif v == \"\" {\n\t\t\tlog.WithFields(log.Fields{\n\t\t\t\t\"name\":   k,\n\t\t\t\t\"source\": source,\n\t\t\t}).Info(\"Ignoring empty configuration parameter. Use value 'none' if \" +\n\t\t\t\t\"your intention is to explicitly disable the default value.\")\n\t\t\tcontinue\n\t\t}\n\t\trawDataCopy[k] = v\n\t}\n\tconfig.sourceToRawConfig[source] = rawDataCopy\n\n\tchanged, err = config.resolve()\n\treturn\n}\n\nfunc (config *Config) IsLeader() bool {\n\treturn config.Variant == \"Calico\"\n}\n\nfunc (config *Config) InterfacePrefixes() []string {\n\treturn strings.Split(config.InterfacePrefix, \",\")\n}\n\nfunc (config *Config) OpenstackActive() bool {\n\tif strings.Contains(strings.ToLower(config.ClusterType), \"openstack\") {\n\t\t// OpenStack is explicitly known to be present.  Newer versions of the OpenStack plugin\n\t\t// set this flag.\n\t\tlog.Debug(\"Cluster type contains OpenStack\")\n\t\treturn true\n\t}\n\t// If we get here, either OpenStack isn't present or we're running against an old version\n\t// of the OpenStack plugin, which doesn't set the flag.  Use heuristics based on the\n\t// presence of the OpenStack-related parameters.\n\tif config.MetadataAddr != \"\" && config.MetadataAddr != \"127.0.0.1\" {\n\t\tlog.Debug(\"OpenStack metadata IP set to non-default, assuming OpenStack active\")\n\t\treturn true\n\t}\n\tif config.MetadataPort != 0 && config.MetadataPort != 8775 {\n\t\tlog.Debug(\"OpenStack metadata port set to non-default, assuming OpenStack active\")\n\t\treturn true\n\t}\n\tfor _, prefix := range config.InterfacePrefixes() {\n\t\tif prefix == \"tap\" {\n\t\t\tlog.Debug(\"Interface prefix list contains 'tap', assuming OpenStack\")\n\t\t\treturn true\n\t\t}\n\t}\n\tlog.Debug(\"No evidence this is an OpenStack deployment; disabling OpenStack special-cases\")\n\treturn false\n}\n\nfunc (config *Config) resolve() (changed bool, err error) {\n\tnewRawValues := make(map[string]string)\n\t// Map from lower-case version of name to the highest-priority source found so far.\n\t// We use the lower-case version of the name since we can calculate it both for\n\t// expected and \"raw\" parameters, which may be used by plugins.\n\tnameToSource := make(map[string]Source)\n\tfor _, source := range SourcesInDescendingOrder {\n\tvalueLoop:\n\t\tfor rawName, rawValue := range config.sourceToRawConfig[source] {\n\t\t\tlowerCaseName := strings.ToLower(rawName)\n\t\t\tcurrentSource := nameToSource[lowerCaseName]\n\t\t\tparam, ok := knownParams[lowerCaseName]\n\t\t\tif !ok {\n\t\t\t\tif source >= currentSource {\n\t\t\t\t\t// Stash the raw value in case it's useful for an external\n\t\t\t\t\t// dataplane driver.  Use the raw name since the driver may\n\t\t\t\t\t// want it.\n\t\t\t\t\tnewRawValues[rawName] = rawValue\n\t\t\t\t\tnameToSource[lowerCaseName] = source\n\t\t\t\t}\n\t\t\t\tlog.WithField(\"raw name\", rawName).Info(\n\t\t\t\t\t\"Ignoring unknown config param.\")\n\t\t\t\tcontinue valueLoop\n\t\t\t}\n\t\t\tmetadata := param.GetMetadata()\n\t\t\tname := metadata.Name\n\t\t\tif metadata.Local && !source.Local() {\n\t\t\t\tlog.Warningf(\"Ignoring local-only configuration for %v from %v\",\n\t\t\t\t\tname, source)\n\t\t\t\tcontinue valueLoop\n\t\t\t}\n\n\t\t\tlog.Infof(\"Parsing value for %v: %v (from %v)\",\n\t\t\t\tname, rawValue, source)\n\t\t\tvar value interface{}\n\t\t\tif strings.ToLower(rawValue) == \"none\" {\n\t\t\t\t// Special case: we allow a value of \"none\" to force the value to\n\t\t\t\t// the zero value for a field.  The zero value often differs from\n\t\t\t\t// the default value.  Typically, the zero value means \"turn off\n\t\t\t\t// the feature\".\n\t\t\t\tif metadata.NonZero {\n\t\t\t\t\terr = errors.New(\"non-zero field cannot be set to none\")\n\t\t\t\t\tlog.Errorf(\n\t\t\t\t\t\t\"Failed to parse value for %v: %v from source %v. %v\",\n\t\t\t\t\t\tname, rawValue, source, err)\n\t\t\t\t\tconfig.Err = err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tvalue = metadata.ZeroValue\n\t\t\t\tlog.Infof(\"Value set to 'none', replacing with zero-value: %#v.\",\n\t\t\t\t\tvalue)\n\t\t\t} else {\n\t\t\t\tvalue, err = param.Parse(rawValue)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogCxt := log.WithError(err).WithField(\"source\", source)\n\t\t\t\t\tif metadata.DieOnParseFailure {\n\t\t\t\t\t\tlogCxt.Error(\"Invalid (required) config value.\")\n\t\t\t\t\t\tconfig.Err = err\n\t\t\t\t\t\treturn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogCxt.WithField(\"default\", metadata.Default).Warn(\n\t\t\t\t\t\t\t\"Replacing invalid value with default\")\n\t\t\t\t\t\tvalue = metadata.Default\n\t\t\t\t\t\terr = nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlog.Infof(\"Parsed value for %v: %v (from %v)\",\n\t\t\t\tname, value, source)\n\t\t\tif source < currentSource {\n\t\t\t\tlog.Infof(\"Skipping config value for %v from %v; \"+\n\t\t\t\t\t\"already have a value from %v\", name,\n\t\t\t\t\tsource, currentSource)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfield := reflect.ValueOf(config).Elem().FieldByName(name)\n\t\t\tfield.Set(reflect.ValueOf(value))\n\t\t\tnewRawValues[name] = rawValue\n\t\t\tnameToSource[lowerCaseName] = source\n\t\t}\n\t}\n\tchanged = !reflect.DeepEqual(newRawValues, config.rawValues)\n\tconfig.rawValues = newRawValues\n\treturn\n}\n\nfunc (config *Config) setBy(name string, source Source) bool {\n\t_, set := config.sourceToRawConfig[source][name]\n\treturn set\n}\n\nfunc (config *Config) setByConfigFileOrEnvironment(name string) bool {\n\treturn config.setBy(name, ConfigFile) || config.setBy(name, EnvironmentVariable)\n}\n\nfunc (config *Config) DatastoreConfig() apiconfig.CalicoAPIConfig {\n\t// We want Felix's datastore connection to be fully configurable using the same\n\t// CALICO_XXX_YYY (or just XXX_YYY) environment variables that work for any libcalico-go\n\t// client - for both the etcdv3 and KDD cases.  However, for the etcd case, Felix has for a\n\t// long time supported FELIX_XXXYYY environment variables, and we want those to keep working\n\t// too.\n\n\t// To achieve that, first build a CalicoAPIConfig using libcalico-go's\n\t// LoadClientConfigFromEnvironment - which means incorporating defaults and CALICO_XXX_YYY\n\t// and XXX_YYY variables.\n\tcfg, err := config.loadClientConfigFromEnvironment()\n\tif err != nil {\n\t\tlog.WithError(err).Panic(\"Failed to create datastore config\")\n\t}\n\n\t// Now allow FELIX_XXXYYY variables or XxxYyy config file settings to override that, in the\n\t// etcd case. Note that that etcd options are set even if the DatastoreType isn't etcdv3.\n\t// This allows the user to rely the default DatastoreType being etcdv3 and still being able\n\t// to configure the other etcdv3 options. As of the time of this code change, the etcd options\n\t// have no affect if the DatastoreType is not etcdv3.\n\n\t// Datastore type, either etcdv3 or kubernetes\n\tif config.setByConfigFileOrEnvironment(\"DatastoreType\") {\n\t\tlog.Infof(\"Overriding DatastoreType from felix config to %s\", config.DatastoreType)\n\t\tif config.DatastoreType == string(apiconfig.EtcdV3) {\n\t\t\tcfg.Spec.DatastoreType = apiconfig.EtcdV3\n\t\t} else if config.DatastoreType == string(apiconfig.Kubernetes) {\n\t\t\tcfg.Spec.DatastoreType = apiconfig.Kubernetes\n\t\t}\n\t}\n\n\t// Endpoints.\n\tif config.setByConfigFileOrEnvironment(\"EtcdEndpoints\") && len(config.EtcdEndpoints) > 0 {\n\t\tlog.Infof(\"Overriding EtcdEndpoints from felix config to %s\", config.EtcdEndpoints)\n\t\tcfg.Spec.EtcdEndpoints = strings.Join(config.EtcdEndpoints, \",\")\n\t} else if config.setByConfigFileOrEnvironment(\"EtcdAddr\") {\n\t\tetcdEndpoints := config.EtcdScheme + \"://\" + config.EtcdAddr\n\t\tlog.Infof(\"Overriding EtcdEndpoints from felix config to %s\", etcdEndpoints)\n\t\tcfg.Spec.EtcdEndpoints = etcdEndpoints\n\t}\n\t// TLS.\n\tif config.setByConfigFileOrEnvironment(\"EtcdKeyFile\") {\n\t\tlog.Infof(\"Overriding EtcdKeyFile from felix config to %s\", config.EtcdKeyFile)\n\t\tcfg.Spec.EtcdKeyFile = config.EtcdKeyFile\n\t}\n\tif config.setByConfigFileOrEnvironment(\"EtcdCertFile\") {\n\t\tlog.Infof(\"Overriding EtcdCertFile from felix config to %s\", config.EtcdCertFile)\n\t\tcfg.Spec.EtcdCertFile = config.EtcdCertFile\n\t}\n\tif config.setByConfigFileOrEnvironment(\"EtcdCaFile\") {\n\t\tlog.Infof(\"Overriding EtcdCaFile from felix config to %s\", config.EtcdCaFile)\n\t\tcfg.Spec.EtcdCACertFile = config.EtcdCaFile\n\t}\n\n\tif !(config.IpInIpEnabled || config.VXLANEnabled || config.BPFEnabled) {\n\t\t// Polling k8s for node updates is expensive (because we get many superfluous\n\t\t// updates) so disable if we don't need it.\n\t\tlog.Info(\"Encap disabled, disabling node poll (if KDD is in use).\")\n\t\tcfg.Spec.K8sDisableNodePoll = true\n\t}\n\treturn *cfg\n}\n\n// Validate() performs cross-field validation.\nfunc (config *Config) Validate() (err error) {\n\tif config.FelixHostname == \"\" {\n\t\terr = errors.New(\"Failed to determine hostname\")\n\t}\n\n\tif config.DatastoreType == \"etcdv3\" && len(config.EtcdEndpoints) == 0 {\n\t\tif config.EtcdScheme == \"\" {\n\t\t\terr = errors.New(\"EtcdEndpoints and EtcdScheme both missing\")\n\t\t}\n\t\tif config.EtcdAddr == \"\" {\n\t\t\terr = errors.New(\"EtcdEndpoints and EtcdAddr both missing\")\n\t\t}\n\t}\n\n\t// If any client-side TLS config parameters are specified, they _all_ must be - except that\n\t// either TyphaCN or TyphaURISAN may be left unset.\n\tif config.TyphaCAFile != \"\" ||\n\t\tconfig.TyphaCertFile != \"\" ||\n\t\tconfig.TyphaKeyFile != \"\" ||\n\t\tconfig.TyphaCN != \"\" ||\n\t\tconfig.TyphaURISAN != \"\" {\n\t\t// Some TLS config specified.\n\t\tif config.TyphaKeyFile == \"\" ||\n\t\t\tconfig.TyphaCertFile == \"\" ||\n\t\t\tconfig.TyphaCAFile == \"\" ||\n\t\t\t(config.TyphaCN == \"\" && config.TyphaURISAN == \"\") {\n\t\t\terr = errors.New(\"If any Felix-Typha TLS config parameters are specified,\" +\n\t\t\t\t\" they _all_ must be\" +\n\t\t\t\t\" - except that either TyphaCN or TyphaURISAN may be left unset.\")\n\t\t}\n\t}\n\n\tif err != nil {\n\t\tconfig.Err = err\n\t}\n\treturn\n}\n\nvar knownParams map[string]param\n\nfunc loadParams() {\n\tknownParams = make(map[string]param)\n\tconfig := Config{}\n\tkind := reflect.TypeOf(config)\n\tmetaRegexp := regexp.MustCompile(`^([^;(]+)(?:\\(([^)]*)\\))?;` +\n\t\t`([^;]*)(?:;` +\n\t\t`([^;]*))?$`)\n\tfor ii := 0; ii < kind.NumField(); ii++ {\n\t\tfield := kind.Field(ii)\n\t\ttag := field.Tag.Get(\"config\")\n\t\tif tag == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tcaptures := metaRegexp.FindStringSubmatch(tag)\n\t\tif len(captures) == 0 {\n\t\t\tlog.Panicf(\"Failed to parse metadata for config param %v\", field.Name)\n\t\t}\n\t\tlog.Debugf(\"%v: metadata captures: %#v\", field.Name, captures)\n\t\tkind := captures[1]       // Type: \"int|oneof|bool|port-list|...\"\n\t\tkindParams := captures[2] // Parameters for the type: e.g. for oneof \"http,https\"\n\t\tdefaultStr := captures[3] // Default value e.g \"1.0\"\n\t\tflags := captures[4]\n\t\tvar param param\n\t\tvar err error\n\t\tswitch kind {\n\t\tcase \"bool\":\n\t\t\tparam = &BoolParam{}\n\t\tcase \"int\":\n\t\t\tmin := minInt\n\t\t\tmax := maxInt\n\t\t\tif kindParams != \"\" {\n\t\t\t\tminAndMax := strings.Split(kindParams, \",\")\n\t\t\t\tmin, err = strconv.Atoi(minAndMax[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Failed to parse min value for %v\", field.Name)\n\t\t\t\t}\n\t\t\t\tmax, err = strconv.Atoi(minAndMax[1])\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Failed to parse max value for %v\", field.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t\tparam = &IntParam{Min: min, Max: max}\n\t\tcase \"int32\":\n\t\t\tparam = &Int32Param{}\n\t\tcase \"mark-bitmask\":\n\t\t\tparam = &MarkBitmaskParam{}\n\t\tcase \"float\":\n\t\t\tparam = &FloatParam{}\n\t\tcase \"seconds\":\n\t\t\tparam = &SecondsParam{}\n\t\tcase \"millis\":\n\t\t\tparam = &MillisParam{}\n\t\tcase \"iface-list\":\n\t\t\tparam = &RegexpParam{Regexp: IfaceListRegexp,\n\t\t\t\tMsg: \"invalid Linux interface name\"}\n\t\tcase \"iface-list-regexp\":\n\t\t\tparam = &RegexpPatternListParam{\n\t\t\t\tNonRegexpElemRegexp: NonRegexpIfaceElemRegexp,\n\t\t\t\tRegexpElemRegexp:    RegexpIfaceElemRegexp,\n\t\t\t\tDelimiter:           \",\",\n\t\t\t\tMsg:                 \"list contains invalid Linux interface name or regex pattern\",\n\t\t\t}\n\t\tcase \"regexp\":\n\t\t\tparam = &RegexpPatternParam{}\n\t\tcase \"iface-param\":\n\t\t\tparam = &RegexpParam{Regexp: IfaceParamRegexp,\n\t\t\t\tMsg: \"invalid Linux interface parameter\"}\n\t\tcase \"file\":\n\t\t\tparam = &FileParam{\n\t\t\t\tMustExist:  strings.Contains(kindParams, \"must-exist\"),\n\t\t\t\tExecutable: strings.Contains(kindParams, \"executable\"),\n\t\t\t}\n\t\tcase \"authority\":\n\t\t\tparam = &RegexpParam{Regexp: AuthorityRegexp,\n\t\t\t\tMsg: \"invalid URL authority\"}\n\t\tcase \"ipv4\":\n\t\t\tparam = &Ipv4Param{}\n\t\tcase \"endpoint-list\":\n\t\t\tparam = &EndpointListParam{}\n\t\tcase \"port-list\":\n\t\t\tparam = &PortListParam{}\n\t\tcase \"portrange\":\n\t\t\tparam = &PortRangeParam{}\n\t\tcase \"portrange-list\":\n\t\t\tparam = &PortRangeListParam{}\n\t\tcase \"hostname\":\n\t\t\tparam = &RegexpParam{Regexp: HostnameRegexp,\n\t\t\t\tMsg: \"invalid hostname\"}\n\t\tcase \"host-address\":\n\t\t\tparam = &RegexpParam{Regexp: HostAddressRegexp,\n\t\t\t\tMsg: \"invalid host address\"}\n\t\tcase \"region\":\n\t\t\tparam = &RegionParam{}\n\t\tcase \"oneof\":\n\t\t\toptions := strings.Split(kindParams, \",\")\n\t\t\tlowerCaseToCanon := make(map[string]string)\n\t\t\tfor _, option := range options {\n\t\t\t\tlowerCaseToCanon[strings.ToLower(option)] = option\n\t\t\t}\n\t\t\tparam = &OneofListParam{\n\t\t\t\tlowerCaseOptionsToCanonical: lowerCaseToCanon}\n\t\tcase \"string\":\n\t\t\tparam = &RegexpParam{Regexp: StringRegexp,\n\t\t\t\tMsg: \"invalid string\"}\n\t\tcase \"cidr-list\":\n\t\t\tparam = &CIDRListParam{}\n\t\tcase \"route-table-range\":\n\t\t\tparam = &RouteTableRangeParam{}\n\t\tdefault:\n\t\t\tlog.Panicf(\"Unknown type of parameter: %v\", kind)\n\t\t}\n\n\t\tmetadata := param.GetMetadata()\n\t\tmetadata.Name = field.Name\n\t\tmetadata.ZeroValue = reflect.ValueOf(config).FieldByName(field.Name).Interface()\n\t\tif strings.Contains(flags, \"non-zero\") {\n\t\t\tmetadata.NonZero = true\n\t\t}\n\t\tif strings.Contains(flags, \"die-on-fail\") {\n\t\t\tmetadata.DieOnParseFailure = true\n\t\t}\n\t\tif strings.Contains(flags, \"local\") {\n\t\t\tmetadata.Local = true\n\t\t}\n\n\t\tif defaultStr != \"\" {\n\t\t\tif strings.Contains(flags, \"skip-default-validation\") {\n\t\t\t\tmetadata.Default = defaultStr\n\t\t\t} else {\n\t\t\t\t// Parse the default value and save it in the metadata. Doing\n\t\t\t\t// that here ensures that we syntax-check the defaults now.\n\t\t\t\tdefaultVal, err := param.Parse(defaultStr)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Invalid default value: %v\", err)\n\t\t\t\t}\n\t\t\t\tmetadata.Default = defaultVal\n\t\t\t}\n\t\t} else {\n\t\t\tmetadata.Default = metadata.ZeroValue\n\t\t}\n\t\tknownParams[strings.ToLower(field.Name)] = param\n\t}\n}\n\nfunc (config *Config) SetUseNodeResourceUpdates(b bool) {\n\tconfig.useNodeResourceUpdates = b\n}\n\nfunc (config *Config) UseNodeResourceUpdates() bool {\n\treturn config.useNodeResourceUpdates\n}\n\nfunc (config *Config) RawValues() map[string]string {\n\treturn config.rawValues\n}\n\nfunc (config *Config) SetLoadClientConfigFromEnvironmentFunction(fnc func() (*apiconfig.CalicoAPIConfig, error)) {\n\tconfig.loadClientConfigFromEnvironment = fnc\n}\n\n// OverrideParam installs a maximum priority parameter override for the given parameter.  This is useful for\n// disabling features that are found to be unsupported, for example. By using an extra priority class, the\n// override will persist even if the host/global config is updated.\nfunc (config *Config) OverrideParam(name, value string) (bool, error) {\n\tconfig.internalOverrides[name] = value\n\treturn config.UpdateFrom(config.internalOverrides, InternalOverride)\n}\n\nfunc New() *Config {\n\tif knownParams == nil {\n\t\tloadParams()\n\t}\n\tp := &Config{\n\t\trawValues:         map[string]string{},\n\t\tsourceToRawConfig: map[Source]map[string]string{},\n\t\tinternalOverrides: map[string]string{},\n\t}\n\tfor _, param := range knownParams {\n\t\tparam.setDefault(p)\n\t}\n\thostname, err := names.Hostname()\n\tif err != nil {\n\t\tlog.Warningf(\"Failed to get hostname from kernel, \"+\n\t\t\t\"trying HOSTNAME variable: %v\", err)\n\t\thostname = strings.ToLower(os.Getenv(\"HOSTNAME\"))\n\t}\n\tp.FelixHostname = hostname\n\tp.loadClientConfigFromEnvironment = apiconfig.LoadClientConfigFromEnvironment\n\n\treturn p\n}\n\ntype param interface {\n\tGetMetadata() *Metadata\n\tParse(raw string) (result interface{}, err error)\n\tsetDefault(*Config)\n}\n", "idx": 1, "id": 17747, "msg": "Suggest making this just `FeatureDetectOverride`; can see it being useful with the BPF dataplane too. It's fairly easy to add a validator, just change `config:\"string;;local\"` to `config:\"feat-override;;local\"`, for example, and then add an entry to the `loadParams()` function below. To validate, you could call the parse function and return an error if the parse fails.", "proj": "projectcalico-felix", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -92,13 +92,6 @@ func OpenKey(s *Server, name string, password string) (*Key, error) {\n \t\treturn nil, errors.New(\"Invalid key for repository\")\n \t}\n \n-\t// test if the chunker polynomial is present in the master key\n-\tif k.master.ChunkerPolynomial == 0 {\n-\t\treturn nil, errors.New(\"Polynomial for content defined chunking is zero\")\n-\t}\n-\n-\tdebug.Log(\"OpenKey\", \"Master keys loaded, polynomial %v\", k.master.ChunkerPolynomial)\n-\n \treturn k, nil\n }\n ", "y": 0, "oldf": "package server\n\nimport (\n\t\"crypto/rand\"\n\t\"crypto/sha256\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"os/user\"\n\t\"time\"\n\n\t\"github.com/restic/restic/backend\"\n\t\"github.com/restic/restic/chunker\"\n\t\"github.com/restic/restic/crypto\"\n\t\"github.com/restic/restic/debug\"\n)\n\nvar (\n\t// ErrNoKeyFound is returned when no key for the repository could be decrypted.\n\tErrNoKeyFound = errors.New(\"no key could be found\")\n)\n\n// TODO: figure out scrypt values on the fly depending on the current\n// hardware.\nconst (\n\tscryptN        = 65536\n\tscryptR        = 8\n\tscryptP        = 1\n\tscryptSaltsize = 64\n)\n\n// Key represents an encrypted master key for a repository.\ntype Key struct {\n\tCreated  time.Time `json:\"created\"`\n\tUsername string    `json:\"username\"`\n\tHostname string    `json:\"hostname\"`\n\n\tKDF  string `json:\"kdf\"`\n\tN    int    `json:\"N\"`\n\tR    int    `json:\"r\"`\n\tP    int    `json:\"p\"`\n\tSalt []byte `json:\"salt\"`\n\tData []byte `json:\"data\"`\n\n\tuser   *crypto.Key\n\tmaster *crypto.Key\n\n\tname string\n}\n\n// CreateKey initializes a master key in the given backend and encrypts it with\n// the password.\nfunc CreateKey(s *Server, password string) (*Key, error) {\n\treturn AddKey(s, password, nil)\n}\n\n// OpenKey tries do decrypt the key specified by name with the given password.\nfunc OpenKey(s *Server, name string, password string) (*Key, error) {\n\tk, err := LoadKey(s, name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// check KDF\n\tif k.KDF != \"scrypt\" {\n\t\treturn nil, errors.New(\"only supported KDF is scrypt()\")\n\t}\n\n\t// derive user key\n\tk.user, err = crypto.KDF(k.N, k.R, k.P, k.Salt, password)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// decrypt master keys\n\tbuf, err := crypto.Decrypt(k.user, []byte{}, k.Data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// restore json\n\tk.master = &crypto.Key{}\n\terr = json.Unmarshal(buf, k.master)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tk.name = name\n\n\tif !k.Valid() {\n\t\treturn nil, errors.New(\"Invalid key for repository\")\n\t}\n\n\t// test if the chunker polynomial is present in the master key\n\tif k.master.ChunkerPolynomial == 0 {\n\t\treturn nil, errors.New(\"Polynomial for content defined chunking is zero\")\n\t}\n\n\tdebug.Log(\"OpenKey\", \"Master keys loaded, polynomial %v\", k.master.ChunkerPolynomial)\n\n\treturn k, nil\n}\n\n// SearchKey tries to decrypt all keys in the backend with the given password.\n// If none could be found, ErrNoKeyFound is returned.\nfunc SearchKey(s *Server, password string) (*Key, error) {\n\t// try all keys in repo\n\tdone := make(chan struct{})\n\tdefer close(done)\n\tfor name := range s.List(backend.Key, done) {\n\t\tkey, err := OpenKey(s, name, password)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\treturn key, nil\n\t}\n\n\treturn nil, ErrNoKeyFound\n}\n\n// LoadKey loads a key from the backend.\nfunc LoadKey(s *Server, name string) (*Key, error) {\n\t// extract data from repo\n\trd, err := s.be.Get(backend.Key, name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rd.Close()\n\n\t// restore json\n\tdec := json.NewDecoder(rd)\n\tk := Key{}\n\terr = dec.Decode(&k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &k, nil\n}\n\n// AddKey adds a new key to an already existing repository.\nfunc AddKey(s *Server, password string, template *Key) (*Key, error) {\n\t// fill meta data about key\n\tnewkey := &Key{\n\t\tCreated: time.Now(),\n\t\tKDF:     \"scrypt\",\n\t\tN:       scryptN,\n\t\tR:       scryptR,\n\t\tP:       scryptP,\n\t}\n\n\thn, err := os.Hostname()\n\tif err == nil {\n\t\tnewkey.Hostname = hn\n\t}\n\n\tusr, err := user.Current()\n\tif err == nil {\n\t\tnewkey.Username = usr.Username\n\t}\n\n\t// generate random salt\n\tnewkey.Salt = make([]byte, scryptSaltsize)\n\tn, err := rand.Read(newkey.Salt)\n\tif n != scryptSaltsize || err != nil {\n\t\tpanic(\"unable to read enough random bytes for salt\")\n\t}\n\n\t// call KDF to derive user key\n\tnewkey.user, err = crypto.KDF(newkey.N, newkey.R, newkey.P, newkey.Salt, password)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif template == nil {\n\t\t// generate new random master keys\n\t\tnewkey.master = crypto.NewRandomKey()\n\t\t// generate random polynomial for cdc\n\t\tp, err := chunker.RandomPolynomial()\n\t\tif err != nil {\n\t\t\tdebug.Log(\"AddKey\", \"error generating new polynomial for cdc: %v\", err)\n\t\t\treturn nil, err\n\t\t}\n\t\tdebug.Log(\"AddKey\", \"generated new polynomial for cdc: %v\", p)\n\t\tnewkey.master.ChunkerPolynomial = p\n\t} else {\n\t\t// copy master keys from old key\n\t\tnewkey.master = template.master\n\t}\n\n\t// encrypt master keys (as json) with user key\n\tbuf, err := json.Marshal(newkey.master)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnewkey.Data, err = crypto.Encrypt(newkey.user, nil, buf)\n\n\t// dump as json\n\tbuf, err = json.Marshal(newkey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// store in repository and return\n\tblob, err := s.be.Create()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tplainhw := backend.NewHashingWriter(blob, sha256.New())\n\n\t_, err = plainhw.Write(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tname := backend.ID(plainhw.Sum(nil)).String()\n\n\terr = blob.Finalize(backend.Key, name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnewkey.name = name\n\n\treturn newkey, nil\n}\n\n// Encrypt encrypts and authenticates data with the master key. Stored in\n// ciphertext is IV || Ciphertext || MAC. Returns the ciphertext, which is\n// extended if necessary.\nfunc (k *Key) Encrypt(ciphertext, plaintext []byte) ([]byte, error) {\n\treturn crypto.Encrypt(k.master, ciphertext, plaintext)\n}\n\n// EncryptTo encrypts and authenticates data with the master key. The returned\n// io.Writer writes IV || Ciphertext || MAC.\nfunc (k *Key) EncryptTo(wr io.Writer) io.WriteCloser {\n\treturn crypto.EncryptTo(k.master, wr)\n}\n\n// Decrypt verifes and decrypts the ciphertext with the master key. Ciphertext\n// must be in the form IV || Ciphertext || MAC.\nfunc (k *Key) Decrypt(plaintext, ciphertext []byte) ([]byte, error) {\n\treturn crypto.Decrypt(k.master, plaintext, ciphertext)\n}\n\n// DecryptFrom verifies and decrypts the ciphertext read from rd and makes it\n// available on the returned Reader. Ciphertext must be in the form IV ||\n// Ciphertext || MAC. In order to correctly verify the ciphertext, rd is\n// drained, locally buffered and made available on the returned Reader\n// afterwards. If a MAC verification failure is observed, it is returned\n// immediately.\nfunc (k *Key) DecryptFrom(rd io.Reader) (io.ReadCloser, error) {\n\treturn crypto.DecryptFrom(k.master, rd)\n}\n\n// Master returns the master keys for this repository. Only included for\n// debug purposes.\nfunc (k *Key) Master() *crypto.Key {\n\treturn k.master\n}\n\n// User returns the user keys for this key. Only included for debug purposes.\nfunc (k *Key) User() *crypto.Key {\n\treturn k.user\n}\n\nfunc (k *Key) String() string {\n\tif k == nil {\n\t\treturn \"<Key nil>\"\n\t}\n\treturn fmt.Sprintf(\"<Key of %s@%s, created on %s>\", k.Username, k.Hostname, k.Created)\n}\n\nfunc (k Key) Name() string {\n\treturn k.name\n}\n\n// Valid tests whether the mac and encryption keys are valid (i.e. not zero)\nfunc (k *Key) Valid() bool {\n\treturn k.user.Valid() && k.master.Valid()\n}\n", "idx": 3, "id": 6468, "msg": "", "proj": "restic-restic", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -22,15 +22,20 @@\n \n \"\"\"Unit tests for utils module.\"\"\"\n \n+import numpy\n import pickle\n import tempfile\n import unittest\n \n from nupic.utils import MovingAverage\n+from nupic.utils import lru_cache as cache\n \n-# Import capnp to force import hook\n-import capnp\n-from nupic.movingaverage_capnp import MovingAverageProto\n+try:\n+  import capnp\n+except ImportError:\n+  capnp = None\n+if capnp:\n+  from nupic.movingaverage_capnp import MovingAverageProto\n \n \n ", "y": 0, "oldf": "#!/usr/bin/env python\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013-2014, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\n\"\"\"Unit tests for utils module.\"\"\"\n\nimport pickle\nimport tempfile\nimport unittest\n\nfrom nupic.utils import MovingAverage\n\n# Import capnp to force import hook\nimport capnp\nfrom nupic.movingaverage_capnp import MovingAverageProto\n\n\n\nclass UtilsTest(unittest.TestCase):\n  \"\"\"testing common.utils\"\"\"\n\n\n  def testMovingAverage(self):\n    \"\"\"\n    Test that the (internal) moving average maintains the averages correctly,\n    even for null initial condition and when the number of values goes over\n    windowSize.  Pass in integers and floats.\n    \"\"\"\n    historicalValues = []\n    total = 0\n    windowSize = 3\n    newAverage, historicalValues, total = (\n      MovingAverage.compute(historicalValues, total, 3, windowSize)\n    )\n\n    self.assertEqual(newAverage, 3.0)\n    self.assertEqual(historicalValues, [3.0])\n    self.assertEqual(total, 3.0)\n\n    newAverage, historicalValues, total = (\n      MovingAverage.compute(historicalValues, total, 4, windowSize)\n    )\n    self.assertEqual(newAverage, 3.5)\n    self.assertListEqual(historicalValues, [3.0, 4.0])\n    self.assertEqual(total, 7.0)\n\n    newAverage, historicalValues, total = (\n      MovingAverage.compute(historicalValues, total, 5.0, windowSize)\n    )\n    self.assertEqual(newAverage, 4.0)\n    self.assertListEqual(historicalValues, [3.0, 4.0, 5.0])\n    self.assertEqual(total, 12.0)\n\n    # Ensure the first value gets popped\n    newAverage, historicalValues, total = (\n      MovingAverage.compute(historicalValues, total, 6.0, windowSize)\n    )\n    self.assertEqual(newAverage, 5.0)\n    self.assertListEqual(historicalValues, [4.0, 5.0, 6.0])\n    self.assertEqual(total, 15.0)\n\n\n  def testMovingAverageInstance(self):\n    \"\"\"\n    Test that the (internal) moving average maintains the averages correctly,\n    even for null initial condition and when the number of values goes over\n    windowSize.  Pass in integers and floats.\n    this is for the instantce method next()\n    \"\"\"\n    ma = MovingAverage(windowSize=3)\n\n    newAverage = ma.next(3)\n    self.assertEqual(newAverage, 3.0)\n    self.assertListEqual(ma.getSlidingWindow(), [3.0])\n    self.assertEqual(ma.total, 3.0)\n\n    newAverage = ma.next(4)\n    self.assertEqual(newAverage, 3.5)\n    self.assertListEqual(ma.getSlidingWindow(), [3.0, 4.0])\n    self.assertEqual(ma.total, 7.0)\n\n    newAverage = ma.next(5)\n    self.assertEqual(newAverage, 4.0)\n    self.assertListEqual(ma.getSlidingWindow(), [3.0, 4.0, 5.0])\n    self.assertEqual(ma.total, 12.0)\n\n    # Ensure the first value gets popped\n    newAverage = ma.next(6)\n    self.assertEqual(newAverage, 5.0)\n    self.assertListEqual(ma.getSlidingWindow(), [4.0, 5.0, 6.0])\n    self.assertEqual(ma.total, 15.0)\n\n\n  def testMovingAverageSlidingWindowInit(self):\n    \"\"\"\n    Test the slidingWindow value is correctly assigned when initializing a\n    new MovingAverage object.\n    \"\"\"\n    # With exisiting historical values; same values as tested in testMovingAverage()\n    ma = MovingAverage(windowSize=3, existingHistoricalValues=[3.0, 4.0, 5.0])\n    self.assertListEqual(ma.getSlidingWindow(), [3.0, 4.0, 5.0])\n\n    # Withoout exisiting historical values\n    ma = MovingAverage(windowSize=3)\n    self.assertListEqual(ma.getSlidingWindow(), [])\n\n\n  def testMovingAverageReadWrite(self):\n    ma = MovingAverage(windowSize=3)\n\n    ma.next(3)\n    ma.next(4)\n    ma.next(5)\n\n    proto1 = MovingAverageProto.new_message()\n    ma.write(proto1)\n\n    # Write the proto to a temp file and read it back into a new proto\n    with tempfile.TemporaryFile() as f:\n      proto1.write(f)\n      f.seek(0)\n      proto2 = MovingAverageProto.read(f)\n\n    resurrectedMa = MovingAverage.read(proto2)\n\n    newAverage = ma.next(6)\n    self.assertEqual(newAverage, resurrectedMa.next(6))\n    self.assertListEqual(ma.getSlidingWindow(),\n                         resurrectedMa.getSlidingWindow())\n    self.assertEqual(ma.total, resurrectedMa.total)\n    self.assertTrue(ma, resurrectedMa) #using the __eq__ method\n\n\n  def testSerialization(self):\n    \"\"\"serialization using pickle\"\"\"\n    ma = MovingAverage(windowSize=3)\n\n    ma.next(3)\n    ma.next(4)\n    ma.next(5)\n\n    stored = pickle.dumps(ma)\n    restored = pickle.loads(stored)\n    self.assertEqual(restored, ma) \n    self.assertEqual(ma.next(6), restored.next(6))\n\n\n  def testEquals(self):\n    ma = MovingAverage(windowSize=3)\n    maP = MovingAverage(windowSize=3)\n    self.assertEqual(ma, maP)\n    \n    maN = MovingAverage(windowSize=10)\n    self.assertNotEqual(ma, maN)\n\n    ma = MovingAverage(windowSize=2, existingHistoricalValues=[3.0, 4.0, 5.0])\n    maP = MovingAverage(windowSize=2, existingHistoricalValues=[3.0, 4.0, 5.0])\n    self.assertEqual(ma, maP)\n    maP.next(6)\n    self.assertNotEqual(ma, maP)\n    ma.next(6)\n    self.assertEqual(ma, maP)\n    \n\n\n\nif __name__ == \"__main__\":\n  unittest.main()\n", "idx": 1, "id": 19444, "msg": "", "proj": "numenta-nupic", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,35 @@\n+\ufeff// Copyright (c) .NET Foundation. All rights reserved.\n+// Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n+\n+using System;\n+using System.Linq;\n+using System.Net.NetworkInformation;\n+using System.Net.Sockets;\n+using Microsoft.AspNetCore.Testing.xunit;\n+\n+namespace Microsoft.AspNetCore.Server.Kestrel.FunctionalTests\n+{\n+    public class IPv6ScopeIdPresentConditionAttribute : Attribute, ITestCondition\n+    {\n+        private static readonly Lazy<bool> _ipv6ScopeIdPresent = new Lazy<bool>(IPv6ScopeIdAddressPresent);\n+\n+        public bool IsMet => _ipv6ScopeIdPresent.Value;\n+\n+        public string SkipReason => \"No IPv6 addresses with scope IDs were found on the host.\";\n+\n+        private static bool IPv6ScopeIdAddressPresent()\n+        {\n+            try\n+            {\n+                return NetworkInterface.GetAllNetworkInterfaces()\n+                    .Where(iface => iface.OperationalStatus == OperationalStatus.Up)\n+                    .SelectMany(iface => iface.GetIPProperties().UnicastAddresses)\n+                    .Any(addrInfo => addrInfo.Address.AddressFamily == AddressFamily.InterNetworkV6 && addrInfo.Address.ScopeId != 0);\n+            }\n+            catch (SocketException)\n+            {\n+                return false;\n+            }\n+        }\n+    }\n+}", "y": 1, "oldf": "", "idx": 1, "id": 11511, "msg": "Shorten to `public bool IsMet => _ipv6Supported.Value`", "proj": "aspnet-KestrelHttpServer", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -40,16 +40,17 @@ def get_log_env(logfile, context, original_env):\n \n # -----------------------------------------------------------------------------\n def get_check_env(path_env_extra, ld_lib_path_extra):\n-    '''\n+    \"\"\"\n     Extending the checker environment.\n     Check environment is extended to find tools if they ar not on\n-    the default places\n-    '''\n+    the default places.\n+    \"\"\"\n     new_env = os.environ.copy()\n \n     if len(path_env_extra) > 0:\n         extra_path = ':'.join(path_env_extra)\n-        LOG.debug_analyzer('Extending PATH environment variable with: ' + extra_path)\n+        LOG.debug_analyzer(\n+            'Extending PATH environment variable with: ' + extra_path)\n \n         try:\n             new_env['PATH'] = extra_path + ':' + new_env['PATH']", "y": 0, "oldf": "# -------------------------------------------------------------------------\n#                     The CodeChecker Infrastructure\n#   This file is distributed under the University of Illinois Open Source\n#   License. See LICENSE.TXT for details.\n# -------------------------------------------------------------------------\n''''''\n\nimport os\n\nfrom codechecker_lib import logger\n\nLOG = logger.get_new_logger('ENV')\n\n\n# ------------------------------------------------------------------------------\ndef get_log_env(logfile, context, original_env):\n    '''\n    Environment for logging. With the ld logger.\n    Keep the original environment unmodified as possible\n    Only environment variables required for logging are changed\n    '''\n    new_env = original_env\n\n    new_env[context.env_var_cc_logger_bin] = context.path_logger_bin\n\n    new_env['LD_PRELOAD'] = context.logger_lib_name\n\n    try:\n        original_ld_library_path = new_env['LD_LIBRARY_PATH']\n        new_env['LD_LIBRARY_PATH'] = context.path_logger_lib + \\\n                                     ':' + original_ld_library_path\n    except:\n        new_env['LD_LIBRARY_PATH'] = context.path_logger_lib\n\n    # set ld logger logfile\n    new_env[context.env_var_cc_logger_file] = logfile\n\n    return new_env\n\n\n# -----------------------------------------------------------------------------\ndef get_check_env(path_env_extra, ld_lib_path_extra):\n    '''\n    Extending the checker environment.\n    Check environment is extended to find tools if they ar not on\n    the default places\n    '''\n    new_env = os.environ.copy()\n\n    if len(path_env_extra) > 0:\n        extra_path = ':'.join(path_env_extra)\n        LOG.debug_analyzer('Extending PATH environment variable with: ' + extra_path)\n\n        try:\n            new_env['PATH'] = extra_path + ':' + new_env['PATH']\n        except:\n            new_env['PATH'] = extra_path\n\n    if len(ld_lib_path_extra) > 0:\n        extra_lib = ':'.join(ld_lib_path_extra)\n        LOG.debug_analyzer('Extending LD_LIBRARY_PATH environment variable with: ' + extra_lib)\n        try:\n            original_ld_library_path = new_env['LD_LIBRARY_PATH']\n            new_env['LD_LIBRARY_PATH'] = extra_lib + ':' + original_ld_library_path\n        except:\n            new_env['LD_LIBRARY_PATH'] = extra_lib\n\n    return new_env\n", "idx": 4, "id": 6203, "msg": "", "proj": "Ericsson-codechecker", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -94,7 +94,7 @@ func main() {\n \tsf := svr.ChainService(cfg.Chain.ID).StateFactory()\n \tdao := svr.ChainService(cfg.Chain.ID).BlockDAO()\n \tif err := bc.Start(context.Background()); err == nil {\n-\t\tlog.L().Info(\"State DB status is normal.\")\n+\t\tlog.L().Debug(\"State DB status is normal.\")\n \t}\n \tdefer func() {\n \t\tif err := bc.Stop(context.Background()); err != nil {", "y": 1, "oldf": "// Copyright (c) 2019 IoTeX Foundation\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\n// This is a recovery tool that recovers a corrupted or missing state database.\n// To use, run \"make recover\"\npackage main\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\tglog \"log\"\n\t\"os\"\n\t\"strings\"\n\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/zap\"\n\n\t\"github.com/iotexproject/iotex-core/blockchain/blockdao\"\n\t\"github.com/iotexproject/iotex-core/blockchain/genesis\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/pkg/util/fileutil\"\n\t\"github.com/iotexproject/iotex-core/server/itx\"\n\t\"github.com/iotexproject/iotex-core/state/factory\"\n)\n\n// recoveryHeight is the blockchain height being recovered to\nvar recoveryHeight int\n\n/**\n * overwritePath is the path to the config file which overwrite default values\n * secretPath is the path to the  config file store secret values\n */\nvar (\n\tgenesisPath    string\n\t_overwritePath string\n\t_secretPath    string\n\t_plugins       strs\n)\n\ntype strs []string\n\nfunc (ss *strs) String() string {\n\treturn strings.Join(*ss, \",\")\n}\n\nfunc (ss *strs) Set(str string) error {\n\t*ss = append(*ss, str)\n\treturn nil\n}\n\nfunc init() {\n\tflag.StringVar(&genesisPath, \"genesis-path\", \"\", \"Genesis path\")\n\tflag.StringVar(&_overwritePath, \"config-path\", \"\", \"Config path\")\n\tflag.StringVar(&_secretPath, \"secret-path\", \"\", \"Secret path\")\n\tflag.Var(&_plugins, \"plugin\", \"Plugin of the node\")\n\tflag.IntVar(&recoveryHeight, \"recovery-height\", 0, \"Recovery height\")\n\tflag.Usage = func() {\n\t\t_, _ = fmt.Fprintf(os.Stderr,\n\t\t\t\"usage: recover -config-path=[string]\\n -recovery-height=[int]\\n\")\n\t\tflag.PrintDefaults()\n\t\tos.Exit(2)\n\t}\n\tflag.Parse()\n}\n\nfunc main() {\n\tgenesisCfg, err := genesis.New(genesisPath)\n\tif err != nil {\n\t\tglog.Fatalln(\"Failed to new genesis config.\", zap.Error(err))\n\t}\n\n\tcfg, err := config.New([]string{_overwritePath, _secretPath}, _plugins)\n\tif err != nil {\n\t\tglog.Fatalln(\"Failed to new config.\", zap.Error(err))\n\t}\n\n\tcfg.Genesis = genesisCfg\n\n\tlog.S().Infof(\"Config in use: %+v\", cfg)\n\n\t// create server\n\tsvr, err := itx.NewServer(cfg)\n\tif err != nil {\n\t\tlog.L().Fatal(\"Failed to create server.\", zap.Error(err))\n\t}\n\n\t// recover chain and state\n\tbc := svr.ChainService(cfg.Chain.ID).Blockchain()\n\tsf := svr.ChainService(cfg.Chain.ID).StateFactory()\n\tdao := svr.ChainService(cfg.Chain.ID).BlockDAO()\n\tif err := bc.Start(context.Background()); err == nil {\n\t\tlog.L().Info(\"State DB status is normal.\")\n\t}\n\tdefer func() {\n\t\tif err := bc.Stop(context.Background()); err != nil {\n\t\t\tlog.L().Fatal(\"Failed to stop blockchain\")\n\t\t}\n\t}()\n\tif err := recoverChainAndState(dao, sf, cfg, uint64(recoveryHeight)); err != nil {\n\t\tlog.L().Fatal(\"Failed to recover chain and state.\", zap.Error(err))\n\t} else {\n\t\tlog.S().Infof(\"Success to recover chain and state to target height %d\", recoveryHeight)\n\t}\n}\n\n// recoverChainAndState recovers the chain to target height and refresh state db if necessary\nfunc recoverChainAndState(dao blockdao.BlockDAO, sf factory.Factory, cfg config.Config, targetHeight uint64) error {\n\t// recover the blockchain to target height(blockDAO)\n\tif err := dao.DeleteBlockToTarget(targetHeight); err != nil {\n\t\treturn errors.Wrapf(err, \"failed to recover blockchain to target height %d\", targetHeight)\n\t}\n\tstateHeight, err := sf.Height()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif targetHeight < stateHeight {\n\t\t// delete existing state DB (build from scratch)\n\t\tif fileutil.FileExists(cfg.Chain.TrieDBPath) && os.Remove(cfg.Chain.TrieDBPath) != nil {\n\t\t\treturn errors.New(\"failed to delete existing state DB\")\n\t\t}\n\t}\n\treturn nil\n}\n", "idx": 1, "id": 23839, "msg": "same for this tool", "proj": "iotexproject-iotex-core", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1129,12 +1129,8 @@ users:\n \t\t\tVPCEndpointID:      \"vpce-12345\",\n \t\t\tHostedZoneID:       \"HZ12345\",\n \t\t},\n-\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n-\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n-\t\t\tStatus:  corev1.ConditionTrue,\n-\t\t\tReason:  \"PrivateLinkAccessReady\",\n-\t\t\tMessage: \"private link access is ready for use\",\n-\t\t}},\n+\t\texpectedConditions: getExpectedConditions(false, \"PrivateLinkAccessReady\",\n+\t\t\t\"private link access is ready for use\"),\n \t}, {\n \t\tname: \"cd with privatelink enabled, no previous private link, associate vpcs fails\",\n ", "y": 0, "oldf": "package awsprivatelink\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/aws/aws-sdk-go/aws/awserr\"\n\t\"github.com/aws/aws-sdk-go/service/ec2\"\n\t\"github.com/aws/aws-sdk-go/service/elbv2\"\n\t\"github.com/aws/aws-sdk-go/service/route53\"\n\t\"github.com/aws/aws-sdk-go/service/sts\"\n\t\"github.com/davecgh/go-spew/spew\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/pkg/errors\"\n\tlog \"github.com/sirupsen/logrus\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/sets\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client/fake\"\n\t\"sigs.k8s.io/controller-runtime/pkg/reconcile\"\n\n\thivev1 \"github.com/openshift/hive/apis/hive/v1\"\n\thivev1aws \"github.com/openshift/hive/apis/hive/v1/aws\"\n\thivev1gcp \"github.com/openshift/hive/apis/hive/v1/gcp\"\n\t\"github.com/openshift/hive/pkg/awsclient\"\n\t\"github.com/openshift/hive/pkg/awsclient/mock\"\n\t\"github.com/openshift/hive/pkg/constants\"\n\ttestcd \"github.com/openshift/hive/pkg/test/clusterdeployment\"\n\t\"github.com/openshift/hive/pkg/test/generic\"\n)\n\nconst (\n\ttestNS = \"test-namespace\"\n)\n\nfunc Test_setErrCondition(t *testing.T) {\n\tscheme := runtime.NewScheme()\n\thivev1.AddToScheme(scheme)\n\tcases := []struct {\n\t\tname string\n\n\t\tconditions []hivev1.ClusterDeploymentCondition\n\t\terr        error\n\t\treason     string\n\n\t\texpectedConditions []hivev1.ClusterDeploymentCondition\n\t}{{\n\t\tname: \"no previous failure\",\n\n\t\terr:    errors.New(\"failed to do something important\"),\n\t\treason: \"FailureToDoSomethingImportant\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}},\n\t}, {\n\t\tname: \"previous failure\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPrevious\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}},\n\t\terr:    errors.New(\"failed to do something important\"),\n\t\treason: \"FailureToDoSomethingImportant\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}},\n\t}, {\n\t\tname: \"previous ready\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"all is looking good\",\n\t\t}},\n\t\terr:    errors.New(\"failed to do something important\"),\n\t\treason: \"FailureToDoSomethingImportant\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}},\n\t}, {\n\t\tname: \"previous failure, ready\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}},\n\t\terr:    errors.New(\"failed to do something important\"),\n\t\treason: \"FailureToDoSomethingImportant\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportant\",\n\t\t\tMessage: \"failed to do something important\",\n\t\t}},\n\t}}\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tcd := testcd.FullBuilder(testNS, \"test\", scheme).Build()\n\t\t\tcd.Status.Conditions = test.conditions\n\n\t\t\tfakeClient := fake.NewFakeClientWithScheme(scheme, cd)\n\t\t\treconciler := &ReconcileAWSPrivateLink{\n\t\t\t\tClient: fakeClient,\n\t\t\t}\n\t\t\tlogger := log.New()\n\t\t\tlogger.SetLevel(log.DebugLevel)\n\t\t\terr := reconciler.setErrCondition(cd, test.reason, test.err, logger)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tcd = &hivev1.ClusterDeployment{}\n\t\t\terr = fakeClient.Get(context.TODO(), client.ObjectKey{Namespace: testNS, Name: \"test\"}, cd)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// santize\n\t\t\tfor i := range cd.Status.Conditions {\n\t\t\t\tcd.Status.Conditions[i].LastProbeTime = metav1.Time{}\n\t\t\t\tcd.Status.Conditions[i].LastTransitionTime = metav1.Time{}\n\t\t\t}\n\n\t\t\tassert.Equal(t, test.expectedConditions, cd.Status.Conditions)\n\t\t})\n\t}\n}\n\nfunc Test_setProgressCondition(t *testing.T) {\n\tscheme := runtime.NewScheme()\n\thivev1.AddToScheme(scheme)\n\tcases := []struct {\n\t\tname string\n\n\t\tconditions []hivev1.ClusterDeploymentCondition\n\t\tcompleted  corev1.ConditionStatus\n\t\tmessage    string\n\t\treason     string\n\n\t\texpectedConditions []hivev1.ClusterDeploymentCondition\n\t}{{\n\t\tname: \"no previous progress, not completed\",\n\n\t\tcompleted: corev1.ConditionFalse,\n\t\tmessage:   \"progresing towards stage 1\",\n\t\treason:    \"InprogesStage1\",\n\t}, {\n\t\tname: \"previous progress, not completed\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"InprogesStage1\",\n\t\t\tMessage: \"progresing towards stage 1\",\n\t\t}},\n\t\tcompleted: corev1.ConditionFalse,\n\t\tmessage:   \"progresing towards stage 2\",\n\t\treason:    \"InprogesStage2\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"InprogesStage2\",\n\t\t\tMessage: \"progresing towards stage 2\",\n\t\t}},\n\t}, {\n\t\tname: \"previous failure, progress no completed\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}},\n\t\tcompleted: corev1.ConditionFalse,\n\t\tmessage:   \"progresing towards stage 1\",\n\t\treason:    \"InprogesStage1\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}},\n\t}, {\n\t\tname: \"previous failure, previous progress no completed\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"InprogesStage1\",\n\t\t\tMessage: \"progresing towards stage 1\",\n\t\t}},\n\t\tcompleted: corev1.ConditionFalse,\n\t\tmessage:   \"progresing towards stage 2\",\n\t\treason:    \"InprogesStage2\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"InprogesStage2\",\n\t\t\tMessage: \"progresing towards stage 2\",\n\t\t}},\n\t}, {\n\t\tname: \"previous failure, progress completed\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"FailureToDoSomethingImportantPreviously\",\n\t\t\tMessage: \"failed to do something important previously\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"InprogesStage1\",\n\t\t\tMessage: \"progresing towards stage 1\",\n\t\t}},\n\t\tcompleted: corev1.ConditionTrue,\n\t\tmessage:   \"All looking good\",\n\t\treason:    \"AllLookingGood\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionFalse,\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"All looking good\",\n\t\t}, {\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"All looking good\",\n\t\t}},\n\t}, {\n\t\tname: \"previous ready, now progressing\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"All looking good\",\n\t\t}},\n\t\tcompleted: corev1.ConditionFalse,\n\t\tmessage:   \"progresing towards stage 1\",\n\t\treason:    \"InprogesStage1\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"All looking good\",\n\t\t}},\n\t}, {\n\t\tname: \"previous ready, now ready with different reason\",\n\n\t\tconditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGood\",\n\t\t\tMessage: \"All looking good\",\n\t\t}},\n\t\tcompleted: corev1.ConditionTrue,\n\t\tmessage:   \"All looking good\",\n\t\treason:    \"AllLookingGoodVersion2\",\n\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tReason:  \"AllLookingGoodVersion2\",\n\t\t\tMessage: \"All looking good\",\n\t\t}},\n\t}}\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tcd := testcd.FullBuilder(testNS, \"test\", scheme).Build()\n\t\t\tcd.Status.Conditions = test.conditions\n\n\t\t\tfakeClient := fake.NewFakeClientWithScheme(scheme, cd)\n\t\t\treconciler := &ReconcileAWSPrivateLink{\n\t\t\t\tClient: fakeClient,\n\t\t\t}\n\t\t\tlogger := log.New()\n\t\t\tlogger.SetLevel(log.DebugLevel)\n\t\t\terr := reconciler.setProgressCondition(cd, test.completed, test.reason, test.message, logger)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tcd = &hivev1.ClusterDeployment{}\n\t\t\terr = fakeClient.Get(context.TODO(), client.ObjectKey{Namespace: testNS, Name: \"test\"}, cd)\n\t\t\trequire.NoError(t, err)\n\n\t\t\t// santize\n\t\t\tfor i := range cd.Status.Conditions {\n\t\t\t\tcd.Status.Conditions[i].LastProbeTime = metav1.Time{}\n\t\t\t\tcd.Status.Conditions[i].LastTransitionTime = metav1.Time{}\n\t\t\t}\n\n\t\t\tassert.Equal(t, test.expectedConditions, cd.Status.Conditions)\n\t\t})\n\t}\n}\n\nfunc TestInitialURL(t *testing.T) {\n\tscheme := runtime.NewScheme()\n\thivev1.AddToScheme(scheme)\n\tcorev1.AddToScheme(scheme)\n\n\ttests := []struct {\n\t\tname string\n\n\t\texisting map[string]string\n\n\t\twant string\n\t}{{\n\t\tname: \"use kubeconfig\",\n\n\t\texisting: map[string]string{\n\t\t\t\"kubeconfig\": `apiVersion: v1\nclusters:\n- cluster:\n    server: https://api.test-cluster:6443\n  name: test-cluster\ncontexts:\n- context:\n    cluster: test-cluster\n    user: admin\n  name: admin\ncurrent-context: admin\nkind: Config\nusers:\n- name: admin\n`,\n\t\t},\n\t\twant: \"api.test-cluster\",\n\t}, {\n\t\tname: \"use raw-kubeconfig\",\n\n\t\texisting: map[string]string{\n\t\t\t\"raw-kubeconfig\": `apiVersion: v1\nclusters:\n- cluster:\n    server: https://api.test-cluster:6443\n  name: test-cluster\ncontexts:\n- context:\n    cluster: test-cluster\n    user: admin\n  name: admin\ncurrent-context: admin\nkind: Config\nusers:\n- name: admin\n`,\n\t\t},\n\t\twant: \"api.test-cluster\",\n\t}, {\n\t\tname: \"use raw-kubeconfig when both present\",\n\n\t\texisting: map[string]string{\n\t\t\t\"raw-kubeconfig\": `apiVersion: v1\nclusters:\n- cluster:\n    server: https://api.test-cluster:6443\n  name: test-cluster\ncontexts:\n- context:\n    cluster: test-cluster\n    user: admin\n  name: admin\ncurrent-context: admin\nkind: Config\nusers:\n- name: admin\n`,\n\t\t\t\"kubeconfig\": `apiVersion: v1\nclusters:\n- cluster:\n    server: https://api.test-cluster:6443\n  name: test-cluster\n- cluster:\n    server: https://api.vanity-domain:6443\n  name: test-cluster-vanity\ncontexts:\n- context:\n    cluster: test-cluster-vanity\n    user: admin\n  name: admin\ncurrent-context: admin\nkind: Config\nusers:\n- name: admin\n`,\n\t\t},\n\t\twant: \"api.test-cluster\",\n\t}}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\ts := testSecret(\"test\", tt.existing)\n\t\t\tfakeClient := fake.NewFakeClientWithScheme(scheme, s)\n\n\t\t\tgot, err := initialURL(fakeClient, client.ObjectKey{Namespace: testNS, Name: \"test\"})\n\t\t\trequire.NoError(t, err)\n\t\t\tassert.Equal(t, tt.want, got)\n\t\t})\n\t}\n}\n\nfunc testSecret(name string, data map[string]string) *corev1.Secret {\n\ts := &corev1.Secret{\n\t\tTypeMeta: metav1.TypeMeta{\n\t\t\tAPIVersion: \"v1\",\n\t\t\tKind:       \"Secret\",\n\t\t},\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tNamespace: testNS,\n\t\t\tName:      name,\n\t\t},\n\t\tData: map[string][]byte{},\n\t}\n\tfor k, v := range data {\n\t\ts.Data[k] = []byte(v)\n\t}\n\treturn s\n}\n\nfunc TestReconcile(t *testing.T) {\n\tscheme := runtime.NewScheme()\n\thivev1.AddToScheme(scheme)\n\tcorev1.AddToScheme(scheme)\n\n\tkey := client.ObjectKey{Name: \"test-cd\", Namespace: testNS}\n\tcdBuilder := testcd.FullBuilder(testNS, \"test-cd\", scheme)\n\tenabledPrivateLinkBuilder := cdBuilder.\n\t\tOptions(testcd.WithAWSPlatform(&hivev1aws.Platform{Region: \"us-east-1\",\n\t\t\tPrivateLink: &hivev1aws.PrivateLinkAccess{Enabled: true}}))\n\tvalidInventory := []hivev1.AWSPrivateLinkInventory{{\n\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\tRegion: \"us-east-1\",\n\t\t\tVPCID:  \"vpc-1\",\n\t\t},\n\t\tSubnets: []hivev1.AWSPrivateLinkSubnet{{\n\t\t\tAvailabilityZone: \"us-east-1a\",\n\t\t\tSubnetID:         \"subnet-1\",\n\t\t}, {\n\t\t\tAvailabilityZone: \"us-east-1b\",\n\t\t\tSubnetID:         \"subnet-2\",\n\t\t}, {\n\t\t\tAvailabilityZone: \"us-east-1c\",\n\t\t\tSubnetID:         \"subnet-3\",\n\t\t}},\n\t}}\n\tkubeConfigSecret := map[string]string{\n\t\t\"kubeconfig\": `apiVersion: v1\nclusters:\n- cluster:\n    server: https://api.test-cluster:6443\n  name: test-cluster\ncontexts:\n- context:\n    cluster: test-cluster\n    user: admin\n  name: admin\ncurrent-context: admin\nkind: Config\nusers:\n- name: admin`,\n\t}\n\n\tmockDiscoverLB := func(m *mock.MockClient) string {\n\t\tclusternlb := &elbv2.LoadBalancer{\n\t\t\tLoadBalancerArn: aws.String(\"aws:elb:12345:nlb-arn\"),\n\t\t\tState: &elbv2.LoadBalancerState{\n\t\t\t\tCode: aws.String(elbv2.LoadBalancerStateEnumActive),\n\t\t\t},\n\t\t}\n\t\tm.EXPECT().DescribeLoadBalancers(gomock.Any()).\n\t\t\tReturn(&elbv2.DescribeLoadBalancersOutput{\n\t\t\t\tLoadBalancers: []*elbv2.LoadBalancer{clusternlb},\n\t\t\t}, nil).AnyTimes()\n\t\treturn *clusternlb.LoadBalancerArn\n\t}\n\n\tmockCreateService := func(m *mock.MockClient, clusternlb string) *ec2.ServiceConfiguration {\n\t\tservice := &ec2.ServiceConfiguration{\n\t\t\tAcceptanceRequired:      aws.Bool(false),\n\t\t\tServiceId:               aws.String(\"vpce-svc-12345\"),\n\t\t\tServiceName:             aws.String(\"vpce-svc-12345.vpc.amazon.com\"),\n\t\t\tServiceState:            aws.String(ec2.ServiceStateAvailable),\n\t\t\tNetworkLoadBalancerArns: aws.StringSlice([]string{clusternlb}),\n\t\t\tAvailabilityZones:       aws.StringSlice([]string{\"us-east-1b\", \"us-east-1c\"}),\n\t\t}\n\t\tm.EXPECT().DescribeVpcEndpointServiceConfigurations(gomock.Any()).\n\t\t\tReturn(&ec2.DescribeVpcEndpointServiceConfigurationsOutput{}, nil)\n\t\tm.EXPECT().CreateVpcEndpointServiceConfiguration(gomock.Any()).\n\t\t\tReturn(&ec2.CreateVpcEndpointServiceConfigurationOutput{\n\t\t\t\tServiceConfiguration: service,\n\t\t\t}, nil)\n\t\tm.EXPECT().DescribeVpcEndpointServiceConfigurations(gomock.Any()).\n\t\t\tReturn(&ec2.DescribeVpcEndpointServiceConfigurationsOutput{\n\t\t\t\tServiceConfigurations: []*ec2.ServiceConfiguration{service},\n\t\t\t}, nil)\n\t\treturn service\n\t}\n\tmockServicePerms := func(m *mock.MockClient, service *ec2.ServiceConfiguration) {\n\t\tm.EXPECT().GetCallerIdentity(gomock.Any()).Return(&sts.GetCallerIdentityOutput{Arn: aws.String(\"aws:iam:12345:hub-user\")}, nil)\n\t\tm.EXPECT().DescribeVpcEndpointServicePermissions(gomock.Any()).\n\t\t\tReturn(&ec2.DescribeVpcEndpointServicePermissionsOutput{}, nil)\n\t\tm.EXPECT().ModifyVpcEndpointServicePermissions(&ec2.ModifyVpcEndpointServicePermissionsInput{\n\t\t\tAddAllowedPrincipals: aws.StringSlice([]string{\"aws:iam:12345:hub-user\"}),\n\t\t\tServiceId:            service.ServiceId,\n\t\t}).Return(nil, nil)\n\t}\n\tmockExistingService := func(m *mock.MockClient, clusternlb string, modify func(*ec2.ServiceConfiguration)) *ec2.ServiceConfiguration {\n\t\tservice := &ec2.ServiceConfiguration{\n\t\t\tAcceptanceRequired:      aws.Bool(false),\n\t\t\tServiceId:               aws.String(\"vpce-svc-12345\"),\n\t\t\tServiceName:             aws.String(\"vpce-svc-12345.vpc.amazon.com\"),\n\t\t\tServiceState:            aws.String(ec2.ServiceStateAvailable),\n\t\t\tNetworkLoadBalancerArns: aws.StringSlice([]string{clusternlb}),\n\t\t}\n\t\tmodify(service)\n\t\tm.EXPECT().DescribeVpcEndpointServiceConfigurations(gomock.Any()).\n\t\t\tReturn(&ec2.DescribeVpcEndpointServiceConfigurationsOutput{\n\t\t\t\tServiceConfigurations: []*ec2.ServiceConfiguration{service},\n\t\t\t}, nil)\n\t\treturn service\n\t}\n\n\tmockCreateEndpoint := func(m *mock.MockClient, service *ec2.ServiceConfiguration) *ec2.VpcEndpoint {\n\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).\n\t\t\tReturn(&ec2.DescribeVpcEndpointsOutput{}, nil).Times(2)\n\t\tm.EXPECT().DescribeVpcEndpointServices(&ec2.DescribeVpcEndpointServicesInput{\n\t\t\tServiceNames: aws.StringSlice([]string{*service.ServiceName}),\n\t\t}).Return(&ec2.DescribeVpcEndpointServicesOutput{\n\t\t\tServiceDetails: []*ec2.ServiceDetail{{AvailabilityZones: service.AvailabilityZones}},\n\t\t}, nil)\n\n\t\tendpoint := &ec2.VpcEndpoint{\n\t\t\tVpcEndpointId: aws.String(\"vpce-12345\"),\n\t\t\tVpcId:         aws.String(\"vpc-1\"),\n\t\t\tState:         aws.String(\"available\"),\n\t\t\tDnsEntries: []*ec2.DnsEntry{{\n\t\t\t\tDnsName:      aws.String(\"vpce-12345-us-east-1.vpce-svc-12345.vpc.amazonaws.com\"),\n\t\t\t\tHostedZoneId: aws.String(\"HZ23456\"),\n\t\t\t}},\n\t\t}\n\t\tm.EXPECT().CreateVpcEndpoint(gomock.Any()).\n\t\t\tReturn(&ec2.CreateVpcEndpointOutput{VpcEndpoint: endpoint}, nil)\n\t\tm.EXPECT().DescribeVpcEndpoints(&ec2.DescribeVpcEndpointsInput{\n\t\t\tVpcEndpointIds: aws.StringSlice([]string{*endpoint.VpcEndpointId}),\n\t\t}).Return(&ec2.DescribeVpcEndpointsOutput{\n\t\t\tVpcEndpoints: []*ec2.VpcEndpoint{endpoint},\n\t\t}, nil)\n\t\treturn endpoint\n\t}\n\n\tmockPHZ := func(m *mock.MockClient, endpoint *ec2.VpcEndpoint, apiDomain string, existingSummary *route53.HostedZoneSummary) string {\n\t\tbyVPCOut := &route53.ListHostedZonesByVPCOutput{}\n\t\tif existingSummary != nil {\n\t\t\tbyVPCOut.HostedZoneSummaries = []*route53.HostedZoneSummary{existingSummary}\n\t\t}\n\t\tm.EXPECT().ListHostedZonesByVPC(&route53.ListHostedZonesByVPCInput{\n\t\t\tMaxItems:  aws.String(\"100\"),\n\t\t\tVPCId:     endpoint.VpcId,\n\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t}).Return(byVPCOut, nil)\n\t\tvar hzID string\n\t\tif existingSummary == nil {\n\t\t\thzID = \"HZ12345\"\n\t\t\tm.EXPECT().CreateHostedZone(newCreateHostedZoneInputMatcher(&route53.CreateHostedZoneInput{\n\t\t\t\tHostedZoneConfig: &route53.HostedZoneConfig{\n\t\t\t\t\tPrivateZone: aws.Bool(true),\n\t\t\t\t},\n\t\t\t\tName: aws.String(apiDomain),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t},\n\t\t\t})).Return(&route53.CreateHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t}, nil)\n\t\t} else {\n\t\t\thzID = aws.StringValue(existingSummary.HostedZoneId)\n\t\t}\n\n\t\tm.EXPECT().ChangeResourceRecordSets(&route53.ChangeResourceRecordSetsInput{\n\t\t\tChangeBatch: &route53.ChangeBatch{\n\t\t\t\tChanges: []*route53.Change{{\n\t\t\t\t\tAction: aws.String(\"UPSERT\"),\n\t\t\t\t\tResourceRecordSet: &route53.ResourceRecordSet{\n\t\t\t\t\t\tAliasTarget: &route53.AliasTarget{\n\t\t\t\t\t\t\tDNSName:              endpoint.DnsEntries[0].DnsName,\n\t\t\t\t\t\t\tEvaluateTargetHealth: aws.Bool(false),\n\t\t\t\t\t\t\tHostedZoneId:         endpoint.DnsEntries[0].HostedZoneId,\n\t\t\t\t\t\t},\n\t\t\t\t\t\tName: aws.String(apiDomain),\n\t\t\t\t\t\tType: aws.String(\"A\"),\n\t\t\t\t\t},\n\t\t\t\t}},\n\t\t\t},\n\t\t\tHostedZoneId: aws.String(hzID),\n\t\t})\n\t\treturn hzID\n\t}\n\n\tcases := []struct {\n\t\tname string\n\n\t\texisting           []runtime.Object\n\t\tinventory          []hivev1.AWSPrivateLinkInventory\n\t\tassociate          []hivev1.AWSAssociatedVPC\n\t\tconfigureAWSClient func(*mock.MockClient)\n\n\t\thasFinalizer        bool\n\t\texpectedAnnotations map[string]string\n\t\texpectedStatus      *hivev1aws.PrivateLinkAccessStatus\n\t\texpectedConditions  []hivev1.ClusterDeploymentCondition\n\t\terr                 string\n\t}{{\n\t\tname: \"cd with gcp platform\",\n\n\t\texisting: []runtime.Object{\n\t\t\tcdBuilder.Options(func(cd *hivev1.ClusterDeployment) {\n\t\t\t\tcd.Spec.Platform.GCP = &hivev1gcp.Platform{Region: \"gcp-region\"}\n\t\t\t}).Build(),\n\t\t},\n\t}, {\n\t\tname: \"cd without privatelink\",\n\n\t\texisting: []runtime.Object{\n\t\t\tcdBuilder.Build(testcd.WithAWSPlatform(&hivev1aws.Platform{Region: \"us-east-1\"})),\n\t\t},\n\t}, {\n\t\tname: \"cd with privatelink disabled\",\n\n\t\texisting: []runtime.Object{\n\t\t\tcdBuilder.Build(testcd.WithAWSPlatform(&hivev1aws.Platform{Region: \"us-east-1\",\n\t\t\t\tPrivateLink: &hivev1aws.PrivateLinkAccess{Enabled: false}})),\n\t\t},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no inventory\",\n\n\t\texisting: []runtime.Object{\n\t\t\tenabledPrivateLinkBuilder.Build(),\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"UnsupportedRegion\",\n\t\t\tMessage: \"cluster deployment region \\\"us-east-1\\\" is not supported as there is no inventory to create necessary resources\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no inventory in given region\",\n\n\t\texisting: []runtime.Object{\n\t\t\tenabledPrivateLinkBuilder.Build(),\n\t\t},\n\t\tinventory: []hivev1.AWSPrivateLinkInventory{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tRegion: \"us-west-1\",\n\t\t\t\tVPCID:  \"vpc-1\",\n\t\t\t},\n\t\t}},\n\n\t\thasFinalizer: true,\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"UnsupportedRegion\",\n\t\t\tMessage: \"cluster deployment region \\\"us-east-1\\\" is not supported as there is no inventory to create necessary resources\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no provision started\",\n\n\t\texisting: []runtime.Object{\n\t\t\tenabledPrivateLinkBuilder.Build(),\n\t\t},\n\t\tinventory: validInventory,\n\n\t\thasFinalizer: true,\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, but no cluster metadata\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\"),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\n\t\thasFinalizer: true,\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, but no admin kubeconfig\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\", provisionWithInfraID(\"test-cd-1234\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\n\t\thasFinalizer: true,\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb describe access denied\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tm.EXPECT().DescribeLoadBalancers(gomock.Any()).\n\t\t\t\tReturn(nil, awserr.New(\"AccessDenied\", \"not authorized to DescribeLoadBalancers\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"DiscoveringNLBFailed\",\n\t\t\tMessage: \"failed to describe load balancer for the cluster: AccessDenied: not authorized to DescribeLoadBalancers\",\n\t\t}},\n\t\terr: \"failed to describe load balancer for the cluster: AccessDenied: not authorized to DescribeLoadBalancers\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb not found\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tm.EXPECT().DescribeLoadBalancers(gomock.Any()).\n\t\t\t\tReturn(nil, awserr.New(\"LoadBalancerNotFound\", \"Loadbalance could not be found\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, endpoint access denied\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).Return(nil, awserr.New(\"AccessDenied\", \"not authorized to DescribeVpcEndpoints\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"VPCEndpointReconcileFailed\",\n\t\t\tMessage: \"AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, previous service exists, acceptance required set to true, endpoint access denied\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockExistingService(m, clusternlb, func(s *ec2.ServiceConfiguration) {\n\t\t\t\ts.AcceptanceRequired = aws.Bool(true)\n\t\t\t})\n\n\t\t\tm.EXPECT().ModifyVpcEndpointServiceConfiguration(&ec2.ModifyVpcEndpointServiceConfigurationInput{\n\t\t\t\tServiceId:          service.ServiceId,\n\t\t\t\tAcceptanceRequired: aws.Bool(false),\n\t\t\t}).Return(&ec2.ModifyVpcEndpointServiceConfigurationOutput{}, nil)\n\n\t\t\tmockServicePerms(m, service)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).Return(nil, awserr.New(\"AccessDenied\", \"not authorized to DescribeVpcEndpoints\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"VPCEndpointReconcileFailed\",\n\t\t\tMessage: \"AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, previous service exists, additional NLB added, endpoint access denied\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockExistingService(m, clusternlb, func(s *ec2.ServiceConfiguration) {\n\t\t\t\ts.NetworkLoadBalancerArns = aws.StringSlice([]string{clusternlb, \"aws:elb:12345:not-cluster-nlb-arn\"})\n\t\t\t})\n\n\t\t\tm.EXPECT().ModifyVpcEndpointServiceConfiguration(&ec2.ModifyVpcEndpointServiceConfigurationInput{\n\t\t\t\tServiceId:                     service.ServiceId,\n\t\t\t\tAcceptanceRequired:            aws.Bool(false),\n\t\t\t\tRemoveNetworkLoadBalancerArns: aws.StringSlice([]string{\"aws:elb:12345:not-cluster-nlb-arn\"}),\n\t\t\t}).Return(&ec2.ModifyVpcEndpointServiceConfigurationOutput{}, nil)\n\n\t\t\tmockServicePerms(m, service)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).Return(nil, awserr.New(\"AccessDenied\", \"not authorized to DescribeVpcEndpoints\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"VPCEndpointReconcileFailed\",\n\t\t\tMessage: \"AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, previous service exists, permissions change, endpoint access denied\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockExistingService(m, clusternlb, func(s *ec2.ServiceConfiguration) {})\n\n\t\t\tm.EXPECT().GetCallerIdentity(gomock.Any()).Return(&sts.GetCallerIdentityOutput{Arn: aws.String(\"aws:iam:12345:hub-user\")}, nil)\n\t\t\tm.EXPECT().DescribeVpcEndpointServicePermissions(gomock.Any()).\n\t\t\t\tReturn(&ec2.DescribeVpcEndpointServicePermissionsOutput{\n\t\t\t\t\tAllowedPrincipals: []*ec2.AllowedPrincipal{{\n\t\t\t\t\t\tPrincipal: aws.String(\"aws:iam:12345:some-that-should-not-be-allowed\"),\n\t\t\t\t\t}},\n\t\t\t\t}, nil)\n\t\t\tm.EXPECT().ModifyVpcEndpointServicePermissions(&ec2.ModifyVpcEndpointServicePermissionsInput{\n\t\t\t\tAddAllowedPrincipals:    aws.StringSlice([]string{\"aws:iam:12345:hub-user\"}),\n\t\t\t\tRemoveAllowedPrincipals: aws.StringSlice([]string{\"aws:iam:12345:some-that-should-not-be-allowed\"}),\n\t\t\t\tServiceId:               service.ServiceId,\n\t\t\t}).Return(nil, nil)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).Return(nil, awserr.New(\"AccessDenied\", \"not authorized to DescribeVpcEndpoints\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"VPCEndpointReconcileFailed\",\n\t\t\tMessage: \"AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: AccessDenied: not authorized to DescribeVpcEndpoints\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, no previous endpoint, no matching az\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: []hivev1.AWSPrivateLinkInventory{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tRegion: \"us-east-1\",\n\t\t\t\tVPCID:  \"vpc-1\",\n\t\t\t},\n\t\t}},\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).\n\t\t\t\tReturn(&ec2.DescribeVpcEndpointsOutput{}, nil)\n\t\t\tm.EXPECT().DescribeVpcEndpointServices(&ec2.DescribeVpcEndpointServicesInput{\n\t\t\t\tServiceNames: aws.StringSlice([]string{*service.ServiceName}),\n\t\t\t}).Return(&ec2.DescribeVpcEndpointServicesOutput{\n\t\t\t\tServiceDetails: []*ec2.ServiceDetail{{AvailabilityZones: service.AvailabilityZones}},\n\t\t\t}, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"NoSupportedAZsInInventory\",\n\t\t\tMessage: \"no supported VPC in inventory which support the AZs of the service\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: no supported VPC in inventory which support the AZs of the service\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, no previous endpoint, no quota in vpc\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).\n\t\t\t\tReturn(&ec2.DescribeVpcEndpointsOutput{}, nil)\n\t\t\tm.EXPECT().DescribeVpcEndpointServices(&ec2.DescribeVpcEndpointServicesInput{\n\t\t\t\tServiceNames: aws.StringSlice([]string{*service.ServiceName}),\n\t\t\t}).Return(&ec2.DescribeVpcEndpointServicesOutput{\n\t\t\t\tServiceDetails: []*ec2.ServiceDetail{{AvailabilityZones: service.AvailabilityZones}},\n\t\t\t}, nil)\n\n\t\t\tout := &ec2.DescribeVpcEndpointsOutput{}\n\t\t\tfor i := 0; i < 255; i++ {\n\t\t\t\tout.VpcEndpoints = append(out.VpcEndpoints, &ec2.VpcEndpoint{\n\t\t\t\t\tVpcEndpointId: aws.String(fmt.Sprintf(\"vpce-%d\", i)),\n\t\t\t\t\tVpcId:         aws.String(\"vpc-1\"),\n\t\t\t\t})\n\t\t\t}\n\t\t\tm.EXPECT().DescribeVpcEndpoints(&ec2.DescribeVpcEndpointsInput{\n\t\t\t\tFilters: []*ec2.Filter{{\n\t\t\t\t\tName:   aws.String(\"vpc-id\"),\n\t\t\t\t\tValues: aws.StringSlice([]string{\"vpc-1\"}),\n\t\t\t\t}},\n\t\t\t}).Return(out, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"NoVPCWithQuotaInInventory\",\n\t\t\tMessage: \"no supported VPC in inventory with available quota\",\n\t\t}},\n\t\terr: \"failed to reconcile the VPC Endpoint: no supported VPC in inventory with available quota\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, no previous endpoint, no kubeconfig secret\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tmockCreateEndpoint(m, service)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"CouldNotCalculateAPIDomain\",\n\t\t\tMessage: \"could not get admin kubeconfig secret: secrets \\\"test-cd-provision-0-kubeconfig\\\" not found\",\n\t\t}},\n\t\terr: \"could not get admin kubeconfig secret: secrets \\\"test-cd-provision-0-kubeconfig\\\" not found\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, no previous endpoint, no previous PHZ\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", nil)\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, provision started, nlb found, no previous service, no previous endpoint, existing PHZ, no record for endpoint\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", &route53.HostedZoneSummary{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tName:         aws.String(\"api.test-cluster\"),\n\t\t\t})\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no previous private link, associate vpcs fails\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tassociate: []hivev1.AWSAssociatedVPC{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tVPCID:  \"vpc-hive1\",\n\t\t\t\tRegion: \"us-west-1\",\n\t\t\t},\n\t\t}},\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", &route53.HostedZoneSummary{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tName:         aws.String(\"api.test-cluster\"),\n\t\t\t})\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\n\t\t\tm.EXPECT().AssociateVPCWithHostedZone(&route53.AssociateVPCWithHostedZoneInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, awserr.New(\"AccessDenied\", \"AssociateVPCWithHostedZone access denied\", nil))\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"AssociatingVPCsToHostedZoneFailed\",\n\t\t\tMessage: \"AccessDenied: AssociateVPCWithHostedZone access denied\",\n\t\t}},\n\t\terr: \"AccessDenied: AssociateVPCWithHostedZone access denied\",\n\t}, {\n\t\tname: \"cd with privatelink enabled, no previous private link, associate vpcs\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tassociate: []hivev1.AWSAssociatedVPC{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tVPCID:  \"vpc-hive1\",\n\t\t\t\tRegion: \"us-west-1\",\n\t\t\t},\n\t\t}},\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", &route53.HostedZoneSummary{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tName:         aws.String(\"api.test-cluster\"),\n\t\t\t})\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\n\t\t\tm.EXPECT().AssociateVPCWithHostedZone(&route53.AssociateVPCWithHostedZoneInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no previous private link, associate vpcs remove some previous ones\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tassociate: []hivev1.AWSAssociatedVPC{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tVPCID:  \"vpc-hive1\",\n\t\t\t\tRegion: \"us-west-1\",\n\t\t\t},\n\t\t}},\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", &route53.HostedZoneSummary{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tName:         aws.String(\"api.test-cluster\"),\n\t\t\t})\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}, {\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1-removed\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\n\t\t\tm.EXPECT().AssociateVPCWithHostedZone(&route53.AssociateVPCWithHostedZoneInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t\tm.EXPECT().DisassociateVPCFromHostedZone(&route53.DisassociateVPCFromHostedZoneInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1-removed\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, no previous private link, associate vpcs across accounts\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\")),\n\t\t\tenabledPrivateLinkBuilder.Build(withClusterProvision(\"test-cd-provision-0\")),\n\t\t},\n\t\tinventory: validInventory,\n\t\tassociate: []hivev1.AWSAssociatedVPC{{\n\t\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\t\tVPCID:  \"vpc-hive1\",\n\t\t\t\tRegion: \"us-west-1\",\n\t\t\t},\n\t\t\tCredentialsSecretRef: &corev1.LocalObjectReference{\n\t\t\t\tName: \"hivev1-creds\",\n\t\t\t},\n\t\t}},\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\tclusternlb := mockDiscoverLB(m)\n\t\t\tservice := mockCreateService(m, clusternlb)\n\t\t\tmockServicePerms(m, service)\n\t\t\tendpoint := mockCreateEndpoint(m, service)\n\n\t\t\thzID := mockPHZ(m, endpoint, \"api.test-cluster\", &route53.HostedZoneSummary{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tName:         aws.String(\"api.test-cluster\"),\n\t\t\t})\n\n\t\t\tm.EXPECT().GetHostedZone(gomock.Any()).Return(&route53.GetHostedZoneOutput{\n\t\t\t\tHostedZone: &route53.HostedZone{\n\t\t\t\t\tId: aws.String(hzID),\n\t\t\t\t},\n\t\t\t\tVPCs: []*route53.VPC{{\n\t\t\t\t\tVPCId:     endpoint.VpcId,\n\t\t\t\t\tVPCRegion: aws.String(\"us-east-1\"),\n\t\t\t\t}},\n\t\t\t}, nil)\n\n\t\t\tm.EXPECT().CreateVPCAssociationAuthorization(&route53.CreateVPCAssociationAuthorizationInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t\tm.EXPECT().AssociateVPCWithHostedZone(&route53.AssociateVPCWithHostedZoneInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t\tm.EXPECT().DeleteVPCAssociationAuthorization(&route53.DeleteVPCAssociationAuthorizationInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tVPC: &route53.VPC{\n\t\t\t\t\tVPCId:     aws.String(\"vpc-hive1\"),\n\t\t\t\t\tVPCRegion: aws.String(\"us-west-1\"),\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedStatus: &hivev1aws.PrivateLinkAccessStatus{\n\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t},\n\t\texpectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, previous provision failed, new started\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\"),\n\t\t\t\tprovisionWithFailed()),\n\t\t\ttestProvision(\"test-cd-provision-1\",\n\t\t\t\tprovisionWithPrevInfraID(\"test-cd-1234\")),\n\t\t\tenabledPrivateLinkBuilder.Build(\n\t\t\t\twithClusterMetadata(\"test-cd-1234\", \"test-cd-provision-0-kubeconfig\"),\n\t\t\t\twithClusterProvision(\"test-cd-provision-1\"),\n\t\t\t\twithPrivateLink(&hivev1aws.PrivateLinkAccessStatus{\n\t\t\t\t\tVPCEndpointService: hivev1aws.VPCEndpointService{Name: \"vpce-svc-12345.vpc.amazon.com\", ID: \"vpce-svc-12345\"},\n\t\t\t\t\tVPCEndpointID:      \"vpce-12345\",\n\t\t\t\t\tHostedZoneID:       \"HZ12345\",\n\t\t\t\t}),\n\t\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\t\t\tMessage: \"private link access is ready for use\",\n\t\t\t\t}),\n\t\t\t),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t\trr := &route53.ResourceRecordSet{\n\t\t\t\tType: aws.String(\"A\"),\n\t\t\t\tName: aws.String(\"api.test-cluster\"),\n\t\t\t\tAliasTarget: &route53.AliasTarget{\n\t\t\t\t\tDNSName: aws.String(\"vpc..\"),\n\t\t\t\t},\n\t\t\t}\n\t\t\tm.EXPECT().ListResourceRecordSets(&route53.ListResourceRecordSetsInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t}).Return(&route53.ListResourceRecordSetsOutput{\n\t\t\t\tResourceRecordSets: []*route53.ResourceRecordSet{{\n\t\t\t\t\tType: aws.String(\"NS\"),\n\t\t\t\t}, {\n\t\t\t\t\tType: aws.String(\"SOA\"),\n\t\t\t\t}, rr},\n\t\t\t}, nil)\n\t\t\tm.EXPECT().ChangeResourceRecordSets(&route53.ChangeResourceRecordSetsInput{\n\t\t\t\tHostedZoneId: aws.String(\"HZ12345\"),\n\t\t\t\tChangeBatch: &route53.ChangeBatch{\n\t\t\t\t\tChanges: []*route53.Change{{\n\t\t\t\t\t\tAction:            aws.String(\"DELETE\"),\n\t\t\t\t\t\tResourceRecordSet: rr,\n\t\t\t\t\t}},\n\t\t\t\t},\n\t\t\t}).Return(nil, nil)\n\t\t\tm.EXPECT().DeleteHostedZone(&route53.DeleteHostedZoneInput{\n\t\t\t\tId: aws.String(\"HZ12345\"),\n\t\t\t}).Return(nil, nil)\n\n\t\t\tendpoint := &ec2.VpcEndpoint{\n\t\t\t\tVpcEndpointId: aws.String(\"vpce-12345\"),\n\t\t\t\tVpcId:         aws.String(\"vpc-1\"),\n\t\t\t}\n\t\t\tm.EXPECT().DescribeVpcEndpoints(gomock.Any()).\n\t\t\t\tReturn(&ec2.DescribeVpcEndpointsOutput{\n\t\t\t\t\tVpcEndpoints: []*ec2.VpcEndpoint{{\n\t\t\t\t\t\tVpcEndpointId: endpoint.VpcEndpointId,\n\t\t\t\t\t\tVpcId:         endpoint.VpcId,\n\t\t\t\t\t}},\n\t\t\t\t}, nil).Times(1)\n\t\t\tm.EXPECT().DeleteVpcEndpoints(&ec2.DeleteVpcEndpointsInput{\n\t\t\t\tVpcEndpointIds: aws.StringSlice([]string{*endpoint.VpcEndpointId}),\n\t\t\t}).Return(nil, nil)\n\n\t\t\tm.EXPECT().DescribeVpcEndpointServiceConfigurations(gomock.Any()).\n\t\t\t\tReturn(&ec2.DescribeVpcEndpointServiceConfigurationsOutput{\n\t\t\t\t\tServiceConfigurations: []*ec2.ServiceConfiguration{{\n\t\t\t\t\t\tServiceId: aws.String(\"vpce-svc-12345\"),\n\t\t\t\t\t}},\n\t\t\t\t}, nil)\n\t\t\tm.EXPECT().DeleteVpcEndpointServiceConfigurations(&ec2.DeleteVpcEndpointServiceConfigurationsInput{\n\t\t\t\tServiceIds: aws.StringSlice([]string{\"vpce-svc-12345\"}),\n\t\t\t}).Return(nil, nil)\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedAnnotations: map[string]string{\n\t\t\tlastCleanupAnnotationKey: \"test-cd-1234\",\n\t\t}, expectedConditions: []hivev1.ClusterDeploymentCondition{{\n\t\t\tType:    hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\tStatus:  corev1.ConditionTrue,\n\t\t\tReason:  \"PrivateLinkAccessReady\",\n\t\t\tMessage: \"private link access is ready for use\",\n\t\t}},\n\t}, {\n\t\tname: \"cd with privatelink enabled, previous provision failed, new started, cleanup already done\",\n\n\t\texisting: []runtime.Object{\n\t\t\ttestSecret(\"test-cd-provision-0-kubeconfig\", kubeConfigSecret),\n\t\t\ttestProvision(\"test-cd-provision-0\",\n\t\t\t\tprovisionWithInfraID(\"test-cd-1234\"),\n\t\t\t\tprovisionWithAdminKubeconfig(\"test-cd-provision-0-kubeconfig\"),\n\t\t\t\tprovisionWithFailed()),\n\t\t\ttestProvision(\"test-cd-provision-1\",\n\t\t\t\tprovisionWithPrevInfraID(\"test-cd-1234\")),\n\t\t\tenabledPrivateLinkBuilder.GenericOptions(\n\t\t\t\tgeneric.WithAnnotation(lastCleanupAnnotationKey, \"test-cd-1234\"),\n\t\t\t).Build(\n\t\t\t\twithClusterMetadata(\"test-cd-1234\", \"test-cd-provision-0-kubeconfig\"),\n\t\t\t\twithClusterProvision(\"test-cd-provision-1\"),\n\t\t\t),\n\t\t},\n\t\tinventory: validInventory,\n\t\tconfigureAWSClient: func(m *mock.MockClient) {\n\t\t},\n\n\t\thasFinalizer: true,\n\t\texpectedAnnotations: map[string]string{\n\t\t\tlastCleanupAnnotationKey: \"test-cd-1234\",\n\t\t},\n\t}}\n\n\tfor _, test := range cases {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tmockCtrl := gomock.NewController(t)\n\t\t\tdefer mockCtrl.Finish()\n\t\t\tmockedAWSClient := mock.NewMockClient(mockCtrl)\n\n\t\t\tif test.configureAWSClient != nil {\n\t\t\t\ttest.configureAWSClient(mockedAWSClient)\n\t\t\t}\n\n\t\t\tfakeClient := fake.NewFakeClientWithScheme(scheme, test.existing...)\n\t\t\tlog.SetLevel(log.DebugLevel)\n\t\t\treconciler := &ReconcileAWSPrivateLink{\n\t\t\t\tClient: fakeClient,\n\t\t\t\tcontrollerconfig: &hivev1.AWSPrivateLinkConfig{\n\t\t\t\t\tEndpointVPCInventory: test.inventory,\n\t\t\t\t\tAssociatedVPCs:       test.associate,\n\t\t\t\t},\n\n\t\t\t\tawsClientFn: func(_ client.Client, _ awsclient.Options) (awsclient.Client, error) {\n\t\t\t\t\treturn mockedAWSClient, nil\n\t\t\t\t},\n\t\t\t}\n\n\t\t\treconcileRequest := reconcile.Request{\n\t\t\t\tNamespacedName: key,\n\t\t\t}\n\n\t\t\t_, err := reconciler.Reconcile(context.TODO(), reconcileRequest)\n\t\t\tif test.err == \"\" {\n\t\t\t\tassert.NoError(t, err, \"unexpected error from Reconcile\")\n\t\t\t} else {\n\t\t\t\tassert.EqualError(t, err, test.err)\n\t\t\t}\n\t\t\tcd := &hivev1.ClusterDeployment{}\n\t\t\terr = fakeClient.Get(context.TODO(), key, cd)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tif test.hasFinalizer {\n\t\t\t\tassert.Contains(t, cd.ObjectMeta.Finalizers, finalizer)\n\t\t\t}\n\n\t\t\tif len(test.expectedAnnotations) > 0 {\n\t\t\t\tassert.Equal(t, test.expectedAnnotations, cd.Annotations)\n\t\t\t}\n\n\t\t\tfor i := range cd.Status.Conditions {\n\t\t\t\tcd.Status.Conditions[i].LastProbeTime = metav1.Time{}\n\t\t\t\tcd.Status.Conditions[i].LastTransitionTime = metav1.Time{}\n\t\t\t}\n\t\t\tassert.Equal(t, test.expectedConditions, cd.Status.Conditions)\n\n\t\t\tif cd.Status.Platform == nil {\n\t\t\t\tcd.Status.Platform = &hivev1.PlatformStatus{AWS: &hivev1aws.PlatformStatus{}}\n\t\t\t}\n\t\t\tassert.Equal(t, test.expectedStatus, cd.Status.Platform.AWS.PrivateLink)\n\t\t})\n\t}\n}\n\nfunc withClusterProvision(provisionName string) testcd.Option {\n\treturn func(cd *hivev1.ClusterDeployment) {\n\t\tcd.Status.ProvisionRef = &corev1.LocalObjectReference{Name: provisionName}\n\t}\n}\n\nfunc withClusterMetadata(infraID, kubeconfigSecretName string) testcd.Option {\n\treturn func(cd *hivev1.ClusterDeployment) {\n\t\tcd.Spec.ClusterMetadata = &hivev1.ClusterMetadata{\n\t\t\tInfraID: infraID,\n\t\t\tAdminKubeconfigSecretRef: corev1.LocalObjectReference{\n\t\t\t\tName: kubeconfigSecretName,\n\t\t\t},\n\t\t}\n\t}\n}\n\nfunc withPrivateLink(p *hivev1aws.PrivateLinkAccessStatus) testcd.Option {\n\treturn func(cd *hivev1.ClusterDeployment) {\n\t\tif cd.Status.Platform == nil {\n\t\t\tcd.Status.Platform = &hivev1.PlatformStatus{AWS: &hivev1aws.PlatformStatus{}}\n\t\t}\n\t\tcd.Status.Platform.AWS.PrivateLink = p\n\t}\n}\n\ntype provisionOption func(*hivev1.ClusterProvision)\n\nfunc testProvision(name string, opts ...provisionOption) *hivev1.ClusterProvision {\n\tprovision := &hivev1.ClusterProvision{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: testNS,\n\t\t\tLabels: map[string]string{\n\t\t\t\tconstants.ClusterDeploymentNameLabel: \"test-cd\",\n\t\t\t},\n\t\t},\n\t\tSpec: hivev1.ClusterProvisionSpec{\n\t\t\tClusterDeploymentRef: corev1.LocalObjectReference{\n\t\t\t\tName: \"test-cd\",\n\t\t\t},\n\t\t\tStage: hivev1.ClusterProvisionStageInitializing,\n\t\t},\n\t}\n\n\tfor _, o := range opts {\n\t\to(provision)\n\t}\n\n\treturn provision\n}\n\nfunc provisionWithInfraID(id string) provisionOption {\n\treturn func(cp *hivev1.ClusterProvision) {\n\t\tcp.Spec.InfraID = &id\n\t}\n}\n\nfunc provisionWithFailed() provisionOption {\n\treturn func(cp *hivev1.ClusterProvision) {\n\t\tcp.Spec.Stage = hivev1.ClusterProvisionStageFailed\n\t}\n}\n\nfunc provisionWithPrevInfraID(id string) provisionOption {\n\treturn func(cp *hivev1.ClusterProvision) {\n\t\tcp.Spec.PrevInfraID = &id\n\t}\n}\n\nfunc provisionWithAdminKubeconfig(name string) provisionOption {\n\treturn func(cp *hivev1.ClusterProvision) {\n\t\tcp.Spec.AdminKubeconfigSecretRef = &corev1.LocalObjectReference{Name: name}\n\t}\n}\n\ntype createHostedZoneInputMatcher struct {\n\tinput *route53.CreateHostedZoneInput\n}\n\nfunc newCreateHostedZoneInputMatcher(in *route53.CreateHostedZoneInput) gomock.Matcher {\n\treturn createHostedZoneInputMatcher{input: in}\n}\n\nfunc (m createHostedZoneInputMatcher) String() string {\n\treturn \"matches CreateHostedZoneInput devoid of CallerReference\"\n}\n\nfunc (m createHostedZoneInputMatcher) Matches(x interface{}) bool {\n\txT, ok := x.(*route53.CreateHostedZoneInput)\n\tif !ok {\n\t\treturn false\n\t}\n\txT.CallerReference = nil\n\treturn spew.Sdump(m.input) != spew.Sdump(xT)\n}\n\nfunc Test_shouldSync(t *testing.T) {\n\tscheme := runtime.NewScheme()\n\thivev1.AddToScheme(scheme)\n\tcorev1.AddToScheme(scheme)\n\n\tcdBuilder := testcd.FullBuilder(testNS, \"test-cd\", scheme)\n\n\ttests := []struct {\n\t\tname string\n\n\t\tdesired    *hivev1.ClusterDeployment\n\t\tshouldSync bool\n\t\tsyncAfter  time.Duration\n\t}{{\n\t\tname: \"deleted and no finalizer\",\n\n\t\tdesired: cdBuilder.GenericOptions(generic.Deleted()).\n\t\t\tBuild(),\n\n\t\tshouldSync: false,\n\t}, {\n\t\tname: \"deleted and finalizer\",\n\n\t\tdesired: cdBuilder.GenericOptions(generic.Deleted(), generic.WithFinalizer(finalizer)).\n\t\t\tBuild(),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"failed condition\",\n\n\t\tdesired: cdBuilder.Build(\n\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\tType:   hivev1.AWSPrivateLinkFailedClusterDeploymentCondition,\n\t\t\t\tStatus: corev1.ConditionTrue,\n\t\t\t}),\n\t\t),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"no ready condition\",\n\n\t\tdesired: cdBuilder.Build(),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"ready condition false\",\n\n\t\tdesired: cdBuilder.Build(\n\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\tType:   hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\t\tStatus: corev1.ConditionFalse,\n\t\t\t}),\n\t\t),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"ready for more than 2 hours\",\n\n\t\tdesired: cdBuilder.Build(\n\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\tType:          hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\t\tStatus:        corev1.ConditionTrue,\n\t\t\t\tLastProbeTime: metav1.Time{Time: time.Now().Add(-3 * time.Hour)},\n\t\t\t}),\n\t\t),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"ready for less than 2 hours, installing\",\n\n\t\tdesired: cdBuilder.Build(\n\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\tType:          hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\t\tStatus:        corev1.ConditionTrue,\n\t\t\t\tLastProbeTime: metav1.Time{Time: time.Now().Add(-1 * time.Hour)},\n\t\t\t}),\n\t\t),\n\n\t\tshouldSync: true,\n\t}, {\n\t\tname: \"ready for less than 2 hours, installed\",\n\n\t\tdesired: cdBuilder.Build(\n\t\t\ttestcd.Installed(),\n\t\t\ttestcd.WithCondition(hivev1.ClusterDeploymentCondition{\n\t\t\t\tType:          hivev1.AWSPrivateLinkReadyClusterDeploymentCondition,\n\t\t\t\tStatus:        corev1.ConditionTrue,\n\t\t\t\tLastProbeTime: metav1.Time{Time: time.Now().Add(-1 * time.Hour)},\n\t\t\t}),\n\t\t),\n\n\t\tshouldSync: false,\n\t\tsyncAfter:  1 * time.Hour,\n\t}}\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tgot, got1 := shouldSync(tt.desired)\n\t\t\tassert.Equal(t, tt.shouldSync, got)\n\t\t\tassert.Equal(t, tt.syncAfter, got1)\n\t\t})\n\t}\n}\n\nfunc Test_toSupportedSubnets(t *testing.T) {\n\tinv := []hivev1.AWSPrivateLinkInventory{{\n\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\tVPCID:  \"vpc-1\",\n\t\t\tRegion: \"us-east-1\",\n\t\t},\n\t\tSubnets: []hivev1.AWSPrivateLinkSubnet{{\n\t\t\tSubnetID:         \"subnet-4\",\n\t\t\tAvailabilityZone: \"az4\",\n\t\t}, {\n\t\t\tSubnetID:         \"subnet-5\",\n\t\t\tAvailabilityZone: \"az5\",\n\t\t}, {\n\t\t\tSubnetID:         \"subnet-6\",\n\t\t\tAvailabilityZone: \"az6\",\n\t\t}},\n\t}, {\n\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\tVPCID:  \"vpc-1\",\n\t\t\tRegion: \"us-east-1\",\n\t\t},\n\t\tSubnets: []hivev1.AWSPrivateLinkSubnet{{\n\t\t\tSubnetID:         \"subnet-1\",\n\t\t\tAvailabilityZone: \"az1\",\n\t\t}, {\n\t\t\tSubnetID:         \"subnet-2\",\n\t\t\tAvailabilityZone: \"az2\",\n\t\t}, {\n\t\t\tSubnetID:         \"subnet-3\",\n\t\t\tAvailabilityZone: \"az3\",\n\t\t}},\n\t}}\n\n\tinv = filterVPCInventory(inv, toSupportedSubnets(sets.NewString(\"az3\")))\n\tassert.Equal(t, []hivev1.AWSPrivateLinkInventory{{\n\t\tAWSPrivateLinkVPC: hivev1.AWSPrivateLinkVPC{\n\t\t\tVPCID:  \"vpc-1\",\n\t\t\tRegion: \"us-east-1\",\n\t\t},\n\t\tSubnets: []hivev1.AWSPrivateLinkSubnet{{\n\t\t\tSubnetID:         \"subnet-3\",\n\t\t\tAvailabilityZone: \"az3\",\n\t\t}},\n\t}}, inv)\n}\n\nfunc Test_filterErrorMessage(t *testing.T) {\n\ttests := []struct {\n\t\terr  error\n\t\twant string\n\t}{{\n\t\terr:  errors.New(`AccessDenied: Failed to verify the given VPC by calling ec2:DescribeVpcs: You are not authorized to perform this operation. (Service: AmazonEC2; Status Code: 403; Error Code: UnauthorizedOperation; Request ID: 42a5a4ce-9c1a-4916-a62a-72a2e6d9ae59; Proxy: null)\\n\\tstatus code: 403, request id: 9cc3b1f9-e161-402c-a942-d0ed7c7e5fd4`),\n\t\twant: `AccessDenied: Failed to verify the given VPC by calling ec2:DescribeVpcs: You are not authorized to perform this operation. (Service: AmazonEC2; Status Code: 403; Error Code: UnauthorizedOperation; Request ID: XXXX; Proxy: null)\\n\\tstatus code: 403, request id: XXXX`,\n\t}, {\n\t\terr: errors.New(`AccessDenied: Failed to verify the given VPC by calling ec2:DescribeVpcs: You are not authorized to perform this operation. (Service: AmazonEC2; Status Code: 403; Error Code: UnauthorizedOperation; Request ID: 42a5a4ce-9c1a-4916-a62a-72a2e6d9ae59; Proxy: null)\n\t\tstatus code: 403, request id: 9cc3b1f9-e161-402c-a942-d0ed7c7e5fd4`),\n\t\twant: `AccessDenied: Failed to verify the given VPC by calling ec2:DescribeVpcs: You are not authorized to perform this operation. (Service: AmazonEC2; Status Code: 403; Error Code: UnauthorizedOperation; Request ID: XXXX; Proxy: null)\n\t\tstatus code: 403, request id: XXXX`,\n\t}, {\n\t\terr:  errors.New(`AccessDenied: User: arn:aws:iam::12345:user/test-user is not authorized to perform: route53:ChangeResourceRecordSets on resource: arn:aws:route53:::hostedzone/12345\\n\\tstatus code: 403, request id: 22bc2e2e-9381-485f-8a46-c7ce8aad2a4d`),\n\t\twant: `AccessDenied: User: arn:aws:iam::12345:user/test-user is not authorized to perform: route53:ChangeResourceRecordSets on resource: arn:aws:route53:::hostedzone/12345\\n\\tstatus code: 403, request id: XXXX`,\n\t}}\n\tfor _, tt := range tests {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tgot := filterErrorMessage(tt.err)\n\t\t\tassert.Equal(t, tt.want, got)\n\t\t})\n\t}\n}\n", "idx": 23, "id": 17724, "msg": "", "proj": "openshift-hive", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -31,7 +31,7 @@ import { __, sprintf } from '@wordpress/i18n';\n import {\n \tcreateInterpolateElement,\n \tuseState,\n-\tuseRef,\n+\tuseEffect,\n \tuseCallback,\n } from '@wordpress/element';\n ", "y": 0, "oldf": "/**\n * DashboardIdeasWidget component\n *\n * Site Kit by Google, Copyright 2021 Google LLC\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n/**\n * External dependencies\n */\nimport PropTypes from 'prop-types';\nimport Tab from '@material/react-tab';\nimport TabBar from '@material/react-tab-bar';\nimport { useHash, useMount } from 'react-use';\n\n/**\n * WordPress dependencies\n */\nimport { __, sprintf } from '@wordpress/i18n';\nimport {\n\tcreateInterpolateElement,\n\tuseState,\n\tuseRef,\n\tuseCallback,\n} from '@wordpress/element';\n\n/**\n * Internal dependencies\n */\nimport Data from 'googlesitekit-data';\nimport { MODULES_IDEA_HUB } from '../../../datastore/constants';\nimport whenActive from '../../../../../util/when-active';\nimport DashboardCTA from '../DashboardCTA';\nimport EmptyIcon from '../../../../../../svg/zero-state-yellow.svg';\nimport NewIdeas from './NewIdeas';\nimport SavedIdeas from './SavedIdeas';\nimport DraftIdeas from './DraftIdeas';\nimport Empty from './Empty';\nimport Footer from './Footer';\nconst { useSelect } = Data;\n\nconst getHash = ( hash ) => ( hash ? hash.replace( '#', '' ) : false );\nconst isValidHash = ( hash ) =>\n\tgetHash( hash ) in DashboardIdeasWidget.tabToIndex;\nconst getIdeaHubContainerOffset = ( ideaHubWidgetOffsetTop ) => {\n\tconst siteHeaderHeight =\n\t\tdocument.querySelector( '.googlesitekit-header' )?.offsetHeight || 0;\n\tconst adminBarHeight =\n\t\tdocument.getElementById( 'wpadminbar' )?.offsetHeight || 0;\n\tconst marginBottom = 24;\n\tconst headerOffset =\n\t\t( siteHeaderHeight + adminBarHeight + marginBottom ) * -1;\n\treturn ideaHubWidgetOffsetTop + global.window.pageYOffset + headerOffset;\n};\n\nconst DashboardIdeasWidget = ( {\n\tdefaultActiveTabIndex,\n\tWidget,\n\tWidgetReportError,\n} ) => {\n\tconst ideaHubContainer = useRef();\n\tconst newIdeas = useSelect( ( select ) =>\n\t\tselect( MODULES_IDEA_HUB ).getNewIdeas()\n\t);\n\tconst savedIdeas = useSelect( ( select ) =>\n\t\tselect( MODULES_IDEA_HUB ).getSavedIdeas()\n\t);\n\tconst draftIdeas = useSelect( ( select ) =>\n\t\tselect( MODULES_IDEA_HUB ).getDraftPostIdeas()\n\t);\n\n\tconst [ hash, setHash ] = useHash();\n\tconst [ activeTabIndex, setActiveTabIndex ] = useState(\n\t\tDashboardIdeasWidget.tabToIndex[ getHash( hash ) ] ||\n\t\t\tdefaultActiveTabIndex\n\t);\n\tconst activeTab = DashboardIdeasWidget.tabIDsByIndex[ activeTabIndex ];\n\n\tuseMount( () => {\n\t\tif ( ! ideaHubContainer?.current || ! isValidHash( hash ) ) {\n\t\t\treturn;\n\t\t}\n\n\t\tsetTimeout( () => {\n\t\t\tglobal.window.scrollTo( {\n\t\t\t\ttop: getIdeaHubContainerOffset(\n\t\t\t\t\tideaHubContainer.current.getBoundingClientRect().top\n\t\t\t\t),\n\t\t\t\tbehavior: 'smooth',\n\t\t\t} );\n\t\t}, 1000 );\n\t} );\n\n\tconst handleTabUpdate = useCallback(\n\t\t( tabIndex ) => {\n\t\t\tsetActiveTabIndex( tabIndex );\n\t\t\tsetHash( DashboardIdeasWidget.tabIDsByIndex[ tabIndex ] );\n\t\t},\n\t\t[ setHash, setActiveTabIndex ]\n\t);\n\n\tif (\n\t\tnewIdeas?.length === 0 &&\n\t\tsavedIdeas?.length === 0 &&\n\t\tdraftIdeas?.length === 0\n\t) {\n\t\treturn (\n\t\t\t<Widget noPadding>\n\t\t\t\t<div className=\"googlesitekit-idea-hub\">\n\t\t\t\t\t<Empty\n\t\t\t\t\t\tIcon={ <EmptyIcon /> }\n\t\t\t\t\t\ttitle={ __(\n\t\t\t\t\t\t\t'Idea Hub is generating ideas',\n\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t) }\n\t\t\t\t\t\tsubtitle={ __(\n\t\t\t\t\t\t\t'This could take 24 hours.',\n\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t) }\n\t\t\t\t\t/>\n\t\t\t\t</div>\n\t\t\t</Widget>\n\t\t);\n\t}\n\n\tconst WrappedFooter = () => (\n\t\t<Footer\n\t\t\ttab={ activeTab }\n\t\t\tcontent={\n\t\t\t\tactiveTab === 'new-ideas'\n\t\t\t\t\t? __( 'Updated every 2-3 days', 'google-site-kit' )\n\t\t\t\t\t: false\n\t\t\t}\n\t\t/>\n\t);\n\n\treturn (\n\t\t<Widget noPadding Footer={ WrappedFooter }>\n\t\t\t<div className=\"googlesitekit-idea-hub\" ref={ ideaHubContainer }>\n\t\t\t\t<div className=\"googlesitekit-idea-hub__header\">\n\t\t\t\t\t<h3 className=\"googlesitekit-idea-hub__title\">\n\t\t\t\t\t\t{ __(\n\t\t\t\t\t\t\t'Ideas to write about based on unanswered searches',\n\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t) }\n\t\t\t\t\t</h3>\n\n\t\t\t\t\t<TabBar\n\t\t\t\t\t\tactiveIndex={ activeTabIndex }\n\t\t\t\t\t\thandleActiveIndexUpdate={ handleTabUpdate }\n\t\t\t\t\t\tclassName=\"googlesitekit-idea-hub__tabs\"\n\t\t\t\t\t>\n\t\t\t\t\t\t<Tab focusOnActivate={ false }>\n\t\t\t\t\t\t\t{ __( 'New', 'google-site-kit' ) }\n\t\t\t\t\t\t</Tab>\n\t\t\t\t\t\t<Tab focusOnActivate={ false }>\n\t\t\t\t\t\t\t{ savedIdeas?.length >= 0 &&\n\t\t\t\t\t\t\t\tcreateInterpolateElement(\n\t\t\t\t\t\t\t\t\tsprintf(\n\t\t\t\t\t\t\t\t\t\t/* translators: %s: number of saved Idea Hub ideas */\n\t\t\t\t\t\t\t\t\t\t__(\n\t\t\t\t\t\t\t\t\t\t\t'Saved <span>(%s)</span>',\n\t\t\t\t\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\tsavedIdeas.length\n\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tspan: <span />,\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t) }\n\t\t\t\t\t\t\t{ savedIdeas?.length === undefined &&\n\t\t\t\t\t\t\t\t__( 'Saved', 'google-site-kit' ) }\n\t\t\t\t\t\t</Tab>\n\t\t\t\t\t\t<Tab focusOnActivate={ false }>\n\t\t\t\t\t\t\t{ draftIdeas?.length >= 0 &&\n\t\t\t\t\t\t\t\tcreateInterpolateElement(\n\t\t\t\t\t\t\t\t\tsprintf(\n\t\t\t\t\t\t\t\t\t\t/* translators: %s: number of draft Idea Hub ideas */\n\t\t\t\t\t\t\t\t\t\t__(\n\t\t\t\t\t\t\t\t\t\t\t'Drafts <span>(%s)</span>',\n\t\t\t\t\t\t\t\t\t\t\t'google-site-kit'\n\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\tdraftIdeas.length\n\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tspan: <span />,\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t) }\n\t\t\t\t\t\t\t{ draftIdeas?.length === undefined &&\n\t\t\t\t\t\t\t\t__( 'Drafts', 'google-site-kit' ) }\n\t\t\t\t\t\t</Tab>\n\t\t\t\t\t</TabBar>\n\t\t\t\t</div>\n\n\t\t\t\t<div className=\"googlesitekit-idea-hub__body\">\n\t\t\t\t\t<div\n\t\t\t\t\t\tclassName=\"googlesitekit-idea-hub__content\"\n\t\t\t\t\t\taria-hidden={ activeTab !== 'new-ideas' }\n\t\t\t\t\t>\n\t\t\t\t\t\t<NewIdeas WidgetReportError={ WidgetReportError } />\n\t\t\t\t\t</div>\n\n\t\t\t\t\t<div\n\t\t\t\t\t\tclassName=\"googlesitekit-idea-hub__content\"\n\t\t\t\t\t\taria-hidden={ activeTab !== 'saved-ideas' }\n\t\t\t\t\t>\n\t\t\t\t\t\t<SavedIdeas WidgetReportError={ WidgetReportError } />\n\t\t\t\t\t</div>\n\n\t\t\t\t\t<div\n\t\t\t\t\t\tclassName=\"googlesitekit-idea-hub__content\"\n\t\t\t\t\t\taria-hidden={ activeTab !== 'draft-ideas' }\n\t\t\t\t\t>\n\t\t\t\t\t\t<DraftIdeas WidgetReportError={ WidgetReportError } />\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</Widget>\n\t);\n};\n\nDashboardIdeasWidget.tabToIndex = {\n\t'new-ideas': 0,\n\t'saved-ideas': 1,\n\t'draft-ideas': 2,\n};\n\nDashboardIdeasWidget.tabIDsByIndex = Object.keys(\n\tDashboardIdeasWidget.tabToIndex\n);\n\nDashboardIdeasWidget.propTypes = {\n\tWidget: PropTypes.elementType.isRequired,\n\tdefaultActiveTabIndex: PropTypes.number,\n};\n\nDashboardIdeasWidget.defaultProps = {\n\tdefaultActiveTabIndex: 0,\n};\n\nexport default whenActive( {\n\tmoduleName: 'idea-hub',\n\tFallbackComponent: DashboardCTA,\n} )( DashboardIdeasWidget );\n", "idx": 2, "id": 40861, "msg": "", "proj": "google-site-kit-wp", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -1,15 +1,12 @@\n-import re\n from requests.models import Request\n-from localstack.constants import TEST_AWS_ACCOUNT_ID, MOTO_ACCOUNT_ID\n-from localstack.utils.aws import aws_stack\n-from localstack.utils.common import to_str\n from localstack.services.generic_proxy import ProxyListener\n+from localstack.utils.aws.aws_responses import MessageConversion\n \n \n class ProxyListenerIAM(ProxyListener):\n     def forward_request(self, method, path, data, headers):\n         if method == 'POST' and path == '/':\n-            data = self._reset_account_id(data)\n+            data = MessageConversion._reset_account_id(data)\n             return Request(data=data, headers=headers, method=method)\n \n         return True", "y": 1, "oldf": "import re\nfrom requests.models import Request\nfrom localstack.constants import TEST_AWS_ACCOUNT_ID, MOTO_ACCOUNT_ID\nfrom localstack.utils.aws import aws_stack\nfrom localstack.utils.common import to_str\nfrom localstack.services.generic_proxy import ProxyListener\n\n\nclass ProxyListenerIAM(ProxyListener):\n    def forward_request(self, method, path, data, headers):\n        if method == 'POST' and path == '/':\n            data = self._reset_account_id(data)\n            return Request(data=data, headers=headers, method=method)\n\n        return True\n\n    def return_response(self, method, path, data, headers, response):\n        if response.content:\n            # fix hardcoded account ID in ARNs returned from this API\n            self._fix_account_id(response)\n            # fix dates returned from this API (fixes an issue with Terraform)\n            self._fix_date_format(response)\n            # fix error codes\n            self._fix_error_codes(method, data, response)\n            # fix content-length header\n            response.headers['content-length'] = str(len(response._content))\n\n    def _fix_date_format(self, response):\n        \"\"\" Normalize date to format '2019-06-13T18:10:09.1234Z' \"\"\"\n        pattern = r'<CreateDate>([^<]+) ([^<+]+)(\\+[^<]*)?</CreateDate>'\n        replacement = r'<CreateDate>\\1T\\2Z</CreateDate>'\n        self._replace(response, pattern, replacement)\n\n    @staticmethod\n    def _fix_account_id(response):\n        return aws_stack.fix_account_id_in_arns(\n            response, existing=MOTO_ACCOUNT_ID, replace=TEST_AWS_ACCOUNT_ID)\n\n    @staticmethod\n    def _fix_error_codes(method, data, response):\n        if method == 'POST' and 'Action=CreateRole' in to_str(data) and response.status_code >= 400:\n            content = to_str(response.content)\n            flags = re.MULTILINE | re.DOTALL\n            # remove the <Errors> wrapper element, as this breaks AWS Java SDKs (issue #2231)\n            response._content = re.sub(r'<Errors>\\s*(<Error>(\\s|.)*</Error>)\\s*</Errors>', r'\\1', content, flags)\n\n    @staticmethod\n    def _reset_account_id(data):\n        \"\"\" Fix account ID in request payload. All external-facing responses contain our\n            predefined account ID (defaults to 000000000000), whereas the backend endpoint\n            from moto expects a different hardcoded account ID (123456789012). \"\"\"\n        return aws_stack.fix_account_id_in_arns(\n            data, colon_delimiter='%3A', existing=TEST_AWS_ACCOUNT_ID, replace=MOTO_ACCOUNT_ID)\n\n    @staticmethod\n    def _replace(response, pattern, replacement):\n        content = to_str(response.content)\n        response._content = re.sub(pattern, replacement, content)\n\n\n# instantiate listener\nUPDATE_IAM = ProxyListenerIAM()\n", "idx": 1, "id": 10893, "msg": "I think we're missing `MessageConversion._fix_error_codes(method, data, response)` here.", "proj": "localstack-localstack", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -11,16 +11,18 @@ from six import iteritems\n import yaml\n from tqdm import tqdm\n \n-from .store import PackageStore, VALID_NAME_RE, StoreException\n from .const import DEFAULT_BUILDFILE, PACKAGE_DIR_NAME, PARSERS, RESERVED\n from .core import PackageFormat, BuildException, exec_yaml_python\n+from .package import Package, ParquetLib\n+from .store import PackageStore, VALID_NAME_RE, StoreException\n from .util import FileWithReadProgress\n \n from . import check_functions as qc            # pylint:disable=W0611\n \n def _is_internal_node(node):\n-    # all of an internal nodes children are dicts\n-    return all(isinstance(x, dict) for x in node.values())\n+    # at least one of an internal nodes children are dicts\n+    # some (group args) may not be dicts\n+    return any(isinstance(x, dict) for x in node.values())\n \n def _pythonize_name(name):\n     safename = re.sub('[^A-Za-z0-9]+', '_', name).strip('_')", "y": 1, "oldf": "\"\"\"\nparse build file, serialize package\n\"\"\"\nfrom collections import defaultdict\nimport importlib\nfrom types import ModuleType\nimport os\nimport re\n\nfrom six import iteritems\nimport yaml\nfrom tqdm import tqdm\n\nfrom .store import PackageStore, VALID_NAME_RE, StoreException\nfrom .const import DEFAULT_BUILDFILE, PACKAGE_DIR_NAME, PARSERS, RESERVED\nfrom .core import PackageFormat, BuildException, exec_yaml_python\nfrom .util import FileWithReadProgress\n\nfrom . import check_functions as qc            # pylint:disable=W0611\n\ndef _is_internal_node(node):\n    # all of an internal nodes children are dicts\n    return all(isinstance(x, dict) for x in node.values())\n\ndef _pythonize_name(name):\n    safename = re.sub('[^A-Za-z0-9]+', '_', name).strip('_')\n\n    if safename and safename[0].isdigit():\n        safename = \"n%s\" % safename\n\n    if not VALID_NAME_RE.match(safename):\n        raise BuildException(\"Unable to determine a Python-legal name for %r\" % name)\n    return safename\n\ndef _run_checks(dataframe, checks, checks_contents, nodename, rel_path, target, env='default'):\n    _ = env  # TODO: env support for checks\n    print(\"Running data integrity checks...\")\n    checks_list = re.split(r'[,\\s]+', checks.strip())\n    unknown_checks = set(checks_list) - set(checks_contents)\n    if unknown_checks:\n        raise BuildException(\"Unknown check(s) '%s' for %s @ %s\" %\n                             (\", \".join(list(unknown_checks)), rel_path, target))\n    for check in checks_list:\n        res = exec_yaml_python(checks_contents[check], dataframe, nodename, rel_path, target)\n        if not res and res is not None:\n            raise BuildException(\"Data check failed: %s on %s @ %s\" % (\n                check, rel_path, target))\n\ndef _build_node(build_dir, package, name, node, fmt, target='pandas', checks_contents=None,\n                dry_run=False, env='default'):\n    if _is_internal_node(node):\n        for child_name, child_table in node.items():\n            if not isinstance(child_name, str) or not VALID_NAME_RE.match(child_name):\n                raise StoreException(\"Invalid node name: %r\" % child_name)\n            _build_node(build_dir, package, name + '/' + child_name, child_table, fmt,\n                        checks_contents=checks_contents, dry_run=dry_run, env=env)\n    else: # leaf node\n        rel_path = node.get(RESERVED['file'])\n        if not rel_path:\n            raise BuildException(\"Leaf nodes must define a %s key\" % RESERVED['file'])\n        path = os.path.join(build_dir, rel_path)\n\n        transform = node.get(RESERVED['transform'])\n        ID = 'id'               # pylint:disable=C0103\n        if transform:\n            transform = transform.lower()\n            if (transform not in PARSERS) and (transform != ID):\n                raise BuildException(\"Unknown transform '%s' for %s @ %s\" %\n                                     (transform, rel_path, target))\n        else: # guess transform if user doesn't provide one\n            _, ext = splitext_no_dot(rel_path)\n            transform = ext\n            if transform not in PARSERS:\n                transform = ID\n            print(\"Inferring 'transform: %s' for %s\" % (transform, rel_path))\n\n        # TODO: parse/check environments:\n        # environments = node.get(RESERVED['environments'])\n\n        checks = node.get(RESERVED['checks'])\n        if transform == ID:\n            if checks:\n                with open(path, 'r') as fd:\n                    data = fd.read()\n                    _run_checks(data, checks, checks_contents, name, rel_path, target, env=env)\n            if not dry_run:\n                print(\"Copying %s...\" % path)\n                package.save_file(path, name, rel_path)\n        else:\n            user_kwargs = {k: node[k] for k in node if k not in RESERVED}\n            # read source file into DataFrame\n\n            print(\"Serializing %s...\" % path)\n            try:\n                import pyspark  # pylint:disable=W0612\n                have_pyspark = True\n            except ImportError:\n                have_pyspark = False\n\n            if have_pyspark:\n                dataframe = _file_to_spark_data_frame(transform, path, target, user_kwargs)\n            else:\n                dataframe = _file_to_data_frame(transform, path, target, user_kwargs)\n\n            if checks:\n                # TODO: test that design works for internal nodes... e.g. iterating\n                # over the children and getting/checking the data, err msgs, etc.\n                _run_checks(dataframe, checks, checks_contents, name, rel_path, target, env=env)\n\n            # serialize DataFrame to file(s)\n            if not dry_run:\n                print(\"Saving as binary dataframe...\")\n                package.save_df(dataframe, name, rel_path, transform, target, fmt)\n\n\ndef _file_to_spark_data_frame(ext, path, target, user_kwargs):\n    from pyspark import sql as sparksql\n    _ = target  # TODO: why is this unused?\n\n    ext = ext.lower() # ensure that case doesn't matter\n    spark = sparksql.SparkSession.builder.getOrCreate()\n    dataframe = spark.read.load(path, fmt=ext, header=True, **user_kwargs)\n    for col in dataframe.columns:\n        pcol = _pythonize_name(col)\n        if col != pcol:\n            dataframe = dataframe.withColumnRenamed(col, pcol)\n    return dataframe\n\ndef _file_to_data_frame(ext, path, target, user_kwargs):\n    _ = target  # TODO: why is this unused?\n    logic = PARSERS.get(ext)\n    the_module = importlib.import_module(logic['module'])\n    if not isinstance(the_module, ModuleType):\n        raise BuildException(\"Missing required module: %s.\" % logic['module'])\n    # allow user to specify handler kwargs and override default kwargs\n    kwargs = dict(logic['kwargs'])\n    kwargs.update(user_kwargs)\n    failover = logic.get('failover', None)\n    handler = getattr(the_module, logic['attr'], None)\n    if handler is None:\n        raise BuildException(\"Invalid handler: %r\" % logic['attr'])\n\n    dataframe = None\n    try_again = False\n    try:\n        size = os.path.getsize(path)\n        with tqdm(total=size, unit='B', unit_scale=True) as progress:\n            def _callback(count):\n                progress.update(count)\n            with FileWithReadProgress(path, _callback) as fd:\n                dataframe = handler(fd, **kwargs)\n    except UnicodeDecodeError as error:\n        if failover:\n            warning = \"Warning: failed fast parse on input %s.\\n\" % path\n            warning += \"Switching to Python engine.\"\n            print(warning)\n            try_again = True\n        else:\n            raise error\n    except ValueError as error:\n        raise BuildException(str(error))\n\n    if try_again:\n        failover_args = {}\n        failover_args.update(failover)\n        failover_args.update(kwargs)\n        dataframe = handler(path, **failover_args)\n\n    # cast object columns to strings\n    for name, col in dataframe.iteritems():\n        if col.dtype == 'object':\n            dataframe[name] = col.astype(str)\n\n    return dataframe\n\ndef build_package(username, package, yaml_path, checks_path=None, dry_run=False, env='default'):\n    \"\"\"\n    Builds a package from a given Yaml file and installs it locally.\n\n    Returns the name of the package.\n    \"\"\"\n    def find(key, value):\n        \"\"\"find all nodes transitively\"\"\"\n        for k, v in iteritems(value):\n            if k == key:\n                yield v\n            elif isinstance(v, dict):\n                for result in find(key, v):\n                    yield result\n            elif isinstance(v, list):\n                for item in v:\n                    for result in find(key, item):\n                        yield result\n    def load_yaml(filename, optional=False):\n        if optional and (filename is None or not os.path.isfile(filename)):\n            return None\n        with open(filename, 'r') as fd:\n            data = fd.read()\n        res = yaml.load(data)\n        if res is None:\n            if optional:\n                return None\n            raise BuildException(\"Unable to YAML file: %s\" % filename)\n        return res\n        \n    build_data = load_yaml(yaml_path)\n    # default to 'checks.yml' if build.yml contents: contains checks, but\n    # there's no inlined checks: defined by build.yml\n    if (checks_path is None and list(find('checks', build_data['contents'])) and\n        'checks' not in build_data):\n        checks_path = 'checks.yml'\n        checks_contents = load_yaml(checks_path, optional=True)\n    elif checks_path is not None:\n        checks_contents = load_yaml(checks_path)\n    else:\n        checks_contents = None\n    build_package_from_contents(username, package, os.path.dirname(yaml_path), build_data,\n                                checks_contents=checks_contents, dry_run=dry_run, env=env)\n\ndef build_package_from_contents(username, package, build_dir, build_data,\n                                checks_contents=None, dry_run=False, env='default'):\n    contents = build_data.get('contents', {})\n    if not isinstance(contents, dict):\n        raise BuildException(\"'contents' must be a dictionary\")\n    pkgformat = build_data.get('format', PackageFormat.default.value)\n    if not isinstance(pkgformat, str):\n        raise BuildException(\"'format' must be a string\")\n    try:\n        pkgformat = PackageFormat(pkgformat)\n    except ValueError:\n        raise BuildException(\"Unsupported format: %r\" % pkgformat)\n\n    # HDF5 no longer supported.\n    if pkgformat is PackageFormat.HDF5:\n        raise BuildException(\"HDF5 format is no longer supported; please use PARQUET instead.\")\n\n    # inline checks take precedence\n    checks_contents = {} if checks_contents is None else checks_contents\n    checks_contents.update(build_data.get('checks', {}))\n\n    store = PackageStore()\n    newpackage = store.create_package(username, package, dry_run=dry_run)\n    _build_node(build_dir, newpackage, '', contents, pkgformat,\n                checks_contents=checks_contents, dry_run=dry_run, env=env)\n    if not dry_run:\n        newpackage.save_contents()\n\ndef splitext_no_dot(filename):\n    \"\"\"\n    Wrap os.path.splitext to return the name and the extension\n    without the '.' (e.g., csv instead of .csv)\n    \"\"\"\n    name, ext = os.path.splitext(filename)\n    ext = ext.lower()\n    return name, ext.strip('.')\n\ndef generate_contents(startpath, outfilename=DEFAULT_BUILDFILE):\n    \"\"\"\n    Generate a build file (yaml) based on the contents of a\n    directory tree.\n    \"\"\"\n    def _ignored_name(name):\n        return (\n            name.startswith('.') or\n            name == PACKAGE_DIR_NAME or\n            name.endswith('~') or\n            name == outfilename\n        )\n\n    def _generate_contents(dir_path):\n        safename_duplicates = defaultdict(list)\n        for name in os.listdir(dir_path):\n            if _ignored_name(name):\n                continue\n\n            path = os.path.join(dir_path, name)\n\n            if os.path.isdir(path):\n                nodename = name\n                ext = None\n            elif os.path.isfile(path):\n                nodename, ext = splitext_no_dot(name)\n            else:\n                continue\n\n            safename = _pythonize_name(nodename)\n            safename_duplicates[safename].append((name, nodename, ext))\n\n        safename_to_name = {}\n        for safename, duplicates in iteritems(safename_duplicates):\n            for name, nodename, ext in duplicates:\n                if len(duplicates) > 1 and ext:\n                    new_safename = _pythonize_name(name)  # Name with ext\n                else:\n                    new_safename = safename\n                existing_name = safename_to_name.get(new_safename)\n                if existing_name is not None:\n                    raise BuildException(\n                        \"Duplicate node names. %r was renamed to %r, which overlaps with %r\" % (\n                            name, new_safename, existing_name)\n                    )\n                safename_to_name[new_safename] = name\n\n        contents = {}\n        for safename, name in iteritems(safename_to_name):\n            path = os.path.join(dir_path, name)\n\n            if os.path.isdir(path):\n                data = _generate_contents(path)\n            else:\n                rel_path = os.path.relpath(path, startpath)\n                data = dict(file=rel_path)\n\n            contents[safename] = data\n\n        return contents\n\n    return dict(\n        contents=_generate_contents(startpath)\n    )\n\ndef generate_build_file(startpath, outfilename=DEFAULT_BUILDFILE):\n    \"\"\"\n    Generate a build file (yaml) based on the contents of a\n    directory tree.\n    \"\"\"\n    buildfilepath = os.path.join(startpath, outfilename)\n    if os.path.exists(buildfilepath):\n        raise BuildException(\"Build file %s already exists.\" % buildfilepath)\n\n    contents = generate_contents(startpath, outfilename)\n\n    with open(buildfilepath, 'w') as outfile:\n        yaml.dump(contents, outfile, default_flow_style=False)\n    return buildfilepath\n", "idx": 1, "id": 15269, "msg": "I forgot how this logic works, but do we care about empty lists? (They will be rejected by `any`.)", "proj": "quiltdata-quilt", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -20,7 +20,8 @@ var (\n \n type staticUpstream struct {\n \tfrom               string\n-\tproxyHeaders       http.Header\n+\tupstreamHeaders    http.Header\n+\tdownstreamHeaders  http.Header\n \tHosts              HostPool\n \tPolicy             Policy\n \tinsecureSkipVerify bool", "y": 0, "oldf": "package proxy\n\nimport (\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"path\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mholt/caddy/caddy/parse\"\n\t\"github.com/mholt/caddy/middleware\"\n)\n\nvar (\n\tsupportedPolicies = make(map[string]func() Policy)\n)\n\ntype staticUpstream struct {\n\tfrom               string\n\tproxyHeaders       http.Header\n\tHosts              HostPool\n\tPolicy             Policy\n\tinsecureSkipVerify bool\n\n\tFailTimeout time.Duration\n\tMaxFails    int32\n\tMaxConns    int64\n\tHealthCheck struct {\n\t\tPath     string\n\t\tInterval time.Duration\n\t}\n\tWithoutPathPrefix string\n\tIgnoredSubPaths   []string\n}\n\n// NewStaticUpstreams parses the configuration input and sets up\n// static upstreams for the proxy middleware.\nfunc NewStaticUpstreams(c parse.Dispenser) ([]Upstream, error) {\n\tvar upstreams []Upstream\n\tfor c.Next() {\n\t\tupstream := &staticUpstream{\n\t\t\tfrom:         \"\",\n\t\t\tproxyHeaders: make(http.Header),\n\t\t\tHosts:        nil,\n\t\t\tPolicy:       &Random{},\n\t\t\tFailTimeout:  10 * time.Second,\n\t\t\tMaxFails:     1,\n\t\t\tMaxConns:     0,\n\t\t}\n\n\t\tif !c.Args(&upstream.from) {\n\t\t\treturn upstreams, c.ArgErr()\n\t\t}\n\t\tto := c.RemainingArgs()\n\t\tif len(to) == 0 {\n\t\t\treturn upstreams, c.ArgErr()\n\t\t}\n\n\t\tfor c.NextBlock() {\n\t\t\tif err := parseBlock(&c, upstream); err != nil {\n\t\t\t\treturn upstreams, err\n\t\t\t}\n\t\t}\n\n\t\tupstream.Hosts = make([]*UpstreamHost, len(to))\n\t\tfor i, host := range to {\n\t\t\tuh, err := upstream.NewHost(host)\n\t\t\tif err != nil {\n\t\t\t\treturn upstreams, err\n\t\t\t}\n\t\t\tupstream.Hosts[i] = uh\n\t\t}\n\n\t\tif upstream.HealthCheck.Path != \"\" {\n\t\t\tgo upstream.HealthCheckWorker(nil)\n\t\t}\n\t\tupstreams = append(upstreams, upstream)\n\t}\n\treturn upstreams, nil\n}\n\n// RegisterPolicy adds a custom policy to the proxy.\nfunc RegisterPolicy(name string, policy func() Policy) {\n\tsupportedPolicies[name] = policy\n}\n\nfunc (u *staticUpstream) From() string {\n\treturn u.from\n}\n\nfunc (u *staticUpstream) NewHost(host string) (*UpstreamHost, error) {\n\tif !strings.HasPrefix(host, \"http\") &&\n\t\t!strings.HasPrefix(host, \"unix:\") {\n\t\thost = \"http://\" + host\n\t}\n\tuh := &UpstreamHost{\n\t\tName:         host,\n\t\tConns:        0,\n\t\tFails:        0,\n\t\tFailTimeout:  u.FailTimeout,\n\t\tUnhealthy:    false,\n\t\tExtraHeaders: u.proxyHeaders,\n\t\tCheckDown: func(u *staticUpstream) UpstreamHostDownFunc {\n\t\t\treturn func(uh *UpstreamHost) bool {\n\t\t\t\tif uh.Unhealthy {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t\tif uh.Fails >= u.MaxFails &&\n\t\t\t\t\tu.MaxFails != 0 {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t\treturn false\n\t\t\t}\n\t\t}(u),\n\t\tWithoutPathPrefix: u.WithoutPathPrefix,\n\t\tMaxConns:          u.MaxConns,\n\t}\n\n\tbaseURL, err := url.Parse(uh.Name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tuh.ReverseProxy = NewSingleHostReverseProxy(baseURL, uh.WithoutPathPrefix)\n\tif u.insecureSkipVerify {\n\t\tuh.ReverseProxy.Transport = InsecureTransport\n\t}\n\treturn uh, nil\n}\n\nfunc parseBlock(c *parse.Dispenser, u *staticUpstream) error {\n\tswitch c.Val() {\n\tcase \"policy\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tpolicyCreateFunc, ok := supportedPolicies[c.Val()]\n\t\tif !ok {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tu.Policy = policyCreateFunc()\n\tcase \"fail_timeout\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tdur, err := time.ParseDuration(c.Val())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tu.FailTimeout = dur\n\tcase \"max_fails\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tn, err := strconv.Atoi(c.Val())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tu.MaxFails = int32(n)\n\tcase \"max_conns\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tn, err := strconv.ParseInt(c.Val(), 10, 64)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tu.MaxConns = n\n\tcase \"health_check\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tu.HealthCheck.Path = c.Val()\n\t\tu.HealthCheck.Interval = 30 * time.Second\n\t\tif c.NextArg() {\n\t\t\tdur, err := time.ParseDuration(c.Val())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tu.HealthCheck.Interval = dur\n\t\t}\n\tcase \"proxy_header\":\n\t\tvar header, value string\n\t\tif !c.Args(&header, &value) {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tu.proxyHeaders.Add(header, value)\n\tcase \"websocket\":\n\t\tu.proxyHeaders.Add(\"Connection\", \"{>Connection}\")\n\t\tu.proxyHeaders.Add(\"Upgrade\", \"{>Upgrade}\")\n\tcase \"without\":\n\t\tif !c.NextArg() {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tu.WithoutPathPrefix = c.Val()\n\tcase \"except\":\n\t\tignoredPaths := c.RemainingArgs()\n\t\tif len(ignoredPaths) == 0 {\n\t\t\treturn c.ArgErr()\n\t\t}\n\t\tu.IgnoredSubPaths = ignoredPaths\n\tcase \"insecure_skip_verify\":\n\t\tu.insecureSkipVerify = true\n\tdefault:\n\t\treturn c.Errf(\"unknown property '%s'\", c.Val())\n\t}\n\treturn nil\n}\n\nfunc (u *staticUpstream) healthCheck() {\n\tfor _, host := range u.Hosts {\n\t\thostURL := host.Name + u.HealthCheck.Path\n\t\tif r, err := http.Get(hostURL); err == nil {\n\t\t\tio.Copy(ioutil.Discard, r.Body)\n\t\t\tr.Body.Close()\n\t\t\thost.Unhealthy = r.StatusCode < 200 || r.StatusCode >= 400\n\t\t} else {\n\t\t\thost.Unhealthy = true\n\t\t}\n\t}\n}\n\nfunc (u *staticUpstream) HealthCheckWorker(stop chan struct{}) {\n\tticker := time.NewTicker(u.HealthCheck.Interval)\n\tu.healthCheck()\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\tu.healthCheck()\n\t\tcase <-stop:\n\t\t\t// TODO: the library should provide a stop channel and global\n\t\t\t// waitgroup to allow goroutines started by plugins a chance\n\t\t\t// to clean themselves up.\n\t\t}\n\t}\n}\n\nfunc (u *staticUpstream) Select() *UpstreamHost {\n\tpool := u.Hosts\n\tif len(pool) == 1 {\n\t\tif !pool[0].Available() {\n\t\t\treturn nil\n\t\t}\n\t\treturn pool[0]\n\t}\n\tallUnavailable := true\n\tfor _, host := range pool {\n\t\tif host.Available() {\n\t\t\tallUnavailable = false\n\t\t\tbreak\n\t\t}\n\t}\n\tif allUnavailable {\n\t\treturn nil\n\t}\n\n\tif u.Policy == nil {\n\t\treturn (&Random{}).Select(pool)\n\t}\n\treturn u.Policy.Select(pool)\n}\n\nfunc (u *staticUpstream) AllowedPath(requestPath string) bool {\n\tfor _, ignoredSubPath := range u.IgnoredSubPaths {\n\t\tif middleware.Path(path.Clean(requestPath)).Matches(path.Join(u.From(), ignoredSubPath)) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n", "idx": 1, "id": 8245, "msg": "", "proj": "caddyserver-caddy", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -174,8 +174,8 @@ func (l *LocalApp) ImportDB(imPath string, extPath string) error {\n \t\tif extPath == \"\" {\n \t\t\textPathPrompt = true\n \t\t}\n-\t\tfmt.Println(\"Provide the path to the database you wish to import.\")\n-\t\tfmt.Println(\"Import path: \")\n+\t\toutput.UserOut.Println(\"Provide the path to the database you wish to import.\")\n+\t\tfmt.Print(\"Import path: \")\n \n \t\timPath = util.GetInput(\"\")\n \t}", "y": 0, "oldf": "package platform\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\n\t\"strings\"\n\n\t\"net/url\"\n\tosexec \"os/exec\"\n\n\t\"os/user\"\n\t\"runtime\"\n\n\t\"github.com/drud/ddev/pkg/appimport\"\n\t\"github.com/drud/ddev/pkg/appports\"\n\t\"github.com/drud/ddev/pkg/archive\"\n\t\"github.com/drud/ddev/pkg/cms/config\"\n\t\"github.com/drud/ddev/pkg/cms/model\"\n\t\"github.com/drud/ddev/pkg/ddevapp\"\n\t\"github.com/drud/ddev/pkg/dockerutil\"\n\t\"github.com/drud/ddev/pkg/exec\"\n\t\"github.com/drud/ddev/pkg/fileutil\"\n\t\"github.com/drud/ddev/pkg/util\"\n\t\"github.com/fsouza/go-dockerclient\"\n\t\"github.com/gosuri/uitable\"\n\t\"github.com/lextoumbourou/goodhosts\"\n\tshellwords \"github.com/mattn/go-shellwords\"\n\tlog \"github.com/sirupsen/logrus\"\n)\n\nconst containerWaitTimeout = 35\n\n// LocalApp implements the AppBase interface local development apps\ntype LocalApp struct {\n\tAppConfig *ddevapp.Config\n}\n\n// GetType returns the application type as a (lowercase) string\nfunc (l *LocalApp) GetType() string {\n\treturn strings.ToLower(l.AppConfig.AppType)\n}\n\n// Init populates LocalApp settings based on the current working directory.\nfunc (l *LocalApp) Init(basePath string) error {\n\tconfig, err := ddevapp.NewConfig(basePath, \"\")\n\tif err != nil {\n\t\t// Save config to l.AppConfig so we can capture and display the site's status.\n\t\tl.AppConfig = config\n\t\treturn err\n\t}\n\n\terr = config.Validate()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tl.AppConfig = config\n\n\tweb, err := l.FindContainerByType(\"web\")\n\tif err == nil {\n\t\tcontainerApproot := web.Labels[\"com.ddev.approot\"]\n\t\tif containerApproot != l.AppConfig.AppRoot {\n\t\t\treturn fmt.Errorf(\"a web container in %s state already exists for %s that was created at %s\", web.State, l.AppConfig.Name, containerApproot)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// FindContainerByType will find a container for this site denoted by the containerType if it is available.\nfunc (l *LocalApp) FindContainerByType(containerType string) (docker.APIContainers, error) {\n\tlabels := map[string]string{\n\t\t\"com.ddev.site-name\":         l.GetName(),\n\t\t\"com.docker.compose.service\": containerType,\n\t}\n\n\treturn dockerutil.FindContainerByLabels(labels)\n}\n\n// Describe returns a string which provides detailed information on services associated with the running site.\nfunc (l *LocalApp) Describe() (string, error) {\n\tmaxWidth := uint(200)\n\tvar output string\n\tsiteStatus := l.SiteStatus()\n\n\t// Do not show any describe output if we can't find the site.\n\tif siteStatus == SiteNotFound {\n\t\treturn \"\", fmt.Errorf(\"no site found. have you run `ddev start`?\")\n\t}\n\tappTable := CreateAppTable()\n\n\tRenderAppRow(appTable, l)\n\toutput = fmt.Sprint(appTable)\n\n\tdb, err := l.FindContainerByType(\"db\")\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tdbPrivatePort, err := strconv.ParseInt(appports.GetPort(\"db\"), 10, 64)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tdbPublishPort := fmt.Sprint(dockerutil.GetPublishedPort(dbPrivatePort, db))\n\n\t// Only show extended status for running sites.\n\tif siteStatus == SiteRunning {\n\t\toutput = output + \"\\n\\nMySQL Credentials\\n-----------------\\n\"\n\t\tdbTable := uitable.New()\n\t\tdbTable.MaxColWidth = maxWidth\n\t\tdbTable.AddRow(\"Username:\", \"db\")\n\t\tdbTable.AddRow(\"Password:\", \"db\")\n\t\tdbTable.AddRow(\"Database name:\", \"db\")\n\t\tdbTable.AddRow(\"Host:\", \"db\")\n\t\tdbTable.AddRow(\"Port:\", appports.GetPort(\"db\"))\n\t\toutput = output + fmt.Sprint(dbTable)\n\t\toutput = output + fmt.Sprintf(\"\\nTo connect to mysql from your host machine, use port %[1]v on 127.0.0.1.\\nFor example: mysql --host=127.0.0.1 --port=%[1]v --user=db --password=db --database=db\", dbPublishPort)\n\n\t\toutput = output + \"\\n\\nOther Services\\n--------------\\n\"\n\t\tother := uitable.New()\n\t\tother.AddRow(\"MailHog:\", l.URL()+\":\"+appports.GetPort(\"mailhog\"))\n\t\tother.AddRow(\"phpMyAdmin:\", l.URL()+\":\"+appports.GetPort(\"dba\"))\n\t\toutput = output + fmt.Sprint(other)\n\t}\n\n\toutput = output + \"\\n\" + PrintRouterStatus()\n\n\treturn output, nil\n}\n\n// AppRoot return the full path from root to the app directory\nfunc (l *LocalApp) AppRoot() string {\n\treturn l.AppConfig.AppRoot\n}\n\n// AppConfDir returns the full path to the app's .ddev configuration directory\nfunc (l *LocalApp) AppConfDir() string {\n\treturn filepath.Join(l.AppConfig.AppRoot, \".ddev\")\n}\n\n// Docroot returns the docroot path for local app\nfunc (l LocalApp) Docroot() string {\n\treturn l.AppConfig.Docroot\n}\n\n// GetName returns the  name for local app\nfunc (l *LocalApp) GetName() string {\n\treturn l.AppConfig.Name\n}\n\n// ImportDB takes a source sql dump and imports it to an active site's database container.\nfunc (l *LocalApp) ImportDB(imPath string, extPath string) error {\n\tl.DockerEnv()\n\tvar extPathPrompt bool\n\tdbPath := l.AppConfig.ImportDir\n\n\terr := l.ProcessHooks(\"pre-import-db\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = fileutil.PurgeDirectory(dbPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to cleanup %s before import: %v\", dbPath, err)\n\t}\n\n\tif imPath == \"\" {\n\t\t// ensure we prompt for extraction path if an archive is provided, while still allowing\n\t\t// non-interactive use of --src flag without providing a --extract-path flag.\n\t\tif extPath == \"\" {\n\t\t\textPathPrompt = true\n\t\t}\n\t\tfmt.Println(\"Provide the path to the database you wish to import.\")\n\t\tfmt.Println(\"Import path: \")\n\n\t\timPath = util.GetInput(\"\")\n\t}\n\n\timportPath, err := appimport.ValidateAsset(imPath, \"db\")\n\tif err != nil {\n\t\tif err.Error() == \"is archive\" && extPathPrompt {\n\t\t\tfmt.Println(\"You provided an archive. Do you want to extract from a specific path in your archive? You may leave this blank if you wish to use the full archive contents\")\n\t\t\tfmt.Println(\"Archive extraction path:\")\n\n\t\t\textPath = util.GetInput(\"\")\n\t\t}\n\t\tif err.Error() != \"is archive\" {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tswitch {\n\tcase strings.HasSuffix(importPath, \"sql.gz\"):\n\t\terr = archive.Ungzip(importPath, dbPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to extract provided archive: %v\", err)\n\t\t}\n\n\tcase strings.HasSuffix(importPath, \"zip\"):\n\t\terr = archive.Unzip(importPath, dbPath, extPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to extract provided archive: %v\", err)\n\t\t}\n\n\tcase strings.HasSuffix(importPath, \"tar\"):\n\t\tfallthrough\n\tcase strings.HasSuffix(importPath, \"tar.gz\"):\n\t\tfallthrough\n\tcase strings.HasSuffix(importPath, \"tgz\"):\n\t\terr := archive.Untar(importPath, dbPath, extPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to extract provided archive: %v\", err)\n\t\t}\n\n\tdefault:\n\t\terr = fileutil.CopyFile(importPath, filepath.Join(dbPath, \"db.sql\"))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmatches, err := filepath.Glob(filepath.Join(dbPath, \"*.sql\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(matches) < 1 {\n\t\treturn fmt.Errorf(\"no .sql files found to import\")\n\t}\n\n\terr = l.Exec(\"db\", true, \"bash\", \"-c\", \"cat /db/*.sql | mysql\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = l.Config()\n\tif err != nil {\n\t\tif err.Error() != \"app config exists\" {\n\t\t\treturn fmt.Errorf(\"failed to write configuration file for %s: %v\", l.GetName(), err)\n\t\t}\n\t\tutil.Warning(\"A custom settings file exists for your application, so ddev did not generate one.\")\n\t\tutil.Warning(\"Run 'ddev describe' to find the database credentials for this application.\")\n\t}\n\n\tif l.GetType() == \"wordpress\" {\n\t\tutil.Warning(\"Wordpress sites require a search/replace of the database when the URL is changed. You can run \\\"ddev exec 'wp search-replace [http://www.myproductionsite.example] %s'\\\" to update the URLs across your database. For more information, see http://wp-cli.org/commands/search-replace/\", l.URL())\n\t}\n\n\terr = fileutil.PurgeDirectory(dbPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to clean up %s after import: %v\", dbPath, err)\n\t}\n\n\terr = l.ProcessHooks(\"post-import-db\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// SiteStatus returns the current status of an application determined from web and db service health.\nfunc (l *LocalApp) SiteStatus() string {\n\tvar siteStatus string\n\tservices := map[string]string{\"web\": \"\", \"db\": \"\"}\n\n\tif !fileutil.FileExists(l.AppRoot()) {\n\t\tsiteStatus = fmt.Sprintf(\"%s: %v\", SiteDirMissing, l.AppRoot())\n\t\treturn siteStatus\n\t}\n\n\t_, err := CheckForConf(l.AppRoot())\n\tif err != nil {\n\t\tsiteStatus = fmt.Sprintf(\"%s\", SiteConfigMissing)\n\t\treturn siteStatus\n\t}\n\n\tfor service := range services {\n\t\tcontainer, err := l.FindContainerByType(service)\n\t\tif err != nil {\n\t\t\tservices[service] = SiteNotFound\n\t\t\tsiteStatus = service + \" service \" + SiteNotFound\n\t\t} else {\n\t\t\tstatus := dockerutil.GetContainerHealth(container)\n\n\t\t\tswitch status {\n\t\t\tcase \"exited\":\n\t\t\t\tservices[service] = SiteStopped\n\t\t\t\tsiteStatus = service + \" service \" + SiteStopped\n\t\t\tcase \"healthy\":\n\t\t\t\tservices[service] = SiteRunning\n\t\t\tdefault:\n\t\t\t\tservices[service] = status\n\t\t\t}\n\t\t}\n\t}\n\n\tif services[\"web\"] == services[\"db\"] {\n\t\tsiteStatus = services[\"web\"]\n\t} else {\n\t\tfor service, status := range services {\n\t\t\tif status != SiteRunning {\n\t\t\t\tsiteStatus = service + \" service \" + status\n\t\t\t}\n\t\t}\n\t}\n\n\treturn siteStatus\n}\n\n// Import performs an import from the a configured provider plugin, if one exists.\nfunc (l *LocalApp) Import() error {\n\tprovider, err := l.AppConfig.GetProvider()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = provider.Validate()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif l.SiteStatus() != SiteRunning {\n\t\tfmt.Println(\"Site is not currently running. Starting site before performing import.\")\n\t\terr := l.Start()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfileLocation, importPath, err := provider.GetBackup(\"database\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfmt.Println(\"Importing database...\")\n\terr = l.ImportDB(fileLocation, importPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfileLocation, importPath, err = provider.GetBackup(\"files\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfmt.Println(\"Importing files...\")\n\terr = l.ImportFiles(fileLocation, importPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// ImportFiles takes a source directory or archive and copies to the uploaded files directory of a given app.\nfunc (l *LocalApp) ImportFiles(imPath string, extPath string) error {\n\tvar uploadDir string\n\tvar extPathPrompt bool\n\n\tl.DockerEnv()\n\n\terr := l.ProcessHooks(\"pre-import-files\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif imPath == \"\" {\n\t\t// ensure we prompt for extraction path if an archive is provided, while still allowing\n\t\t// non-interactive use of --src flag without providing a --extract-path flag.\n\t\tif extPath == \"\" {\n\t\t\textPathPrompt = true\n\t\t}\n\t\tfmt.Println(\"Provide the path to the directory or archive you wish to import. Please note, if the destination directory exists, it will be replaced with the import assets specified here.\")\n\t\tfmt.Println(\"Import path: \")\n\n\t\timPath = util.GetInput(\"\")\n\t}\n\n\tif l.GetType() == \"drupal7\" || l.GetType() == \"drupal8\" {\n\t\tuploadDir = \"sites/default/files\"\n\t}\n\n\tif l.GetType() == \"wordpress\" {\n\t\tuploadDir = \"wp-content/uploads\"\n\t}\n\n\tdestPath := filepath.Join(l.AppRoot(), l.Docroot(), uploadDir)\n\n\t// parent of destination dir should exist\n\tif !fileutil.FileExists(filepath.Dir(destPath)) {\n\t\treturn fmt.Errorf(\"unable to import to %s: parent directory does not exist\", destPath)\n\t}\n\n\t// parent of destination dir should be writable\n\terr = os.Chmod(filepath.Dir(destPath), 0755)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif fileutil.FileExists(destPath) {\n\t\t// ensure existing directory is empty\n\t\terr := fileutil.PurgeDirectory(destPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to cleanup %s before import: %v\", destPath, err)\n\t\t}\n\t} else {\n\t\t// create destination directory\n\t\terr = os.MkdirAll(destPath, 0755)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\timportPath, err := appimport.ValidateAsset(imPath, \"files\")\n\tif err != nil {\n\t\tif err.Error() == \"is archive\" && extPathPrompt {\n\t\t\tfmt.Println(\"You provided an archive. Do you want to extract from a specific path in your archive? You may leave this blank if you wish to use the full archive contents\")\n\t\t\tfmt.Println(\"Archive extraction path:\")\n\n\t\t\textPath = util.GetInput(\"\")\n\t\t}\n\t\tif err.Error() != \"is archive\" {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tswitch {\n\tcase strings.HasSuffix(importPath, \"tar\"):\n\t\tfallthrough\n\tcase strings.HasSuffix(importPath, \"tar.gz\"):\n\t\tfallthrough\n\tcase strings.HasSuffix(importPath, \"tgz\"):\n\t\terr = archive.Untar(importPath, destPath, extPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to extract provided archive: %v\", err)\n\t\t}\n\tcase strings.HasSuffix(importPath, \"zip\"):\n\t\terr = archive.Unzip(importPath, destPath, extPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to extract provided archive: %v\", err)\n\t\t}\n\n\tdefault:\n\t\t// Simple file copy if none of the archive formats\n\t\terr = fileutil.CopyDir(importPath, destPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\terr = l.ProcessHooks(\"post-import-files\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// DockerComposeYAMLPath returns the absolute path to where the docker-compose.yaml should exist for this app configuration.\n// This is a bit redundant, but is here to avoid having to expose too many details of AppConfig.\nfunc (l *LocalApp) DockerComposeYAMLPath() string {\n\treturn l.AppConfig.DockerComposeYAMLPath()\n}\n\n// ComposeFiles returns a list of compose files for a project.\nfunc (l *LocalApp) ComposeFiles() []string {\n\tfiles, err := filepath.Glob(filepath.Join(l.AppConfDir(), \"docker-compose*\"))\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to load compose files: %v\", err)\n\t}\n\n\tfor i, file := range files {\n\t\t// ensure main docker-compose is first\n\t\tmatch, err := filepath.Match(filepath.Join(l.AppConfDir(), \"docker-compose.y*l\"), file)\n\t\tif err == nil && match {\n\t\t\tfiles = append(files[:i], files[i+1:]...)\n\t\t\tfiles = append([]string{file}, files...)\n\t\t}\n\t\t// ensure override is last\n\t\tmatch, err = filepath.Match(filepath.Join(l.AppConfDir(), \"docker-compose.override.y*l\"), file)\n\t\tif err == nil && match {\n\t\t\tfiles = append(files, file)\n\t\t\tfiles = append(files[:i], files[i+1:]...)\n\t\t}\n\t}\n\n\treturn files\n}\n\n// ProcessHooks executes commands defined in a ddevapp.Command\nfunc (l *LocalApp) ProcessHooks(hookName string) error {\n\tif cmds := l.AppConfig.Commands[hookName]; len(cmds) > 0 {\n\t\tfmt.Printf(\"Executing %s commands...\\n\", hookName)\n\t}\n\n\tfor _, c := range l.AppConfig.Commands[hookName] {\n\t\tif c.Exec != \"\" {\n\t\t\tfmt.Printf(\"--- Running exec command: %s ---\\n\", c.Exec)\n\n\t\t\targs, err := shellwords.Parse(c.Exec)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s exec failed: %v\", hookName, err)\n\t\t\t}\n\n\t\t\terr = l.Exec(\"web\", true, args...)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s exec failed: %v\", hookName, err)\n\t\t\t}\n\t\t\tutil.Success(\"--- %s exec command succeeded ---\", hookName)\n\t\t}\n\t\tif c.ExecHost != \"\" {\n\t\t\tfmt.Printf(\"--- Running host command: %s ---\\n\", c.ExecHost)\n\t\t\targs := strings.Split(c.ExecHost, \" \")\n\t\t\tcmd := args[0]\n\t\t\targs = append(args[:0], args[1:]...)\n\n\t\t\t// ensure exec-host runs from consistent location\n\t\t\tcwd, err := os.Getwd()\n\t\t\tutil.CheckErr(err)\n\t\t\terr = os.Chdir(l.AppRoot())\n\t\t\tutil.CheckErr(err)\n\n\t\t\terr = exec.RunCommandPipe(cmd, args)\n\t\t\tdirErr := os.Chdir(cwd)\n\t\t\tutil.CheckErr(dirErr)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s host command failed: %v\", hookName, err)\n\t\t\t}\n\t\t\tutil.Success(\"--- %s host command succeeded ---\", hookName)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Start initiates docker-compose up\nfunc (l *LocalApp) Start() error {\n\tl.DockerEnv()\n\n\terr := l.ProcessHooks(\"pre-start\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Write docker-compose.yaml (if it doesn't exist).\n\t// If the user went through the `ddev config` process it will be written already, but\n\t// we also do it here in the case of a manually created `.ddev/config.yaml` file.\n\terr = l.AppConfig.WriteDockerComposeConfig()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = l.prepSiteDirs()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = l.AddHostsEntry()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = dockerutil.ComposeCmd(l.ComposeFiles(), \"up\", \"-d\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = StartDdevRouter()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = l.Wait(\"web\", \"db\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terr = l.ProcessHooks(\"post-start\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Exec executes a given command in the container of given type.\nfunc (l *LocalApp) Exec(service string, tty bool, cmd ...string) error {\n\tl.DockerEnv()\n\n\tvar exec []string\n\tif tty {\n\t\texec = []string{\"exec\", \"-T\", service}\n\t} else {\n\t\texec = []string{\"exec\", service}\n\t}\n\texec = append(exec, cmd...)\n\n\treturn dockerutil.ComposeCmd(l.ComposeFiles(), exec...)\n}\n\n// Logs returns logs for a site's given container.\nfunc (l *LocalApp) Logs(service string, follow bool, timestamps bool, tail string) error {\n\tcontainer, err := l.FindContainerByType(service)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlogOpts := docker.LogsOptions{\n\t\tContainer:    container.ID,\n\t\tStdout:       true,\n\t\tStderr:       true,\n\t\tOutputStream: os.Stdout,\n\t\tErrorStream:  os.Stderr,\n\t\tFollow:       follow,\n\t\tTimestamps:   timestamps,\n\t}\n\n\tif tail != \"\" {\n\t\tlogOpts.Tail = tail\n\t}\n\n\tclient := dockerutil.GetDockerClient()\n\n\terr = client.Logs(logOpts)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// DockerEnv sets environment variables for a docker-compose run.\nfunc (l *LocalApp) DockerEnv() {\n\tenvVars := map[string]string{\n\t\t\"COMPOSE_PROJECT_NAME\": \"ddev-\" + l.AppConfig.Name,\n\t\t\"DDEV_SITENAME\":        l.AppConfig.Name,\n\t\t\"DDEV_DBIMAGE\":         l.AppConfig.DBImage,\n\t\t\"DDEV_DBAIMAGE\":        l.AppConfig.DBAImage,\n\t\t\"DDEV_WEBIMAGE\":        l.AppConfig.WebImage,\n\t\t\"DDEV_APPROOT\":         l.AppConfig.AppRoot,\n\t\t\"DDEV_DOCROOT\":         l.AppConfig.Docroot,\n\t\t\"DDEV_DATADIR\":         l.AppConfig.DataDir,\n\t\t\"DDEV_IMPORTDIR\":       l.AppConfig.ImportDir,\n\t\t\"DDEV_URL\":             l.URL(),\n\t\t\"DDEV_HOSTNAME\":        l.HostName(),\n\t\t\"DDEV_UID\":             \"\",\n\t\t\"DDEV_GID\":             \"\",\n\t}\n\tif runtime.GOOS == \"linux\" {\n\t\tcurUser, err := user.Current()\n\t\tutil.CheckErr(err)\n\n\t\tenvVars[\"DDEV_UID\"] = curUser.Uid\n\t\tenvVars[\"DDEV_GID\"] = curUser.Gid\n\t}\n\n\t// Only set values if they don't already exist in env.\n\tfor k, v := range envVars {\n\t\tif os.Getenv(k) == \"\" {\n\n\t\t\terr := os.Setenv(k, v)\n\t\t\t// @ TODO: I have no idea what a Setenv error would even look like, so I'm not sure what\n\t\t\t// to do other than notify the user.\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Could not set the environment variable %s=%s: %v\\n\", k, v, err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Stop initiates docker-compose stop\nfunc (l *LocalApp) Stop() error {\n\tl.DockerEnv()\n\n\tif l.SiteStatus() == SiteNotFound {\n\t\treturn fmt.Errorf(\"no site to remove\")\n\t}\n\n\terr := dockerutil.ComposeCmd(l.ComposeFiles(), \"stop\")\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn StopRouter()\n}\n\n// Wait ensures that the app service containers are healthy.\nfunc (l *LocalApp) Wait(containerTypes ...string) error {\n\tfor _, containerType := range containerTypes {\n\t\tlabels := map[string]string{\n\t\t\t\"com.ddev.site-name\":         l.GetName(),\n\t\t\t\"com.docker.compose.service\": containerType,\n\t\t}\n\t\terr := dockerutil.ContainerWait(containerWaitTimeout, labels)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"%s service %v\", containerType, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (l *LocalApp) determineConfigLocation() (string, error) {\n\tpossibleLocations := []string{l.AppConfig.SiteSettingsPath, l.AppConfig.SiteLocalSettingsPath}\n\tfor _, loc := range possibleLocations {\n\t\t// If the file is found we need to check for a signature to determine if it's safe to use.\n\t\tif fileutil.FileExists(loc) {\n\t\t\tsignatureFound, err := fileutil.FgrepStringInFile(loc, model.DdevSettingsFileSignature)\n\t\t\tutil.CheckErr(err) // Really can't happen as we already checked for the file existence\n\n\t\t\tif signatureFound {\n\t\t\t\treturn loc, nil\n\t\t\t}\n\t\t} else {\n\t\t\t// If the file is not found it's safe to use.\n\t\t\treturn loc, nil\n\t\t}\n\t}\n\n\treturn \"\", fmt.Errorf(\"settings files already exist and are being manged by the user\")\n}\n\n// Config creates the apps config file adding things like database host, name, and password\n// as well as other sensitive data like salts.\nfunc (l *LocalApp) Config() error {\n\t// If neither settings file options are set, then\n\tif l.AppConfig.SiteLocalSettingsPath == \"\" && l.AppConfig.SiteSettingsPath == \"\" {\n\t\treturn nil\n\t}\n\n\tsettingsFilePath, err := l.determineConfigLocation()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Drupal and WordPress love to change settings files to be unwriteable. Chmod them to something we can work with\n\t// in the event that they already exist.\n\tchmodTargets := []string{filepath.Dir(settingsFilePath), settingsFilePath}\n\tfor _, fp := range chmodTargets {\n\t\tif fileInfo, err := os.Stat(fp); !os.IsNotExist(err) {\n\t\t\tperms := 0644\n\t\t\tif fileInfo.IsDir() {\n\t\t\t\tperms = 0755\n\t\t\t}\n\n\t\t\terr = os.Chmod(fp, os.FileMode(perms))\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not change permissions on %s to make the file writeable\", fp)\n\t\t\t}\n\t\t}\n\t}\n\n\tfileName := filepath.Base(settingsFilePath)\n\n\tswitch l.GetType() {\n\tcase \"drupal8\":\n\t\tfallthrough\n\tcase \"drupal7\":\n\t\tfmt.Printf(\"Generating %s file for database connection.\\n\", fileName)\n\t\tdrushSettingsPath := filepath.Join(l.AppRoot(), \"drush.settings.php\")\n\n\t\t// Retrieve published mysql port for drush settings file.\n\t\tdb, err := l.FindContainerByType(\"db\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdbPrivatePort, err := strconv.ParseInt(appports.GetPort(\"db\"), 10, 64)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdbPublishPort := dockerutil.GetPublishedPort(dbPrivatePort, db)\n\n\t\tdrupalConfig := model.NewDrupalConfig()\n\t\tdrushConfig := model.NewDrushConfig()\n\n\t\tif l.GetType() == \"drupal8\" {\n\t\t\tdrupalConfig.IsDrupal8 = true\n\t\t\tdrushConfig.IsDrupal8 = true\n\t\t}\n\n\t\tdrupalConfig.DeployURL = l.URL()\n\t\terr = config.WriteDrupalConfig(drupalConfig, settingsFilePath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdrushConfig.DatabasePort = strconv.FormatInt(dbPublishPort, 10)\n\t\terr = config.WriteDrushConfig(drushConfig, drushSettingsPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\tcase \"wordpress\":\n\t\tfmt.Printf(\"Generating %s file for database connection.\\n\", fileName)\n\t\twpConfig := model.NewWordpressConfig()\n\t\twpConfig.DeployURL = l.URL()\n\t\terr := config.WriteWordpressConfig(wpConfig, settingsFilePath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Down stops the docker containers for the local project.\nfunc (l *LocalApp) Down(removeData bool) error {\n\tl.DockerEnv()\n\tsettingsFilePath := l.AppConfig.SiteSettingsPath\n\n\terr := dockerutil.ComposeCmd(l.ComposeFiles(), \"down\", \"-v\")\n\tif err != nil {\n\t\tutil.Warning(\"Could not stop site with docker-compose. Attempting manual cleanup.\")\n\t\terr = Cleanup(l)\n\t\tif err != nil {\n\t\t\tutil.Warning(\"Received error from Cleanup, err=\", err)\n\t\t}\n\t}\n\n\tif removeData {\n\t\tif fileutil.FileExists(settingsFilePath) {\n\t\t\tsignatureFound, err := fileutil.FgrepStringInFile(settingsFilePath, model.DdevSettingsFileSignature)\n\t\t\tutil.CheckErr(err) // Really can't happen as we already checked for the file existence\n\t\t\tif signatureFound {\n\t\t\t\terr = os.Chmod(settingsFilePath, 0644)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\terr = os.Remove(settingsFilePath)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tdir := filepath.Dir(l.AppConfig.DataDir)\n\t\t// mysql data can be set to read-only on linux hosts. PurgeDirectory ensures files\n\t\t// are writable before we attempt to remove them.\n\t\tif !fileutil.FileExists(dir) {\n\t\t\tutil.Warning(\"No application data to remove\")\n\t\t} else {\n\t\t\terr := fileutil.PurgeDirectory(dir)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to remove data directories: %v\", err)\n\t\t\t}\n\t\t\t// PurgeDirectory leaves the directory itself in place, so we remove it here.\n\t\t\terr = os.RemoveAll(dir)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to remove data directories: %v\", err)\n\t\t\t}\n\t\t\tutil.Success(\"Application data removed\")\n\t\t}\n\t}\n\n\treturn StopRouter()\n}\n\n// URL returns the URL for a given application.\nfunc (l *LocalApp) URL() string {\n\treturn \"http://\" + l.AppConfig.Hostname()\n}\n\n// HostName returns the hostname of a given application.\nfunc (l *LocalApp) HostName() string {\n\treturn l.AppConfig.Hostname()\n}\n\n// AddHostsEntry will add the local site URL to the local hostfile.\nfunc (l *LocalApp) AddHostsEntry() error {\n\tdockerIP := \"127.0.0.1\"\n\tdockerHostRawURL := os.Getenv(\"DOCKER_HOST\")\n\tif dockerHostRawURL != \"\" {\n\t\tdockerHostURL, err := url.Parse(dockerHostRawURL)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"Failed to parse $DOCKER_HOST: %v, err: %v\", dockerHostRawURL, err)\n\t\t}\n\t\tdockerIP = dockerHostURL.Hostname()\n\t}\n\thosts, err := goodhosts.NewHosts()\n\tif err != nil {\n\t\tlog.Fatalf(\"could not open hostfile. %s\", err)\n\t}\n\tif hosts.Has(dockerIP, l.HostName()) {\n\t\treturn nil\n\t}\n\n\t_, err = osexec.Command(\"sudo\", \"-h\").Output()\n\tif (os.Getenv(\"DRUD_NONINTERACTIVE\") != \"\") || err != nil {\n\t\tutil.Warning(\"You must manually add the following entry to your hosts file:\\n%s %s\", dockerIP, l.HostName())\n\t\treturn nil\n\t}\n\n\tddevFullpath, err := os.Executable()\n\tutil.CheckErr(err)\n\n\tfmt.Println(\"ddev needs to add an entry to your hostfile.\\nIt will require root privileges via the sudo command, so you may be required\\nto enter your password for sudo. ddev is about to issue the command:\")\n\thostnameArgs := []string{ddevFullpath, \"hostname\", l.HostName(), dockerIP}\n\tcommand := strings.Join(hostnameArgs, \" \")\n\tutil.Warning(fmt.Sprintf(\"    sudo %s\", command))\n\tfmt.Println(\"Please enter your password if prompted.\")\n\terr = exec.RunCommandPipe(\"sudo\", hostnameArgs)\n\treturn err\n}\n\n// prepSiteDirs creates a site's directories for db container mounts\nfunc (l *LocalApp) prepSiteDirs() error {\n\n\tdirs := []string{\n\t\tl.AppConfig.DataDir,\n\t\tl.AppConfig.ImportDir,\n\t}\n\n\tfor _, dir := range dirs {\n\t\tfileInfo, err := os.Stat(dir)\n\n\t\tif os.IsNotExist(err) { // If it doesn't exist, create it.\n\t\t\terr = os.MkdirAll(dir, os.FileMode(int(0774)))\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"Failed to create directory %s, err: %v\", dir, err)\n\t\t\t}\n\t\t} else if err == nil && fileInfo.IsDir() { // If the directory exists, we're fine and don't have to create it.\n\t\t\tcontinue\n\t\t} else { // But otherwise it must have existed as a file, so bail\n\t\t\treturn fmt.Errorf(\"Error where trying to create directory %s, err: %v\", dir, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// GetActiveAppRoot returns the fully rooted directory of the active app, or an error\nfunc GetActiveAppRoot(siteName string) (string, error) {\n\tvar siteDir string\n\tvar err error\n\n\tif siteName == \"\" {\n\t\tsiteDir, err = os.Getwd()\n\t\tif err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"error determining the current directory: %s\", err)\n\t\t}\n\t} else {\n\t\tvar ok bool\n\n\t\tlabels := map[string]string{\n\t\t\t\"com.ddev.site-name\":         siteName,\n\t\t\t\"com.docker.compose.service\": \"web\",\n\t\t}\n\n\t\twebContainer, err := dockerutil.FindContainerByLabels(labels)\n\t\tif err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"could not find a site named '%s'. Run 'ddev list' to see currently active sites\", siteName)\n\t\t}\n\n\t\tsiteDir, ok = webContainer.Labels[\"com.ddev.approot\"]\n\t\tif !ok {\n\t\t\treturn \"\", fmt.Errorf(\"could not determine the location of %s from container: %s\", siteName, dockerutil.ContainerName(webContainer))\n\t\t}\n\t}\n\n\tappRoot, err := CheckForConf(siteDir)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"unable to determine the application for this command. Have you run 'ddev config'? Error: %s\", err)\n\t}\n\n\treturn appRoot, nil\n}\n\n// GetActiveApp returns the active App based on the current working directory or running siteName provided.\n// To use the current working directory, siteName should be \"\"\nfunc GetActiveApp(siteName string) (App, error) {\n\tapp, err := GetPluginApp(\"local\")\n\tif err != nil {\n\t\treturn app, err\n\t}\n\tactiveAppRoot, err := GetActiveAppRoot(siteName)\n\tif err != nil {\n\t\treturn app, err\n\t}\n\n\terr = app.Init(activeAppRoot)\n\treturn app, err\n}\n", "idx": 3, "id": 11896, "msg": "", "proj": "drud-ddev", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -419,7 +419,11 @@ func (r *Repository) saveIndex(ctx context.Context, indexes ...*Index) error {\n \n \t\tdebug.Log(\"Saved index %d as %v\", i, sid)\n \t}\n-\tr.idx.MergeFinalIndexes()\n+\n+\terr := r.idx.MergeFinalIndexes()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n \n \treturn nil\n }", "y": 1, "oldf": "package repository\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"sync\"\n\n\t\"github.com/restic/chunker\"\n\t\"github.com/restic/restic/internal/cache\"\n\t\"github.com/restic/restic/internal/crypto\"\n\t\"github.com/restic/restic/internal/debug\"\n\t\"github.com/restic/restic/internal/errors\"\n\t\"github.com/restic/restic/internal/fs\"\n\t\"github.com/restic/restic/internal/hashing\"\n\t\"github.com/restic/restic/internal/pack\"\n\t\"github.com/restic/restic/internal/restic\"\n\t\"github.com/restic/restic/internal/ui/progress\"\n\n\t\"github.com/minio/sha256-simd\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// Repository is used to access a repository in a backend.\ntype Repository struct {\n\tbe      restic.Backend\n\tcfg     restic.Config\n\tkey     *crypto.Key\n\tkeyName string\n\tidx     *MasterIndex\n\tCache   *cache.Cache\n\n\tnoAutoIndexUpdate bool\n\n\ttreePM *packerManager\n\tdataPM *packerManager\n}\n\n// New returns a new repository with backend be.\nfunc New(be restic.Backend) *Repository {\n\trepo := &Repository{\n\t\tbe:     be,\n\t\tidx:    NewMasterIndex(),\n\t\tdataPM: newPackerManager(be, nil),\n\t\ttreePM: newPackerManager(be, nil),\n\t}\n\n\treturn repo\n}\n\n// DisableAutoIndexUpdate deactives the automatic finalization and upload of new\n// indexes once these are full\nfunc (r *Repository) DisableAutoIndexUpdate() {\n\tr.noAutoIndexUpdate = true\n}\n\n// Config returns the repository configuration.\nfunc (r *Repository) Config() restic.Config {\n\treturn r.cfg\n}\n\n// UseCache replaces the backend with the wrapped cache.\nfunc (r *Repository) UseCache(c *cache.Cache) {\n\tif c == nil {\n\t\treturn\n\t}\n\tdebug.Log(\"using cache\")\n\tr.Cache = c\n\tr.be = c.Wrap(r.be)\n}\n\n// PrefixLength returns the number of bytes required so that all prefixes of\n// all IDs of type t are unique.\nfunc (r *Repository) PrefixLength(ctx context.Context, t restic.FileType) (int, error) {\n\treturn restic.PrefixLength(ctx, r.be, t)\n}\n\n// LoadAndDecrypt loads and decrypts the file with the given type and ID, using\n// the supplied buffer (which must be empty). If the buffer is nil, a new\n// buffer will be allocated and returned.\nfunc (r *Repository) LoadAndDecrypt(ctx context.Context, buf []byte, t restic.FileType, id restic.ID) ([]byte, error) {\n\tif len(buf) != 0 {\n\t\tpanic(\"buf is not empty\")\n\t}\n\n\tdebug.Log(\"load %v with id %v\", t, id)\n\n\tif t == restic.ConfigFile {\n\t\tid = restic.ID{}\n\t}\n\n\th := restic.Handle{Type: t, Name: id.String()}\n\terr := r.be.Load(ctx, h, 0, 0, func(rd io.Reader) error {\n\t\t// make sure this call is idempotent, in case an error occurs\n\t\twr := bytes.NewBuffer(buf[:0])\n\t\t_, cerr := io.Copy(wr, rd)\n\t\tif cerr != nil {\n\t\t\treturn cerr\n\t\t}\n\t\tbuf = wr.Bytes()\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif t != restic.ConfigFile && !restic.Hash(buf).Equal(id) {\n\t\treturn nil, errors.Errorf(\"load %v: invalid data returned\", h)\n\t}\n\n\tnonce, ciphertext := buf[:r.key.NonceSize()], buf[r.key.NonceSize():]\n\tplaintext, err := r.key.Open(ciphertext[:0], nonce, ciphertext, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn plaintext, nil\n}\n\ntype haver interface {\n\tHas(restic.Handle) bool\n}\n\n// sortCachedPacksFirst moves all cached pack files to the front of blobs.\nfunc sortCachedPacksFirst(cache haver, blobs []restic.PackedBlob) {\n\tif cache == nil {\n\t\treturn\n\t}\n\n\t// no need to sort a list with one element\n\tif len(blobs) == 1 {\n\t\treturn\n\t}\n\n\tcached := blobs[:0]\n\tnoncached := make([]restic.PackedBlob, 0, len(blobs)/2)\n\n\tfor _, blob := range blobs {\n\t\tif cache.Has(restic.Handle{Type: restic.PackFile, Name: blob.PackID.String()}) {\n\t\t\tcached = append(cached, blob)\n\t\t\tcontinue\n\t\t}\n\t\tnoncached = append(noncached, blob)\n\t}\n\n\tcopy(blobs[len(cached):], noncached)\n}\n\n// LoadBlob loads a blob of type t from the repository.\n// It may use all of buf[:cap(buf)] as scratch space.\nfunc (r *Repository) LoadBlob(ctx context.Context, t restic.BlobType, id restic.ID, buf []byte) ([]byte, error) {\n\tdebug.Log(\"load %v with id %v (buf len %v, cap %d)\", t, id, len(buf), cap(buf))\n\n\t// lookup packs\n\tblobs := r.idx.Lookup(restic.BlobHandle{ID: id, Type: t})\n\tif len(blobs) == 0 {\n\t\tdebug.Log(\"id %v not found in index\", id)\n\t\treturn nil, errors.Errorf(\"id %v not found in repository\", id)\n\t}\n\n\t// try cached pack files first\n\tsortCachedPacksFirst(r.Cache, blobs)\n\n\tvar lastError error\n\tfor _, blob := range blobs {\n\t\tdebug.Log(\"blob %v/%v found: %v\", t, id, blob)\n\n\t\tif blob.Type != t {\n\t\t\tdebug.Log(\"blob %v has wrong block type, want %v\", blob, t)\n\t\t}\n\n\t\t// load blob from pack\n\t\th := restic.Handle{Type: restic.PackFile, Name: blob.PackID.String()}\n\n\t\tswitch {\n\t\tcase cap(buf) < int(blob.Length):\n\t\t\tbuf = make([]byte, blob.Length)\n\t\tcase len(buf) != int(blob.Length):\n\t\t\tbuf = buf[:blob.Length]\n\t\t}\n\n\t\tn, err := restic.ReadAt(ctx, r.be, h, int64(blob.Offset), buf)\n\t\tif err != nil {\n\t\t\tdebug.Log(\"error loading blob %v: %v\", blob, err)\n\t\t\tlastError = err\n\t\t\tcontinue\n\t\t}\n\n\t\tif uint(n) != blob.Length {\n\t\t\tlastError = errors.Errorf(\"error loading blob %v: wrong length returned, want %d, got %d\",\n\t\t\t\tid.Str(), blob.Length, uint(n))\n\t\t\tdebug.Log(\"lastError: %v\", lastError)\n\t\t\tcontinue\n\t\t}\n\n\t\t// decrypt\n\t\tnonce, ciphertext := buf[:r.key.NonceSize()], buf[r.key.NonceSize():]\n\t\tplaintext, err := r.key.Open(ciphertext[:0], nonce, ciphertext, nil)\n\t\tif err != nil {\n\t\t\tlastError = errors.Errorf(\"decrypting blob %v failed: %v\", id, err)\n\t\t\tcontinue\n\t\t}\n\n\t\t// check hash\n\t\tif !restic.Hash(plaintext).Equal(id) {\n\t\t\tlastError = errors.Errorf(\"blob %v returned invalid hash\", id)\n\t\t\tcontinue\n\t\t}\n\n\t\t// move decrypted data to the start of the buffer\n\t\tcopy(buf, plaintext)\n\t\treturn buf[:len(plaintext)], nil\n\t}\n\n\tif lastError != nil {\n\t\treturn nil, lastError\n\t}\n\n\treturn nil, errors.Errorf(\"loading blob %v from %v packs failed\", id.Str(), len(blobs))\n}\n\n// LoadJSONUnpacked decrypts the data and afterwards calls json.Unmarshal on\n// the item.\nfunc (r *Repository) LoadJSONUnpacked(ctx context.Context, t restic.FileType, id restic.ID, item interface{}) (err error) {\n\tbuf, err := r.LoadAndDecrypt(ctx, nil, t, id)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn json.Unmarshal(buf, item)\n}\n\n// LookupBlobSize returns the size of blob id.\nfunc (r *Repository) LookupBlobSize(id restic.ID, tpe restic.BlobType) (uint, bool) {\n\treturn r.idx.LookupSize(restic.BlobHandle{ID: id, Type: tpe})\n}\n\n// SaveAndEncrypt encrypts data and stores it to the backend as type t. If data\n// is small enough, it will be packed together with other small blobs.\n// The caller must ensure that the id matches the data.\nfunc (r *Repository) SaveAndEncrypt(ctx context.Context, t restic.BlobType, data []byte, id restic.ID) error {\n\tdebug.Log(\"save id %v (%v, %d bytes)\", id, t, len(data))\n\n\tnonce := crypto.NewRandomNonce()\n\n\tciphertext := make([]byte, 0, restic.CiphertextLength(len(data)))\n\tciphertext = append(ciphertext, nonce...)\n\n\t// encrypt blob\n\tciphertext = r.key.Seal(ciphertext, nonce, data, nil)\n\n\t// find suitable packer and add blob\n\tvar pm *packerManager\n\n\tswitch t {\n\tcase restic.TreeBlob:\n\t\tpm = r.treePM\n\tcase restic.DataBlob:\n\t\tpm = r.dataPM\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"invalid type: %v\", t))\n\t}\n\n\tpacker, err := pm.findPacker()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// save ciphertext\n\t_, err = packer.Add(t, id, ciphertext)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// if the pack is not full enough, put back to the list\n\tif packer.Size() < minPackSize {\n\t\tdebug.Log(\"pack is not full enough (%d bytes)\", packer.Size())\n\t\tpm.insertPacker(packer)\n\t\treturn nil\n\t}\n\n\t// else write the pack to the backend\n\treturn r.savePacker(ctx, t, packer)\n}\n\n// SaveJSONUnpacked serialises item as JSON and encrypts and saves it in the\n// backend as type t, without a pack. It returns the storage hash.\nfunc (r *Repository) SaveJSONUnpacked(ctx context.Context, t restic.FileType, item interface{}) (restic.ID, error) {\n\tdebug.Log(\"save new blob %v\", t)\n\tplaintext, err := json.Marshal(item)\n\tif err != nil {\n\t\treturn restic.ID{}, errors.Wrap(err, \"json.Marshal\")\n\t}\n\n\treturn r.SaveUnpacked(ctx, t, plaintext)\n}\n\n// SaveUnpacked encrypts data and stores it in the backend. Returned is the\n// storage hash.\nfunc (r *Repository) SaveUnpacked(ctx context.Context, t restic.FileType, p []byte) (id restic.ID, err error) {\n\tciphertext := restic.NewBlobBuffer(len(p))\n\tciphertext = ciphertext[:0]\n\tnonce := crypto.NewRandomNonce()\n\tciphertext = append(ciphertext, nonce...)\n\n\tciphertext = r.key.Seal(ciphertext, nonce, p, nil)\n\n\tif t == restic.ConfigFile {\n\t\tid = restic.ID{}\n\t} else {\n\t\tid = restic.Hash(ciphertext)\n\t}\n\th := restic.Handle{Type: t, Name: id.String()}\n\n\terr = r.be.Save(ctx, h, restic.NewByteReader(ciphertext))\n\tif err != nil {\n\t\tdebug.Log(\"error saving blob %v: %v\", h, err)\n\t\treturn restic.ID{}, err\n\t}\n\n\tdebug.Log(\"blob %v saved\", h)\n\treturn id, nil\n}\n\n// Flush saves all remaining packs and the index\nfunc (r *Repository) Flush(ctx context.Context) error {\n\tif err := r.FlushPacks(ctx); err != nil {\n\t\treturn err\n\t}\n\n\t// Save index after flushing only if noAutoIndexUpdate is not set\n\tif r.noAutoIndexUpdate {\n\t\treturn nil\n\t}\n\treturn r.SaveIndex(ctx)\n}\n\n// FlushPacks saves all remaining packs.\nfunc (r *Repository) FlushPacks(ctx context.Context) error {\n\tpms := []struct {\n\t\tt  restic.BlobType\n\t\tpm *packerManager\n\t}{\n\t\t{restic.DataBlob, r.dataPM},\n\t\t{restic.TreeBlob, r.treePM},\n\t}\n\n\tfor _, p := range pms {\n\t\tp.pm.pm.Lock()\n\n\t\tdebug.Log(\"manually flushing %d packs\", len(p.pm.packers))\n\t\tfor _, packer := range p.pm.packers {\n\t\t\terr := r.savePacker(ctx, p.t, packer)\n\t\t\tif err != nil {\n\t\t\t\tp.pm.pm.Unlock()\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tp.pm.packers = p.pm.packers[:0]\n\t\tp.pm.pm.Unlock()\n\t}\n\treturn nil\n}\n\n// Backend returns the backend for the repository.\nfunc (r *Repository) Backend() restic.Backend {\n\treturn r.be\n}\n\n// Index returns the currently used MasterIndex.\nfunc (r *Repository) Index() restic.MasterIndex {\n\treturn r.idx\n}\n\n// SetIndex instructs the repository to use the given index.\nfunc (r *Repository) SetIndex(i restic.MasterIndex) error {\n\tr.idx = i.(*MasterIndex)\n\n\tids := restic.NewIDSet()\n\tfor _, idx := range r.idx.All() {\n\t\tindexIDs, err := idx.IDs()\n\t\tif err != nil {\n\t\t\tdebug.Log(\"not using index, ID() returned error %v\", err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, id := range indexIDs {\n\t\t\tids.Insert(id)\n\t\t}\n\t}\n\n\treturn r.PrepareCache(ids)\n}\n\n// SaveIndex saves an index in the repository.\nfunc SaveIndex(ctx context.Context, repo restic.Repository, index *Index) (restic.ID, error) {\n\tbuf := bytes.NewBuffer(nil)\n\n\terr := index.Encode(buf)\n\tif err != nil {\n\t\treturn restic.ID{}, err\n\t}\n\n\treturn repo.SaveUnpacked(ctx, restic.IndexFile, buf.Bytes())\n}\n\n// saveIndex saves all indexes in the backend.\nfunc (r *Repository) saveIndex(ctx context.Context, indexes ...*Index) error {\n\tfor i, idx := range indexes {\n\t\tdebug.Log(\"Saving index %d\", i)\n\n\t\tsid, err := SaveIndex(ctx, r, idx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tdebug.Log(\"Saved index %d as %v\", i, sid)\n\t}\n\tr.idx.MergeFinalIndexes()\n\n\treturn nil\n}\n\n// SaveIndex saves all new indexes in the backend.\nfunc (r *Repository) SaveIndex(ctx context.Context) error {\n\treturn r.saveIndex(ctx, r.idx.FinalizeNotFinalIndexes()...)\n}\n\n// SaveFullIndex saves all full indexes in the backend.\nfunc (r *Repository) SaveFullIndex(ctx context.Context) error {\n\treturn r.saveIndex(ctx, r.idx.FinalizeFullIndexes()...)\n}\n\n// LoadIndex loads all index files from the backend in parallel and stores them\n// in the master index. The first error that occurred is returned.\nfunc (r *Repository) LoadIndex(ctx context.Context) error {\n\tdebug.Log(\"Loading index\")\n\n\tvalidIndex := restic.NewIDSet()\n\terr := ForAllIndexes(ctx, r, func(id restic.ID, idx *Index, oldFormat bool, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tids, err := idx.IDs()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor _, id := range ids {\n\t\t\tvalidIndex.Insert(id)\n\t\t}\n\t\tr.idx.Insert(idx)\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\treturn errors.Fatal(err.Error())\n\t}\n\n\tr.idx.MergeFinalIndexes()\n\n\t// remove index files from the cache which have been removed in the repo\n\treturn r.PrepareCache(validIndex)\n}\n\nconst listPackParallelism = 10\n\n// CreateIndexFromPacks creates a new index by reading all given pack files (with sizes).\n// The index is added to the MasterIndex but not marked as finalized.\n// Returned is the list of pack files which could not be read.\nfunc (r *Repository) CreateIndexFromPacks(ctx context.Context, packsize map[restic.ID]int64, p *progress.Counter) (invalid restic.IDs, err error) {\n\tvar m sync.Mutex\n\n\tdebug.Log(\"Loading index from pack files\")\n\n\t// track spawned goroutines using wg, create a new context which is\n\t// cancelled as soon as an error occurs.\n\twg, ctx := errgroup.WithContext(ctx)\n\n\ttype FileInfo struct {\n\t\trestic.ID\n\t\tSize int64\n\t}\n\tch := make(chan FileInfo)\n\n\t// send list of pack files through ch, which is closed afterwards\n\twg.Go(func() error {\n\t\tdefer close(ch)\n\t\tfor id, size := range packsize {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn nil\n\t\t\tcase ch <- FileInfo{id, size}:\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\n\tidx := NewIndex()\n\t// a worker receives an pack ID from ch, reads the pack contents, and adds them to idx\n\tworker := func() error {\n\t\tfor fi := range ch {\n\t\t\tentries, _, err := r.ListPack(ctx, fi.ID, fi.Size)\n\t\t\tif err != nil {\n\t\t\t\tdebug.Log(\"unable to list pack file %v\", fi.ID.Str())\n\t\t\t\tm.Lock()\n\t\t\t\tinvalid = append(invalid, fi.ID)\n\t\t\t\tm.Unlock()\n\t\t\t}\n\t\t\tidx.StorePack(fi.ID, entries)\n\t\t\tp.Add(1)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// run workers on ch\n\twg.Go(func() error {\n\t\treturn RunWorkers(listPackParallelism, worker)\n\t})\n\n\terr = wg.Wait()\n\tif err != nil {\n\t\treturn invalid, errors.Fatal(err.Error())\n\t}\n\n\t// Add idx to MasterIndex\n\tr.idx.Insert(idx)\n\n\treturn invalid, nil\n}\n\n// PrepareCache initializes the local cache. indexIDs is the list of IDs of\n// index files still present in the repo.\nfunc (r *Repository) PrepareCache(indexIDs restic.IDSet) error {\n\tif r.Cache == nil {\n\t\treturn nil\n\t}\n\n\tdebug.Log(\"prepare cache with %d index files\", len(indexIDs))\n\n\t// clear old index files\n\terr := r.Cache.Clear(restic.IndexFile, indexIDs)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"error clearing index files in cache: %v\\n\", err)\n\t}\n\n\tpacks := restic.NewIDSet()\n\tfor _, idx := range r.idx.All() {\n\t\tfor id := range idx.Packs() {\n\t\t\tpacks.Insert(id)\n\t\t}\n\t}\n\n\t// clear old packs\n\terr = r.Cache.Clear(restic.PackFile, packs)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"error clearing pack files in cache: %v\\n\", err)\n\t}\n\n\ttreePacks := restic.NewIDSet()\n\tfor _, idx := range r.idx.All() {\n\t\tfor _, id := range idx.TreePacks() {\n\t\t\ttreePacks.Insert(id)\n\t\t}\n\t}\n\n\t// use readahead\n\tdebug.Log(\"using readahead\")\n\tcache := r.Cache\n\tcache.PerformReadahead = func(h restic.Handle) bool {\n\t\tif h.Type != restic.PackFile {\n\t\t\tdebug.Log(\"no readahead for %v, is not a pack file\", h)\n\t\t\treturn false\n\t\t}\n\n\t\tid, err := restic.ParseID(h.Name)\n\t\tif err != nil {\n\t\t\tdebug.Log(\"no readahead for %v, invalid ID\", h)\n\t\t\treturn false\n\t\t}\n\n\t\tif treePacks.Has(id) {\n\t\t\tdebug.Log(\"perform readahead for %v\", h)\n\t\t\treturn true\n\t\t}\n\t\tdebug.Log(\"no readahead for %v, not tree file\", h)\n\t\treturn false\n\t}\n\n\treturn nil\n}\n\n// SearchKey finds a key with the supplied password, afterwards the config is\n// read and parsed. It tries at most maxKeys key files in the repo.\nfunc (r *Repository) SearchKey(ctx context.Context, password string, maxKeys int, keyHint string) error {\n\tkey, err := SearchKey(ctx, r, password, maxKeys, keyHint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tr.key = key.master\n\tr.dataPM.key = key.master\n\tr.treePM.key = key.master\n\tr.keyName = key.Name()\n\tr.cfg, err = restic.LoadConfig(ctx, r)\n\tif err != nil {\n\t\treturn errors.Fatalf(\"config cannot be loaded: %v\", err)\n\t}\n\treturn nil\n}\n\n// Init creates a new master key with the supplied password, initializes and\n// saves the repository config.\nfunc (r *Repository) Init(ctx context.Context, password string, chunkerPolynomial *chunker.Pol) error {\n\thas, err := r.be.Test(ctx, restic.Handle{Type: restic.ConfigFile})\n\tif err != nil {\n\t\treturn err\n\t}\n\tif has {\n\t\treturn errors.New(\"repository master key and config already initialized\")\n\t}\n\n\tcfg, err := restic.CreateConfig()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif chunkerPolynomial != nil {\n\t\tcfg.ChunkerPolynomial = *chunkerPolynomial\n\t}\n\n\treturn r.init(ctx, password, cfg)\n}\n\n// init creates a new master key with the supplied password and uses it to save\n// the config into the repo.\nfunc (r *Repository) init(ctx context.Context, password string, cfg restic.Config) error {\n\tkey, err := createMasterKey(ctx, r, password)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tr.key = key.master\n\tr.dataPM.key = key.master\n\tr.treePM.key = key.master\n\tr.keyName = key.Name()\n\tr.cfg = cfg\n\t_, err = r.SaveJSONUnpacked(ctx, restic.ConfigFile, cfg)\n\treturn err\n}\n\n// Key returns the current master key.\nfunc (r *Repository) Key() *crypto.Key {\n\treturn r.key\n}\n\n// KeyName returns the name of the current key in the backend.\nfunc (r *Repository) KeyName() string {\n\treturn r.keyName\n}\n\n// List runs fn for all files of type t in the repo.\nfunc (r *Repository) List(ctx context.Context, t restic.FileType, fn func(restic.ID, int64) error) error {\n\treturn r.be.List(ctx, t, func(fi restic.FileInfo) error {\n\t\tid, err := restic.ParseID(fi.Name)\n\t\tif err != nil {\n\t\t\tdebug.Log(\"unable to parse %v as an ID\", fi.Name)\n\t\t\treturn nil\n\t\t}\n\t\treturn fn(id, fi.Size)\n\t})\n}\n\n// ListPack returns the list of blobs saved in the pack id and the length of\n// the the pack header.\nfunc (r *Repository) ListPack(ctx context.Context, id restic.ID, size int64) ([]restic.Blob, uint32, error) {\n\th := restic.Handle{Type: restic.PackFile, Name: id.String()}\n\n\treturn pack.List(r.Key(), restic.ReaderAt(ctx, r.Backend(), h), size)\n}\n\n// Delete calls backend.Delete() if implemented, and returns an error\n// otherwise.\nfunc (r *Repository) Delete(ctx context.Context) error {\n\treturn r.be.Delete(ctx)\n}\n\n// Close closes the repository by closing the backend.\nfunc (r *Repository) Close() error {\n\treturn r.be.Close()\n}\n\n// SaveBlob saves a blob of type t into the repository.\n// It takes care that no duplicates are saved; this can be overwritten\n// by setting storeDuplicate to true.\n// If id is the null id, it will be computed and returned.\n// Also returns if the blob was already known before\nfunc (r *Repository) SaveBlob(ctx context.Context, t restic.BlobType, buf []byte, id restic.ID, storeDuplicate bool) (newID restic.ID, known bool, err error) {\n\n\t// compute plaintext hash if not already set\n\tif id.IsNull() {\n\t\tnewID = restic.Hash(buf)\n\t} else {\n\t\tnewID = id\n\t}\n\n\t// first try to add to pending blobs; if not successful, this blob is already known\n\tknown = !r.idx.addPending(restic.BlobHandle{ID: newID, Type: t})\n\n\t// only save when needed or explicitly told\n\tif !known || storeDuplicate {\n\t\terr = r.SaveAndEncrypt(ctx, t, buf, newID)\n\t}\n\n\treturn newID, known, err\n}\n\n// LoadTree loads a tree from the repository.\nfunc (r *Repository) LoadTree(ctx context.Context, id restic.ID) (*restic.Tree, error) {\n\tdebug.Log(\"load tree %v\", id)\n\n\tbuf, err := r.LoadBlob(ctx, restic.TreeBlob, id, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tt := &restic.Tree{}\n\terr = json.Unmarshal(buf, t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn t, nil\n}\n\n// SaveTree stores a tree into the repository and returns the ID. The ID is\n// checked against the index. The tree is only stored when the index does not\n// contain the ID.\nfunc (r *Repository) SaveTree(ctx context.Context, t *restic.Tree) (restic.ID, error) {\n\tbuf, err := json.Marshal(t)\n\tif err != nil {\n\t\treturn restic.ID{}, errors.Wrap(err, \"MarshalJSON\")\n\t}\n\n\t// append a newline so that the data is always consistent (json.Encoder\n\t// adds a newline after each object)\n\tbuf = append(buf, '\\n')\n\n\tid, _, err := r.SaveBlob(ctx, restic.TreeBlob, buf, restic.ID{}, false)\n\treturn id, err\n}\n\n// Loader allows loading data from a backend.\ntype Loader interface {\n\tLoad(ctx context.Context, h restic.Handle, length int, offset int64, fn func(rd io.Reader) error) error\n}\n\n// DownloadAndHash is all-in-one helper to download content of the file at h to a temporary filesystem location\n// and calculate ID of the contents. Returned (temporary) file is positioned at the beginning of the file;\n// it is the reponsibility of the caller to close and delete the file.\nfunc DownloadAndHash(ctx context.Context, be Loader, h restic.Handle) (tmpfile *os.File, hash restic.ID, size int64, err error) {\n\ttmpfile, err = fs.TempFile(\"\", \"restic-temp-\")\n\tif err != nil {\n\t\treturn nil, restic.ID{}, -1, errors.Wrap(err, \"TempFile\")\n\t}\n\n\terr = be.Load(ctx, h, 0, 0, func(rd io.Reader) (ierr error) {\n\t\t_, ierr = tmpfile.Seek(0, io.SeekStart)\n\t\tif ierr == nil {\n\t\t\tierr = tmpfile.Truncate(0)\n\t\t}\n\t\tif ierr != nil {\n\t\t\treturn ierr\n\t\t}\n\t\thrd := hashing.NewReader(rd, sha256.New())\n\t\tsize, ierr = io.Copy(tmpfile, hrd)\n\t\thash = restic.IDFromHash(hrd.Sum(nil))\n\t\treturn ierr\n\t})\n\tif err != nil {\n\t\ttmpfile.Close()\n\t\tos.Remove(tmpfile.Name())\n\t\treturn nil, restic.ID{}, -1, errors.Wrap(err, \"Load\")\n\t}\n\n\t_, err = tmpfile.Seek(0, io.SeekStart)\n\tif err != nil {\n\t\ttmpfile.Close()\n\t\tos.Remove(tmpfile.Name())\n\t\treturn nil, restic.ID{}, -1, errors.Wrap(err, \"Seek\")\n\t}\n\n\treturn tmpfile, hash, size, err\n}\n", "idx": 1, "id": 14933, "msg": "`return r.idx.MergeFinalIndexes()` would be simpler", "proj": "restic-restic", "lang": "go", "sampling_weight": 0.13389525325539522}
