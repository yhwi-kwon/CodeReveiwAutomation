{"patch": "@@ -1738,7 +1738,7 @@ class RandomCenterCropPad(object):\n \n \n @PIPELINES.register_module()\n-class CutOut(object):\n+class CutOut:\n     \"\"\"CutOut operation.\n \n     Randomly drop some regions of image used in", "y": 0, "oldf": "import copy\nimport inspect\n\nimport mmcv\nimport numpy as np\nfrom numpy import random\n\nfrom mmdet.core import PolygonMasks\nfrom mmdet.core.evaluation.bbox_overlaps import bbox_overlaps\nfrom ..builder import PIPELINES\n\ntry:\n    from imagecorruptions import corrupt\nexcept ImportError:\n    corrupt = None\n\ntry:\n    import albumentations\n    from albumentations import Compose\nexcept ImportError:\n    albumentations = None\n    Compose = None\n\n\n@PIPELINES.register_module()\nclass Resize(object):\n    \"\"\"Resize images & bbox & mask.\n\n    This transform resizes the input image to some scale. Bboxes and masks are\n    then resized with the same scale factor. If the input dict contains the key\n    \"scale\", then the scale in the input dict is used, otherwise the specified\n    scale in the init method is used. If the input dict contains the key\n    \"scale_factor\" (if MultiScaleFlipAug does not give img_scale but\n    scale_factor), the actual scale will be computed by image shape and\n    scale_factor.\n\n    `img_scale` can either be a tuple (single-scale) or a list of tuple\n    (multi-scale). There are 3 multiscale modes:\n\n    - ``ratio_range is not None``: randomly sample a ratio from the ratio \\\n      range and multiply it with the image scale.\n    - ``ratio_range is None`` and ``multiscale_mode == \"range\"``: randomly \\\n      sample a scale from the multiscale range.\n    - ``ratio_range is None`` and ``multiscale_mode == \"value\"``: randomly \\\n      sample a scale from multiple scales.\n\n    Args:\n        img_scale (tuple or list[tuple]): Images scales for resizing.\n        multiscale_mode (str): Either \"range\" or \"value\".\n        ratio_range (tuple[float]): (min_ratio, max_ratio)\n        keep_ratio (bool): Whether to keep the aspect ratio when resizing the\n            image.\n        bbox_clip_border (bool, optional): Whether clip the objects outside\n            the border of the image. Defaults to True.\n        backend (str): Image resize backend, choices are 'cv2' and 'pillow'.\n            These two backends generates slightly different results. Defaults\n            to 'cv2'.\n        override (bool, optional): Whether to override `scale` and\n            `scale_factor` so as to call resize twice. Default False. If True,\n            after the first resizing, the existed `scale` and `scale_factor`\n            will be ignored so the second resizing can be allowed.\n            This option is a work-around for multiple times of resize in DETR.\n            Defaults to False.\n    \"\"\"\n\n    def __init__(self,\n                 img_scale=None,\n                 multiscale_mode='range',\n                 ratio_range=None,\n                 keep_ratio=True,\n                 bbox_clip_border=True,\n                 backend='cv2',\n                 override=False):\n        if img_scale is None:\n            self.img_scale = None\n        else:\n            if isinstance(img_scale, list):\n                self.img_scale = img_scale\n            else:\n                self.img_scale = [img_scale]\n            assert mmcv.is_list_of(self.img_scale, tuple)\n\n        if ratio_range is not None:\n            # mode 1: given a scale and a range of image ratio\n            assert len(self.img_scale) == 1\n        else:\n            # mode 2: given multiple scales or a range of scales\n            assert multiscale_mode in ['value', 'range']\n\n        self.backend = backend\n        self.multiscale_mode = multiscale_mode\n        self.ratio_range = ratio_range\n        self.keep_ratio = keep_ratio\n        # TODO: refactor the override option in Resize\n        self.override = override\n        self.bbox_clip_border = bbox_clip_border\n\n    @staticmethod\n    def random_select(img_scales):\n        \"\"\"Randomly select an img_scale from given candidates.\n\n        Args:\n            img_scales (list[tuple]): Images scales for selection.\n\n        Returns:\n            (tuple, int): Returns a tuple ``(img_scale, scale_dix)``, \\\n                where ``img_scale`` is the selected image scale and \\\n                ``scale_idx`` is the selected index in the given candidates.\n        \"\"\"\n\n        assert mmcv.is_list_of(img_scales, tuple)\n        scale_idx = np.random.randint(len(img_scales))\n        img_scale = img_scales[scale_idx]\n        return img_scale, scale_idx\n\n    @staticmethod\n    def random_sample(img_scales):\n        \"\"\"Randomly sample an img_scale when ``multiscale_mode=='range'``.\n\n        Args:\n            img_scales (list[tuple]): Images scale range for sampling.\n                There must be two tuples in img_scales, which specify the lower\n                and uper bound of image scales.\n\n        Returns:\n            (tuple, None): Returns a tuple ``(img_scale, None)``, where \\\n                ``img_scale`` is sampled scale and None is just a placeholder \\\n                to be consistent with :func:`random_select`.\n        \"\"\"\n\n        assert mmcv.is_list_of(img_scales, tuple) and len(img_scales) == 2\n        img_scale_long = [max(s) for s in img_scales]\n        img_scale_short = [min(s) for s in img_scales]\n        long_edge = np.random.randint(\n            min(img_scale_long),\n            max(img_scale_long) + 1)\n        short_edge = np.random.randint(\n            min(img_scale_short),\n            max(img_scale_short) + 1)\n        img_scale = (long_edge, short_edge)\n        return img_scale, None\n\n    @staticmethod\n    def random_sample_ratio(img_scale, ratio_range):\n        \"\"\"Randomly sample an img_scale when ``ratio_range`` is specified.\n\n        A ratio will be randomly sampled from the range specified by\n        ``ratio_range``. Then it would be multiplied with ``img_scale`` to\n        generate sampled scale.\n\n        Args:\n            img_scale (tuple): Images scale base to multiply with ratio.\n            ratio_range (tuple[float]): The minimum and maximum ratio to scale\n                the ``img_scale``.\n\n        Returns:\n            (tuple, None): Returns a tuple ``(scale, None)``, where \\\n                ``scale`` is sampled ratio multiplied with ``img_scale`` and \\\n                None is just a placeholder to be consistent with \\\n                :func:`random_select`.\n        \"\"\"\n\n        assert isinstance(img_scale, tuple) and len(img_scale) == 2\n        min_ratio, max_ratio = ratio_range\n        assert min_ratio <= max_ratio\n        ratio = np.random.random_sample() * (max_ratio - min_ratio) + min_ratio\n        scale = int(img_scale[0] * ratio), int(img_scale[1] * ratio)\n        return scale, None\n\n    def _random_scale(self, results):\n        \"\"\"Randomly sample an img_scale according to ``ratio_range`` and\n        ``multiscale_mode``.\n\n        If ``ratio_range`` is specified, a ratio will be sampled and be\n        multiplied with ``img_scale``.\n        If multiple scales are specified by ``img_scale``, a scale will be\n        sampled according to ``multiscale_mode``.\n        Otherwise, single scale will be used.\n\n        Args:\n            results (dict): Result dict from :obj:`dataset`.\n\n        Returns:\n            dict: Two new keys 'scale` and 'scale_idx` are added into \\\n                ``results``, which would be used by subsequent pipelines.\n        \"\"\"\n\n        if self.ratio_range is not None:\n            scale, scale_idx = self.random_sample_ratio(\n                self.img_scale[0], self.ratio_range)\n        elif len(self.img_scale) == 1:\n            scale, scale_idx = self.img_scale[0], 0\n        elif self.multiscale_mode == 'range':\n            scale, scale_idx = self.random_sample(self.img_scale)\n        elif self.multiscale_mode == 'value':\n            scale, scale_idx = self.random_select(self.img_scale)\n        else:\n            raise NotImplementedError\n\n        results['scale'] = scale\n        results['scale_idx'] = scale_idx\n\n    def _resize_img(self, results):\n        \"\"\"Resize images with ``results['scale']``.\"\"\"\n        for key in results.get('img_fields', ['img']):\n            if self.keep_ratio:\n                img, scale_factor = mmcv.imrescale(\n                    results[key],\n                    results['scale'],\n                    return_scale=True,\n                    backend=self.backend)\n                # the w_scale and h_scale has minor difference\n                # a real fix should be done in the mmcv.imrescale in the future\n                new_h, new_w = img.shape[:2]\n                h, w = results[key].shape[:2]\n                w_scale = new_w / w\n                h_scale = new_h / h\n            else:\n                img, w_scale, h_scale = mmcv.imresize(\n                    results[key],\n                    results['scale'],\n                    return_scale=True,\n                    backend=self.backend)\n            results[key] = img\n\n            scale_factor = np.array([w_scale, h_scale, w_scale, h_scale],\n                                    dtype=np.float32)\n            results['img_shape'] = img.shape\n            # in case that there is no padding\n            results['pad_shape'] = img.shape\n            results['scale_factor'] = scale_factor\n            results['keep_ratio'] = self.keep_ratio\n\n    def _resize_bboxes(self, results):\n        \"\"\"Resize bounding boxes with ``results['scale_factor']``.\"\"\"\n        for key in results.get('bbox_fields', []):\n            bboxes = results[key] * results['scale_factor']\n            if self.bbox_clip_border:\n                img_shape = results['img_shape']\n                bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[1])\n                bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[0])\n            results[key] = bboxes\n\n    def _resize_masks(self, results):\n        \"\"\"Resize masks with ``results['scale']``\"\"\"\n        for key in results.get('mask_fields', []):\n            if results[key] is None:\n                continue\n            if self.keep_ratio:\n                results[key] = results[key].rescale(results['scale'])\n            else:\n                results[key] = results[key].resize(results['img_shape'][:2])\n\n    def _resize_seg(self, results):\n        \"\"\"Resize semantic segmentation map with ``results['scale']``.\"\"\"\n        for key in results.get('seg_fields', []):\n            if self.keep_ratio:\n                gt_seg = mmcv.imrescale(\n                    results[key],\n                    results['scale'],\n                    interpolation='nearest',\n                    backend=self.backend)\n            else:\n                gt_seg = mmcv.imresize(\n                    results[key],\n                    results['scale'],\n                    interpolation='nearest',\n                    backend=self.backend)\n            results['gt_semantic_seg'] = gt_seg\n\n    def __call__(self, results):\n        \"\"\"Call function to resize images, bounding boxes, masks, semantic\n        segmentation map.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Resized results, 'img_shape', 'pad_shape', 'scale_factor', \\\n                'keep_ratio' keys are added into result dict.\n        \"\"\"\n\n        if 'scale' not in results:\n            if 'scale_factor' in results:\n                img_shape = results['img'].shape[:2]\n                scale_factor = results['scale_factor']\n                assert isinstance(scale_factor, float)\n                results['scale'] = tuple(\n                    [int(x * scale_factor) for x in img_shape][::-1])\n            else:\n                self._random_scale(results)\n        else:\n            if not self.override:\n                assert 'scale_factor' not in results, (\n                    'scale and scale_factor cannot be both set.')\n            else:\n                results.pop('scale')\n                if 'scale_factor' in results:\n                    results.pop('scale_factor')\n                self._random_scale(results)\n\n        self._resize_img(results)\n        self._resize_bboxes(results)\n        self._resize_masks(results)\n        self._resize_seg(results)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(img_scale={self.img_scale}, '\n        repr_str += f'multiscale_mode={self.multiscale_mode}, '\n        repr_str += f'ratio_range={self.ratio_range}, '\n        repr_str += f'keep_ratio={self.keep_ratio}, '\n        repr_str += f'bbox_clip_border={self.bbox_clip_border})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomFlip(object):\n    \"\"\"Flip the image & bbox & mask.\n\n    If the input dict contains the key \"flip\", then the flag will be used,\n    otherwise it will be randomly decided by a ratio specified in the init\n    method.\n\n    When random flip is enabled, ``flip_ratio``/``direction`` can either be a\n    float/string or tuple of float/string. There are 3 flip modes:\n\n    - ``flip_ratio`` is float, ``direction`` is string: the image will be\n        ``direction``ly flipped with probability of ``flip_ratio`` .\n        E.g., ``flip_ratio=0.5``, ``direction='horizontal'``,\n        then image will be horizontally flipped with probability of 0.5.\n    - ``flip_ratio`` is float, ``direction`` is list of string: the image wil\n        be ``direction[i]``ly flipped with probability of\n        ``flip_ratio/len(direction)``.\n        E.g., ``flip_ratio=0.5``, ``direction=['horizontal', 'vertical']``,\n        then image will be horizontally flipped with probability of 0.25,\n        vertically with probability of 0.25.\n    - ``flip_ratio`` is list of float, ``direction`` is list of string:\n        given ``len(flip_ratio) == len(direction)``, the image wil\n        be ``direction[i]``ly flipped with probability of ``flip_ratio[i]``.\n        E.g., ``flip_ratio=[0.3, 0.5]``, ``direction=['horizontal',\n        'vertical']``, then image will be horizontally flipped with probability\n         of 0.3, vertically with probability of 0.5\n\n    Args:\n        flip_ratio (float | list[float], optional): The flipping probability.\n            Default: None.\n        direction(str | list[str], optional): The flipping direction. Options\n            are 'horizontal', 'vertical', 'diagonal'. Default: 'horizontal'.\n            If input is a list, the length must equal ``flip_ratio``. Each\n            element in ``flip_ratio`` indicates the flip probability of\n            corresponding direction.\n    \"\"\"\n\n    def __init__(self, flip_ratio=None, direction='horizontal'):\n        if isinstance(flip_ratio, list):\n            assert mmcv.is_list_of(flip_ratio, float)\n            assert 0 <= sum(flip_ratio) <= 1\n        elif isinstance(flip_ratio, float):\n            assert 0 <= flip_ratio <= 1\n        elif flip_ratio is None:\n            pass\n        else:\n            raise ValueError('flip_ratios must be None, float, '\n                             'or list of float')\n        self.flip_ratio = flip_ratio\n\n        valid_directions = ['horizontal', 'vertical', 'diagonal']\n        if isinstance(direction, str):\n            assert direction in valid_directions\n        elif isinstance(direction, list):\n            assert mmcv.is_list_of(direction, str)\n            assert set(direction).issubset(set(valid_directions))\n        else:\n            raise ValueError('direction must be either str or list of str')\n        self.direction = direction\n\n        if isinstance(flip_ratio, list):\n            assert len(self.flip_ratio) == len(self.direction)\n\n    def bbox_flip(self, bboxes, img_shape, direction):\n        \"\"\"Flip bboxes horizontally.\n\n        Args:\n            bboxes (numpy.ndarray): Bounding boxes, shape (..., 4*k)\n            img_shape (tuple[int]): Image shape (height, width)\n            direction (str): Flip direction. Options are 'horizontal',\n                'vertical'.\n\n        Returns:\n            numpy.ndarray: Flipped bounding boxes.\n        \"\"\"\n\n        assert bboxes.shape[-1] % 4 == 0\n        flipped = bboxes.copy()\n        if direction == 'horizontal':\n            w = img_shape[1]\n            flipped[..., 0::4] = w - bboxes[..., 2::4]\n            flipped[..., 2::4] = w - bboxes[..., 0::4]\n        elif direction == 'vertical':\n            h = img_shape[0]\n            flipped[..., 1::4] = h - bboxes[..., 3::4]\n            flipped[..., 3::4] = h - bboxes[..., 1::4]\n        elif direction == 'diagonal':\n            w = img_shape[1]\n            h = img_shape[0]\n            flipped[..., 0::4] = w - bboxes[..., 2::4]\n            flipped[..., 1::4] = h - bboxes[..., 3::4]\n            flipped[..., 2::4] = w - bboxes[..., 0::4]\n            flipped[..., 3::4] = h - bboxes[..., 1::4]\n        else:\n            raise ValueError(f\"Invalid flipping direction '{direction}'\")\n        return flipped\n\n    def __call__(self, results):\n        \"\"\"Call function to flip bounding boxes, masks, semantic segmentation\n        maps.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Flipped results, 'flip', 'flip_direction' keys are added \\\n                into result dict.\n        \"\"\"\n\n        if 'flip' not in results:\n            if isinstance(self.direction, list):\n                # None means non-flip\n                direction_list = self.direction + [None]\n            else:\n                # None means non-flip\n                direction_list = [self.direction, None]\n\n            if isinstance(self.flip_ratio, list):\n                non_flip_ratio = 1 - sum(self.flip_ratio)\n                flip_ratio_list = self.flip_ratio + [non_flip_ratio]\n            else:\n                non_flip_ratio = 1 - self.flip_ratio\n                # exclude non-flip\n                single_ratio = self.flip_ratio / (len(direction_list) - 1)\n                flip_ratio_list = [single_ratio] * (len(direction_list) -\n                                                    1) + [non_flip_ratio]\n\n            cur_dir = np.random.choice(direction_list, p=flip_ratio_list)\n\n            results['flip'] = cur_dir is not None\n        if 'flip_direction' not in results:\n            results['flip_direction'] = cur_dir\n        if results['flip']:\n            # flip image\n            for key in results.get('img_fields', ['img']):\n                results[key] = mmcv.imflip(\n                    results[key], direction=results['flip_direction'])\n            # flip bboxes\n            for key in results.get('bbox_fields', []):\n                results[key] = self.bbox_flip(results[key],\n                                              results['img_shape'],\n                                              results['flip_direction'])\n            # flip masks\n            for key in results.get('mask_fields', []):\n                results[key] = results[key].flip(results['flip_direction'])\n\n            # flip segs\n            for key in results.get('seg_fields', []):\n                results[key] = mmcv.imflip(\n                    results[key], direction=results['flip_direction'])\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f'(flip_ratio={self.flip_ratio})'\n\n\n@PIPELINES.register_module()\nclass Pad(object):\n    \"\"\"Pad the image & mask.\n\n    There are two padding modes: (1) pad to a fixed size and (2) pad to the\n    minimum size that is divisible by some number.\n    Added keys are \"pad_shape\", \"pad_fixed_size\", \"pad_size_divisor\",\n\n    Args:\n        size (tuple, optional): Fixed padding size.\n        size_divisor (int, optional): The divisor of padded size.\n        pad_val (float, optional): Padding value, 0 by default.\n    \"\"\"\n\n    def __init__(self, size=None, size_divisor=None, pad_val=0):\n        self.size = size\n        self.size_divisor = size_divisor\n        self.pad_val = pad_val\n        # only one of size and size_divisor should be valid\n        assert size is not None or size_divisor is not None\n        assert size is None or size_divisor is None\n\n    def _pad_img(self, results):\n        \"\"\"Pad images according to ``self.size``.\"\"\"\n        for key in results.get('img_fields', ['img']):\n            if self.size is not None:\n                padded_img = mmcv.impad(\n                    results[key], shape=self.size, pad_val=self.pad_val)\n            elif self.size_divisor is not None:\n                padded_img = mmcv.impad_to_multiple(\n                    results[key], self.size_divisor, pad_val=self.pad_val)\n            results[key] = padded_img\n        results['pad_shape'] = padded_img.shape\n        results['pad_fixed_size'] = self.size\n        results['pad_size_divisor'] = self.size_divisor\n\n    def _pad_masks(self, results):\n        \"\"\"Pad masks according to ``results['pad_shape']``.\"\"\"\n        pad_shape = results['pad_shape'][:2]\n        for key in results.get('mask_fields', []):\n            results[key] = results[key].pad(pad_shape, pad_val=self.pad_val)\n\n    def _pad_seg(self, results):\n        \"\"\"Pad semantic segmentation map according to\n        ``results['pad_shape']``.\"\"\"\n        for key in results.get('seg_fields', []):\n            results[key] = mmcv.impad(\n                results[key], shape=results['pad_shape'][:2])\n\n    def __call__(self, results):\n        \"\"\"Call function to pad images, masks, semantic segmentation maps.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Updated result dict.\n        \"\"\"\n        self._pad_img(results)\n        self._pad_masks(results)\n        self._pad_seg(results)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(size={self.size}, '\n        repr_str += f'size_divisor={self.size_divisor}, '\n        repr_str += f'pad_val={self.pad_val})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Normalize(object):\n    \"\"\"Normalize the image.\n\n    Added key is \"img_norm_cfg\".\n\n    Args:\n        mean (sequence): Mean values of 3 channels.\n        std (sequence): Std values of 3 channels.\n        to_rgb (bool): Whether to convert the image from BGR to RGB,\n            default is true.\n    \"\"\"\n\n    def __init__(self, mean, std, to_rgb=True):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n\n    def __call__(self, results):\n        \"\"\"Call function to normalize images.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Normalized results, 'img_norm_cfg' key is added into\n                result dict.\n        \"\"\"\n        for key in results.get('img_fields', ['img']):\n            results[key] = mmcv.imnormalize(results[key], self.mean, self.std,\n                                            self.to_rgb)\n        results['img_norm_cfg'] = dict(\n            mean=self.mean, std=self.std, to_rgb=self.to_rgb)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(mean={self.mean}, std={self.std}, to_rgb={self.to_rgb})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomCrop(object):\n    \"\"\"Random crop the image & bboxes & masks.\n\n    The absolute `crop_size` is sampled based on `crop_type` and `image_size`,\n    then the cropped results are generated.\n\n    Args:\n        crop_size (tuple): The relative ratio or absolute pixels of\n            height and width.\n        crop_type (str, optional): one of \"relative_range\", \"relative\",\n            \"absolute\", \"absolute_range\". \"relative\" randomly crops\n            (h * crop_size[0], w * crop_size[1]) part from an input of size\n            (h, w). \"relative_range\" uniformly samples relative crop size from\n            range [crop_size[0], 1] and [crop_size[1], 1] for height and width\n            respectively. \"absolute\" crops from an input with absolute size\n            (crop_size[0], crop_size[1]). \"absolute_range\" uniformly samples\n            crop_h in range [crop_size[0], min(h, crop_size[1])] and crop_w\n            in range [crop_size[0], min(w, crop_size[1])]. Default \"absolute\".\n        allow_negative_crop (bool, optional): Whether to allow a crop that does\n            not contain any bbox area. Default False.\n        bbox_clip_border (bool, optional): Whether clip the objects outside\n            the border of the image. Defaults to True.\n\n    Note:\n        - If the image is smaller than the absolute crop size, return the\n            original image.\n        - The keys for bboxes, labels and masks must be aligned. That is,\n          `gt_bboxes` corresponds to `gt_labels` and `gt_masks`, and\n          `gt_bboxes_ignore` corresponds to `gt_labels_ignore` and\n          `gt_masks_ignore`.\n        - If the crop does not contain any gt-bbox region and\n          `allow_negative_crop` is set to False, skip this image.\n    \"\"\"\n\n    def __init__(self,\n                 crop_size,\n                 crop_type='absolute',\n                 allow_negative_crop=False,\n                 bbox_clip_border=True):\n        if crop_type not in [\n                'relative_range', 'relative', 'absolute', 'absolute_range'\n        ]:\n            raise ValueError(f'Invalid crop_type {crop_type}.')\n        if crop_type in ['absolute', 'absolute_range']:\n            assert crop_size[0] > 0 and crop_size[1] > 0\n            assert isinstance(crop_size[0], int) and isinstance(\n                crop_size[1], int)\n        else:\n            assert 0 < crop_size[0] <= 1 and 0 < crop_size[1] <= 1\n        self.crop_size = crop_size\n        self.crop_type = crop_type\n        self.allow_negative_crop = allow_negative_crop\n        self.bbox_clip_border = bbox_clip_border\n        # The key correspondence from bboxes to labels and masks.\n        self.bbox2label = {\n            'gt_bboxes': 'gt_labels',\n            'gt_bboxes_ignore': 'gt_labels_ignore'\n        }\n        self.bbox2mask = {\n            'gt_bboxes': 'gt_masks',\n            'gt_bboxes_ignore': 'gt_masks_ignore'\n        }\n\n    def _crop_data(self, results, crop_size, allow_negative_crop):\n        \"\"\"Function to randomly crop images, bounding boxes, masks, semantic\n        segmentation maps.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n            crop_size (tuple): Expected absolute size after cropping, (h, w).\n            allow_negative_crop (bool): Whether to allow a crop that does not\n                contain any bbox area. Default to False.\n\n        Returns:\n            dict: Randomly cropped results, 'img_shape' key in result dict is\n                updated according to crop size.\n        \"\"\"\n        assert crop_size[0] > 0 and crop_size[1] > 0\n        for key in results.get('img_fields', ['img']):\n            img = results[key]\n            margin_h = max(img.shape[0] - crop_size[0], 0)\n            margin_w = max(img.shape[1] - crop_size[1], 0)\n            offset_h = np.random.randint(0, margin_h + 1)\n            offset_w = np.random.randint(0, margin_w + 1)\n            crop_y1, crop_y2 = offset_h, offset_h + crop_size[0]\n            crop_x1, crop_x2 = offset_w, offset_w + crop_size[1]\n\n            # crop the image\n            img = img[crop_y1:crop_y2, crop_x1:crop_x2, ...]\n            img_shape = img.shape\n            results[key] = img\n        results['img_shape'] = img_shape\n\n        # crop bboxes accordingly and clip to the image boundary\n        for key in results.get('bbox_fields', []):\n            # e.g. gt_bboxes and gt_bboxes_ignore\n            bbox_offset = np.array([offset_w, offset_h, offset_w, offset_h],\n                                   dtype=np.float32)\n            bboxes = results[key] - bbox_offset\n            if self.bbox_clip_border:\n                bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[1])\n                bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[0])\n            valid_inds = (bboxes[:, 2] > bboxes[:, 0]) & (\n                bboxes[:, 3] > bboxes[:, 1])\n            # If the crop does not contain any gt-bbox area and\n            # allow_negative_crop is False, skip this image.\n            if (key == 'gt_bboxes' and not valid_inds.any()\n                    and not allow_negative_crop):\n                return None\n            results[key] = bboxes[valid_inds, :]\n            # label fields. e.g. gt_labels and gt_labels_ignore\n            label_key = self.bbox2label.get(key)\n            if label_key in results:\n                results[label_key] = results[label_key][valid_inds]\n\n            # mask fields, e.g. gt_masks and gt_masks_ignore\n            mask_key = self.bbox2mask.get(key)\n            if mask_key in results:\n                results[mask_key] = results[mask_key][\n                    valid_inds.nonzero()[0]].crop(\n                        np.asarray([crop_x1, crop_y1, crop_x2, crop_y2]))\n\n        # crop semantic seg\n        for key in results.get('seg_fields', []):\n            results[key] = results[key][crop_y1:crop_y2, crop_x1:crop_x2]\n\n        return results\n\n    def _get_crop_size(self, image_size):\n        \"\"\"Randomly generates the absolute crop size based on `crop_type` and\n        `image_size`.\n\n        Args:\n            image_size (tuple): (h, w).\n\n        Returns:\n            crop_size (tuple): (crop_h, crop_w) in absolute pixels.\n        \"\"\"\n        h, w = image_size\n        if self.crop_type == 'absolute':\n            return (min(self.crop_size[0], h), min(self.crop_size[1], w))\n        elif self.crop_type == 'absolute_range':\n            assert self.crop_size[0] <= self.crop_size[1]\n            crop_h = np.random.randint(\n                min(h, self.crop_size[0]),\n                min(h, self.crop_size[1]) + 1)\n            crop_w = np.random.randint(\n                min(w, self.crop_size[0]),\n                min(w, self.crop_size[1]) + 1)\n            return crop_h, crop_w\n        elif self.crop_type == 'relative':\n            crop_h, crop_w = self.crop_size\n            return int(h * crop_h + 0.5), int(w * crop_w + 0.5)\n        elif self.crop_type == 'relative_range':\n            crop_size = np.asarray(self.crop_size, dtype=np.float32)\n            crop_h, crop_w = crop_size + np.random.rand(2) * (1 - crop_size)\n            return int(h * crop_h + 0.5), int(w * crop_w + 0.5)\n\n    def __call__(self, results):\n        \"\"\"Call function to randomly crop images, bounding boxes, masks,\n        semantic segmentation maps.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Randomly cropped results, 'img_shape' key in result dict is\n                updated according to crop size.\n        \"\"\"\n        image_size = results['img'].shape[:2]\n        crop_size = self._get_crop_size(image_size)\n        results = self._crop_data(results, crop_size, self.allow_negative_crop)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(crop_size={self.crop_size}, '\n        repr_str += f'crop_type={self.crop_type}, '\n        repr_str += f'allow_negative_crop={self.allow_negative_crop}, '\n        repr_str += f'bbox_clip_border={self.bbox_clip_border})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass SegRescale(object):\n    \"\"\"Rescale semantic segmentation maps.\n\n    Args:\n        scale_factor (float): The scale factor of the final output.\n        backend (str): Image rescale backend, choices are 'cv2' and 'pillow'.\n            These two backends generates slightly different results. Defaults\n            to 'cv2'.\n    \"\"\"\n\n    def __init__(self, scale_factor=1, backend='cv2'):\n        self.scale_factor = scale_factor\n        self.backend = backend\n\n    def __call__(self, results):\n        \"\"\"Call function to scale the semantic segmentation map.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Result dict with semantic segmentation map scaled.\n        \"\"\"\n\n        for key in results.get('seg_fields', []):\n            if self.scale_factor != 1:\n                results[key] = mmcv.imrescale(\n                    results[key],\n                    self.scale_factor,\n                    interpolation='nearest',\n                    backend=self.backend)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f'(scale_factor={self.scale_factor})'\n\n\n@PIPELINES.register_module()\nclass PhotoMetricDistortion(object):\n    \"\"\"Apply photometric distortion to image sequentially, every transformation\n    is applied with a probability of 0.5. The position of random contrast is in\n    second or second to last.\n\n    1. random brightness\n    2. random contrast (mode 0)\n    3. convert color from BGR to HSV\n    4. random saturation\n    5. random hue\n    6. convert color from HSV to BGR\n    7. random contrast (mode 1)\n    8. randomly swap channels\n\n    Args:\n        brightness_delta (int): delta of brightness.\n        contrast_range (tuple): range of contrast.\n        saturation_range (tuple): range of saturation.\n        hue_delta (int): delta of hue.\n    \"\"\"\n\n    def __init__(self,\n                 brightness_delta=32,\n                 contrast_range=(0.5, 1.5),\n                 saturation_range=(0.5, 1.5),\n                 hue_delta=18):\n        self.brightness_delta = brightness_delta\n        self.contrast_lower, self.contrast_upper = contrast_range\n        self.saturation_lower, self.saturation_upper = saturation_range\n        self.hue_delta = hue_delta\n\n    def __call__(self, results):\n        \"\"\"Call function to perform photometric distortion on images.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Result dict with images distorted.\n        \"\"\"\n\n        if 'img_fields' in results:\n            assert results['img_fields'] == ['img'], \\\n                'Only single img_fields is allowed'\n        img = results['img']\n        assert img.dtype == np.float32, \\\n            'PhotoMetricDistortion needs the input image of dtype np.float32,'\\\n            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline'\n        # random brightness\n        if random.randint(2):\n            delta = random.uniform(-self.brightness_delta,\n                                   self.brightness_delta)\n            img += delta\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = random.randint(2)\n        if mode == 1:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # convert color from BGR to HSV\n        img = mmcv.bgr2hsv(img)\n\n        # random saturation\n        if random.randint(2):\n            img[..., 1] *= random.uniform(self.saturation_lower,\n                                          self.saturation_upper)\n\n        # random hue\n        if random.randint(2):\n            img[..., 0] += random.uniform(-self.hue_delta, self.hue_delta)\n            img[..., 0][img[..., 0] > 360] -= 360\n            img[..., 0][img[..., 0] < 0] += 360\n\n        # convert color from HSV to BGR\n        img = mmcv.hsv2bgr(img)\n\n        # random contrast\n        if mode == 0:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # randomly swap channels\n        if random.randint(2):\n            img = img[..., random.permutation(3)]\n\n        results['img'] = img\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(\\nbrightness_delta={self.brightness_delta},\\n'\n        repr_str += 'contrast_range='\n        repr_str += f'{(self.contrast_lower, self.contrast_upper)},\\n'\n        repr_str += 'saturation_range='\n        repr_str += f'{(self.saturation_lower, self.saturation_upper)},\\n'\n        repr_str += f'hue_delta={self.hue_delta})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Expand(object):\n    \"\"\"Random expand the image & bboxes.\n\n    Randomly place the original image on a canvas of 'ratio' x original image\n    size filled with mean values. The ratio is in the range of ratio_range.\n\n    Args:\n        mean (tuple): mean value of dataset.\n        to_rgb (bool): if need to convert the order of mean to align with RGB.\n        ratio_range (tuple): range of expand ratio.\n        prob (float): probability of applying this transformation\n    \"\"\"\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 to_rgb=True,\n                 ratio_range=(1, 4),\n                 seg_ignore_label=None,\n                 prob=0.5):\n        self.to_rgb = to_rgb\n        self.ratio_range = ratio_range\n        if to_rgb:\n            self.mean = mean[::-1]\n        else:\n            self.mean = mean\n        self.min_ratio, self.max_ratio = ratio_range\n        self.seg_ignore_label = seg_ignore_label\n        self.prob = prob\n\n    def __call__(self, results):\n        \"\"\"Call function to expand images, bounding boxes.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Result dict with images, bounding boxes expanded\n        \"\"\"\n\n        if random.uniform(0, 1) > self.prob:\n            return results\n\n        if 'img_fields' in results:\n            assert results['img_fields'] == ['img'], \\\n                'Only single img_fields is allowed'\n        img = results['img']\n\n        h, w, c = img.shape\n        ratio = random.uniform(self.min_ratio, self.max_ratio)\n        # speedup expand when meets large image\n        if np.all(self.mean == self.mean[0]):\n            expand_img = np.empty((int(h * ratio), int(w * ratio), c),\n                                  img.dtype)\n            expand_img.fill(self.mean[0])\n        else:\n            expand_img = np.full((int(h * ratio), int(w * ratio), c),\n                                 self.mean,\n                                 dtype=img.dtype)\n        left = int(random.uniform(0, w * ratio - w))\n        top = int(random.uniform(0, h * ratio - h))\n        expand_img[top:top + h, left:left + w] = img\n\n        results['img'] = expand_img\n        # expand bboxes\n        for key in results.get('bbox_fields', []):\n            results[key] = results[key] + np.tile(\n                (left, top), 2).astype(results[key].dtype)\n\n        # expand masks\n        for key in results.get('mask_fields', []):\n            results[key] = results[key].expand(\n                int(h * ratio), int(w * ratio), top, left)\n\n        # expand segs\n        for key in results.get('seg_fields', []):\n            gt_seg = results[key]\n            expand_gt_seg = np.full((int(h * ratio), int(w * ratio)),\n                                    self.seg_ignore_label,\n                                    dtype=gt_seg.dtype)\n            expand_gt_seg[top:top + h, left:left + w] = gt_seg\n            results[key] = expand_gt_seg\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(mean={self.mean}, to_rgb={self.to_rgb}, '\n        repr_str += f'ratio_range={self.ratio_range}, '\n        repr_str += f'seg_ignore_label={self.seg_ignore_label})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass MinIoURandomCrop(object):\n    \"\"\"Random crop the image & bboxes, the cropped patches have minimum IoU\n    requirement with original image & bboxes, the IoU threshold is randomly\n    selected from min_ious.\n\n    Args:\n        min_ious (tuple): minimum IoU threshold for all intersections with\n        bounding boxes\n        min_crop_size (float): minimum crop's size (i.e. h,w := a*h, a*w,\n        where a >= min_crop_size).\n        bbox_clip_border (bool, optional): Whether clip the objects outside\n            the border of the image. Defaults to True.\n\n    Note:\n        The keys for bboxes, labels and masks should be paired. That is, \\\n        `gt_bboxes` corresponds to `gt_labels` and `gt_masks`, and \\\n        `gt_bboxes_ignore` to `gt_labels_ignore` and `gt_masks_ignore`.\n    \"\"\"\n\n    def __init__(self,\n                 min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n                 min_crop_size=0.3,\n                 bbox_clip_border=True):\n        # 1: return ori img\n        self.min_ious = min_ious\n        self.sample_mode = (1, *min_ious, 0)\n        self.min_crop_size = min_crop_size\n        self.bbox_clip_border = bbox_clip_border\n        self.bbox2label = {\n            'gt_bboxes': 'gt_labels',\n            'gt_bboxes_ignore': 'gt_labels_ignore'\n        }\n        self.bbox2mask = {\n            'gt_bboxes': 'gt_masks',\n            'gt_bboxes_ignore': 'gt_masks_ignore'\n        }\n\n    def __call__(self, results):\n        \"\"\"Call function to crop images and bounding boxes with minimum IoU\n        constraint.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Result dict with images and bounding boxes cropped, \\\n                'img_shape' key is updated.\n        \"\"\"\n\n        if 'img_fields' in results:\n            assert results['img_fields'] == ['img'], \\\n                'Only single img_fields is allowed'\n        img = results['img']\n        assert 'bbox_fields' in results\n        boxes = [results[key] for key in results['bbox_fields']]\n        boxes = np.concatenate(boxes, 0)\n        h, w, c = img.shape\n        while True:\n            mode = random.choice(self.sample_mode)\n            self.mode = mode\n            if mode == 1:\n                return results\n\n            min_iou = mode\n            for i in range(50):\n                new_w = random.uniform(self.min_crop_size * w, w)\n                new_h = random.uniform(self.min_crop_size * h, h)\n\n                # h / w in [0.5, 2]\n                if new_h / new_w < 0.5 or new_h / new_w > 2:\n                    continue\n\n                left = random.uniform(w - new_w)\n                top = random.uniform(h - new_h)\n\n                patch = np.array(\n                    (int(left), int(top), int(left + new_w), int(top + new_h)))\n                # Line or point crop is not allowed\n                if patch[2] == patch[0] or patch[3] == patch[1]:\n                    continue\n                overlaps = bbox_overlaps(\n                    patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n                if len(overlaps) > 0 and overlaps.min() < min_iou:\n                    continue\n\n                # center of boxes should inside the crop img\n                # only adjust boxes and instance masks when the gt is not empty\n                if len(overlaps) > 0:\n                    # adjust boxes\n                    def is_center_of_bboxes_in_patch(boxes, patch):\n                        center = (boxes[:, :2] + boxes[:, 2:]) / 2\n                        mask = ((center[:, 0] > patch[0]) *\n                                (center[:, 1] > patch[1]) *\n                                (center[:, 0] < patch[2]) *\n                                (center[:, 1] < patch[3]))\n                        return mask\n\n                    mask = is_center_of_bboxes_in_patch(boxes, patch)\n                    if not mask.any():\n                        continue\n                    for key in results.get('bbox_fields', []):\n                        boxes = results[key].copy()\n                        mask = is_center_of_bboxes_in_patch(boxes, patch)\n                        boxes = boxes[mask]\n                        if self.bbox_clip_border:\n                            boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n                            boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n                        boxes -= np.tile(patch[:2], 2)\n\n                        results[key] = boxes\n                        # labels\n                        label_key = self.bbox2label.get(key)\n                        if label_key in results:\n                            results[label_key] = results[label_key][mask]\n\n                        # mask fields\n                        mask_key = self.bbox2mask.get(key)\n                        if mask_key in results:\n                            results[mask_key] = results[mask_key][\n                                mask.nonzero()[0]].crop(patch)\n                # adjust the img no matter whether the gt is empty before crop\n                img = img[patch[1]:patch[3], patch[0]:patch[2]]\n                results['img'] = img\n                results['img_shape'] = img.shape\n\n                # seg fields\n                for key in results.get('seg_fields', []):\n                    results[key] = results[key][patch[1]:patch[3],\n                                                patch[0]:patch[2]]\n                return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(min_ious={self.min_ious}, '\n        repr_str += f'min_crop_size={self.min_crop_size}, '\n        repr_str += f'bbox_clip_border={self.bbox_clip_border})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Corrupt(object):\n    \"\"\"Corruption augmentation.\n\n    Corruption transforms implemented based on\n    `imagecorruptions <https://github.com/bethgelab/imagecorruptions>`_.\n\n    Args:\n        corruption (str): Corruption name.\n        severity (int, optional): The severity of corruption. Default: 1.\n    \"\"\"\n\n    def __init__(self, corruption, severity=1):\n        self.corruption = corruption\n        self.severity = severity\n\n    def __call__(self, results):\n        \"\"\"Call function to corrupt image.\n\n        Args:\n            results (dict): Result dict from loading pipeline.\n\n        Returns:\n            dict: Result dict with images corrupted.\n        \"\"\"\n\n        if corrupt is None:\n            raise RuntimeError('imagecorruptions is not installed')\n        if 'img_fields' in results:\n            assert results['img_fields'] == ['img'], \\\n                'Only single img_fields is allowed'\n        results['img'] = corrupt(\n            results['img'].astype(np.uint8),\n            corruption_name=self.corruption,\n            severity=self.severity)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(corruption={self.corruption}, '\n        repr_str += f'severity={self.severity})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Albu(object):\n    \"\"\"Albumentation augmentation.\n\n    Adds custom transformations from Albumentations library.\n    Please, visit `https://albumentations.readthedocs.io`\n    to get more information.\n\n    An example of ``transforms`` is as followed:\n\n    .. code-block::\n\n        [\n            dict(\n                type='ShiftScaleRotate',\n                shift_limit=0.0625,\n                scale_limit=0.0,\n                rotate_limit=0,\n                interpolation=1,\n                p=0.5),\n            dict(\n                type='RandomBrightnessContrast',\n                brightness_limit=[0.1, 0.3],\n                contrast_limit=[0.1, 0.3],\n                p=0.2),\n            dict(type='ChannelShuffle', p=0.1),\n            dict(\n                type='OneOf',\n                transforms=[\n                    dict(type='Blur', blur_limit=3, p=1.0),\n                    dict(type='MedianBlur', blur_limit=3, p=1.0)\n                ],\n                p=0.1),\n        ]\n\n    Args:\n        transforms (list[dict]): A list of albu transformations\n        bbox_params (dict): Bbox_params for albumentation `Compose`\n        keymap (dict): Contains {'input key':'albumentation-style key'}\n        skip_img_without_anno (bool): Whether to skip the image if no ann left\n            after aug\n    \"\"\"\n\n    def __init__(self,\n                 transforms,\n                 bbox_params=None,\n                 keymap=None,\n                 update_pad_shape=False,\n                 skip_img_without_anno=False):\n        if Compose is None:\n            raise RuntimeError('albumentations is not installed')\n\n        # Args will be modified later, copying it will be safer\n        transforms = copy.deepcopy(transforms)\n        if bbox_params is not None:\n            bbox_params = copy.deepcopy(bbox_params)\n        if keymap is not None:\n            keymap = copy.deepcopy(keymap)\n        self.transforms = transforms\n        self.filter_lost_elements = False\n        self.update_pad_shape = update_pad_shape\n        self.skip_img_without_anno = skip_img_without_anno\n\n        # A simple workaround to remove masks without boxes\n        if (isinstance(bbox_params, dict) and 'label_fields' in bbox_params\n                and 'filter_lost_elements' in bbox_params):\n            self.filter_lost_elements = True\n            self.origin_label_fields = bbox_params['label_fields']\n            bbox_params['label_fields'] = ['idx_mapper']\n            del bbox_params['filter_lost_elements']\n\n        self.bbox_params = (\n            self.albu_builder(bbox_params) if bbox_params else None)\n        self.aug = Compose([self.albu_builder(t) for t in self.transforms],\n                           bbox_params=self.bbox_params)\n\n        if not keymap:\n            self.keymap_to_albu = {\n                'img': 'image',\n                'gt_masks': 'masks',\n                'gt_bboxes': 'bboxes'\n            }\n        else:\n            self.keymap_to_albu = keymap\n        self.keymap_back = {v: k for k, v in self.keymap_to_albu.items()}\n\n    def albu_builder(self, cfg):\n        \"\"\"Import a module from albumentations.\n\n        It inherits some of :func:`build_from_cfg` logic.\n\n        Args:\n            cfg (dict): Config dict. It should at least contain the key \"type\".\n\n        Returns:\n            obj: The constructed object.\n        \"\"\"\n\n        assert isinstance(cfg, dict) and 'type' in cfg\n        args = cfg.copy()\n\n        obj_type = args.pop('type')\n        if mmcv.is_str(obj_type):\n            if albumentations is None:\n                raise RuntimeError('albumentations is not installed')\n            obj_cls = getattr(albumentations, obj_type)\n        elif inspect.isclass(obj_type):\n            obj_cls = obj_type\n        else:\n            raise TypeError(\n                f'type must be a str or valid type, but got {type(obj_type)}')\n\n        if 'transforms' in args:\n            args['transforms'] = [\n                self.albu_builder(transform)\n                for transform in args['transforms']\n            ]\n\n        return obj_cls(**args)\n\n    @staticmethod\n    def mapper(d, keymap):\n        \"\"\"Dictionary mapper. Renames keys according to keymap provided.\n\n        Args:\n            d (dict): old dict\n            keymap (dict): {'old_key':'new_key'}\n        Returns:\n            dict: new dict.\n        \"\"\"\n\n        updated_dict = {}\n        for k, v in zip(d.keys(), d.values()):\n            new_k = keymap.get(k, k)\n            updated_dict[new_k] = d[k]\n        return updated_dict\n\n    def __call__(self, results):\n        # dict to albumentations format\n        results = self.mapper(results, self.keymap_to_albu)\n        # TODO: add bbox_fields\n        if 'bboxes' in results:\n            # to list of boxes\n            if isinstance(results['bboxes'], np.ndarray):\n                results['bboxes'] = [x for x in results['bboxes']]\n            # add pseudo-field for filtration\n            if self.filter_lost_elements:\n                results['idx_mapper'] = np.arange(len(results['bboxes']))\n\n        # TODO: Support mask structure in albu\n        if 'masks' in results:\n            if isinstance(results['masks'], PolygonMasks):\n                raise NotImplementedError(\n                    'Albu only supports BitMap masks now')\n            ori_masks = results['masks']\n            if albumentations.__version__ < '0.5':\n                results['masks'] = results['masks'].masks\n            else:\n                results['masks'] = [mask for mask in results['masks'].masks]\n\n        results = self.aug(**results)\n\n        if 'bboxes' in results:\n            if isinstance(results['bboxes'], list):\n                results['bboxes'] = np.array(\n                    results['bboxes'], dtype=np.float32)\n            results['bboxes'] = results['bboxes'].reshape(-1, 4)\n\n            # filter label_fields\n            if self.filter_lost_elements:\n\n                for label in self.origin_label_fields:\n                    results[label] = np.array(\n                        [results[label][i] for i in results['idx_mapper']])\n                if 'masks' in results:\n                    results['masks'] = np.array(\n                        [results['masks'][i] for i in results['idx_mapper']])\n                    results['masks'] = ori_masks.__class__(\n                        results['masks'], results['image'].shape[0],\n                        results['image'].shape[1])\n\n                if (not len(results['idx_mapper'])\n                        and self.skip_img_without_anno):\n                    return None\n\n        if 'gt_labels' in results:\n            if isinstance(results['gt_labels'], list):\n                results['gt_labels'] = np.array(results['gt_labels'])\n            results['gt_labels'] = results['gt_labels'].astype(np.int64)\n\n        # back to the original format\n        results = self.mapper(results, self.keymap_back)\n\n        # update final shape\n        if self.update_pad_shape:\n            results['pad_shape'] = results['img'].shape\n\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__ + f'(transforms={self.transforms})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomCenterCropPad(object):\n    \"\"\"Random center crop and random around padding for CornerNet.\n\n    This operation generates randomly cropped image from the original image and\n    pads it simultaneously. Different from :class:`RandomCrop`, the output\n    shape may not equal to ``crop_size`` strictly. We choose a random value\n    from ``ratios`` and the output shape could be larger or smaller than\n    ``crop_size``. The padding operation is also different from :class:`Pad`,\n    here we use around padding instead of right-bottom padding.\n\n    The relation between output image (padding image) and original image:\n\n    .. code:: text\n\n                        output image\n\n               +----------------------------+\n               |          padded area       |\n        +------|----------------------------|----------+\n        |      |         cropped area       |          |\n        |      |         +---------------+  |          |\n        |      |         |    .   center |  |          | original image\n        |      |         |        range  |  |          |\n        |      |         +---------------+  |          |\n        +------|----------------------------|----------+\n               |          padded area       |\n               +----------------------------+\n\n    There are 5 main areas in the figure:\n\n    - output image: output image of this operation, also called padding\n      image in following instruction.\n    - original image: input image of this operation.\n    - padded area: non-intersect area of output image and original image.\n    - cropped area: the overlap of output image and original image.\n    - center range: a smaller area where random center chosen from.\n      center range is computed by ``border`` and original image's shape\n      to avoid our random center is too close to original image's border.\n\n    Also this operation act differently in train and test mode, the summary\n    pipeline is listed below.\n\n    Train pipeline:\n\n    1. Choose a ``random_ratio`` from ``ratios``, the shape of padding image\n       will be ``random_ratio * crop_size``.\n    2. Choose a ``random_center`` in center range.\n    3. Generate padding image with center matches the ``random_center``.\n    4. Initialize the padding image with pixel value equals to ``mean``.\n    5. Copy the cropped area to padding image.\n    6. Refine annotations.\n\n    Test pipeline:\n\n    1. Compute output shape according to ``test_pad_mode``.\n    2. Generate padding image with center matches the original image\n       center.\n    3. Initialize the padding image with pixel value equals to ``mean``.\n    4. Copy the ``cropped area`` to padding image.\n\n    Args:\n        crop_size (tuple | None): expected size after crop, final size will\n            computed according to ratio. Requires (h, w) in train mode, and\n            None in test mode.\n        ratios (tuple): random select a ratio from tuple and crop image to\n            (crop_size[0] * ratio) * (crop_size[1] * ratio).\n            Only available in train mode.\n        border (int): max distance from center select area to image border.\n            Only available in train mode.\n        mean (sequence): Mean values of 3 channels.\n        std (sequence): Std values of 3 channels.\n        to_rgb (bool): Whether to convert the image from BGR to RGB.\n        test_mode (bool): whether involve random variables in transform.\n            In train mode, crop_size is fixed, center coords and ratio is\n            random selected from predefined lists. In test mode, crop_size\n            is image's original shape, center coords and ratio is fixed.\n        test_pad_mode (tuple): padding method and padding shape value, only\n            available in test mode. Default is using 'logical_or' with\n            127 as padding shape value.\n\n            - 'logical_or': final_shape = input_shape | padding_shape_value\n            - 'size_divisor': final_shape = int(\n              ceil(input_shape / padding_shape_value) * padding_shape_value)\n        bbox_clip_border (bool, optional): Whether clip the objects outside\n            the border of the image. Defaults to True.\n    \"\"\"\n\n    def __init__(self,\n                 crop_size=None,\n                 ratios=(0.9, 1.0, 1.1),\n                 border=128,\n                 mean=None,\n                 std=None,\n                 to_rgb=None,\n                 test_mode=False,\n                 test_pad_mode=('logical_or', 127),\n                 bbox_clip_border=True):\n        if test_mode:\n            assert crop_size is None, 'crop_size must be None in test mode'\n            assert ratios is None, 'ratios must be None in test mode'\n            assert border is None, 'border must be None in test mode'\n            assert isinstance(test_pad_mode, (list, tuple))\n            assert test_pad_mode[0] in ['logical_or', 'size_divisor']\n        else:\n            assert isinstance(crop_size, (list, tuple))\n            assert crop_size[0] > 0 and crop_size[1] > 0, (\n                'crop_size must > 0 in train mode')\n            assert isinstance(ratios, (list, tuple))\n            assert test_pad_mode is None, (\n                'test_pad_mode must be None in train mode')\n\n        self.crop_size = crop_size\n        self.ratios = ratios\n        self.border = border\n        # We do not set default value to mean, std and to_rgb because these\n        # hyper-parameters are easy to forget but could affect the performance.\n        # Please use the same setting as Normalize for performance assurance.\n        assert mean is not None and std is not None and to_rgb is not None\n        self.to_rgb = to_rgb\n        self.input_mean = mean\n        self.input_std = std\n        if to_rgb:\n            self.mean = mean[::-1]\n            self.std = std[::-1]\n        else:\n            self.mean = mean\n            self.std = std\n        self.test_mode = test_mode\n        self.test_pad_mode = test_pad_mode\n        self.bbox_clip_border = bbox_clip_border\n\n    def _get_border(self, border, size):\n        \"\"\"Get final border for the target size.\n\n        This function generates a ``final_border`` according to image's shape.\n        The area between ``final_border`` and ``size - final_border`` is the\n        ``center range``. We randomly choose center from the ``center range``\n        to avoid our random center is too close to original image's border.\n        Also ``center range`` should be larger than 0.\n\n        Args:\n            border (int): The initial border, default is 128.\n            size (int): The width or height of original image.\n        Returns:\n            int: The final border.\n        \"\"\"\n        k = 2 * border / size\n        i = pow(2, np.ceil(np.log2(np.ceil(k))) + (k == int(k)))\n        return border // i\n\n    def _filter_boxes(self, patch, boxes):\n        \"\"\"Check whether the center of each box is in the patch.\n\n        Args:\n            patch (list[int]): The cropped area, [left, top, right, bottom].\n            boxes (numpy array, (N x 4)): Ground truth boxes.\n\n        Returns:\n            mask (numpy array, (N,)): Each box is inside or outside the patch.\n        \"\"\"\n        center = (boxes[:, :2] + boxes[:, 2:]) / 2\n        mask = (center[:, 0] > patch[0]) * (center[:, 1] > patch[1]) * (\n            center[:, 0] < patch[2]) * (\n                center[:, 1] < patch[3])\n        return mask\n\n    def _crop_image_and_paste(self, image, center, size):\n        \"\"\"Crop image with a given center and size, then paste the cropped\n        image to a blank image with two centers align.\n\n        This function is equivalent to generating a blank image with ``size``\n        as its shape. Then cover it on the original image with two centers (\n        the center of blank image and the random center of original image)\n        aligned. The overlap area is paste from the original image and the\n        outside area is filled with ``mean pixel``.\n\n        Args:\n            image (np array, H x W x C): Original image.\n            center (list[int]): Target crop center coord.\n            size (list[int]): Target crop size. [target_h, target_w]\n\n        Returns:\n            cropped_img (np array, target_h x target_w x C): Cropped image.\n            border (np array, 4): The distance of four border of\n                ``cropped_img`` to the original image area, [top, bottom,\n                left, right]\n            patch (list[int]): The cropped area, [left, top, right, bottom].\n        \"\"\"\n        center_y, center_x = center\n        target_h, target_w = size\n        img_h, img_w, img_c = image.shape\n\n        x0 = max(0, center_x - target_w // 2)\n        x1 = min(center_x + target_w // 2, img_w)\n        y0 = max(0, center_y - target_h // 2)\n        y1 = min(center_y + target_h // 2, img_h)\n        patch = np.array((int(x0), int(y0), int(x1), int(y1)))\n\n        left, right = center_x - x0, x1 - center_x\n        top, bottom = center_y - y0, y1 - center_y\n\n        cropped_center_y, cropped_center_x = target_h // 2, target_w // 2\n        cropped_img = np.zeros((target_h, target_w, img_c), dtype=image.dtype)\n        for i in range(img_c):\n            cropped_img[:, :, i] += self.mean[i]\n        y_slice = slice(cropped_center_y - top, cropped_center_y + bottom)\n        x_slice = slice(cropped_center_x - left, cropped_center_x + right)\n        cropped_img[y_slice, x_slice, :] = image[y0:y1, x0:x1, :]\n\n        border = np.array([\n            cropped_center_y - top, cropped_center_y + bottom,\n            cropped_center_x - left, cropped_center_x + right\n        ],\n                          dtype=np.float32)\n\n        return cropped_img, border, patch\n\n    def _train_aug(self, results):\n        \"\"\"Random crop and around padding the original image.\n\n        Args:\n            results (dict): Image infomations in the augment pipeline.\n\n        Returns:\n            results (dict): The updated dict.\n        \"\"\"\n        img = results['img']\n        h, w, c = img.shape\n        boxes = results['gt_bboxes']\n        while True:\n            scale = random.choice(self.ratios)\n            new_h = int(self.crop_size[0] * scale)\n            new_w = int(self.crop_size[1] * scale)\n            h_border = self._get_border(self.border, h)\n            w_border = self._get_border(self.border, w)\n\n            for i in range(50):\n                center_x = random.randint(low=w_border, high=w - w_border)\n                center_y = random.randint(low=h_border, high=h - h_border)\n\n                cropped_img, border, patch = self._crop_image_and_paste(\n                    img, [center_y, center_x], [new_h, new_w])\n\n                mask = self._filter_boxes(patch, boxes)\n                # if image do not have valid bbox, any crop patch is valid.\n                if not mask.any() and len(boxes) > 0:\n                    continue\n\n                results['img'] = cropped_img\n                results['img_shape'] = cropped_img.shape\n                results['pad_shape'] = cropped_img.shape\n\n                x0, y0, x1, y1 = patch\n\n                left_w, top_h = center_x - x0, center_y - y0\n                cropped_center_x, cropped_center_y = new_w // 2, new_h // 2\n\n                # crop bboxes accordingly and clip to the image boundary\n                for key in results.get('bbox_fields', []):\n                    mask = self._filter_boxes(patch, results[key])\n                    bboxes = results[key][mask]\n                    bboxes[:, 0:4:2] += cropped_center_x - left_w - x0\n                    bboxes[:, 1:4:2] += cropped_center_y - top_h - y0\n                    if self.bbox_clip_border:\n                        bboxes[:, 0:4:2] = np.clip(bboxes[:, 0:4:2], 0, new_w)\n                        bboxes[:, 1:4:2] = np.clip(bboxes[:, 1:4:2], 0, new_h)\n                    keep = (bboxes[:, 2] > bboxes[:, 0]) & (\n                        bboxes[:, 3] > bboxes[:, 1])\n                    bboxes = bboxes[keep]\n                    results[key] = bboxes\n                    if key in ['gt_bboxes']:\n                        if 'gt_labels' in results:\n                            labels = results['gt_labels'][mask]\n                            labels = labels[keep]\n                            results['gt_labels'] = labels\n                        if 'gt_masks' in results:\n                            raise NotImplementedError(\n                                'RandomCenterCropPad only supports bbox.')\n\n                # crop semantic seg\n                for key in results.get('seg_fields', []):\n                    raise NotImplementedError(\n                        'RandomCenterCropPad only supports bbox.')\n                return results\n\n    def _test_aug(self, results):\n        \"\"\"Around padding the original image without cropping.\n\n        The padding mode and value are from ``test_pad_mode``.\n\n        Args:\n            results (dict): Image infomations in the augment pipeline.\n\n        Returns:\n            results (dict): The updated dict.\n        \"\"\"\n        img = results['img']\n        h, w, c = img.shape\n        results['img_shape'] = img.shape\n        if self.test_pad_mode[0] in ['logical_or']:\n            target_h = h | self.test_pad_mode[1]\n            target_w = w | self.test_pad_mode[1]\n        elif self.test_pad_mode[0] in ['size_divisor']:\n            divisor = self.test_pad_mode[1]\n            target_h = int(np.ceil(h / divisor)) * divisor\n            target_w = int(np.ceil(w / divisor)) * divisor\n        else:\n            raise NotImplementedError(\n                'RandomCenterCropPad only support two testing pad mode:'\n                'logical-or and size_divisor.')\n\n        cropped_img, border, _ = self._crop_image_and_paste(\n            img, [h // 2, w // 2], [target_h, target_w])\n        results['img'] = cropped_img\n        results['pad_shape'] = cropped_img.shape\n        results['border'] = border\n        return results\n\n    def __call__(self, results):\n        img = results['img']\n        assert img.dtype == np.float32, (\n            'RandomCenterCropPad needs the input image of dtype np.float32,'\n            ' please set \"to_float32=True\" in \"LoadImageFromFile\" pipeline')\n        h, w, c = img.shape\n        assert c == len(self.mean)\n        if self.test_mode:\n            return self._test_aug(results)\n        else:\n            return self._train_aug(results)\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(crop_size={self.crop_size}, '\n        repr_str += f'ratios={self.ratios}, '\n        repr_str += f'border={self.border}, '\n        repr_str += f'mean={self.input_mean}, '\n        repr_str += f'std={self.input_std}, '\n        repr_str += f'to_rgb={self.to_rgb}, '\n        repr_str += f'test_mode={self.test_mode}, '\n        repr_str += f'test_pad_mode={self.test_pad_mode}, '\n        repr_str += f'bbox_clip_border={self.bbox_clip_border})'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass CutOut(object):\n    \"\"\"CutOut operation.\n\n    Randomly drop some regions of image used in\n    `Cutout <https://arxiv.org/abs/1708.04552>`_.\n\n    Args:\n        n_holes (int | tuple[int, int]): Number of regions to be dropped.\n            If it is given as a list, number of holes will be randomly\n            selected from the closed interval [`n_holes[0]`, `n_holes[1]`].\n        cutout_shape (tuple[int, int] | list[tuple[int, int]]): The candidate\n            shape of dropped regions. It can be `tuple[int, int]` to use a\n            fixed cutout shape, or `list[tuple[int, int]]` to randomly choose\n            shape from the list.\n        cutout_ratio (tuple[float, float] | list[tuple[float, float]]): The\n            candidate ratio of dropped regions. It can be `tuple[float, float]`\n            to use a fixed ratio or `list[tuple[float, float]]` to randomly\n            choose ratio from the list. Please note that `cutout_shape`\n            and `cutout_ratio` cannot be both given at the same time.\n        fill_in (tuple[float, float, float] | tuple[int, int, int]): The value\n            of pixel to fill in the dropped regions. Default: (0, 0, 0).\n    \"\"\"\n\n    def __init__(self,\n                 n_holes,\n                 cutout_shape=None,\n                 cutout_ratio=None,\n                 fill_in=(0, 0, 0)):\n\n        assert (cutout_shape is None) ^ (cutout_ratio is None), \\\n            'Either cutout_shape or cutout_ratio should be specified.'\n        assert (isinstance(cutout_shape, (list, tuple))\n                or isinstance(cutout_ratio, (list, tuple)))\n        if isinstance(n_holes, tuple):\n            assert len(n_holes) == 2 and 0 <= n_holes[0] < n_holes[1]\n        else:\n            n_holes = (n_holes, n_holes)\n        self.n_holes = n_holes\n        self.fill_in = fill_in\n        self.with_ratio = cutout_ratio is not None\n        self.candidates = cutout_ratio if self.with_ratio else cutout_shape\n        if not isinstance(self.candidates, list):\n            self.candidates = [self.candidates]\n\n    def __call__(self, results):\n        \"\"\"Call function to drop some regions of image.\"\"\"\n        h, w, c = results['img'].shape\n        n_holes = np.random.randint(self.n_holes[0], self.n_holes[1] + 1)\n        for _ in range(n_holes):\n            x1 = np.random.randint(0, w)\n            y1 = np.random.randint(0, h)\n            index = np.random.randint(0, len(self.candidates))\n            if not self.with_ratio:\n                cutout_w, cutout_h = self.candidates[index]\n            else:\n                cutout_w = int(self.candidates[index][0] * w)\n                cutout_h = int(self.candidates[index][1] * h)\n\n            x2 = np.clip(x1 + cutout_w, 0, w)\n            y2 = np.clip(y1 + cutout_h, 0, h)\n            results['img'][y1:y2, x1:x2, :] = self.fill_in\n\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(n_holes={self.n_holes}, '\n        repr_str += (f'cutout_ratio={self.candidates}, ' if self.with_ratio\n                     else f'cutout_shape={self.candidates}, ')\n        repr_str += f'fill_in={self.fill_in})'\n        return repr_str\n", "idx": 20, "id": 24834, "msg": "", "proj": "open-mmlab-mmdetection", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -394,6 +394,7 @@ func (app *App) createAutomationPolicy(ctx caddy.Context) error {\n \tif acmeIssuer.Challenges.HTTP.AlternatePort == 0 {\n \t\t// don't overwrite existing explicit config\n \t\tacmeIssuer.Challenges.HTTP.AlternatePort = app.HTTPPort\n+\t\tacmeIssuer.Provision(ctx)\n \t}\n \tif acmeIssuer.Challenges.TLSALPN == nil {\n \t\tacmeIssuer.Challenges.TLSALPN = new(caddytls.TLSALPNChallengeConfig)", "y": 1, "oldf": "package caddyhttp\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\t\"strconv\"\n\n\t\"github.com/caddyserver/caddy/v2\"\n\t\"github.com/caddyserver/caddy/v2/modules/caddytls\"\n\t\"github.com/caddyserver/certmagic\"\n\t\"go.uber.org/zap\"\n)\n\n// AutoHTTPSConfig is used to disable automatic HTTPS\n// or certain aspects of it for a specific server.\n// HTTPS is enabled automatically and by default when\n// qualifying hostnames are available from the config.\ntype AutoHTTPSConfig struct {\n\t// If true, automatic HTTPS will be entirely disabled.\n\tDisabled bool `json:\"disable,omitempty\"`\n\n\t// If true, only automatic HTTP->HTTPS redirects will\n\t// be disabled.\n\tDisableRedir bool `json:\"disable_redirects,omitempty\"`\n\n\t// Hosts/domain names listed here will not be included\n\t// in automatic HTTPS (they will not have certificates\n\t// loaded nor redirects applied).\n\tSkip []string `json:\"skip,omitempty\"`\n\n\t// Hosts/domain names listed here will still be enabled\n\t// for automatic HTTPS (unless in the Skip list), except\n\t// that certificates will not be provisioned and managed\n\t// for these names.\n\tSkipCerts []string `json:\"skip_certificates,omitempty\"`\n\n\t// By default, automatic HTTPS will obtain and renew\n\t// certificates for qualifying hostnames. However, if\n\t// a certificate with a matching SAN is already loaded\n\t// into the cache, certificate management will not be\n\t// enabled. To force automated certificate management\n\t// regardless of loaded certificates, set this to true.\n\tIgnoreLoadedCerts bool `json:\"ignore_loaded_certificates,omitempty\"`\n}\n\n// Skipped returns true if name is in skipSlice, which\n// should be either the Skip or SkipCerts field on ahc.\nfunc (ahc AutoHTTPSConfig) Skipped(name string, skipSlice []string) bool {\n\tfor _, n := range skipSlice {\n\t\tif name == n {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// automaticHTTPSPhase1 provisions all route matchers, determines\n// which domain names found in the routes qualify for automatic\n// HTTPS, and sets up HTTP->HTTPS redirects. This phase must occur\n// at the beginning of provisioning, because it may add routes and\n// even servers to the app, which still need to be set up with the\n// rest of them during provisioning.\nfunc (app *App) automaticHTTPSPhase1(ctx caddy.Context, repl *caddy.Replacer) error {\n\t// this map acts as a set to store the domain names\n\t// for which we will manage certificates automatically\n\tuniqueDomainsForCerts := make(map[string]struct{})\n\n\t// this maps domain names for automatic HTTP->HTTPS\n\t// redirects to their destination server address\n\tredirDomains := make(map[string]caddy.ParsedAddress)\n\n\tfor srvName, srv := range app.Servers {\n\t\t// as a prerequisite, provision route matchers; this is\n\t\t// required for all routes on all servers, and must be\n\t\t// done before we attempt to do phase 1 of auto HTTPS,\n\t\t// since we have to access the decoded host matchers the\n\t\t// handlers will be provisioned later\n\t\tif srv.Routes != nil {\n\t\t\terr := srv.Routes.ProvisionMatchers(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"server %s: setting up route matchers: %v\", srvName, err)\n\t\t\t}\n\t\t}\n\n\t\t// prepare for automatic HTTPS\n\t\tif srv.AutoHTTPS == nil {\n\t\t\tsrv.AutoHTTPS = new(AutoHTTPSConfig)\n\t\t}\n\t\tif srv.AutoHTTPS.Disabled {\n\t\t\tcontinue\n\t\t}\n\n\t\t// skip if all listeners use the HTTP port\n\t\tif !srv.listenersUseAnyPortOtherThan(app.httpPort()) {\n\t\t\tapp.logger.Info(\"server is listening only on the HTTP port, so no automatic HTTPS will be applied to this server\",\n\t\t\t\tzap.String(\"server_name\", srvName),\n\t\t\t\tzap.Int(\"http_port\", app.httpPort()),\n\t\t\t)\n\t\t\tsrv.AutoHTTPS.Disabled = true\n\t\t\tcontinue\n\t\t}\n\n\t\tdefaultConnPolicies := caddytls.ConnectionPolicies{\n\t\t\t&caddytls.ConnectionPolicy{ALPN: defaultALPN},\n\t\t}\n\n\t\t// if all listeners are on the HTTPS port, make sure\n\t\t// there is at least one TLS connection policy; it\n\t\t// should be obvious that they want to use TLS without\n\t\t// needing to specify one empty policy to enable it\n\t\tif srv.TLSConnPolicies == nil &&\n\t\t\t!srv.listenersUseAnyPortOtherThan(app.httpsPort()) {\n\t\t\tapp.logger.Info(\"server is listening only on the HTTPS port but has no TLS connection policies; adding one to enable TLS\",\n\t\t\t\tzap.String(\"server_name\", srvName),\n\t\t\t\tzap.Int(\"https_port\", app.httpsPort()),\n\t\t\t)\n\t\t\tsrv.TLSConnPolicies = defaultConnPolicies\n\t\t}\n\n\t\t// find all qualifying domain names (deduplicated) in this server\n\t\tserverDomainSet := make(map[string]struct{})\n\t\tfor routeIdx, route := range srv.Routes {\n\t\t\tfor matcherSetIdx, matcherSet := range route.MatcherSets {\n\t\t\t\tfor matcherIdx, m := range matcherSet {\n\t\t\t\t\tif hm, ok := m.(*MatchHost); ok {\n\t\t\t\t\t\tfor hostMatcherIdx, d := range *hm {\n\t\t\t\t\t\t\tvar err error\n\t\t\t\t\t\t\td, err = repl.ReplaceOrErr(d, true, false)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn fmt.Errorf(\"%s: route %d, matcher set %d, matcher %d, host matcher %d: %v\",\n\t\t\t\t\t\t\t\t\tsrvName, routeIdx, matcherSetIdx, matcherIdx, hostMatcherIdx, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif certmagic.HostQualifies(d) &&\n\t\t\t\t\t\t\t\t!srv.AutoHTTPS.Skipped(d, srv.AutoHTTPS.Skip) {\n\t\t\t\t\t\t\t\tserverDomainSet[d] = struct{}{}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// nothing more to do here if there are no\n\t\t// domains that qualify for automatic HTTPS\n\t\tif len(serverDomainSet) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// for all the hostnames we found, filter them so we have\n\t\t// a deduplicated list of names for which to obtain certs\n\t\tfor d := range serverDomainSet {\n\t\t\tif !srv.AutoHTTPS.Skipped(d, srv.AutoHTTPS.SkipCerts) {\n\t\t\t\t// if a certificate for this name is already loaded,\n\t\t\t\t// don't obtain another one for it, unless we are\n\t\t\t\t// supposed to ignore loaded certificates\n\t\t\t\tif !srv.AutoHTTPS.IgnoreLoadedCerts &&\n\t\t\t\t\tlen(app.tlsApp.AllMatchingCertificates(d)) > 0 {\n\t\t\t\t\tapp.logger.Info(\"skipping automatic certificate management because one or more matching certificates are already loaded\",\n\t\t\t\t\t\tzap.String(\"domain\", d),\n\t\t\t\t\t\tzap.String(\"server_name\", srvName),\n\t\t\t\t\t)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tuniqueDomainsForCerts[d] = struct{}{}\n\t\t\t}\n\t\t}\n\n\t\t// tell the server to use TLS if it is not already doing so\n\t\tif srv.TLSConnPolicies == nil {\n\t\t\tsrv.TLSConnPolicies = defaultConnPolicies\n\t\t}\n\n\t\t// nothing left to do if auto redirects are disabled\n\t\tif srv.AutoHTTPS.DisableRedir {\n\t\t\tcontinue\n\t\t}\n\n\t\tapp.logger.Info(\"enabling automatic HTTP->HTTPS redirects\",\n\t\t\tzap.String(\"server_name\", srvName),\n\t\t)\n\n\t\t// create HTTP->HTTPS redirects\n\t\tfor _, addr := range srv.Listen {\n\t\t\t// figure out the address we will redirect to...\n\t\t\taddr, err := caddy.ParseNetworkAddress(addr)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"%s: invalid listener address: %v\", srvName, addr)\n\t\t\t}\n\n\t\t\t// ...and associate it with each domain in this server\n\t\t\tfor d := range serverDomainSet {\n\t\t\t\t// if this domain is used on more than one HTTPS-enabled\n\t\t\t\t// port, we'll have to choose one, so prefer the HTTPS port\n\t\t\t\tif _, ok := redirDomains[d]; !ok ||\n\t\t\t\t\taddr.StartPort == uint(app.httpsPort()) {\n\t\t\t\t\tredirDomains[d] = addr\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// we now have a list of all the unique names for which we need certs;\n\t// turn the set into a slice so that phase 2 can use it\n\tapp.allCertDomains = make([]string, 0, len(uniqueDomainsForCerts))\n\tfor d := range uniqueDomainsForCerts {\n\t\tapp.allCertDomains = append(app.allCertDomains, d)\n\t}\n\n\t// ensure there is an automation policy to handle these certs\n\terr := app.createAutomationPolicy(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// we're done if there are no HTTP->HTTPS redirects to add\n\tif len(redirDomains) == 0 {\n\t\treturn nil\n\t}\n\n\t// we need to reduce the mapping, i.e. group domains by address\n\t// since new routes are appended to servers by their address\n\tdomainsByAddr := make(map[string][]string)\n\tfor domain, addr := range redirDomains {\n\t\taddrStr := addr.String()\n\t\tdomainsByAddr[addrStr] = append(domainsByAddr[addrStr], domain)\n\t}\n\n\t// these keep track of the redirect server address(es)\n\t// and the routes for those servers which actually\n\t// respond with the redirects\n\tredirServerAddrs := make(map[string]struct{})\n\tvar redirRoutes RouteList\n\n\tredirServers := make(map[string][]Route)\n\n\tfor addrStr, domains := range domainsByAddr {\n\t\t// build the matcher set for this redirect route\n\t\t// (note that we happen to bypass Provision and\n\t\t// Validate steps for these matcher modules)\n\t\tmatcherSet := MatcherSet{\n\t\t\tMatchProtocol(\"http\"),\n\t\t\tMatchHost(domains),\n\t\t}\n\n\t\t// build the address to which to redirect\n\t\taddr, err := caddy.ParseNetworkAddress(addrStr)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tredirTo := \"https://{http.request.host}\"\n\t\tif addr.StartPort != DefaultHTTPSPort {\n\t\t\tredirTo += \":\" + strconv.Itoa(int(addr.StartPort))\n\t\t}\n\t\tredirTo += \"{http.request.uri}\"\n\n\t\t// build the route\n\t\tredirRoute := Route{\n\t\t\tMatcherSets: []MatcherSet{matcherSet},\n\t\t\tHandlers: []MiddlewareHandler{\n\t\t\t\tStaticResponse{\n\t\t\t\t\tStatusCode: WeakString(strconv.Itoa(http.StatusPermanentRedirect)),\n\t\t\t\t\tHeaders: http.Header{\n\t\t\t\t\t\t\"Location\":   []string{redirTo},\n\t\t\t\t\t\t\"Connection\": []string{\"close\"},\n\t\t\t\t\t},\n\t\t\t\t\tClose: true,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\t// use the network/host information from the address,\n\t\t// but change the port to the HTTP port then rebuild\n\t\tredirAddr := addr\n\t\tredirAddr.StartPort = uint(app.httpPort())\n\t\tredirAddr.EndPort = redirAddr.StartPort\n\t\tredirAddrStr := redirAddr.String()\n\n\t\tredirServers[redirAddrStr] = append(redirServers[redirAddrStr], redirRoute)\n\t}\n\n\t// on-demand TLS means that hostnames may be used which are not\n\t// explicitly defined in the config, and we still need to redirect\n\t// those; so we can append a single catch-all route (notice there\n\t// is no Host matcher) after the other redirect routes which will\n\t// allow us to handle unexpected/new hostnames... however, it's\n\t// not entirely clear what the redirect destination should be,\n\t// so I'm going to just hard-code the app's HTTPS port and call\n\t// it good for now...\n\tappendCatchAll := func(routes []Route) []Route {\n\t\tredirTo := \"https://{http.request.host}\"\n\t\tif app.httpsPort() != DefaultHTTPSPort {\n\t\t\tredirTo += \":\" + strconv.Itoa(app.httpsPort())\n\t\t}\n\t\tredirTo += \"{http.request.uri}\"\n\t\troutes = append(routes, Route{\n\t\t\tMatcherSets: []MatcherSet{MatcherSet{MatchProtocol(\"http\")}},\n\t\t\tHandlers: []MiddlewareHandler{\n\t\t\t\tStaticResponse{\n\t\t\t\t\tStatusCode: WeakString(strconv.Itoa(http.StatusPermanentRedirect)),\n\t\t\t\t\tHeaders: http.Header{\n\t\t\t\t\t\t\"Location\":   []string{redirTo},\n\t\t\t\t\t\t\"Connection\": []string{\"close\"},\n\t\t\t\t\t},\n\t\t\t\t\tClose: true,\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\treturn routes\n\t}\n\nredirServersLoop:\n\tfor redirServerAddr, routes := range redirServers {\n\t\t// for each redirect listener, see if there's already a\n\t\t// server configured to listen on that exact address; if so,\n\t\t// simply add the redirect route to the end of its route\n\t\t// list; otherwise, we'll create a new server for all the\n\t\t// listener addresses that are unused and serve the\n\t\t// remaining redirects from it\n\t\tfor srvName, srv := range app.Servers {\n\t\t\tif srv.hasListenerAddress(redirServerAddr) {\n\t\t\t\t// user has configured a server for the same address\n\t\t\t\t// that the redirect runs from; simply append our\n\t\t\t\t// redirect route to the existing routes, with a\n\t\t\t\t// caveat that their config might override ours\n\t\t\t\tapp.logger.Warn(\"user server is listening on same interface as automatic HTTP->HTTPS redirects; user-configured routes might override these redirects\",\n\t\t\t\t\tzap.String(\"server_name\", srvName),\n\t\t\t\t\tzap.String(\"interface\", redirServerAddr),\n\t\t\t\t)\n\t\t\t\tsrv.Routes = append(srv.Routes, appendCatchAll(routes)...)\n\t\t\t\tcontinue redirServersLoop\n\t\t\t}\n\t\t}\n\n\t\t// no server with this listener address exists;\n\t\t// save this address and route for custom server\n\t\tredirServerAddrs[redirServerAddr] = struct{}{}\n\t\tredirRoutes = append(redirRoutes, routes...)\n\t}\n\n\t// if there are routes remaining which do not belong\n\t// in any existing server, make our own to serve the\n\t// rest of the redirects\n\tif len(redirServerAddrs) > 0 {\n\t\tredirServerAddrsList := make([]string, 0, len(redirServerAddrs))\n\t\tfor a := range redirServerAddrs {\n\t\t\tredirServerAddrsList = append(redirServerAddrsList, a)\n\t\t}\n\t\tapp.Servers[\"remaining_auto_https_redirects\"] = &Server{\n\t\t\tListen: redirServerAddrsList,\n\t\t\tRoutes: appendCatchAll(redirRoutes),\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// createAutomationPolicy ensures that certificates for this app are\n// managed properly; for example, it's implied that the HTTPPort\n// should also be the port the HTTP challenge is solved on; the same\n// for HTTPS port and TLS-ALPN challenge also. We need to tell the\n// TLS app to manage these certs by honoring those port configurations,\n// so we either find an existing matching automation policy with an\n// ACME issuer, or make a new one and append it.\nfunc (app *App) createAutomationPolicy(ctx caddy.Context) error {\n\tvar matchingPolicy *caddytls.AutomationPolicy\n\tvar acmeIssuer *caddytls.ACMEIssuer\n\tif app.tlsApp.Automation != nil {\n\t\t// maybe we can find an exisitng one that matches; this is\n\t\t// useful if the user made a single automation policy to\n\t\t// set the CA endpoint to a test/staging endpoint (very\n\t\t// common), but forgot to customize the ports here, while\n\t\t// setting them in the HTTP app instead (I did this too\n\t\t// many times)\n\t\tfor _, ap := range app.tlsApp.Automation.Policies {\n\t\t\tif len(ap.Hosts) == 0 {\n\t\t\t\tmatchingPolicy = ap\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif matchingPolicy != nil {\n\t\t// if it has an ACME issuer, maybe we can just use that\n\t\tacmeIssuer, _ = matchingPolicy.Issuer.(*caddytls.ACMEIssuer)\n\t}\n\tif acmeIssuer == nil {\n\t\tacmeIssuer = new(caddytls.ACMEIssuer)\n\t}\n\tif acmeIssuer.Challenges == nil {\n\t\tacmeIssuer.Challenges = new(caddytls.ChallengesConfig)\n\t}\n\tif acmeIssuer.Challenges.HTTP == nil {\n\t\tacmeIssuer.Challenges.HTTP = new(caddytls.HTTPChallengeConfig)\n\t}\n\tif acmeIssuer.Challenges.HTTP.AlternatePort == 0 {\n\t\t// don't overwrite existing explicit config\n\t\tacmeIssuer.Challenges.HTTP.AlternatePort = app.HTTPPort\n\t}\n\tif acmeIssuer.Challenges.TLSALPN == nil {\n\t\tacmeIssuer.Challenges.TLSALPN = new(caddytls.TLSALPNChallengeConfig)\n\t}\n\tif acmeIssuer.Challenges.TLSALPN.AlternatePort == 0 {\n\t\t// don't overwrite existing explicit config\n\t\tacmeIssuer.Challenges.TLSALPN.AlternatePort = app.HTTPSPort\n\t}\n\n\tif matchingPolicy == nil {\n\t\t// if there was no matching policy, we'll have to append our own\n\t\terr := app.tlsApp.AddAutomationPolicy(&caddytls.AutomationPolicy{\n\t\t\tHosts:  app.allCertDomains,\n\t\t\tIssuer: acmeIssuer,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// if there was an existing matching policy, we need to reprovision\n\t\t// its issuer (because we just changed its port settings and it has\n\t\t// to re-build its stored certmagic config template with the new\n\t\t// values), then re-assign the Issuer pointer on the policy struct\n\t\t// because our type assertion changed the address\n\t\terr := acmeIssuer.Provision(ctx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmatchingPolicy.Issuer = acmeIssuer\n\t}\n\n\treturn nil\n}\n\n// automaticHTTPSPhase2 begins certificate management for\n// all names in the qualifying domain set for each server.\n// This phase must occur after provisioning and at the end\n// of app start, after all the servers have been started.\n// Doing this last ensures that there won't be any race\n// for listeners on the HTTP or HTTPS ports when management\n// is async (if CertMagic's solvers bind to those ports\n// first, then our servers would fail to bind to them,\n// which would be bad, since CertMagic's bindings are\n// temporary and don't serve the user's sites!).\nfunc (app *App) automaticHTTPSPhase2() error {\n\tif len(app.allCertDomains) == 0 {\n\t\treturn nil\n\t}\n\tapp.logger.Info(\"enabling automatic TLS certificate management\",\n\t\tzap.Strings(\"domains\", app.allCertDomains),\n\t)\n\terr := app.tlsApp.Manage(app.allCertDomains)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"managing certificates for %v: %s\", app.allCertDomains, err)\n\t}\n\tapp.allCertDomains = nil // no longer needed; allow GC to deallocate\n\treturn nil\n}\n", "idx": 1, "id": 14351, "msg": "Move this down to line 405 (old) or 406 (new).", "proj": "caddyserver-caddy", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -66,12 +66,12 @@ public class DefaultMailCreator implements MailCreator {\n     if (emailList != null && !emailList.isEmpty()) {\n       message.addAllToAddress(emailList);\n       message.setMimeType(\"text/html\");\n-      message.setSubject(\"Flow '\" + flow.getFlowId() + \"' has encountered a failure on \"\n-          + azkabanName);\n+      message.setSubject(\"Flow '\" + flow.getFlowId()\n+          + \"' has encountered a failure on \" + azkabanName);\n \n       message.println(\"<h2 style=\\\"color:#FF0000\\\"> Execution '\"\n-          + flow.getExecutionId() + \"' of flow '\" + flow.getFlowId()\n-          + \"' has encountered a failure on \" + azkabanName + \"</h2>\");\n+          + flow.getExecutionId() + \"' of flow '\" + flow.getFlowId() + \"' of project '\"\n+          + flow.getProjectName() + \"' has encountered a failure on \" + azkabanName + \"</h2>\");\n \n       if (option.getFailureAction() == FailureAction.CANCEL_ALL) {\n         message", "y": 1, "oldf": "/*\n * Copyright 2012 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n */\n\npackage azkaban.executor.mail;\n\nimport java.text.DateFormat;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.List;\n\nimport azkaban.executor.ExecutableFlow;\nimport azkaban.executor.ExecutionOptions;\nimport azkaban.executor.ExecutionOptions.FailureAction;\nimport azkaban.utils.EmailMessage;\nimport azkaban.utils.Emailer;\nimport azkaban.utils.Utils;\n\npublic class DefaultMailCreator implements MailCreator {\n  public static final String DEFAULT_MAIL_CREATOR = \"default\";\n  private static HashMap<String, MailCreator> registeredCreators = new HashMap<>();\n  private static MailCreator defaultCreator;\n\n  private static final DateFormat DATE_FORMATTER = new SimpleDateFormat(\n      \"yyyy/MM/dd HH:mm:ss z\");\n\n  public static void registerCreator(String name, MailCreator creator) {\n    registeredCreators.put(name, creator);\n  }\n\n  public static MailCreator getCreator(String name) {\n    MailCreator creator = registeredCreators.get(name);\n    if (creator == null) {\n      creator = defaultCreator;\n    }\n    return creator;\n  }\n\n  static {\n    defaultCreator = new DefaultMailCreator();\n    registerCreator(DEFAULT_MAIL_CREATOR, defaultCreator);\n  }\n\n  @Override\n  public boolean createFirstErrorMessage(ExecutableFlow flow,\n      EmailMessage message, String azkabanName, String scheme,\n      String clientHostname, String clientPortNumber, String... vars) {\n\n    ExecutionOptions option = flow.getExecutionOptions();\n    List<String> emailList = option.getFailureEmails();\n    int execId = flow.getExecutionId();\n\n    if (emailList != null && !emailList.isEmpty()) {\n      message.addAllToAddress(emailList);\n      message.setMimeType(\"text/html\");\n      message.setSubject(\"Flow '\" + flow.getFlowId() + \"' has encountered a failure on \"\n          + azkabanName);\n\n      message.println(\"<h2 style=\\\"color:#FF0000\\\"> Execution '\"\n          + flow.getExecutionId() + \"' of flow '\" + flow.getFlowId()\n          + \"' has encountered a failure on \" + azkabanName + \"</h2>\");\n\n      if (option.getFailureAction() == FailureAction.CANCEL_ALL) {\n        message\n            .println(\"This flow is set to cancel all currently running jobs.\");\n      } else if (option.getFailureAction() == FailureAction.FINISH_ALL_POSSIBLE) {\n        message\n            .println(\"This flow is set to complete all jobs that aren't blocked by the failure.\");\n      } else {\n        message\n            .println(\"This flow is set to complete all currently running jobs before stopping.\");\n      }\n\n      message.println(\"<table>\");\n      message.println(\"<tr><td>Start Time</td><td>\"\n          + convertMSToString(flow.getStartTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>End Time</td><td>\"\n          + convertMSToString(flow.getEndTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>Duration</td><td>\"\n          + Utils.formatDuration(flow.getStartTime(), flow.getEndTime())\n          + \"</td></tr>\");\n      message.println(\"<tr><td>Status</td><td>\" + flow.getStatus() + \"</td></tr>\");\n      message.println(\"</table>\");\n      message.println(\"\");\n      String executionUrl =\n          scheme + \"://\" + clientHostname + \":\" + clientPortNumber + \"/\"\n              + \"executor?\" + \"execid=\" + execId;\n      message.println(\"<a href=\\\"\" + executionUrl + \"\\\">\" + flow.getFlowId()\n          + \" Execution Link</a>\");\n\n      message.println(\"\");\n      message.println(\"<h3>Reason</h3>\");\n      List<String> failedJobs = Emailer.findFailedJobs(flow);\n      message.println(\"<ul>\");\n      for (String jobId : failedJobs) {\n        message.println(\"<li><a href=\\\"\" + executionUrl + \"&job=\" + jobId\n            + \"\\\">Failed job '\" + jobId + \"' Link</a></li>\");\n      }\n\n      message.println(\"</ul>\");\n      return true;\n    }\n\n    return false;\n  }\n\n  @Override\n  public boolean createErrorEmail(ExecutableFlow flow, EmailMessage message,\n      String azkabanName, String scheme, String clientHostname,\n      String clientPortNumber, String... vars) {\n\n    ExecutionOptions option = flow.getExecutionOptions();\n\n    List<String> emailList = option.getFailureEmails();\n    int execId = flow.getExecutionId();\n\n    if (emailList != null && !emailList.isEmpty()) {\n      message.addAllToAddress(emailList);\n      message.setMimeType(\"text/html\");\n      message.setSubject(\"Flow '\" + flow.getFlowId() + \"' has failed on \"\n          + azkabanName);\n\n      message.println(\"<h2 style=\\\"color:#FF0000\\\"> Execution '\" + execId\n          + \"' of flow '\" + flow.getFlowId() + \"' has failed on \" + azkabanName\n          + \"</h2>\");\n      message.println(\"<table>\");\n      message.println(\"<tr><td>Start Time</td><td>\"\n          + convertMSToString(flow.getStartTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>End Time</td><td>\"\n          + convertMSToString(flow.getEndTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>Duration</td><td>\"\n          + Utils.formatDuration(flow.getStartTime(), flow.getEndTime())\n          + \"</td></tr>\");\n      message.println(\"<tr><td>Status</td><td>\" + flow.getStatus() + \"</td></tr>\");\n      message.println(\"</table>\");\n      message.println(\"\");\n      String executionUrl =\n          scheme + \"://\" + clientHostname + \":\" + clientPortNumber + \"/\"\n              + \"executor?\" + \"execid=\" + execId;\n      message.println(\"<a href=\\\"\" + executionUrl + \"\\\">\" + flow.getFlowId()\n          + \" Execution Link</a>\");\n\n      message.println(\"\");\n      message.println(\"<h3>Reason</h3>\");\n      List<String> failedJobs = Emailer.findFailedJobs(flow);\n      message.println(\"<ul>\");\n      for (String jobId : failedJobs) {\n        message.println(\"<li><a href=\\\"\" + executionUrl + \"&job=\" + jobId\n            + \"\\\">Failed job '\" + jobId + \"' Link</a></li>\");\n      }\n      for (String reasons : vars) {\n        message.println(\"<li>\" + reasons + \"</li>\");\n      }\n\n      message.println(\"</ul>\");\n      return true;\n    }\n    return false;\n  }\n\n  @Override\n  public boolean createSuccessEmail(ExecutableFlow flow, EmailMessage message,\n      String azkabanName, String scheme, String clientHostname,\n      String clientPortNumber, String... vars) {\n\n    ExecutionOptions option = flow.getExecutionOptions();\n    List<String> emailList = option.getSuccessEmails();\n\n    int execId = flow.getExecutionId();\n\n    if (emailList != null && !emailList.isEmpty()) {\n      message.addAllToAddress(emailList);\n      message.setMimeType(\"text/html\");\n      message.setSubject(\"Flow '\" + flow.getFlowId() + \"' has succeeded on \"\n          + azkabanName);\n\n      message.println(\"<h2> Execution '\" + flow.getExecutionId()\n          + \"' of flow '\" + flow.getFlowId() + \"' has succeeded on \"\n          + azkabanName + \"</h2>\");\n      message.println(\"<table>\");\n      message.println(\"<tr><td>Start Time</td><td>\"\n          + convertMSToString(flow.getStartTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>End Time</td><td>\"\n          + convertMSToString(flow.getEndTime()) + \"</td></tr>\");\n      message.println(\"<tr><td>Duration</td><td>\"\n          + Utils.formatDuration(flow.getStartTime(), flow.getEndTime())\n          + \"</td></tr>\");\n      message.println(\"<tr><td>Status</td><td>\" + flow.getStatus() + \"</td></tr>\");\n      message.println(\"</table>\");\n      message.println(\"\");\n      String executionUrl =\n          scheme + \"://\" + clientHostname + \":\" + clientPortNumber + \"/\"\n              + \"executor?\" + \"execid=\" + execId;\n      message.println(\"<a href=\\\"\" + executionUrl + \"\\\">\" + flow.getFlowId()\n          + \" Execution Link</a>\");\n      return true;\n    }\n    return false;\n  }\n\n  private static String convertMSToString(long timeInMS) {\n    if (timeInMS < 0) {\n      return \"N/A\";\n    } else {\n      return DATE_FORMATTER.format(new Date(timeInMS));\n    }\n  }\n}\n", "idx": 1, "id": 13096, "msg": "i think we can have those strings as constant variable in this class and use string.format", "proj": "azkaban-azkaban", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -661,6 +661,17 @@ public class RealTimeGetComponent extends SearchComponent\n     return sid;\n   }\n \n+  private static void ensureDocFieldsDecorated(Set<String> onlyTheseNonStoredDVs, SolrDocumentBase doc, int docid, SolrDocumentFetcher docFetcher, boolean resolveNestedFields) throws IOException {\n+    if (onlyTheseNonStoredDVs != null) {\n+      docFetcher.decorateDocValueFields(doc, docid, onlyTheseNonStoredDVs);\n+    } else {\n+      docFetcher.decorateDocValueFields(doc, docid, docFetcher.getNonStoredDVsWithoutCopyTargets());\n+    }\n+    if(resolveNestedFields) {\n+      docFetcher.decorateDocValueFields(doc, docid, NESTED_META_FIELDS);\n+    }\n+  }\n+\n   private static SolrInputDocument toSolrInputDocument(Document doc, IndexSchema schema) {\n     SolrInputDocument out = new SolrInputDocument();\n     for( IndexableField f : doc.getFields() ) {", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.handler.component;\n\nimport java.io.IOException;\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.stream.Collectors;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.DocValuesType;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.LeafReaderContext;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.MatchAllDocsQuery;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.ScoreMode;\nimport org.apache.lucene.search.Scorer;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.BytesRefBuilder;\nimport org.apache.solr.client.solrj.SolrResponse;\nimport org.apache.solr.cloud.CloudDescriptor;\nimport org.apache.solr.cloud.ZkController;\nimport org.apache.solr.common.SolrDocument;\nimport org.apache.solr.common.SolrDocumentList;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.common.SolrException.ErrorCode;\nimport org.apache.solr.common.SolrInputDocument;\nimport org.apache.solr.common.StringUtils;\nimport org.apache.solr.common.cloud.ClusterState;\nimport org.apache.solr.common.cloud.DocCollection;\nimport org.apache.solr.common.cloud.Replica;\nimport org.apache.solr.common.cloud.Slice;\nimport org.apache.solr.common.params.CommonParams;\nimport org.apache.solr.common.params.ModifiableSolrParams;\nimport org.apache.solr.common.params.ShardParams;\nimport org.apache.solr.common.params.SolrParams;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.common.util.StrUtils;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.request.SolrQueryRequest;\nimport org.apache.solr.response.ResultContext;\nimport org.apache.solr.response.SolrQueryResponse;\nimport org.apache.solr.response.transform.DocTransformer;\nimport org.apache.solr.schema.FieldType;\nimport org.apache.solr.schema.IndexSchema;\nimport org.apache.solr.schema.SchemaField;\nimport org.apache.solr.search.DocList;\nimport org.apache.solr.search.QParser;\nimport org.apache.solr.search.ReturnFields;\nimport org.apache.solr.search.SolrDocumentFetcher;\nimport org.apache.solr.search.SolrIndexSearcher;\nimport org.apache.solr.search.SolrReturnFields;\nimport org.apache.solr.search.SyntaxError;\nimport org.apache.solr.update.CdcrUpdateLog;\nimport org.apache.solr.update.DocumentBuilder;\nimport org.apache.solr.update.IndexFingerprint;\nimport org.apache.solr.update.PeerSync;\nimport org.apache.solr.update.PeerSyncWithLeader;\nimport org.apache.solr.update.UpdateLog;\nimport org.apache.solr.util.RefCounted;\nimport org.apache.solr.util.TestInjection;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static org.apache.solr.common.params.CommonParams.DISTRIB;\nimport static org.apache.solr.common.params.CommonParams.ID;\nimport static org.apache.solr.common.params.CommonParams.VERSION_FIELD;\n\npublic class RealTimeGetComponent extends SearchComponent\n{\n  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n  public static final String COMPONENT_NAME = \"get\";\n\n  @Override\n  public void prepare(ResponseBuilder rb) throws IOException {\n    // Set field flags\n    ReturnFields returnFields = new SolrReturnFields( rb.req );\n    rb.rsp.setReturnFields( returnFields );\n  }\n\n\n  @Override\n  public void process(ResponseBuilder rb) throws IOException\n  {\n    SolrQueryRequest req = rb.req;\n    SolrQueryResponse rsp = rb.rsp;\n    SolrParams params = req.getParams();\n    CloudDescriptor cloudDesc = req.getCore().getCoreDescriptor().getCloudDescriptor();\n\n    if (cloudDesc != null) {\n      Replica.Type replicaType = cloudDesc.getReplicaType();\n      if (replicaType != null) {\n        if (replicaType == Replica.Type.PULL) {\n          throw new SolrException(ErrorCode.BAD_REQUEST, \n              String.format(Locale.ROOT, \"%s can't handle realtime get requests. Replicas of type %s do not support these type of requests\", \n                  cloudDesc.getCoreNodeName(),\n                  Replica.Type.PULL));\n        } \n        // non-leader TLOG replicas should not respond to distrib /get requests, but internal requests are OK\n      }\n    }\n    \n    if (!params.getBool(COMPONENT_NAME, true)) {\n      return;\n    }\n    \n    // This seems rather kludgey, may there is better way to indicate\n    // that replica can support handling version ranges\n    String val = params.get(\"checkCanHandleVersionRanges\");\n    if(val != null) {\n      rb.rsp.add(\"canHandleVersionRanges\", true);\n      return;\n    }\n    \n    val = params.get(\"getFingerprint\");\n    if(val != null) {\n      processGetFingeprint(rb);\n      return;\n    }\n    \n    val = params.get(\"getVersions\");\n    if (val != null) {\n      processGetVersions(rb);\n      return;\n    }\n\n    val = params.get(\"getUpdates\");\n    if (val != null) {\n      // solrcloud_debug\n      if (log.isDebugEnabled()) {\n        try {\n          RefCounted<SolrIndexSearcher> searchHolder = req.getCore()\n              .getNewestSearcher(false);\n          SolrIndexSearcher searcher = searchHolder.get();\n          try {\n            log.debug(req.getCore()\n                .getCoreContainer().getZkController().getNodeName()\n                + \" min count to sync to (from most recent searcher view) \"\n                + searcher.count(new MatchAllDocsQuery()));\n          } finally {\n            searchHolder.decref();\n          }\n        } catch (Exception e) {\n          log.debug(\"Error in solrcloud_debug block\", e);\n        }\n      }\n      \n      processGetUpdates(rb);\n      return;\n    }\n    \n    val = params.get(\"getInputDocument\");\n    if (val != null) {\n      processGetInputDocument(rb);\n      return;\n    }\n\n    final IdsRequsted reqIds = IdsRequsted.parseParams(req);\n    \n    if (reqIds.allIds.isEmpty()) {\n      return;\n    }\n\n    // parse any existing filters\n    try {\n      String[] fqs = req.getParams().getParams(CommonParams.FQ);\n      if (fqs!=null && fqs.length!=0) {\n        List<Query> filters = rb.getFilters();\n        // if filters already exists, make a copy instead of modifying the original\n        filters = filters == null ? new ArrayList<Query>(fqs.length) : new ArrayList<>(filters);\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0) {\n            QParser fqp = QParser.getParser(fq, req);\n            filters.add(fqp.getQuery());\n          }\n        }\n        if (!filters.isEmpty()) {\n          rb.setFilters( filters );\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    final SolrCore core = req.getCore();\n    SchemaField idField = core.getLatestSchema().getUniqueKeyField();\n    FieldType fieldType = idField.getType();\n\n    SolrDocumentList docList = new SolrDocumentList();\n    UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n\n    SearcherInfo searcherInfo =  new SearcherInfo(core);\n    \n    // this is initialized & set on the context *after* any searcher (re-)opening\n    ResultContext resultContext = null;\n    final DocTransformer transformer = rsp.getReturnFields().getTransformer();\n\n    // true in any situation where we have to use a realtime searcher rather then returning docs\n    // directly from the UpdateLog\n    final boolean mustUseRealtimeSearcher =\n      // if we have filters, we need to check those against the indexed form of the doc\n      (rb.getFilters() != null)\n      || ((null != transformer) && transformer.needsSolrIndexSearcher());\n\n   try {\n\n\n     BytesRefBuilder idBytes = new BytesRefBuilder();\n     for (String idStr : reqIds.allIds) {\n       fieldType.readableToIndexed(idStr, idBytes);\n       if (ulog != null) {\n         Object o = ulog.lookup(idBytes.get());\n         if (o != null) {\n           // should currently be a List<Oper,Ver,Doc/Id>\n           List entry = (List)o;\n           assert entry.size() >= 3;\n           int oper = (Integer)entry.get(UpdateLog.FLAGS_IDX) & UpdateLog.OPERATION_MASK;\n           switch (oper) {\n             case UpdateLog.UPDATE_INPLACE: // fall through to ADD\n             case UpdateLog.ADD:\n\n               if (mustUseRealtimeSearcher) {\n                 // close handles to current searchers & result context\n                 searcherInfo.clear();\n                 resultContext = null;\n                 ulog.openRealtimeSearcher();  // force open a new realtime searcher\n                 o = null;  // pretend we never found this record and fall through to use the searcher\n                 break;\n               }\n\n               SolrDocument doc;\n               if (oper == UpdateLog.ADD) {\n                 doc = toSolrDoc((SolrInputDocument)entry.get(entry.size()-1), core.getLatestSchema());\n               } else if (oper == UpdateLog.UPDATE_INPLACE) {\n                 if (ulog instanceof CdcrUpdateLog) {\n                   assert entry.size() == 6;\n                 } else {\n                   assert entry.size() == 5;\n                 }\n                 // For in-place update case, we have obtained the partial document till now. We need to\n                 // resolve it to a full document to be returned to the user.\n                 doc = resolveFullDocument(core, idBytes.get(), rsp.getReturnFields(), (SolrInputDocument)entry.get(entry.size()-1), entry, null);\n                 if (doc == null) {\n                   break; // document has been deleted as the resolve was going on\n                 }\n               } else {\n                 throw new SolrException(ErrorCode.INVALID_STATE, \"Expected ADD or UPDATE_INPLACE. Got: \" + oper);\n               }\n               if (transformer!=null) {\n                 transformer.transform(doc, -1); // unknown docID\n               }\n              docList.add(doc);\n              break;\n             case UpdateLog.DELETE:\n              break;\n             default:\n               throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,  \"Unknown Operation! \" + oper);\n           }\n           if (o != null) continue;\n         }\n       }\n\n       // didn't find it in the update log, so it should be in the newest searcher opened\n       searcherInfo.init();\n       // don't bother with ResultContext yet, we won't need it if doc doesn't match filters\n\n       int docid = -1;\n       long segAndId = searcherInfo.getSearcher().lookupId(idBytes.get());\n       if (segAndId >= 0) {\n         int segid = (int) segAndId;\n         LeafReaderContext ctx = searcherInfo.getSearcher().getTopReaderContext().leaves().get((int) (segAndId >> 32));\n         docid = segid + ctx.docBase;\n\n         if (rb.getFilters() != null) {\n           for (Query raw : rb.getFilters()) {\n             Query q = raw.rewrite(searcherInfo.getSearcher().getIndexReader());\n             Scorer scorer = searcherInfo.getSearcher().createWeight(q, ScoreMode.COMPLETE_NO_SCORES, 1f).scorer(ctx);\n             if (scorer == null || segid != scorer.iterator().advance(segid)) {\n               // filter doesn't match.\n               docid = -1;\n               break;\n             }\n           }\n         }\n       }\n\n       if (docid < 0) continue;\n       \n       Document luceneDocument = searcherInfo.getSearcher().doc(docid, rsp.getReturnFields().getLuceneFieldNames());\n       SolrDocument doc = toSolrDoc(luceneDocument,  core.getLatestSchema());\n       SolrDocumentFetcher docFetcher = searcherInfo.getSearcher().getDocFetcher();\n       docFetcher.decorateDocValueFields(doc, docid, docFetcher.getNonStoredDVs(true));\n       if ( null != transformer) {\n         if (null == resultContext) {\n           // either first pass, or we've re-opened searcher - either way now we setContext\n           resultContext = new RTGResultContext(rsp.getReturnFields(), searcherInfo.getSearcher(), req);\n           transformer.setContext(resultContext);\n         }\n         transformer.transform(doc, docid);\n       }\n       docList.add(doc);\n     }\n\n   } finally {\n     searcherInfo.clear();\n   }\n\n   addDocListToResponse(rb, docList);\n  }\n  \n  /**\n   * Return the requested SolrInputDocument from the tlog/index. This will\n   * always be a full document, i.e. any partial in-place document will be resolved.\n   */\n  void processGetInputDocument(ResponseBuilder rb) throws IOException {\n    SolrQueryRequest req = rb.req;\n    SolrQueryResponse rsp = rb.rsp;\n    SolrParams params = req.getParams();\n\n    if (!params.getBool(COMPONENT_NAME, true)) {\n      return;\n    }\n\n    String idStr = params.get(\"getInputDocument\", null);\n    if (idStr == null) return;\n    AtomicLong version = new AtomicLong();\n    SolrInputDocument doc = getInputDocument(req.getCore(), new BytesRef(idStr), version, false, null, true);\n    log.info(\"getInputDocument called for id=\"+idStr+\", returning: \"+doc);\n    rb.rsp.add(\"inputDocument\", doc);\n    rb.rsp.add(\"version\", version.get());\n  }\n\n  /**\n   * A SearcherInfo provides mechanism for obtaining RT searcher, from\n   * a SolrCore, and closing it, while taking care of the RefCounted references.\n   */\n  private static class SearcherInfo {\n    private RefCounted<SolrIndexSearcher> searcherHolder = null;\n    private SolrIndexSearcher searcher = null;\n    final SolrCore core;\n    \n    public SearcherInfo(SolrCore core) {\n      this.core = core;\n    }\n    \n    void clear(){\n      if (searcherHolder != null) {\n        // close handles to current searchers\n        searcher = null;\n        searcherHolder.decref();\n        searcherHolder = null;\n      }\n    }\n\n    void init(){\n      if (searcher == null) {\n        searcherHolder = core.getRealtimeSearcher();\n        searcher = searcherHolder.get();\n      }\n    }\n    \n    public SolrIndexSearcher getSearcher() {\n      assert null != searcher : \"init not called!\";\n      return searcher;\n    }\n  }\n\n  /***\n   * Given a partial document obtained from the transaction log (e.g. as a result of RTG), resolve to a full document\n   * by populating all the partial updates that were applied on top of that last full document update.\n   * \n   * @param onlyTheseFields When a non-null set of field names is passed in, the resolve process only attempts to populate\n   *        the given fields in this set. When this set is null, it resolves all fields.\n   * @return Returns the merged document, i.e. the resolved full document, or null if the document was not found (deleted\n   *          after the resolving began)\n   */\n  private static SolrDocument resolveFullDocument(SolrCore core, BytesRef idBytes,\n                                           ReturnFields returnFields, SolrInputDocument partialDoc, List logEntry, Set<String> onlyTheseFields) throws IOException {\n    if (idBytes == null || (logEntry.size() != 5 && logEntry.size() != 6)) {\n      throw new SolrException(ErrorCode.INVALID_STATE, \"Either Id field not present in partial document or log entry doesn't have previous version.\");\n    }\n    long prevPointer = (long) logEntry.get(UpdateLog.PREV_POINTER_IDX);\n    long prevVersion = (long) logEntry.get(UpdateLog.PREV_VERSION_IDX);\n\n    // get the last full document from ulog\n    UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n    long lastPrevPointer = ulog.applyPartialUpdates(idBytes, prevPointer, prevVersion, onlyTheseFields, partialDoc);\n\n    if (lastPrevPointer == -1) { // full document was not found in tlog, but exists in index\n      SolrDocument mergedDoc = mergePartialDocWithFullDocFromIndex(core, idBytes, returnFields, onlyTheseFields, partialDoc);\n      return mergedDoc;\n    } else if (lastPrevPointer > 0) {\n      // We were supposed to have found the last full doc also in the tlogs, but the prevPointer links led to nowhere\n      // We should reopen a new RT searcher and get the doc. This should be a rare occurrence\n      Term idTerm = new Term(core.getLatestSchema().getUniqueKeyField().getName(), idBytes);\n      SolrDocument mergedDoc = reopenRealtimeSearcherAndGet(core, idTerm, returnFields);\n      if (mergedDoc == null) {\n        return null; // the document may have been deleted as the resolving was going on.\n      }\n      return mergedDoc;\n    } else { // i.e. lastPrevPointer==0\n      assert lastPrevPointer == 0;\n      // We have successfully resolved the document based off the tlogs\n      return toSolrDoc(partialDoc, core.getLatestSchema());\n    }\n  }\n\n  /**\n   * Re-open the RT searcher and get the document, referred to by the idTerm, from that searcher. \n   * @return Returns the document or null if not found.\n   */\n  private static SolrDocument reopenRealtimeSearcherAndGet(SolrCore core, Term idTerm, ReturnFields returnFields) throws IOException {\n    UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n    ulog.openRealtimeSearcher();\n    RefCounted<SolrIndexSearcher> searcherHolder = core.getRealtimeSearcher();\n    try {\n      SolrIndexSearcher searcher = searcherHolder.get();\n\n      int docid = searcher.getFirstMatch(idTerm);\n      if (docid < 0) {\n        return null;\n      }\n      Document luceneDocument = searcher.doc(docid, returnFields.getLuceneFieldNames());\n      SolrDocument doc = toSolrDoc(luceneDocument, core.getLatestSchema());\n      SolrDocumentFetcher docFetcher = searcher.getDocFetcher();\n      docFetcher.decorateDocValueFields(doc, docid, docFetcher.getNonStoredDVs(false));\n\n      return doc;\n    } finally {\n      searcherHolder.decref();\n    }\n  }\n\n  /**\n   * Gets a document from the index by id. If a non-null partial document (for in-place update) is passed in,\n   * this method obtains the document from the tlog/index by the given id, merges the partial document on top of it and then returns\n   * the resultant document.\n   *\n   * @param core           A SolrCore instance, useful for obtaining a realtimesearcher and the schema\n   * @param idBytes        Binary representation of the value of the unique key field\n   * @param returnFields   Return fields, as requested\n   * @param onlyTheseFields When a non-null set of field names is passed in, the merge process only attempts to merge\n   *        the given fields in this set. When this set is null, it merges all fields.\n   * @param partialDoc     A partial document (containing an in-place update) used for merging against a full document\n   *                       from index; this maybe be null.\n   * @return If partial document is null, this returns document from the index or null if not found. \n   *         If partial document is not null, this returns a document from index merged with the partial document, or null if\n   *         document doesn't exist in the index.\n   */\n  private static SolrDocument mergePartialDocWithFullDocFromIndex(SolrCore core, BytesRef idBytes, ReturnFields returnFields,\n             Set<String> onlyTheseFields, SolrInputDocument partialDoc) throws IOException {\n    RefCounted<SolrIndexSearcher> searcherHolder = core.getRealtimeSearcher(); //Searcher();\n    try {\n      // now fetch last document from index, and merge partialDoc on top of it\n      SolrIndexSearcher searcher = searcherHolder.get();\n      SchemaField idField = core.getLatestSchema().getUniqueKeyField();\n      Term idTerm = new Term(idField.getName(), idBytes);\n\n      int docid = searcher.getFirstMatch(idTerm);\n      if (docid < 0) {\n        // The document was not found in index! Reopen a new RT searcher (to be sure) and get again.\n        // This should be because the document was deleted recently.\n        SolrDocument doc = reopenRealtimeSearcherAndGet(core, idTerm, returnFields);\n        if (doc == null) {\n          // Unable to resolve the last full doc in tlog fully,\n          // and document not found in index even after opening new rt searcher.\n          // This must be a case of deleted doc\n          return null;\n        }\n        return doc;\n      }\n\n      SolrDocument doc;\n      Set<String> decorateFields = onlyTheseFields == null ? searcher.getDocFetcher().getNonStoredDVs(false): onlyTheseFields;\n      Document luceneDocument = searcher.doc(docid, returnFields.getLuceneFieldNames());\n      doc = toSolrDoc(luceneDocument, core.getLatestSchema());\n      searcher.getDocFetcher().decorateDocValueFields(doc, docid, decorateFields);\n\n      long docVersion = (long) doc.getFirstValue(VERSION_FIELD);\n      Object partialVersionObj = partialDoc.getFieldValue(VERSION_FIELD);\n      long partialDocVersion = partialVersionObj instanceof Field? ((Field) partialVersionObj).numericValue().longValue():\n        partialVersionObj instanceof Number? ((Number) partialVersionObj).longValue(): Long.parseLong(partialVersionObj.toString());\n      if (docVersion > partialDocVersion) {\n        return doc;\n      }\n      for (String fieldName: partialDoc.getFieldNames()) {\n        doc.setField(fieldName.toString(), partialDoc.getFieldValue(fieldName));  // since partial doc will only contain single valued fields, this is fine\n      }\n\n      return doc;\n    } finally {\n      if (searcherHolder != null) {\n        searcherHolder.decref();\n      }\n    }\n  }\n\n  public static SolrInputDocument DELETED = new SolrInputDocument();\n\n  /** returns the SolrInputDocument from the current tlog, or DELETED if it has been deleted, or\n   * null if there is no record of it in the current update log.  If null is returned, it could\n   * still be in the latest index.\n   * @param versionReturned If a non-null AtomicLong is passed in, it is set to the version of the update returned from the TLog.\n   * @param resolveFullDocument In case the document is fetched from the tlog, it could only be a partial document if the last update\n   *                  was an in-place update. In that case, should this partial document be resolved to a full document (by following\n   *                  back prevPointer/prevVersion)?\n   */\n  public static SolrInputDocument getInputDocumentFromTlog(SolrCore core, BytesRef idBytes, AtomicLong versionReturned,\n      Set<String> onlyTheseNonStoredDVs, boolean resolveFullDocument) {\n\n    UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n\n    if (ulog != null) {\n      Object o = ulog.lookup(idBytes);\n      if (o != null) {\n        // should currently be a List<Oper,Ver,Doc/Id>\n        List entry = (List)o;\n        assert entry.size() >= 3;\n        int oper = (Integer)entry.get(0) & UpdateLog.OPERATION_MASK;\n        if (versionReturned != null) {\n          versionReturned.set((long)entry.get(UpdateLog.VERSION_IDX));\n        }\n        switch (oper) {\n          case UpdateLog.UPDATE_INPLACE:\n            if (ulog instanceof CdcrUpdateLog) {\n              assert entry.size() == 6;\n            } else {\n              assert entry.size() == 5;\n            }\n\n            if (resolveFullDocument) {\n              SolrInputDocument doc = (SolrInputDocument)entry.get(entry.size()-1);\n              try {\n                // For in-place update case, we have obtained the partial document till now. We need to\n                // resolve it to a full document to be returned to the user.\n                SolrDocument sdoc = resolveFullDocument(core, idBytes, new SolrReturnFields(), doc, entry, onlyTheseNonStoredDVs);\n                if (sdoc == null) {\n                  return DELETED;\n                }\n                doc = toSolrInputDocument(sdoc, core.getLatestSchema());\n                return doc;\n              } catch (IOException ex) {\n                throw new SolrException(ErrorCode.SERVER_ERROR, \"Error while resolving full document. \", ex);\n              }\n            } else {\n              // fall through to ADD, so as to get only the partial document\n            }\n          case UpdateLog.ADD:\n            return (SolrInputDocument) entry.get(entry.size()-1);\n          case UpdateLog.DELETE:\n            return DELETED;\n          default:\n            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,  \"Unknown Operation! \" + oper);\n        }\n      }\n    }\n\n    return null;\n  }\n\n  /**\n   * Obtains the latest document for a given id from the tlog or index (if not found in the tlog).\n   * \n   * NOTE: This method uses the effective value for avoidRetrievingStoredFields param as false and\n   * for nonStoredDVs as null in the call to @see {@link RealTimeGetComponent#getInputDocument(SolrCore, BytesRef, AtomicLong, boolean, Set, boolean)},\n   * so as to retrieve all stored and non-stored DV fields from all documents. Also, it uses the effective value of\n   * resolveFullDocument param as true, i.e. it resolves any partial documents (in-place updates), in case the \n   * document is fetched from the tlog, to a full document.\n   */\n  public static SolrInputDocument getInputDocument(SolrCore core, BytesRef idBytes) throws IOException {\n    return getInputDocument (core, idBytes, null, false, null, true);\n  }\n  \n  /**\n   * Obtains the latest document for a given id from the tlog or through the realtime searcher (if not found in the tlog). \n   * @param versionReturned If a non-null AtomicLong is passed in, it is set to the version of the update returned from the TLog.\n   * @param avoidRetrievingStoredFields Setting this to true avoids fetching stored fields through the realtime searcher,\n   *                  however has no effect on documents obtained from the tlog. \n   *                  Non-stored docValues fields are populated anyway, and are not affected by this parameter. Note that if\n   *                  the id field is a stored field, it will not be populated if this parameter is true and the document is\n   *                  obtained from the index.\n   * @param onlyTheseNonStoredDVs If not-null, populate only these DV fields in the document fetched through the realtime searcher. \n   *                  If this is null, decorate all non-stored  DVs (that are not targets of copy fields) from the searcher.\n   * @param resolveFullDocument In case the document is fetched from the tlog, it could only be a partial document if the last update\n   *                  was an in-place update. In that case, should this partial document be resolved to a full document (by following\n   *                  back prevPointer/prevVersion)?\n   */\n  public static SolrInputDocument getInputDocument(SolrCore core, BytesRef idBytes, AtomicLong versionReturned, boolean avoidRetrievingStoredFields,\n      Set<String> onlyTheseNonStoredDVs, boolean resolveFullDocument) throws IOException {\n    SolrInputDocument sid = null;\n    RefCounted<SolrIndexSearcher> searcherHolder = null;\n    try {\n      SolrIndexSearcher searcher = null;\n      sid = getInputDocumentFromTlog(core, idBytes, versionReturned, onlyTheseNonStoredDVs, resolveFullDocument);\n      if (sid == DELETED) {\n        return null;\n      }\n\n      if (sid == null) {\n        // didn't find it in the update log, so it should be in the newest searcher opened\n        if (searcher == null) {\n          searcherHolder = core.getRealtimeSearcher();\n          searcher = searcherHolder.get();\n        }\n\n        // SolrCore.verbose(\"RealTimeGet using searcher \", searcher);\n        SchemaField idField = core.getLatestSchema().getUniqueKeyField();\n\n        int docid = searcher.getFirstMatch(new Term(idField.getName(), idBytes));\n        if (docid < 0) return null;\n\n        SolrDocumentFetcher docFetcher = searcher.getDocFetcher();\n        if (avoidRetrievingStoredFields) {\n          sid = new SolrInputDocument();\n        } else {\n          Document luceneDocument = docFetcher.doc(docid);\n          sid = toSolrInputDocument(luceneDocument, core.getLatestSchema());\n        }\n        if (onlyTheseNonStoredDVs != null) {\n          docFetcher.decorateDocValueFields(sid, docid, onlyTheseNonStoredDVs);\n        } else {\n          docFetcher.decorateDocValueFields(sid, docid, docFetcher.getNonStoredDVsWithoutCopyTargets());\n        }\n      }\n    } finally {\n      if (searcherHolder != null) {\n        searcherHolder.decref();\n      }\n    }\n\n    if (versionReturned != null) {\n      if (sid.containsKey(VERSION_FIELD)) {\n        versionReturned.set((long)sid.getFieldValue(VERSION_FIELD));\n      }\n    }\n    return sid;\n  }\n\n  private static SolrInputDocument toSolrInputDocument(Document doc, IndexSchema schema) {\n    SolrInputDocument out = new SolrInputDocument();\n    for( IndexableField f : doc.getFields() ) {\n      String fname = f.name();\n      SchemaField sf = schema.getFieldOrNull(f.name());\n      Object val = null;\n      if (sf != null) {\n        if ((!sf.hasDocValues() && !sf.stored()) || schema.isCopyFieldTarget(sf)) continue;\n        val = sf.getType().toObject(f);   // object or external string?\n      } else {\n        val = f.stringValue();\n        if (val == null) val = f.numericValue();\n        if (val == null) val = f.binaryValue();\n        if (val == null) val = f;\n      }\n\n      // todo: how to handle targets of copy fields (including polyfield sub-fields)?\n      out.addField(fname, val);\n    }\n    return out;\n  }\n\n  private static SolrInputDocument toSolrInputDocument(SolrDocument doc, IndexSchema schema) {\n    SolrInputDocument out = new SolrInputDocument();\n    for( String fname : doc.getFieldNames() ) {\n      SchemaField sf = schema.getFieldOrNull(fname);\n      if (sf != null) {\n        if ((!sf.hasDocValues() && !sf.stored()) || schema.isCopyFieldTarget(sf)) continue;\n      }\n      for (Object val: doc.getFieldValues(fname)) {\n        if (val instanceof Field) {\n          Field f = (Field) val;\n          if (sf != null) {\n            val = sf.getType().toObject(f);   // object or external string?\n          } else {\n            val = f.stringValue();\n            if (val == null) val = f.numericValue();\n            if (val == null) val = f.binaryValue();\n            if (val == null) val = f;\n          }\n        }\n        out.addField(fname, val);\n      }\n    }\n    return out;\n  }\n\n  private static SolrDocument toSolrDoc(Document doc, IndexSchema schema) {\n    SolrDocument out = new SolrDocument();\n    for( IndexableField f : doc.getFields() ) {\n      // Make sure multivalued fields are represented as lists\n      Object existing = out.get(f.name());\n      if (existing == null) {\n        SchemaField sf = schema.getFieldOrNull(f.name());\n\n        // don't return copyField targets\n        if (sf != null && schema.isCopyFieldTarget(sf)) continue;\n\n        if (sf != null && sf.multiValued()) {\n          List<Object> vals = new ArrayList<>();\n          if (f.fieldType().docValuesType() == DocValuesType.SORTED_NUMERIC) {\n            // SORTED_NUMERICS store sortable bits version of the value, need to retrieve the original\n            vals.add(sf.getType().toObject(f)); // (will materialize by side-effect)\n          } else {\n            vals.add( materialize(f) );\n          }\n          out.setField( f.name(), vals );\n        }\n        else{\n          out.setField( f.name(), materialize(f) );\n        }\n      }\n      else {\n        out.addField( f.name(), materialize(f) );\n      }\n    }\n    return out;\n  }\n\n  /**\n   * Ensure we don't have {@link org.apache.lucene.document.LazyDocument.LazyField} or equivalent.\n   * It can pose problems if the searcher is about to be closed and we haven't fetched a value yet.\n   */\n  private static IndexableField materialize(IndexableField in) {\n    if (in instanceof Field) { // already materialized\n      return in;\n    }\n    return new ClonedField(in);\n  }\n\n  private static class ClonedField extends Field { // TODO Lucene Field has no copy constructor; maybe it should?\n    ClonedField(IndexableField in) {\n      super(in.name(), in.fieldType());\n      this.fieldsData = in.numericValue();\n      if (this.fieldsData == null) {\n        this.fieldsData = in.binaryValue();\n        if (this.fieldsData == null) {\n          this.fieldsData = in.stringValue();\n          if (this.fieldsData == null) {\n            // fallback:\n            assert false : in; // unexpected\n          }\n        }\n      }\n    }\n  }\n\n  /**\n   * Converts a SolrInputDocument to SolrDocument, using an IndexSchema instance. \n   * @lucene.experimental\n   */\n  public static SolrDocument toSolrDoc(SolrInputDocument sdoc, IndexSchema schema) {\n    // TODO what about child / nested docs?\n    // TODO: do something more performant than this double conversion\n    Document doc = DocumentBuilder.toDocument(sdoc, schema);\n\n    // copy the stored fields only\n    Document out = new Document();\n    for (IndexableField f : doc.getFields()) {\n      if (f.fieldType().stored()) {\n        out.add(f);\n      } else if (f.fieldType().docValuesType() != DocValuesType.NONE) {\n        SchemaField schemaField = schema.getFieldOrNull(f.name());\n        if (schemaField != null && !schemaField.stored() && schemaField.useDocValuesAsStored()) {\n          out.add(f);\n        }\n      } else {\n        log.debug(\"Don't know how to handle field {}\", f);\n      }\n    }\n\n    return toSolrDoc(out, schema);\n  }\n\n  @Override\n  public int distributedProcess(ResponseBuilder rb) throws IOException {\n    if (rb.stage < ResponseBuilder.STAGE_GET_FIELDS)\n      return ResponseBuilder.STAGE_GET_FIELDS;\n    if (rb.stage == ResponseBuilder.STAGE_GET_FIELDS) {\n      return createSubRequests(rb);\n    }\n    return ResponseBuilder.STAGE_DONE;\n  }\n\n  public int createSubRequests(ResponseBuilder rb) throws IOException {\n    \n    final IdsRequsted reqIds = IdsRequsted.parseParams(rb.req);\n    if (reqIds.allIds.isEmpty()) {\n      return ResponseBuilder.STAGE_DONE;\n    }\n    \n    SolrParams params = rb.req.getParams();\n\n    // TODO: handle collection=...?\n\n    ZkController zkController = rb.req.getCore().getCoreContainer().getZkController();\n\n    // if shards=... then use that\n    if (zkController != null && params.get(ShardParams.SHARDS) == null) {\n      CloudDescriptor cloudDescriptor = rb.req.getCore().getCoreDescriptor().getCloudDescriptor();\n\n      String collection = cloudDescriptor.getCollectionName();\n      ClusterState clusterState = zkController.getClusterState();\n      DocCollection coll = clusterState.getCollection(collection);\n\n\n      Map<String, List<String>> sliceToId = new HashMap<>();\n      for (String id : reqIds.allIds) {\n        Slice slice = coll.getRouter().getTargetSlice(id, null, null, params, coll);\n\n        List<String> idsForShard = sliceToId.get(slice.getName());\n        if (idsForShard == null) {\n          idsForShard = new ArrayList<>(2);\n          sliceToId.put(slice.getName(), idsForShard);\n        }\n        idsForShard.add(id);\n      }\n\n      for (Map.Entry<String,List<String>> entry : sliceToId.entrySet()) {\n        String shard = entry.getKey();\n\n        ShardRequest sreq = createShardRequest(rb, entry.getValue());\n        // sreq.shards = new String[]{shard};    // TODO: would be nice if this would work...\n        sreq.shards = sliceToShards(rb, collection, shard);\n        sreq.actualShards = sreq.shards;\n        \n        rb.addRequest(this, sreq);\n      }      \n    } else {\n      ShardRequest sreq = createShardRequest(rb, reqIds.allIds);\n      sreq.shards = null;  // ALL\n      sreq.actualShards = sreq.shards;\n\n      rb.addRequest(this, sreq);\n    }\n\n    return ResponseBuilder.STAGE_DONE;\n  }\n\n  /**\n   * Helper method for creating a new ShardRequest for the specified ids, based on the params \n   * specified for the current request.  The new ShardRequest does not yet know anything about \n   * which shard/slice it will be sent to.\n   */\n  private ShardRequest createShardRequest(final ResponseBuilder rb, final List<String> ids) {\n    final ShardRequest sreq = new ShardRequest();\n    sreq.purpose = 1;\n    sreq.params = new ModifiableSolrParams(rb.req.getParams());\n\n    // TODO: how to avoid hardcoding this and hit the same handler?\n    sreq.params.set(ShardParams.SHARDS_QT,\"/get\");      \n    sreq.params.set(DISTRIB,false);\n\n    sreq.params.remove(ShardParams.SHARDS);\n    sreq.params.remove(ID);\n    sreq.params.remove(\"ids\");\n    sreq.params.set(\"ids\", StrUtils.join(ids, ','));\n    \n    return sreq;\n  }\n  \n  private String[] sliceToShards(ResponseBuilder rb, String collection, String slice) {\n    String lookup = collection + '_' + slice;  // seems either form may be filled in rb.slices?\n    \n    // We use this since the shard handler already filled in the slice to shards mapping.\n    // A better approach would be to avoid filling out every slice each time, or to cache\n    // the mappings.\n\n    for (int i=0; i<rb.slices.length; i++) {\n      log.info(\"LOOKUP_SLICE:\" + rb.slices[i] + \"=\" + rb.shards[i]);\n      if (lookup.equals(rb.slices[i]) || slice.equals(rb.slices[i])) {\n        return new String[]{rb.shards[i]};\n      }\n    }\n\n\n    throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Can't find shard '\" + lookup + \"'\");\n  }\n\n  /***\n  private void handleRegularResponses(ResponseBuilder rb, ShardRequest sreq) {\n  }\n  ***/\n\n  @Override\n  public void finishStage(ResponseBuilder rb) {\n    if (rb.stage != ResponseBuilder.STAGE_GET_FIELDS) {\n      return;\n    }\n    \n    mergeResponses(rb);\n  }\n  \n  private void mergeResponses(ResponseBuilder rb) {\n    SolrDocumentList docList = new SolrDocumentList();\n    \n    for (ShardRequest sreq : rb.finished) {\n      // if shards=shard1,shard2 was used, then  we query both shards for each id and\n      // can get more than one response\n      for (ShardResponse srsp : sreq.responses) {\n        SolrResponse sr = srsp.getSolrResponse();\n        NamedList nl = sr.getResponse();\n        SolrDocumentList subList = (SolrDocumentList)nl.get(\"response\");\n        docList.addAll(subList);\n      }\n    }\n    \n    addDocListToResponse(rb, docList);\n  }\n\n  /**\n   * Encapsulates logic for how a {@link SolrDocumentList} should be added to the response\n   * based on the request params used\n   */\n  private void addDocListToResponse(final ResponseBuilder rb, final SolrDocumentList docList) {\n    assert null != docList;\n    \n    final SolrQueryResponse rsp = rb.rsp;\n    final IdsRequsted reqIds = IdsRequsted.parseParams(rb.req);\n    \n    if (reqIds.useSingleDocResponse) {\n      assert docList.size() <= 1;\n      // if the doc was not found, then use a value of null.\n      rsp.add(\"doc\", docList.size() > 0 ? docList.get(0) : null);\n    } else {\n      docList.setNumFound(docList.size());\n      rsp.addResponse(docList);\n    }\n  }\n\n                                                                                               \n\n  ////////////////////////////////////////////\n  ///  SolrInfoBean\n  ////////////////////////////////////////////\n\n  @Override\n  public String getDescription() {\n    return \"query\";\n  }\n\n  @Override\n  public Category getCategory() {\n    return Category.QUERY;\n  }\n\n  public void processGetFingeprint(ResponseBuilder rb) throws IOException {\n    TestInjection.injectFailIndexFingerprintRequests();\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    long maxVersion = params.getLong(\"getFingerprint\", Long.MAX_VALUE);\n    if (TestInjection.injectWrongIndexFingerprint())  {\n      maxVersion = -1;\n    }\n    IndexFingerprint fingerprint = IndexFingerprint.getFingerprint(req.getCore(), Math.abs(maxVersion));\n    rb.rsp.add(\"fingerprint\", fingerprint);\n  }\n  \n\n  ///////////////////////////////////////////////////////////////////////////////////\n  // Returns last versions added to index\n  ///////////////////////////////////////////////////////////////////////////////////\n\n\n  public void processGetVersions(ResponseBuilder rb) throws IOException\n  {\n    SolrQueryRequest req = rb.req;\n    SolrQueryResponse rsp = rb.rsp;\n    SolrParams params = req.getParams();\n\n    if (!params.getBool(COMPONENT_NAME, true)) {\n      return;\n    }\n\n    int nVersions = params.getInt(\"getVersions\", -1);\n    if (nVersions == -1) return;\n\n    boolean doFingerprint = params.getBool(\"fingerprint\", false);\n\n    String sync = params.get(\"sync\");\n    if (sync != null) {\n      processSync(rb, nVersions, sync);\n      return;\n    }\n\n    UpdateLog ulog = req.getCore().getUpdateHandler().getUpdateLog();\n    if (ulog == null) return;\n    String syncWithLeader = params.get(\"syncWithLeader\");\n    if (syncWithLeader != null) {\n      List<Long> versions;\n      try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n        versions = recentUpdates.getVersions(nVersions);\n      }\n      processSyncWithLeader(rb, nVersions, syncWithLeader, versions);\n      return;\n    }\n\n    // get fingerprint first as it will cause a soft commit\n    // and would avoid mismatch if documents are being actively index especially during PeerSync\n    if (doFingerprint) {\n      IndexFingerprint fingerprint = IndexFingerprint.getFingerprint(req.getCore(), Long.MAX_VALUE);\n      rb.rsp.add(\"fingerprint\", fingerprint);\n    }\n\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      List<Long> versions = recentUpdates.getVersions(nVersions);\n      rb.rsp.add(\"versions\", versions);\n    }\n  }\n\n  public void processSyncWithLeader(ResponseBuilder rb, int nVersions, String syncWithLeader, List<Long> versions) {\n    PeerSyncWithLeader peerSync = new PeerSyncWithLeader(rb.req.getCore(), syncWithLeader, nVersions);\n    boolean success = peerSync.sync(versions).isSuccess();\n    rb.rsp.add(\"syncWithLeader\", success);\n  }\n\n  \n  public void processSync(ResponseBuilder rb, int nVersions, String sync) {\n    \n    boolean onlyIfActive = rb.req.getParams().getBool(\"onlyIfActive\", false);\n    \n    if (onlyIfActive) {\n      if (rb.req.getCore().getCoreDescriptor().getCloudDescriptor().getLastPublished() != Replica.State.ACTIVE) {\n        log.info(\"Last published state was not ACTIVE, cannot sync.\");\n        rb.rsp.add(\"sync\", \"false\");\n        return;\n      }\n    }\n    \n    List<String> replicas = StrUtils.splitSmart(sync, \",\", true);\n    \n    boolean cantReachIsSuccess = rb.req.getParams().getBool(\"cantReachIsSuccess\", false);\n    \n    PeerSync peerSync = new PeerSync(rb.req.getCore(), replicas, nVersions, cantReachIsSuccess);\n    boolean success = peerSync.sync().isSuccess();\n    \n    // TODO: more complex response?\n    rb.rsp.add(\"sync\", success);\n  }\n  \n\n  public void processGetUpdates(ResponseBuilder rb) throws IOException\n  {\n    SolrQueryRequest req = rb.req;\n    SolrQueryResponse rsp = rb.rsp;\n    SolrParams params = req.getParams();\n\n    if (!params.getBool(COMPONENT_NAME, true)) {\n      return;\n    }\n\n    String versionsStr = params.get(\"getUpdates\");\n    if (versionsStr == null) return;\n\n    UpdateLog ulog = req.getCore().getUpdateHandler().getUpdateLog();\n    if (ulog == null) return;\n\n    // handle version ranges\n    List<Long> versions = null;\n    if (versionsStr.indexOf(\"...\") != -1) {\n      versions = resolveVersionRanges(versionsStr, ulog);\n    } else {\n      versions = StrUtils.splitSmart(versionsStr, \",\", true).stream().map(Long::parseLong)\n          .collect(Collectors.toList());\n    }\n\n    // find fingerprint for max version for which updates are requested\n    boolean doFingerprint = params.getBool(\"fingerprint\", false);\n    if (doFingerprint) {\n      long maxVersionForUpdate = Collections.min(versions, PeerSync.absComparator);\n      IndexFingerprint fingerprint = IndexFingerprint.getFingerprint(req.getCore(), Math.abs(maxVersionForUpdate));\n      rb.rsp.add(\"fingerprint\", fingerprint);\n    }\n\n    List<Object> updates = new ArrayList<>(versions.size());\n\n    long minVersion = Long.MAX_VALUE;\n\n    // TODO: get this from cache instead of rebuilding?\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      for (Long version : versions) {\n        try {\n          Object o = recentUpdates.lookup(version);\n          if (o == null) continue;\n\n          if (version > 0) {\n            minVersion = Math.min(minVersion, version);\n          }\n\n          // TODO: do any kind of validation here?\n          updates.add(o);\n\n        } catch (SolrException | ClassCastException e) {\n          log.warn(\"Exception reading log for updates\", e);\n        }\n      }\n\n      // Must return all delete-by-query commands that occur after the first add requested\n      // since they may apply.\n      if (params.getBool(\"skipDbq\", false)) {\n        updates.addAll(recentUpdates.getDeleteByQuery(minVersion));\n      }\n\n      rb.rsp.add(\"updates\", updates);\n\n    }\n  }\n  \n  \n  private List<Long> resolveVersionRanges(String versionsStr, UpdateLog ulog) {\n    if (StringUtils.isEmpty(versionsStr)) {\n      return Collections.emptyList();\n    }\n    \n    List<String> ranges = StrUtils.splitSmart(versionsStr, \",\", true);\n    \n    // TODO merge ranges.\n    \n    // get all the versions from updatelog and sort them\n    List<Long> versionAvailable = null;\n    try (UpdateLog.RecentUpdates recentUpdates = ulog.getRecentUpdates()) {\n      versionAvailable = recentUpdates.getVersions(ulog.getNumRecordsToKeep());\n    }\n    // sort versions\n    Collections.sort(versionAvailable, PeerSync.absComparator);\n    \n    // This can be done with single pass over both ranges and versionsAvailable, that would require \n    // merging ranges. We currently use Set to ensure there are no duplicates.\n    Set<Long> versionsToRet = new HashSet<>(ulog.getNumRecordsToKeep());\n    for (String range : ranges) {\n      String[] rangeBounds = range.split(\"\\\\.{3}\");\n      int indexStart = Collections.binarySearch(versionAvailable, Long.valueOf(rangeBounds[1]), PeerSync.absComparator);\n      int indexEnd = Collections.binarySearch(versionAvailable, Long.valueOf(rangeBounds[0]), PeerSync.absComparator); \n      if(indexStart >=0 && indexEnd >= 0) {\n        versionsToRet.addAll(versionAvailable.subList(indexStart, indexEnd + 1)); // indexEnd is exclusive\n      }\n    }\n    // TODO do we need to sort versions using PeerSync.absComparator?\n    return new ArrayList<>(versionsToRet);\n  }\n\n  /** \n   * Simple struct for tracking what ids were requested and what response format is expected \n   * acording to the request params\n   */\n  private final static class IdsRequsted {\n    /** An List (which may be empty but will never be null) of the uniqueKeys requested. */\n    public final List<String> allIds;\n    /** \n     * true if the params provided by the user indicate that a single doc response structure \n     * should be used.  \n     * Value is meaninless if <code>ids</code> is empty.\n     */\n    public final boolean useSingleDocResponse;\n    private IdsRequsted(List<String> allIds, boolean useSingleDocResponse) {\n      assert null != allIds;\n      this.allIds = allIds;\n      this.useSingleDocResponse = useSingleDocResponse;\n    }\n    \n    /**\n     * Parsers the <code>id</code> and <code>ids</code> params attached to the specified request object, \n     * and returns an <code>IdsRequsted</code> struct to use for this request.\n     * The <code>IdsRequsted</code> is cached in the {@link SolrQueryRequest#getContext} so subsequent \n     * method calls on the same request will not re-parse the params.\n     */\n    public static IdsRequsted parseParams(SolrQueryRequest req) {\n      final String contextKey = IdsRequsted.class.toString() + \"_PARSED_ID_PARAMS\";\n      if (req.getContext().containsKey(contextKey)) {\n        return (IdsRequsted)req.getContext().get(contextKey);\n      }\n      final SolrParams params = req.getParams();\n      final String id[] = params.getParams(ID);\n      final String ids[] = params.getParams(\"ids\");\n      \n      if (id == null && ids == null) {\n        IdsRequsted result = new IdsRequsted(Collections.<String>emptyList(), true);\n        req.getContext().put(contextKey, result);\n        return result;\n      }\n      final List<String> allIds = new ArrayList<>((null == id ? 0 : id.length)\n                                                  + (null == ids ? 0 : (2 * ids.length)));\n      if (null != id) {\n        for (String singleId : id) {\n          allIds.add(singleId);\n        }\n      }\n      if (null != ids) {\n        for (String idList : ids) {\n          allIds.addAll( StrUtils.splitSmart(idList, \",\", true) );\n        }\n      }\n      // if the client specified a single id=foo, then use \"doc\":{\n      // otherwise use a standard doclist\n      IdsRequsted result = new IdsRequsted(allIds, (ids == null && allIds.size() <= 1));\n      req.getContext().put(contextKey, result);\n      return result;\n    }\n  }\n\n  \n  /**\n   * A lite weight ResultContext for use with RTG requests that can point at Realtime Searchers\n   */\n  private static final class RTGResultContext extends ResultContext {\n    final ReturnFields returnFields;\n    final SolrIndexSearcher searcher;\n    final SolrQueryRequest req;\n    public RTGResultContext(ReturnFields returnFields, SolrIndexSearcher searcher, SolrQueryRequest req) {\n      this.returnFields = returnFields;\n      this.searcher = searcher;\n      this.req = req;\n    }\n    \n    /** @returns null */\n    public DocList getDocList() {\n      return null;\n    }\n    \n    public ReturnFields getReturnFields() {\n      return this.returnFields;\n    }\n    \n    public SolrIndexSearcher getSearcher() {\n      return this.searcher;\n    }\n    \n    /** @returns null */\n    public Query getQuery() {\n      return null;\n    }\n    \n    public SolrQueryRequest getRequest() {\n      return this.req;\n    }\n    \n    /** @returns null */\n    public Iterator<SolrDocument> getProcessedDocuments() {\n      return null;\n    }\n  }\n  \n}\n", "idx": 11, "id": 27717, "msg": "", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -1428,6 +1428,23 @@ class GlobalCommands(ScriptableObject):\n \tscript_toggleMouseTracking.__doc__=_(\"Toggles the reporting of information as the mouse moves\")\n \tscript_toggleMouseTracking.category=SCRCAT_MOUSE\n \n+\tdef script_toggleMouseTextResolution(self,gesture):\n+\t\tvalues = textInfos.MOUSE_TEXT_RESOLUTION_UNITS\n+\t\tlabels = [textInfos.unitLabels[x] for x in values]\n+\t\ttry:\n+\t\t\tindex = values.index(config.conf[\"mouse\"][\"mouseTextUnit\"])\n+\t\texcept:\n+\t\t\tindex=0\n+\t\tnewIndex = (index+1) % len(values)\n+\t\tconfig.conf[\"mouse\"][\"mouseTextUnit\"]= values[newIndex]\n+\t\t# Translators: Reports the new state of the mouse text unit resolution:.\n+\t\t# %s will be replaced with the new label.\n+\t\t# For example, the full message might be \"Mouse text unit resolution character\"\n+\t\tui.message(_(\"Mouse text unit resolution %s\")%labels[newIndex])\n+\t# Translators: Input help mode message for toggle mouse text unit resolution command.\n+\tscript_toggleMouseTextResolution.__doc__=_(\"Toggles how much text will be spoken when the mouse moves\")\n+\tscript_toggleMouseTextResolution.category=SCRCAT_MOUSE\n+\n \tdef script_title(self,gesture):\n \t\tobj=api.getForegroundObject()\n \t\ttitle=obj.name", "y": 1, "oldf": "# -*- coding: UTF-8 -*-\r\n#globalCommands.py\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n#Copyright (C) 2006-2018 NV Access Limited, Peter V\u00e1gner, Aleksey Sadovoy, Rui Batista, Joseph Lee, Leonard de Ruijter, Derek Riemer, Babbage B.V., Davy Kager, Ethan Holliger, \u0141ukasz Golonka\r\n\r\nimport time\r\nimport itertools\r\nimport tones\r\nimport audioDucking\r\nimport touchHandler\r\nimport keyboardHandler\r\nimport mouseHandler\r\nimport eventHandler\r\nimport review\r\nimport controlTypes\r\nimport api\r\nimport textInfos\r\nimport speech\r\nimport sayAllHandler\r\nfrom NVDAObjects import NVDAObject, NVDAObjectTextInfo\r\nimport globalVars\r\nfrom logHandler import log\r\nfrom synthDriverHandler import *\r\nimport gui\r\nimport wx\r\nimport config\r\nimport winUser\r\nimport appModuleHandler\r\nimport winKernel\r\nimport treeInterceptorHandler\r\nimport browseMode\r\nimport scriptHandler\r\nimport ui\r\nimport braille\r\nimport brailleInput\r\nimport inputCore\r\nimport virtualBuffers\r\nimport characterProcessing\r\nfrom baseObject import ScriptableObject\r\nimport core\r\nimport winVersion\r\n\r\n#: Script category for text review commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_TEXTREVIEW = _(\"Text review\")\r\n#: Script category for Object navigation commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_OBJECTNAVIGATION = _(\"Object navigation\")\r\n#: Script category for system caret commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_SYSTEMCARET = _(\"System caret\")\r\n#: Script category for mouse commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_MOUSE = _(\"Mouse\")\r\n#: Script category for speech commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_SPEECH = _(\"Speech\")\r\n#: Script category for configuration dialogs commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_CONFIG = _(\"Configuration\")\r\n#: Script category for Braille commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_BRAILLE = _(\"Braille\")\r\n#: Script category for tools commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_TOOLS = pgettext('script category', 'Tools')\r\n#: Script category for touch commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_TOUCH = _(\"Touch screen\")\r\n#: Script category for focus commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_FOCUS = _(\"System focus\")\r\n#: Script category for system status commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_SYSTEM = _(\"System status\")\r\n#: Script category for input commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_INPUT = _(\"Input\")\r\n#: Script category for document formatting commands.\r\n# Translators: The name of a category of NVDA commands.\r\nSCRCAT_DOCUMENTFORMATTING = _(\"Document formatting\")\r\n\r\nclass GlobalCommands(ScriptableObject):\r\n\t\"\"\"Commands that are available at all times, regardless of the current focus.\r\n\t\"\"\"\r\n\r\n\tdef script_cycleAudioDuckingMode(self,gesture):\r\n\t\tif not audioDucking.isAudioDuckingSupported():\r\n\t\t\t# Translators: a message when audio ducking is not supported on this machine\r\n\t\t\tui.message(_(\"Audio ducking not supported\"))\r\n\t\t\treturn\r\n\t\tcurMode=config.conf['audio']['audioDuckingMode']\r\n\t\tnumModes=len(audioDucking.audioDuckingModes)\r\n\t\tnextMode=(curMode+1)%numModes\r\n\t\taudioDucking.setAudioDuckingMode(nextMode)\r\n\t\tconfig.conf['audio']['audioDuckingMode']=nextMode\r\n\t\tnextLabel=audioDucking.audioDuckingModes[nextMode]\r\n\t\tui.message(nextLabel)\r\n\t# Translators: Describes the Cycle audio ducking mode command.\r\n\tscript_cycleAudioDuckingMode.__doc__=_(\"Cycles through audio ducking modes which determine when NVDA lowers the volume of other sounds\")\r\n\r\n\tdef script_toggleInputHelp(self,gesture):\r\n\t\tinputCore.manager.isInputHelpActive = not inputCore.manager.isInputHelpActive\r\n\t\t# Translators: This will be presented when the input help is toggled.\r\n\t\tstateOn = _(\"input help on\")\r\n\t\t# Translators: This will be presented when the input help is toggled.\r\n\t\tstateOff = _(\"input help off\")\r\n\t\tstate = stateOn if inputCore.manager.isInputHelpActive else stateOff\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle input help command.\r\n\tscript_toggleInputHelp.__doc__=_(\"Turns input help on or off. When on, any input such as pressing a key on the keyboard will tell you what script is associated with that input, if any.\")\r\n\tscript_toggleInputHelp.category=SCRCAT_INPUT\r\n\r\n\tdef script_toggleCurrentAppSleepMode(self,gesture):\r\n\t\tcurFocus=api.getFocusObject()\r\n\t\tcurApp=curFocus.appModule\r\n\t\tif curApp.sleepMode:\r\n\t\t\tcurApp.sleepMode=False\r\n\t\t\t# Translators: This is presented when sleep mode is deactivated, NVDA will continue working as expected.\r\n\t\t\tui.message(_(\"Sleep mode off\"))\r\n\t\t\teventHandler.executeEvent(\"gainFocus\",curFocus)\r\n\t\telse:\r\n\t\t\teventHandler.executeEvent(\"loseFocus\",curFocus)\r\n\t\t\tcurApp.sleepMode=True\r\n\t\t\t# Translators: This is presented when sleep mode is activated, the focused application is self voicing, such as klango or openbook.\r\n\t\t\tui.message(_(\"Sleep mode on\"))\r\n\t# Translators: Input help mode message for toggle sleep mode command.\r\n\tscript_toggleCurrentAppSleepMode.__doc__=_(\"Toggles sleep mode on and off for the active application.\")\r\n\tscript_toggleCurrentAppSleepMode.allowInSleepMode=True\r\n\r\n\tdef script_reportCurrentLine(self,gesture):\r\n\t\tobj=api.getFocusObject()\r\n\t\ttreeInterceptor=obj.treeInterceptor\r\n\t\tif isinstance(treeInterceptor,treeInterceptorHandler.DocumentTreeInterceptor) and not treeInterceptor.passThrough:\r\n\t\t\tobj=treeInterceptor\r\n\t\ttry:\r\n\t\t\tinfo=obj.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\texcept (NotImplementedError, RuntimeError):\r\n\t\t\tinfo=obj.makeTextInfo(textInfos.POSITION_FIRST)\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tscriptCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif scriptCount==0:\r\n\t\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t\telse:\r\n\t\t\tspeech.spellTextInfo(info,useCharacterDescriptions=scriptCount>1)\r\n\t# Translators: Input help mode message for report current line command.\r\n\tscript_reportCurrentLine.__doc__=_(\"Reports the current line under the application cursor. Pressing this key twice will spell the current line. Pressing three times will spell the line using character descriptions.\")\r\n\tscript_reportCurrentLine.category=SCRCAT_SYSTEMCARET\r\n\r\n\tdef script_leftMouseClick(self,gesture):\r\n\t\t# Translators: Reported when left mouse button is clicked.\r\n\t\tui.message(_(\"Left click\"))\r\n\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_LEFTDOWN,0,0)\r\n\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_LEFTUP,0,0)\r\n\t# Translators: Input help mode message for left mouse click command.\r\n\tscript_leftMouseClick.__doc__=_(\"Clicks the left mouse button once at the current mouse position\")\r\n\tscript_leftMouseClick.category=SCRCAT_MOUSE\r\n\r\n\tdef script_rightMouseClick(self,gesture):\r\n\t\t# Translators: Reported when right mouse button is clicked.\r\n\t\tui.message(_(\"Right click\"))\r\n\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_RIGHTDOWN,0,0)\r\n\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_RIGHTUP,0,0)\r\n\t# Translators: Input help mode message for right mouse click command.\r\n\tscript_rightMouseClick.__doc__=_(\"Clicks the right mouse button once at the current mouse position\")\r\n\tscript_rightMouseClick.category=SCRCAT_MOUSE\r\n\r\n\tdef script_toggleLeftMouseButton(self,gesture):\r\n\t\tif winUser.getKeyState(winUser.VK_LBUTTON)&32768:\r\n\t\t\t# Translators: This is presented when the left mouse button lock is released (used for drag and drop).\r\n\t\t\tui.message(_(\"Left mouse button unlock\"))\r\n\t\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_LEFTUP,0,0)\r\n\t\telse:\r\n\t\t\t# Translators: This is presented when the left mouse button is locked down (used for drag and drop).\r\n\t\t\tui.message(_(\"Left mouse button lock\"))\r\n\t\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_LEFTDOWN,0,0)\r\n\t# Translators: Input help mode message for left mouse lock/unlock toggle command.\r\n\tscript_toggleLeftMouseButton.__doc__=_(\"Locks or unlocks the left mouse button\")\r\n\tscript_toggleLeftMouseButton.category=SCRCAT_MOUSE\r\n\r\n\tdef script_toggleRightMouseButton(self,gesture):\r\n\t\tif winUser.getKeyState(winUser.VK_RBUTTON)&32768:\r\n\t\t\t# Translators: This is presented when the right mouse button lock is released (used for drag and drop).\r\n\t\t\tui.message(_(\"Right mouse button unlock\"))\r\n\t\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_RIGHTUP,0,0)\r\n\t\telse:\r\n\t\t\t# Translators: This is presented when the right mouse button is locked down (used for drag and drop).\r\n\t\t\tui.message(_(\"Right mouse button lock\"))\r\n\t\t\tmouseHandler.executeMouseEvent(winUser.MOUSEEVENTF_RIGHTDOWN,0,0)\r\n\t# Translators: Input help mode message for right mouse lock/unlock command.\r\n\tscript_toggleRightMouseButton.__doc__=_(\"Locks or unlocks the right mouse button\")\r\n\tscript_toggleRightMouseButton.category=SCRCAT_MOUSE\r\n\r\n\tdef script_reportCurrentSelection(self,gesture):\r\n\t\tobj=api.getFocusObject()\r\n\t\ttreeInterceptor=obj.treeInterceptor\r\n\t\tif isinstance(treeInterceptor,treeInterceptorHandler.DocumentTreeInterceptor) and not treeInterceptor.passThrough:\r\n\t\t\tobj=treeInterceptor\r\n\t\ttry:\r\n\t\t\tinfo=obj.makeTextInfo(textInfos.POSITION_SELECTION)\r\n\t\texcept (RuntimeError, NotImplementedError):\r\n\t\t\tinfo=None\r\n\t\tif not info or info.isCollapsed:\r\n\t\t\tspeech.speakMessage(_(\"No selection\"))\r\n\t\telse:\r\n\t\t\tspeech.speakMessage(_(\"Selected %s\")%info.text)\r\n\t# Translators: Input help mode message for report current selection command.\r\n\tscript_reportCurrentSelection.__doc__=_(\"Announces the current selection in edit controls and documents. If there is no selection it says so.\")\r\n\tscript_reportCurrentSelection.category=SCRCAT_SYSTEMCARET\r\n\r\n\tdef script_dateTime(self,gesture):\r\n\t\tif scriptHandler.getLastScriptRepeatCount()==0:\r\n\t\t\ttext=winKernel.GetTimeFormatEx(winKernel.LOCALE_NAME_USER_DEFAULT, winKernel.TIME_NOSECONDS, None, None)\r\n\t\telse:\r\n\t\t\ttext=winKernel.GetDateFormatEx(winKernel.LOCALE_NAME_USER_DEFAULT, winKernel.DATE_LONGDATE, None, None)\r\n\t\tui.message(text)\r\n\t# Translators: Input help mode message for report date and time command.\r\n\tscript_dateTime.__doc__=_(\"If pressed once, reports the current time. If pressed twice, reports the current date\")\r\n\tscript_dateTime.category=SCRCAT_SYSTEM\r\n\r\n\tdef script_increaseSynthSetting(self,gesture):\r\n\t\tsettingName=globalVars.settingsRing.currentSettingName\r\n\t\tif not settingName:\r\n\t\t\t# Translators: Reported when there are no settings to configure in synth settings ring (example: when there is no setting for language).\r\n\t\t\tui.message(_(\"No settings\"))\r\n\t\t\treturn\r\n\t\tsettingValue=globalVars.settingsRing.increase()\r\n\t\tui.message(\"%s %s\" % (settingName,settingValue))\r\n\t# Translators: Input help mode message for increase synth setting value command.\r\n\tscript_increaseSynthSetting.__doc__=_(\"Increases the currently active setting in the synth settings ring\")\r\n\tscript_increaseSynthSetting.category=SCRCAT_SPEECH\r\n\r\n\tdef script_decreaseSynthSetting(self,gesture):\r\n\t\tsettingName=globalVars.settingsRing.currentSettingName\r\n\t\tif not settingName:\r\n\t\t\tui.message(_(\"No settings\"))\r\n\t\t\treturn\r\n\t\tsettingValue=globalVars.settingsRing.decrease()\r\n\t\tui.message(\"%s %s\" % (settingName,settingValue))\r\n\t# Translators: Input help mode message for decrease synth setting value command.\r\n\tscript_decreaseSynthSetting.__doc__=_(\"Decreases the currently active setting in the synth settings ring\")\r\n\tscript_decreaseSynthSetting.category=SCRCAT_SPEECH\r\n\r\n\tdef script_nextSynthSetting(self,gesture):\r\n\t\tnextSettingName=globalVars.settingsRing.next()\r\n\t\tif not nextSettingName:\r\n\t\t\tui.message(_(\"No settings\"))\r\n\t\t\treturn\r\n\t\tnextSettingValue=globalVars.settingsRing.currentSettingValue\r\n\t\tui.message(\"%s %s\"%(nextSettingName,nextSettingValue))\r\n\t# Translators: Input help mode message for next synth setting command.\r\n\tscript_nextSynthSetting.__doc__=_(\"Moves to the next available setting in the synth settings ring\")\r\n\tscript_nextSynthSetting.category=SCRCAT_SPEECH\r\n\r\n\tdef script_previousSynthSetting(self,gesture):\r\n\t\tpreviousSettingName=globalVars.settingsRing.previous()\r\n\t\tif not previousSettingName:\r\n\t\t\tui.message(_(\"No settings\"))\r\n\t\t\treturn\r\n\t\tpreviousSettingValue=globalVars.settingsRing.currentSettingValue\r\n\t\tui.message(\"%s %s\"%(previousSettingName,previousSettingValue))\r\n\t# Translators: Input help mode message for previous synth setting command.\r\n\tscript_previousSynthSetting.__doc__=_(\"Moves to the previous available setting in the synth settings ring\")\r\n\tscript_previousSynthSetting.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleSpeakTypedCharacters(self,gesture):\r\n\t\tif config.conf[\"keyboard\"][\"speakTypedCharacters\"]:\r\n\t\t\t# Translators: The message announced when toggling the speak typed characters keyboard setting.\r\n\t\t\tstate = _(\"speak typed characters off\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakTypedCharacters\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the speak typed characters keyboard setting.\r\n\t\t\tstate = _(\"speak typed characters on\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakTypedCharacters\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle speaked typed characters command.\r\n\tscript_toggleSpeakTypedCharacters.__doc__=_(\"Toggles on and off the speaking of typed characters\")\r\n\tscript_toggleSpeakTypedCharacters.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleSpeakTypedWords(self,gesture):\r\n\t\tif config.conf[\"keyboard\"][\"speakTypedWords\"]:\r\n\t\t\t# Translators: The message announced when toggling the speak typed words keyboard setting.\r\n\t\t\tstate = _(\"speak typed words off\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakTypedWords\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the speak typed words keyboard setting.\r\n\t\t\tstate = _(\"speak typed words on\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakTypedWords\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle speak typed words command.\r\n\tscript_toggleSpeakTypedWords.__doc__=_(\"Toggles on and off the speaking of typed words\")\r\n\tscript_toggleSpeakTypedWords.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleSpeakCommandKeys(self,gesture):\r\n\t\tif config.conf[\"keyboard\"][\"speakCommandKeys\"]:\r\n\t\t\t# Translators: The message announced when toggling the speak typed command keyboard setting.\r\n\t\t\tstate = _(\"speak command keys off\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakCommandKeys\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the speak typed command keyboard setting.\r\n\t\t\tstate = _(\"speak command keys on\")\r\n\t\t\tconfig.conf[\"keyboard\"][\"speakCommandKeys\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle speak command keys command.\r\n\tscript_toggleSpeakCommandKeys.__doc__=_(\"Toggles on and off the speaking of typed keys, that are not specifically characters\")\r\n\tscript_toggleSpeakCommandKeys.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleReportFontName(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportFontName\"]:\r\n\t\t\t# Translators: The message announced when toggling the report font name document formatting setting.\r\n\t\t\tstate = _(\"report font name off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontName\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report font name document formatting setting.\r\n\t\t\tstate = _(\"report font name on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontName\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report font name command.\r\n\tscript_toggleReportFontName.__doc__=_(\"Toggles on and off the reporting of font changes\")\r\n\tscript_toggleReportFontName.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportFontSize(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportFontSize\"]:\r\n\t\t\t# Translators: The message announced when toggling the report font size document formatting setting.\r\n\t\t\tstate = _(\"report font size off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontSize\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report font size document formatting setting.\r\n\t\t\tstate = _(\"report font size on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontSize\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report font size command.\r\n\tscript_toggleReportFontSize.__doc__=_(\"Toggles on and off the reporting of font size changes\")\r\n\tscript_toggleReportFontSize.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportFontAttributes(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportFontAttributes\"]:\r\n\t\t\t# Translators: The message announced when toggling the report font attributes document formatting setting.\r\n\t\t\tstate = _(\"report font attributes off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontAttributes\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report font attributes document formatting setting.\r\n\t\t\tstate = _(\"report font attributes on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFontAttributes\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report font attributes command.\r\n\tscript_toggleReportFontAttributes.__doc__=_(\"Toggles on and off the reporting of font attributes\")\r\n\tscript_toggleReportFontAttributes.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportRevisions(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportRevisions\"]:\r\n\t\t\t# Translators: The message announced when toggling the report revisions document formatting setting.\r\n\t\t\tstate = _(\"report revisions off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportRevisions\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report revisions document formatting setting.\r\n\t\t\tstate = _(\"report revisions on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportRevisions\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report revisions command.\r\n\tscript_toggleReportRevisions.__doc__=_(\"Toggles on and off the reporting of revisions\")\r\n\tscript_toggleReportRevisions.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportEmphasis(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportEmphasis\"]:\r\n\t\t\t# Translators: The message announced when toggling the report emphasis document formatting setting.\r\n\t\t\tstate = _(\"report emphasis off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportEmphasis\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report emphasis document formatting setting.\r\n\t\t\tstate = _(\"report emphasis on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportEmphasis\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report emphasis command.\r\n\tscript_toggleReportEmphasis.__doc__=_(\"Toggles on and off the reporting of emphasis\")\r\n\tscript_toggleReportEmphasis.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportColor(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportColor\"]:\r\n\t\t\t# Translators: The message announced when toggling the report colors document formatting setting.\r\n\t\t\tstate = _(\"report colors off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportColor\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report colors document formatting setting.\r\n\t\t\tstate = _(\"report colors on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportColor\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report colors command.\r\n\tscript_toggleReportColor.__doc__=_(\"Toggles on and off the reporting of colors\")\r\n\tscript_toggleReportColor.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportAlignment(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportAlignment\"]:\r\n\t\t\t# Translators: The message announced when toggling the report alignment document formatting setting.\r\n\t\t\tstate = _(\"report alignment off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportAlignment\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report alignment document formatting setting.\r\n\t\t\tstate = _(\"report alignment on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportAlignment\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report alignment command.\r\n\tscript_toggleReportAlignment.__doc__=_(\"Toggles on and off the reporting of text alignment\")\r\n\tscript_toggleReportAlignment.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportStyle(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportStyle\"]:\r\n\t\t\t# Translators: The message announced when toggling the report style document formatting setting.\r\n\t\t\tstate = _(\"report style off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportStyle\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report style document formatting setting.\r\n\t\t\tstate = _(\"report style on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportStyle\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report style command.\r\n\tscript_toggleReportStyle.__doc__=_(\"Toggles on and off the reporting of style changes\")\r\n\tscript_toggleReportStyle.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportSpellingErrors(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportSpellingErrors\"]:\r\n\t\t\t# Translators: The message announced when toggling the report spelling errors document formatting setting.\r\n\t\t\tstate = _(\"report spelling errors off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportSpellingErrors\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report spelling errors document formatting setting.\r\n\t\t\tstate = _(\"report spelling errors on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportSpellingErrors\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report spelling errors command.\r\n\tscript_toggleReportSpellingErrors.__doc__=_(\"Toggles on and off the reporting of spelling errors\")\r\n\tscript_toggleReportSpellingErrors.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportPage(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportPage\"]:\r\n\t\t\t# Translators: The message announced when toggling the report pages document formatting setting.\r\n\t\t\tstate = _(\"report pages off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportPage\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report pages document formatting setting.\r\n\t\t\tstate = _(\"report pages on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportPage\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report pages command.\r\n\tscript_toggleReportPage.__doc__=_(\"Toggles on and off the reporting of pages\")\r\n\tscript_toggleReportPage.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLineNumber(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportLineNumber\"]:\r\n\t\t\t# Translators: The message announced when toggling the report line numbers document formatting setting.\r\n\t\t\tstate = _(\"report line numbers off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLineNumber\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report line numbers document formatting setting.\r\n\t\t\tstate = _(\"report line numbers on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLineNumber\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report line numbers command.\r\n\tscript_toggleReportLineNumber.__doc__=_(\"Toggles on and off the reporting of line numbers\")\r\n\tscript_toggleReportLineNumber.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLineIndentation(self,gesture):\r\n\t\tlineIndentationSpeech = config.conf[\"documentFormatting\"][\"reportLineIndentation\"]\r\n\t\tlineIndentationTones = config.conf[\"documentFormatting\"][\"reportLineIndentationWithTones\"]\r\n\t\tif not lineIndentationSpeech and not lineIndentationTones:\r\n\t\t\t# Translators: A message reported when cycling through line indentation settings.\r\n\t\t\tui.message(_(\"Report line indentation with speech\"))\r\n\t\t\tlineIndentationSpeech = True\r\n\t\telif lineIndentationSpeech and not lineIndentationTones:\r\n\t\t\t# Translators: A message reported when cycling through line indentation settings.\r\n\t\t\tui.message(_(\"Report line indentation with tones\"))\r\n\t\t\tlineIndentationSpeech = False\r\n\t\t\tlineIndentationTones = True\r\n\t\telif not lineIndentationSpeech and lineIndentationTones:\r\n\t\t\t# Translators: A message reported when cycling through line indentation settings.\r\n\t\t\tui.message(_(\"Report line indentation with speech and tones\"))\r\n\t\t\tlineIndentationSpeech = True\r\n\t\telse:\r\n\t\t\t# Translators: A message reported when cycling through line indentation settings.\r\n\t\t\tui.message(_(\"Report line indentation off\"))\r\n\t\t\tlineIndentationSpeech = False\r\n\t\t\tlineIndentationTones = False\r\n\t\tconfig.conf[\"documentFormatting\"][\"reportLineIndentation\"] = lineIndentationSpeech\r\n\t\tconfig.conf[\"documentFormatting\"][\"reportLineIndentationWithTones\"] = lineIndentationTones\r\n\t# Translators: Input help mode message for toggle report line indentation command.\r\n\tscript_toggleReportLineIndentation.__doc__=_(\"Cycles through line indentation settings\")\r\n\tscript_toggleReportLineIndentation.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportParagraphIndentation(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportParagraphIndentation\"]:\r\n\t\t\t# Translators: The message announced when toggling the report paragraph indentation document formatting setting.\r\n\t\t\tstate = _(\"report paragraph indentation off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportParagraphIndentation\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report paragraph indentation document formatting setting.\r\n\t\t\tstate = _(\"report paragraph indentation on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportParagraphIndentation\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report paragraph indentation command.\r\n\tscript_toggleReportParagraphIndentation.__doc__=_(\"Toggles on and off the reporting of paragraph indentation\")\r\n\tscript_toggleReportParagraphIndentation.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLineSpacing(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportLineSpacing\"]:\r\n\t\t\t# Translators: The message announced when toggling the report line spacing document formatting setting.\r\n\t\t\tstate = _(\"report line spacing off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLineSpacing\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report line spacing document formatting setting.\r\n\t\t\tstate = _(\"report line spacing on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLineSpacing\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report line spacing command.\r\n\tscript_toggleReportLineSpacing.__doc__=_(\"Toggles on and off the reporting of line spacing\")\r\n\tscript_toggleReportLineSpacing.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportTables(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportTables\"]:\r\n\t\t\t# Translators: The message announced when toggling the report tables document formatting setting.\r\n\t\t\tstate = _(\"report tables off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTables\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report tables document formatting setting.\r\n\t\t\tstate = _(\"report tables on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTables\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report tables command.\r\n\tscript_toggleReportTables.__doc__=_(\"Toggles on and off the reporting of tables\")\r\n\tscript_toggleReportTables.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportTableHeaders(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportTableHeaders\"]:\r\n\t\t\t# Translators: The message announced when toggling the report table row/column headers document formatting setting.\r\n\t\t\tstate = _(\"report table row and column headers off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTableHeaders\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report table row/column headers document formatting setting.\r\n\t\t\tstate = _(\"report table row and column headers on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTableHeaders\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report table row/column headers command.\r\n\tscript_toggleReportTableHeaders.__doc__=_(\"Toggles on and off the reporting of table row and column headers\")\r\n\tscript_toggleReportTableHeaders.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportTableCellCoords(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportTableCellCoords\"]:\r\n\t\t\t# Translators: The message announced when toggling the report table cell coordinates document formatting setting.\r\n\t\t\tstate = _(\"report table cell coordinates off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTableCellCoords\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report table cell coordinates document formatting setting.\r\n\t\t\tstate = _(\"report table cell coordinates on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportTableCellCoords\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report table cell coordinates command.\r\n\tscript_toggleReportTableCellCoords.__doc__=_(\"Toggles on and off the reporting of table cell coordinates\")\r\n\tscript_toggleReportTableCellCoords.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLinks(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportLinks\"]:\r\n\t\t\t# Translators: The message announced when toggling the report links document formatting setting.\r\n\t\t\tstate = _(\"report links off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLinks\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report links document formatting setting.\r\n\t\t\tstate = _(\"report links on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLinks\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report links command.\r\n\tscript_toggleReportLinks.__doc__=_(\"Toggles on and off the reporting of links\")\r\n\tscript_toggleReportLinks.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportComments(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportComments\"]:\r\n\t\t\t# Translators: The message announced when toggling the report comments document formatting setting.\r\n\t\t\tstate = _(\"report comments off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportComments\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report comments document formatting setting.\r\n\t\t\tstate = _(\"report comments on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportComments\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report comments command.\r\n\tscript_toggleReportComments.__doc__=_(\"Toggles on and off the reporting of comments\")\r\n\tscript_toggleReportComments.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLists(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportLists\"]:\r\n\t\t\t# Translators: The message announced when toggling the report lists document formatting setting.\r\n\t\t\tstate = _(\"report lists off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLists\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report lists document formatting setting.\r\n\t\t\tstate = _(\"report lists on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLists\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report lists command.\r\n\tscript_toggleReportLists.__doc__=_(\"Toggles on and off the reporting of lists\")\r\n\tscript_toggleReportLists.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportHeadings(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportHeadings\"]:\r\n\t\t\t# Translators: The message announced when toggling the report headings document formatting setting.\r\n\t\t\tstate = _(\"report headings off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportHeadings\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report headings document formatting setting.\r\n\t\t\tstate = _(\"report headings on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportHeadings\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report headings command.\r\n\tscript_toggleReportHeadings.__doc__=_(\"Toggles on and off the reporting of headings\")\r\n\tscript_toggleReportHeadings.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportBlockQuotes(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportBlockQuotes\"]:\r\n\t\t\t# Translators: The message announced when toggling the report block quotes document formatting setting.\r\n\t\t\tstate = _(\"report block quotes off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportBlockQuotes\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report block quotes document formatting setting.\r\n\t\t\tstate = _(\"report block quotes on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportBlockQuotes\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report block quotes command.\r\n\tscript_toggleReportBlockQuotes.__doc__=_(\"Toggles on and off the reporting of block quotes\")\r\n\tscript_toggleReportBlockQuotes.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportLandmarks(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportLandmarks\"]:\r\n\t\t\t# Translators: The message announced when toggling the report landmarks document formatting setting.\r\n\t\t\tstate = _(\"report landmarks off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLandmarks\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report landmarks document formatting setting.\r\n\t\t\tstate = _(\"report landmarks on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportLandmarks\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report landmarks command.\r\n\tscript_toggleReportLandmarks.__doc__=_(\"Toggles on and off the reporting of landmarks\")\r\n\tscript_toggleReportLandmarks.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportFrames(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportFrames\"]:\r\n\t\t\t# Translators: The message announced when toggling the report frames document formatting setting.\r\n\t\t\tstate = _(\"report frames off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFrames\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report frames document formatting setting.\r\n\t\t\tstate = _(\"report frames on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportFrames\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report frames command.\r\n\tscript_toggleReportFrames.__doc__=_(\"Toggles on and off the reporting of frames\")\r\n\tscript_toggleReportFrames.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_toggleReportClickable(self,gesture):\r\n\t\tif config.conf[\"documentFormatting\"][\"reportClickable\"]:\r\n\t\t\t# Translators: The message announced when toggling the report if clickable document formatting setting.\r\n\t\t\tstate = _(\"report if clickable off\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportClickable\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the report if clickable document formatting setting.\r\n\t\t\tstate = _(\"report if clickable on\")\r\n\t\t\tconfig.conf[\"documentFormatting\"][\"reportClickable\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle report if clickable command.\r\n\tscript_toggleReportClickable.__doc__=_(\"Toggles on and off reporting if clickable\")\r\n\tscript_toggleReportClickable.category=SCRCAT_DOCUMENTFORMATTING\r\n\r\n\tdef script_cycleSpeechSymbolLevel(self,gesture):\r\n\t\tcurLevel = config.conf[\"speech\"][\"symbolLevel\"]\r\n\t\tfor level in characterProcessing.CONFIGURABLE_SPEECH_SYMBOL_LEVELS:\r\n\t\t\tif level > curLevel:\r\n\t\t\t\tbreak\r\n\t\telse:\r\n\t\t\tlevel = characterProcessing.SYMLVL_NONE\r\n\t\tname = characterProcessing.SPEECH_SYMBOL_LEVEL_LABELS[level]\r\n\t\tconfig.conf[\"speech\"][\"symbolLevel\"] = level\r\n\t\t# Translators: Reported when the user cycles through speech symbol levels\r\n\t\t# which determine what symbols are spoken.\r\n\t\t# %s will be replaced with the symbol level; e.g. none, some, most and all.\r\n\t\tui.message(_(\"Symbol level %s\") % name)\r\n\t# Translators: Input help mode message for cycle speech symbol level command.\r\n\tscript_cycleSpeechSymbolLevel.__doc__=_(\"Cycles through speech symbol levels which determine what symbols are spoken\")\r\n\tscript_cycleSpeechSymbolLevel.category=SCRCAT_SPEECH\r\n\r\n\tdef script_moveMouseToNavigatorObject(self,gesture):\r\n\t\ttry:\r\n\t\t\tp=api.getReviewPosition().pointAtStart\r\n\t\texcept (NotImplementedError, LookupError):\r\n\t\t\tp=None\r\n\t\tif p:\r\n\t\t\tx=p.x\r\n\t\t\ty=p.y\r\n\t\telse:\r\n\t\t\ttry:\r\n\t\t\t\t(left,top,width,height)=api.getNavigatorObject().location\r\n\t\t\texcept:\r\n\t\t\t\t# Translators: Reported when the object has no location for the mouse to move to it.\r\n\t\t\t\tui.message(_(\"Object has no location\"))\r\n\t\t\t\treturn\r\n\t\t\tx=left+(width/2)\r\n\t\t\ty=top+(height/2)\r\n\t\twinUser.setCursorPos(x,y)\r\n\t\tmouseHandler.executeMouseMoveEvent(x,y)\r\n\t# Translators: Input help mode message for move mouse to navigator object command.\r\n\tscript_moveMouseToNavigatorObject.__doc__=_(\"Moves the mouse pointer to the current navigator object\")\r\n\tscript_moveMouseToNavigatorObject.category=SCRCAT_MOUSE\r\n\r\n\tdef script_moveNavigatorObjectToMouse(self,gesture):\r\n\t\t# Translators: Reported when attempting to move the navigator object to the object under mouse pointer.\r\n\t\tui.message(_(\"Move navigator object to mouse\"))\r\n\t\tobj=api.getMouseObject()\r\n\t\tapi.setNavigatorObject(obj)\r\n\t\tspeech.speakObject(obj)\r\n\t# Translators: Input help mode message for move navigator object to mouse command.\r\n\tscript_moveNavigatorObjectToMouse.__doc__=_(\"Sets the navigator object to the current object under the mouse pointer and speaks it\")\r\n\tscript_moveNavigatorObjectToMouse.category=SCRCAT_MOUSE\r\n\r\n\tdef script_reviewMode_next(self,gesture):\r\n\t\tlabel=review.nextMode()\r\n\t\tif label:\r\n\t\t\tui.reviewMessage(label)\r\n\t\t\tpos=api.getReviewPosition().copy()\r\n\t\t\tpos.expand(textInfos.UNIT_LINE)\r\n\t\t\tbraille.handler.setTether(braille.handler.TETHER_REVIEW, auto=True)\r\n\t\t\tspeech.speakTextInfo(pos)\r\n\t\telse:\r\n\t\t\t# Translators: reported when there are no other available review modes for this object \r\n\t\t\tui.reviewMessage(_(\"No next review mode\"))\r\n\t# Translators: Script help message for next review mode command.\r\n\tscript_reviewMode_next.__doc__=_(\"Switches to the next review mode (e.g. object, document or screen) and positions the review position at the point of the navigator object\")\r\n\tscript_reviewMode_next.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_reviewMode_previous(self,gesture):\r\n\t\tlabel=review.nextMode(prev=True)\r\n\t\tif label:\r\n\t\t\tui.reviewMessage(label)\r\n\t\t\tpos=api.getReviewPosition().copy()\r\n\t\t\tpos.expand(textInfos.UNIT_LINE)\r\n\t\t\tbraille.handler.setTether(braille.handler.TETHER_REVIEW, auto=True)\r\n\t\t\tspeech.speakTextInfo(pos)\r\n\t\telse:\r\n\t\t\t# Translators: reported when there are no other available review modes for this object \r\n\t\t\tui.reviewMessage(_(\"No previous review mode\"))\r\n\t# Translators: Script help message for previous review mode command.\r\n\tscript_reviewMode_previous.__doc__=_(\"Switches to the previous review mode (e.g. object, document or screen) and positions the review position at the point of the navigator object\") \r\n\tscript_reviewMode_previous.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_toggleSimpleReviewMode(self,gesture):\r\n\t\tif config.conf[\"reviewCursor\"][\"simpleReviewMode\"]:\r\n\t\t\t# Translators: The message announced when toggling simple review mode.\r\n\t\t\tstate = _(\"Simple review mode off\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"simpleReviewMode\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling simple review mode.\r\n\t\t\tstate = _(\"Simple review mode on\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"simpleReviewMode\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle simple review mode command.\r\n\tscript_toggleSimpleReviewMode.__doc__=_(\"Toggles simple review mode on and off\")\r\n\tscript_toggleSimpleReviewMode.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_current(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tif not isinstance(curObject,NVDAObject):\r\n\t\t\t# Translators: Reported when the user tries to perform a command related to the navigator object\r\n\t\t\t# but there is no current navigator object.\r\n\t\t\tui.reviewMessage(_(\"No navigator object\"))\r\n\t\t\treturn\r\n\t\tif scriptHandler.getLastScriptRepeatCount()>=1:\r\n\t\t\tif curObject.TextInfo!=NVDAObjectTextInfo:\r\n\t\t\t\ttextList=[]\r\n\t\t\t\tif curObject.name and isinstance(curObject.name, basestring) and not curObject.name.isspace():\r\n\t\t\t\t\ttextList.append(curObject.name)\r\n\t\t\t\ttry:\r\n\t\t\t\t\tinfo=curObject.makeTextInfo(textInfos.POSITION_SELECTION)\r\n\t\t\t\t\tif not info.isCollapsed:\r\n\t\t\t\t\t\ttextList.append(info.text)\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\t\t\t\t\tif not info.isCollapsed:\r\n\t\t\t\t\t\t\ttextList.append(info.text)\r\n\t\t\t\texcept (RuntimeError, NotImplementedError):\r\n\t\t\t\t\t# No caret or selection on this object.\r\n\t\t\t\t\tpass\r\n\t\t\telse:\r\n\t\t\t\ttextList=[prop for prop in (curObject.name, curObject.value) if prop and isinstance(prop, basestring) and not prop.isspace()]\r\n\t\t\ttext=\" \".join(textList)\r\n\t\t\tif len(text)>0 and not text.isspace():\r\n\t\t\t\tif scriptHandler.getLastScriptRepeatCount()==1:\r\n\t\t\t\t\tspeech.speakSpelling(text)\r\n\t\t\t\telse:\r\n\t\t\t\t\tif api.copyToClip(text):\r\n\t\t\t\t\t\t# Translators: Indicates something has been copied to clipboard (example output: title text copied to clipboard).\r\n\t\t\t\t\t\tspeech.speakMessage(_(\"%s copied to clipboard\")%text)\r\n\t\telse:\r\n\t\t\tspeech.speakObject(curObject,reason=controlTypes.REASON_QUERY)\r\n\t# Translators: Input help mode message for report current navigator object command.\r\n\tscript_navigatorObject_current.__doc__=_(\"Reports the current navigator object. Pressing twice spells this information, and pressing three times Copies name and value of this object to the clipboard\")\r\n\tscript_navigatorObject_current.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_currentDimensions(self,gesture):\r\n\t\tcount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tlocationText=api.getReviewPosition().locationText if count==0 else None\r\n\t\tif not locationText:\r\n\t\t\tlocationText=api.getNavigatorObject().locationText\r\n\t\tif not locationText:\r\n\t\t\t# Translators: message when there is no location information for the review cursor\r\n\t\t\tui.message(_(\"No location information\"))\r\n\t\t\treturn\r\n\t\tui.message(locationText)\r\n\t# Translators: Description for report review cursor location command.\r\n\tscript_navigatorObject_currentDimensions.__doc__=_(\"Reports information about the location of the text or object at the review cursor. Pressing twice may provide further detail.\") \r\n\tscript_navigatorObject_currentDimensions.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_toFocus(self,gesture):\r\n\t\tobj=api.getFocusObject()\r\n\t\ttry:\r\n\t\t\tpos=obj.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\texcept (NotImplementedError,RuntimeError):\r\n\t\t\tpos=obj.makeTextInfo(textInfos.POSITION_FIRST)\r\n\t\tapi.setReviewPosition(pos)\r\n\t\t# Translators: Reported when attempting to move the navigator object to focus.\r\n\t\tspeech.speakMessage(_(\"Move to focus\"))\r\n\t\tspeech.speakObject(obj,reason=controlTypes.REASON_FOCUS)\r\n\t# Translators: Input help mode message for move navigator object to current focus command.\r\n\tscript_navigatorObject_toFocus.__doc__=_(\"Sets the navigator object to the current focus, and the review cursor to the position of the caret inside it, if possible.\")\r\n\tscript_navigatorObject_toFocus.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_moveFocus(self,gesture):\r\n\t\tobj=api.getNavigatorObject()\r\n\t\tif not isinstance(obj,NVDAObject):\r\n\t\t\t# Translators: Reported when:\r\n\t\t\t# 1. There is no focusable object e.g. cannot use tab and shift tab to move to controls.\r\n\t\t\t# 2. Trying to move focus to navigator object but there is no focus.\r\n\t\t\tui.message(_(\"No focus\"))\r\n\t\tif scriptHandler.getLastScriptRepeatCount()==0:\r\n\t\t\t# Translators: Reported when attempting to move focus to navigator object.\r\n\t\t\tui.message(_(\"Move focus\"))\r\n\t\t\tobj.setFocus()\r\n\t\telse:\r\n\t\t\treview=api.getReviewPosition()\r\n\t\t\ttry:\r\n\t\t\t\treview.updateCaret()\r\n\t\t\texcept NotImplementedError:\r\n\t\t\t\t# Translators: Reported when trying to move caret to the position of the review cursor but there is no caret.\r\n\t\t\t\tui.message(_(\"No caret\"))\r\n\t\t\t\treturn\r\n\t\t\tinfo=review.copy()\r\n\t\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\t\tspeech.speakTextInfo(info,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move focus to current navigator object command.\r\n\tscript_navigatorObject_moveFocus.__doc__=_(\"Pressed once sets the keyboard focus to the navigator object, pressed twice sets the system caret to the position of the review cursor\")\r\n\tscript_navigatorObject_moveFocus.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_parent(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tif not isinstance(curObject,NVDAObject):\r\n\t\t\t# Translators: Reported when the user tries to perform a command related to the navigator object\r\n\t\t\t# but there is no current navigator object.\r\n\t\t\tui.reviewMessage(_(\"No navigator object\"))\r\n\t\t\treturn\r\n\t\tsimpleReviewMode=config.conf[\"reviewCursor\"][\"simpleReviewMode\"]\r\n\t\tcurObject=curObject.simpleParent if simpleReviewMode else curObject.parent\r\n\t\tif curObject is not None:\r\n\t\t\tapi.setNavigatorObject(curObject)\r\n\t\t\tspeech.speakObject(curObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: Reported when there is no containing (parent) object such as when focused on desktop.\r\n\t\t\tui.reviewMessage(_(\"No containing object\"))\r\n\t# Translators: Input help mode message for move to parent object command.\r\n\tscript_navigatorObject_parent.__doc__=_(\"Moves the navigator object to the object containing it\")\r\n\tscript_navigatorObject_parent.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_next(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tif not isinstance(curObject,NVDAObject):\r\n\t\t\t# Translators: Reported when the user tries to perform a command related to the navigator object\r\n\t\t\t# but there is no current navigator object.\r\n\t\t\tui.reviewMessage(_(\"No navigator object\"))\r\n\t\t\treturn\r\n\t\tsimpleReviewMode=config.conf[\"reviewCursor\"][\"simpleReviewMode\"]\r\n\t\tcurObject=curObject.simpleNext if simpleReviewMode else curObject.next\r\n\t\tif curObject is not None:\r\n\t\t\tapi.setNavigatorObject(curObject)\r\n\t\t\tspeech.speakObject(curObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: Reported when there is no next object (current object is the last object).\r\n\t\t\tui.reviewMessage(_(\"No next\"))\r\n\t# Translators: Input help mode message for move to next object command.\r\n\tscript_navigatorObject_next.__doc__=_(\"Moves the navigator object to the next object\")\r\n\tscript_navigatorObject_next.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_previous(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tif not isinstance(curObject,NVDAObject):\r\n\t\t\t# Translators: Reported when the user tries to perform a command related to the navigator object\r\n\t\t\t# but there is no current navigator object.\r\n\t\t\tui.reviewMessage(_(\"No navigator object\"))\r\n\t\t\treturn\r\n\t\tsimpleReviewMode=config.conf[\"reviewCursor\"][\"simpleReviewMode\"]\r\n\t\tcurObject=curObject.simplePrevious if simpleReviewMode else curObject.previous\r\n\t\tif curObject is not None:\r\n\t\t\tapi.setNavigatorObject(curObject)\r\n\t\t\tspeech.speakObject(curObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: Reported when there is no previous object (current object is the first object).\r\n\t\t\tui.reviewMessage(_(\"No previous\"))\r\n\t# Translators: Input help mode message for move to previous object command.\r\n\tscript_navigatorObject_previous.__doc__=_(\"Moves the navigator object to the previous object\")\r\n\tscript_navigatorObject_previous.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_firstChild(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tif not isinstance(curObject,NVDAObject):\r\n\t\t\t# Translators: Reported when the user tries to perform a command related to the navigator object\r\n\t\t\t# but there is no current navigator object.\r\n\t\t\tui.reviewMessage(_(\"No navigator object\"))\r\n\t\t\treturn\r\n\t\tsimpleReviewMode=config.conf[\"reviewCursor\"][\"simpleReviewMode\"]\r\n\t\tcurObject=curObject.simpleFirstChild if simpleReviewMode else curObject.firstChild\r\n\t\tif curObject is not None:\r\n\t\t\tapi.setNavigatorObject(curObject)\r\n\t\t\tspeech.speakObject(curObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: Reported when there is no contained (first child) object such as inside a document.\r\n\t\t\tui.reviewMessage(_(\"No objects inside\"))\r\n\t# Translators: Input help mode message for move to first child object command.\r\n\tscript_navigatorObject_firstChild.__doc__=_(\"Moves the navigator object to the first object inside it\")\r\n\tscript_navigatorObject_firstChild.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_review_activate(self,gesture):\r\n\t\t# Translators: a message reported when the action at the position of the review cursor or navigator object is performed.\r\n\t\tactionName=_(\"Activate\")\r\n\t\tpos=api.getReviewPosition()\r\n\t\ttry:\r\n\t\t\tpos.activate()\r\n\t\t\tif isinstance(gesture,touchHandler.TouchInputGesture):\r\n\t\t\t\ttouchHandler.handler.notifyInteraction(pos.NVDAObjectAtStart)\r\n\t\t\tui.message(actionName)\r\n\t\t\treturn\r\n\t\texcept NotImplementedError:\r\n\t\t\tpass\r\n\t\tobj=api.getNavigatorObject()\r\n\t\twhile obj:\r\n\t\t\trealActionName=actionName\r\n\t\t\ttry:\r\n\t\t\t\trealActionName=obj.getActionName()\r\n\t\t\texcept:\r\n\t\t\t\tpass\r\n\t\t\ttry:\r\n\t\t\t\tobj.doAction()\r\n\t\t\t\tif isinstance(gesture,touchHandler.TouchInputGesture):\r\n\t\t\t\t\ttouchHandler.handler.notifyInteraction(obj)\r\n\t\t\t\tui.message(realActionName)\r\n\t\t\t\treturn\r\n\t\t\texcept NotImplementedError:\r\n\t\t\t\tpass\r\n\t\t\tobj=obj.parent\r\n\t\t# Translators: the message reported when there is no action to perform on the review position or navigator object.\r\n\t\tui.message(_(\"No action\"))\r\n\t# Translators: Input help mode message for activate current object command.\r\n\tscript_review_activate.__doc__=_(\"Performs the default action on the current navigator object (example: presses it if it is a button).\")\r\n\tscript_review_activate.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_review_top(self,gesture):\r\n\t\tinfo=api.getReviewPosition().obj.makeTextInfo(textInfos.POSITION_FIRST)\r\n\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tui.reviewMessage(_(\"Top\"))\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to top line command.\r\n\tscript_review_top.__doc__=_(\"Moves the review cursor to the top line of the current navigator object and speaks it\")\r\n\tscript_review_top.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_previousLine(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tinfo.collapse()\r\n\t\tres=info.move(textInfos.UNIT_LINE,-1)\r\n\t\tif res==0:\r\n\t\t\t# Translators: a message reported when review cursor is at the top line of the current navigator object.\r\n\t\t\tui.reviewMessage(_(\"Top\"))\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to previous line command.\r\n\tscript_review_previousLine.__doc__=_(\"Moves the review cursor to the previous line of the current navigator object and speaks it\")\r\n\tscript_review_previousLine.resumeSayAllMode=sayAllHandler.CURSOR_REVIEW\r\n\tscript_review_previousLine.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_currentLine(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\t# Explicitly tether here\r\n\t\tbraille.handler.setTether(braille.handler.TETHER_REVIEW, auto=True)\r\n\t\tscriptCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif scriptCount==0:\r\n\t\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t\telse:\r\n\t\t\tspeech.spellTextInfo(info,useCharacterDescriptions=scriptCount>1)\r\n\t# Translators: Input help mode message for read current line under review cursor command.\r\n\tscript_review_currentLine.__doc__=_(\"Reports the line of the current navigator object where the review cursor is situated. If this key is pressed twice, the current line will be spelled. Pressing three times will spell the line using character descriptions.\")\r\n\tscript_review_currentLine.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_nextLine(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tinfo.collapse()\r\n\t\tres=info.move(textInfos.UNIT_LINE,1)\r\n\t\tif res==0:\r\n\t\t\t# Translators: a message reported when review cursor is at the bottom line of the current navigator object.\r\n\t\t\tui.reviewMessage(_(\"Bottom\"))\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to next line command.\r\n\tscript_review_nextLine.__doc__=_(\"Moves the review cursor to the next line of the current navigator object and speaks it\")\r\n\tscript_review_nextLine.resumeSayAllMode=sayAllHandler.CURSOR_REVIEW\r\n\tscript_review_nextLine.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_bottom(self,gesture):\r\n\t\tinfo=api.getReviewPosition().obj.makeTextInfo(textInfos.POSITION_LAST)\r\n\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tui.reviewMessage(_(\"Bottom\"))\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_LINE,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to bottom line command.\r\n\tscript_review_bottom.__doc__=_(\"Moves the review cursor to the bottom line of the current navigator object and speaks it\")\r\n\tscript_review_bottom.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_previousWord(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_WORD)\r\n\t\tinfo.collapse()\r\n\t\tres=info.move(textInfos.UNIT_WORD,-1)\r\n\t\tif res==0:\r\n\t\t\t# Translators: a message reported when review cursor is at the top line of the current navigator object.\r\n\t\t\tui.reviewMessage(_(\"Top\"))\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_WORD)\r\n\t\tspeech.speakTextInfo(info,reason=controlTypes.REASON_CARET,unit=textInfos.UNIT_WORD)\r\n\t# Translators: Input help mode message for move review cursor to previous word command.\r\n\tscript_review_previousWord.__doc__=_(\"Moves the review cursor to the previous word of the current navigator object and speaks it\")\r\n\tscript_review_previousWord.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_currentWord(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_WORD)\r\n\t\t# Explicitly tether here\r\n\t\tbraille.handler.setTether(braille.handler.TETHER_REVIEW, auto=True)\r\n\t\tscriptCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif scriptCount==0:\r\n\t\t\tspeech.speakTextInfo(info,reason=controlTypes.REASON_CARET,unit=textInfos.UNIT_WORD)\r\n\t\telse:\r\n\t\t\tspeech.spellTextInfo(info,useCharacterDescriptions=scriptCount>1)\r\n\t# Translators: Input help mode message for report current word under review cursor command.\r\n\tscript_review_currentWord.__doc__=_(\"Speaks the word of the current navigator object where the review cursor is situated. Pressing twice spells the word. Pressing three times spells the word using character descriptions\")\r\n\tscript_review_currentWord.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_nextWord(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_WORD)\r\n\t\tinfo.collapse()\r\n\t\tres=info.move(textInfos.UNIT_WORD,1)\r\n\t\tif res==0:\r\n\t\t\t# Translators: a message reported when review cursor is at the bottom line of the current navigator object.\r\n\t\t\tui.reviewMessage(_(\"Bottom\"))\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_WORD)\r\n\t\tspeech.speakTextInfo(info,reason=controlTypes.REASON_CARET,unit=textInfos.UNIT_WORD)\r\n\t# Translators: Input help mode message for move review cursor to next word command.\r\n\tscript_review_nextWord.__doc__=_(\"Moves the review cursor to the next word of the current navigator object and speaks it\")\r\n\tscript_review_nextWord.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_startOfLine(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tinfo.collapse()\r\n\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tui.reviewMessage(_(\"Left\"))\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to start of current line command.\r\n\tscript_review_startOfLine.__doc__=_(\"Moves the review cursor to the first character of the line where it is situated in the current navigator object and speaks it\")\r\n\tscript_review_startOfLine.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_previousCharacter(self,gesture):\r\n\t\tlineInfo=api.getReviewPosition().copy()\r\n\t\tlineInfo.expand(textInfos.UNIT_LINE)\r\n\t\tcharInfo=api.getReviewPosition().copy()\r\n\t\tcharInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tcharInfo.collapse()\r\n\t\tres=charInfo.move(textInfos.UNIT_CHARACTER,-1)\r\n\t\tif res==0 or charInfo.compareEndPoints(lineInfo,\"startToStart\")<0:\r\n\t\t\t# Translators: a message reported when review cursor is at the leftmost character of the current navigator object's text.\r\n\t\t\tui.reviewMessage(_(\"Left\"))\r\n\t\t\treviewInfo=api.getReviewPosition().copy()\r\n\t\t\treviewInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t\tspeech.speakTextInfo(reviewInfo,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(charInfo)\r\n\t\t\tcharInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t\tspeech.speakTextInfo(charInfo,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to previous character command.\r\n\tscript_review_previousCharacter.__doc__=_(\"Moves the review cursor to the previous character of the current navigator object and speaks it\")\r\n\tscript_review_previousCharacter.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_currentCharacter(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t# Explicitly tether here\r\n\t\tbraille.handler.setTether(braille.handler.TETHER_REVIEW, auto=True)\r\n\t\tscriptCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif scriptCount==0:\r\n\t\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t\telif scriptCount==1:\r\n\t\t\tspeech.spellTextInfo(info,useCharacterDescriptions=True)\r\n\t\telse:\r\n\t\t\ttry:\r\n\t\t\t\tc = ord(info.text)\r\n\t\t\texcept TypeError:\r\n\t\t\t\t# This might be a character taking multiple code points.\r\n\t\t\t\t# If it is a 32 bit character, encode it to UTF-32 and calculate the ord manually.\r\n\t\t\t\t# In Python 3, this is no longer necessary.\r\n\t\t\t\ttry:\r\n\t\t\t\t\tencoded = info.text.encode(\"utf_32_le\")\r\n\t\t\t\texcept UnicodeEncodeError:\r\n\t\t\t\t\tc = None\r\n\t\t\t\telse:\r\n\t\t\t\t\tif len(encoded)==4:\r\n\t\t\t\t\t\tc = sum(ord(cp)<<i*8 for i, cp in enumerate(encoded))\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\tc = None\r\n\t\t\tif c is not None:\r\n\t\t\t\tspeech.speakMessage(\"%d,\" % c)\r\n\t\t\t\tspeech.speakSpelling(hex(c))\r\n\t\t\telse:\r\n\t\t\t\tlog.debugWarning(\"Couldn't calculate ordinal for character %r\" % info.text)\r\n\t\t\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for report current character under review cursor command.\r\n\tscript_review_currentCharacter.__doc__=_(\"Reports the character of the current navigator object where the review cursor is situated. Pressing twice reports a description or example of that character. Pressing three times reports the numeric value of the character in decimal and hexadecimal\")\r\n\tscript_review_currentCharacter.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_nextCharacter(self,gesture):\r\n\t\tlineInfo=api.getReviewPosition().copy()\r\n\t\tlineInfo.expand(textInfos.UNIT_LINE)\r\n\t\tcharInfo=api.getReviewPosition().copy()\r\n\t\tcharInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tcharInfo.collapse()\r\n\t\tres=charInfo.move(textInfos.UNIT_CHARACTER,1)\r\n\t\tif res==0 or charInfo.compareEndPoints(lineInfo,\"endToEnd\")>=0:\r\n\t\t\t# Translators: a message reported when review cursor is at the rightmost character of the current navigator object's text.\r\n\t\t\tui.reviewMessage(_(\"Right\"))\r\n\t\t\treviewInfo=api.getReviewPosition().copy()\r\n\t\t\treviewInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t\tspeech.speakTextInfo(reviewInfo,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t\telse:\r\n\t\t\tapi.setReviewPosition(charInfo)\r\n\t\t\tcharInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t\tspeech.speakTextInfo(charInfo,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to next character command.\r\n\tscript_review_nextCharacter.__doc__=_(\"Moves the review cursor to the next character of the current navigator object and speaks it\")\r\n\tscript_review_nextCharacter.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_endOfLine(self,gesture):\r\n\t\tinfo=api.getReviewPosition().copy()\r\n\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\tinfo.collapse(end=True)\r\n\t\tinfo.move(textInfos.UNIT_CHARACTER,-1)\r\n\t\tapi.setReviewPosition(info)\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tui.reviewMessage(_(\"Right\"))\r\n\t\tspeech.speakTextInfo(info,unit=textInfos.UNIT_CHARACTER,reason=controlTypes.REASON_CARET)\r\n\t# Translators: Input help mode message for move review cursor to end of current line command.\r\n\tscript_review_endOfLine.__doc__=_(\"Moves the review cursor to the last character of the line where it is situated in the current navigator object and speaks it\")\r\n\tscript_review_endOfLine.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_speechMode(self,gesture):\r\n\t\tcurMode=speech.speechMode\r\n\t\tspeech.speechMode=speech.speechMode_talk\r\n\t\tnewMode=(curMode+1)%3\r\n\t\tif newMode==speech.speechMode_off:\r\n\t\t\t# Translators: A speech mode which disables speech output.\r\n\t\t\tname=_(\"Speech mode off\")\r\n\t\telif newMode==speech.speechMode_beeps:\r\n\t\t\t# Translators: A speech mode which will cause NVDA to beep instead of speaking.\r\n\t\t\tname=_(\"Speech mode beeps\")\r\n\t\telif newMode==speech.speechMode_talk:\r\n\t\t\t# Translators: The normal speech mode; i.e. NVDA will talk as normal.\r\n\t\t\tname=_(\"Speech mode talk\")\r\n\t\tspeech.cancelSpeech()\r\n\t\tui.message(name)\r\n\t\tspeech.speechMode=newMode\r\n\t# Translators: Input help mode message for toggle speech mode command.\r\n\tscript_speechMode.__doc__=_(\"Toggles between the speech modes of off, beep and talk. When set to off NVDA will not speak anything. If beeps then NVDA will simply beep each time it its supposed to speak something. If talk then NVDA wil just speak normally.\")\r\n\tscript_speechMode.category=SCRCAT_SPEECH\r\n\r\n\tdef script_moveToParentTreeInterceptor(self,gesture):\r\n\t\tobj=api.getFocusObject()\r\n\t\tparent=obj.parent\r\n\t\t#Move up parents until the tree interceptor of the parent is different to the tree interceptor of the object.\r\n\t\t#Note that this could include the situation where the parent has no tree interceptor but the object did.\r\n\t\twhile parent and parent.treeInterceptor==obj.treeInterceptor:\r\n\t\t\tparent=parent.parent\r\n\t\t#If the parent has no tree interceptor, keep moving up the parents until we find a parent that does have one.\r\n\t\twhile parent and not parent.treeInterceptor:\r\n\t\t\tparent=parent.parent\r\n\t\tif parent:\r\n\t\t\tparent.treeInterceptor.rootNVDAObject.setFocus()\r\n\t\t\timport eventHandler\r\n\t\t\timport wx\r\n\t\t\t# We must use core.callLater rather than wx.CallLater to ensure that the callback runs within NVDA's core pump.\r\n\t\t\t# If it didn't, and it directly or indirectly called wx.Yield, it could start executing NVDA's core pump from within the yield, causing recursion.\r\n\t\t\tcore.callLater(50,eventHandler.executeEvent,\"gainFocus\",parent.treeInterceptor.rootNVDAObject)\r\n\t# Translators: Input help mode message for move to next document with focus command, mostly used in web browsing to move from embedded object to the webpage document.\r\n\tscript_moveToParentTreeInterceptor.__doc__=_(\"Moves the focus to the next closest document that contains the focus\")\r\n\tscript_moveToParentTreeInterceptor.category=SCRCAT_FOCUS\r\n\r\n\tdef script_toggleVirtualBufferPassThrough(self,gesture):\r\n\t\tfocus = api.getFocusObject()\r\n\t\tvbuf = focus.treeInterceptor\r\n\t\tif not vbuf:\r\n\t\t\t# #2023: Search the focus and its ancestors for an object for which browse mode is optional.\r\n\t\t\tfor obj in itertools.chain((api.getFocusObject(),), reversed(api.getFocusAncestors())):\r\n\t\t\t\tif obj.shouldCreateTreeInterceptor:\r\n\t\t\t\t\tcontinue\r\n\t\t\t\ttry:\r\n\t\t\t\t\tobj.treeInterceptorClass\r\n\t\t\t\texcept:\r\n\t\t\t\t\tcontinue\r\n\t\t\t\tbreak\r\n\t\t\telse:\r\n\t\t\t\treturn\r\n\t\t\t# Force the tree interceptor to be created.\r\n\t\t\tobj.shouldCreateTreeInterceptor = True\r\n\t\t\tti = treeInterceptorHandler.update(obj)\r\n\t\t\tif not ti:\r\n\t\t\t\treturn\r\n\t\t\tif focus in ti:\r\n\t\t\t\t# Update the focus, as it will have cached that there is no tree interceptor.\r\n\t\t\t\tfocus.treeInterceptor = ti\r\n\t\t\t\t# If we just happened to create a browse mode TreeInterceptor\r\n\t\t\t\t# Then ensure that browse mode is reported here. From the users point of view, browse mode was turned on.\r\n\t\t\t\tif isinstance(ti,browseMode.BrowseModeTreeInterceptor) and not ti.passThrough:\r\n\t\t\t\t\tbrowseMode.reportPassThrough(ti,False)\r\n\t\t\t\t\tbraille.handler.handleGainFocus(ti)\r\n\t\t\treturn\r\n\r\n\t\tif not isinstance(vbuf, browseMode.BrowseModeTreeInterceptor):\r\n\t\t\treturn\r\n\t\t# Toggle browse mode pass-through.\r\n\t\tvbuf.passThrough = not vbuf.passThrough\r\n\t\tif isinstance(vbuf,browseMode.BrowseModeTreeInterceptor):\r\n\t\t\t# If we are enabling pass-through, the user has explicitly chosen to do so, so disable auto-pass-through.\r\n\t\t\t# If we're disabling pass-through, re-enable auto-pass-through.\r\n\t\t\tvbuf.disableAutoPassThrough = vbuf.passThrough\r\n\t\tbrowseMode.reportPassThrough(vbuf)\r\n\t# Translators: Input help mode message for toggle focus and browse mode command in web browsing and other situations.\r\n\tscript_toggleVirtualBufferPassThrough.__doc__=_(\"Toggles between browse mode and focus mode. When in focus mode, keys will pass straight through to the application, allowing you to interact directly with a control. When in browse mode, you can navigate the document with the cursor, quick navigation keys, etc.\")\r\n\tscript_toggleVirtualBufferPassThrough.category=inputCore.SCRCAT_BROWSEMODE\r\n\r\n\tdef script_quit(self,gesture):\r\n\t\tgui.quit()\r\n\t# Translators: Input help mode message for quit NVDA command.\r\n\tscript_quit.__doc__=_(\"Quits NVDA!\")\r\n\r\n\tdef script_restart(self,gesture):\r\n\t\tcore.restart()\r\n\t# Translators: Input help mode message for restart NVDA command.\r\n\tscript_restart.__doc__=_(\"Restarts NVDA!\")\r\n\r\n\tdef script_showGui(self,gesture):\r\n\t\tgui.showGui()\r\n\t# Translators: Input help mode message for show NVDA menu command.\r\n\tscript_showGui.__doc__=_(\"Shows the NVDA menu\")\r\n\r\n\tdef script_review_sayAll(self,gesture):\r\n\t\tsayAllHandler.readText(sayAllHandler.CURSOR_REVIEW)\r\n\t# Translators: Input help mode message for say all in review cursor command.\r\n\tscript_review_sayAll.__doc__ = _(\"Reads from the review cursor up to end of current text, moving the review cursor as it goes\")\r\n\tscript_review_sayAll.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_sayAll(self,gesture):\r\n\t\tsayAllHandler.readText(sayAllHandler.CURSOR_CARET)\r\n\t# Translators: Input help mode message for say all with system caret command.\r\n\tscript_sayAll.__doc__ = _(\"Reads from the system caret up to the end of the text, moving the caret as it goes\")\r\n\tscript_sayAll.category=SCRCAT_SYSTEMCARET\r\n\r\n\tdef _reportFormattingHelper(self, info, browseable=False):\r\n\t\tformatConfig={\r\n\t\t\t\"detectFormatAfterCursor\":False,\r\n\t\t\t\"reportFontName\":True,\"reportFontSize\":True,\"reportFontAttributes\":True,\"reportColor\":True,\"reportRevisions\":False,\"reportEmphasis\":False,\r\n\t\t\t\"reportStyle\":True,\"reportAlignment\":True,\"reportSpellingErrors\":True,\r\n\t\t\t\"reportPage\":False,\"reportLineNumber\":False,\"reportLineIndentation\":True,\"reportLineIndentationWithTones\":False,\"reportParagraphIndentation\":True,\"reportLineSpacing\":True,\"reportTables\":False,\r\n\t\t\t\"reportLinks\":False,\"reportHeadings\":False,\"reportLists\":False,\r\n\t\t\t\"reportBlockQuotes\":False,\"reportComments\":False,\r\n\t\t\t\"reportBorderStyle\":True,\"reportBorderColor\":True,\r\n\t\t}\r\n\t\ttextList=[]\r\n\r\n\t\t# First, fetch indentation.\r\n\t\tline=info.copy()\r\n\t\tline.expand(textInfos.UNIT_LINE)\r\n\t\tindentation,content=speech.splitTextIndentation(line.text)\r\n\t\tif indentation:\r\n\t\t\ttextList.append(speech.getIndentationSpeech(indentation, formatConfig))\r\n\t\t\r\n\t\tinfo=info.copy()\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tformatField=textInfos.FormatField()\r\n\t\tfor field in info.getTextWithFields(formatConfig):\r\n\t\t\tif isinstance(field,textInfos.FieldCommand) and isinstance(field.field,textInfos.FormatField):\r\n\t\t\t\tformatField.update(field.field)\r\n\r\n\t\tif not browseable:\r\n\t\t\ttext=info.getFormatFieldSpeech(formatField,formatConfig=formatConfig) if formatField else None\r\n\t\t\tif text:\r\n\t\t\t\ttextList.append(text)\r\n\r\n\t\t\tif not textList:\r\n\t\t\t# Translators: Reported when trying to obtain formatting information (such as font name, indentation and so on) but there is no formatting information for the text under cursor.\r\n\t\t\t\tui.message(_(\"No formatting information\"))\r\n\t\t\t\treturn\r\n\t\t\t\t\r\n\t\t\tui.message(\" \".join(textList))\r\n\t\telse:\r\n\t\t\ttext=info.getFormatFieldSpeech(formatField,formatConfig=formatConfig , separator=\"\\n\") if formatField else None\r\n\t\t\tif text:\r\n\t\t\t\ttextList.append(text)\r\n\r\n\t\t\tif not textList:\r\n\t\t\t\t# Translators: Reported when trying to obtain formatting information (such as font name, indentation and so on) but there is no formatting information for the text under cursor.\r\n\t\t\t\tui.message(_(\"No formatting information\"))\r\n\t\t\t\treturn\r\n\r\n\t\t\t# Translators: title for formatting information dialog.\r\n\t\t\tui.browseableMessage(\"\\n\".join(textList), _(\"Formatting\"))\r\n\r\n\tdef script_reportFormatting(self,gesture):\r\n\t\tinfo=api.getReviewPosition()\r\n\t\trepeats=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif repeats==0:\r\n\t\t\tself._reportFormattingHelper(info,False)\r\n\t\telif repeats==1:\r\n\t\t\tself._reportFormattingHelper(info,True)\r\n\t# Translators: Input help mode message for report formatting command.\r\n\tscript_reportFormatting.__doc__ = _(\"Reports formatting info for the current review cursor position within a document. If pressed twice, presents the information in browse mode\")\r\n\tscript_reportFormatting.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_reportCurrentFocus(self,gesture):\r\n\t\tfocusObject=api.getFocusObject()\r\n\t\tif isinstance(focusObject,NVDAObject):\r\n\t\t\tif scriptHandler.getLastScriptRepeatCount()==0:\r\n\t\t\t\tspeech.speakObject(focusObject, reason=controlTypes.REASON_QUERY)\r\n\t\t\telse:\r\n\t\t\t\tspeech.speakSpelling(focusObject.name)\r\n\t\telse:\r\n\t\t\tui.message(_(\"No focus\"))\r\n\t# Translators: Input help mode message for report current focus command.\r\n\tscript_reportCurrentFocus.__doc__ = _(\"Reports the object with focus. If pressed twice, spells the information\")\r\n\tscript_reportCurrentFocus.category=SCRCAT_FOCUS\r\n\r\n\tdef script_reportStatusLine(self,gesture):\r\n\t\tobj = api.getStatusBar()\r\n\t\tfound=False\r\n\t\tif obj:\r\n\t\t\ttext = api.getStatusBarText(obj)\r\n\t\t\tapi.setNavigatorObject(obj)\r\n\t\t\tfound=True\r\n\t\telse:\r\n\t\t\tinfo=api.getForegroundObject().flatReviewPosition\r\n\t\t\tif info:\r\n\t\t\t\tinfo.expand(textInfos.UNIT_STORY)\r\n\t\t\t\tinfo.collapse(True)\r\n\t\t\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\t\t\ttext=info.text\r\n\t\t\t\tinfo.collapse()\r\n\t\t\t\tapi.setReviewPosition(info)\r\n\t\t\t\tfound=True\r\n\t\tif not found:\r\n\t\t\t# Translators: Reported when there is no status line for the current program or window.\r\n\t\t\tui.message(_(\"No status line found\"))\r\n\t\t\treturn\r\n\t\tif scriptHandler.getLastScriptRepeatCount()==0:\r\n\t\t\tif not text.strip():\r\n\t\t\t\t# Translators: Reported when status line exist, but is empty.\r\n\t\t\t\tui.message(_(\"no status bar information\"))\r\n\t\t\telse:\r\n\t\t\t\tui.message(text)\r\n\t\telif scriptHandler.getLastScriptRepeatCount()==1:\r\n\t\t\tif not  text.strip():\r\n\t\t\t\t# Translators: Reported when status line exist, but is empty.\r\n\t\t\t\tui.message(_(\"no status bar information\"))\r\n\t\t\telse:\r\n\t\t\t\tspeech.speakSpelling(text)\r\n\t\telse:\r\n\t\t\tif not text.strip():\r\n\t\t\t\t# Translators: Reported when user attempts to copy content of the empty status line.\r\n\t\t\t\tui.message(_(\"unable to copy status bar content to clipboard\"))\r\n\t\t\telse:\r\n\t\t\t\tif api.copyToClip(text):\r\n\t\t\t\t\t# Translators: The message presented when the status bar is copied to the clipboard.\r\n\t\t\t\t\tui.message(_(\"%s copied to clipboard\")%text)\r\n\t# Translators: Input help mode message for report status line text command.\r\n\tscript_reportStatusLine.__doc__ = _(\"Reads the current application status bar and moves the navigator to it. If pressed twice, spells the information. If pressed three times, copies the status bar to the clipboard\")\r\n\tscript_reportStatusLine.category=SCRCAT_FOCUS\r\n\r\n\tdef script_toggleMouseTracking(self,gesture):\r\n\t\tif config.conf[\"mouse\"][\"enableMouseTracking\"]:\r\n\t\t\t# Translators: presented when the mouse tracking is toggled.\r\n\t\t\tstate = _(\"Mouse tracking off\")\r\n\t\t\tconfig.conf[\"mouse\"][\"enableMouseTracking\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: presented when the mouse tracking is toggled.\r\n\t\t\tstate = _(\"Mouse tracking on\")\r\n\t\t\tconfig.conf[\"mouse\"][\"enableMouseTracking\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle mouse tracking command.\r\n\tscript_toggleMouseTracking.__doc__=_(\"Toggles the reporting of information as the mouse moves\")\r\n\tscript_toggleMouseTracking.category=SCRCAT_MOUSE\r\n\r\n\tdef script_title(self,gesture):\r\n\t\tobj=api.getForegroundObject()\r\n\t\ttitle=obj.name\r\n\t\tif not isinstance(title,basestring) or not title or title.isspace():\r\n\t\t\ttitle=obj.appModule.appName if obj.appModule else None\r\n\t\t\tif not isinstance(title,basestring) or not title or title.isspace():\r\n\t\t\t\t# Translators: Reported when there is no title text for current program or window.\r\n\t\t\t\ttitle=_(\"No title\")\r\n\t\trepeatCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif repeatCount==0:\r\n\t\t\tui.message(title)\r\n\t\telif repeatCount==1:\r\n\t\t\tspeech.speakSpelling(title)\r\n\t\telse:\r\n\t\t\tif api.copyToClip(title):\r\n\t\t\t\tui.message(_(\"%s copied to clipboard\")%title)\r\n\t# Translators: Input help mode message for report title bar command.\r\n\tscript_title.__doc__=_(\"Reports the title of the current application or foreground window. If pressed twice, spells the title. If pressed three times, copies the title to the clipboard\")\r\n\tscript_title.category=SCRCAT_FOCUS\r\n\r\n\tdef script_speakForeground(self,gesture):\r\n\t\tobj=api.getForegroundObject()\r\n\t\tif obj:\r\n\t\t\tsayAllHandler.readObjects(obj)\r\n\t# Translators: Input help mode message for read foreground object command (usually the foreground window).\r\n\tscript_speakForeground.__doc__ = _(\"Speaks the current foreground object\")\r\n\tscript_speakForeground.category=SCRCAT_FOCUS\r\n\r\n\tdef script_test_navigatorDisplayModelText(self,gesture):\r\n\t\tobj=api.getNavigatorObject()\r\n\t\ttext=obj.displayText\r\n\t\tspeech.speakMessage(text)\r\n\t\tlog.info(text)\r\n\r\n\tdef script_navigatorObject_devInfo(self,gesture):\r\n\t\tobj=api.getNavigatorObject()\r\n\t\tlog.info(\"Developer info for navigator object:\\n%s\" % \"\\n\".join(obj.devInfo), activateLogViewer=True)\r\n\t# Translators: Input help mode message for developer info for current navigator object command, used by developers to examine technical info on navigator object. This command also serves as a shortcut to open NVDA log viewer.\r\n\tscript_navigatorObject_devInfo.__doc__ = _(\"Logs information about the current navigator object which is useful to developers and activates the log viewer so the information can be examined.\")\r\n\tscript_navigatorObject_devInfo.category=SCRCAT_TOOLS\r\n\r\n\tdef script_toggleProgressBarOutput(self,gesture):\r\n\t\toutputMode=config.conf[\"presentation\"][\"progressBarUpdates\"][\"progressBarOutputMode\"]\r\n\t\tif outputMode==\"both\":\r\n\t\t\toutputMode=\"off\"\r\n\t\t\t# Translators: A mode where no progress bar updates are given.\r\n\t\t\tui.message(_(\"No progress bar updates\"))\r\n\t\telif outputMode==\"off\":\r\n\t\t\toutputMode=\"speak\"\r\n\t\t\t# Translators: A mode where progress bar updates will be spoken.\r\n\t\t\tui.message(_(\"Speak progress bar updates\"))\r\n\t\telif outputMode==\"speak\":\r\n\t\t\toutputMode=\"beep\"\r\n\t\t\t# Translators: A mode where beeps will indicate progress bar updates (beeps rise in pitch as progress bar updates).\r\n\t\t\tui.message(_(\"Beep for progress bar updates\"))\r\n\t\telse:\r\n\t\t\toutputMode=\"both\"\r\n\t\t\t# Translators: A mode where both speech and beeps will indicate progress bar updates.\r\n\t\t\tui.message(_(\"Beep and speak progress bar updates\"))\r\n\t\tconfig.conf[\"presentation\"][\"progressBarUpdates\"][\"progressBarOutputMode\"]=outputMode\r\n\t# Translators: Input help mode message for toggle progress bar output command.\r\n\tscript_toggleProgressBarOutput.__doc__=_(\"Toggles between beeps, speech, beeps and speech, and off, for reporting progress bar updates\")\r\n\tscript_toggleProgressBarOutput.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleReportDynamicContentChanges(self,gesture):\r\n\t\tif config.conf[\"presentation\"][\"reportDynamicContentChanges\"]:\r\n\t\t\t# Translators: presented when the present dynamic changes is toggled.\r\n\t\t\tstate = _(\"report dynamic content changes off\")\r\n\t\t\tconfig.conf[\"presentation\"][\"reportDynamicContentChanges\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: presented when the present dynamic changes is toggled.\r\n\t\t\tstate = _(\"report dynamic content changes on\")\r\n\t\t\tconfig.conf[\"presentation\"][\"reportDynamicContentChanges\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle dynamic content changes command.\r\n\tscript_toggleReportDynamicContentChanges.__doc__=_(\"Toggles on and off the reporting of dynamic content changes, such as new text in dos console windows\")\r\n\tscript_toggleReportDynamicContentChanges.category=SCRCAT_SPEECH\r\n\r\n\tdef script_toggleCaretMovesReviewCursor(self,gesture):\r\n\t\tif config.conf[\"reviewCursor\"][\"followCaret\"]:\r\n\t\t\t# Translators: presented when toggled.\r\n\t\t\tstate = _(\"caret moves review cursor off\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"followCaret\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: presented when toggled.\r\n\t\t\tstate = _(\"caret moves review cursor on\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"followCaret\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle caret moves review cursor command.\r\n\tscript_toggleCaretMovesReviewCursor.__doc__=_(\"Toggles on and off the movement of the review cursor due to the caret moving.\")\r\n\tscript_toggleCaretMovesReviewCursor.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_toggleFocusMovesNavigatorObject(self,gesture):\r\n\t\tif config.conf[\"reviewCursor\"][\"followFocus\"]:\r\n\t\t\t# Translators: presented when toggled.\r\n\t\t\tstate = _(\"focus moves navigator object off\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"followFocus\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: presented when toggled.\r\n\t\t\tstate = _(\"focus moves navigator object on\")\r\n\t\t\tconfig.conf[\"reviewCursor\"][\"followFocus\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle focus moves navigator object command.\r\n\tscript_toggleFocusMovesNavigatorObject.__doc__=_(\"Toggles on and off the movement of the navigator object due to focus changes\") \r\n\tscript_toggleFocusMovesNavigatorObject.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\t#added by Rui Batista<ruiandrebatista@gmail.com> to implement a battery status script\r\n\tdef script_say_battery_status(self,gesture):\r\n\t\tUNKNOWN_BATTERY_STATUS = 0xFF\r\n\t\tAC_ONLINE = 0X1\r\n\t\tNO_SYSTEM_BATTERY = 0X80\r\n\t\tsps = winKernel.SYSTEM_POWER_STATUS()\r\n\t\tif not winKernel.GetSystemPowerStatus(sps) or sps.BatteryFlag is UNKNOWN_BATTERY_STATUS:\r\n\t\t\tlog.error(\"error accessing system power status\")\r\n\t\t\treturn\r\n\t\tif sps.BatteryFlag & NO_SYSTEM_BATTERY:\r\n\t\t\t# Translators: This is presented when there is no battery such as desktop computers and laptops with battery pack removed.\r\n\t\t\tui.message(_(\"No system battery\"))\r\n\t\t\treturn\r\n\t\t# Translators: This is presented to inform the user of the current battery percentage.\r\n\t\ttext = _(\"%d percent\") % sps.BatteryLifePercent + \" \"\r\n\t\t# Translators: This is presented when AC power is connected such as when recharging a laptop battery.\r\n\t\tif sps.ACLineStatus & AC_ONLINE: text += _(\"AC power on\")\r\n\t\telif sps.BatteryLifeTime!=0xffffffff: \r\n\t\t\t# Translators: This is the estimated remaining runtime of the laptop battery.\r\n\t\t\ttext += _(\"{hours:d} hours and {minutes:d} minutes remaining\") .format(hours=sps.BatteryLifeTime / 3600, minutes=(sps.BatteryLifeTime % 3600) / 60)\r\n\t\tui.message(text)\r\n\t# Translators: Input help mode message for report battery status command.\r\n\tscript_say_battery_status.__doc__ = _(\"Reports battery status and time remaining if AC is not plugged in\")\r\n\tscript_say_battery_status.category=SCRCAT_SYSTEM\r\n\r\n\tdef script_passNextKeyThrough(self,gesture):\r\n\t\tkeyboardHandler.passNextKeyThrough()\r\n\t\t# Translators: Spoken to indicate that the next key press will be sent straight to the current program as though NVDA is not running.\r\n\t\tui.message(_(\"Pass next key through\"))\r\n\t# Translators: Input help mode message for pass next key through command.\r\n\tscript_passNextKeyThrough.__doc__=_(\"The next key that is pressed will not be handled at all by NVDA, it will be passed directly through to Windows.\")\r\n\tscript_passNextKeyThrough.category=SCRCAT_INPUT\r\n\r\n\tdef script_reportAppModuleInfo(self,gesture):\r\n\t\tfocus=api.getFocusObject()\r\n\t\tappName=appModuleHandler.getAppNameFromProcessID(focus.processID,True)\r\n\t\t# Translators: Indicates the name of the current program (example output: Currently running application is explorer.exe).\r\n\t\t# Note that it does not give friendly name such as Windows Explorer; it presents the file name of the current application.\r\n\t\t# If there is an appModule for the current program, NVDA speaks the name of the module after presenting this message.\r\n\t\tmessage = _(\"Currently running application is %s\") % appName\r\n\t\tmod=focus.appModule\r\n\t\tif isinstance(mod,appModuleHandler.AppModule) and type(mod)!=appModuleHandler.AppModule:\r\n\t\t\t# Translators: Indicates the name of the appModule for the current program (example output: and currently loaded module is explorer).\r\n\t\t\t# For example, the complete message for Windows explorer is: Currently running application is explorer.exe and currently loaded module is explorer.\r\n\t\t\t# This message will not be presented if there is no module for the current program.\r\n\t\t\tmessage += _(\" and currently loaded module is %s\") % mod.appModuleName.split(\".\")[0]\r\n\t\tui.message(message)\r\n\t# Translators: Input help mode message for report current program name and app module name command.\r\n\tscript_reportAppModuleInfo.__doc__ = _(\"Speaks the filename of the active application along with the name of the currently loaded appModule\")\r\n\tscript_reportAppModuleInfo.category=SCRCAT_TOOLS\r\n\r\n\tdef script_activateGeneralSettingsDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onGeneralSettingsCommand, None)\r\n\t# Translators: Input help mode message for go to general settings command.\r\n\tscript_activateGeneralSettingsDialog.__doc__ = _(\"Shows NVDA's general settings\")\r\n\tscript_activateGeneralSettingsDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateSynthesizerDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onSelectSynthesizerCommand, None)\r\n\t# Translators: Input help mode message for go to select synthesizer command.\r\n\tscript_activateSynthesizerDialog.__doc__ = _(\"Shows the NVDA synthesizer selection dialog\")\r\n\tscript_activateSynthesizerDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateVoiceDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onSpeechSettingsCommand, None)\r\n\t# Translators: Input help mode message for go to speech settings command.\r\n\tscript_activateVoiceDialog.__doc__ = _(\"Shows NVDA's speech settings\")\r\n\tscript_activateVoiceDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateBrailleDisplayDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onSelectBrailleDisplayCommand, None)\r\n\t# Translators: Input help mode message for go to select braille display command.\r\n\tscript_activateBrailleDisplayDialog.__doc__ = _(\"Shows the NVDA braille display selection dialog\")\r\n\tscript_activateBrailleDisplayDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateBrailleSettingsDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onBrailleSettingsCommand, None)\r\n\t# Translators: Input help mode message for go to braille settings command.\r\n\tscript_activateBrailleSettingsDialog.__doc__ = _(\"Shows NVDA's braille settings\")\r\n\tscript_activateBrailleSettingsDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateKeyboardSettingsDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onKeyboardSettingsCommand, None)\r\n\t# Translators: Input help mode message for go to keyboard settings command.\r\n\tscript_activateKeyboardSettingsDialog.__doc__ = _(\"Shows NVDA's keyboard settings\")\r\n\tscript_activateKeyboardSettingsDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateMouseSettingsDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onMouseSettingsCommand, None)\r\n\t# Translators: Input help mode message for go to mouse settings command.\r\n\tscript_activateMouseSettingsDialog.__doc__ = _(\"Shows NVDA's mouse settings\")\r\n\tscript_activateMouseSettingsDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateReviewCursorDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onReviewCursorCommand, None)\r\n\t# Translators: Input help mode message for go to review cursor settings command.\r\n\tscript_activateReviewCursorDialog.__doc__ = _(\"Shows NVDA's review cursor settings\")\r\n\tscript_activateReviewCursorDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateInputCompositionDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onInputCompositionCommand, None)\r\n\t# Translators: Input help mode message for go to input composition settings command.\r\n\tscript_activateInputCompositionDialog.__doc__ = _(\"Shows NVDA's input composition settings\")\r\n\tscript_activateInputCompositionDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateObjectPresentationDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame. onObjectPresentationCommand, None)\r\n\t# Translators: Input help mode message for go to object presentation settings command.\r\n\tscript_activateObjectPresentationDialog.__doc__ = _(\"Shows NVDA's object presentation settings\")\r\n\tscript_activateObjectPresentationDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateBrowseModeDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onBrowseModeCommand, None)\r\n\t# Translators: Input help mode message for go to browse mode settings command.\r\n\tscript_activateBrowseModeDialog.__doc__ = _(\"Shows NVDA's browse mode settings\")\r\n\tscript_activateBrowseModeDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateDocumentFormattingDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onDocumentFormattingCommand, None)\r\n\t# Translators: Input help mode message for go to document formatting settings command.\r\n\tscript_activateDocumentFormattingDialog.__doc__ = _(\"Shows NVDA's document formatting settings\")\r\n\tscript_activateDocumentFormattingDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateDefaultDictionaryDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onDefaultDictionaryCommand, None)\r\n\t# Translators: Input help mode message for opening default dictionary dialog.\r\n\tscript_activateDefaultDictionaryDialog.__doc__ = _(\"Shows the NVDA default dictionary dialog\")\r\n\tscript_activateDefaultDictionaryDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateVoiceDictionaryDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onVoiceDictionaryCommand, None)\r\n\t# Translators: Input help mode message for opening voice-specific dictionary dialog.\r\n\tscript_activateVoiceDictionaryDialog.__doc__ = _(\"Shows the NVDA voice-specific dictionary dialog\")\r\n\tscript_activateVoiceDictionaryDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateTemporaryDictionaryDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onTemporaryDictionaryCommand, None)\r\n\t# Translators: Input help mode message for opening temporary dictionary.\r\n\tscript_activateTemporaryDictionaryDialog.__doc__ = _(\"Shows the NVDA temporary dictionary dialog\")\r\n\tscript_activateTemporaryDictionaryDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateSpeechSymbolsDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onSpeechSymbolsCommand, None)\r\n\t# Translators: Input help mode message for go to punctuation/symbol pronunciation dialog.\r\n\tscript_activateSpeechSymbolsDialog.__doc__ = _(\"Shows the NVDA symbol pronunciation dialog\")\r\n\tscript_activateSpeechSymbolsDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activateInputGesturesDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onInputGesturesCommand, None)\r\n\t# Translators: Input help mode message for go to input gestures dialog command.\r\n\tscript_activateInputGesturesDialog.__doc__ = _(\"Shows the NVDA input gestures dialog\")\r\n\tscript_activateInputGesturesDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_saveConfiguration(self,gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onSaveConfigurationCommand, None)\r\n\t# Translators: Input help mode message for save current configuration command.\r\n\tscript_saveConfiguration.__doc__ = _(\"Saves the current NVDA configuration\")\r\n\tscript_saveConfiguration.category=SCRCAT_CONFIG\r\n\r\n\tdef script_revertConfiguration(self,gesture):\r\n\t\tscriptCount=scriptHandler.getLastScriptRepeatCount()\r\n\t\tif scriptCount==0:\r\n\t\t\tgui.mainFrame.onRevertToSavedConfigurationCommand(None)\r\n\t\telif scriptCount==2:\r\n\t\t\tgui.mainFrame.onRevertToDefaultConfigurationCommand(None)\r\n\t# Translators: Input help mode message for apply last saved or default settings command.\r\n\tscript_revertConfiguration.__doc__ = _(\"Pressing once reverts the current configuration to the most recently saved state. Pressing three times reverts to factory defaults.\")\r\n\tscript_revertConfiguration.category=SCRCAT_CONFIG\r\n\r\n\tdef script_activatePythonConsole(self,gesture):\r\n\t\tif globalVars.appArgs.secure or config.isAppX:\r\n\t\t\treturn\r\n\t\timport pythonConsole\r\n\t\tif not pythonConsole.consoleUI:\r\n\t\t\tpythonConsole.initialize()\r\n\t\tpythonConsole.consoleUI.console.updateNamespaceSnapshotVars()\r\n\t\tpythonConsole.activate()\r\n\t# Translators: Input help mode message for activate python console command.\r\n\tscript_activatePythonConsole.__doc__ = _(\"Activates the NVDA Python Console, primarily useful for development\")\r\n\tscript_activatePythonConsole.category=SCRCAT_TOOLS\r\n\r\n\tdef script_activateAddonsManager(self,gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onAddonsManagerCommand, None)\r\n\t\t# Translators: Input help mode message for activate manage add-ons command.\r\n\tscript_activateAddonsManager.__doc__ = _(\"Activates the NVDA Add-ons Manager to install and uninstall add-on packages for NVDA\")\r\n\tscript_activateAddonsManager.category=SCRCAT_TOOLS\r\n\r\n\tdef script_toggleSpeechViewer(self,gesture):\r\n\t\tif gui.speechViewer.isActive:\r\n\t\t\t# Translators: The message announced when disabling speech viewer.\r\n\t\t\tstate = _(\"speech viewer disabled\")\r\n\t\t\tgui.speechViewer.deactivate()\r\n\t\t\tgui.mainFrame.sysTrayIcon.menu_tools_toggleSpeechViewer.Check(False)\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when enabling speech viewer.\r\n\t\t\tstate = _(\"speech viewer enabled\")\r\n\t\t\tgui.speechViewer.activate()\r\n\t\t\tgui.mainFrame.sysTrayIcon.menu_tools_toggleSpeechViewer.Check(True)\r\n\t\tui.message(state)\r\n\t\t# Translators: Input help mode message for toggle speech viewer command.\r\n\tscript_toggleSpeechViewer.__doc__ = _(\"Toggles the NVDA Speech viewer, a floating window that allows you to view all the text that NVDA is currently speaking\")\r\n\tscript_toggleSpeechViewer.category=SCRCAT_TOOLS\r\n\r\n\tdef script_braille_toggleTether(self, gesture):\r\n\t\tvalues = [x[0] for x in braille.handler.tetherValues]\r\n\t\tlabels = [x[1] for x in braille.handler.tetherValues]\r\n\t\ttry:\r\n\t\t\tindex = values.index(\r\n\t\t\t\tbraille.handler.TETHER_AUTO if config.conf[\"braille\"][\"autoTether\"] else config.conf[\"braille\"][\"tetherTo\"]\r\n\t\t\t)\r\n\t\texcept:\r\n\t\t\tindex=0\r\n\t\tnewIndex = (index+1) % len(values)\r\n\t\tnewTetherChoice = values[newIndex]\r\n\t\tif newTetherChoice==braille.handler.TETHER_AUTO:\r\n\t\t\tconfig.conf[\"braille\"][\"autoTether\"] = True\r\n\t\t\tconfig.conf[\"braille\"][\"tetherTo\"] = braille.handler.TETHER_FOCUS\r\n\t\telse:\r\n\t\t\tconfig.conf[\"braille\"][\"autoTether\"] = False\r\n\t\t\tbraille.handler.setTether(newTetherChoice, auto=False)\r\n\t\t\tif newTetherChoice==braille.handler.TETHER_REVIEW:\r\n\t\t\t\tbraille.handler.handleReviewMove(shouldAutoTether=False)\r\n\t\t\telse:\r\n\t\t\t\tbraille.handler.handleGainFocus(api.getFocusObject(),shouldAutoTether=False)\r\n\t\t# Translators: Reports which position braille is tethered to\r\n\t\t# (braille can be tethered automatically or to either focus or review position).\r\n\t\tui.message(_(\"Braille tethered %s\") % labels[newIndex])\r\n\t# Translators: Input help mode message for toggle braille tether to command (tethered means connected to or follows).\r\n\tscript_braille_toggleTether.__doc__ = _(\"Toggle tethering of braille between the focus and the review position\")\r\n\tscript_braille_toggleTether.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_toggleFocusContextPresentation(self, gesture):\r\n\t\tvalues = [x[0] for x in braille.focusContextPresentations]\r\n\t\tlabels = [x[1] for x in braille.focusContextPresentations]\r\n\t\ttry:\r\n\t\t\tindex = values.index(config.conf[\"braille\"][\"focusContextPresentation\"])\r\n\t\texcept:\r\n\t\t\tindex=0\r\n\t\tnewIndex = (index+1) % len(values)\r\n\t\tconfig.conf[\"braille\"][\"focusContextPresentation\"] = values[newIndex]\r\n\t\tbraille.invalidateCachedFocusAncestors(0)\r\n\t\tbraille.handler.handleGainFocus(api.getFocusObject())\r\n\t\t# Translators: Reports the new state of braille focus context presentation.\r\n\t\t# %s will be replaced with the context presentation setting.\r\n\t\t# For example, the full message might be \"Braille focus context presentation: fill display for context changes\"\r\n\t\tui.message(_(\"Braille focus context presentation: %s\")%labels[newIndex].lower())\r\n\t# Translators: Input help mode message for toggle braille focus context presentation command.\r\n\tscript_braille_toggleFocusContextPresentation.__doc__ = _(\"Toggle the way context information is presented in braille\")\r\n\tscript_braille_toggleFocusContextPresentation.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_toggleShowCursor(self, gesture):\r\n\t\tif config.conf[\"braille\"][\"showCursor\"]:\r\n\t\t\t# Translators: The message announced when toggling the braille cursor.\r\n\t\t\tstate = _(\"Braille cursor off\")\r\n\t\t\tconfig.conf[\"braille\"][\"showCursor\"]=False\r\n\t\telse:\r\n\t\t\t# Translators: The message announced when toggling the braille cursor.\r\n\t\t\tstate = _(\"Braille cursor on\")\r\n\t\t\tconfig.conf[\"braille\"][\"showCursor\"]=True\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle braille cursor command.\r\n\tscript_braille_toggleShowCursor.__doc__ = _(\"Toggle the braille cursor on and off\")\r\n\tscript_braille_toggleShowCursor.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_cycleCursorShape(self, gesture):\r\n\t\tif not config.conf[\"braille\"][\"showCursor\"]:\r\n\t\t\t# Translators: A message reported when changing the braille cursor shape when the braille cursor is turned off.\r\n\t\t\tui.message(_(\"Braille cursor is turned off\"))\r\n\t\t\treturn\r\n\t\tshapes = [s[0] for s in braille.CURSOR_SHAPES]\r\n\t\tif braille.handler.getTether() == braille.handler.TETHER_FOCUS:\r\n\t\t\tcursorShape = \"cursorShapeFocus\"\r\n\t\telse:\r\n\t\t\tcursorShape = \"cursorShapeReview\"\r\n\t\ttry:\r\n\t\t\tindex = shapes.index(config.conf[\"braille\"][cursorShape]) + 1\r\n\t\texcept:\r\n\t\t\tindex = 1\r\n\t\tif index >= len(braille.CURSOR_SHAPES):\r\n\t\t\tindex = 0\r\n\t\tconfig.conf[\"braille\"][cursorShape] = braille.CURSOR_SHAPES[index][0]\r\n\t\tshapeMsg = braille.CURSOR_SHAPES[index][1]\r\n\t\t# Translators: Reports which braille cursor shape is activated.\r\n\t\tui.message(_(\"Braille cursor %s\") % shapeMsg)\r\n\t# Translators: Input help mode message for cycle braille cursor shape command.\r\n\tscript_braille_cycleCursorShape.__doc__ = _(\"Cycle through the braille cursor shapes\")\r\n\tscript_braille_cycleCursorShape.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_reportClipboardText(self,gesture):\r\n\t\ttry:\r\n\t\t\ttext = api.getClipData()\r\n\t\texcept:\r\n\t\t\ttext = None\r\n\t\tif not text or not isinstance(text,basestring) or text.isspace():\r\n\t\t\t# Translators: Presented when there is no text on the clipboard.\r\n\t\t\tui.message(_(\"There is no text on the clipboard\"))\r\n\t\t\treturn\r\n\t\tif len(text) < 1024: \r\n\t\t\tui.message(text)\r\n\t\telse:\r\n\t\t\t# Translators: If the number of characters on the clipboard is greater than about 1000, it reports this message and gives number of characters on the clipboard.\r\n\t\t\t# Example output: The clipboard contains a large portion of text. It is 2300 characters long.\r\n\t\t\tui.message(_(\"The clipboard contains a large portion of text. It is %s characters long\") % len(text))\r\n\t# Translators: Input help mode message for report clipboard text command.\r\n\tscript_reportClipboardText.__doc__ = _(\"Reports the text on the Windows clipboard\")\r\n\tscript_reportClipboardText.category=SCRCAT_SYSTEM\r\n\r\n\tdef script_review_markStartForCopy(self, gesture):\r\n\t\treviewPos = api.getReviewPosition()\r\n\t\t# attach the marker to obj so that the marker is cleaned up when obj is cleaned up.\r\n\t\treviewPos.obj._copyStartMarker = reviewPos.copy() # represents the start location\r\n\t\treviewPos.obj._selectThenCopyRange = None # we may be part way through a select, reset the copy range.\r\n\t\t# Translators: Indicates start of review cursor text to be copied to clipboard.\r\n\t\tui.message(_(\"Start marked\"))\r\n\t# Translators: Input help mode message for mark review cursor position for a select or copy command (that is, marks the current review cursor position as the starting point for text to be selected).\r\n\tscript_review_markStartForCopy.__doc__ = _(\"Marks the current position of the review cursor as the start of text to be selected or copied\")\r\n\tscript_review_markStartForCopy.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_review_copy(self, gesture):\r\n\t\tpos = api.getReviewPosition().copy()\r\n\t\tif not getattr(pos.obj, \"_copyStartMarker\", None):\r\n\t\t\t# Translators: Presented when attempting to copy some review cursor text but there is no start marker.\r\n\t\t\tui.message(_(\"No start marker set\"))\r\n\t\t\treturn\r\n\t\tstartMarker = api.getReviewPosition().obj._copyStartMarker\r\n\t\t# first call, try to set the selection.\r\n\t\tif scriptHandler.getLastScriptRepeatCount()==0 :\r\n\t\t\tif getattr(pos.obj, \"_selectThenCopyRange\", None):\r\n\t\t\t\t# we have already tried selecting the text, dont try again. For now selections can not be ammended.\r\n\t\t\t\t# Translators: Presented when text has already been marked for selection, but not yet copied.\r\n\t\t\t\tui.message(_(\"Press twice to copy or reset the start marker\"))\r\n\t\t\t\treturn\r\n\t\t\tcopyMarker = startMarker.copy()\r\n\t\t\t# Check if the end position has moved\r\n\t\t\tif pos.compareEndPoints(startMarker, \"endToEnd\") > 0: # user has moved the cursor 'forward'\r\n\t\t\t\t# start becomes the original start\r\n\t\t\t\tcopyMarker.setEndPoint(startMarker, \"startToStart\")\r\n\t\t\t\t# end needs to be updated to the current cursor position.\r\n\t\t\t\tcopyMarker.setEndPoint(pos, \"endToEnd\")\r\n\t\t\t\tcopyMarker.move(textInfos.UNIT_CHARACTER, 1, endPoint=\"end\")\r\n\t\t\telse:# user has moved the cursor 'backwards' or not at all.\r\n\t\t\t\t# when the cursor is not moved at all we still want to select the character have under the cursor\r\n\t\t\t\t# start becomes the current cursor position position\r\n\t\t\t\tcopyMarker.setEndPoint(pos, \"startToStart\")\r\n\t\t\t\t# end becomes the original start position plus 1\r\n\t\t\t\tcopyMarker.setEndPoint(startMarker, \"endToEnd\")\r\n\t\t\t\tcopyMarker.move(textInfos.UNIT_CHARACTER, 1, endPoint=\"end\")\r\n\t\t\tif copyMarker.compareEndPoints(copyMarker, \"startToEnd\") == 0:\r\n\t\t\t\t# Translators: Presented when there is no text selection to copy from review cursor.\r\n\t\t\t\tui.message(_(\"No text to copy\"))\r\n\t\t\t\tapi.getReviewPosition().obj._copyStartMarker = None\r\n\t\t\t\treturn\r\n\t\t\tapi.getReviewPosition().obj._selectThenCopyRange = copyMarker\r\n\t\t\t# for applications such as word, where the selected text is not automatically spoken we must monitor it ourself\r\n\t\t\ttry:\r\n\t\t\t\t# old selection info must be saved so that its possible to report on the changes to the selection.\r\n\t\t\t\toldInfo=pos.obj.makeTextInfo(textInfos.POSITION_SELECTION)\r\n\t\t\texcept Exception as e:\r\n\t\t\t\tlog.debug(\"Error trying to get initial selection information %s\" % e)\r\n\t\t\t\tpass\r\n\t\t\ttry:\r\n\t\t\t\tcopyMarker.updateSelection()\r\n\t\t\t\tif hasattr(pos.obj, \"reportSelectionChange\"):\r\n\t\t\t\t\t# wait for applications such as word to update their selection so that we can detect it\r\n\t\t\t\t\ttry:\r\n\t\t\t\t\t\tpos.obj.reportSelectionChange(oldInfo)\r\n\t\t\t\t\texcept Exception as e:\r\n\t\t\t\t\t\tlog.debug(\"Error trying to report the updated selection: %s\" % e)\r\n\t\t\texcept NotImplementedError as e:\r\n\t\t\t\t# we are unable to select the text, leave the _copyStartMarker in place in case the user wishes to copy the text.\r\n\t\t\t\t# Translators: Presented when unable to select the marked text.\r\n\t\t\t\tui.message(_(\"Can't select text, press twice to copy\"))\r\n\t\t\t\tlog.debug(\"Error trying to update selection: %s\" % e)\r\n\t\t\t\treturn\r\n\t\telif scriptHandler.getLastScriptRepeatCount()==1: # the second call, try to copy the text\r\n\t\t\tcopyMarker = pos.obj._selectThenCopyRange\r\n\t\t\tif copyMarker.copyToClipboard():\r\n\t\t\t\t# Translators: Presented when some review text has been copied to clipboard.\r\n\t\t\t\tui.message(_(\"Review selection copied to clipboard\"))\r\n\t\t\telse:\r\n\t\t\t\t# Translators: Presented when unable to copy to the clipboard because of an error.\r\n\t\t\t\tui.message(_(\"Unable to copy\"))\r\n\t\t\t# on the second call always clean up the start marker\r\n\t\t\tapi.getReviewPosition().obj._selectThenCopyRange = None\r\n\t\t\tapi.getReviewPosition().obj._copyStartMarker = None\r\n\t\treturn\r\n\t# Translators: Input help mode message for the select then copy command. The select then copy command first selects the review cursor text, then copies it to the clipboard.\r\n\tscript_review_copy.__doc__ = _(\"If pressed once, the text from the previously set start marker up to and including the current position of the review cursor is selected. If pressed twice, the text is copied to the clipboard\")\r\n\tscript_review_copy.category=SCRCAT_TEXTREVIEW\r\n\r\n\tdef script_braille_scrollBack(self, gesture):\r\n\t\tbraille.handler.scrollBack()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_scrollBack.__doc__ = _(\"Scrolls the braille display back\")\r\n\tscript_braille_scrollBack.bypassInputHelp = True\r\n\tscript_braille_scrollBack.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_scrollForward(self, gesture):\r\n\t\tbraille.handler.scrollForward()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_scrollForward.__doc__ = _(\"Scrolls the braille display forward\")\r\n\tscript_braille_scrollForward.bypassInputHelp = True\r\n\tscript_braille_scrollForward.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_routeTo(self, gesture):\r\n\t\tbraille.handler.routeTo(gesture.routingIndex)\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_routeTo.__doc__ = _(\"Routes the cursor to or activates the object under this braille cell\")\r\n\tscript_braille_routeTo.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_reportFormatting(self, gesture):\r\n\t\tinfo = braille.handler.getTextInfoForWindowPos(gesture.routingIndex)\r\n\t\tif info is None:\r\n\t\t\t# Translators: Reported when trying to obtain formatting information (such as font name, indentation and so on) but there is no formatting information for the text under cursor.\r\n\t\t\tui.message(_(\"No formatting information\"))\r\n\t\t\treturn\r\n\t\tself._reportFormattingHelper(info, False)\r\n\t# Translators: Input help mode message for Braille report formatting command.\r\n\tscript_braille_reportFormatting.__doc__ = _(\"Reports formatting info for the text under this braille cell\")\r\n\tscript_braille_reportFormatting.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_previousLine(self, gesture):\r\n\t\tif braille.handler.buffer.regions: \r\n\t\t\tbraille.handler.buffer.regions[-1].previousLine(start=True)\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_previousLine.__doc__ = _(\"Moves the braille display to the previous line\")\r\n\tscript_braille_previousLine.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_nextLine(self, gesture):\r\n\t\tif braille.handler.buffer.regions: \r\n\t\t\tbraille.handler.buffer.regions[-1].nextLine()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_nextLine.__doc__ = _(\"Moves the braille display to the next line\")\r\n\tscript_braille_nextLine.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_dots(self, gesture):\r\n\t\tbrailleInput.handler.input(gesture.dots)\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_dots.__doc__= _(\"Inputs braille dots via the braille keyboard\")\r\n\tscript_braille_dots.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_toFocus(self, gesture):\r\n\t\tbraille.handler.setTether(braille.handler.TETHER_FOCUS, auto=True)\r\n\t\tif braille.handler.getTether() == braille.handler.TETHER_REVIEW:\r\n\t\t\tself.script_navigatorObject_toFocus(gesture)\r\n\t\telse:\r\n\t\t\tobj = api.getFocusObject()\r\n\t\t\tregion = braille.handler.mainBuffer.regions[-1] if braille.handler.mainBuffer.regions else None\r\n\t\t\tif region and region.obj==obj:\r\n\t\t\t\tbraille.handler.mainBuffer.focus(region)\r\n\t\t\t\tif region.brailleCursorPos is not None:\r\n\t\t\t\t\tbraille.handler.mainBuffer.scrollTo(region, region.brailleCursorPos)\r\n\t\t\t\telif region.brailleSelectionStart is not None:\r\n\t\t\t\t\tbraille.handler.mainBuffer.scrollTo(region, region.brailleSelectionStart)\r\n\t\t\t\tbraille.handler.mainBuffer.updateDisplay()\r\n\t\t\telse:\r\n\t\t\t\tbraille.handler.handleGainFocus(obj,shouldAutoTether=False)\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toFocus.__doc__= _(\"Moves the braille display to the current focus\")\r\n\tscript_braille_toFocus.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_eraseLastCell(self, gesture):\r\n\t\tbrailleInput.handler.eraseLastCell()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_eraseLastCell.__doc__= _(\"Erases the last entered braille cell or character\")\r\n\tscript_braille_eraseLastCell.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_enter(self, gesture):\r\n\t\tbrailleInput.handler.enter()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_enter.__doc__= _(\"Translates any braille input and presses the enter key\")\r\n\tscript_braille_enter.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_translate(self, gesture):\r\n\t\tbrailleInput.handler.translate()\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_translate.__doc__= _(\"Translates any braille input\")\r\n\tscript_braille_translate.category=SCRCAT_BRAILLE\r\n\r\n\tdef script_braille_toggleShift(self, gesture):\r\n\t\tbrailleInput.handler.toggleModifier(\"shift\")\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toggleShift.__doc__= _(\"Virtually toggles the shift key to emulate a keyboard shortcut with braille input\")\r\n\tscript_braille_toggleShift.category=inputCore.SCRCAT_KBEMU\r\n\tscript_braille_toggleShift.bypassInputHelp = True\r\n\r\n\tdef script_braille_toggleControl(self, gesture):\r\n\t\tbrailleInput.handler.toggleModifier(\"control\")\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toggleControl.__doc__= _(\"Virtually toggles the control key to emulate a keyboard shortcut with braille input\")\r\n\tscript_braille_toggleControl.category=inputCore.SCRCAT_KBEMU\r\n\tscript_braille_toggleControl.bypassInputHelp = True\r\n\r\n\tdef script_braille_toggleAlt(self, gesture):\r\n\t\tbrailleInput.handler.toggleModifier(\"alt\")\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toggleAlt.__doc__= _(\"Virtually toggles the alt key to emulate a keyboard shortcut with braille input\")\r\n\tscript_braille_toggleAlt.category=inputCore.SCRCAT_KBEMU\r\n\tscript_braille_toggleAlt.bypassInputHelp = True\r\n\r\n\tdef script_braille_toggleWindows(self, gesture):\r\n\t\tbrailleInput.handler.toggleModifier(\"leftWindows\")\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toggleWindows.__doc__= _(\"Virtually toggles the left windows key to emulate a keyboard shortcut with braille input\")\r\n\tscript_braille_toggleWindows.category=inputCore.SCRCAT_KBEMU\r\n\tscript_braille_toggleAlt.bypassInputHelp = True\r\n\r\n\tdef script_braille_toggleNVDAKey(self, gesture):\r\n\t\tbrailleInput.handler.toggleModifier(\"NVDA\")\r\n\t# Translators: Input help mode message for a braille command.\r\n\tscript_braille_toggleNVDAKey.__doc__= _(\"Virtually toggles the NVDA key to emulate a keyboard shortcut with braille input\")\r\n\tscript_braille_toggleNVDAKey.category=inputCore.SCRCAT_KBEMU\r\n\tscript_braille_toggleNVDAKey.bypassInputHelp = True\r\n\r\n\tdef script_reloadPlugins(self, gesture):\r\n\t\timport globalPluginHandler\r\n\t\tappModuleHandler.reloadAppModules()\r\n\t\tglobalPluginHandler.reloadGlobalPlugins()\r\n\t\tNVDAObject.clearDynamicClassCache()\r\n\t\t# Translators: Presented when plugins (app modules and global plugins) are reloaded.\r\n\t\tui.message(_(\"Plugins reloaded\"))\r\n\t# Translators: Input help mode message for reload plugins command.\r\n\tscript_reloadPlugins.__doc__=_(\"Reloads app modules and global plugins without restarting NVDA, which can be Useful for developers\")\r\n\tscript_reloadPlugins.category=SCRCAT_TOOLS\r\n\r\n\tdef script_navigatorObject_nextInFlow(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tnewObject=None\r\n\t\tif curObject.simpleFirstChild:\r\n\t\t\tnewObject=curObject.simpleFirstChild\r\n\t\telif curObject.simpleNext:\r\n\t\t\tnewObject=curObject.simpleNext\r\n\t\telif curObject.simpleParent:\r\n\t\t\tparent=curObject.simpleParent\r\n\t\t\twhile parent and not parent.simpleNext:\r\n\t\t\t\tparent=parent.simpleParent\r\n\t\t\tif parent:\r\n\t\t\t\tnewObject=parent.simpleNext\r\n\t\tif newObject:\r\n\t\t\tapi.setNavigatorObject(newObject)\r\n\t\t\tspeech.speakObject(newObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: a message when there is no next object when navigating\r\n\t\t\tui.reviewMessage(_(\"No next\"))\r\n\t# Translators: Input help mode message for a touchscreen gesture.\r\n\tscript_navigatorObject_nextInFlow.__doc__=_(\"Moves to the next object in a flattened view of the object navigation hierarchy\")\r\n\tscript_navigatorObject_nextInFlow.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_navigatorObject_previousInFlow(self,gesture):\r\n\t\tcurObject=api.getNavigatorObject()\r\n\t\tnewObject=curObject.simplePrevious\r\n\t\tif newObject:\r\n\t\t\twhile newObject.simpleLastChild:\r\n\t\t\t\tnewObject=newObject.simpleLastChild\r\n\t\telse:\r\n\t\t\tnewObject=curObject.simpleParent\r\n\t\tif newObject:\r\n\t\t\tapi.setNavigatorObject(newObject)\r\n\t\t\tspeech.speakObject(newObject,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# Translators: a message when there is no previous object when navigating\r\n\t\t\tui.reviewMessage(_(\"No previous\"))\r\n\t# Translators: Input help mode message for a touchscreen gesture.\r\n\tscript_navigatorObject_previousInFlow.__doc__=_(\"Moves to the previous object in a flattened view of the object navigation hierarchy\")\r\n\tscript_navigatorObject_previousInFlow.category=SCRCAT_OBJECTNAVIGATION\r\n\r\n\tdef script_touch_changeMode(self,gesture):\r\n\t\tmode=touchHandler.handler._curTouchMode\r\n\t\tindex=touchHandler.availableTouchModes.index(mode)\r\n\t\tindex=(index+1)%len(touchHandler.availableTouchModes)\r\n\t\tnewMode=touchHandler.availableTouchModes[index]\r\n\t\ttouchHandler.handler._curTouchMode=newMode\r\n\t\ttry:\r\n\t\t\tnewModeLabel=touchHandler.touchModeLabels[newMode]\r\n\t\texcept KeyError:\r\n\t\t\t# Translators: Cycles through available touch modes (a group of related touch gestures; example output: \"object mode\"; see the user guide for more information on touch modes).\r\n\t\t\tnewModeLabel=_(\"%s mode\")%newMode\r\n\t\tui.message(newModeLabel)\r\n\t# Translators: Input help mode message for a touchscreen gesture.\r\n\tscript_touch_changeMode.__doc__=_(\"Cycles between available touch modes\")\r\n\tscript_touch_changeMode.category=SCRCAT_TOUCH\r\n\r\n\r\n\tdef script_touch_newExplore(self,gesture):\r\n\t\ttouchHandler.handler.screenExplorer.moveTo(gesture.x,gesture.y,new=True)\r\n\t# Translators: Input help mode message for a touchscreen gesture.\r\n\tscript_touch_newExplore.__doc__=_(\"Reports the object and content directly under your finger\")\r\n\tscript_touch_newExplore.category=SCRCAT_TOUCH\r\n\r\n\tdef script_touch_explore(self,gesture):\r\n\t\ttouchHandler.handler.screenExplorer.moveTo(gesture.x,gesture.y)\r\n\t# Translators: Input help mode message for a touchscreen gesture.\r\n\tscript_touch_explore.__doc__=_(\"Reports the new object or content under your finger if different to where your finger was last\")\r\n\tscript_touch_explore.category=SCRCAT_TOUCH\r\n\r\n\tdef script_touch_hoverUp(self,gesture):\r\n\t\t#Specifically for touch typing with onscreen keyboard keys\r\n\t\t# #7309: by default, one mustdouble tap the touch key. To restore old behavior, go to Touch Interaction dialog and change touch typing option.\r\n\t\tif config.conf[\"touch\"][\"touchTyping\"]:\r\n\t\t\tobj=api.getNavigatorObject()\r\n\t\t\timport NVDAObjects.UIA\r\n\t\t\tif isinstance(obj,NVDAObjects.UIA.UIA) and obj.UIAElement.cachedClassName==\"CRootKey\":\r\n\t\t\t\tobj.doAction()\r\n\tscript_touch_hoverUp.category=SCRCAT_TOUCH\r\n\r\n\tdef script_activateConfigProfilesDialog(self, gesture):\r\n\t\twx.CallAfter(gui.mainFrame.onConfigProfilesCommand, None)\r\n\t# Translators: Describes the command to open the Configuration Profiles dialog.\r\n\tscript_activateConfigProfilesDialog.__doc__ = _(\"Shows the NVDA Configuration Profiles dialog\")\r\n\tscript_activateConfigProfilesDialog.category=SCRCAT_CONFIG\r\n\r\n\tdef script_toggleConfigProfileTriggers(self,gesture):\r\n\t\tif config.conf.profileTriggersEnabled:\r\n\t\t\tconfig.conf.disableProfileTriggers()\r\n\t\t\t# Translators: The message announced when temporarily disabling all configuration profile triggers.\r\n\t\t\tstate = _(\"Configuration profile triggers disabled\")\r\n\t\telse:\r\n\t\t\tconfig.conf.enableProfileTriggers()\r\n\t\t\t# Explicitly trigger profiles for the current application.\r\n\t\t\tmod = api.getForegroundObject().appModule\r\n\t\t\ttrigger = mod._configProfileTrigger = appModuleHandler.AppProfileTrigger(mod.appName)\r\n\t\t\ttrigger.enter()\r\n\t\t\t# Translators: The message announced when re-enabling all configuration profile triggers.\r\n\t\t\tstate = _(\"Configuration profile triggers enabled\")\r\n\t\tui.message(state)\r\n\t# Translators: Input help mode message for toggle configuration profile triggers command.\r\n\tscript_toggleConfigProfileTriggers.__doc__=_(\"Toggles disabling of all configuration profile triggers. Disabling remains in effect until NVDA is restarted\")\r\n\tscript_toggleConfigProfileTriggers.category=SCRCAT_CONFIG\r\n\r\n\tdef script_interactWithMath(self, gesture):\r\n\t\timport mathPres\r\n\t\tmathMl = mathPres.getMathMlFromTextInfo(api.getReviewPosition())\r\n\t\tif not mathMl:\r\n\t\t\tobj = api.getNavigatorObject()\r\n\t\t\tif obj.role == controlTypes.ROLE_MATH:\r\n\t\t\t\ttry:\r\n\t\t\t\t\tmathMl = obj.mathMl\r\n\t\t\t\texcept (NotImplementedError, LookupError):\r\n\t\t\t\t\tmathMl = None\r\n\t\tif not mathMl:\r\n\t\t\t# Translators: Reported when the user attempts math interaction\r\n\t\t\t# with something that isn't math.\r\n\t\t\tui.message(_(\"Not math\"))\r\n\t\t\treturn\r\n\t\tmathPres.interactWithMathMl(mathMl)\r\n\t# Translators: Describes a command.\r\n\tscript_interactWithMath.__doc__ = _(\"Begins interaction with math content\")\r\n\r\n\tdef script_recognizeWithUwpOcr(self, gesture):\r\n\t\tif not winVersion.isUwpOcrAvailable():\r\n\t\t\t# Translators: Reported when Windows 10 OCR is not available.\r\n\t\t\tui.message(_(\"Windows 10 OCR not available\"))\r\n\t\t\treturn\r\n\t\tfrom contentRecog import uwpOcr, recogUi\r\n\t\trecog = uwpOcr.UwpOcr()\r\n\t\trecogUi.recognizeNavigatorObject(recog)\r\n\t# Translators: Describes a command.\r\n\tscript_recognizeWithUwpOcr.__doc__ = _(\"Recognizes the content of the current navigator object with Windows 10 OCR\")\r\n\r\n\t__gestures = {\r\n\t\t# Basic\r\n\t\t\"kb:NVDA+n\": \"showGui\",\r\n\t\t\"kb:NVDA+1\": \"toggleInputHelp\",\r\n\t\t\"kb:NVDA+q\": \"quit\",\r\n\t\t\"kb:NVDA+f2\": \"passNextKeyThrough\",\r\n\t\t\"kb(desktop):NVDA+shift+s\":\"toggleCurrentAppSleepMode\",\r\n\t\t\"kb(laptop):NVDA+shift+z\":\"toggleCurrentAppSleepMode\",\r\n\r\n\t\t# System status\r\n\t\t\"kb:NVDA+f12\": \"dateTime\",\r\n\t\t\"kb:NVDA+shift+b\": \"say_battery_status\",\r\n\t\t\"kb:NVDA+c\": \"reportClipboardText\",\r\n\r\n\t\t# System focus\r\n\t\t\"kb:NVDA+tab\": \"reportCurrentFocus\",\r\n\t\t\"kb:NVDA+t\": \"title\",\r\n\t\t\"kb:NVDA+b\": \"speakForeground\",\r\n\t\t\"kb(desktop):NVDA+end\": \"reportStatusLine\",\r\n\t\t\"kb(laptop):NVDA+shift+end\": \"reportStatusLine\",\r\n\r\n\t\t# System caret\r\n\t\t\"kb(desktop):NVDA+downArrow\": \"sayAll\",\r\n\t\t\"kb(laptop):NVDA+a\": \"sayAll\",\r\n\t\t\"kb(desktop):NVDA+upArrow\": \"reportCurrentLine\",\r\n\t\t\"kb(laptop):NVDA+l\": \"reportCurrentLine\",\r\n\t\t\"kb(desktop):NVDA+shift+upArrow\": \"reportCurrentSelection\",\r\n\t\t\"kb(laptop):NVDA+shift+s\": \"reportCurrentSelection\",\r\n\t\t\"kb:NVDA+f\": \"reportFormatting\",\r\n\r\n\t\t# Object navigation\r\n\t\t\"kb:NVDA+numpad5\": \"navigatorObject_current\",\r\n\t\t\"kb(laptop):NVDA+shift+o\": \"navigatorObject_current\",\r\n\t\t\"kb:NVDA+numpad8\": \"navigatorObject_parent\",\r\n\t\t\"kb(laptop):NVDA+shift+upArrow\": \"navigatorObject_parent\",\r\n\t\t\"ts(object):flickup\":\"navigatorObject_parent\",\r\n\t\t\"kb:NVDA+numpad4\": \"navigatorObject_previous\",\r\n\t\t\"kb(laptop):NVDA+shift+leftArrow\": \"navigatorObject_previous\",\r\n\t\t\"ts(object):flickleft\":\"navigatorObject_previousInFlow\",\r\n\t\t\"ts(object):2finger_flickleft\":\"navigatorObject_previous\",\r\n\t\t\"kb:NVDA+numpad6\": \"navigatorObject_next\",\r\n\t\t\"kb(laptop):NVDA+shift+rightArrow\": \"navigatorObject_next\",\r\n\t\t\"ts(object):flickright\":\"navigatorObject_nextInFlow\",\r\n\t\t\"ts(object):2finger_flickright\":\"navigatorObject_next\",\r\n\t\t\"kb:NVDA+numpad2\": \"navigatorObject_firstChild\",\r\n\t\t\"kb(laptop):NVDA+shift+downArrow\": \"navigatorObject_firstChild\",\r\n\t\t\"ts(object):flickdown\":\"navigatorObject_firstChild\",\r\n\t\t\"kb:NVDA+numpadMinus\": \"navigatorObject_toFocus\",\r\n\t\t\"kb(laptop):NVDA+backspace\": \"navigatorObject_toFocus\",\r\n\t\t\"kb:NVDA+numpadEnter\": \"review_activate\",\r\n\t\t\"kb(laptop):NVDA+enter\": \"review_activate\",\r\n\t\t\"ts:double_tap\": \"review_activate\",\r\n\t\t\"kb:NVDA+shift+numpadMinus\": \"navigatorObject_moveFocus\",\r\n\t\t\"kb(laptop):NVDA+shift+backspace\": \"navigatorObject_moveFocus\",\r\n\t\t\"kb:NVDA+numpadDelete\": \"navigatorObject_currentDimensions\",\r\n\t\t\"kb(laptop):NVDA+delete\": \"navigatorObject_currentDimensions\",\r\n\r\n\t\t#Touch-specific commands\r\n\t\t\"ts:tap\":\"touch_newExplore\",\r\n\t\t\"ts:hoverDown\":\"touch_newExplore\",\r\n\t\t\"ts:hover\":\"touch_explore\",\r\n\t\t\"ts:3finger_tap\":\"touch_changeMode\",\r\n\t\t\"ts:2finger_double_tap\":\"showGui\",\r\n\t\t\"ts:hoverUp\":\"touch_hoverUp\",\r\n\t\t# Review cursor\r\n\t\t\"kb:shift+numpad7\": \"review_top\",\r\n\t\t\"kb(laptop):NVDA+control+home\": \"review_top\",\r\n\t\t\"kb:numpad7\": \"review_previousLine\",\r\n\t\t\"ts(text):flickUp\":\"review_previousLine\",\r\n\t\t\"kb(laptop):NVDA+upArrow\": \"review_previousLine\",\r\n\t\t\"kb:numpad8\": \"review_currentLine\",\r\n\t\t\"kb(laptop):NVDA+shift+.\": \"review_currentLine\",\r\n\t\t\"kb:numpad9\": \"review_nextLine\",\r\n\t\t\"kb(laptop):NVDA+downArrow\": \"review_nextLine\",\r\n\t\t\"ts(text):flickDown\":\"review_nextLine\",\r\n\t\t\"kb:shift+numpad9\": \"review_bottom\",\r\n\t\t\"kb(laptop):NVDA+control+end\": \"review_bottom\",\r\n\t\t\"kb:numpad4\": \"review_previousWord\",\r\n\t\t\"kb(laptop):NVDA+control+leftArrow\": \"review_previousWord\",\r\n\t\t\"ts(text):2finger_flickLeft\":\"review_previousWord\",\r\n\t\t\"kb:numpad5\": \"review_currentWord\",\r\n\t\t\"kb(laptop):NVDA+control+.\": \"review_currentWord\",\r\n\t\t\"ts(text):hoverUp\":\"review_currentWord\",\r\n\t\t\"kb:numpad6\": \"review_nextWord\",\r\n\t\t\"kb(laptop):NVDA+control+rightArrow\": \"review_nextWord\",\r\n\t\t\"ts(text):2finger_flickRight\":\"review_nextWord\",\r\n\t\t\"kb:shift+numpad1\": \"review_startOfLine\",\r\n\t\t\"kb(laptop):NVDA+home\": \"review_startOfLine\",\r\n\t\t\"kb:numpad1\": \"review_previousCharacter\",\r\n\t\t\"kb(laptop):NVDA+leftArrow\": \"review_previousCharacter\",\r\n\t\t\"ts(text):flickLeft\":\"review_previousCharacter\",\r\n\t\t\"kb:numpad2\": \"review_currentCharacter\",\r\n\t\t\"kb(laptop):NVDA+.\": \"review_currentCharacter\",\r\n\t\t\"kb:numpad3\": \"review_nextCharacter\",\r\n\t\t\"kb(laptop):NVDA+rightArrow\": \"review_nextCharacter\",\r\n\t\t\"ts(text):flickRight\":\"review_nextCharacter\",\r\n\t\t\"kb:shift+numpad3\": \"review_endOfLine\",\r\n\t\t\"kb(laptop):NVDA+end\": \"review_endOfLine\",\r\n\t\t\"kb:numpadPlus\": \"review_sayAll\",\r\n\t\t\"kb(laptop):NVDA+shift+a\": \"review_sayAll\",\r\n\t\t\"ts(text):3finger_flickDown\":\"review_sayAll\",\r\n\t\t\"kb:NVDA+f9\": \"review_markStartForCopy\",\r\n\t\t\"kb:NVDA+f10\": \"review_copy\",\r\n\r\n\t\t# Flat review\r\n\t\t\"kb:NVDA+numpad7\": \"reviewMode_next\",\r\n\t\t\"kb(laptop):NVDA+pageUp\": \"reviewMode_next\",\r\n\t\t\"ts(object):2finger_flickUp\": \"reviewMode_next\",\r\n\t\t\"kb:NVDA+numpad1\": \"reviewMode_previous\",\r\n\t\t\"kb(laptop):NVDA+pageDown\": \"reviewMode_previous\",\r\n\t\t\"ts(object):2finger_flickDown\": \"reviewMode_previous\",\r\n\r\n\t\t# Mouse\r\n\t\t\"kb:numpadDivide\": \"leftMouseClick\",\r\n\t\t\"kb(laptop):NVDA+[\": \"leftMouseClick\",\r\n\t\t\"kb:shift+numpadDivide\": \"toggleLeftMouseButton\",\r\n\t\t\"kb(laptop):NVDA+control+[\": \"toggleLeftMouseButton\",\r\n\t\t\"kb:numpadMultiply\": \"rightMouseClick\",\r\n\t\t\"kb(laptop):NVDA+]\": \"rightMouseClick\",\r\n\t\t\"kb:shift+numpadMultiply\": \"toggleRightMouseButton\",\r\n\t\t\"kb(laptop):NVDA+control+]\": \"toggleRightMouseButton\",\r\n\t\t\"kb:NVDA+numpadDivide\": \"moveMouseToNavigatorObject\",\r\n\t\t\"kb(laptop):NVDA+shift+m\": \"moveMouseToNavigatorObject\",\r\n\t\t\"kb:NVDA+numpadMultiply\": \"moveNavigatorObjectToMouse\",\r\n\t\t\"kb(laptop):NVDA+shift+n\": \"moveNavigatorObjectToMouse\",\r\n\r\n\t\t# Tree interceptors\r\n\t\t\"kb:NVDA+space\": \"toggleVirtualBufferPassThrough\",\r\n\t\t\"kb:NVDA+control+space\": \"moveToParentTreeInterceptor\",\r\n\r\n\t\t# Preferences dialogs and panels\r\n\t\t\"kb:NVDA+control+g\": \"activateGeneralSettingsDialog\",\r\n\t\t\"kb:NVDA+control+s\": \"activateSynthesizerDialog\",\r\n\t\t\"kb:NVDA+control+v\": \"activateVoiceDialog\",\r\n\t\t\"kb:NVDA+control+a\": \"activateBrailleDisplayDialog\",\r\n\t\t\"kb:NVDA+control+k\": \"activateKeyboardSettingsDialog\",\r\n\t\t\"kb:NVDA+control+m\": \"activateMouseSettingsDialog\",\r\n\t\t\"kb:NVDA+control+o\": \"activateObjectPresentationDialog\",\r\n\t\t\"kb:NVDA+control+b\": \"activateBrowseModeDialog\",\r\n\t\t\"kb:NVDA+control+d\": \"activateDocumentFormattingDialog\",\r\n\r\n\t\t# Configuration management\r\n\t\t\"kb:NVDA+control+c\": \"saveConfiguration\",\r\n\t\t\"kb:NVDA+control+r\": \"revertConfiguration\",\r\n\t\t\"kb:NVDA+control+p\": \"activateConfigProfilesDialog\",\r\n\r\n\t\t# Settings\r\n\t\t\"kb:NVDA+shift+d\":\"cycleAudioDuckingMode\",\r\n\t\t\"kb:NVDA+2\": \"toggleSpeakTypedCharacters\",\r\n\t\t\"kb:NVDA+3\": \"toggleSpeakTypedWords\",\r\n\t\t\"kb:NVDA+4\": \"toggleSpeakCommandKeys\",\r\n\t\t\"kb:NVDA+p\": \"cycleSpeechSymbolLevel\",\r\n\t\t\"kb:NVDA+s\": \"speechMode\",\r\n\t\t\"kb:NVDA+m\": \"toggleMouseTracking\",\r\n\t\t\"kb:NVDA+u\": \"toggleProgressBarOutput\",\r\n\t\t\"kb:NVDA+5\": \"toggleReportDynamicContentChanges\",\r\n\t\t\"kb:NVDA+6\": \"toggleCaretMovesReviewCursor\",\r\n\t\t\"kb:NVDA+7\": \"toggleFocusMovesNavigatorObject\",\r\n\t\t\"kb:NVDA+control+t\": \"braille_toggleTether\",\r\n\r\n\t\t# Synth settings ring\r\n\t\t\"kb(desktop):NVDA+control+leftArrow\": \"previousSynthSetting\",\r\n\t\t\"kb(laptop):NVDA+shift+control+leftArrow\": \"previousSynthSetting\",\r\n\t\t\"kb(desktop):NVDA+control+rightArrow\": \"nextSynthSetting\",\r\n\t\t\"kb(laptop):NVDA+shift+control+rightArrow\": \"nextSynthSetting\",\r\n\t\t\"kb(desktop):NVDA+control+upArrow\": \"increaseSynthSetting\",\r\n\t\t\"kb(laptop):NVDA+shift+control+upArrow\": \"increaseSynthSetting\",\r\n\t\t\"kb(desktop):NVDA+control+downArrow\": \"decreaseSynthSetting\",\r\n\t\t\"kb(laptop):NVDA+control+shift+downArrow\": \"decreaseSynthSetting\",\r\n\r\n\t\t# Braille keyboard\r\n\t\t\"bk:dots\" : \"braille_dots\",\r\n\t\t\"bk:dot7\" : \"braille_eraseLastCell\",\r\n\t\t\"bk:dot8\" : \"braille_enter\",\r\n\t\t\"bk:dot7+dot8\" : \"braille_translate\",\r\n\r\n\t\t# Tools\r\n\t\t\"kb:NVDA+f1\": \"navigatorObject_devInfo\",\r\n\t\t\"kb:NVDA+control+f1\": \"reportAppModuleInfo\",\r\n\t\t\"kb:NVDA+control+z\": \"activatePythonConsole\",\r\n\t\t\"kb:NVDA+control+f3\": \"reloadPlugins\",\r\n\t\t\"kb(desktop):NVDA+control+f2\": \"test_navigatorDisplayModelText\",\r\n\t\t\"kb:NVDA+alt+m\": \"interactWithMath\",\r\n\t\t\"kb:NVDA+r\": \"recognizeWithUwpOcr\",\r\n\t}\r\n\r\n#: The single global commands instance.\r\n#: @type: L{GlobalCommands}\r\ncommands = GlobalCommands()\r\n", "idx": 1, "id": 23829, "msg": "I think it's a good idea to log something here.", "proj": "nvaccess-nvda", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -92,6 +92,7 @@ func NewReporterKBPKI(config Config, maxErrors, bufSize int) *ReporterKBPKI {\n \t\tlog:              config.MakeLogger(\"\"),\n \t\tnotifyBuffer:     make(chan *keybase1.FSNotification, bufSize),\n \t\tnotifySyncBuffer: make(chan *keybase1.FSPathSyncStatus, bufSize),\n+\t\tsuppressCh:       make(chan time.Duration, 1),\n \t}\n \tvar ctx context.Context\n \tctx, r.canceler = context.WithCancel(context.Background())", "y": 0, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/logger\"\n\t\"github.com/keybase/client/go/protocol/keybase1\"\n\t\"github.com/keybase/kbfs/kbfsmd\"\n\t\"github.com/keybase/kbfs/tlf\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/net/context\"\n)\n\nconst (\n\t// error param keys\n\terrorParamTlf                 = \"tlf\"\n\terrorParamMode                = \"mode\"\n\terrorParamFeature             = \"feature\"\n\terrorParamUsername            = \"username\"\n\terrorParamExternal            = \"external\"\n\terrorParamRekeySelf           = \"rekeyself\"\n\terrorParamUsageBytes          = \"usageBytes\"\n\terrorParamLimitBytes          = \"limitBytes\"\n\terrorParamUsageFiles          = \"usageFiles\"\n\terrorParamLimitFiles          = \"limitFiles\"\n\terrorParamRenameOldFilename   = \"oldFilename\"\n\terrorParamFoldersCreated      = \"foldersCreated\"\n\terrorParamFolderLimit         = \"folderLimit\"\n\terrorParamApplicationExecPath = \"applicationExecPath\"\n\n\t// error operation modes\n\terrorModeRead  = \"read\"\n\terrorModeWrite = \"write\"\n\n\t// features that aren't ready yet\n\terrorFeatureFileLimit = \"2gbFileLimit\"\n\terrorFeatureDirLimit  = \"512kbDirLimit\"\n)\n\nconst connectionStatusConnected keybase1.FSStatusCode = keybase1.FSStatusCode_START\nconst connectionStatusDisconnected keybase1.FSStatusCode = keybase1.FSStatusCode_ERROR\n\n// noErrorNames are lookup names that should not result in an error\n// notification.  These should all be reserved or illegal Keybase\n// usernames that will never be associated with a real account.\nvar noErrorNames = map[string]bool{\n\t\"objects\":        true, // git shells\n\t\"gemfile\":        true, // rvm\n\t\"Gemfile\":        true, // rvm\n\t\"devfs\":          true, // lsof?  KBFS-823\n\t\"_mtn\":           true, // emacs on Linux\n\t\"_MTN\":           true, // emacs on Linux\n\t\"docker-machine\": true, // docker shell stuff\n\t\"HEAD\":           true, // git shell\n\t\"Keybase.app\":    true, // some OSX mount thing\n\t\"DCIM\":           true, // looking for digital pic folder\n\t\"Thumbs.db\":      true, // Windows mounts\n\t\"config\":         true, // Windows, possibly 7-Zip?\n\t\"m4root\":         true, // OS X, iMovie?\n\t\"BDMV\":           true, // OS X, iMovie?\n\t\"node_modules\":   true, // Some npm shell configuration\n\t\"folder\":         true, // Dolphin?  keybase/client#7304\n\t\"avchd\":          true, // Sony PlayMemories Home, keybase/client#6801\n\t\"avchd_bk\":       true, // Sony PlayMemories Home, keybase/client#6801\n\t\"sony\":           true, // Sony PlayMemories Home, keybase/client#6801\n}\n\n// ReporterKBPKI implements the Notify function of the Reporter\n// interface in addition to embedding ReporterSimple for error\n// tracking.  Notify will make RPCs to the keybase daemon.\ntype ReporterKBPKI struct {\n\t*ReporterSimple\n\tconfig           Config\n\tlog              logger.Logger\n\tnotifyBuffer     chan *keybase1.FSNotification\n\tnotifySyncBuffer chan *keybase1.FSPathSyncStatus\n\tcanceler         func()\n}\n\n// NewReporterKBPKI creates a new ReporterKBPKI.\nfunc NewReporterKBPKI(config Config, maxErrors, bufSize int) *ReporterKBPKI {\n\tr := &ReporterKBPKI{\n\t\tReporterSimple:   NewReporterSimple(config.Clock(), maxErrors),\n\t\tconfig:           config,\n\t\tlog:              config.MakeLogger(\"\"),\n\t\tnotifyBuffer:     make(chan *keybase1.FSNotification, bufSize),\n\t\tnotifySyncBuffer: make(chan *keybase1.FSPathSyncStatus, bufSize),\n\t}\n\tvar ctx context.Context\n\tctx, r.canceler = context.WithCancel(context.Background())\n\tgo r.send(ctx)\n\treturn r\n}\n\n// ReportErr implements the Reporter interface for ReporterKBPKI.\nfunc (r *ReporterKBPKI) ReportErr(ctx context.Context,\n\ttlfName tlf.CanonicalName, t tlf.Type, mode ErrorModeType, err error) {\n\tr.ReporterSimple.ReportErr(ctx, tlfName, t, mode, err)\n\n\t// Fire off error popups\n\tparams := make(map[string]string)\n\tfilename := \"\"\n\tvar code keybase1.FSErrorType = -1\n\tswitch e := errors.Cause(err).(type) {\n\tcase ReadAccessError:\n\t\tcode = keybase1.FSErrorType_ACCESS_DENIED\n\t\tparams[errorParamMode] = errorModeRead\n\t\tfilename = e.Filename\n\tcase WriteAccessError:\n\t\tcode = keybase1.FSErrorType_ACCESS_DENIED\n\t\tparams[errorParamUsername] = e.User.String()\n\t\tparams[errorParamMode] = errorModeWrite\n\t\tfilename = e.Filename\n\tcase WriteUnsupportedError:\n\t\tcode = keybase1.FSErrorType_ACCESS_DENIED\n\t\tparams[errorParamMode] = errorModeWrite\n\t\tfilename = e.Filename\n\tcase NoSuchUserError:\n\t\tif !noErrorNames[e.Input] {\n\t\t\tcode = keybase1.FSErrorType_USER_NOT_FOUND\n\t\t\tparams[errorParamUsername] = e.Input\n\t\t\tif strings.ContainsAny(e.Input, \"@:\") {\n\t\t\t\tparams[errorParamExternal] = \"true\"\n\t\t\t} else {\n\t\t\t\tparams[errorParamExternal] = \"false\"\n\t\t\t}\n\t\t}\n\tcase UnverifiableTlfUpdateError:\n\t\tcode = keybase1.FSErrorType_REVOKED_DATA_DETECTED\n\tcase NoCurrentSessionError:\n\t\tcode = keybase1.FSErrorType_NOT_LOGGED_IN\n\tcase NeedSelfRekeyError:\n\t\tcode = keybase1.FSErrorType_REKEY_NEEDED\n\t\tparams[errorParamRekeySelf] = \"true\"\n\tcase NeedOtherRekeyError:\n\t\tcode = keybase1.FSErrorType_REKEY_NEEDED\n\t\tparams[errorParamRekeySelf] = \"false\"\n\tcase FileTooBigError:\n\t\tcode = keybase1.FSErrorType_NOT_IMPLEMENTED\n\t\tparams[errorParamFeature] = errorFeatureFileLimit\n\tcase FileTooBigForCRError:\n\t\tcode = keybase1.FSErrorType_NOT_IMPLEMENTED\n\t\tparams[errorParamFeature] = errorFeatureFileLimit\n\tcase DirTooBigError:\n\t\tcode = keybase1.FSErrorType_NOT_IMPLEMENTED\n\t\tparams[errorParamFeature] = errorFeatureDirLimit\n\tcase kbfsmd.NewMetadataVersionError:\n\t\tcode = keybase1.FSErrorType_OLD_VERSION\n\t\terr = OutdatedVersionError{}\n\tcase kbfsmd.NewMerkleVersionError:\n\t\tcode = keybase1.FSErrorType_OLD_VERSION\n\t\terr = OutdatedVersionError{}\n\tcase NewDataVersionError:\n\t\tcode = keybase1.FSErrorType_OLD_VERSION\n\t\terr = OutdatedVersionError{}\n\tcase OverQuotaWarning:\n\t\tcode = keybase1.FSErrorType_OVER_QUOTA\n\t\tparams[errorParamUsageBytes] = strconv.FormatInt(e.UsageBytes, 10)\n\t\tparams[errorParamLimitBytes] = strconv.FormatInt(e.LimitBytes, 10)\n\tcase *ErrDiskLimitTimeout:\n\t\tif !e.reportable {\n\t\t\treturn\n\t\t}\n\t\tcode = keybase1.FSErrorType_DISK_LIMIT_REACHED\n\t\tparams[errorParamUsageBytes] = strconv.FormatInt(e.usageBytes, 10)\n\t\tparams[errorParamLimitBytes] =\n\t\t\tstrconv.FormatFloat(e.limitBytes, 'f', 0, 64)\n\t\tparams[errorParamUsageFiles] = strconv.FormatInt(e.usageFiles, 10)\n\t\tparams[errorParamLimitFiles] =\n\t\t\tstrconv.FormatFloat(e.limitFiles, 'f', 0, 64)\n\tcase NoSigChainError:\n\t\tcode = keybase1.FSErrorType_NO_SIG_CHAIN\n\t\tparams[errorParamUsername] = e.User.String()\n\tcase kbfsmd.ServerErrorTooManyFoldersCreated:\n\t\tcode = keybase1.FSErrorType_TOO_MANY_FOLDERS\n\t\tparams[errorParamFolderLimit] = strconv.FormatUint(e.Limit, 10)\n\t\tparams[errorParamFoldersCreated] = strconv.FormatUint(e.Created, 10)\n\tcase RenameAcrossDirsError:\n\t\tif len(e.ApplicationExecPath) > 0 {\n\t\t\tcode = keybase1.FSErrorType_EXDEV_NOT_SUPPORTED\n\t\t\tparams[errorParamApplicationExecPath] = e.ApplicationExecPath\n\t\t}\n\t}\n\n\tif code < 0 && err == context.DeadlineExceeded {\n\t\tcode = keybase1.FSErrorType_TIMEOUT\n\t\t// Workaround for DESKTOP-2442\n\t\tfilename = string(tlfName)\n\t}\n\n\tif code >= 0 {\n\t\tn := errorNotification(err, code, tlfName, t, mode, filename, params)\n\t\tr.Notify(ctx, n)\n\t}\n}\n\n// Notify implements the Reporter interface for ReporterKBPKI.\n//\n// TODO: might be useful to get the debug tags out of ctx and store\n//       them in the notifyBuffer as well so that send() can put\n//       them back in its context.\nfunc (r *ReporterKBPKI) Notify(ctx context.Context, notification *keybase1.FSNotification) {\n\tselect {\n\tcase r.notifyBuffer <- notification:\n\tdefault:\n\t\tr.log.CDebugf(ctx, \"ReporterKBPKI: notify buffer full, dropping %+v\",\n\t\t\tnotification)\n\t}\n}\n\n// NotifySyncStatus implements the Reporter interface for ReporterKBPKI.\n//\n// TODO: might be useful to get the debug tags out of ctx and store\n//       them in the notifyBuffer as well so that send() can put\n//       them back in its context.\nfunc (r *ReporterKBPKI) NotifySyncStatus(ctx context.Context,\n\tstatus *keybase1.FSPathSyncStatus) {\n\tselect {\n\tcase r.notifySyncBuffer <- status:\n\tdefault:\n\t\tr.log.CDebugf(ctx, \"ReporterKBPKI: notify sync buffer full, \"+\n\t\t\t\"dropping %+v\", status)\n\t}\n}\n\n// Shutdown implements the Reporter interface for ReporterKBPKI.\nfunc (r *ReporterKBPKI) Shutdown() {\n\tr.canceler()\n\tclose(r.notifyBuffer)\n\tclose(r.notifySyncBuffer)\n}\n\n// send takes notifications out of notifyBuffer and notifySyncBuffer\n// and sends them to the keybase daemon.\nfunc (r *ReporterKBPKI) send(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase notification, ok := <-r.notifyBuffer:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := r.config.KeybaseService().Notify(ctx,\n\t\t\t\tnotification); err != nil {\n\t\t\t\tr.log.CDebugf(ctx, \"ReporterDaemon: error sending \"+\n\t\t\t\t\t\"notification: %s\", err)\n\t\t\t}\n\t\tcase status, ok := <-r.notifySyncBuffer:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := r.config.KeybaseService().NotifySyncStatus(ctx,\n\t\t\t\tstatus); err != nil {\n\t\t\t\tr.log.CDebugf(ctx, \"ReporterDaemon: error sending \"+\n\t\t\t\t\t\"sync status: %s\", err)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// writeNotification creates FSNotifications from paths for file\n// write events.\nfunc writeNotification(file path, finish bool) *keybase1.FSNotification {\n\tn := baseNotification(file, finish)\n\tif file.Tlf.Type() == tlf.Public {\n\t\tn.NotificationType = keybase1.FSNotificationType_SIGNING\n\t} else {\n\t\tn.NotificationType = keybase1.FSNotificationType_ENCRYPTING\n\t}\n\treturn n\n}\n\n// readNotification creates FSNotifications from paths for file\n// read events.\nfunc readNotification(file path, finish bool) *keybase1.FSNotification {\n\tn := baseNotification(file, finish)\n\tif file.Tlf.Type() == tlf.Public {\n\t\tn.NotificationType = keybase1.FSNotificationType_VERIFYING\n\t} else {\n\t\tn.NotificationType = keybase1.FSNotificationType_DECRYPTING\n\t}\n\treturn n\n}\n\n// rekeyNotification creates FSNotifications from TlfHandles for rekey\n// events.\nfunc rekeyNotification(ctx context.Context, config Config, handle *TlfHandle, finish bool) *keybase1.FSNotification {\n\tcode := keybase1.FSStatusCode_START\n\tif finish {\n\t\tcode = keybase1.FSStatusCode_FINISH\n\t}\n\n\treturn &keybase1.FSNotification{\n\t\tFolderType:       handle.Type().FolderType(),\n\t\tFilename:         string(handle.GetCanonicalPath()),\n\t\tStatusCode:       code,\n\t\tNotificationType: keybase1.FSNotificationType_REKEYING,\n\t}\n}\n\nfunc baseFileEditNotification(file path, writer keybase1.UID,\n\tlocalTime time.Time) *keybase1.FSNotification {\n\tn := baseNotification(file, true)\n\tn.WriterUid = writer\n\tn.LocalTime = keybase1.ToTime(localTime)\n\treturn n\n}\n\n// fileCreateNotification creates FSNotifications from paths for file\n// create events.\nfunc fileCreateNotification(file path, writer keybase1.UID,\n\tlocalTime time.Time) *keybase1.FSNotification {\n\tn := baseFileEditNotification(file, writer, localTime)\n\tn.NotificationType = keybase1.FSNotificationType_FILE_CREATED\n\treturn n\n}\n\n// fileModifyNotification creates FSNotifications from paths for file\n// modification events.\nfunc fileModifyNotification(file path, writer keybase1.UID,\n\tlocalTime time.Time) *keybase1.FSNotification {\n\tn := baseFileEditNotification(file, writer, localTime)\n\tn.NotificationType = keybase1.FSNotificationType_FILE_MODIFIED\n\treturn n\n}\n\n// fileDeleteNotification creates FSNotifications from paths for file\n// delete events.\nfunc fileDeleteNotification(file path, writer keybase1.UID,\n\tlocalTime time.Time) *keybase1.FSNotification {\n\tn := baseFileEditNotification(file, writer, localTime)\n\tn.NotificationType = keybase1.FSNotificationType_FILE_DELETED\n\treturn n\n}\n\n// fileRenameNotification creates FSNotifications from paths for file\n// rename events.\nfunc fileRenameNotification(oldFile path, newFile path, writer keybase1.UID,\n\tlocalTime time.Time) *keybase1.FSNotification {\n\tn := baseFileEditNotification(newFile, writer, localTime)\n\tn.NotificationType = keybase1.FSNotificationType_FILE_RENAMED\n\tn.Params = map[string]string{errorParamRenameOldFilename: oldFile.CanonicalPathString()}\n\treturn n\n}\n\n// connectionNotification creates FSNotifications based on whether\n// or not KBFS is online.\nfunc connectionNotification(status keybase1.FSStatusCode) *keybase1.FSNotification {\n\t// TODO finish placeholder\n\treturn &keybase1.FSNotification{\n\t\tNotificationType: keybase1.FSNotificationType_CONNECTION,\n\t\tStatusCode:       status,\n\t}\n}\n\n// baseNotification creates a basic FSNotification without a\n// NotificationType from a path.\nfunc baseNotification(file path, finish bool) *keybase1.FSNotification {\n\tcode := keybase1.FSStatusCode_START\n\tif finish {\n\t\tcode = keybase1.FSStatusCode_FINISH\n\t}\n\n\treturn &keybase1.FSNotification{\n\t\tFilename:   file.CanonicalPathString(),\n\t\tStatusCode: code,\n\t}\n}\n\n// errorNotification creates FSNotifications for errors.\nfunc errorNotification(err error, errType keybase1.FSErrorType,\n\ttlfName tlf.CanonicalName, t tlf.Type, mode ErrorModeType,\n\tfilename string, params map[string]string) *keybase1.FSNotification {\n\tif tlfName != \"\" {\n\t\tparams[errorParamTlf] = string(tlfName)\n\t}\n\tvar nType keybase1.FSNotificationType\n\tswitch mode {\n\tcase ReadMode:\n\t\tparams[errorParamMode] = errorModeRead\n\t\tif t == tlf.Public {\n\t\t\tnType = keybase1.FSNotificationType_VERIFYING\n\t\t} else {\n\t\t\tnType = keybase1.FSNotificationType_DECRYPTING\n\t\t}\n\tcase WriteMode:\n\t\tparams[errorParamMode] = errorModeWrite\n\t\tif t == tlf.Public {\n\t\t\tnType = keybase1.FSNotificationType_SIGNING\n\t\t} else {\n\t\t\tnType = keybase1.FSNotificationType_ENCRYPTING\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"Unknown mode: %v\", mode))\n\t}\n\treturn &keybase1.FSNotification{\n\t\tFolderType:       t.FolderType(),\n\t\tFilename:         filename,\n\t\tStatusCode:       keybase1.FSStatusCode_ERROR,\n\t\tStatus:           err.Error(),\n\t\tErrorType:        errType,\n\t\tParams:           params,\n\t\tNotificationType: nType,\n\t}\n}\n\nfunc mdReadSuccessNotification(handle *TlfHandle,\n\tpublic bool) *keybase1.FSNotification {\n\tparams := make(map[string]string)\n\tif handle != nil {\n\t\tparams[errorParamTlf] = string(handle.GetCanonicalName())\n\t}\n\treturn &keybase1.FSNotification{\n\t\tFolderType:       handle.Type().FolderType(),\n\t\tFilename:         string(handle.GetCanonicalPath()),\n\t\tStatusCode:       keybase1.FSStatusCode_START,\n\t\tNotificationType: keybase1.FSNotificationType_MD_READ_SUCCESS,\n\t\tParams:           params,\n\t}\n}\n", "idx": 2, "id": 19840, "msg": "", "proj": "keybase-kbfs", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,44 @@\n+#\n+# Copyright (C) 2019 Databricks, Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+import unittest\n+\n+import pandas as pd\n+\n+from databricks import koalas\n+from databricks.koalas.testing.utils import ReusedSQLTestCase, TestUtils\n+\n+\n+class SeriesTest(ReusedSQLTestCase, TestUtils):\n+    def test_substraction(self):\n+        date1 = pd.Series(pd.date_range('2012-1-1 12:00:00', periods=3, freq='M'))\n+        date2 = pd.Series(pd.date_range('2013-3-11 21:45:00', periods=3, freq='W'))\n+\n+        pdf = pd.DataFrame(dict(start_date=date1, end_date=date2))\n+\n+        kdf = koalas.from_pandas(pdf)\n+        kdf['diff_seconds'] = kdf['end_date'] - kdf['start_date']\n+\n+        self.assertEqual(list(kdf['diff_seconds'].toPandas()), [35545500, 33644700, 31571100])\n+\n+\n+if __name__ == \"__main__\":\n+    try:\n+        import xmlrunner\n+        testRunner = xmlrunner.XMLTestRunner(output='target/test-reports')\n+    except ImportError:\n+        testRunner = None\n+    unittest.main(testRunner=testRunner, verbosity=2)", "y": 1, "oldf": "", "idx": 1, "id": 8409, "msg": "maybe create a SeriesDatetimeTest, and we will need to put more stuff here.", "proj": "databricks-koalas", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -119,8 +119,11 @@ lldb::TypeSP DWARFASTParserSwift::ParseTypeFromDWARF(const SymbolContext &sc,\n   }\n \n   if (!compiler_type && name) {\n-    if (name.GetStringRef().startswith(\"$swift.\") ||\n-        name.GetStringRef().startswith(SwiftLanguageRuntime::GetCurrentMangledName(\"_TtBp\").c_str())) {  // This is the RawPointerType, need to figure out its name from the AST.\n+    if (name.GetStringRef().startswith(\"$\") ||\n+        name.GetStringRef().startswith(\n+            SwiftLanguageRuntime::GetCurrentMangledName(\"_TtBp\")\n+                .c_str())) { // This is the RawPointerType, need to figure out\n+                             // its name from the AST.\n       swift::ASTContext *swift_ast_ctx = m_ast.GetASTContext();\n       if (swift_ast_ctx)\n         compiler_type =", "y": 1, "oldf": "//===-- DWARFASTParserSwift.cpp ---------------------------------*- C++ -*-===//\n//\n// This source file is part of the Swift.org open source project\n//\n// Copyright (c) 2014 - 2016 Apple Inc. and the Swift project authors\n// Licensed under Apache License v2.0 with Runtime Library Exception\n//\n// See https://swift.org/LICENSE.txt for license information\n// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors\n//\n//===----------------------------------------------------------------------===//\n\n#include \"DWARFASTParserSwift.h\"\n\n#include \"DWARFCompileUnit.h\"\n#include \"DWARFDIE.h\"\n#include \"DWARFDebugInfo.h\"\n#include \"DWARFDefines.h\"\n#include \"SymbolFileDWARF.h\"\n\n#include \"swift/AST/ASTContext.h\"\n\n#include \"lldb/Utility/Status.h\"\n#include \"lldb/Core/Module.h\"\n#include \"lldb/Symbol/CompileUnit.h\"\n#include \"lldb/Symbol/Function.h\"\n#include \"lldb/Symbol/ObjectFile.h\"\n#include \"lldb/Symbol/SwiftASTContext.h\"\n#include \"lldb/Symbol/Type.h\"\n#include \"lldb/Target/SwiftLanguageRuntime.h\"\n#include \"lldb/Utility/Log.h\"\n\nusing namespace lldb;\nusing namespace lldb_private;\n\nDWARFASTParserSwift::DWARFASTParserSwift(SwiftASTContext &ast) : m_ast(ast) {}\n\nDWARFASTParserSwift::~DWARFASTParserSwift() {}\n\nlldb::TypeSP DWARFASTParserSwift::ParseTypeFromDWARF(const SymbolContext &sc,\n                                                     const DWARFDIE &die,\n                                                     Log *log,\n                                                     bool *type_is_new_ptr) {\n  lldb::TypeSP type_sp;\n  CompilerType compiler_type;\n  Status error;\n\n  Declaration decl;\n  ConstString mangled_name;\n  ConstString name;\n\n  DWARFAttributes attributes;\n  const size_t num_attributes = die.GetAttributes(attributes);\n  DWARFFormValue type_attr;\n\n  if (num_attributes > 0) {\n    uint32_t i;\n    for (i = 0; i < num_attributes; ++i) {\n      const dw_attr_t attr = attributes.AttributeAtIndex(i);\n      DWARFFormValue form_value;\n      if (attributes.ExtractFormValueAtIndex(i, form_value)) {\n        switch (attr) {\n        case DW_AT_decl_file:\n          decl.SetFile(sc.comp_unit->GetSupportFiles().GetFileSpecAtIndex(\n              form_value.Unsigned()));\n          break;\n        case DW_AT_decl_line:\n          decl.SetLine(form_value.Unsigned());\n          break;\n        case DW_AT_decl_column:\n          decl.SetColumn(form_value.Unsigned());\n          break;\n        case DW_AT_name:\n          name.SetCString(form_value.AsCString());\n          break;\n        case DW_AT_linkage_name:\n        case DW_AT_MIPS_linkage_name:\n          mangled_name.SetCString(form_value.AsCString());\n          break;\n        default:\n          break;\n        }\n      }\n    }\n  }\n\n  if (!mangled_name && name) {\n    if (SwiftLanguageRuntime::IsSwiftMangledName(name.GetCString()))\n      mangled_name = name;\n    else {\n      const char *type_name_cstr = name.GetCString();\n      // TODO: remove this once all mangled names are always included for all\n      // types in DWARF\n      swift::ModuleDecl *swift_module = m_ast.GetModule(decl.GetFile(), error);\n      if (swift_module)\n        compiler_type = m_ast.FindType(type_name_cstr, swift_module);\n\n      if (!compiler_type) {\n        // Anything from the swift module might be in a DW_TAG_typedef with a\n        // name of \"Int\"\n        // so we shuld also check the swift module if we fail to find our type\n        // until we get\n        // <rdar://problem/15290346> fixed.\n        compiler_type =\n            m_ast.FindFirstType(type_name_cstr, ConstString(\"Swift\"));\n      }\n    }\n  }\n\n  if (mangled_name) {\n    // see if we parsed this type already\n    type_sp = m_ast.GetCachedType(mangled_name);\n    if (type_sp)\n      return type_sp;\n\n    // otherwise figure it out yourself\n    compiler_type =\n        m_ast.GetTypeFromMangledTypename(mangled_name.GetCString(), error);\n  }\n\n  if (!compiler_type && name) {\n    if (name.GetStringRef().startswith(\"$swift.\") ||\n        name.GetStringRef().startswith(SwiftLanguageRuntime::GetCurrentMangledName(\"_TtBp\").c_str())) {  // This is the RawPointerType, need to figure out its name from the AST.\n      swift::ASTContext *swift_ast_ctx = m_ast.GetASTContext();\n      if (swift_ast_ctx)\n        compiler_type =\n            CompilerType(swift_ast_ctx, swift_ast_ctx->TheRawPointerType);\n      else {\n        if (log) {\n          const char *file_name = \"<unknown>\";\n          SymbolFile *sym_file = m_ast.GetSymbolFile();\n          if (sym_file) {\n            ObjectFile *obj_file = sym_file->GetObjectFile();\n            if (obj_file) {\n              ModuleSP module_sp = obj_file->GetModule();\n              if (module_sp)\n                file_name = module_sp->GetFileSpec().GetFilename().AsCString();\n            }\n          }\n          log->Printf(\"Got null AST context while looking up %s in %s.\",\n                      name.AsCString(), file_name);\n        }\n        return TypeSP();\n      }\n    }\n  }\n\n  switch (die.Tag()) {\n  case DW_TAG_inlined_subroutine:\n  case DW_TAG_subprogram:\n  case DW_TAG_subroutine_type:\n    if (!compiler_type || !compiler_type.IsFunctionType()) {\n      // Make sure we at least have some function type. The mangling for the\n      // \"top_level_code\"\n      // is currently returning the emptyTupleType (originally \"_TtT_\") which is not a function type...\n      compiler_type = m_ast.GetVoidFunctionType();\n    }\n    break;\n  default:\n    break;\n  }\n\n  if (compiler_type) {\n    type_sp = TypeSP(new Type(\n        die.GetID(), die.GetDWARF(), compiler_type.GetTypeName(),\n        compiler_type.GetByteSize(nullptr), NULL, LLDB_INVALID_UID,\n        Type::eEncodingIsUID, &decl, compiler_type, Type::eResolveStateFull));\n  }\n\n  // cache this type\n  if (type_sp && mangled_name\n      && SwiftLanguageRuntime::IsSwiftMangledName(mangled_name.GetCString()))\n    m_ast.SetCachedType(mangled_name, type_sp);\n\n  return type_sp;\n}\n\nFunction *DWARFASTParserSwift::ParseFunctionFromDWARF(const SymbolContext &sc,\n                                                      const DWARFDIE &die) {\n  DWARFRangeList func_ranges;\n  const char *name = NULL;\n  const char *mangled = NULL;\n  int decl_file = 0;\n  int decl_line = 0;\n  int decl_column = 0;\n  int call_file = 0;\n  int call_line = 0;\n  int call_column = 0;\n  DWARFExpression frame_base(die.GetCU());\n\n  if (die.Tag() != DW_TAG_subprogram)\n    return NULL;\n\n  if (die.GetDIENamesAndRanges(name, mangled, func_ranges, decl_file, decl_line,\n                               decl_column, call_file, call_line, call_column,\n                               &frame_base)) {\n    // Union of all ranges in the function DIE (if the function is\n    // discontiguous)\n    SymbolFileDWARF *dwarf = die.GetDWARF();\n    AddressRange func_range;\n    lldb::addr_t lowest_func_addr = func_ranges.GetMinRangeBase(0);\n    lldb::addr_t highest_func_addr = func_ranges.GetMaxRangeEnd(0);\n    if (lowest_func_addr != LLDB_INVALID_ADDRESS &&\n        lowest_func_addr <= highest_func_addr) {\n      ModuleSP module_sp(dwarf->GetObjectFile()->GetModule());\n      func_range.GetBaseAddress().ResolveAddressUsingFileSections(\n          lowest_func_addr, module_sp->GetSectionList());\n      if (func_range.GetBaseAddress().IsValid())\n        func_range.SetByteSize(highest_func_addr - lowest_func_addr);\n    }\n\n    if (func_range.GetBaseAddress().IsValid()) {\n      Mangled func_name;\n      if (mangled)\n        func_name.SetValue(ConstString(mangled), true);\n      else\n        func_name.SetValue(ConstString(name), false);\n      \n      // See if this function can throw.  We can't get that from the\n      // mangled name (even though the information is often there)\n      // because Swift reserves the right to omit it from the name\n      // if it doesn't need it.  So instead we look for the\n      // DW_TAG_thrown_type:\n      \n      bool can_throw = false;\n      \n      DWARFDebugInfoEntry *child(die.GetFirstChild().GetDIE());\n      while (child)\n      {\n        if (child->Tag() == DW_TAG_thrown_type)\n        {\n          can_throw = true;\n          break;\n        }\n        child = child->GetSibling();\n      }\n\n      FunctionSP func_sp;\n      std::unique_ptr<Declaration> decl_ap;\n      if (decl_file != 0 || decl_line != 0 || decl_column != 0)\n        decl_ap.reset(new Declaration(\n            sc.comp_unit->GetSupportFiles().GetFileSpecAtIndex(decl_file),\n            decl_line, decl_column));\n\n      if (dwarf->FixupAddress(func_range.GetBaseAddress())) {\n        const user_id_t func_user_id = die.GetID();\n        func_sp.reset(new Function(sc.comp_unit, func_user_id, func_user_id,\n                                   func_name, nullptr,\n                                   func_range, can_throw)); // first address range\n\n        if (func_sp.get() != NULL) {\n          if (frame_base.IsValid())\n            func_sp->GetFrameBaseExpression() = frame_base;\n          sc.comp_unit->AddFunction(func_sp);\n          return func_sp.get();\n        }\n      }\n    }\n  }\n  return NULL;\n}\n\nlldb_private::CompilerDeclContext\nDWARFASTParserSwift::GetDeclContextForUIDFromDWARF(const DWARFDIE &die) {\n  return CompilerDeclContext();\n}\n\nlldb_private::CompilerDeclContext\nDWARFASTParserSwift::GetDeclContextContainingUIDFromDWARF(const DWARFDIE &die) {\n  return CompilerDeclContext();\n}\n", "idx": 1, "id": 16548, "msg": "Should we switch to the new mangled name here? Also, isn't the second condition made redundant by the first? All the mangled names now start with `$S`", "proj": "apple-swift-lldb", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -552,7 +552,7 @@ function consoleHandler(entry) {\n       '[' + entry.level.name + '] ' +\n       entry.message;\n \n-  var level = entry.level.value;\n+  const level = entry.level.value;\n   if (level >= Level.SEVERE.value) {\n     console.error(msg);\n   } else if (level >= Level.WARNING.value) {", "y": 0, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\n'use strict';\n\n/**\n * @fileoverview Defines WebDriver's logging system. The logging system is\n * broken into major components: local and remote logging.\n *\n * The local logging API, which is anchored by the {@linkplain Logger} class is\n * similar to Java's logging API. Loggers, retrieved by\n * {@linkplain #getLogger getLogger(name)}, use hierarchical, dot-delimited\n * namespaces (e.g. \"\" > \"webdriver\" > \"webdriver.logging\"). Recorded log\n * messages are represented by the {@linkplain Entry} class. You can capture log\n * records by {@linkplain Logger#addHandler attaching} a handler function to the\n * desired logger. For convenience, you can quickly enable logging to the\n * console by simply calling {@linkplain #installConsoleHandler\n * installConsoleHandler}.\n *\n * The [remote logging API](https://github.com/SeleniumHQ/selenium/wiki/Logging)\n * allows you to retrieve logs from a remote WebDriver server. This API uses the\n * {@link Preferences} class to define desired log levels prior to creating\n * a WebDriver session:\n *\n *     var prefs = new logging.Preferences();\n *     prefs.setLevel(logging.Type.BROWSER, logging.Level.DEBUG);\n *\n *     var caps = Capabilities.chrome();\n *     caps.setLoggingPrefs(prefs);\n *     // ...\n *\n * Remote log entries, also represented by the {@link Entry} class, may be\n * retrieved via {@link webdriver.WebDriver.Logs}:\n *\n *     driver.manage().logs().get(logging.Type.BROWSER)\n *         .then(function(entries) {\n *            entries.forEach(function(entry) {\n *              console.log('[%s] %s', entry.level.name, entry.message);\n *            });\n *         });\n *\n * **NOTE:** Only a few browsers support the remote logging API (notably\n * Firefox and Chrome). Firefox supports basic logging functionality, while\n * Chrome exposes robust\n * [performance logging](https://chromedriver.chromium.org/logging)\n * options. Remote logging is still considered a non-standard feature, and the\n * APIs exposed by this module for it are non-frozen. This module will be\n * updated, possibly breaking backwards-compatibility, once logging is\n * officially defined by the\n * [W3C WebDriver spec](http://www.w3.org/TR/webdriver/).\n */\n\n/**\n * Defines a message level that may be used to control logging output.\n *\n * @final\n */\nclass Level {\n  /**\n   * @param {string} name the level's name.\n   * @param {number} level the level's numeric value.\n   */\n  constructor(name, level) {\n    if (level < 0) {\n      throw new TypeError('Level must be >= 0');\n    }\n\n    /** @private {string} */\n    this.name_ = name;\n\n    /** @private {number} */\n    this.value_ = level;\n  }\n\n  /** This logger's name. */\n  get name() {\n    return this.name_;\n  }\n\n  /** The numeric log level. */\n  get value() {\n    return this.value_;\n  }\n\n  /** @override */\n  toString() {\n    return this.name;\n  }\n}\n\n/**\n * Indicates no log messages should be recorded.\n * @const\n */\nLevel.OFF = new Level('OFF', Infinity);\n\n\n/**\n * Log messages with a level of `1000` or higher.\n * @const\n */\nLevel.SEVERE = new Level('SEVERE', 1000);\n\n\n/**\n * Log messages with a level of `900` or higher.\n * @const\n */\nLevel.WARNING = new Level('WARNING', 900);\n\n\n/**\n * Log messages with a level of `800` or higher.\n * @const\n */\nLevel.INFO = new Level('INFO', 800);\n\n\n/**\n * Log messages with a level of `700` or higher.\n * @const\n */\nLevel.DEBUG = new Level('DEBUG', 700);\n\n\n/**\n * Log messages with a level of `500` or higher.\n * @const\n */\nLevel.FINE = new Level('FINE', 500);\n\n\n/**\n * Log messages with a level of `400` or higher.\n * @const\n */\nLevel.FINER = new Level('FINER', 400);\n\n\n/**\n * Log messages with a level of `300` or higher.\n * @const\n */\nLevel.FINEST = new Level('FINEST', 300);\n\n\n/**\n * Indicates all log messages should be recorded.\n * @const\n */\nLevel.ALL = new Level('ALL', 0);\n\n\nconst ALL_LEVELS = /** !Set<Level> */new Set([\n  Level.OFF,\n  Level.SEVERE,\n  Level.WARNING,\n  Level.INFO,\n  Level.DEBUG,\n  Level.FINE,\n  Level.FINER,\n  Level.FINEST,\n  Level.ALL\n]);\n\n\nconst LEVELS_BY_NAME = /** !Map<string, !Level> */ new Map([\n  [Level.OFF.name, Level.OFF],\n  [Level.SEVERE.name, Level.SEVERE],\n  [Level.WARNING.name, Level.WARNING],\n  [Level.INFO.name, Level.INFO],\n  [Level.DEBUG.name, Level.DEBUG],\n  [Level.FINE.name, Level.FINE],\n  [Level.FINER.name, Level.FINER],\n  [Level.FINEST.name, Level.FINEST],\n  [Level.ALL.name, Level.ALL]\n]);\n\n\n/**\n * Converts a level name or value to a {@link Level} value. If the name/value\n * is not recognized, {@link Level.ALL} will be returned.\n *\n * @param {(number|string)} nameOrValue The log level name, or value, to\n *     convert.\n * @return {!Level} The converted level.\n */\nfunction getLevel(nameOrValue) {\n  if (typeof nameOrValue === 'string') {\n    return LEVELS_BY_NAME.get(nameOrValue) || Level.ALL;\n  }\n  if (typeof nameOrValue !== 'number') {\n    throw new TypeError('not a string or number');\n  }\n  for (let level of ALL_LEVELS) {\n    if (nameOrValue >= level.value) {\n      return level;\n    }\n  }\n  return Level.ALL;\n}\n\n\n/**\n * Describes a single log entry.\n *\n * @final\n */\nclass Entry {\n  /**\n   * @param {(!Level|string|number)} level The entry level.\n   * @param {string} message The log message.\n   * @param {number=} opt_timestamp The time this entry was generated, in\n   *     milliseconds since 0:00:00, January 1, 1970 UTC. If omitted, the\n   *     current time will be used.\n   * @param {string=} opt_type The log type, if known.\n   */\n  constructor(level, message, opt_timestamp, opt_type) {\n    this.level = level instanceof Level ? level : getLevel(level);\n    this.message = message;\n    this.timestamp =\n        typeof opt_timestamp === 'number' ? opt_timestamp : Date.now();\n    this.type = opt_type || '';\n  }\n\n  /**\n   * @return {{level: string, message: string, timestamp: number,\n   *           type: string}} The JSON representation of this entry.\n   */\n  toJSON() {\n    return {\n      'level': this.level.name,\n      'message': this.message,\n      'timestamp': this.timestamp,\n      'type': this.type\n    };\n  }\n}\n\n\n/**\n * An object used to log debugging messages. Loggers use a hierarchical,\n * dot-separated naming scheme. For instance, \"foo\" is considered the parent of\n * the \"foo.bar\" and an ancestor of \"foo.bar.baz\".\n *\n * Each logger may be assigned a {@linkplain #setLevel log level}, which\n * controls which level of messages will be reported to the\n * {@linkplain #addHandler handlers} attached to this instance. If a log level\n * is not explicitly set on a logger, it will inherit its parent.\n *\n * This class should never be directly instantiated. Instead, users should\n * obtain logger references using the {@linkplain ./logging.getLogger()\n * getLogger()} function.\n *\n * @final\n */\nclass Logger {\n  /**\n   * @param {string} name the name of this logger.\n   * @param {Level=} opt_level the initial level for this logger.\n   */\n  constructor(name, opt_level) {\n    /** @private {string} */\n    this.name_ = name;\n\n    /** @private {Level} */\n    this.level_ = opt_level || null;\n\n    /** @private {Logger} */\n    this.parent_ = null;\n\n    /** @private {Set<function(!Entry)>} */\n    this.handlers_ = null;\n  }\n\n  /** @return {string} the name of this logger. */\n  getName() {\n    return this.name_;\n  }\n\n  /**\n   * @param {Level} level the new level for this logger, or `null` if the logger\n   *     should inherit its level from its parent logger.\n   */\n  setLevel(level) {\n    this.level_ = level;\n  }\n\n  /** @return {Level} the log level for this logger. */\n  getLevel() {\n    return this.level_;\n  }\n\n  /**\n   * @return {!Level} the effective level for this logger.\n   */\n  getEffectiveLevel() {\n    let logger = this;\n    let level;\n    do {\n      level = logger.level_;\n      logger = logger.parent_;\n    } while (logger && !level);\n    return level || Level.OFF;\n  }\n\n  /**\n   * @param {!Level} level the level to check.\n   * @return {boolean} whether messages recorded at the given level are loggable\n   *     by this instance.\n   */\n  isLoggable(level) {\n    return level.value !== Level.OFF.value\n        && level.value >= this.getEffectiveLevel().value;\n  }\n\n  /**\n   * Adds a handler to this logger. The handler will be invoked for each message\n   * logged with this instance, or any of its descendants.\n   *\n   * @param {function(!Entry)} handler the handler to add.\n   */\n  addHandler(handler) {\n    if (!this.handlers_) {\n      this.handlers_ = new Set;\n    }\n    this.handlers_.add(handler);\n  }\n\n  /**\n   * Removes a handler from this logger.\n   *\n   * @param {function(!Entry)} handler the handler to remove.\n   * @return {boolean} whether a handler was successfully removed.\n   */\n  removeHandler(handler) {\n    if (!this.handlers_) {\n      return false;\n    }\n    return this.handlers_.delete(handler);\n  }\n\n  /**\n   * Logs a message at the given level. The message may be defined as a string\n   * or as a function that will return the message. If a function is provided,\n   * it will only be invoked if this logger's\n   * {@linkplain #getEffectiveLevel() effective log level} includes the given\n   * `level`.\n   *\n   * @param {!Level} level the level at which to log the message.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  log(level, loggable) {\n    if (!this.isLoggable(level)) {\n      return;\n    }\n    let message = '[' + this.name_ + '] '\n        + (typeof loggable === 'function' ? loggable() : loggable);\n    let entry = new Entry(level, message, Date.now());\n    for (let logger = this; !!logger; logger = logger.parent_) {\n      if (logger.handlers_) {\n        for (let handler of logger.handlers_) {\n          handler(entry);\n        }\n      }\n    }\n  }\n\n  /**\n   * Logs a message at the {@link Level.SEVERE} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  severe(loggable) {\n    this.log(Level.SEVERE, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.WARNING} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  warning(loggable) {\n    this.log(Level.WARNING, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.INFO} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  info(loggable) {\n    this.log(Level.INFO, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.DEBUG} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  debug(loggable) {\n    this.log(Level.DEBUG, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.FINE} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  fine(loggable) {\n    this.log(Level.FINE, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.FINER} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  finer(loggable) {\n    this.log(Level.FINER, loggable);\n  }\n\n  /**\n   * Logs a message at the {@link Level.FINEST} log level.\n   * @param {(string|function(): string)} loggable the message to log, or a\n   *     function that will return the message.\n   */\n  finest(loggable) {\n    this.log(Level.FINEST, loggable);\n  }\n}\n\n\n/**\n * Maintains a collection of loggers.\n *\n * @final\n */\nclass LogManager {\n  constructor() {\n    /** @private {!Map<string, !Logger>} */\n    this.loggers_ = new Map;\n    this.root_ = new Logger('', Level.OFF);\n  }\n\n  /**\n   * Retrieves a named logger, creating it in the process. This function will\n   * implicitly create the requested logger, and any of its parents, if they\n   * do not yet exist.\n   *\n   * @param {string} name the logger's name.\n   * @return {!Logger} the requested logger.\n   */\n  getLogger(name) {\n    if (!name) {\n      return this.root_;\n    }\n    let parent = this.root_;\n    for (let i = name.indexOf('.'); i != -1; i = name.indexOf('.', i + 1)) {\n      let parentName = name.substr(0, i);\n      parent = this.createLogger_(parentName, parent);\n    }\n    return this.createLogger_(name, parent);\n  }\n\n  /**\n   * Creates a new logger.\n   *\n   * @param {string} name the logger's name.\n   * @param {!Logger} parent the logger's parent.\n   * @return {!Logger} the new logger.\n   * @private\n   */\n  createLogger_(name, parent) {\n    if (this.loggers_.has(name)) {\n      return /** @type {!Logger} */(this.loggers_.get(name));\n    }\n    let logger = new Logger(name, null);\n    logger.parent_ = parent;\n    this.loggers_.set(name, logger);\n    return logger;\n  }\n}\n\n\nconst logManager = new LogManager;\n\n\n/**\n * Retrieves a named logger, creating it in the process. This function will\n * implicitly create the requested logger, and any of its parents, if they\n * do not yet exist.\n *\n * The log level will be unspecified for newly created loggers. Use\n * {@link Logger#setLevel(level)} to explicitly set a level.\n *\n * @param {string} name the logger's name.\n * @return {!Logger} the requested logger.\n */\nfunction getLogger(name) {\n  return logManager.getLogger(name);\n}\n\n\n/**\n * Pads a number to ensure it has a minimum of two digits.\n *\n * @param {number} n the number to be padded.\n * @return {string} the padded number.\n */\nfunction pad(n) {\n  if (n >= 10) {\n    return '' + n;\n  } else {\n    return '0' + n;\n  }\n}\n\n\n/**\n * Logs all messages to the Console API.\n * @param {!Entry} entry the entry to log.\n */\nfunction consoleHandler(entry) {\n  if (typeof console === 'undefined' || !console) {\n    return;\n  }\n\n  var timestamp = new Date(entry.timestamp);\n  var msg =\n      '[' + timestamp.getUTCFullYear() + '-' +\n      pad(timestamp.getUTCMonth() + 1) + '-' +\n      pad(timestamp.getUTCDate()) + 'T' +\n      pad(timestamp.getUTCHours()) + ':' +\n      pad(timestamp.getUTCMinutes()) + ':' +\n      pad(timestamp.getUTCSeconds()) + 'Z] ' +\n      '[' + entry.level.name + '] ' +\n      entry.message;\n\n  var level = entry.level.value;\n  if (level >= Level.SEVERE.value) {\n    console.error(msg);\n  } else if (level >= Level.WARNING.value) {\n    console.warn(msg);\n  } else {\n    console.log(msg);\n  }\n}\n\n\n/**\n * Adds the console handler to the given logger. The console handler will log\n * all messages using the JavaScript Console API.\n *\n * @param {Logger=} opt_logger The logger to add the handler to; defaults\n *     to the root logger.\n */\nfunction addConsoleHandler(opt_logger) {\n  let logger = opt_logger || logManager.root_;\n  logger.addHandler(consoleHandler);\n}\n\n\n/**\n * Removes the console log handler from the given logger.\n *\n * @param {Logger=} opt_logger The logger to remove the handler from; defaults\n *     to the root logger.\n * @see exports.addConsoleHandler\n */\nfunction removeConsoleHandler(opt_logger) {\n  let logger = opt_logger || logManager.root_;\n  logger.removeHandler(consoleHandler);\n}\n\n\n/**\n * Installs the console log handler on the root logger.\n */\nfunction installConsoleHandler() {\n  addConsoleHandler(logManager.root_);\n}\n\n\n/**\n * Common log types.\n * @enum {string}\n */\nconst Type = {\n  /** Logs originating from the browser. */\n  BROWSER: 'browser',\n  /** Logs from a WebDriver client. */\n  CLIENT: 'client',\n  /** Logs from a WebDriver implementation. */\n  DRIVER: 'driver',\n  /** Logs related to performance. */\n  PERFORMANCE: 'performance',\n  /** Logs from the remote server. */\n  SERVER: 'server'\n};\n\n\n/**\n * Describes the log preferences for a WebDriver session.\n *\n * @final\n */\nclass Preferences {\n  constructor() {\n    /** @private {!Map<string, !Level>} */\n    this.prefs_ = new Map;\n  }\n\n  /**\n   * Sets the desired logging level for a particular log type.\n   * @param {(string|Type)} type The log type.\n   * @param {(!Level|string|number)} level The desired log level.\n   * @throws {TypeError} if `type` is not a `string`.\n   */\n  setLevel(type, level) {\n    if (typeof type !== 'string') {\n      throw TypeError('specified log type is not a string: ' + typeof type);\n    }\n    this.prefs_.set(type, level instanceof Level ? level : getLevel(level));\n  }\n\n  /**\n   * Converts this instance to its JSON representation.\n   * @return {!Object<string, string>} The JSON representation of this set of\n   *     preferences.\n   */\n  toJSON() {\n    let json = {};\n    for (let key of this.prefs_.keys()) {\n      json[key] = this.prefs_.get(key).name;\n    }\n    return json;\n  }\n}\n\n\n// PUBLIC API\n\n\nmodule.exports = {\n  Entry: Entry,\n  Level: Level,\n  LogManager: LogManager,\n  Logger: Logger,\n  Preferences: Preferences,\n  Type: Type,\n  addConsoleHandler: addConsoleHandler,\n  getLevel: getLevel,\n  getLogger: getLogger,\n  installConsoleHandler: installConsoleHandler,\n  removeConsoleHandler: removeConsoleHandler\n};\n", "idx": 2, "id": 17440, "msg": "", "proj": "SeleniumHQ-selenium", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -4663,12 +4663,12 @@ public class ZMSImpl implements Authorizer, KeyStore, ZMSHandler {\n         // for consistent handling of all requests, we're going to convert\n         // all incoming object values into lower case (e.g. domain, role,\n         // policy, service, etc name)\n-        \n+\n         domainName = domainName.toLowerCase();\n         setRequestDomain(ctx, domainName);\n-        \n+\n         ServiceIdentities result = new ServiceIdentities();\n-        \n+\n         AthenzDomain domain = getAthenzDomain(domainName, false);\n         if (domain == null) {\n             throw ZMSUtils.notFoundError(\"getServiceIdentities: Domain not found: '\"", "y": 0, "oldf": "/*\n * Copyright 2016 Yahoo Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.yahoo.athenz.zms;\n\nimport com.google.common.primitives.Bytes;\nimport com.yahoo.athenz.auth.*;\nimport com.yahoo.athenz.auth.impl.SimplePrincipal;\nimport com.yahoo.athenz.auth.token.PrincipalToken;\nimport com.yahoo.athenz.auth.util.Crypto;\nimport com.yahoo.athenz.common.metrics.Metric;\nimport com.yahoo.athenz.common.metrics.MetricFactory;\nimport com.yahoo.athenz.common.server.audit.AuditReferenceValidator;\nimport com.yahoo.athenz.common.server.audit.AuditReferenceValidatorFactory;\nimport com.yahoo.athenz.common.server.log.AuditLogger;\nimport com.yahoo.athenz.common.server.log.AuditLoggerFactory;\nimport com.yahoo.athenz.common.server.notification.Notification;\nimport com.yahoo.athenz.common.server.notification.NotificationManager;\nimport com.yahoo.athenz.common.server.rest.Http;\nimport com.yahoo.athenz.common.server.rest.Http.AuthorityList;\nimport com.yahoo.athenz.common.server.status.StatusCheckException;\nimport com.yahoo.athenz.common.server.status.StatusChecker;\nimport com.yahoo.athenz.common.server.status.StatusCheckerFactory;\nimport com.yahoo.athenz.common.server.util.ConfigProperties;\nimport com.yahoo.athenz.common.server.util.ServletRequestUtil;\nimport com.yahoo.athenz.common.server.util.StringUtils;\nimport com.yahoo.athenz.common.utils.SignUtils;\nimport com.yahoo.athenz.zms.config.AllowedOperation;\nimport com.yahoo.athenz.zms.config.AuthorizedService;\nimport com.yahoo.athenz.zms.config.AuthorizedServices;\nimport com.yahoo.athenz.zms.config.SolutionTemplates;\nimport com.yahoo.athenz.zms.notification.*;\nimport com.yahoo.athenz.zms.store.AthenzDomain;\nimport com.yahoo.athenz.zms.store.ObjectStore;\nimport com.yahoo.athenz.zms.store.ObjectStoreFactory;\nimport com.yahoo.athenz.zms.utils.ZMSUtils;\nimport com.yahoo.rdl.JSON;\nimport com.yahoo.rdl.Schema;\nimport com.yahoo.rdl.Timestamp;\nimport com.yahoo.rdl.UUID;\nimport com.yahoo.rdl.Validator;\nimport com.yahoo.rdl.Validator.Result;\nimport org.eclipse.jetty.util.StringUtil;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport javax.ws.rs.core.EntityTag;\nimport javax.ws.rs.core.Response;\nimport java.io.File;\nimport java.io.IOException;\nimport java.net.InetAddress;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.security.PrivateKey;\nimport java.security.PublicKey;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.*;\nimport java.util.concurrent.*;\nimport java.util.function.BiConsumer;\nimport java.util.function.Function;\nimport java.util.regex.Pattern;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nimport static com.yahoo.athenz.common.ServerCommonConsts.USER_DOMAIN_PREFIX;\nimport static com.yahoo.athenz.common.server.notification.NotificationServiceConstants.*;\n\npublic class ZMSImpl implements Authorizer, KeyStore, ZMSHandler {\n\n    private static final Logger LOG = LoggerFactory.getLogger(ZMSImpl.class);\n\n    private static String ROOT_DIR;\n\n    private static final String ROLE_PREFIX = \"role.\";\n    private static final String POLICY_PREFIX = \"policy.\";\n\n    private static final String ADMIN_POLICY_NAME = \"admin\";\n    private static final String ADMIN_ROLE_NAME = \"admin\";\n\n    private static final String META_ATTR_ACCOUNT = \"account\";\n    private static final String META_ATTR_YPM_ID = \"ypmid\";\n    private static final String META_ATTR_ALL = \"all\";\n\n    private static final String SYS_AUTH = \"sys.auth\";\n    private static final String USER_TOKEN_DEFAULT_NAME = \"_self_\";\n\n    // data validation types\n    private static final String TYPE_DOMAIN_NAME = \"DomainName\";\n    private static final String TYPE_ENTITY_NAME = \"EntityName\";\n    private static final String TYPE_SIMPLE_NAME = \"SimpleName\";\n    private static final String TYPE_MEMBER_NAME = \"MemberName\";\n    private static final String TYPE_COMPOUND_NAME = \"CompoundName\";\n    private static final String TYPE_RESOURCE_NAME = \"ResourceName\";\n    private static final String TYPE_SERVICE_NAME = \"ServiceName\";\n    private static final String TYPE_ROLE = \"Role\";\n    private static final String TYPE_POLICY = \"Policy\";\n    private static final String TYPE_ASSERTION = \"Assertion\";\n    private static final String TYPE_SERVICE_IDENTITY = \"ServiceIdentity\";\n    private static final String TYPE_TOP_LEVEL_DOMAIN = \"TopLevelDomain\";\n    private static final String TYPE_SUB_DOMAIN = \"SubDomain\";\n    private static final String TYPE_USER_DOMAIN = \"UserDomain\";\n    private static final String TYPE_DOMAIN_META = \"DomainMeta\";\n    private static final String TYPE_DOMAIN_TEMPLATE = \"DomainTemplate\";\n    private static final String TYPE_TENANT_RESOURCE_GROUP_ROLES = \"TenantResourceGroupRoles\";\n    private static final String TYPE_PROVIDER_RESOURCE_GROUP_ROLES = \"ProviderResourceGroupRoles\";\n    private static final String TYPE_PUBLIC_KEY_ENTRY = \"PublicKeyEntry\";\n    private static final String TYPE_MEMBERSHIP = \"Membership\";\n    private static final String TYPE_QUOTA = \"Quota\";\n    private static final String TYPE_ROLE_SYSTEM_META = \"RoleSystemMeta\";\n    private static final String TYPE_ROLE_META = \"RoleMeta\";\n    private static final String TYPE_SERVICE_IDENTITY_SYSTEM_META = \"ServiceIdentitySystemMeta\";\n    private static final String TYPE_RESOURCE_NAMES = \"ResourceNames\";\n    private static final String TYPE_AUTHORITY_KEYWORD = \"AuthorityKeyword\";\n    private static final String TYPE_AUTHORITY_KEYWORDS = \"AuthorityKeywords\";\n\n    private static final String SERVER_READ_ONLY_MESSAGE = \"Server in Maintenance Read-Only mode. Please try your request later\";\n\n    private static final byte[] PERIOD = { 46 };\n\n    public static Metric metric;\n    public static String serverHostName  = null;\n\n    protected DBService dbService = null;\n    protected Schema schema = null;\n    protected ServerPrivateKey privateKey = null;\n    protected ServerPrivateKey privateECKey = null;\n    protected ServerPrivateKey privateRSAKey = null;\n    protected int userTokenTimeout = 3600;\n    protected boolean virtualDomainSupport = true;\n    protected boolean productIdSupport = false;\n    protected int virtualDomainLimit = 2;\n    protected long signedPolicyTimeout;\n    protected int domainNameMaxLen;\n    protected AuthorizedServices serverAuthorizedServices = null;\n    protected SolutionTemplates serverSolutionTemplates = null;\n    protected Map<String, String> serverPublicKeyMap = null;\n    protected boolean readOnlyMode = false;\n    protected boolean validateUserRoleMembers = false;\n    protected boolean validateServiceRoleMembers = false;\n    protected boolean useMasterCopyForSignedDomains = false;\n    protected Set<String> validateServiceMemberSkipDomains;\n    protected static Validator validator;\n    protected String userDomain;\n    protected String userDomainPrefix;\n    protected String homeDomain;\n    protected String homeDomainPrefix;\n    protected String userDomainAlias;\n    protected String userDomainAliasPrefix;\n    protected String serverRegion = null;\n    protected List<String> addlUserCheckDomainPrefixList = null;\n    protected Http.AuthorityList authorities = null;\n    protected List<String> providerEndpoints = null;\n    protected Set<String> reservedServiceNames = null;\n    protected PrivateKeyStore keyStore = null;\n    protected boolean secureRequestsOnly = true;\n    protected AuditLogger auditLogger = null;\n    protected Authority userAuthority = null;\n    protected Authority principalAuthority = null;\n    protected Set<String> authFreeUriSet = null;\n    protected List<Pattern> authFreeUriList = null;\n    protected Set<String> corsOriginList = null;\n    protected int httpPort;\n    protected int httpsPort;\n    protected int statusPort;\n    protected int serviceNameMinLength;\n    protected Status successServerStatus = null;\n    protected Set<String> reservedSystemDomains = null;\n    protected File healthCheckFile = null;\n    protected AuditReferenceValidator auditReferenceValidator = null;\n    protected NotificationManager notificationManager = null;\n    protected ObjectMapper jsonMapper;\n    protected StatusChecker statusChecker = null;\n    protected ObjectStore objectStore = null;\n\n    // enum to represent our access response since in some cases we want to\n    // handle domain not founds differently instead of just returning failure\n\n    enum AccessStatus {\n        ALLOWED,\n        DENIED,\n        DENIED_INVALID_ROLE_TOKEN\n    }\n\n    enum AthenzObject {\n        ASSERTION {\n            void convertToLowerCase(Object obj) {\n                Assertion assertion = (Assertion) obj;\n                assertion.setAction(assertion.getAction().toLowerCase());\n                assertion.setResource(assertion.getResource().toLowerCase());\n                assertion.setRole(assertion.getRole().toLowerCase());\n            }\n        },\n        DEFAULT_ADMINS {\n            void convertToLowerCase(Object obj) {\n                DefaultAdmins defaultAdmins = (DefaultAdmins) obj;\n                LIST.convertToLowerCase(defaultAdmins.getAdmins());\n            }\n        },\n        DOMAIN_TEMPLATE {\n            void convertToLowerCase(Object obj) {\n                DomainTemplate template = (DomainTemplate) obj;\n                if (template != null) {\n                    LIST.convertToLowerCase(template.getTemplateNames());\n                    List<TemplateParam> params = template.getParams();\n                    if (params != null) {\n                        for (TemplateParam param : params) {\n                            param.setName(param.getName().toLowerCase());\n                            param.setValue(param.getValue().toLowerCase());\n                        }\n                    }\n                }\n            }\n        },\n        DOMAIN_TEMPLATE_LIST {\n            void convertToLowerCase(Object obj) {\n                DomainTemplateList templates = (DomainTemplateList) obj;\n                if (templates != null) {\n                    LIST.convertToLowerCase(templates.getTemplateNames());\n                }\n            }\n        },\n        ENTITY {\n            void convertToLowerCase(Object obj) {\n                Entity entity = (Entity) obj;\n                entity.setName(entity.getName().toLowerCase());\n            }\n        },\n        LIST {\n            void convertToLowerCase(Object obj) {\n                @SuppressWarnings(\"unchecked\")\n                List<String> list = (List<String>) obj;\n                if (list != null) {\n                    ListIterator<String> iter = list.listIterator();\n                    while (iter.hasNext()) {\n                        iter.set(iter.next().toLowerCase());\n                    }\n                }\n            }\n        },\n        MEMBERSHIP {\n            void convertToLowerCase(Object obj) {\n                Membership membership = (Membership) obj;\n                membership.setMemberName(membership.getMemberName().toLowerCase());\n                if (membership.getRoleName() != null) {\n                    membership.setRoleName(membership.getRoleName().toLowerCase());\n                }\n            }\n        },\n        POLICY {\n            void convertToLowerCase(Object obj) {\n                Policy policy = (Policy) obj;\n                policy.setName(policy.getName().toLowerCase());\n                if (policy.getAssertions() != null) {\n                    for (Assertion assertion : policy.getAssertions()) {\n                        ASSERTION.convertToLowerCase(assertion);\n                    }\n                }\n            }\n        },\n        PROVIDER_RESOURCE_GROUP_ROLES {\n            void convertToLowerCase(Object obj) {\n                ProviderResourceGroupRoles tenantRoles = (ProviderResourceGroupRoles) obj;\n                tenantRoles.setDomain(tenantRoles.getDomain().toLowerCase());\n                tenantRoles.setService(tenantRoles.getService().toLowerCase());\n                tenantRoles.setTenant(tenantRoles.getTenant().toLowerCase());\n                tenantRoles.setResourceGroup(tenantRoles.getResourceGroup().toLowerCase());\n                if (tenantRoles.getRoles() != null) {\n                    for (TenantRoleAction roleAction : tenantRoles.getRoles()) {\n                        TENANT_ROLE_ACTION.convertToLowerCase(roleAction);\n                    }\n                }\n            }\n        },\n        PUBLIC_KEY_ENTRY {\n            void convertToLowerCase(Object obj) {\n                PublicKeyEntry keyEntry = (PublicKeyEntry) obj;\n                keyEntry.setId(keyEntry.getId().toLowerCase());\n            }\n        },\n        ROLE {\n            void convertToLowerCase(Object obj) {\n                Role role = (Role) obj;\n                role.setName(role.getName().toLowerCase());\n                if (role.getTrust() != null) {\n                    role.setTrust(role.getTrust().toLowerCase());\n                }\n                LIST.convertToLowerCase(role.getMembers());\n                ROLE_MEMBER.convertToLowerCase(role.getRoleMembers());\n            }\n        },\n        ROLE_META {\n            void convertToLowerCase(Object obj) {\n                RoleMeta roleMeta = (RoleMeta) obj;\n                if (roleMeta.getNotifyRoles() != null) {\n                    roleMeta.setNotifyRoles(roleMeta.getNotifyRoles().toLowerCase());\n                }\n                if (roleMeta.getSignAlgorithm() != null) {\n                    roleMeta.setSignAlgorithm(roleMeta.getSignAlgorithm().toLowerCase());\n                }\n            }\n        },\n        ROLE_MEMBER {\n            void convertToLowerCase(Object obj) {\n                @SuppressWarnings(\"unchecked\")\n                List<RoleMember> list = (List<RoleMember>) obj;\n                if (list != null) {\n                    ListIterator<RoleMember> iter = list.listIterator();\n                    while (iter.hasNext()) {\n                        RoleMember roleMember = iter.next();\n                        iter.set(roleMember.setMemberName(roleMember.getMemberName().toLowerCase()));\n                    }\n                }\n            }\n        },\n        SERVICE_IDENTITY {\n            void convertToLowerCase(Object obj) {\n                ServiceIdentity service = (ServiceIdentity) obj;\n                service.setName(service.getName().toLowerCase());\n                LIST.convertToLowerCase(service.getHosts());\n                if (service.getPublicKeys() != null) {\n                    for (PublicKeyEntry key : service.getPublicKeys()) {\n                        PUBLIC_KEY_ENTRY.convertToLowerCase(key);\n                    }\n                }\n            }\n        },\n        SUB_DOMAIN {\n            void convertToLowerCase(Object obj) {\n                SubDomain subdomain = (SubDomain) obj;\n                subdomain.setName(subdomain.getName().toLowerCase());\n                subdomain.setParent(subdomain.getParent().toLowerCase());\n                if (subdomain.getSignAlgorithm() != null) {\n                    subdomain.setSignAlgorithm(subdomain.getSignAlgorithm().toLowerCase());\n                }\n                LIST.convertToLowerCase(subdomain.getAdminUsers());\n                DOMAIN_TEMPLATE_LIST.convertToLowerCase(subdomain.getTemplates());\n            }\n        },\n        TENANCY {\n            void convertToLowerCase(Object obj) {\n                Tenancy tenancy = (Tenancy) obj;\n                tenancy.setDomain(tenancy.getDomain().toLowerCase());\n                tenancy.setService(tenancy.getService().toLowerCase());\n                LIST.convertToLowerCase(tenancy.getResourceGroups());\n            }\n        },\n        TENANT_RESOURCE_GROUP_ROLES {\n            void convertToLowerCase(Object obj) {\n                TenantResourceGroupRoles tenantRoles = (TenantResourceGroupRoles) obj;\n                tenantRoles.setDomain(tenantRoles.getDomain().toLowerCase());\n                tenantRoles.setService(tenantRoles.getService().toLowerCase());\n                tenantRoles.setTenant(tenantRoles.getTenant().toLowerCase());\n                tenantRoles.setResourceGroup(tenantRoles.getResourceGroup().toLowerCase());\n                if (tenantRoles.getRoles() != null) {\n                    for (TenantRoleAction roleAction : tenantRoles.getRoles()) {\n                        TENANT_ROLE_ACTION.convertToLowerCase(roleAction);\n                    }\n                }\n            }\n        },\n        TENANT_ROLE_ACTION {\n            void convertToLowerCase(Object obj) {\n                TenantRoleAction roleAction = (TenantRoleAction) obj;\n                roleAction.setAction(roleAction.getAction().toLowerCase());\n                roleAction.setRole(roleAction.getRole().toLowerCase());\n            }\n        },\n        TOP_LEVEL_DOMAIN {\n            void convertToLowerCase(Object obj) {\n                TopLevelDomain domain = (TopLevelDomain) obj;\n                domain.setName(domain.getName().toLowerCase());\n                LIST.convertToLowerCase(domain.getAdminUsers());\n                DOMAIN_TEMPLATE_LIST.convertToLowerCase(domain.getTemplates());\n                if (domain.getOrg() != null) {\n                    domain.setOrg(domain.getOrg().toLowerCase());\n                }\n                if (domain.getSignAlgorithm() != null) {\n                    domain.setSignAlgorithm(domain.getSignAlgorithm().toLowerCase());\n                }\n            }\n        },\n        QUOTA {\n            void convertToLowerCase(Object obj) {\n                Quota quota = (Quota) obj;\n                quota.setName(quota.getName().toLowerCase());\n            }\n        },\n        USER_DOMAIN {\n            void convertToLowerCase(Object obj) {\n                UserDomain userDomain = (UserDomain) obj;\n                userDomain.setName(userDomain.getName().toLowerCase());\n                if (userDomain.getSignAlgorithm() != null) {\n                    userDomain.setSignAlgorithm(userDomain.getSignAlgorithm().toLowerCase());\n                }\n                DOMAIN_TEMPLATE_LIST.convertToLowerCase(userDomain.getTemplates());\n            }\n        },\n        DOMAIN_META {\n            void convertToLowerCase(Object obj) {\n                DomainMeta domainMeta = (DomainMeta) obj;\n                if (domainMeta.getCertDnsDomain() != null) {\n                    domainMeta.setCertDnsDomain(domainMeta.getCertDnsDomain().toLowerCase());\n                }\n                if (domainMeta.getOrg() != null) {\n                    domainMeta.setOrg(domainMeta.getOrg().toLowerCase());\n                }\n                if (domainMeta.getSignAlgorithm() != null) {\n                    domainMeta.setSignAlgorithm(domainMeta.getSignAlgorithm().toLowerCase());\n                }\n            }\n        };\n\n        abstract void convertToLowerCase(Object obj);\n    }\n\n    public ZMSImpl() {\n\n        // before doing anything else we need to load our\n        // system properties from our config file\n\n        loadSystemProperties();\n\n        // let's first get our server hostname\n\n        ZMSImpl.serverHostName = getServerHostName();\n\n        // create our json mapper\n\n        jsonMapper = new ObjectMapper();\n\n        // before we do anything we need to load our configuration\n        // settings\n\n        loadConfigurationSettings();\n\n        // load our schema validator - we need this before we initialize\n        // our store, if necessary\n\n        loadSchemaValidator();\n\n        // let's load our audit logger\n\n        loadAuditLogger();\n\n        // load any audit reference validator\n\n        loadAuditRefValidator();\n\n        // load any configured authorities to authenticate principals\n\n        loadAuthorities();\n\n        // we need a private key to sign any tokens and documents\n\n        loadPrivateKeyStore();\n\n        // check if we need to load any metric support for stats\n\n        loadMetricObject();\n\n        // load the Solution templates\n\n        loadSolutionTemplates();\n\n        // our object store - either mysql or file based\n\n        loadObjectStore();\n\n        // initialize our store with default domains\n        // this should only happen when running ZMS in local/debug mode\n        // otherwise the store should have been initialized by now\n\n        initObjectStore();\n\n        // load the list of authorized services\n\n        loadAuthorizedServices();\n\n        // retrieve our public keys\n\n        loadServerPublicKeys();\n\n        // make sure to set the keystore for any instance that requires it\n\n        setAuthorityKeyStore();\n\n        // Initialize Notification Manager\n\n        setNotificationManager();\n\n        //autoupdate templates\n\n        autoApplyTemplates();\n\n        // load the StatusChecker\n\n        loadStatusChecker();\n    }\n\n    private void setNotificationManager() {\n        ZMSNotificationTaskFactory zmsNotificationTaskFactory = new ZMSNotificationTaskFactory(dbService, userDomainPrefix);\n        notificationManager = new NotificationManager(zmsNotificationTaskFactory.getNotificationTasks());\n    }\n\n    void loadSystemProperties() {\n        String propFile = System.getProperty(ZMSConsts.ZMS_PROP_FILE_NAME,\n                getRootDir() + \"/conf/zms_server/zms.properties\");\n        ConfigProperties.loadProperties(propFile);\n    }\n\n    void setAuthorityKeyStore() {\n        for (Authority authority : authorities.getAuthorities()) {\n            if (AuthorityKeyStore.class.isInstance(authority)) {\n                ((AuthorityKeyStore) authority).setKeyStore(this);\n            }\n        }\n    }\n\n    void loadSchemaValidator() {\n        schema = ZMSSchema.instance();\n        validator = new Validator(schema);\n    }\n\n    void loadConfigurationSettings() {\n\n        // make sure all requests run in secure mode\n\n        secureRequestsOnly = Boolean.parseBoolean(System.getProperty(ZMSConsts.ZMS_PROP_SECURE_REQUESTS_ONLY, \"true\"));\n\n        // retrieve the regular and status ports\n\n        httpPort = ConfigProperties.getPortNumber(ZMSConsts.ZMS_PROP_HTTP_PORT,\n                ZMSConsts.ZMS_HTTP_PORT_DEFAULT);\n        httpsPort = ConfigProperties.getPortNumber(ZMSConsts.ZMS_PROP_HTTPS_PORT,\n                ZMSConsts.ZMS_HTTPS_PORT_DEFAULT);\n        statusPort = ConfigProperties.getPortNumber(ZMSConsts.ZMS_PROP_STATUS_PORT, 0);\n\n        successServerStatus = new Status().setCode(ResourceException.OK).setMessage(\"OK\");\n\n        // retrieve the user domain we're supposed to use\n\n        userDomain = System.getProperty(ZMSConsts.ZMS_PROP_USER_DOMAIN, ZMSConsts.USER_DOMAIN);\n        userDomainPrefix = userDomain + \".\";\n\n        userDomainAlias = System.getProperty(ZMSConsts.ZMS_PROP_USER_DOMAIN_ALIAS);\n        if (userDomainAlias != null) {\n            userDomainAliasPrefix = userDomainAlias + \".\";\n        }\n\n        final String addlUserCheckDomains = System.getProperty(ZMSConsts.ZMS_PROP_ADDL_USER_CHECK_DOMAINS);\n        if (addlUserCheckDomains != null && !addlUserCheckDomains.isEmpty()) {\n            String[] checkDomains = addlUserCheckDomains.split(\",\");\n            addlUserCheckDomainPrefixList = new ArrayList<>();\n            for (String checkDomain : checkDomains) {\n                addlUserCheckDomainPrefixList.add(checkDomain + \".\");\n            }\n        }\n\n        homeDomain = System.getProperty(ZMSConsts.ZMS_PROP_HOME_DOMAIN, userDomain);\n        homeDomainPrefix = homeDomain + \".\";\n\n        // default token timeout for issued tokens\n\n        userTokenTimeout = Integer.parseInt(\n                System.getProperty(ZMSConsts.ZMS_PROP_TIMEOUT, \"3600\"));\n\n        // check if we need to run in maintenance read only mode\n\n        readOnlyMode = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_READ_ONLY_MODE, \"false\"));\n\n        // check to see if we need to validate all user and service members\n        // when adding them to roles\n\n        validateUserRoleMembers = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_VALIDATE_USER_MEMBERS, \"false\"));\n        validateServiceRoleMembers = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_VALIDATE_SERVICE_MEMBERS, \"false\"));\n\n        // there are going to be domains like our ci/cd dynamic project domain\n        // where we can't verify the service role members so for those we're\n        // going to skip specific domains from validation checks\n\n        final String skipDomains = System.getProperty(\n                ZMSConsts.ZMS_PROP_VALIDATE_SERVICE_MEMBERS_SKIP_DOMAINS, \"\");\n        validateServiceMemberSkipDomains = new HashSet<>(Arrays.asList(skipDomains.split(\",\")));\n\n        // check to see if we need to support product ids as required\n        // for top level domains\n\n        productIdSupport = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_PRODUCT_ID_SUPPORT, \"false\"));\n\n        // get the list of valid provider endpoints\n\n        final String endPoints = System.getProperty(ZMSConsts.ZMS_PROP_PROVIDER_ENDPOINTS);\n        if (endPoints != null) {\n            providerEndpoints = Arrays.asList(endPoints.split(\",\"));\n        }\n\n        // retrieve virtual domain support and limit. If we're given an invalid negative\n        // value for limit, we'll default back to our configured value of 5\n\n        virtualDomainSupport = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_VIRTUAL_DOMAIN, \"true\"));\n        virtualDomainLimit = Integer.parseInt(\n                System.getProperty(ZMSConsts.ZMS_PROP_VIRTUAL_DOMAIN_LIMIT, \"5\"));\n        if (virtualDomainLimit < 0) {\n            virtualDomainLimit = 5;\n        }\n\n        // signedPolicyTimeout is in milliseconds but the config setting should be in seconds\n        // to be consistent with other configuration properties (Default 7 days)\n\n        signedPolicyTimeout = 1000 * Long.parseLong(\n                System.getProperty(ZMSConsts.ZMS_PROP_SIGNED_POLICY_TIMEOUT, \"604800\"));\n        if (signedPolicyTimeout < 0) {\n            signedPolicyTimeout = 1000 * 604800;\n        }\n\n        useMasterCopyForSignedDomains = Boolean.parseBoolean(\n                System.getProperty(ZMSConsts.ZMS_PROP_MASTER_COPY_FOR_SIGNED_DOMAINS, \"false\"));\n\n        // get the maximum length allowed for a top level domain name\n\n        domainNameMaxLen = Integer.parseInt(System.getProperty(\n                ZMSConsts.ZMS_PROP_DOMAIN_NAME_MAX_SIZE, ZMSConsts.ZMS_DOMAIN_NAME_MAX_SIZE_DEFAULT));\n        if (domainNameMaxLen < 10) { // 10 is arbitrary\n            int domNameMaxDefault = Integer.parseInt(ZMSConsts.ZMS_DOMAIN_NAME_MAX_SIZE_DEFAULT);\n            LOG.warn(\"init: Warning: maximum domain name length specified is too small: \" +\n                domainNameMaxLen + \" : reverting to default: \" + domNameMaxDefault);\n            domainNameMaxLen = domNameMaxDefault;\n        }\n        LOG.info(\"init: using maximum domain name length: \" + domainNameMaxLen);\n\n        // get the list of uris that we want to allow an-authenticated access\n\n        final String uriList = System.getProperty(ZMSConsts.ZMS_PROP_NOAUTH_URI_LIST);\n        if (uriList != null) {\n            authFreeUriSet = new HashSet<>();\n            authFreeUriList = new ArrayList<>();\n            String[] list = uriList.split(\",\");\n            for (String uri : list) {\n                if (uri.indexOf('+') != -1) {\n                    authFreeUriList.add(Pattern.compile(uri));\n                } else {\n                    authFreeUriSet.add(uri);\n                }\n            }\n        }\n\n        // get the list of white listed origin values for cors requests\n\n        final String originList = System.getProperty(ZMSConsts.ZMS_PROP_CORS_ORIGIN_LIST);\n        if (originList != null) {\n            corsOriginList = new HashSet<>(Arrays.asList(originList.split(\",\")));\n        }\n\n        // get the list of valid provider endpoints\n\n        final String serviceNames = System.getProperty(ZMSConsts.ZMS_PROP_RESERVED_SERVICE_NAMES,\n                ZMSConsts.ZMS_RESERVED_SERVICE_NAMES_DEFAULT);\n        reservedServiceNames = new HashSet<>(Arrays.asList(serviceNames.split(\",\")));\n\n        // min length for service names\n\n        serviceNameMinLength = Integer.parseInt(\n                System.getProperty(ZMSConsts.ZMS_PROP_SERVICE_NAME_MIN_LENGTH, \"3\"));\n\n        // setup our reserved system domain names\n\n        reservedSystemDomains = new HashSet<>();\n        reservedSystemDomains.add(\"sys\");\n        reservedSystemDomains.add(\"sys.auth\");\n        reservedSystemDomains.add(\"sys.auth.audit\");\n        reservedSystemDomains.add(\"sys.auth.audit.org\");\n        reservedSystemDomains.add(\"sys.auth.audit.domain\");\n        reservedSystemDomains.add(userDomain);\n        reservedSystemDomains.add(homeDomain);\n\n        // setup our health check file\n\n        final String healthCheckPath = System.getProperty(ZMSConsts.ZMS_PROP_HEALTH_CHECK_PATH);\n        if (healthCheckPath != null && !healthCheckPath.isEmpty()) {\n            healthCheckFile = new File(healthCheckPath);\n        }\n\n        // get server region\n\n        serverRegion = System.getProperty(ZMSConsts.ZMS_PROP_SERVER_REGION);\n    }\n\n    void loadObjectStore() {\n        String objFactoryClass = System.getProperty(ZMSConsts.ZMS_PROP_OBJECT_STORE_FACTORY_CLASS,\n                ZMSConsts.ZMS_OBJECT_STORE_FACTORY_CLASS);\n        ObjectStoreFactory objFactory;\n        try {\n            objFactory = (ObjectStoreFactory) Class.forName(objFactoryClass).newInstance();\n        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n            LOG.error(\"Invalid ObjectStoreFactory class: \" + objFactoryClass\n                    + \" error: \" + e.getMessage());\n            throw new IllegalArgumentException(\"Invalid object store\");\n        }\n\n        ZMSConfig zmsConfig = new ZMSConfig();\n        zmsConfig.setUserDomain(userDomain);\n        zmsConfig.setAddlUserCheckDomainPrefixList(addlUserCheckDomainPrefixList);\n        zmsConfig.setUserDomainPrefix(userDomainPrefix);\n        zmsConfig.setServerHostName(serverHostName);\n        zmsConfig.setServerSolutionTemplates(serverSolutionTemplates);\n        zmsConfig.setUserAuthority(userAuthority);\n\n        objectStore = objFactory.create(keyStore);\n        dbService = new DBService(objectStore, auditLogger, zmsConfig, auditReferenceValidator);\n    }\n\n    void loadMetricObject() {\n\n        String metricFactoryClass = System.getProperty(ZMSConsts.ZMS_PROP_METRIC_FACTORY_CLASS,\n                ZMSConsts.ZMS_METRIC_FACTORY_CLASS);\n        MetricFactory metricFactory;\n        try {\n            metricFactory = (MetricFactory) Class.forName(metricFactoryClass).newInstance();\n        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n            LOG.error(\"Invalid MetricFactory class: \" + metricFactoryClass\n                    + \" error: \" + e.getMessage());\n            throw new IllegalArgumentException(\"Invalid metric class\");\n        }\n\n        // create our metric and increment our startup count\n\n        ZMSImpl.metric = metricFactory.create();\n        metric.increment(\"zms_sa_startup\");\n    }\n\n    void loadPrivateKeyStore() {\n\n        String pkeyFactoryClass = System.getProperty(ZMSConsts.ZMS_PROP_PRIVATE_KEY_STORE_FACTORY_CLASS,\n                ZMSConsts.ZMS_PRIVATE_KEY_STORE_FACTORY_CLASS);\n        PrivateKeyStoreFactory pkeyFactory;\n        try {\n            pkeyFactory = (PrivateKeyStoreFactory) Class.forName(pkeyFactoryClass).newInstance();\n        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n            LOG.error(\"Invalid PrivateKeyStoreFactory class: \" + pkeyFactoryClass\n                    + \" error: \" + e.getMessage());\n            throw new IllegalArgumentException(\"Invalid private key store\");\n        }\n\n        // extract the private key and public keys for our service\n\n        keyStore = pkeyFactory.create();\n\n        privateECKey = keyStore.getPrivateKey(ZMSConsts.ZMS_SERVICE, serverHostName,\n                serverRegion, ZMSConsts.EC);\n\n        privateRSAKey = keyStore.getPrivateKey(ZMSConsts.ZMS_SERVICE, serverHostName,\n                serverRegion, ZMSConsts.RSA);\n\n        // if we don't have ec and rsa specific keys specified then we're going to fall\n        // back and use the old private key api and use that for our private key\n        // if both ec and rsa keys are provided, we use the ec key as preferred\n        // when signing policy files\n\n        if (privateECKey == null && privateRSAKey == null) {\n            StringBuilder privKeyId = new StringBuilder(256);\n            PrivateKey pkey = keyStore.getPrivateKey(ZMSConsts.ZMS_SERVICE, serverHostName, privKeyId);\n            privateKey = new ServerPrivateKey(pkey, privKeyId.toString());\n        } else if (privateECKey != null) {\n            privateKey = privateECKey;\n        } else {\n            privateKey = privateRSAKey;\n        }\n    }\n\n    void loadAuthorities() {\n\n        // get our authorities\n\n        final String authListConfig = System.getProperty(ZMSConsts.ZMS_PROP_AUTHORITY_CLASSES,\n                ZMSConsts.ZMS_PRINCIPAL_AUTHORITY_CLASS);\n        final String principalAuthorityClass = System.getProperty(ZMSConsts.ZMS_PROP_PRINCIPAL_AUTHORITY_CLASS);\n        final String userAuthorityClass = System.getProperty(ZMSConsts.ZMS_PROP_USER_AUTHORITY_CLASS);\n\n        authorities = new AuthorityList();\n\n        String[] authorityList = authListConfig.split(\",\");\n        for (String authorityClass : authorityList) {\n            Authority authority = getAuthority(authorityClass);\n            if (authority == null) {\n                throw new IllegalArgumentException(\"Invalid authority\");\n            }\n            if (authorityClass.equals(principalAuthorityClass)) {\n                principalAuthority = authority;\n            }\n            if (authorityClass.equals(userAuthorityClass)) {\n                userAuthority = authority;\n            }\n            authority.initialize();\n            authorities.add(authority);\n        }\n    }\n\n    void loadAuditLogger() {\n\n        String auditFactoryClass = System.getProperty(ZMSConsts.ZMS_PROP_AUDIT_LOGGER_FACTORY_CLASS,\n                ZMSConsts.ZMS_AUDIT_LOGGER_FACTORY_CLASS);\n        AuditLoggerFactory auditLogFactory;\n\n        try {\n            auditLogFactory = (AuditLoggerFactory) Class.forName(auditFactoryClass).newInstance();\n        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n            LOG.error(\"Invalid AuditLoggerFactory class: \" + auditFactoryClass\n                    + \" error: \" + e.getMessage());\n            throw new IllegalArgumentException(\"Invalid audit logger class\");\n        }\n\n        // create our audit logger\n\n        auditLogger = auditLogFactory.create();\n    }\n\n    void loadAuditRefValidator() {\n        final String auditRefValidatorClass = System.getProperty(ZMSConsts.ZMS_PROP_AUDIT_REF_VALIDATOR_FACTORY_CLASS);\n        AuditReferenceValidatorFactory auditReferenceValidatorFactory;\n\n        if (auditRefValidatorClass != null && !auditRefValidatorClass.isEmpty()) {\n\n            try {\n                auditReferenceValidatorFactory = (AuditReferenceValidatorFactory) Class.forName(auditRefValidatorClass).newInstance();\n            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n                LOG.error(\"Invalid AuditReferenceValidatorFactory class: \" + auditRefValidatorClass\n                        + \" error: \" + e.getMessage());\n                throw new IllegalArgumentException(\"Invalid audit reference factory class\");\n            }\n\n            // create our audit reference validator\n\n            auditReferenceValidator = auditReferenceValidatorFactory.create();\n        }\n    }\n\n    void loadServerPublicKeys() {\n\n        // initialize our public key map\n\n        serverPublicKeyMap = new ConcurrentHashMap<>();\n\n        // retrieve our zms service identity object\n\n        ServiceIdentity identity = dbService.getServiceIdentity(SYS_AUTH, ZMSConsts.ZMS_SERVICE, false);\n        if (identity != null) {\n\n            // process all the public keys and add them to the map\n\n            List<PublicKeyEntry> publicKeyList = identity.getPublicKeys();\n            if (publicKeyList != null) {\n                for (PublicKeyEntry entry : publicKeyList) {\n                    serverPublicKeyMap.put(entry.getId(), entry.getKey());\n                }\n            }\n        }\n\n        // this should never happen but just in case we'll just\n        // use the public key we retrieved ourselves to the map\n\n        if (serverPublicKeyMap.isEmpty() && privateKey != null) {\n            final String publicKey = Crypto.convertToPEMFormat(Crypto.extractPublicKey(privateKey.getKey()));\n            serverPublicKeyMap.put(privateKey.getId(), Crypto.ybase64EncodeString(publicKey));\n        }\n    }\n\n    void loadSolutionTemplates() {\n\n        // get the configured path for the list of service templates\n\n        String solutionTemplatesFname = System.getProperty(ZMSConsts.ZMS_PROP_SOLUTION_TEMPLATE_FNAME,\n                getRootDir() + \"/conf/zms_server/solution_templates.json\");\n\n        Path path = Paths.get(solutionTemplatesFname);\n        try {\n            serverSolutionTemplates = JSON.fromBytes(Files.readAllBytes(path), SolutionTemplates.class);\n        } catch (IOException ex) {\n            LOG.error(\"Unable to parse service templates file {}: {}\",\n                    solutionTemplatesFname, ex.getMessage());\n        }\n\n        if (serverSolutionTemplates == null) {\n            LOG.error(\"Generating empty solution template list...\");\n            serverSolutionTemplates = new SolutionTemplates();\n            serverSolutionTemplates.setTemplates(new HashMap<>());\n        }\n    }\n\n    void autoApplyTemplates() {\n        Map<String, Integer> eligibleTemplatesForAutoUpdate = new HashMap<>();\n        for (String templateName : serverSolutionTemplates.getTemplates().keySet()) {\n            Template template = serverSolutionTemplates.get(templateName);\n            if (template.getMetadata().getAutoUpdate()\n                    && template.getMetadata().getKeywordsToReplace().isEmpty()) {\n                eligibleTemplatesForAutoUpdate.put(templateName, template.getMetadata().getLatestVersion());\n            }\n        }\n        if (Boolean.parseBoolean(System.getProperty(ZMSConsts.ZMS_AUTO_UPDATE_TEMPLATE_FEATURE_FLAG, \"false\"))\n                && !eligibleTemplatesForAutoUpdate.isEmpty()) {\n            ExecutorService executor = Executors.newSingleThreadExecutor();\n            executor.execute(new AutoApplyTemplate(eligibleTemplatesForAutoUpdate));\n            executor.shutdown();\n        }\n    }\n\n    void loadAuthorizedServices() {\n\n        // get the configured path for the list of authorized services and what operations\n        // those services are allowed to process\n\n        String authzServiceFname =  System.getProperty(ZMSConsts.ZMS_PROP_AUTHZ_SERVICE_FNAME,\n                getRootDir() + \"/conf/zms_server/authorized_services.json\");\n\n        Path path = Paths.get(authzServiceFname);\n        try {\n            serverAuthorizedServices = JSON.fromBytes(Files.readAllBytes(path), AuthorizedServices.class);\n        } catch (IOException ex) {\n            LOG.error(\"Unable to parse authorized service file {}: {}\",\n                    authzServiceFname, ex.getMessage());\n        }\n\n        if (serverAuthorizedServices == null) {\n            LOG.error(\"Generating empty authorized service list...\");\n            serverAuthorizedServices = new AuthorizedServices();\n            serverAuthorizedServices.setTemplates(new HashMap<>());\n        }\n    }\n\n    void loadStatusChecker() {\n        final String statusCheckerFactoryClass = System.getProperty(ZMSConsts.ZMS_PROP_STATUS_CHECKER_FACTORY_CLASS);\n        StatusCheckerFactory statusCheckerFactory;\n\n        if (statusCheckerFactoryClass != null && !statusCheckerFactoryClass.isEmpty()) {\n\n            try {\n                statusCheckerFactory = (StatusCheckerFactory) Class.forName(statusCheckerFactoryClass).newInstance();\n            } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n                LOG.error(\"Invalid StatusCheckerFactory class: \" + statusCheckerFactoryClass\n                        + \" error: \" + e.getMessage());\n                throw new IllegalArgumentException(\"Invalid status checker factory class\");\n            }\n\n            // create our status checker\n\n            statusChecker = statusCheckerFactory.create();\n        }\n    }\n\n    void initObjectStore() {\n\n        final String caller = \"initstore\";\n\n        List<String> domains = dbService.listDomains(null, 0, true);\n        if (domains.size() > 0 && domains.contains(SYS_AUTH)) {\n            return;\n        }\n\n        String adminUserList = System.getProperty(ZMSConsts.ZMS_PROP_DOMAIN_ADMIN);\n        if (adminUserList == null) {\n            throw ZMSUtils.internalServerError(\"init: No ZMS admin user specified\", caller);\n        }\n\n        String[] users = adminUserList.split(\",\");\n        ArrayList<String> adminUsers = new ArrayList<>();\n        for (String user : users) {\n            final String adminUser = user.trim();\n            if (!adminUser.startsWith(userDomainPrefix)) {\n                throw ZMSUtils.internalServerError(\"init: Bad domain user name(\" + adminUser +\n                        \"), must begin with (\" + userDomainPrefix + \")\", caller);\n            }\n            adminUsers.add(adminUser);\n        }\n\n        // create system required top level domains\n\n        Domain domain = new Domain().setName(userDomain).setDescription(\"The reserved domain for user authentication\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createTopLevelDomain(null, domain, adminUsers, null, \"System Setup\");\n        if (!ZMSConsts.USER_DOMAIN.equals(userDomain)) {\n            domain = new Domain().setName(ZMSConsts.USER_DOMAIN).setDescription(\"The reserved domain for user authentication\")\n                    .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n            createTopLevelDomain(null, domain, adminUsers, null, \"System Setup\");\n        }\n        if (!homeDomain.equals(userDomain)) {\n            domain = new Domain().setName(homeDomain).setDescription(\"The reserved domain for personal user domains\")\n                    .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n            createTopLevelDomain(null, domain, adminUsers, null, \"System Setup\");\n        }\n        domain = new Domain().setName(\"sys\").setDescription(\"The reserved domain for system related information\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createTopLevelDomain(null, domain, adminUsers, null, \"System Setup\");\n\n        // now create required subdomains in sys top level domain\n\n        domain = new Domain().setName(\"sys.auth\").setDescription(\"The Athenz domain\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createSubDomain(null, domain, adminUsers, null, \"System Setup\", caller);\n\n        domain = new Domain().setName(\"sys.auth.audit\").setDescription(\"The Athenz audit domain\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createSubDomain(null, domain, adminUsers, null, \"System Setup\", caller);\n\n        domain = new Domain().setName(\"sys.auth.audit.org\").setDescription(\"The Athenz audit domain based on org name\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createSubDomain(null, domain, adminUsers, null, \"System Setup\", caller);\n\n        domain = new Domain().setName(\"sys.auth.audit.domain\").setDescription(\"The Athenz audit domain based on domain name\")\n                .setId(UUID.fromCurrentTime()).setModified(Timestamp.fromCurrentTime());\n        createSubDomain(null, domain, adminUsers, null, \"System Setup\", caller);\n\n        if (privateKey != null) {\n            List<PublicKeyEntry> pubKeys = new ArrayList<>();\n            final String publicKey = Crypto.convertToPEMFormat(Crypto.extractPublicKey(privateKey.getKey()));\n            pubKeys.add(new PublicKeyEntry().setId(privateKey.getId()).setKey(Crypto.ybase64EncodeString(publicKey)));\n            ServiceIdentity id = new ServiceIdentity().setName(\"sys.auth.zms\").setPublicKeys(pubKeys);\n            dbService.executePutServiceIdentity(null, SYS_AUTH, ZMSConsts.ZMS_SERVICE, id, null, caller);\n        } else {\n            if (LOG.isWarnEnabled()) {\n                LOG.warn(\"init: Warning: no public key, cannot register sys.auth.zms identity\");\n            }\n        }\n    }\n\n    /**\n     * @return the ZMS Schema object, describing its API and types.\n     */\n    public Schema schema() {\n        return schema;\n    }\n\n    public DomainList getDomainList(ResourceContext ctx, Integer limit, String skip, String prefix,\n            Integer depth, String account, Integer productId, String roleMember, String roleName,\n            String modifiedSince) {\n\n        final String caller = \"getdomainlist\";\n\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getDomainList: limit: \" + limit + \" skip: \" + skip\n                    + \" prefix: \" + prefix + \" depth: \" + depth + \" modifiedSince: \" + modifiedSince);\n        }\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        if (skip != null) {\n            skip = skip.toLowerCase();\n        }\n        if (prefix != null) {\n            prefix = prefix.toLowerCase();\n        }\n        if (roleMember != null) {\n            roleMember = roleMember.toLowerCase();\n            validate(roleMember, TYPE_ENTITY_NAME, caller);\n        }\n        if (roleName != null) {\n            roleName = roleName.toLowerCase();\n            validate(roleName, TYPE_ENTITY_NAME, caller);\n        }\n        if (limit != null && limit <= 0) {\n            throw ZMSUtils.requestError(\"getDomainList: limit must be positive: \" + limit, caller);\n        }\n\n        long modTime = 0;\n        if (modifiedSince != null && !modifiedSince.isEmpty()) {\n            // we only support RFC1123 format for if-modified-since format\n\n            SimpleDateFormat dateFmt = new SimpleDateFormat(ZMSConsts.HTTP_RFC1123_DATE_FORMAT);\n            dateFmt.setTimeZone(TimeZone.getTimeZone(ZMSConsts.HTTP_DATE_GMT_ZONE));\n            try {\n                Date date = dateFmt.parse(modifiedSince);\n                modTime = date.getTime();\n            } catch (ParseException ex) {\n                throw ZMSUtils.requestError(\"getDomainList: If-Modified-Since header value must be valid RFC1123 date\"\n                        + ex.getMessage(), caller);\n            }\n        }\n\n        // if we have account specified then we're going to ignore all\n        // other fields since there should only be one domain that\n        // matches the specified account. Otherwise, we're going to do\n        // the same thing for product id since there should also be one\n        // domain with that id. If neither one is present, then we'll\n        // do our regular domain list\n\n        DomainList dlist;\n        if (account != null && !account.isEmpty()) {\n            dlist = dbService.lookupDomainByAccount(account);\n        } else if (productId != null && productId != 0) {\n            dlist = dbService.lookupDomainByProductId(productId);\n        } else if (roleMember != null || roleName != null) {\n            dlist = dbService.lookupDomainByRole(normalizeDomainAliasUser(roleMember), roleName);\n        } else {\n            dlist = listDomains(limit, skip, prefix, depth, modTime, false);\n        }\n\n        return dlist;\n    }\n\n    public Domain getDomain(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getdomain\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        Domain domain = dbService.getDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"getDomain: Domain not found: \" + domainName, caller);\n        }\n\n        return domain;\n    }\n\n    public Domain postTopLevelDomain(ResourceContext ctx, String auditRef, TopLevelDomain detail) {\n\n        final String caller = \"posttopleveldomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(detail, TYPE_TOP_LEVEL_DOMAIN, caller);\n\n        String domainName = detail.getName();\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        if (domainName.indexOf('_') != -1 && !isSysAdminUser(((RsrcCtxWrapper) ctx).principal())) {\n            throw ZMSUtils.requestError(\"Domain name cannot contain underscores\", caller);\n        }\n\n        // verify length of domain name\n\n        if (domainName.length() > domainNameMaxLen) {\n            throw ZMSUtils.requestError(\"Invalid Domain name: \" + domainName\n                    + \" : name length cannot exceed: \" + domainNameMaxLen, caller);\n        }\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        AthenzObject.TOP_LEVEL_DOMAIN.convertToLowerCase(detail);\n\n        List<String> solutionTemplates = null;\n        DomainTemplateList templates = detail.getTemplates();\n        if (templates != null) {\n            solutionTemplates = templates.getTemplateNames();\n            validateSolutionTemplates(solutionTemplates, caller);\n        }\n\n        // check to see if we need to validate our product id for the top\n        // level domains. The server code assumes that product id with\n        // 0 indicates no enforcement\n\n        int productId = 0;\n        if (productIdSupport) {\n            if (detail.getYpmId() != null) {\n                if ((productId = detail.getYpmId()) <= 0) {\n                    throw ZMSUtils.requestError(\"Product Id must be a positive integer\", caller);\n                }\n            } else {\n                throw ZMSUtils.requestError(\"Product Id is required when creating top level domain\", caller);\n            }\n        }\n\n        // if we're provided a user authority filter then we need to\n        // make sure it's valid\n\n        validateUserAuthorityFilterAttribute(detail.getUserAuthorityFilter(), caller);\n\n        // process our top level domain request\n\n        Domain topLevelDomain = new Domain()\n                .setName(domainName)\n                .setAuditEnabled(detail.getAuditEnabled())\n                .setDescription(detail.getDescription())\n                .setOrg(detail.getOrg())\n                .setId(UUID.fromCurrentTime())\n                .setAccount(detail.getAccount())\n                .setYpmId(productId)\n                .setModified(Timestamp.fromCurrentTime())\n                .setApplicationId(detail.getApplicationId())\n                .setMemberExpiryDays(detail.getMemberExpiryDays())\n                .setServiceExpiryDays(detail.getServiceExpiryDays())\n                .setTokenExpiryMins(detail.getTokenExpiryMins())\n                .setServiceCertExpiryMins(detail.getServiceCertExpiryMins())\n                .setRoleCertExpiryMins(detail.getRoleCertExpiryMins())\n                .setSignAlgorithm(detail.getSignAlgorithm())\n                .setUserAuthorityFilter(detail.getUserAuthorityFilter());\n\n        // before processing validate the fields\n\n        validateDomainValues(topLevelDomain);\n\n        List<String> adminUsers = normalizedAdminUsers(detail.getAdminUsers(), detail.getUserAuthorityFilter(), caller);\n        return createTopLevelDomain(ctx, topLevelDomain, adminUsers, solutionTemplates, auditRef);\n    }\n\n    public void deleteTopLevelDomain(ResourceContext ctx, String domainName, String auditRef) {\n\n        final String caller = \"deletetopleveldomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        deleteDomain(ctx, auditRef, domainName, caller);\n    }\n\n    void deleteDomain(ResourceContext ctx, String auditRef, String domainName, String caller) {\n\n        // make sure we're not deleting any of the reserved system domain\n\n        if (reservedSystemDomains.contains(domainName)) {\n            throw ZMSUtils.requestError(\"Cannot delete reserved system domain\", caller);\n        }\n\n        DomainList subDomainList = listDomains(null, null, domainName + \".\", null, 0, true);\n        if (subDomainList.getNames().size() > 0) {\n            throw ZMSUtils.requestError(caller + \": Cannot delete domain \" +\n                    domainName + \": \" + subDomainList.getNames().size() + \" subdomains of it exist\", caller);\n        }\n\n        dbService.executeDeleteDomain(ctx, domainName, auditRef, caller);\n    }\n\n    boolean isVirtualDomain(String domain) {\n\n        // all virtual domains start with our user domain\n\n        return domain.startsWith(homeDomainPrefix);\n    }\n\n    boolean hasExceededVirtualSubDomainLimit(String domain) {\n\n        // we need to find our username which is our second\n        // component in the domain name - e.g. user.joe[.subdomain]\n        // when counting we need to make to include the trailing .\n        // since we're counting subdomains and we need to make sure\n        // not to match other users who have the same prefix\n\n        String userDomainCheck;\n        int idx = domain.indexOf('.', homeDomainPrefix.length());\n        if (idx == -1) {\n            userDomainCheck = domain + \".\";\n        } else {\n            userDomainCheck = domain.substring(0, idx + 1);\n        }\n\n        // retrieve the number of domains with this prefix\n\n        DomainList dlist = listDomains(null, null, userDomainCheck, null, 0, true);\n        if (dlist.getNames().size() < virtualDomainLimit) {\n            return false;\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"hasExceededVirtualSubDomainLimit: subdomains with prefix \" + userDomainCheck\n                    + \": \" + dlist.getNames().size() + \" while limit is: \" + virtualDomainLimit);\n        }\n\n        return true;\n    }\n\n    public Domain postUserDomain(ResourceContext ctx, String name, String auditRef, UserDomain detail) {\n\n        final String caller = \"postuserdomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(detail, TYPE_USER_DOMAIN, caller);\n        validate(name, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        name = name.toLowerCase();\n        setRequestDomain(ctx, name);\n        AthenzObject.USER_DOMAIN.convertToLowerCase(detail);\n\n        if (detail.getName().indexOf('_') != -1 && !isSysAdminUser(((RsrcCtxWrapper) ctx).principal())) {\n            throw ZMSUtils.requestError(\"Domain name cannot contain underscores\", caller);\n        }\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        if (!name.equals(detail.getName())) {\n            throw ZMSUtils.forbiddenError(\"postUserDomain: Request and detail domain names do not match\", caller);\n        }\n\n        // we're dealing with user's top level domain so the parent is going\n        // to be the home domain and the admin of the domain is the user\n\n        final String userDomainAdmin = userDomainPrefix + principal.getName();\n        validateRoleMemberPrincipal(userDomainAdmin, null, caller);\n\n        List<String> adminUsers = new ArrayList<>();\n        adminUsers.add(userDomainAdmin);\n\n        List<String> solutionTemplates = null;\n        DomainTemplateList templates = detail.getTemplates();\n        if (templates != null) {\n            solutionTemplates = templates.getTemplateNames();\n            validateSolutionTemplates(solutionTemplates, caller);\n        }\n\n        Domain subDomain = new Domain()\n                .setName(homeDomain + \".\" + getUserDomainName(detail.getName()))\n                .setAuditEnabled(detail.getAuditEnabled())\n                .setDescription(detail.getDescription())\n                .setOrg(detail.getOrg())\n                .setId(UUID.fromCurrentTime())\n                .setAccount(detail.getAccount())\n                .setModified(Timestamp.fromCurrentTime())\n                .setApplicationId(detail.getApplicationId())\n                .setMemberExpiryDays(detail.getMemberExpiryDays())\n                .setServiceExpiryDays(detail.getServiceExpiryDays())\n                .setTokenExpiryMins(detail.getTokenExpiryMins())\n                .setServiceCertExpiryMins(detail.getServiceCertExpiryMins())\n                .setRoleCertExpiryMins(detail.getRoleCertExpiryMins())\n                .setSignAlgorithm(detail.getSignAlgorithm());\n\n        // before processing validate the fields\n\n        validateDomainValues(subDomain);\n\n        return createSubDomain(ctx, subDomain, adminUsers, solutionTemplates, auditRef, caller);\n    }\n\n    public Domain postSubDomain(ResourceContext ctx, String parent, String auditRef, SubDomain detail) {\n\n        final String caller = \"postsubdomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(detail, TYPE_SUB_DOMAIN, caller);\n        validate(parent, TYPE_DOMAIN_NAME, caller);\n        validate(detail.getName(), TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        parent = parent.toLowerCase();\n        setRequestDomain(ctx, parent);\n        AthenzObject.SUB_DOMAIN.convertToLowerCase(detail);\n\n        if (detail.getName().indexOf('_') != -1 && !isSysAdminUser(((RsrcCtxWrapper) ctx).principal())) {\n            throw ZMSUtils.requestError(\"Domain name cannot contain underscores\", caller);\n        }\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        if (!parent.equals(detail.getParent())) {\n            throw ZMSUtils.forbiddenError(\"postSubDomain: Request and detail parent domains do not match\", caller);\n        }\n\n        // if we're dealing with virtual/home domains (in the user's own namespace)\n        // and we don't have unlimited support for virtual domains then we need to\n        // make sure we don't exceed our configured number of virtual subdomains \n        // allowed per user\n\n        if (virtualDomainLimit != 0 && isVirtualDomain(parent) && hasExceededVirtualSubDomainLimit(parent)) {\n            throw ZMSUtils.forbiddenError(\"postSubDomain: Exceeding the configured number of virtual subdomains\", caller);\n        }\n\n        List<String> solutionTemplates = null;\n        DomainTemplateList templates = detail.getTemplates();\n        if (templates != null) {\n            solutionTemplates = templates.getTemplateNames();\n            validateSolutionTemplates(solutionTemplates, caller);\n        }\n\n        // while it's not required for sub domains to have product ids\n        // we're going to store it in case there is a requirement to\n        // generate reports based on product ids even for subdomains\n        // unlike top level domains, passing 0 is ok here as it indicates\n        // that there is no product id\n\n        int productId = 0;\n        if (productIdSupport) {\n            if (detail.getYpmId() != null) {\n                if ((productId = detail.getYpmId()) < 0) {\n                    throw ZMSUtils.requestError(\"Product Id must be a positive integer\", caller);\n                }\n            }\n        }\n\n        // verify that the parent domain exists\n\n        AthenzDomain parentDomain = getAthenzDomain(parent, false);\n        if (parentDomain == null || parentDomain.getDomain() == null) {\n            throw ZMSUtils.notFoundError(\"Invalid parent domain: \" + parent, caller);\n        }\n\n        // inherit audit_enabled flag, organization and user authority settings\n        // from the parent domain\n\n        detail.setAuditEnabled(parentDomain.getDomain().getAuditEnabled());\n        detail.setOrg(parentDomain.getDomain().getOrg());\n        detail.setUserAuthorityFilter(parentDomain.getDomain().getUserAuthorityFilter());\n\n        // generate and verify admin users\n\n        List<String> adminUsers = normalizedAdminUsers(detail.getAdminUsers(), detail.getUserAuthorityFilter(), caller);\n\n        Domain subDomain = new Domain()\n                .setName(detail.getParent() + \".\" + detail.getName())\n                .setAuditEnabled(detail.getAuditEnabled())\n                .setDescription(detail.getDescription())\n                .setOrg(detail.getOrg())\n                .setId(UUID.fromCurrentTime())\n                .setYpmId(productId)\n                .setAccount(detail.getAccount())\n                .setModified(Timestamp.fromCurrentTime())\n                .setApplicationId(detail.getApplicationId())\n                .setMemberExpiryDays(detail.getMemberExpiryDays())\n                .setServiceExpiryDays(detail.getServiceExpiryDays())\n                .setTokenExpiryMins(detail.getTokenExpiryMins())\n                .setServiceCertExpiryMins(detail.getServiceCertExpiryMins())\n                .setRoleCertExpiryMins(detail.getRoleCertExpiryMins())\n                .setSignAlgorithm(detail.getSignAlgorithm());\n\n        // before processing validate the fields\n\n        validateDomainValues(subDomain);\n\n        return createSubDomain(ctx, subDomain, adminUsers, solutionTemplates, auditRef, caller);\n    }\n\n    boolean isSysAdminUser(Principal principal) {\n\n        // verify we're dealing with system administrator\n        // authorize (\"CREATE\", \"sys.auth:domain\");\n\n        // first check - the domain must be the user domain\n\n        if (!principal.getDomain().equals(userDomain)) {\n            return false;\n        }\n\n        AthenzDomain domain = getAthenzDomain(SYS_AUTH, true);\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n        // our action are always converted to lowercase\n\n        String resource = SYS_AUTH + \":domain\";\n        AccessStatus accessStatus = evaluateAccess(domain, principal.getFullName(), \"create\",\n                resource, null, null);\n\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n\n    boolean isAllowedResourceLookForAllUsers(Principal principal) {\n\n        // the authorization policy resides in official sys.auth domain\n\n        AthenzDomain domain = getAthenzDomain(SYS_AUTH, true);\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n        // our action are always converted to lowercase\n\n        String resource = SYS_AUTH + \":resource-lookup-all\";\n        AccessStatus accessStatus = evaluateAccess(domain, principal.getFullName(), \"access\",\n                resource, null, null);\n\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n\n    public void deleteSubDomain(ResourceContext ctx, String parent, String name, String auditRef) {\n\n        final String caller = \"deletesubdomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        parent = parent.toLowerCase();\n        name = name.toLowerCase();\n        String domainName = parent + \".\" + name;\n        setRequestDomain(ctx, parent);\n\n        validate(parent, TYPE_DOMAIN_NAME, caller);\n        validate(name, TYPE_SIMPLE_NAME, caller);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        deleteDomain(ctx, auditRef, domainName, caller);\n    }\n\n    public void deleteUserDomain(ResourceContext ctx, String name, String auditRef) {\n\n        final String caller = \"deleteuserdomain\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(name, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        name = name.toLowerCase();\n        setRequestDomain(ctx, name);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        String domainName = homeDomainPrefix + name;\n        deleteDomain(ctx, auditRef, domainName, caller);\n    }\n\n    public UserList getUserList(ResourceContext ctx) {\n\n        final String caller = \"getuserlist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        List<String> names = dbService.listPrincipals(userDomain, true);\n        return new UserList().setNames(names);\n    }\n\n    @Override\n    public void deleteDomainRoleMember(ResourceContext ctx, String domainName, String memberName, String auditRef) {\n\n        final String caller = \"deletedomainrolemember\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        memberName = memberName.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        dbService.executeDeleteDomainRoleMember(ctx, domainName, memberName, auditRef, caller);\n    }\n\n    @Override\n    public void deleteUser(ResourceContext ctx, String name, String auditRef) {\n\n        final String caller = \"deleteuser\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(name, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        name = name.toLowerCase();\n        setRequestDomain(ctx, name);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        String userName = userDomainPrefix + name;\n        String domainName = homeDomainPrefix + getUserDomainName(name);\n        dbService.executeDeleteUser(ctx, userName, domainName, auditRef, caller);\n    }\n\n    String getUserDomainName(String userName) {\n        return (userAuthority == null) ? userName : userAuthority.getUserDomainName(userName);\n    }\n\n    @Override\n    public void putDomainMeta(ResourceContext ctx, String domainName, String auditRef,\n            DomainMeta meta) {\n\n        final String caller = \"putdomainmeta\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        // validate meta values - validator will enforce any patters\n        // defined in the schema and we need to validate the rest of the\n        // integer and string values. for now we're making sure we're not\n        // getting any negative values for our integer settings\n\n        validate(meta, TYPE_DOMAIN_META, caller);\n        validateDomainMetaValues(meta);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        AthenzObject.DOMAIN_META.convertToLowerCase(meta);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(),\n                caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putDomainMeta: name={}, meta={}\", domainName, meta);\n        }\n\n        // process put domain meta request\n\n        dbService.executePutDomainMeta(ctx, domainName, meta, null, false, auditRef, caller);\n    }\n\n    void validateString(final String value, final String type, final String caller) {\n        if (value != null && !value.isEmpty()) {\n            validate(value, type, caller);\n        }\n    }\n\n    void validateIntegerValue(final Integer value, final String fieldName) {\n        if (value != null && value < 0) {\n            throw ZMSUtils.requestError(fieldName + \" cannot be negative\", \"validateMetaFields\");\n        }\n    }\n\n    void validateDomainValues(Domain domain) {\n\n        final String caller = \"validateDomainValues\";\n\n        validateIntegerValue(domain.getServiceCertExpiryMins(), \"serviceCertExpiryMins\");\n        validateIntegerValue(domain.getMemberExpiryDays(), \"memberExpiryDays\");\n        validateIntegerValue(domain.getRoleCertExpiryMins(), \"roleCertExpiryMins\");\n        validateIntegerValue(domain.getServiceExpiryDays(), \"serviceExpiryDays\");\n        validateIntegerValue(domain.getTokenExpiryMins(), \"tokenExpiryMins\");\n\n        validateString(domain.getApplicationId(), TYPE_COMPOUND_NAME, caller);\n        validateString(domain.getAccount(), TYPE_COMPOUND_NAME, caller);\n        validateString(domain.getUserAuthorityFilter(), TYPE_AUTHORITY_KEYWORDS, caller);\n    }\n\n    void validateDomainMetaValues(DomainMeta meta) {\n\n        final String caller = \"validateDomainMetaValues\";\n\n        validateIntegerValue(meta.getServiceCertExpiryMins(), \"serviceCertExpiryMins\");\n        validateIntegerValue(meta.getMemberExpiryDays(), \"memberExpiryDays\");\n        validateIntegerValue(meta.getRoleCertExpiryMins(), \"roleCertExpiryMins\");\n        validateIntegerValue(meta.getServiceExpiryDays(), \"serviceExpiryDays\");\n        validateIntegerValue(meta.getTokenExpiryMins(), \"tokenExpiryMins\");\n        validateIntegerValue(meta.getYpmId(), \"ypmId\");\n\n        validateString(meta.getApplicationId(), TYPE_COMPOUND_NAME, caller);\n        validateString(meta.getAccount(), TYPE_COMPOUND_NAME, caller);\n    }\n\n    void validateRoleMetaValues(RoleMeta meta) {\n\n        final String caller = \"validateRoleMetaValues\";\n\n        validateIntegerValue(meta.getMemberExpiryDays(), \"memberExpiryDays\");\n        validateIntegerValue(meta.getServiceExpiryDays(), \"serviceExpiryDays\");\n        validateIntegerValue(meta.getTokenExpiryMins(), \"tokenExpiryMins\");\n        validateIntegerValue(meta.getCertExpiryMins(), \"certExpiryMins\");\n        validateIntegerValue(meta.getMemberReviewDays(), \"memberReviewDays\");\n        validateIntegerValue(meta.getServiceReviewDays(), \"serviceReviewDays\");\n\n        validateString(meta.getNotifyRoles(), TYPE_RESOURCE_NAMES, caller);\n        validateString(meta.getUserAuthorityFilter(), TYPE_AUTHORITY_KEYWORDS, caller);\n        validateString(meta.getUserAuthorityExpiration(), TYPE_AUTHORITY_KEYWORD, caller);\n    }\n\n    void validateRoleValues(Role role) {\n\n        final String caller = \"validateRoleValues\";\n\n        validateIntegerValue(role.getMemberExpiryDays(), \"memberExpiryDays\");\n        validateIntegerValue(role.getServiceExpiryDays(), \"serviceExpiryDays\");\n        validateIntegerValue(role.getTokenExpiryMins(), \"tokenExpiryMins\");\n        validateIntegerValue(role.getCertExpiryMins(), \"certExpiryMins\");\n        validateIntegerValue(role.getMemberReviewDays(), \"memberReviewDays\");\n        validateIntegerValue(role.getServiceReviewDays(), \"serviceReviewDays\");\n\n        validateString(role.getNotifyRoles(), TYPE_RESOURCE_NAMES, caller);\n        validateString(role.getUserAuthorityFilter(), TYPE_AUTHORITY_KEYWORDS, caller);\n        validateString(role.getUserAuthorityExpiration(), TYPE_AUTHORITY_KEYWORD, caller);\n    }\n\n    @Override\n    public void putDomainSystemMeta(ResourceContext ctx, String domainName, String attribute,\n            String auditRef, DomainMeta meta) {\n\n        final String caller = \"putdomainsystemmeta\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(attribute, TYPE_SIMPLE_NAME, caller);\n\n        // validate meta values - validator will enforce any patters\n        // defined in the schema and we need to validate the rest of the\n        // integer and string values. for now we're making sure we're not\n        // getting any negative values for our integer settings\n\n        validate(meta, TYPE_DOMAIN_META, caller);\n        validateDomainMetaValues(meta);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        attribute = attribute.toLowerCase();\n        AthenzObject.DOMAIN_META.convertToLowerCase(meta);\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putDomainSystemMeta: name={}, attribute={}, meta={}\",\n                    domainName, attribute, meta);\n        }\n\n        // if we are resetting the configured value then the caller\n        // must also have a delete action available for the same resource\n\n        boolean deleteAllowed = isAllowedSystemMetaDelete(principal, domainName, attribute, \"domain\");\n\n        // if this productId is already used by any domain it will be\n        // seen in dbService and exception thrown but we want to make\n        // sure here if product id support is required then we must\n        // have one specified for a top level domain.\n\n        if (productIdSupport && meta.getYpmId() == null && domainName.indexOf('.') == -1 &&\n                ZMSConsts.SYSTEM_META_PRODUCT_ID.equals(attribute)) {\n             throw ZMSUtils.requestError(\"Unique Product Id must be specified for top level domain\", caller);\n        }\n\n        // if we're provided a user authority filter then we need to\n        // make sure it's valid\n\n        validateUserAuthorityFilterAttribute(meta.getUserAuthorityFilter(), caller);\n\n        // if this is just to update the timestamp then we will handle it separately\n\n        if (ZMSConsts.SYSTEM_META_LAST_MOD_TIME.equals(attribute)) {\n            dbService.updateDomainModTimestamp(domainName);\n        } else {\n            dbService.executePutDomainMeta(ctx, domainName, meta, attribute, deleteAllowed, auditRef, caller);\n        }\n    }\n\n    void validateSolutionTemplates(List<String> templateNames, String caller) {\n        for (String templateName : templateNames) {\n            if (!serverSolutionTemplates.contains(templateName)) {\n                throw ZMSUtils.notFoundError(\"validateSolutionTemplates: Template not found: \"\n                        + templateName, caller);\n            }\n        }\n    }\n\n    public DomainTemplateList getDomainTemplateList(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getdomaintemplatelist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        DomainTemplateList domainTemplateList = dbService.listDomainTemplates(domainName);\n        if (domainTemplateList == null) {\n            throw ZMSUtils.notFoundError(\"getDomainTemplateList: Domain not found: '\" + domainName + \"'\", caller);\n        }\n\n        return domainTemplateList;\n    }\n\n    @Override\n    public void putDomainTemplate(ResourceContext ctx, String domainName, String auditRef,\n            DomainTemplate domainTemplate) {\n\n        final String caller = \"putdomaintemplate\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(domainTemplate, TYPE_DOMAIN_TEMPLATE, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        AthenzObject.DOMAIN_TEMPLATE.convertToLowerCase(domainTemplate);\n\n        // verify that all template names are valid\n\n        List<String> templateNames = domainTemplate.getTemplateNames();\n        if (templateNames == null || templateNames.size() == 0) {\n            throw ZMSUtils.requestError(\"putDomainTemplate: No templates specified\", caller);\n        }\n        validateSolutionTemplates(templateNames, caller);\n\n        // verify that request is properly authenticated for this request\n        // Make sure each template name is verified\n\n        for (String templateName : domainTemplate.getTemplateNames()) {\n            verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(),\n                    caller, \"name\", templateName);\n        }\n\n        dbService.executePutDomainTemplate(ctx, domainName, domainTemplate, auditRef, caller);\n    }\n\n    @Override\n    public void putDomainTemplateExt(ResourceContext ctx, String domainName,\n            String templateName, String auditRef, DomainTemplate domainTemplate) {\n\n        final String caller = \"putdomaintemplateext\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(templateName, TYPE_SIMPLE_NAME, caller);\n        validate(domainTemplate, TYPE_DOMAIN_TEMPLATE, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        templateName = templateName.toLowerCase();\n        AthenzObject.DOMAIN_TEMPLATE.convertToLowerCase(domainTemplate);\n\n        // verify that all template names are valid\n\n        List<String> templateNames = domainTemplate.getTemplateNames();\n        if (templateNames == null) {\n            throw ZMSUtils.requestError(\"putDomainTemplateExt: No templates specified\", caller);\n        }\n\n        // the template name in the object must match to the uri\n\n        if (!(templateNames.size() == 1 && templateNames.get(0).equals(templateName))) {\n            throw ZMSUtils.requestError(\"putDomainTemplateExt: template name mismatch\", caller);\n        }\n        validateSolutionTemplates(templateNames, caller);\n\n        // verify that request is properly authenticated for this request\n        // Make sure each template name is verified\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(),\n                caller, \"name\", templateName);\n\n        dbService.executePutDomainTemplate(ctx, domainName, domainTemplate, auditRef, caller);\n    }\n\n    public void deleteDomainTemplate(ResourceContext ctx, String domainName, String templateName, String auditRef) {\n\n        final String caller = \"deletedomaintemplate\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(templateName, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        templateName = templateName.toLowerCase();\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"deleteDomainTemplate: domain=\" + domainName + \", template=\" + templateName);\n        }\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(),\n                caller, \"name\", templateName);\n\n        List<String> templateNames = new ArrayList<>();\n        templateNames.add(templateName);\n        validateSolutionTemplates(templateNames, caller);\n\n        dbService.executeDeleteDomainTemplate(ctx, domainName, templateName, auditRef, caller);\n    }\n\n    Principal createPrincipalForName(String principalName) {\n\n        String domain;\n        String name;\n\n        // if we have no . in the principal name we're going to default\n        // to our configured user domain\n\n        int idx = principalName.lastIndexOf('.');\n        if (idx == -1) {\n            domain = userDomain;\n            name = principalName;\n        } else {\n            domain = principalName.substring(0, idx);\n            if (userDomainAlias != null && userDomainAlias.equals(domain)) {\n                domain = userDomain;\n            }\n            name = principalName.substring(idx + 1);\n        }\n\n        return SimplePrincipal.create(domain, name, (String) null);\n    }\n\n    boolean validateRoleBasedAccessCheck(List<String> roles, final String trustDomain, final String domainName,\n                                         final String principalName) {\n\n        if (trustDomain != null) {\n            LOG.error(\"validateRoleBasedAccessCheck: Cannot access cross-domain resources with role\");\n            return false;\n        }\n\n        // for Role tokens we don't have a name component in the principal\n        // so the principal name should be the same as the domain value\n        // thus it must match the domain name from the resource\n\n        boolean bResourceDomainMatch = domainName.equalsIgnoreCase(principalName);\n\n        // now we're going to go through all the roles specified and make\n        // sure if it contains ':role.' separator then the domain must\n        // our requested domain name. If the role does not have the separator\n        // then the bResourceDomainMatch must be true\n\n        final String prefix = domainName + AuthorityConsts.ROLE_SEP;\n        for (String role : roles) {\n\n            // if our role starts with the prefix then we're good\n\n            if (role.startsWith(prefix)) {\n                continue;\n            }\n\n            // otherwise if it has a role separator then it's an error\n            // not to match the domain\n\n            if (role.contains(AuthorityConsts.ROLE_SEP)) {\n                LOG.error(\"validateRoleBasedAccessCheck: role {} does not start with resource domain {}\",\n                        role, domainName);\n                return false;\n            }\n\n            // so at this point we don't have a separator so our\n            // resource and principal domains must match\n\n            if (!bResourceDomainMatch) {\n                LOG.error(\"validateRoleBasedAccessCheck: resource domain {} does not match role domain {}\",\n                        domainName, principalName);\n                return false;\n            }\n        }\n\n        return true;\n    }\n\n    AthenzDomain getAthenzDomain(String domainName, boolean ignoreExceptions) {\n        return getAthenzDomain(domainName, ignoreExceptions, false);\n    }\n\n    AthenzDomain getAthenzDomain(String domainName, boolean ignoreExceptions, boolean masterCopy) {\n\n        AthenzDomain domain = null;\n        try {\n            domain = dbService.getAthenzDomain(domainName, masterCopy);\n        } catch (ResourceException ex) {\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"getAthenzDomain failure: \" + ex.getMessage());\n            }\n\n            if (!ignoreExceptions) {\n                if (ex.getCode() != ResourceException.NOT_FOUND) {\n                    throw ex;\n                }\n            }\n        }\n        return domain;\n    }\n\n    AthenzDomain retrieveAccessDomain(String domainName, Principal principal) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"retrieveAccessDomain: identity: {} domain: {}\", principal.getFullName(), domainName);\n        }\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain != null) {\n            return domain;\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"retrieveAccessDomain: domain not found, looking for virtual domain\");\n        }\n\n        // if we don't have virtual/home domains enabled then no need\n        // to continue further\n\n        if (!virtualDomainSupport) {\n            return null;\n        }\n\n        if (principal.getDomain() == null) {\n            return null;\n        }\n\n        // the principals user name must match to the corresponding\n        // home domain name for the user\n\n        if (!principal.getDomain().equals(userDomain)) {\n            return null;\n        }\n\n        final String userHomeDomain = homeDomainPrefix + getUserDomainName(principal.getName());\n        if (!userHomeDomain.equals(domainName)) {\n            return null;\n        }\n\n        return virtualHomeDomain(principal, domainName);\n    }\n\n    AccessStatus evaluateAccess(AthenzDomain domain, String identity, String action, String resource,\n            List<String> authenticatedRoles, String trustDomain) {\n\n        AccessStatus accessStatus = AccessStatus.DENIED;\n\n        List<Policy> policies = domain.getPolicies();\n        List<Role> roles = domain.getRoles();\n\n        for (Policy policy : policies) {\n\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"evaluateAccess: processing policy: {}\", policy.getName());\n            }\n\n            // we are going to process all the assertions defined in this\n            // policy. As soon as we get a match for an assertion that\n            // denies access, we're going to return that result. If we\n            // get a match for an assertion that allows access we're\n            // going to remember that result and continue looking at\n            // all the assertions in case there is something else that\n            // explicitly denies access\n\n            List<Assertion> assertions = policy.getAssertions();\n            if (assertions == null) {\n                continue;\n            }\n\n            for (Assertion assertion : assertions) {\n\n                // get the effect for the assertion which is set\n                // as allowed by default\n\n                AssertionEffect effect = assertion.getEffect();\n                if (effect == null) {\n                    effect = AssertionEffect.ALLOW;\n                }\n\n                // if we have already matched an allow assertion then\n                // we'll automatically skip any assertion that has\n                // allow effect since there is no point of matching it\n\n                if (accessStatus == AccessStatus.ALLOWED && effect == AssertionEffect.ALLOW) {\n                    continue;\n                }\n\n                // if no match then process the next assertion\n\n                if (!assertionMatch(assertion, identity, action, resource, domain.getName(),\n                        roles, authenticatedRoles, trustDomain)) {\n                    continue;\n                }\n\n                // if the assertion has matched and the effect is deny\n                // then we're going to return right away otherwise we'll\n                // set our return allow matched flag to true and continue\n                // processing other assertions\n\n                if (effect == AssertionEffect.DENY) {\n                    return AccessStatus.DENIED;\n                }\n\n                accessStatus = AccessStatus.ALLOWED;\n            }\n        }\n\n        return accessStatus;\n    }\n\n    String userHomeDomainResource(String resource) {\n\n        // if the resource does not start with user domain prefix then\n        // we have nothing to do and we'll return resource as is\n\n        if (!resource.startsWith(USER_DOMAIN_PREFIX)) {\n            return resource;\n        }\n\n        String homeResource = null;\n\n        // if we have different userDomain and homeDomain values then\n        // we need to replace both domain and user names otherwise\n        // we only need to update the domain value\n\n        if (!userDomain.equals(homeDomain)) {\n\n            // let's extract the user name. at this point we should\n            // have the format user.<user-name>:resource\n\n            int idx = resource.indexOf(':');\n            if (idx == -1) {\n                return resource;\n            }\n\n            final String userName = resource.substring(USER_DOMAIN_PREFIX.length(), idx);\n            homeResource = homeDomainPrefix + getUserDomainName(userName) + resource.substring(idx);\n\n        } else if (!homeDomain.equals(ZMSConsts.USER_DOMAIN)) {\n            homeResource = homeDomainPrefix + resource.substring(USER_DOMAIN_PREFIX.length());\n        }\n        return homeResource == null ? resource : homeResource;\n    }\n\n    public boolean access(String action, String resource, Principal principal, String trustDomain) {\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        resource = resource.toLowerCase();\n        if (trustDomain != null) {\n            trustDomain = trustDomain.toLowerCase();\n        }\n        action = action.toLowerCase();\n\n        // if the resource starts with the user domain and the environment is using\n        // a different domain name we'll dynamically update the resource value\n\n        resource = userHomeDomainResource(resource);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"access:(\" + action + \", \" + resource + \", \" + principal + \", \" + trustDomain + \")\");\n        }\n\n        // check to see if the authority is allowed to be processed in\n        // authorization checks. If this value is false then the principal\n        // must get a usertoken from ZMS first and the submit the request\n        // with that token\n\n        if (!authorityAuthorizationAllowed(principal)) {\n            LOG.error(\"Authority is not allowed to support authorization checks\");\n            return false;\n        }\n\n        // retrieve our domain based on resource and action/trustDomain pair\n        // we want to provider better error reporting to the users so if we get a\n        // request where the domain is not found instead of just returning 403\n        // forbidden (which is confusing since it assumes the user doesn't have\n        // access as oppose to possible mistype of the domain name by the user)\n        // we want to return 404 not found. The athenz server common has special handling\n        // for rest.ResourceExceptions so we'll throw that exception in this\n        // special case of not found domains.\n\n        String domainName = retrieveResourceDomain(resource, action, trustDomain);\n        if (domainName == null) {\n            throw new com.yahoo.athenz.common.server.rest.ResourceException(\n                    ResourceException.NOT_FOUND, \"Domain not found\");\n        }\n        AthenzDomain domain = retrieveAccessDomain(domainName, principal);\n        if (domain == null) {\n            throw new com.yahoo.athenz.common.server.rest.ResourceException(\n                    ResourceException.NOT_FOUND, \"Domain not found\");\n        }\n\n        // if the domain is disabled then we're going to reject this\n        // request right away\n\n        if (domain.getDomain().getEnabled() == Boolean.FALSE) {\n            throw new com.yahoo.athenz.common.server.rest.ResourceException(\n                    ResourceException.FORBIDDEN, \"Disabled Domain\");\n        }\n\n        AccessStatus accessStatus = hasAccess(domain, action, resource, principal, trustDomain);\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n\n    boolean authorityAuthorizationAllowed(Principal principal) {\n\n        Authority authority = principal.getAuthority();\n        if (authority == null) {\n            return true;\n        }\n\n        return authority.allowAuthorization();\n    }\n\n    String retrieveResourceDomain(String resource, String op, String trustDomain) {\n\n        // special handling for ASSUME_ROLE assertions. Since any assertion with\n        // that action refers to a resource in another domain, there is no point\n        // to retrieve the domain name from the resource. In these cases the caller\n        // must specify the trust domain attribute so we'll use that instead and\n        // if one is not specified then we'll fall back to using the domain name\n        // from the resource\n\n        String domainName;\n        if (ZMSConsts.ACTION_ASSUME_ROLE.equalsIgnoreCase(op) && trustDomain != null) {\n            domainName = trustDomain;\n        } else {\n            domainName = extractDomainName(resource);\n        }\n        return domainName;\n    }\n\n    AccessStatus hasAccess(AthenzDomain domain, String action, String resource,\n            Principal principal, String trustDomain) {\n\n        String identity = principal.getFullName();\n\n        // if we're dealing with an access check based on a Role token then\n        // make sure it's valid before processing it\n\n        List<String> authenticatedRoles = principal.getRoles();\n        if (authenticatedRoles != null && !validateRoleBasedAccessCheck(authenticatedRoles, trustDomain,\n                domain.getName(), identity)) {\n            return AccessStatus.DENIED_INVALID_ROLE_TOKEN;\n        }\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n\n        return evaluateAccess(domain, identity, action, resource, authenticatedRoles, trustDomain);\n    }\n\n    public Access getAccessExt(ResourceContext ctx, String action, String resource,\n            String trustDomain, String checkPrincipal) {\n\n        final String caller = \"getaccessext\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(action, TYPE_COMPOUND_NAME, caller);\n\n        return getAccessCheck(((RsrcCtxWrapper) ctx).principal(), action, resource,\n                trustDomain, checkPrincipal, ctx);\n    }\n\n    public Access getAccess(ResourceContext ctx, String action, String resource,\n            String trustDomain, String checkPrincipal) {\n\n        final String caller = \"getaccess\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(action, TYPE_COMPOUND_NAME, caller);\n        validate(resource, TYPE_RESOURCE_NAME, caller);\n\n        return getAccessCheck(((RsrcCtxWrapper) ctx).principal(), action, resource,\n                trustDomain, checkPrincipal, ctx);\n    }\n\n    Access getAccessCheck(Principal principal, String action, String resource,\n            String trustDomain, String checkPrincipal, ResourceContext ctx) {\n\n        final String caller = \"getaccess\";\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getAccessCheck:(\" + action + \", \" + resource + \", \" + principal +\n                    \", \" + trustDomain + \", \" + checkPrincipal + \")\");\n        }\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        action = action.toLowerCase();\n        resource = resource.toLowerCase();\n        if (checkPrincipal != null) {\n            checkPrincipal = checkPrincipal.toLowerCase();\n        }\n        if (trustDomain != null) {\n            trustDomain = trustDomain.toLowerCase();\n        }\n\n        // retrieve the domain based on our resource and action/trustDomain pair\n\n        String domainName = retrieveResourceDomain(resource, action, trustDomain);\n        setRequestDomain(ctx, domainName);\n        if (domainName == null) {\n            setRequestDomain(ctx, ZMSConsts.ZMS_INVALID_DOMAIN);\n            throw ZMSUtils.notFoundError(\"getAccessCheck: Unable to extract resource domain\", caller);\n        }\n        AthenzDomain domain = retrieveAccessDomain(domainName, principal);\n        if (domain == null) {\n            setRequestDomain(ctx, ZMSConsts.ZMS_UNKNOWN_DOMAIN);\n            throw ZMSUtils.notFoundError(\"getAccessCheck: Resource Domain not found: '\"\n                    + domainName + \"'\", caller);\n        }\n\n        // if the domain is disabled then we're going to reject this\n        // request right away\n\n        if (domain.getDomain().getEnabled() == Boolean.FALSE) {\n            throw ZMSUtils.forbiddenError(\"getAccessCheck: Disabled domain: '\"\n                    + domainName + \"'\", caller);\n        }\n\n        // if the check principal is given then we need to carry out the access\n        // check against that principal\n\n        if (checkPrincipal != null) {\n            principal = createPrincipalForName(checkPrincipal);\n            if (principal == null) {\n                throw ZMSUtils.unauthorizedError(\"getAccessCheck: Invalid check principal value specified\", caller);\n            }\n        }\n\n        boolean accessAllowed = false;\n        AccessStatus accessStatus = hasAccess(domain, action, resource, principal, trustDomain);\n        if (accessStatus == AccessStatus.ALLOWED) {\n            accessAllowed = true;\n        }\n        return new Access().setGranted(accessAllowed);\n    }\n\n    void validateEntity(String entityName, Entity entity) {\n\n        final String caller = \"validateentity\";\n\n        if (!entityName.equals(entity.getName())) {\n            throw ZMSUtils.requestError(\"validateEntity: Entity name mismatch: \" + entityName + \" != \" + entity.getName(), caller);\n        }\n        if (entity.getValue() == null) {\n            throw ZMSUtils.requestError(\"validateEntity: Entity value is empty: \" + entityName, caller);\n        }\n    }\n\n    @Override\n    public void putEntity(ResourceContext ctx, String domainName, String entityName, String auditRef, Entity resource) {\n\n        final String caller = \"putentity\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(entityName, TYPE_ENTITY_NAME, caller);\n        validateEntity(entityName, resource);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        entityName = entityName.toLowerCase();\n        AthenzObject.ENTITY.convertToLowerCase(resource);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        dbService.executePutEntity(ctx, domainName, entityName, resource, auditRef, caller);\n    }\n\n    @Override\n    public EntityList getEntityList(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getentitylist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        EntityList result = new EntityList();\n        List<String> names = dbService.listEntities(domainName);\n        result.setNames(names);\n\n        return result;\n    }\n\n    public Entity getEntity(ResourceContext ctx, String domainName, String entityName) {\n\n        final String caller = \"getentity\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(entityName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        entityName = entityName.toLowerCase();\n\n        Entity entity = dbService.getEntity(domainName, entityName);\n        if (entity == null) {\n            throw ZMSUtils.notFoundError(\"getEntity: Entity not found: '\" +\n                    ZMSUtils.entityResourceName(domainName, entityName) + \"'\", caller);\n        }\n\n        return entity;\n    }\n\n    public void deleteEntity(ResourceContext ctx, String domainName, String entityName, String auditRef) {\n\n        final String caller = \"deleteentity\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(entityName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        entityName = entityName.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        dbService.executeDeleteEntity(ctx, domainName, entityName, auditRef, caller);\n    }\n\n    public ServerTemplateList getServerTemplateList(ResourceContext ctx) {\n\n        final String caller = \"getservertemplatelist\";\n\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        ServerTemplateList result = new ServerTemplateList();\n        result.setTemplateNames(new ArrayList<>(serverSolutionTemplates.names()));\n\n        return result;\n    }\n\n    public Template getTemplate(ResourceContext ctx, String templateName) {\n\n        final String caller = \"gettemplate\";\n\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(templateName, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        templateName = templateName.toLowerCase();\n        Template template = serverSolutionTemplates.get(templateName);\n        if (template == null) {\n            throw ZMSUtils.notFoundError(\"getTemplate: Template not found: '\" + templateName + \"'\", caller);\n        }\n\n        List<Role> roles = template.getRoles();\n        if (roles != null && !roles.isEmpty()) {\n            for (Role role : roles) {\n                List<RoleMember> roleMembers = role.getRoleMembers();\n                if (roleMembers != null) {\n                    role.setMembers(ZMSUtils.convertRoleMembersToMembers(roleMembers));\n                }\n            }\n        }\n\n        return template;\n    }\n\n    @Override\n    public DomainTemplateDetailsList getDomainTemplateDetailsList(ResourceContext ctx, String domainName) {\n        final String caller = \"getDomainTemplateDetailsList\";\n        logPrincipal(ctx);\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all domain into lower case\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        List<TemplateMetaData> templateDomainMapping = dbService.getDomainTemplates(domainName);\n        DomainTemplateDetailsList domainTemplateDetailsList = null;\n        if (templateDomainMapping != null) {\n            domainTemplateDetailsList = new DomainTemplateDetailsList();\n            for (TemplateMetaData metaData : templateDomainMapping) {\n                Template template = serverSolutionTemplates.get(metaData.getTemplateName());\n                // there is a possibility of a stale template coming back from DB over time(caused by template clean up)\n                if (template != null) {\n                    //Merging template metadata fields from solution-templates.json and template data from DB\n                    metaData.setLatestVersion(template.getMetadata().getLatestVersion());\n                    metaData.setAutoUpdate(template.getMetadata().getAutoUpdate());\n                    metaData.setDescription(template.getMetadata().getDescription());\n                    metaData.setKeywordsToReplace(template.metadata.getKeywordsToReplace());\n                    metaData.setTimestamp(template.metadata.getTimestamp());\n                }\n            }\n            domainTemplateDetailsList.setMetaData(templateDomainMapping);\n        }\n        return domainTemplateDetailsList;\n    }\n\n    public RoleList getRoleList(ResourceContext ctx, String domainName, Integer limit, String skip) {\n\n        final String caller = \"getrolelist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        if (skip != null) {\n            skip = skip.toLowerCase();\n        }\n\n        RoleList result = new RoleList();\n\n        List<String> names = new ArrayList<>();\n        String next = processListRequest(domainName, AthenzObject.ROLE, limit, skip, names);\n        result.setNames(names);\n        if (next != null) {\n            result.setNext(next);\n        }\n\n        return result;\n    }\n\n    List<Role> setupRoleList(AthenzDomain domain, Boolean members) {\n\n        // if we're asked to return the members as well then we\n        // just need to return the data as is without any modifications\n\n        List<Role> roles;\n        if (members == Boolean.TRUE) {\n            roles = domain.getRoles();\n        } else {\n            roles = new ArrayList<>();\n            for (Role role : domain.getRoles()) {\n                Role newRole = new Role()\n                        .setName(role.getName())\n                        .setModified(role.getModified())\n                        .setTrust(role.getTrust())\n                        .setAuditEnabled(role.getAuditEnabled())\n                        .setSelfServe(role.getSelfServe())\n                        .setMemberExpiryDays(role.getMemberExpiryDays())\n                        .setServiceExpiryDays(role.getServiceExpiryDays())\n                        .setTokenExpiryMins(role.getTokenExpiryMins())\n                        .setCertExpiryMins(role.getCertExpiryMins())\n                        .setMemberReviewDays(role.getMemberReviewDays())\n                        .setServiceReviewDays(role.getServiceReviewDays())\n                        .setSignAlgorithm(role.getSignAlgorithm())\n                        .setReviewEnabled(role.getReviewEnabled())\n                        .setLastReviewedDate(role.getLastReviewedDate());\n                roles.add(newRole);\n            }\n        }\n\n        return roles;\n    }\n\n    public Roles getRoles(ResourceContext ctx, String domainName, Boolean members) {\n\n        final String caller = \"getroles\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        Roles result = new Roles();\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"getRoles: Domain not found: '\" + domainName + \"'\", caller);\n        }\n\n        result.setList(setupRoleList(domain, members));\n        return result;\n    }\n\n    @Override\n    public DomainRoleMembers getDomainRoleMembers(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getdomainrolemembers\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        return dbService.listDomainRoleMembers(domainName);\n    }\n\n    @Override\n    public DomainRoleMember getPrincipalRoles(ResourceContext context, String principal, String domainName) {\n        final String caller = \"getPrincipalRoles\";\n        logPrincipal(context);\n\n        if (StringUtil.isEmpty(principal)) {\n            // If principal not specified, get roles for current user\n            principal = ((RsrcCtxWrapper) context).principal().getFullName();\n        }\n        validateRequest(context.request(), caller);\n        validate(principal, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        principal = principal.toLowerCase();\n\n        if (!StringUtil.isEmpty(domainName)) {\n            validate(domainName, TYPE_DOMAIN_NAME, caller);\n            domainName = domainName.toLowerCase();\n            setRequestDomain(context, domainName);\n        }\n\n        return dbService.getPrincipalRoles(principal, domainName);\n    }\n\n    @Override\n    public Role getRole(ResourceContext ctx, String domainName, String roleName,\n            Boolean auditLog, Boolean expand, Boolean pending) {\n\n        final String caller = \"getrole\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n\n        Role role = dbService.getRole(domainName, roleName, auditLog, expand, pending);\n        if (role == null) {\n            throw ZMSUtils.notFoundError(\"getRole: Role not found: '\" +\n                    ZMSUtils.roleResourceName(domainName, roleName) + \"'\", caller);\n        }\n\n        return role;\n    }\n\n    List<String> normalizedAdminUsers(List<String> admins, final String domainUserAuthorityFilter, final String caller) {\n        List<String> normalizedAdmins = new ArrayList<>();\n        for (String admin : admins) {\n            normalizedAdmins.add(normalizeDomainAliasUser(admin));\n        }\n        // now go through the list and make sure they're all valid\n        for (String admin : normalizedAdmins) {\n            validateRoleMemberPrincipal(admin, domainUserAuthorityFilter, caller);\n        }\n        return normalizedAdmins;\n    }\n\n    String normalizeDomainAliasUser(String user) {\n        if (user != null && userDomainAliasPrefix != null && user.startsWith(userDomainAliasPrefix)) {\n            if (user.indexOf('.', userDomainAliasPrefix.length()) == -1) {\n                return userDomainPrefix + user.substring(userDomainAliasPrefix.length());\n            }\n        }\n        return user;\n    }\n\n    private void addNormalizedRoleMember(Map<String, RoleMember> normalizedMembers,\n            RoleMember member) {\n\n        member.setMemberName(normalizeDomainAliasUser(member.getMemberName()));\n\n        // we'll automatically ignore any duplicates\n\n        if (!normalizedMembers.containsKey(member.getMemberName())) {\n            normalizedMembers.put(member.getMemberName(), member);\n        }\n    }\n\n    void normalizeRoleMembers(Role role) {\n\n        Map<String, RoleMember> normalizedMembers = new HashMap<>();\n\n        // normalize getMembers() first\n\n        List<String> members = role.getMembers();\n        if (members != null) {\n            for (String memberOld : members) {\n                RoleMember member = new RoleMember().setMemberName(memberOld);\n                addNormalizedRoleMember(normalizedMembers, member);\n            }\n        }\n\n        // normalize getRoleMembers() now\n\n        List<RoleMember> roleMembers = role.getRoleMembers();\n        if (roleMembers != null) {\n            for (RoleMember member : roleMembers) {\n                addNormalizedRoleMember(normalizedMembers, member);\n            }\n        }\n        role.setRoleMembers(new ArrayList<>(normalizedMembers.values()));\n        role.setMembers(null);\n    }\n\n    boolean isConsistentRoleName(final String domainName, final String roleName, Role role) {\n\n        String resourceName = ZMSUtils.roleResourceName(domainName, roleName);\n\n        // first lets assume we have the expected name specified in the role\n\n        if (resourceName.equals(role.getName())) {\n            return true;\n        }\n\n        // if not check to see if the role contains the relative local name\n        // part only instead of the expected resourceName and update accordingly\n\n        if (roleName.equals(role.getName())) {\n            role.setName(resourceName);\n            return true;\n        }\n\n        // we have a mismatch\n\n        return false;\n    }\n\n    @Override\n    public void putRole(ResourceContext ctx, String domainName, String roleName, String auditRef, Role role) {\n\n        final String caller = \"putrole\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(role, TYPE_ROLE, caller);\n        validateRoleValues(role);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        AthenzObject.ROLE.convertToLowerCase(role);\n\n        // validate the user authority settings if they're provided\n\n        validateRoleUserAuthorityAttributes(role.getUserAuthorityFilter(), role.getUserAuthorityExpiration(), caller);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        // verify the role name in the URI and request are consistent\n\n        if (!isConsistentRoleName(domainName, roleName, role)) {\n            throw ZMSUtils.requestError(\"putRole: Inconsistent role names - expected: \"\n                    + ZMSUtils.roleResourceName(domainName, roleName) + \", actual: \"\n                    + role.getName(), caller);\n        }\n\n        Domain domain = dbService.getDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"No such domain: \" + domainName, caller);\n        }\n\n        // validate role and trust settings are as expected\n\n        validateRoleStructure(role, domainName, caller);\n\n        // normalize and remove duplicate members\n\n        normalizeRoleMembers(role);\n\n        // check to see if we need to validate user and service members\n        // and possibly user authority filter restrictions\n\n        validateRoleMemberPrincipals(role, domain.getUserAuthorityFilter(), caller);\n\n        // if the role is review enabled then it cannot contain\n        // role members as we want review and audit enabled roles\n        // to be enabled as such and then add individual members\n\n        if (role.getReviewEnabled() == Boolean.TRUE && !role.getRoleMembers().isEmpty()) {\n            throw ZMSUtils.requestError(\"Set review enabled flag using role meta api\", caller);\n        }\n\n        // update role expiry based on our configurations\n\n        updateRoleMemberExpiration(\n                domain.getMemberExpiryDays(),\n                role.getMemberExpiryDays(),\n                domain.getServiceExpiryDays(),\n                role.getServiceExpiryDays(),\n                role.getRoleMembers());\n\n        // update role expiry based on user authority expiry\n        // if configured\n\n        updateRoleMemberUserAuthorityExpiry(role, caller);\n\n        // update role review based on our configurations\n\n        updateRoleMemberReviewReminder(role.getMemberReviewDays(), role.getServiceReviewDays(), role.getRoleMembers());\n\n        // process our request\n\n        dbService.executePutRole(ctx, domainName, roleName, role, auditRef, caller);\n    }\n\n   void validateRoleStructure(final Role role, final String domainName, final String caller) {\n\n        if ((role.getMembers() != null && !role.getMembers().isEmpty())\n                && (role.getRoleMembers() != null && !role.getRoleMembers().isEmpty())) {\n            throw ZMSUtils.requestError(\"validateRoleMembers: Role cannot have both members and roleMembers set\", caller);\n        }\n\n        // if this is a delegated role then validate that it's not\n        // delegated back to itself and there are no members since\n        // those 2 fields are mutually exclusive\n\n        if (role.getTrust() != null && !role.getTrust().isEmpty()) {\n\n            AthenzDomain athenzDomain = getAthenzDomain(role.getTrust(), true);\n            if (athenzDomain == null) {\n                throw ZMSUtils.requestError(\"Delegated role assigned to non existing domain\", caller);\n            }\n\n            if (role.getRoleMembers() != null && !role.getRoleMembers().isEmpty()) {\n                throw ZMSUtils.requestError(\"validateRoleMembers: Role cannot have both roleMembers and delegated domain set\", caller);\n            }\n\n            if (role.getMembers() != null && !role.getMembers().isEmpty()) {\n                throw ZMSUtils.requestError(\"validateRoleMembers: Role cannot have both members and delegated domain set\", caller);\n            }\n\n            if (domainName.equals(role.getTrust())) {\n                throw ZMSUtils.requestError(\"validateRoleMembers: Role cannot be delegated to itself\", caller);\n            }\n        }\n    }\n\n    void validateRoleMemberPrincipals(final Role role, final String domainUserAuthorityFilter, final String caller) {\n\n        // make sure we have either one of the options enabled for verification\n\n        final String userAuthorityFilter = enforcedUserAuthorityFilter(role.getUserAuthorityFilter(),\n                domainUserAuthorityFilter);\n\n        if (!shouldValidateRoleMembers(userAuthorityFilter)) {\n            return;\n        }\n\n        for (RoleMember roleMember : role.getRoleMembers()) {\n            validateRoleMemberPrincipal(roleMember.getMemberName(), userAuthorityFilter, caller);\n        }\n    }\n\n    void updateRoleMemberUserAuthorityExpiry(final Role role, final String caller) {\n\n        final String userAuthorityExpiry = getUserAuthorityExpiryAttr(role);\n        if (userAuthorityExpiry == null) {\n            return;\n        }\n\n        for (RoleMember roleMember : role.getRoleMembers()) {\n\n            // we only process users and automatically ignore services which\n            // are not handled by user authority\n\n            if (ZMSUtils.isUserDomainPrincipal(roleMember.getMemberName(), userDomainPrefix,\n                    addlUserCheckDomainPrefixList)) {\n\n                // if we don't have an expiry specified for the user\n                // then we're not going to allow this member\n\n                Date expiry = userAuthority.getDateAttribute(roleMember.getMemberName(), userAuthorityExpiry);\n                if (expiry == null) {\n                    throw ZMSUtils.requestError(\"Invalid member: \" + roleMember.getMemberName() +\n                            \". No expiry date attribute specified in user authority\", caller);\n                }\n                roleMember.setExpiration(Timestamp.fromDate(expiry));\n            }\n        }\n    }\n\n    void validateRoleMemberPrincipal(final String memberName, final String userAuthorityFilter, final String caller) {\n\n        boolean bUser = ZMSUtils.isUserDomainPrincipal(memberName, userDomainPrefix, addlUserCheckDomainPrefixList);\n        if (bUser) {\n\n            // if the account contains a wildcard then we're going\n            // to let the user authority decide if it's valid or not\n\n            if (validateUserRoleMembers && userAuthority != null) {\n                if (!userAuthority.isValidUser(memberName)) {\n                    throw ZMSUtils.requestError(\"Principal \" + memberName + \" is not valid\", caller);\n                }\n            }\n\n            // once we know it's a valid principal and we have a user\n            // authority filter configured, we'll check that as well\n            // if we're already determined that the principal is not\n            // valid there is no point of running this check\n\n            if (!StringUtil.isEmpty(userAuthorityFilter)) {\n                if (!ZMSUtils.isUserAuthorityFilterValid(userAuthority, userAuthorityFilter, memberName)) {\n                    throw ZMSUtils.requestError(\"Invalid member: \" + memberName +\n                            \". Required user authority filter not valid for the member\", caller);\n                }\n            }\n\n        } else {\n\n            if (validateServiceRoleMembers) {\n\n                // if the account contains a wildcard character then\n                // we're going to assume it's valid\n\n                int idx = memberName.indexOf('*');\n                if (idx == -1) {\n                    idx = memberName.lastIndexOf('.');\n                    if (idx != -1) {\n                        final String domainName = memberName.substring(0, idx);\n                        final String serviceName = memberName.substring(idx + 1);\n\n                        // first we need to check if the domain is on the list of\n                        // our skip domains for service member validation. these\n                        // are typically domains (like for ci/cd) where services\n                        // are dynamic and do not need to be registered in Athenz\n\n                        if (!validateServiceMemberSkipDomains.contains(domainName)) {\n                            if (dbService.getServiceIdentity(domainName, serviceName, true) == null) {\n                                throw ZMSUtils.requestError(\"Principal \" + memberName + \" is not a valid service\", caller);\n                            }\n                        }\n                    } else {\n                        throw ZMSUtils.requestError(\"Principal \" + memberName + \" is not valid\", caller);\n                    }\n                }\n            }\n        }\n    }\n\n    public void deleteRole(ResourceContext ctx, String domainName, String roleName, String auditRef) {\n\n        final String caller = \"deleterole\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        /* we are not going to allow any user to delete\n         * the admin role and policy since those are required\n         * for standard domain operations */\n\n        if (roleName.equalsIgnoreCase(ADMIN_ROLE_NAME)) {\n            throw ZMSUtils.requestError(\"deleteRole: admin role cannot be deleted\", caller);\n        }\n\n        dbService.executeDeleteRole(ctx, domainName, roleName, auditRef, caller);\n    }\n\n    boolean memberNameMatch(String memberName, String matchName) {\n        // we are supporting 3 formats for role members\n        // *, <domain>.* and <domain>.<user>*\n        if (memberName.equals(\"*\")) {\n            return true;\n        } else if (memberName.endsWith(\"*\")) {\n            return matchName.startsWith(memberName.substring(0, memberName.length() - 1));\n        } else {\n            return memberName.equals(matchName);\n        }\n    }\n\n    boolean isMemberEnabled(RoleMember roleMember) {\n        return (roleMember.getSystemDisabled() == null || roleMember.getSystemDisabled() == 0);\n    }\n\n    boolean isMemberExpired(RoleMember roleMember) {\n        // check expiration, if is not defined, its not expired.\n        Timestamp expiration = roleMember.getExpiration();\n        return (expiration != null && expiration.millis() < System.currentTimeMillis());\n    }\n\n    boolean checkRoleMemberValidity(List<RoleMember> roleMembers, String member) {\n\n        // we need to make sure that both the user is not expired\n        // and not disabled by the system\n\n        boolean isMember = false;\n        for (RoleMember memberInfo: roleMembers) {\n            final String memberName = memberInfo.getMemberName();\n            if (memberNameMatch(memberName, member)) {\n                isMember = isMemberEnabled(memberInfo) && !isMemberExpired(memberInfo);\n                break;\n            }\n        }\n        return isMember;\n    }\n\n    boolean isMemberOfRole(Role role, String member) {\n        List<RoleMember> roleMembers = role.getRoleMembers();\n        if (roleMembers == null) {\n            return false;\n        }\n        return checkRoleMemberValidity(roleMembers, member);\n    }\n\n    @Override\n    public Membership getMembership(ResourceContext ctx, String domainName,\n            String roleName, String memberName, String expiration) {\n\n        final String caller = \"getmembership\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        memberName = normalizeDomainAliasUser(memberName.toLowerCase());\n        long expiryTimestamp = getModTimestamp(expiration);\n\n        return dbService.getMembership(domainName, roleName, memberName, expiryTimestamp, false);\n    }\n\n    @Override\n    public DomainRoleMembers getOverdueReview(ResourceContext ctx, String domainName) {\n        final String caller = \"getoverduereview\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        return dbService.listOverdueReviewRoleMembers(domainName);\n    }\n\n    long configuredDueDateMillis(Integer domainDueDateDays, Integer roleDueDateDays) {\n\n        // the role expiry days settings overrides the domain one if one configured\n\n        int expiryDays = 0;\n        if (roleDueDateDays != null && roleDueDateDays > 0) {\n            expiryDays = roleDueDateDays;\n        } else if (domainDueDateDays != null && domainDueDateDays > 0) {\n            expiryDays = domainDueDateDays;\n        }\n        return expiryDays == 0 ? 0 : System.currentTimeMillis() + TimeUnit.MILLISECONDS.convert(expiryDays, TimeUnit.DAYS);\n    }\n\n    Timestamp getMemberDueDate(long cfgDueDateMillis, Timestamp memberDueDate) {\n        if (memberDueDate == null) {\n            return Timestamp.fromMillis(cfgDueDateMillis);\n        } else if (memberDueDate.millis() > cfgDueDateMillis) {\n            return Timestamp.fromMillis(cfgDueDateMillis);\n        } else {\n            return memberDueDate;\n        }\n    }\n\n    void updateRoleMemberExpiration(Integer domainUserMemberDueDateDays,\n                                    Integer roleUserMemberDueDateDays,\n                                    Integer domainServiceMemberDueDateDays,\n                                    Integer roleServiceMemberDueDateDays,\n                                    List<RoleMember> roleMembers) {\n        updateRoleMemberDueDate(\n                domainUserMemberDueDateDays,\n                roleUserMemberDueDateDays,\n                domainServiceMemberDueDateDays,\n                roleServiceMemberDueDateDays,\n                roleMembers,\n                roleMember -> roleMember.getExpiration(),\n                (roleMember, expiration) -> roleMember.setExpiration(expiration));\n    }\n\n    void updateRoleMemberReviewReminder(Integer roleUserMemberDueDateDays,\n                                        Integer roleServiceMemberDueDateDays,\n                                        List<RoleMember> roleMembers) {\n        updateRoleMemberDueDate(\n                null,\n                roleUserMemberDueDateDays,\n                null,\n                roleServiceMemberDueDateDays,\n                roleMembers,\n                roleMember -> roleMember.getReviewReminder(),\n                (roleMember, reviewReminder) -> roleMember.setReviewReminder(reviewReminder));\n    }\n\n    private void updateRoleMemberDueDate(Integer domainUserMemberDueDateDays,\n                                 Integer roleUserMemberDueDateDays,\n                                 Integer domainServiceMemberDueDateDays,\n                                 Integer roleServiceMemberDueDateDays,\n                                 List<RoleMember> roleMembers,\n                                 Function<RoleMember, Timestamp> dueDateGetter,\n                                 BiConsumer<RoleMember, Timestamp> dueDateSetter) {\n\n        long cfgUserMemberDueDateMillis = configuredDueDateMillis(domainUserMemberDueDateDays, roleUserMemberDueDateDays);\n        long cfgServiceMemberDueDateMillis = configuredDueDateMillis(domainServiceMemberDueDateDays, roleServiceMemberDueDateDays);\n\n        // if we have no value configured then we have nothing to\n        // do so we'll just return right away\n\n        if (cfgUserMemberDueDateMillis == 0 && cfgServiceMemberDueDateMillis == 0) {\n            return;\n        }\n\n        // go through the members and update due date as necessary\n\n        for (RoleMember roleMember : roleMembers) {\n            boolean bUser = ZMSUtils.isUserDomainPrincipal(roleMember.getMemberName(), userDomainPrefix,\n                    addlUserCheckDomainPrefixList);\n            Timestamp currentDueDate = dueDateGetter.apply(roleMember);\n            if (bUser && cfgUserMemberDueDateMillis != 0) {\n                Timestamp newDueDate = getMemberDueDate(cfgUserMemberDueDateMillis, currentDueDate);\n                dueDateSetter.accept(roleMember, newDueDate);\n            } else if (!bUser && cfgServiceMemberDueDateMillis != 0) {\n                Timestamp newDueDate = getMemberDueDate(cfgServiceMemberDueDateMillis, currentDueDate);\n                dueDateSetter.accept(roleMember, newDueDate);\n            }\n        }\n    }\n\n    Timestamp memberDueDateTimestamp(Integer domainDueDateDays, Integer roleDueDateDays, Timestamp memberDueDate) {\n\n        long cfgExpiryMillis = configuredDueDateMillis(domainDueDateDays, roleDueDateDays);\n\n        // if we have no value configured then return\n        // the membership expiration as is\n\n        if (cfgExpiryMillis == 0) {\n            return memberDueDate;\n        }\n\n        // otherwise compare the configured expiry days with the specified\n        // membership value and choose the smallest expiration value\n\n        return getMemberDueDate(cfgExpiryMillis, memberDueDate);\n    }\n\n    @Override\n    public void putMembership(ResourceContext ctx, String domainName, String roleName,\n            String memberName, String auditRef, Membership membership) {\n\n        final String caller = \"putmembership\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        setRequestDomain(ctx, domainName);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n        validate(membership, TYPE_MEMBERSHIP, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        roleName = roleName.toLowerCase();\n        memberName = memberName.toLowerCase();\n        AthenzObject.MEMBERSHIP.convertToLowerCase(membership);\n\n        final Principal principal = ((RsrcCtxWrapper) ctx).principal();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceRoleOperation(principal.getAuthorizedService(), caller, roleName);\n\n        // verify that the member name in the URI and object provided match\n\n        if (!memberName.equals(membership.getMemberName())) {\n            throw ZMSUtils.requestError(\"putMembership: Member name in URI and Membership object do not match\", caller);\n        }\n\n        // role name is optional so we'll verify only if the value is present in the object\n\n        if (membership.getRoleName() != null && !roleName.equals(membership.getRoleName())) {\n            throw ZMSUtils.requestError(\"putMembership: Role name in URI and Membership object do not match\", caller);\n        }\n\n        // extract our role object to get its attributes\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        Role role = getRoleFromDomain(roleName, domain);\n\n        if (role == null) {\n            throw ZMSUtils.requestError(\"Invalid rolename specified\", caller);\n        }\n\n        // create and normalize the role member object\n\n        RoleMember roleMember = new RoleMember();\n        roleMember.setMemberName(normalizeDomainAliasUser(memberName));\n        setRoleMemberExpiration(domain, role, roleMember, membership, caller);\n        setRoleMemberReview(role, roleMember, membership);\n\n        // check to see if we need to validate the principal\n\n        final String userAuthorityFilter = enforcedUserAuthorityFilter(role.getUserAuthorityFilter(),\n                domain.getDomain().getUserAuthorityFilter());\n        if (shouldValidateRoleMembers(userAuthorityFilter)) {\n            validateRoleMemberPrincipal(roleMember.getMemberName(), userAuthorityFilter, caller);\n        }\n\n        // authorization check which also automatically updates\n        // the active and approved flags for the request\n\n        if (!isAllowedPutMembership(principal, domain, role, roleMember)) {\n            throw ZMSUtils.forbiddenError(\"putMembership: principal is not authorized to add members\", caller);\n        }\n\n        // add the member to the specified role\n\n        dbService.executePutMembership(ctx, domainName, roleName, roleMember, auditRef, caller);\n\n        // new role member with pending status. Notify approvers\n\n        if (roleMember.getApproved() == Boolean.FALSE) {\n            sendMembershipApprovalNotification(domainName, domain.getDomain().getOrg(), roleName,\n                    roleMember.getMemberName(), auditRef, principal.getFullName(), role);\n        }\n    }\n\n    String enforcedUserAuthorityFilter(final String roleUserAuthorityFilter, final String domainUserAuthorityFilter) {\n\n        // for a filter to be enforced we need to make sure we have\n        // a valid user authority object along with non-empty filter\n\n        if (userAuthority == null) {\n            return null;\n        }\n\n        return ZMSUtils.combineUserAuthorityFilters(roleUserAuthorityFilter, domainUserAuthorityFilter);\n    }\n\n    boolean shouldValidateRoleMembers(final String userAuthorityFilter) {\n        return validateUserRoleMembers || validateServiceRoleMembers || userAuthorityFilter != null;\n    }\n\n    String getUserAuthorityExpiryAttr(final Role role) {\n\n        // we must have a valid user authority\n\n        if (userAuthority == null) {\n            return null;\n        }\n\n        final String userAuthorityExpiry = role.getUserAuthorityExpiration();\n        if (userAuthorityExpiry == null || userAuthorityExpiry.isEmpty()) {\n            return null;\n        }\n\n        return userAuthorityExpiry;\n    }\n\n    Timestamp getUserAuthorityExpiry(final String userName, final Role role, final String caller) {\n\n        final String userAuthorityExpiry = getUserAuthorityExpiryAttr(role);\n        if (userAuthorityExpiry == null) {\n            return null;\n        }\n\n        // if we don't get an expiry then we're going to throw an exception\n        // since based on our config we must have an expiry specified\n\n        Date expiry = userAuthority.getDateAttribute(userName, userAuthorityExpiry);\n        if (expiry == null) {\n            throw ZMSUtils.requestError(\"User does not have required user authority expiry configured\", caller);\n        }\n\n        return Timestamp.fromDate(expiry);\n    }\n\n    void setRoleMemberExpiration(final AthenzDomain domain, final Role role, final RoleMember roleMember,\n            final Membership membership, final String caller) {\n\n        boolean bUser = ZMSUtils.isUserDomainPrincipal(roleMember.getMemberName(), userDomainPrefix,\n                addlUserCheckDomainPrefixList);\n\n        if (bUser) {\n            Timestamp userAuthorityExpiry = getUserAuthorityExpiry(roleMember.memberName, role, caller);\n            if (userAuthorityExpiry != null) {\n                roleMember.setExpiration(userAuthorityExpiry);\n            } else {\n                roleMember.setExpiration(memberDueDateTimestamp(domain.getDomain().getMemberExpiryDays(),\n                        role.getMemberExpiryDays(), membership.getExpiration()));\n            }\n        } else {\n            roleMember.setExpiration(memberDueDateTimestamp(domain.getDomain().getServiceExpiryDays(),\n                    role.getServiceExpiryDays(), membership.getExpiration()));\n        }\n    }\n\n    void setRoleMemberReview(final Role role, final RoleMember roleMember,\n                                 final Membership membership) {\n\n        boolean bUser = ZMSUtils.isUserDomainPrincipal(roleMember.getMemberName(), userDomainPrefix,\n                addlUserCheckDomainPrefixList);\n        if (bUser) {\n            roleMember.setReviewReminder(memberDueDateTimestamp(null,\n                    role.getMemberReviewDays(), membership.getReviewReminder()));\n        } else {\n            roleMember.setReviewReminder(memberDueDateTimestamp(null,\n                    role.getServiceReviewDays(), membership.getReviewReminder()));\n        }\n    }\n\n     void sendMembershipApprovalNotification(final String domain, final String org, final String roleName,\n            final String member, final String auditRef, final String principal, final Role role) {\n        Map<String, String> details = new HashMap<>();\n        details.put(NOTIFICATION_DETAILS_DOMAIN, domain);\n        details.put(NOTIFICATION_DETAILS_ROLE, roleName);\n        details.put(NOTIFICATION_DETAILS_MEMBER, member);\n        details.put(NOTIFICATION_DETAILS_REASON, auditRef);\n        details.put(NOTIFICATION_DETAILS_REQUESTER, principal);\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Sending Membership Approval notification after putMembership\");\n        }\n\n        List<Notification> notifications = new PutMembershipNotificationTask(domain, org, role, details, dbService, userDomainPrefix).getNotifications();\n        notificationManager.sendNotifications(notifications);\n    }\n\n    public void deletePendingMembership(ResourceContext ctx, String domainName, String roleName,\n            String memberName, String auditRef) {\n\n        final String caller = \"deletependingmembership\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        memberName = normalizeDomainAliasUser(memberName.toLowerCase());\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceRoleOperation(principal.getAuthorizedService(), caller, roleName);\n\n        // authorization check - there are two supported use cases\n        // 1) the caller has authorization in the domain to update members in a role\n        // 2) the caller is the original requestor for the pending request\n\n        if (!isAllowedDeletePendingMembership(principal, domainName, roleName, memberName)) {\n            throw ZMSUtils.forbiddenError(\"deletePendingMembership: principal is not authorized to delete pending members\", caller);\n        }\n\n        // add the member to the specified role\n\n        dbService.executeDeletePendingMembership(ctx, domainName, roleName, memberName, auditRef, caller);\n    }\n\n    public void deleteMembership(ResourceContext ctx, String domainName, String roleName,\n            String memberName, String auditRef) {\n\n        final String caller = \"deletemembership\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        memberName = memberName.toLowerCase();\n        \n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceRoleOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller, roleName);\n\n        dbService.executeDeleteMembership(ctx, domainName, roleName,\n                normalizeDomainAliasUser(memberName), auditRef, caller);\n    }\n\n    public Quota getQuota(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getquota\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        return dbService.getQuota(domainName);\n    }\n\n    @Override\n    public void putQuota(ResourceContext ctx, String domainName, String auditRef, Quota quota) {\n\n        final String caller = \"putQuota\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(quota, TYPE_QUOTA, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        AthenzObject.QUOTA.convertToLowerCase(quota);\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(),\n                caller);\n        \n        // verify that the domain name in the URI and object provided match\n        \n        if (!domainName.equals(quota.getName())) {\n            throw ZMSUtils.requestError(\"putQuota: Domain name in URI and Quota object do not match\", caller);\n        }\n\n        dbService.executePutQuota(ctx, domainName, quota, auditRef, caller);\n    }\n\n    public void deleteQuota(ResourceContext ctx, String domainName, String auditRef) {\n\n        final String caller = \"deleteQuota\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        \n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        dbService.executeDeleteQuota(ctx, domainName, auditRef, caller);\n    }\n    \n    boolean hasExceededListLimit(Integer limit, int count) {\n        \n        if (limit == null) {\n            return false;\n        }\n\n        return limit > 0 && count > limit;\n    }\n    \n    /**\n     * process the list request for the given object type - e.g. role, policy, etc\n     * if the limit is specified and we have reached that limit then return\n     * the name of the object that should be set at the next item for the\n     * subsequent list operation.\n     */\n    String processListRequest(String domainName, AthenzObject objType, Integer limit,\n            String skip, List<String> names) {\n        \n        switch (objType) {\n            case ROLE:\n                names.addAll(dbService.listRoles(domainName));\n                break;\n            case POLICY:\n                names.addAll(dbService.listPolicies(domainName));\n                break;\n            case SERVICE_IDENTITY:\n                names.addAll(dbService.listServiceIdentities(domainName));\n                break;\n            default:\n                return null;\n        }\n        \n        int count = names.size();\n        if (skip != null) {\n            for (int i = 0; i < count; i++) {\n                String name = names.get(i);\n                if (skip.equals(name)) {\n                    names.subList(0, i + 1).clear();\n                    count = names.size();\n                    break;\n                }\n            }\n        }\n        \n        String next = null;\n        if (hasExceededListLimit(limit, count)) {\n            names.subList(limit, count).clear();\n            next = names.get(limit - 1);\n        }\n        \n        return next;\n    }\n    \n    public PolicyList getPolicyList(ResourceContext ctx, String domainName, Integer limit, String skip) {\n\n        final String caller = \"getpolicylist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        if (skip != null) {\n            skip = skip.toLowerCase();\n        }\n        \n        List<String> names = new ArrayList<>();\n        String next = processListRequest(domainName, AthenzObject.POLICY, limit, skip, names);\n        PolicyList result = new PolicyList().setNames(names);\n        if (next != null) {\n            result.setNext(next);\n        }\n        \n        return result;\n    }\n\n    List<Policy> setupPolicyList(AthenzDomain domain, Boolean assertions) {\n        \n        // if we're asked to return the assertions as well then we\n        // just need to return the data as is without any modifications\n        \n        List<Policy> policies;\n        if (assertions == Boolean.TRUE) {\n            policies = domain.getPolicies();\n        } else {\n            policies = new ArrayList<>();\n            for (Policy policy : domain.getPolicies()) {\n                Policy newPolicy = new Policy()\n                        .setName(policy.getName())\n                        .setModified(policy.getModified());\n                policies.add(newPolicy);\n            }\n        }\n        \n        return policies;\n    }\n    \n    public Policies getPolicies(ResourceContext ctx, String domainName, Boolean assertions) {\n\n        final String caller = \"getpolicies\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        Policies result = new Policies();\n        \n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"getPolicies: Domain not found: '\" + domainName + \"'\", caller);\n        }\n\n        result.setList(setupPolicyList(domain, assertions));\n        return result;\n    }\n    \n    public Policy getPolicy(ResourceContext ctx, String domainName, String policyName) {\n\n        final String caller = \"getpolicy\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n        \n        Policy policy = dbService.getPolicy(domainName, policyName);\n        if (policy == null) {\n            throw ZMSUtils.notFoundError(\"getPolicy: Policy not found: '\" +\n                    ZMSUtils.policyResourceName(domainName, policyName) + \"'\", caller);\n        }\n\n        return policy;\n    }\n    \n    public Assertion getAssertion(ResourceContext ctx, String domainName, String policyName,\n            Long assertionId) {\n\n        final String caller = \"getassertion\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n        \n        Assertion assertion = dbService.getAssertion(domainName, policyName, assertionId);\n        if (assertion == null) {\n            throw ZMSUtils.notFoundError(\"getAssertion: Assertion not found: '\" +\n                    ZMSUtils.policyResourceName(domainName, policyName) + \"' Assertion: '\" +\n                    assertionId + \"'\", caller);\n        }\n\n        return assertion;\n    }\n\n    @Override\n    public Assertion putAssertion(ResourceContext ctx, String domainName, String policyName,\n            String auditRef, Assertion assertion) {\n\n        final String caller = \"putassertion\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_COMPOUND_NAME, caller);\n        validate(assertion, TYPE_ASSERTION, caller);\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n        AthenzObject.ASSERTION.convertToLowerCase(assertion);\n\n        // we are not going to allow any user to update\n        // the admin policy since that is required\n        // for standard domain operations */\n        \n        if (policyName.equalsIgnoreCase(ADMIN_POLICY_NAME)) {\n            throw ZMSUtils.requestError(\"putAssertion: admin policy cannot be modified\", caller);\n        }\n        \n        // validate to make sure we have expected values for assertion fields\n        \n        validatePolicyAssertion(assertion, caller);\n        \n        dbService.executePutAssertion(ctx, domainName, policyName, assertion, auditRef, caller);\n        return assertion;\n    }\n    \n    public void deleteAssertion(ResourceContext ctx, String domainName, String policyName,\n            Long assertionId, String auditRef) {\n\n        final String caller = \"deleteassertion\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_ENTITY_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n        \n        // we are not going to allow any user to update\n        // the admin policy since that is required\n        // for standard domain operations */\n        \n        if (policyName.equalsIgnoreCase(ADMIN_POLICY_NAME)) {\n            throw ZMSUtils.requestError(\"deleteAssertion: admin policy cannot be modified\", caller);\n        }\n        \n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        dbService.executeDeleteAssertion(ctx, domainName, policyName, assertionId, auditRef, caller);\n    }\n    \n    void validatePolicyAssertions(List<Assertion> assertions, String caller) {\n        \n        if (assertions == null) {\n            return;\n        }\n        \n        for (Assertion assertion : assertions) {\n            validatePolicyAssertion(assertion, caller);\n        }\n    }\n    \n    void validatePolicyAssertion(Assertion assertion, String caller) {\n            \n        // extract the domain name from the resource\n        \n        final String resource = assertion.getResource();\n        int idx = resource.indexOf(':');\n        if (idx == -1) {\n            throw ZMSUtils.requestError(\"Missing domain name from assertion resource: \"\n                    + resource, caller);\n        }\n\n        // we need to validate our domain name with special\n        // case of * that is allowed to match any domain\n        \n        String domainName = resource.substring(0, idx);\n        if (!domainName.equals(\"*\")) {\n            validate(domainName, TYPE_DOMAIN_NAME, caller);\n        }\n        \n        // we'll also verify that the resource does not contain\n        // any control characters since those cause issues when\n        // data is serialized/deserialized and signature is generated\n        \n        if (StringUtils.containsControlCharacter(resource)) {\n            throw ZMSUtils.requestError(\"Assertion resource contains control characters: \"\n                    + resource, caller);\n        }\n        \n        // verify the action is not empty and does not contain\n        // any control characters\n        \n        final String action = assertion.getAction();\n        if (action == null || action.isEmpty()) {\n            throw ZMSUtils.requestError(\"Assertion action cannot be empty\", caller);\n        }\n        \n        if (StringUtils.containsControlCharacter(action)) {\n            throw ZMSUtils.requestError(\"Assertion action contains control characters: \"\n                    + resource, caller);\n        }\n    }\n    \n    boolean isConsistentPolicyName(final String domainName, final String policyName, Policy policy) {\n        \n        String resourceName = ZMSUtils.policyResourceName(domainName, policyName);\n        \n        // first lets assume we have the expected name specified in the policy\n        \n        if (resourceName.equals(policy.getName())) {\n            return true;\n        }\n\n        // if not check to see if the policy contains the relative local name\n        // part only instead of the expected resourceName and update accordingly\n        \n        if (policyName.equals(policy.getName())) {\n            policy.setName(resourceName);\n            return true;\n        }\n        \n        // we have a mismatch\n        \n        return false;\n    }\n\n    @Override\n    public void putPolicy(ResourceContext ctx, String domainName, String policyName, String auditRef, Policy policy) {\n\n        final String caller = \"putpolicy\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_COMPOUND_NAME, caller);\n        validate(policy, TYPE_POLICY, caller);\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n        AthenzObject.POLICY.convertToLowerCase(policy);\n        \n        // we are not going to allow any user to update\n        // the admin policy since that is required\n        // for standard domain operations */\n        \n        if (policyName.equalsIgnoreCase(ADMIN_POLICY_NAME)) {\n            throw ZMSUtils.requestError(\"putPolicy: admin policy cannot be modified\", caller);\n        }\n        \n        // verify the policy name in the URI and request are consistent\n        \n        if (!isConsistentPolicyName(domainName, policyName, policy)) {\n            throw ZMSUtils.requestError(\"putPolicy: Inconsistent policy names - expected: \"\n                    + ZMSUtils.policyResourceName(domainName, policyName) + \", actual: \"\n                    + policy.getName(), caller);\n        }\n        \n        // validate to make sure we have expected values for assertion fields\n        \n        validatePolicyAssertions(policy.getAssertions(), caller);\n        \n        dbService.executePutPolicy(ctx, domainName, policyName, policy, auditRef, caller);\n    }\n    \n    public void deletePolicy(ResourceContext ctx, String domainName, String policyName, String auditRef) {\n\n        final String caller = \"deletepolicy\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(policyName, TYPE_ENTITY_NAME, caller);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        policyName = policyName.toLowerCase();\n\n        // we are not going to allow any user to delete\n        // the admin role and policy since those are required\n        // for standard domain operations */\n        \n        if (policyName.equalsIgnoreCase(ADMIN_POLICY_NAME)) {\n            throw ZMSUtils.requestError(\"deletePolicy: admin policy cannot be deleted\", caller);\n        }\n        \n        dbService.executeDeletePolicy(ctx, domainName, policyName, auditRef, caller);\n    }\n\n    boolean matchDelegatedTrustAssertion(Assertion assertion, String roleName, \n            String roleMember, List<Role> roles) {\n        \n        if (!ZMSUtils.assumeRoleResourceMatch(roleName, assertion)) {\n            return false;\n        }\n        \n        String rolePattern = StringUtils.patternFromGlob(assertion.getRole());\n        for (Role role : roles) {\n            String name = role.getName();\n            if (!name.matches(rolePattern)) {\n                continue;\n            }\n            \n            if (isMemberOfRole(role, roleMember)) {\n                return true;\n            }\n        }\n        \n        return false;\n    }\n    \n    boolean matchDelegatedTrustPolicy(Policy policy, String roleName, String roleMember, List<Role> roles) {\n        \n        List<Assertion> assertions = policy.getAssertions();\n        if (assertions == null) {\n            return false;\n        }\n        \n        for (Assertion assertion : assertions) {\n            if (matchDelegatedTrustAssertion(assertion, roleName, roleMember, roles)) {\n                return true;\n            }\n        }\n        \n        return false;\n    }\n    \n    boolean delegatedTrust(String domainName, String roleName, String roleMember) {\n        \n        AthenzDomain domain = getAthenzDomain(domainName, true);\n        if (domain == null) {\n            return false;\n        }\n        \n        for (Policy policy : domain.getPolicies()) {\n            if (matchDelegatedTrustPolicy(policy, roleName, roleMember, domain.getRoles())) {\n                return true;\n            }\n        }\n        \n        return false;\n    }\n\n    boolean matchRole(String domain, List<Role> roles, String rolePattern, List<String> authenticatedRoles) {\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"matchRole domain: {} rolePattern: {}\", domain, rolePattern);\n        }\n        \n        String prefix = domain + AuthorityConsts.ROLE_SEP;\n        int prefixLen = prefix.length();\n        for (Role role : roles) {\n            final String name = role.getName();\n            if (!name.matches(rolePattern)) {\n                continue;\n            }\n\n            // depending if the authority we either have the full role name\n            // or only the short name (we have verified the prefix already)\n            // so we're going to check both\n\n            if (authenticatedRoles.contains(name) || authenticatedRoles.contains(name.substring(prefixLen))) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    boolean shouldRunDelegatedTrustCheck(String trust, String trustDomain) {\n        \n        // if no trust field field then no delegated trust check\n        \n        if (trust == null) {\n            return false;\n        }\n        \n        // if no specific trust domain specifies then we need\n        // run the delegated trust check for this domain\n        \n        if (trustDomain == null) {\n            return true;\n        }\n        \n        // otherwise we'll run the delegated trust check only if\n        // domain name matches\n        \n        return trust.equalsIgnoreCase(trustDomain);\n    }\n    \n    boolean matchPrincipalInRole(Role role, String roleName, String fullUser, String trustDomain) {\n        \n        // if we have members in the role then we're going to check\n        // against that list only\n        \n        if (role.getRoleMembers() != null) {\n            return isMemberOfRole(role, fullUser);\n        }\n        \n        // no members so let's check if this is a trust domain\n        \n        String trust = role.getTrust();\n        if (!shouldRunDelegatedTrustCheck(trust, trustDomain)) {\n            return false;\n        }\n\n        // delegate to another domain.\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"matchPrincipal: [delegated trust. Checking with: \" + trust + \"]\");\n        }\n        \n        return delegatedTrust(trust, roleName, fullUser);\n    }\n    \n    boolean matchPrincipal(List<Role> roles, String rolePattern, String fullUser, String trustDomain) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"matchPrincipal - rolePattern: {} user: {} trust: {}\", rolePattern, fullUser, trustDomain);\n        }\n\n        for (Role role : roles) {\n            \n            String name = role.getName();\n            if (!name.matches(rolePattern)) {\n                continue;\n            }\n            \n            if (matchPrincipalInRole(role, name, fullUser, trustDomain)) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"assertionMatch: -> OK (by principal)\");\n                }\n                return true;\n            }\n        }\n        return false;\n    }\n\n    AthenzDomain virtualHomeDomain(Principal principal, String domainName) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"homeDomain: home domain detected. Create on the fly.\");\n        }\n        \n        AthenzDomain athenzDomain = new AthenzDomain(domainName);\n        \n        Domain domain = new Domain().setName(domainName).setEnabled(Boolean.TRUE);\n        athenzDomain.setDomain(domain);\n        \n        List<String> adminUsers = new ArrayList<>();\n        adminUsers.add(principal.getFullName());\n        \n        Role role = ZMSUtils.makeAdminRole(domainName, adminUsers);\n        athenzDomain.getRoles().add(role);\n        \n        Policy policy = ZMSUtils.makeAdminPolicy(domainName, role);\n        athenzDomain.getPolicies().add(policy);\n        \n        return athenzDomain;\n    }\n    \n    boolean assertionMatch(Assertion assertion, String identity, String action, String resource,\n            String domain, List<Role> roles, List<String> authenticatedRoles, String trustDomain) {\n        \n        String actionPattern = StringUtils.patternFromGlob(assertion.getAction());\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"assertionMatch: action '{}' pattern '{}'\", action, actionPattern);\n        }\n        if (!action.matches(actionPattern)) {\n            return false;\n        }\n        \n        String rezPattern = StringUtils.patternFromGlob(assertion.getResource());\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"assertionMatch: resource '{}' pattern '{}'\", resource, rezPattern);\n        }\n        if (!resource.matches(rezPattern)) {\n            return false;\n        }\n        \n        boolean matchResult;\n        String rolePattern = StringUtils.patternFromGlob(assertion.getRole());\n        if (authenticatedRoles != null) {\n            matchResult = matchRole(domain, roles, rolePattern, authenticatedRoles);\n        } else {\n            matchResult = matchPrincipal(roles, rolePattern, identity, trustDomain);\n        }\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"assertionMatch: -> {} (effect: {})\", matchResult, assertion.getEffect());\n        }\n\n        return matchResult;\n    }\n    \n    boolean verifyProviderEndpoint(String providerEndpoint) {\n        \n        // verify that we have a valid endpoint that ends in one of our\n        // configured domains. if it's not present or an empty value then\n        // there is no field to verify\n        \n        if (providerEndpoint == null) {\n            return true;\n        }\n        \n        if (providerEndpoint.isEmpty()) {\n            return true;\n        }\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"verifyProviderEndpoint: verifying endpoint: \" + providerEndpoint);\n        }\n        \n        java.net.URI uri;\n        try {\n            uri = new java.net.URI(providerEndpoint);\n        } catch (URISyntaxException ex) {\n            return false;\n        }\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"verifyProviderEndpoint: host: \" + uri.getHost() + \" scheme: \" + uri.getScheme());\n        }\n        \n        String scheme = uri.getScheme();\n        if (scheme == null) {\n            return false;\n        }\n        \n        scheme = scheme.toLowerCase();\n        \n        // if our scheme is class then we have no further checks to carry\n        \n        if (scheme.equalsIgnoreCase(ZMSConsts.SCHEME_CLASS)) {\n            return true;\n        }\n        \n        // otherwise it must be one of our http schemes\n\n        if (!(scheme.equalsIgnoreCase(ZMSConsts.SCHEME_HTTP) || scheme.equalsIgnoreCase(ZMSConsts.SCHEME_HTTPS))) {\n            return false;\n        }\n        \n        String host = uri.getHost();\n        if (host == null) {\n            return false;\n        }\n        host = host.toLowerCase();\n        \n        // if we have no endpoint configured then we should\n        // allow all hostnames\n        \n        if (providerEndpoints == null || providerEndpoints.isEmpty()) {\n            return true;\n        }\n        \n        // we're going to allow localhost as a special case since\n        // that's often used for dev testing\n        \n        boolean valid = host.equals(ZMSConsts.LOCALHOST);\n        if (!valid) {\n            for (String endpoint : providerEndpoints) {\n                valid = host.endsWith(endpoint);\n                if (valid) {\n                    break;\n                }\n            }\n        }\n\n        return valid;\n    }\n    \n    boolean verifyServicePublicKey(String key) {\n        try {\n            PublicKey pub = Crypto.loadPublicKey(Crypto.ybase64DecodeString(key));\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"verifyServicePublicKey: public key looks valid: \" + pub);\n            }\n        } catch (Exception ex) {\n            LOG.error(\"verifyServicePublicKey: Invalid Public Key: \" + ex.getMessage());\n            return false;\n        }\n        return true;\n    }\n    \n    boolean verifyServicePublicKeys(ServiceIdentity service) {\n\n        // verify that the public keys specified are valid\n        // It's okay to not specify any public keys\n\n        List<PublicKeyEntry> publicKeyList = service.getPublicKeys();\n        if (publicKeyList == null || publicKeyList.size() == 0) {\n            return true;\n        }\n\n        for (PublicKeyEntry entry : publicKeyList) {\n            if (!verifyServicePublicKey(entry.getKey())) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    public boolean isValidServiceName(final String serviceName) {\n\n        if (reservedServiceNames != null && reservedServiceNames.contains(serviceName)) {\n            return false;\n        }\n\n        return serviceNameMinLength <= 0 || serviceNameMinLength <= serviceName.length();\n    }\n\n    @Override\n    public void putServiceIdentity(ResourceContext ctx, String domainName, String serviceName,\n                                   String auditRef, ServiceIdentity service) {\n\n        final String caller = \"putserviceidentity\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n        validate(service, TYPE_SERVICE_IDENTITY, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n        AthenzObject.SERVICE_IDENTITY.convertToLowerCase(service);\n\n        // validate that the service name is valid\n\n        if (!isValidServiceName(serviceName)) {\n            throw ZMSUtils.requestError(\"putServiceIdentity: Invalid/Reserved service name\", caller);\n        }\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        if (!ZMSUtils.serviceResourceName(domainName, serviceName).equals(service.getName())) {\n            throw ZMSUtils.requestError(\"putServiceIdentity: Inconsistent service/domain names\", caller);\n        }\n        \n        if (!verifyServicePublicKeys(service)) {\n            throw ZMSUtils.requestError(\"putServiceIdentity: Provided public key is invalid\", caller);\n        }\n\n        if (!verifyProviderEndpoint(service.getProviderEndpoint())) {\n            throw ZMSUtils.requestError(\"putServiceIdentity: Invalid endpoint: \"\n                + service.getProviderEndpoint() + \" - must be http(s) and in configured domain\", caller);\n        }\n        \n        dbService.executePutServiceIdentity(ctx, domainName, serviceName, service, auditRef, caller);\n    }\n\n    @Override\n    public void putServiceIdentitySystemMeta(ResourceContext ctx, String domainName, String serviceName,\n             String attribute, String auditRef, ServiceIdentitySystemMeta meta) {\n\n        final String caller = \"putservicesystemmeta\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n        validate(meta, TYPE_SERVICE_IDENTITY_SYSTEM_META, caller);\n        validate(attribute, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n        attribute = attribute.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putServiceSystemMeta: name={}, service={} attribute={}, meta={}\",\n                    domainName, serviceName, attribute, meta);\n        }\n\n        // if we are resetting the configured value then the caller\n        // must also have a delete action available for the same resource\n\n        boolean deleteAllowed = isAllowedSystemMetaDelete(principal, domainName, attribute, \"service\");\n\n        dbService.executePutServiceIdentitySystemMeta(ctx, domainName, serviceName, meta, attribute,\n                deleteAllowed, auditRef, caller);\n    }\n\n    public ServiceIdentity getServiceIdentity(ResourceContext ctx, String domainName, String serviceName) {\n\n        final String caller = \"getserviceidentity\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n\n        ServiceIdentity service = dbService.getServiceIdentity(domainName, serviceName, false);\n        if (service == null) {\n            throw ZMSUtils.notFoundError(\"getServiceIdentity: Service not found: '\" +\n                    ZMSUtils.serviceResourceName(domainName, serviceName) + \"'\", caller);\n        }\n        \n        return service;\n    }\n    \n    public void deleteServiceIdentity(ResourceContext ctx, String domainName,\n            String serviceName, String auditRef) {\n\n        final String caller = \"deleteserviceidentity\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        dbService.executeDeleteServiceIdentity(ctx, domainName, serviceName, auditRef, caller);\n    }\n\n    List<ServiceIdentity> setupServiceIdentityList(AthenzDomain domain, Boolean publicKeys, Boolean hosts) {\n        \n        // if we're asked to return the public keys and hosts as well then we\n        // just need to return the data as is without any modifications\n        \n        List<ServiceIdentity> services;\n        if (publicKeys == Boolean.TRUE && hosts == Boolean.TRUE) {\n            services = domain.getServices();\n        } else {\n            services = new ArrayList<>();\n            for (ServiceIdentity service : domain.getServices()) {\n                ServiceIdentity newService = new ServiceIdentity()\n                        .setName(service.getName())\n                        .setModified(service.getModified())\n                        .setExecutable(service.getExecutable())\n                        .setGroup(service.getGroup())\n                        .setUser(service.getUser())\n                        .setProviderEndpoint(service.getProviderEndpoint());\n                if (publicKeys == Boolean.TRUE) {\n                    newService.setPublicKeys(service.getPublicKeys());\n                } else if (hosts == Boolean.TRUE) {\n                    newService.setHosts(service.getHosts());\n                }\n                services.add(newService);\n            }\n        }\n        \n        return services;\n    }\n    \n    public ServiceIdentities getServiceIdentities(ResourceContext ctx, String domainName,\n            Boolean publicKeys, Boolean hosts) {\n\n        final String caller = \"getserviceidentities\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        \n        ServiceIdentities result = new ServiceIdentities();\n        \n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"getServiceIdentities: Domain not found: '\"\n                    + domainName + \"'\", caller);\n        }\n\n        result.setList(setupServiceIdentityList(domain, publicKeys, hosts));\n        return result;\n    }\n    \n    public ServiceIdentityList getServiceIdentityList(ResourceContext ctx, String domainName,\n            Integer limit, String skip) {\n\n        final String caller = \"getserviceidentitylist\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        if (skip != null) {\n            skip = skip.toLowerCase();\n        }\n        \n        List<String> names = new ArrayList<>();\n        String next = processListRequest(domainName, AthenzObject.SERVICE_IDENTITY, limit, skip, names);\n        ServiceIdentityList result = new ServiceIdentityList().setNames(names);\n        if (next != null) {\n            result.setNext(next);\n        }\n\n        return result;\n    }\n\n    public PublicKeyEntry getPublicKeyEntry(ResourceContext ctx, String domainName, String serviceName, String keyId) {\n\n        final String caller = \"getpublickeyentry\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n        keyId = keyId.toLowerCase();\n        \n        PublicKeyEntry entry = dbService.getServicePublicKeyEntry(domainName, serviceName, keyId, false);\n        if (entry == null) {\n            throw ZMSUtils.notFoundError(\"getPublicKeyEntry: PublicKey \" + keyId + \" in service \" +\n                    ZMSUtils.serviceResourceName(domainName, serviceName) + \" not found\", caller);\n        }\n        \n        return entry;\n    }\n\n    public void deletePublicKeyEntry(ResourceContext ctx, String domainName, String serviceName,\n            String keyId, String auditRef) {\n\n        final String caller = \"deletepublickeyentry\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n        \n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n        keyId = keyId.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        dbService.executeDeletePublicKeyEntry(ctx, domainName, serviceName, keyId, auditRef, caller);\n    }\n\n    @Override\n    public void putPublicKeyEntry(ResourceContext ctx, String domainName, String serviceName,\n            String keyId, String auditRef, PublicKeyEntry keyEntry) {\n\n        final String caller = \"putpublickeyentry\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(serviceName, TYPE_SIMPLE_NAME, caller);\n        validate(keyEntry, TYPE_PUBLIC_KEY_ENTRY, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        serviceName = serviceName.toLowerCase();\n        keyId = keyId.toLowerCase();\n        AthenzObject.PUBLIC_KEY_ENTRY.convertToLowerCase(keyEntry);\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        // verify that key id specified in request and object do match\n        \n        if (!keyId.equals(keyEntry.getId())) {\n            throw ZMSUtils.requestError(\"putPublicKeyEntry: keyId in URI and PublicKeyEntry object do not match\", caller);\n        }\n        \n        // verify we have a valid public key specified\n        \n        if (!verifyServicePublicKey(keyEntry.getKey())) {\n            throw ZMSUtils.requestError(\"putPublicKeyEntry: Invalid public key\", caller);\n        }\n        \n        dbService.executePutPublicKeyEntry(ctx, domainName, serviceName, keyEntry, auditRef, caller);\n    }\n\n    String removeQuotes(String value) {\n        if (value.startsWith(\"\\\"\")) {\n            value = value.substring(1);\n        }\n        if (value.endsWith(\"\\\"\")) {\n            value = value.substring(0, value.length() - 1);\n        }\n        return value;\n    }\n    \n    long getModTimestamp(String matchingTag) {\n        \n        long timestamp = 0;\n        if (matchingTag == null) {\n            return timestamp;\n        }\n\n        matchingTag = removeQuotes(matchingTag);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getModTimestamp: matching tag ({})\", matchingTag);\n        }\n\n        if (matchingTag.isEmpty()) {\n            return timestamp;\n        }\n\n        try {\n            Timestamp tagStamp = Timestamp.fromString(matchingTag);\n            if (tagStamp == null) {\n                throw new IllegalArgumentException(\"Timestamp failed\");\n            }\n            timestamp = tagStamp.millis();\n        } catch (IllegalArgumentException exc) {\n            if (LOG.isWarnEnabled()) {\n                LOG.warn(\"getModTimestamp: matching tag({}) has bad format. Return 0 by default.\",\n                        matchingTag);\n            }\n        }\n        \n        return timestamp;\n    }\n\n    SignedDomain createSignedDomain(String domainName, long modifiedTime) {\n        SignedDomain signedDomain = new SignedDomain();\n        DomainData domainData = new DomainData().setName(domainName);\n        signedDomain.setDomain(domainData);\n        domainData.setModified(Timestamp.fromMillis(modifiedTime));\n        return signedDomain;\n    }\n\n    SignedDomain retrieveSignedDomainMeta(final Domain domain, final String metaAttr) {\n\n        SignedDomain signedDomain = createSignedDomain(domain.getName(), domain.getModified().millis());\n        if (metaAttr != null) {\n            switch (metaAttr) {\n                case META_ATTR_ACCOUNT:\n                    final String account = domain.getAccount();\n                    if (account == null) {\n                        return null;\n                    }\n                    signedDomain.getDomain().setAccount(account);\n                    break;\n                case META_ATTR_YPM_ID:\n                    final Integer ypmId = domain.getYpmId();\n                    if (ypmId == null || ypmId == 0) {\n                        return null;\n                    }\n                    signedDomain.getDomain().setYpmId(ypmId);\n                    break;\n                case META_ATTR_ALL:\n                    DomainData domainData = signedDomain.getDomain();\n                    domainData.setDescription(domain.getDescription());\n                    domainData.setAccount(domain.getAccount());\n                    domainData.setYpmId(domain.getYpmId());\n                    domainData.setApplicationId(domain.getApplicationId());\n                    domainData.setMemberExpiryDays(domain.getMemberExpiryDays());\n                    domainData.setServiceExpiryDays(domain.getServiceExpiryDays());\n                    domainData.setRoleCertExpiryMins(domain.getRoleCertExpiryMins());\n                    domainData.setServiceCertExpiryMins(domain.getServiceCertExpiryMins());\n                    domainData.setTokenExpiryMins(domain.getTokenExpiryMins());\n                    domainData.setOrg(domain.getOrg());\n                    domainData.setAuditEnabled(domain.getAuditEnabled());\n                    break;\n            }\n        }\n        return signedDomain;\n    }\n\n    SignedDomain retrieveSignedDomain(Domain domain, final String metaAttr, boolean setMetaDataOnly, boolean masterCopy) {\n\n        // check if we're asked to only return the meta data which\n        // we already have - name and last modified time, so we can\n        // add the domain to our return list and continue with the\n        // next domain\n\n        SignedDomain signedDomain;\n        if (setMetaDataOnly) {\n            signedDomain = retrieveSignedDomainMeta(domain, metaAttr);\n        } else {\n            signedDomain = retrieveSignedDomainData(domain.getName(), domain.getModified().millis(), masterCopy);\n        }\n        return signedDomain;\n    }\n\n    SignedDomain retrieveSignedDomainData(final String domainName, long modifiedTime, boolean masterCopy) {\n\n        // generate our signed domain object\n\n        SignedDomain signedDomain = createSignedDomain(domainName, modifiedTime);\n\n        // get the policies, roles, and service identities to create the\n        // DomainData\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"retrieveSignedDomain: retrieving domain \" + domainName);\n        }\n        \n        AthenzDomain athenzDomain = getAthenzDomain(domainName, true, masterCopy);\n        \n        // it's possible that our domain was deleted by another\n        // thread while we were processing this request so\n        // we'll return null so the caller can skip this domain\n        \n        if (athenzDomain == null) {\n            return null;\n        }\n\n        // set domain attributes - for enabled flag only set it\n        // if it set to false\n\n        DomainData domainData = signedDomain.getDomain();\n\n        if (athenzDomain.getDomain().getEnabled() == Boolean.FALSE) {\n            domainData.setEnabled(false);\n        }\n        if (athenzDomain.getDomain().getAuditEnabled() == Boolean.TRUE) {\n            domainData.setAuditEnabled(true);\n        }\n        domainData.setAccount(athenzDomain.getDomain().getAccount());\n        domainData.setYpmId(athenzDomain.getDomain().getYpmId());\n        domainData.setApplicationId(athenzDomain.getDomain().getApplicationId());\n        domainData.setSignAlgorithm(athenzDomain.getDomain().getSignAlgorithm());\n        if (athenzDomain.getDomain().getServiceCertExpiryMins() != null) {\n            domainData.setServiceCertExpiryMins(athenzDomain.getDomain().getServiceCertExpiryMins());\n        }\n        if (athenzDomain.getDomain().getRoleCertExpiryMins() != null) {\n            domainData.setRoleCertExpiryMins(athenzDomain.getDomain().getRoleCertExpiryMins());\n        }\n        if (athenzDomain.getDomain().getTokenExpiryMins() != null) {\n            domainData.setTokenExpiryMins(athenzDomain.getDomain().getTokenExpiryMins());\n        }\n\n        // set the roles and services\n\n        domainData.setRoles(athenzDomain.getRoles());\n        domainData.setServices(athenzDomain.getServices());\n\n        // generate the domain policy object that includes the domain\n        // name and all policies. Then we'll sign this struct using\n        // server's private key to get signed policy object\n        \n        DomainPolicies domainPolicies = new DomainPolicies().setDomain(domainName);\n        domainPolicies.setPolicies(getPolicyListWithoutAssertionId(athenzDomain.getPolicies()));\n        SignedPolicies signedPolicies = new SignedPolicies();\n        signedPolicies.setContents(domainPolicies);\n        domainData.setPolicies(signedPolicies);\n\n        String signature = Crypto.sign(\n                SignUtils.asCanonicalString(signedPolicies.getContents()), privateKey.getKey());\n        signedPolicies.setSignature(signature).setKeyId(privateKey.getId());\n\n        // then sign the data and set the data and signature in a SignedDomain\n        \n        signature = Crypto.sign(SignUtils.asCanonicalString(domainData), privateKey.getKey());\n        signedDomain.setSignature(signature).setKeyId(privateKey.getId());\n        return signedDomain;\n    }\n\n    @Override\n    public Response getSignedDomains(ResourceContext ctx, String domainName, String metaOnly,\n            String metaAttr, Boolean master, String matchingTag) {\n\n        final String caller = \"getsigneddomains\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        if (domainName != null) {\n            domainName = domainName.toLowerCase();\n            validate(domainName, TYPE_DOMAIN_NAME, caller);\n            setRequestDomain(ctx, domainName);\n        }\n        if (metaAttr != null) {\n            metaAttr = metaAttr.toLowerCase();\n            validate(metaAttr, TYPE_SIMPLE_NAME, caller);\n        }\n        \n        boolean setMetaDataOnly = ZMSUtils.parseBoolean(metaOnly, false);\n        long timestamp = getModTimestamp(matchingTag);\n        \n        // if this is one of our system principals then we're going to\n        // to use the master copy instead of read-only replicas\n        // unless we're configured to always use read-only replicas\n        // for all signed domain operations\n        \n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        boolean masterCopy = (useMasterCopyForSignedDomains || master == Boolean.TRUE)\n                && principal.getFullName().startsWith(\"sys.\");\n        \n        // if we're given a specific domain then we don't need to\n        // retrieve the list of modified domains\n        \n        List<SignedDomain> sdList = new ArrayList<>();\n        Long youngestDomMod = -1L;\n\n        if (domainName != null && !domainName.isEmpty()) {\n        \n            Domain domain = null;\n            try {\n                domain = dbService.getDomain(domainName, masterCopy);\n            } catch (ResourceException ex) {\n                \n                // in case the domain does not exist we're just\n                // going to return an empty set\n                \n                if (ex.getCode() != ResourceException.NOT_FOUND) {\n                    throw ex;\n                }\n            }\n\n            if (domain != null) {\n                youngestDomMod = domain.getModified().millis();\n\n                if (timestamp != 0 && youngestDomMod <= timestamp) {\n                    EntityTag eTag = new EntityTag(domain.getModified().toString());\n                    return Response.status(ResourceException.NOT_MODIFIED)\n                            .header(\"ETag\", eTag.toString()).build();\n                }\n                \n                // generate our signed domain object\n                \n                SignedDomain signedDomain = retrieveSignedDomain(domain, metaAttr, setMetaDataOnly, masterCopy);\n                \n                if (signedDomain != null) {\n                    sdList.add(signedDomain);\n                }\n            } else {\n                youngestDomMod = System.currentTimeMillis();\n            }\n            \n        } else {\n\n            // if we don't have a domain name then the meta flag must\n            // be set to true otherwise it's expensive to fetch all\n            // domains and sign all domains into a single response\n            // unless the request is from a system service\n\n            if (!setMetaDataOnly && !masterCopy)  {\n                return Response.status(ResourceException.BAD_REQUEST).build();\n            }\n\n            // we should get our matching tag before calling get modified list\n            // in case we get a domain added/updated right after an empty domain list\n            // was returned and before the matchingTag was set to a value\n            \n            if (matchingTag == null) {\n                EntityTag eTag = new EntityTag(Timestamp.fromMillis(0).toString());\n                matchingTag = eTag.toString();\n            }\n            \n            DomainMetaList dmlist = dbService.listModifiedDomains(timestamp);\n            List<Domain> modlist = dmlist.getDomains();\n            if (modlist == null || modlist.size() == 0) {\n                return Response.status(ResourceException.NOT_MODIFIED)\n                        .header(\"ETag\", matchingTag).build();\n            }\n            \n            // now we can iterate through our list and retrieve each domain\n\n            for (Domain dmod : modlist) {\n                \n                Long domModMillis = dmod.getModified().millis();\n                if (domModMillis.compareTo(youngestDomMod) > 0) {\n                    youngestDomMod = domModMillis;\n                }\n                \n                // generate our signed domain object\n                \n                SignedDomain signedDomain = retrieveSignedDomain(dmod, metaAttr, setMetaDataOnly, masterCopy);\n                \n                // it's possible that our domain was deleted by another\n                // thread while we were processing this request so\n                // if we get a null object, we'll just skip this\n                // item and continue with the next one\n                \n                if (signedDomain == null) {\n                    continue;\n                }\n                \n                // we have a valid domain so we'll add it to our return list\n                \n                sdList.add(signedDomain);\n            }\n        }\n\n        SignedDomains sdoms = new SignedDomains();\n        sdoms.setDomains(sdList);\n\n        Timestamp youngest = Timestamp.fromMillis(youngestDomMod);\n        EntityTag eTag = new EntityTag(youngest.toString());\n\n        return Response.status(ResourceException.OK).entity(sdoms)\n                .header(\"ETag\", eTag.toString()).build();\n    }\n\n    @Override\n    public JWSDomain getJWSDomain(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getjwsdomain\";\n\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        // generate our signed domain object\n\n        JWSDomain jwsDomain = retrieveJWSDomain(domainName);\n        if (jwsDomain == null) {\n            throw ZMSUtils.notFoundError(\"Unable to retrieve domain=\" + domainName, caller);\n        }\n\n        return jwsDomain;\n    }\n\n    JWSDomain retrieveJWSDomain(final String domainName) {\n\n        // get the policies, roles, and service identities to create the\n        // DomainData\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"retrieveJWSDomain: retrieving domain {}\", domainName);\n        }\n\n        AthenzDomain athenzDomain = getAthenzDomain(domainName, true, false);\n        if (athenzDomain == null) {\n            return null;\n        }\n\n        // set all domain attributes including roles and services\n\n        final Domain domain = athenzDomain.getDomain();\n\n        DomainData domainData = new DomainData()\n                .setName(domainName)\n                .setModified(domain.getModified())\n                .setEnabled(domain.getEnabled())\n                .setAuditEnabled(domain.getAuditEnabled())\n                .setAccount(domain.getAccount())\n                .setYpmId(domain.getYpmId())\n                .setApplicationId(domain.getApplicationId())\n                .setSignAlgorithm(domain.getSignAlgorithm())\n                .setServiceCertExpiryMins(domain.getServiceCertExpiryMins())\n                .setRoleCertExpiryMins(domain.getRoleCertExpiryMins())\n                .setTokenExpiryMins(domain.getTokenExpiryMins())\n                .setServiceExpiryDays(domain.getServiceExpiryDays())\n                .setDescription(domain.getDescription())\n                .setOrg(domain.getOrg())\n                .setCertDnsDomain(domain.getCertDnsDomain())\n                .setMemberExpiryDays(domain.getMemberExpiryDays())\n                .setRoles(athenzDomain.getRoles())\n                .setServices(athenzDomain.getServices());\n\n        // generate the domain policy object that includes the domain\n        // name and all policies.\n\n        DomainPolicies domainPolicies = new DomainPolicies().setDomain(domainName);\n        domainPolicies.setPolicies(getPolicyListWithoutAssertionId(athenzDomain.getPolicies()));\n        SignedPolicies signedPolicies = new SignedPolicies();\n        signedPolicies.setContents(domainPolicies);\n        domainData.setPolicies(signedPolicies);\n\n        return signJwsDomain(domainData);\n    }\n\n    JWSDomain signJwsDomain(DomainData domainData) {\n\n        // https://tools.ietf.org/html/rfc7515#section-7.2.2\n        // first generate the json output of our object\n\n        JWSDomain jwsDomain = null;\n        try {\n            // spec requires base64 url encoder without any padding\n\n            final Base64.Encoder encoder = Base64.getUrlEncoder().withoutPadding();\n\n            // generate our domain data payload and encode it\n\n            final byte[] jsonDomain = jsonMapper.writeValueAsBytes(domainData);\n            final byte[] encodedDomain = encoder.encode(jsonDomain);\n\n            // generate our protected header - just includes the algorithm\n\n            final String protectedHeader = \"{\\\"alg\\\":\\\"\" + privateKey.getAlgorithm() + \"\\\"}\";\n            final byte[] encodedHeader = encoder.encode(protectedHeader.getBytes(StandardCharsets.UTF_8));\n\n            // combine protectedheader . payload and sign the result\n\n            final byte[] signature = encoder.encode(Crypto.sign(\n                    Bytes.concat(encodedHeader, PERIOD, encodedDomain), privateKey.getKey(), Crypto.SHA256));\n\n            // our header contains a single entry with the keyid\n\n            final Map<String, String> headerMap = new HashMap<>();\n            headerMap.put(\"keyid\", privateKey.getId());\n\n            jwsDomain = new JWSDomain().setHeader(headerMap)\n                    .setPayload(new String(encodedDomain))\n                    .setProtectedHeader(new String(encodedHeader))\n                    .setSignature(new String(signature));\n\n        } catch (Exception ex) {\n            LOG.error(\"Unable to generate signed athenz domain object\", ex);\n        }\n        return jwsDomain;\n    }\n\n    List<Policy> getPolicyListWithoutAssertionId(List<Policy> policies) {\n        \n        if (policies == null) {\n            return null;\n        }\n        \n        // we are going to remove the assertion id from our assertions\n        // since the data is signed and the clients don't need to be\n        // updated due to this new attribute being returned\n        \n        List<Policy> policyList = new ArrayList<>();\n\n        for (Policy policy : policies) {\n            Policy newPolicy = new Policy()\n                    .setModified(policy.getModified())\n                    .setName(policy.getName());\n            if (policy.getAssertions() != null) {\n                List<Assertion> assertions = new ArrayList<>();\n                for (Assertion assertion : policy.getAssertions()) {\n                    Assertion newAssertion = new Assertion()\n                            .setAction(assertion.getAction())\n                            .setResource(assertion.getResource())\n                            .setRole(assertion.getRole());\n                    if (assertion.getEffect() != null) {\n                        newAssertion.setEffect(assertion.getEffect());\n                    } else {\n                        newAssertion.setEffect(AssertionEffect.ALLOW);\n                    }\n                    assertions.add(newAssertion);\n                }\n                newPolicy.setAssertions(assertions);\n            }\n            policyList.add(newPolicy);\n        }\n        return policyList;\n    }\n\n    boolean isValidUserTokenRequest(Principal principal, String userName) {\n        \n        if (principal == null) {\n            return false;\n        }\n\n        Authority authority = principal.getAuthority();\n        if (authority == null) {\n            return false;\n        }\n\n        // if authority allowed to carry out authorization checks there\n        // is no need to request user tokens\n        \n        if (authority.allowAuthorization()) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"User Token request - Authority cannot request user tokens\");\n            }\n            return false;\n        }\n        \n        String authDomain = authority.getDomain();\n        if (authDomain == null || !authDomain.equalsIgnoreCase(userDomain)) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"User Token request - not authenticated by User Authority\");\n            }\n            return false;\n        }\n\n        // if the username is not our pre-defined skip value we are going\n        // to verify that it matches to the principal's name\n        \n        if (userName.equalsIgnoreCase(USER_TOKEN_DEFAULT_NAME)) {\n            return true;\n        }\n        \n        if (!userName.equalsIgnoreCase(principal.getName())) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"User Token request - mismatch between request user name and userid\");\n            }\n            return false;\n        }\n        \n        return true;\n    }\n    \n    @Override\n    public UserToken getUserToken(ResourceContext ctx, String userName, String authorizedServices,\n            Boolean header) {\n\n        final String caller = \"getusertoken\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        userName = userName.toLowerCase();\n        \n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        if (!isValidUserTokenRequest(principal, userName)) {\n            throw ZMSUtils.unauthorizedError(\"getUserToken: Invalid request - missing User credentials or userName mismatch\", caller);\n        }\n\n        // if the user is requesting authorized services we need to verify that\n        // all the service names are valid\n        \n        List<String> services = null;\n        if (authorizedServices != null && !authorizedServices.isEmpty()) {\n            services = Arrays.asList(authorizedServices.split(\",\"));\n            for (String service : services) {\n                if (!serverAuthorizedServices.contains(service)) {\n                    throw ZMSUtils.unauthorizedError(\"getUserToken: Service \" + service + \" is not authorized in ZMS\", caller);\n                }\n            }\n        }\n        \n        PrincipalToken token = new PrincipalToken.Builder(\"U1\", userDomain, principal.getName())\n            .expirationWindow(userTokenTimeout).keyId(privateKey.getId()).host(serverHostName)\n            .ip(ServletRequestUtil.getRemoteAddress(ctx.request())).authorizedServices(services).build();\n        \n        token.sign(privateKey.getKey());\n        UserToken userToken = new UserToken().setToken(token.getSignedToken());\n        \n        if (header == Boolean.TRUE && principalAuthority != null) {\n            userToken.setHeader(principalAuthority.getHeader());\n        }\n        \n        // set our standard CORS headers in our response if we're processing\n        // a get user token for an authorized service\n        \n        if (services != null)  {\n            setStandardCORSHeaders(ctx);\n        }\n\n        return userToken;\n    }\n\n    public UserToken optionsUserToken(ResourceContext ctx, String userName, String authorizedServices) {\n\n        final String caller = \"optionsusertoken\";\n\n        validateRequest(ctx.request(), caller);\n\n        // if the user must be requesting authorized service token\n        \n        if (authorizedServices == null || authorizedServices.isEmpty()) {\n            throw ZMSUtils.requestError(\"optionsUserToken: No authorized services specified in the request\", caller);\n        }\n        \n        // verify that all specified services are valid\n        \n        String[] services = authorizedServices.split(\",\");\n        for (String service : services) {\n            if (!serverAuthorizedServices.contains(service)) {\n                throw ZMSUtils.requestError(\"optionsUserToken: Service \" + service + \" is not authorized in ZMS\", caller);\n            }\n        }\n        \n        // set our standard CORS headers in our response\n        \n        setStandardCORSHeaders(ctx);\n        \n        // since this is the preflight request we are going to report that\n        // we only allow GET method and configure the user-agent to cache\n        // this request results for up-to 30 days\n        \n        ctx.response().addHeader(ZMSConsts.HTTP_ACCESS_CONTROL_ALLOW_METHODS, ZMSConsts.HTTP_GET);\n        ctx.response().addHeader(ZMSConsts.HTTP_ACCESS_CONTROL_MAX_AGE, \"2592000\");\n        \n        return null;\n    }\n\n    boolean isValidCORSOrigin(final String origin) {\n\n        // first check for non-empty origin value\n\n        if (origin == null || origin.isEmpty()) {\n            return false;\n        }\n\n        // check if we have whitelist configured\n\n        if (corsOriginList == null || corsOriginList.isEmpty()) {\n            return true;\n        }\n\n        return corsOriginList.contains(origin);\n    }\n\n    void setStandardCORSHeaders(ResourceContext ctx) {\n\n        // if we get an Origin header in our request then we're going to return\n        // the same value in the Allow-Origin header\n        \n        String origin = ctx.request().getHeader(ZMSConsts.HTTP_ORIGIN);\n        if (isValidCORSOrigin(origin)) {\n            ctx.response().addHeader(ZMSConsts.HTTP_ACCESS_CONTROL_ALLOW_ORIGIN, origin);\n        }\n        \n        // we must allow credentials to be passed by the client\n        \n        ctx.response().addHeader(ZMSConsts.HTTP_ACCESS_CONTROL_ALLOW_CREDENTIALS, \"true\");\n        \n        // if the client is asking us to allow any headers then we're going\n        // to return that set back as allowed\n        \n        String allowHeaders = ctx.request().getHeader(ZMSConsts.HTTP_ACCESS_CONTROL_REQUEST_HEADERS);\n        if (allowHeaders != null && !allowHeaders.isEmpty()) {\n            ctx.response().addHeader(ZMSConsts.HTTP_ACCESS_CONTROL_ALLOW_HEADERS, allowHeaders);\n        }\n    }\n\n    String providerServiceDomain(String provider) {\n        int n = provider.lastIndexOf('.');\n        if (n <= 0 || n == provider.length() - 1) {\n            return null;\n        }\n        return provider.substring(0, n);\n    }\n    \n    String providerServiceName(String provider) {\n        int n = provider.lastIndexOf('.');\n        if (n <= 0 || n == provider.length() - 1) {\n            return null;\n        }\n        return provider.substring(n + 1);\n    }\n\n    @Override\n    public void putTenancy(ResourceContext ctx, String tenantDomain, String provider,\n            String auditRef, Tenancy detail) {\n\n        final String caller = \"puttenancy\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provider, TYPE_SERVICE_NAME, caller); //the fully qualified service name to provision on\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        tenantDomain = tenantDomain.toLowerCase();\n        setRequestDomain(ctx, tenantDomain);\n        provider = provider.toLowerCase();\n        AthenzObject.TENANCY.convertToLowerCase(detail);\n\n        // validate our detail object against uri components\n\n        if (!validateTenancyObject(detail, tenantDomain, provider)) {\n            throw ZMSUtils.requestError(\"Invalid tenancy object\", caller);\n        }\n\n        // verify that request is properly authenticated for this request\n        \n        String authorizedService = ((RsrcCtxWrapper) ctx).principal().getAuthorizedService();\n        verifyAuthorizedServiceOperation(authorizedService, caller);\n\n        String provSvcDomain = providerServiceDomain(provider); // provider service domain\n        String provSvcName = providerServiceName(provider); // provider service name\n\n        // we can't have the provider and tenant be in the same domain\n        // as we don't allow delegation of roles onto themselves\n\n        if (provSvcDomain.equals(tenantDomain)) {\n            throw ZMSUtils.requestError(\"Provider and tenant domains cannot be the same\", caller);\n        }\n\n        if (dbService.getServiceIdentity(provSvcDomain, provSvcName, true) == null) {\n            throw ZMSUtils.notFoundError(\"Unable to retrieve service=\" + provider, caller);\n        }\n\n        // we are going to allow the authorize service token owner to call\n        // put tenancy on its own service\n\n        boolean authzServiceTokenOperation = isAuthorizedProviderService(authorizedService,\n                provSvcDomain, provSvcName);\n\n        if (authorizedService != null && !authzServiceTokenOperation) {\n            throw ZMSUtils.requestError(\"Authorized service provider mismatch: \"\n                    + provider + \"/\" + authorizedService, caller);\n        }\n\n        // set up our tenant admin policy so provider can check admin's access\n        \n        dbService.setupTenantAdminPolicy(tenantDomain, provSvcDomain,\n                provSvcName, auditRef, caller);\n        \n        // if this is an authorized service token request then we're going to create\n        // the corresponding admin role in the provider domain since that's been\n        // authenticated already\n        \n        if (authzServiceTokenOperation) {\n            setupTenantAdminPolicyInProvider(ctx, provSvcDomain, provSvcName, tenantDomain,\n                    auditRef, caller);\n        }\n    }\n\n    @Override\n    public void deleteTenancy(ResourceContext ctx, String tenantDomain, String provider, String auditRef) {\n\n        final String caller = \"deletetenancy\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provider, TYPE_SERVICE_NAME, caller); // fully qualified provider's service name\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        tenantDomain = tenantDomain.toLowerCase();\n        setRequestDomain(ctx, tenantDomain);\n        provider = provider.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n        \n        String authorizedService = ((RsrcCtxWrapper) ctx).principal().getAuthorizedService();\n        verifyAuthorizedServiceOperation(authorizedService, caller);\n\n        // make sure we have a valid provider service\n        \n        String provSvcDomain = providerServiceDomain(provider);\n        String provSvcName   = providerServiceName(provider);\n\n        if (dbService.getServiceIdentity(provSvcDomain, provSvcName, true) == null) {\n            throw ZMSUtils.notFoundError(\"Unable to retrieve service: \" + provider, caller);\n        }\n\n        // we are going to allow the authorize service token owner to call\n        // delete tenancy on its own service without configuring a controller\n        // end point\n        \n        boolean authzServiceTokenOperation = isAuthorizedProviderService(authorizedService,\n            provSvcDomain, provSvcName);\n        \n        if (authzServiceTokenOperation) {\n            dbService.executeDeleteTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain, null,\n                auditRef, caller);\n        }\n\n        // now clean-up local domain roles and policies for this tenant\n        \n        dbService.executeDeleteTenancy(ctx, tenantDomain, provSvcDomain, provSvcName,\n                null, auditRef, caller);\n    }\n\n    @Override\n    public void putTenant(ResourceContext ctx, String providerDomain, String providerService,\n           String tenantDomain, String auditRef, Tenancy detail) {\n\n        final String caller = \"puttenant\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(providerDomain, TYPE_DOMAIN_NAME, caller);\n        validate(providerService, TYPE_SIMPLE_NAME, caller);\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        providerDomain = providerDomain.toLowerCase();\n        setRequestDomain(ctx, providerDomain);\n        providerService = providerService.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        AthenzObject.TENANCY.convertToLowerCase(detail);\n\n        // we can't have the provider and tenant be in the same domain\n        // as we don't allow delegation of roles onto themselves\n\n        if (providerDomain.equals(tenantDomain)) {\n            throw ZMSUtils.requestError(\"Provider and tenant domains cannot be the same\", caller);\n        }\n\n        // validate our detail object against uri components\n\n        if (!validateTenancyObject(detail, tenantDomain, providerDomain + \".\" + providerService)) {\n            throw ZMSUtils.requestError(\"Invalid tenancy object\", caller);\n        }\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        if (dbService.getServiceIdentity(providerDomain, providerService, true) == null) {\n            throw ZMSUtils.notFoundError(\"Unable to retrieve service=\" + providerService, caller);\n        }\n\n        setupTenantAdminPolicyInProvider(ctx, providerDomain, providerService, tenantDomain,\n                auditRef, caller);\n    }\n\n    @Override\n    public void deleteTenant(ResourceContext ctx, String providerDomain, String providerService,\n            String tenantDomain, String auditRef) {\n\n        final String caller = \"deletetenant\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(providerDomain, TYPE_DOMAIN_NAME, caller);\n        validate(providerService, TYPE_SIMPLE_NAME, caller);\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        providerDomain = providerDomain.toLowerCase();\n        setRequestDomain(ctx, providerDomain);\n        providerService = providerService.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        if (dbService.getServiceIdentity(providerDomain, providerService, true) == null) {\n            throw ZMSUtils.notFoundError(\"Unable to retrieve service=\" + providerService, caller);\n        }\n\n        dbService.executeDeleteTenantRoles(ctx, providerDomain, providerService, tenantDomain,\n                null, auditRef, caller);\n    }\n\n    boolean validateTenancyObject(Tenancy tenant, final String tenantDomain, final String providerService) {\n\n        if (!tenant.getDomain().equals(tenantDomain)) {\n            return false;\n        }\n        return tenant.getService().equals(providerService);\n    }\n\n    boolean validateTenantResourceGroupRolesObject(TenantResourceGroupRoles roles, final String providerDomain,\n            final String providerService, final String tenantDomain, final String resourceGroup) {\n\n        if (!providerDomain.equals(roles.getDomain())) {\n            return false;\n        }\n        if (!providerService.equals(roles.getService())) {\n            return false;\n        }\n        if (!tenantDomain.equals(roles.getTenant())) {\n            return false;\n        }\n        if (!resourceGroup.equals(roles.getResourceGroup())) {\n            return false;\n        }\n\n        // we must have at least one role in the object\n\n        List<TenantRoleAction> list = roles.getRoles();\n        return (list != null && list.size() > 0);\n    }\n\n    boolean validateProviderResourceGroupRolesObject(ProviderResourceGroupRoles roles, final String providerDomain,\n            final String providerService, final String tenantDomain, final String resourceGroup) {\n\n        if (!providerDomain.equals(roles.getDomain())) {\n            return false;\n        }\n        if (!providerService.equals(roles.getService())) {\n            return false;\n        }\n        if (!tenantDomain.equals(roles.getTenant())) {\n            return false;\n        }\n        if (!resourceGroup.equals(roles.getResourceGroup())) {\n            return false;\n        }\n\n        // we must have at least one role in the object\n\n        List<TenantRoleAction> list = roles.getRoles();\n        return (list != null && list.size() > 0);\n    }\n\n    // put the trust roles into provider domain\n    //\n    @Override\n    public TenantResourceGroupRoles putTenantResourceGroupRoles(ResourceContext ctx, String provSvcDomain,\n            String provSvcName, String tenantDomain, String resourceGroup, String auditRef,\n            TenantResourceGroupRoles detail) {\n\n        final String caller = \"puttenantresourcegrouproles\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller); //not including the domain, this is the domain's service\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(detail, TYPE_TENANT_RESOURCE_GROUP_ROLES, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n        AthenzObject.TENANT_RESOURCE_GROUP_ROLES.convertToLowerCase(detail);\n\n        // we can't have the provider and tenant be in the same domain\n        // as we don't allow delegation of roles onto themselves\n\n        if (provSvcDomain.equals(tenantDomain)) {\n            throw ZMSUtils.requestError(\"Provider and tenant domains cannot be the same\", caller);\n        }\n\n        // validate our detail object against uri components\n\n        if (!validateTenantResourceGroupRolesObject(detail, provSvcDomain, provSvcName, tenantDomain,\n                resourceGroup)) {\n            throw ZMSUtils.requestError(\"Invalid tenant resource group role object\", caller);\n        }\n        \n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        if (LOG.isInfoEnabled()) {\n            LOG.info(\"putTenantResourceGroupRoles: ==== putTenantRoles(domain=\" + provSvcDomain + \", service=\" +\n                provSvcName + \", tenant-domain=\" + tenantDomain + \", resource-group=\" + resourceGroup +\n                \", detail=\" + detail + \")\");\n        }\n\n        // first setup the domain as a tenant in the provider domain\n\n        setupTenantAdminPolicyInProvider(ctx, provSvcDomain, provSvcName, tenantDomain,\n                auditRef, caller);\n\n        // then setup the requested resource group roles\n\n        dbService.executePutTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain,\n                resourceGroup, detail.getRoles(), auditRef, caller);\n        return detail;\n    }\n\n    public DomainDataCheck getDomainDataCheck(ResourceContext ctx, String domainName) {\n\n        final String caller = \"getdomaindatacheck\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getDomainDataCheck: domain=\" + domainName);\n        }\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"getDomainDataCheck: Domain not found: '\" + domainName + \"'\", caller);\n        }\n\n        // build set of roles\n        // iterate them to look for trust roles - in case this is a provider domain\n        \n        Set<String> roleSet      = new HashSet<>();\n        Set<String> trustRoleSet = new HashSet<>();\n\n        // map per trust/tenant domain that contains the trust roles\n        \n        Map<String, Set<String>> trustRoleMap = new HashMap<>();\n        for (Role role : domain.getRoles()) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"getDomainDataCheck: processing role - \" + role.getName());\n            }\n            roleSet.add(role.getName());\n            String roleName = ZMSUtils.removeDomainPrefix(role.getName(), domainName, ROLE_PREFIX);\n            String trustDomain = role.getTrust();\n            if (trustDomain != null) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"trust role for domain: \" + trustDomain);\n                }\n                trustRoleSet.add(trustDomain);\n                Set<String> tset = trustRoleMap.computeIfAbsent(trustDomain, k -> new HashSet<>());\n                tset.add(roleName);\n            }\n        }\n\n        // look for dangling roles and policies\n        //\n        int assertionCount = 0;\n        int roleWildcardCount = 0;\n        Set<String> usedRoleSet = new HashSet<>(); // keep track of roles used by policies\n        Set<String> providerSet = new HashSet<>(); // keep track of providers from assume_role policies\n\n        // assume_role resources are placed into the set per provider service domain\n        \n        Map<String, Set<String>> svcRoleMap = new HashMap<>();\n        List<DanglingPolicy> danglingPolicies = new ArrayList<>();\n        List<Policy> policies = domain.getPolicies();\n        for (Policy policy : policies) {\n            String pname = ZMSUtils.removeDomainPrefix(policy.getName(), domainName, POLICY_PREFIX);\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"getDomainDataCheck: processing policy=\" + pname + \" in domain=\" + domainName);\n            }\n\n            List<Assertion> assertions = policy.getAssertions();\n            if (assertions == null) {\n                continue;\n            }\n            \n            for (Assertion assertion : assertions) {\n                assertionCount++;\n                if (ZMSConsts.ACTION_ASSUME_ROLE.equalsIgnoreCase(assertion.getAction())) {\n                    // get provider domain+service name and add to set of providers\n                    // Note there may be a resource appended - to be dealt with later\n                    // ex: testgetdomaindatacheck:policy.tenancy.testgetdomaindatacheckprovider.storage.reader\n                    // ex: testgetdomaindatacheck:policy.tenancy.testgetdomaindatacheckprovider.sub.storage.res_group.ravers.reader\n                    // index after \"tenancy.\" and index of last dot\n                    int index = pname.indexOf(\"tenancy.\");\n                    if (index == -1) {\n                        continue;\n                    }\n                    int lindex = pname.lastIndexOf('.');\n                    if (lindex == -1) {\n                        continue;\n                    }\n                    String provSvcDomain = pname.substring(index + \"tenancy.\".length(), lindex);\n                    providerSet.add(provSvcDomain);\n\n                    // lets collect the resource field that is name of role in provider\n                    // ex: testgetdomaindatacheckprovider.sub:role.storage.tenant.testgetdomaindatacheck.reader\n                    // ex: testgetdomaindatacheckprovider.sub:role.storage.tenant.testgetdomaindatacheck.res_group.ravers.reader\n                    String rsrc = assertion.getResource();\n                    Set<String> rset = svcRoleMap.computeIfAbsent(provSvcDomain, k -> new HashSet<>());\n                    rset.add(rsrc);\n                }\n\n                String roleName = assertion.getRole();\n\n                // check for wildcard role\n                if (roleName.lastIndexOf('*') != -1) {\n                    roleWildcardCount++;\n                    // make sure there is at least 1 role that can match\n                    // this wildcard - else its a dangling policy\n                    String rolePattern = StringUtils.patternFromGlob(roleName);\n                    boolean wildCardMatch = false;\n                    for (String role: roleSet) {\n                        if (role.matches(rolePattern)) {\n                            wildCardMatch = true;\n                            break;\n                        }\n                    }\n                    if (!wildCardMatch) { // dangling policy\n                        DanglingPolicy dp = new DanglingPolicy();\n                        // we need to remove the domain:role. and domain:policy prefixes\n                        // according to RDL definitions for role and policy names\n                        dp.setRoleName(ZMSUtils.removeDomainPrefix(roleName, domainName, ROLE_PREFIX));\n                        dp.setPolicyName(ZMSUtils.removeDomainPrefix(pname, domainName, POLICY_PREFIX));\n                        danglingPolicies.add(dp);\n                    }\n                } else if (roleSet.contains(roleName)) {\n                    usedRoleSet.add(roleName);\n                } else { // dangling policy\n                    DanglingPolicy dp = new DanglingPolicy();\n                    // we need to remove the domain:role. and domain:policy prefixes\n                    // according to RDL definitions for role and policy names\n                    dp.setRoleName(ZMSUtils.removeDomainPrefix(roleName, domainName, ROLE_PREFIX));\n                    dp.setPolicyName(ZMSUtils.removeDomainPrefix(pname, domainName, POLICY_PREFIX));\n                    danglingPolicies.add(dp);\n                }\n            }\n        }\n\n        DomainDataCheck ddc = new DomainDataCheck();\n        ddc.setPolicyCount(policies.size());\n        ddc.setAssertionCount(assertionCount);\n        ddc.setRoleWildCardCount(roleWildcardCount);\n        if (!danglingPolicies.isEmpty()) {\n            ddc.setDanglingPolicies(danglingPolicies);\n        }\n\n        if (roleSet.size() != usedRoleSet.size()) {\n            // oh oh, some roles are unused - need to subtract the usedRoleSet\n            // from roleSet - the leftovers are the unused roles\n            roleSet.removeAll(usedRoleSet);\n            // we need to remove the domain:role. prefix according to\n            // RDL definition for dangling role names\n            List<String> danglingRoleList = new ArrayList<>();\n            for (String roleName : roleSet) {\n                danglingRoleList.add(ZMSUtils.removeDomainPrefix(roleName, domainName, ROLE_PREFIX));\n            }\n            ddc.setDanglingRoles(danglingRoleList);\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getDomainDataCheck: domain=\" + domainName +\n                \" policy-count=\" + policies.size() + \" assertion-count=\" +\n                assertionCount + \" wildcard-count==\" + roleWildcardCount +\n                \" dangling-policies=\" + danglingPolicies.size() +\n                \" dangling-roles=\" + roleSet.size());\n        }\n\n        // Tenant Domain Check: does each provider fully support this tenant?\n        // collect Service names (domain.service) for domains that don't contain\n        // trust role\n        List<String> provsWithoutTrust = new ArrayList<>();\n        for (String provSvc : providerSet) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"getDomainDataCheck: domain=\" + domainName +\n                    \" provider-service=\" + provSvc);\n            }\n\n            // 2 cases to resolve, one with resource group, one without\n            // ex: iaas.stuff.storage.read\n            // ex: iaas.stuff.storage.res_group.my_resource_group.read\n            \n            int idx = provSvc.indexOf(\".res_group.\");\n            String provSvcDomain;\n            if (idx == -1) {\n                provSvcDomain = providerServiceDomain(provSvc);\n            } else {\n                provSvcDomain = providerServiceDomain(provSvc.substring(0, idx));\n            }\n            \n            AthenzDomain providerDomain = getAthenzDomain(provSvcDomain, true);\n            Set<String> rset = svcRoleMap.get(provSvc);\n            if (rset == null || rset.isEmpty() || providerDomain == null) {\n                provsWithoutTrust.add(provSvc);\n                continue;\n            }\n            \n            // find trust role in the provider that contains the tenant domain\n            int foundTrust = 0;\n            for (Role role : providerDomain.getRoles()) {\n                String trustDomain = role.getTrust();\n                if (trustDomain != null) {\n                    if (domainName.equals(trustDomain)) {\n                        // is this role a match for an assume role in the tenant\n                        // look for the role in the role set for this service\n                        if (rset.contains(role.getName())) {\n                            foundTrust++;\n                        }\n                    }\n                }\n            }\n            if (foundTrust != rset.size()) {\n                provsWithoutTrust.add(provSvc);\n            }\n        }\n        if (!provsWithoutTrust.isEmpty()) {\n            ddc.setProvidersWithoutTrust(provsWithoutTrust);\n        }\n\n        // Provider Domain Check: does each tenant have all the assume_role\n        // assertions to match each trust role.\n\n        // tenantsWithoutProv: names of Tenant domains that don't contain assume\n        // role assertions if this is a provider domain\n        List<String> tenantsWithoutProv = new ArrayList<>();\n\n        // tenantDomMap: optimize reading tenant domains once already read\n        // This is optimizing for Providers with lots of tenants.\n        Map<String, AthenzDomain> tenantDomMap = new HashMap<>();\n        for (String trustRole: trustRoleSet) {\n            \n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"getDomainDataCheck: processing trust role: \" + trustRole);\n            }\n            \n            AthenzDomain tenantDomain = tenantDomMap.get(trustRole);\n            if (tenantDomain == null) {\n                tenantDomain = getAthenzDomain(trustRole, true);\n                if (tenantDomain == null) {\n                    tenantsWithoutProv.add(trustRole);\n                    continue;\n                } else {\n                    tenantDomMap.put(trustRole, tenantDomain);\n                }\n            }\n\n            // Get set of providers trust roles for trust/tenant domain.\n            Set<String> tset = trustRoleMap.get(trustRole);\n            if (tset == null || tset.isEmpty()) {\n                tenantsWithoutProv.add(trustRole);\n                continue;\n            }\n\n            int foundProviderCnt = 0;\n\n            // Check for assume_role containing the provider in the tenantDomain\n            for (Policy policy : tenantDomain.getPolicies()) {\n                List<Assertion> assertions = policy.getAssertions();\n                if (assertions == null) {\n                    continue;\n                }\n                for (Assertion assertion : assertions) {\n                    if (ZMSConsts.ACTION_ASSUME_ROLE.equalsIgnoreCase(assertion.getAction())) {\n                        String rsrc = assertion.getResource();\n                        // If the provider domain contains a role that matches\n                        // the tenant domain resource - then the tenant is supported\n                        if (roleSet.contains(rsrc)) {\n                            // HAVE: an assume_role with resource pointing at the provider\n                            foundProviderCnt++;\n                        }\n                    }\n                }\n            }\n            if (foundProviderCnt < tset.size()) {\n                // didn't find all required matching provider trust-role to assume_role-resource pairs\n                tenantsWithoutProv.add(trustRole);\n            }\n        }\n        if (!tenantsWithoutProv.isEmpty()) {\n            ddc.setTenantsWithoutAssumeRole(tenantsWithoutProv);\n        }\n\n        return ddc;\n    }\n     \n    public void deleteProviderResourceGroupRoles(ResourceContext ctx, String tenantDomain,\n             String provSvcDomain, String provSvcName, String resourceGroup, String auditRef) {\n\n        final String caller = \"deleteproviderresourcegrouproles\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n        \n        validateRequest(ctx.request(), caller);\n\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller);\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n        \n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        // first clean-up local domain roles and policies for this tenant\n        \n        dbService.executeDeleteTenancy(ctx, tenantDomain, provSvcDomain, provSvcName,\n             resourceGroup, auditRef, caller);\n\n        // at this point the tenant side is complete. If the token was a chained\n        // token signed by the provider service then we're going to process the\n        // provider side as well thus complete the tenancy delete process\n        \n        String authorizedService = ((RsrcCtxWrapper) ctx).principal().getAuthorizedService();\n        if (isAuthorizedProviderService(authorizedService, provSvcDomain, provSvcName)) {\n         \n            dbService.executeDeleteTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain,\n                resourceGroup, auditRef, caller);\n        }\n    }\n\n    public ProviderResourceGroupRoles getProviderResourceGroupRoles(ResourceContext ctx, String tenantDomain,\n            String provSvcDomain, String provSvcName, String resourceGroup) {\n\n        final String caller = \"getproviderresourcegrouproles\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller);\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n\n        if (dbService.getDomain(tenantDomain, false) == null) {\n            throw ZMSUtils.notFoundError(\"No such domain: \" + tenantDomain, caller);\n        }\n\n        // look for this provider roles, ex: storage.tenant.sports.reader\n\n        String rolePrefix = ZMSUtils.getProviderResourceGroupRolePrefix(provSvcDomain, provSvcName, resourceGroup);\n        ProviderResourceGroupRoles provRoles = new ProviderResourceGroupRoles().setDomain(provSvcDomain)\n                .setService(provSvcName).setTenant(tenantDomain).setResourceGroup(resourceGroup);\n\n        List<TenantRoleAction> tralist = new ArrayList<>();\n\n        // find roles matching the prefix\n\n        List<String> rcollection = dbService.listRoles(tenantDomain);\n        for (String rname: rcollection) {\n\n            if (dbService.isTenantRolePrefixMatch(rname, rolePrefix, resourceGroup, null)) {\n\n                // for provider roles we don't have the action, that's\n                // for the provider domain only so we're just going\n                // to return the list of roles without any actions\n                // for the role name we must return the SimpleName\n                // part only so we'll remove the prefix section\n\n                TenantRoleAction tra = new TenantRoleAction()\n                        .setRole(rname.substring(rolePrefix.length()))\n                        .setAction(\"n/a\");\n                tralist.add(tra);\n            }\n        }\n        provRoles.setRoles(tralist);\n\n        return provRoles;\n    }\n     \n    boolean isAuthorizedProviderService(String authorizedService, String provSvcDomain,\n             String provSvcName) {\n        \n         // make sure we have a service provided and it matches to our provider\n         \n         if (authorizedService == null) {\n             return false;\n         }\n         \n         if (!authorizedService.equals(provSvcDomain + \".\" + provSvcName)) {\n             return false;\n         }\n         \n         // verify that provider service does indeed have access to provision\n         // its own tenants. the authorize statement for the putTenantRole\n         // command is defined in the RDL as:\n         // authorize (\"UPDATE\", \"{domain}:tenant.{service}\");\n\n         AthenzDomain domain = getAthenzDomain(provSvcDomain, true);\n         if (domain == null) {\n             return false;\n         }\n         \n         // evaluate our domain's roles and policies to see if access\n         // is allowed or not for the given operation and resource\n         \n         String resource = provSvcDomain + \":tenant.\" + provSvcName;\n         AccessStatus accessStatus = evaluateAccess(domain, authorizedService, \"update\",\n                 resource, null, null);\n\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n     \n    /**\n     * This sets up the assume roles in the tenant. If the tenants admin user\n     * token has been authorized by the provider, the providers domain will be\n     * updated as well, thus completing the tenancy on-boarding in a single step.\n    **/\n    @Override\n    public ProviderResourceGroupRoles putProviderResourceGroupRoles(ResourceContext ctx, String tenantDomain,\n             String provSvcDomain, String provSvcName, String resourceGroup, String auditRef,\n             ProviderResourceGroupRoles detail) {\n\n        final String caller = \"putproviderresourcegrouproles\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller); //not including the domain, this is the domain's service\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(detail, TYPE_PROVIDER_RESOURCE_GROUP_ROLES, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n        AthenzObject.PROVIDER_RESOURCE_GROUP_ROLES.convertToLowerCase(detail);\n\n        // we can't have the provider and tenant be in the same domain\n        // as we don't allow delegation of roles onto themselves\n\n        if (provSvcDomain.equals(tenantDomain)) {\n            throw ZMSUtils.requestError(\"Provider and tenant domains cannot be the same\", caller);\n        }\n\n        // validate our detail object against uri components\n\n        if (!validateProviderResourceGroupRolesObject(detail, provSvcDomain, provSvcName, tenantDomain,\n                resourceGroup)) {\n            throw ZMSUtils.requestError(\"Invalid provider resource group role object\", caller);\n        }\n        \n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        if (LOG.isInfoEnabled()) {\n            LOG.info(\"putProviderResourceGroupRoles: domain=\" + provSvcDomain + \", service=\" +\n                provSvcName + \", tenant-domain=\" + tenantDomain + \", resource-group=\" + resourceGroup +\n                \", detail=\" + detail);\n        }\n        \n        // set up our tenant admin policy so provider can check admin's access\n        \n        dbService.setupTenantAdminPolicy(tenantDomain, provSvcDomain, provSvcName, auditRef, caller);\n        \n        // now we're going to setup our roles\n        \n        List<TenantRoleAction> roleActions = detail.getRoles();\n        List<String> roles = new ArrayList<>();\n        for (TenantRoleAction roleAction : roleActions) {\n            roles.add(roleAction.getRole());\n        }\n        \n        // we're going to create a separate role for each one of tenant roles returned\n        // based on its action and set the caller as a member in each role\n        \n        dbService.executePutProviderRoles(ctx, tenantDomain, provSvcDomain, provSvcName, resourceGroup,\n            roles, auditRef, caller);\n        \n        // at this point the tenant side is complete. If the token was a chained\n        // token signed by the provider service then we're going to process the\n        // provider side as well thus complete the tenancy on-boarding process\n        \n        String authorizedService = ((RsrcCtxWrapper) ctx).principal().getAuthorizedService();\n        if (isAuthorizedProviderService(authorizedService, provSvcDomain, provSvcName)) {\n\n            // first we need to setup the admin roles in case this\n            // happens to be the first resource group\n\n            setupTenantAdminPolicyInProvider(ctx, provSvcDomain, provSvcName, tenantDomain,\n                    auditRef, caller);\n\n            // now onboard the requested resource group\n\n            dbService.executePutTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain,\n                    resourceGroup, roleActions, auditRef, caller);\n        }\n\n        return detail;\n    }\n\n    void setupTenantAdminPolicyInProvider(ResourceContext ctx, final String provSvcDomain,\n            final String provSvcName, final String tenantDomain, final String auditRef,\n            final String caller) {\n\n        List<TenantRoleAction> roles = new ArrayList<>();\n        TenantRoleAction roleAction = new TenantRoleAction().setAction(\"*\").setRole(ADMIN_ROLE_NAME);\n        roles.add(roleAction);\n        dbService.executePutTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain, null,\n                roles, auditRef, caller);\n    }\n\n    String getProviderRoleAction(String provSvcDomain, String roleName) {\n        \n        // if no match then we're going to default action of empty string\n        \n        Policy policy = dbService.getPolicy(provSvcDomain, roleName); // policy has same name\n        if (policy == null) {\n            return \"\";\n        }\n        \n        List<Assertion> assertions = policy.getAssertions();\n        if (assertions == null) {\n            return \"\";\n        }\n\n        for (Assertion assertion : assertions) {\n            if (!assertion.getRole().endsWith(roleName)) {\n                continue;\n            }\n            \n            return assertion.getAction();\n        }\n        \n        return \"\";\n    }\n    \n    public TenantResourceGroupRoles getTenantResourceGroupRoles(ResourceContext ctx, String provSvcDomain,\n            String provSvcName, String tenantDomain, String resourceGroup) {\n\n        final String caller = \"gettenantresourcegrouproles\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller); // not including the domain, this is the domain's service type\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n\n        if (dbService.getDomain(provSvcDomain, false) == null) {\n            throw ZMSUtils.notFoundError(\"getTenantResourceGroupRoles: No such domain: \" + provSvcDomain, caller);\n        }\n\n        // look for this tenants roles, ex: storage.tenant.sports.reader\n\n        String rolePrefix = ZMSUtils.getTenantResourceGroupRolePrefix(provSvcName, tenantDomain, resourceGroup);\n        TenantResourceGroupRoles troles = new TenantResourceGroupRoles().setDomain(provSvcDomain)\n                .setService(provSvcName).setTenant(tenantDomain).setResourceGroup(resourceGroup);\n\n        List<TenantRoleAction> tralist = new ArrayList<>();\n        \n        // find roles matching the prefix\n        \n        List<String> rcollection = dbService.listRoles(provSvcDomain);\n        for (String rname: rcollection) {\n            if (dbService.isTrustRoleForTenant(provSvcDomain, rname, rolePrefix, resourceGroup, tenantDomain)) {\n                \n                // good, its exactly what we are looking for, but\n                // now we want the ACTION that was set in the provider\n                \n                String action = getProviderRoleAction(provSvcDomain, rname);\n                \n                // for the role name we must return the SimpleName\n                // part only so we'll remove the prefix section\n                \n                TenantRoleAction tra = new TenantRoleAction()\n                        .setRole(rname.substring(rolePrefix.length()))\n                        .setAction(action);\n                tralist.add(tra);\n            }\n        }\n        troles.setRoles(tralist);\n        return troles;\n    }\n\n    public void deleteTenantResourceGroupRoles(ResourceContext ctx, String provSvcDomain,\n            String provSvcName, String tenantDomain, String resourceGroup, String auditRef) {\n\n        final String caller = \"deletetenantresourcegrouproles\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(provSvcDomain, TYPE_DOMAIN_NAME, caller);\n        validate(provSvcName, TYPE_SIMPLE_NAME, caller); // not including the domain, this is the domain's service type\n        validate(tenantDomain, TYPE_DOMAIN_NAME, caller);\n        validate(resourceGroup, TYPE_COMPOUND_NAME, caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        provSvcDomain = provSvcDomain.toLowerCase();\n        setRequestDomain(ctx, provSvcDomain);\n        provSvcName = provSvcName.toLowerCase();\n        tenantDomain = tenantDomain.toLowerCase();\n        resourceGroup = resourceGroup.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        dbService.executeDeleteTenantRoles(ctx, provSvcDomain, provSvcName, tenantDomain,\n                resourceGroup, auditRef, caller);\n    }\n    \n    String extractDomainName(String resource) {\n        int idx = resource.indexOf(':');\n        if (idx == -1) {\n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"extractDomainName: missing domain name: \" + resource);\n            }\n            return null;\n        }\n        return resource.substring(0, idx);\n    }\n\n    void validateRequest(HttpServletRequest request, String caller) {\n        validateRequest(request, caller, false);\n    }\n    \n    void validateRequest(HttpServletRequest request, String caller, boolean statusRequest) {\n        \n        // first validate if we're required process this over TLS only\n        \n        if (secureRequestsOnly && !request.isSecure()) {\n            throw ZMSUtils.requestError(caller + \"request must be over TLS\", caller);\n        }\n        \n        // second check if this is a status port so we can only\n        // process on status requests\n        \n        if (statusPort > 0 && statusPort != httpPort && statusPort != httpsPort) {\n            \n            // non status requests must not take place on the status port\n            \n            if (!statusRequest && request.getLocalPort() == statusPort) {\n                throw ZMSUtils.requestError(\"incorrect port number for a non-status request\", caller);\n            }\n            \n            // status requests must not take place on a non-status port\n            \n            if (statusRequest && request.getLocalPort() != statusPort) {\n                throw ZMSUtils.requestError(\"incorrect port number for a status request\", caller);\n            }\n        }\n    }\n    \n    void validate(Object val, String type, String caller) {\n        if (val == null) {\n            throw ZMSUtils.requestError(\"Missing or malformed \" + type, caller);\n        }\n        \n        Result result = validator.validate(val, type);\n        if (!result.valid) {\n            throw ZMSUtils.requestError(\"Invalid \" + type  + \" error: \" + result.error, caller);\n        }\n    }\n    \n    List<String> validatedAdminUsers(List<String> lst) {\n        \n        final String caller = \"validatedadminusers\";\n        \n        if (lst == null || lst.size() == 0) {\n            throw ZMSUtils.requestError(\"validatedAdminUsers: Missing adminUsers\", caller);\n        }\n        Set<String> users = new HashSet<>();\n        for (String user : lst) {\n            validate(user, TYPE_RESOURCE_NAME, caller);\n            users.add(user);\n        }\n        return new ArrayList<>(users);\n    }\n    \n    Domain createTopLevelDomain(ResourceContext ctx, Domain domain, List<String> adminUsers,\n                List<String> solutionTemplates, String auditRef) {\n        List<String> users = validatedAdminUsers(adminUsers);\n        return dbService.makeDomain(ctx, domain, users, solutionTemplates, auditRef);\n    }\n    \n    Domain createSubDomain(ResourceContext ctx, Domain domain, List<String> adminUsers,\n                List<String> solutionTemplates, String auditRef, String caller) {\n\n        // verify length of full sub domain name\n\n        if (domain.getName().length() > domainNameMaxLen) {\n            throw ZMSUtils.requestError(\"Invalid SubDomain name: \" + domain.getName()\n                    + \" : name length cannot exceed: \" + domainNameMaxLen, caller);\n        } \n\n        List<String> users = validatedAdminUsers(adminUsers);\n        return dbService.makeDomain(ctx, domain, users, solutionTemplates, auditRef);\n    }\n\n    int countDots(String str) {\n        int count = 0;\n        int i = str.indexOf('.');\n        while (i >= 0) {\n            count++;\n            i = str.indexOf('.', i + 1);\n        }\n        return count;\n    }\n\n    boolean hasExceededDepthLimit(Integer depth, String name) {\n        \n        if (depth == null) {\n            return false;\n        }\n        \n        // depth=0 means only top level\n\n        return countDots(name) > depth;\n    }\n    \n    DomainList listDomains(Integer limit, String skip, String prefix, Integer depth, long modTime, boolean masterCopy) {\n            \n        //note: we don't use the store's options, because we also need to filter on depth\n        \n        List<String> allDomains = dbService.listDomains(prefix, modTime, masterCopy);\n        List<String> names = new ArrayList<>();\n        \n        for (String name : allDomains) {\n            if (hasExceededDepthLimit(depth, name)) {\n                continue;\n            }\n            names.add(name);\n        }\n        \n        int count = names.size();\n        if (skip != null) {\n            for (int i = 0; i < count; i++) {\n                String name = names.get(i);\n                if (skip.equals(name)) {\n                    names = names.subList(i + 1, count);\n                    count = names.size();\n                    break;\n                }\n            }\n        }\n        \n        DomainList result = new DomainList();\n\n        // if we have exceeded our requested list then\n        // set the next skip entry in our result\n        \n        if (hasExceededListLimit(limit, count)) {\n            names = names.subList(0, limit);\n            result.setNext(names.get(limit - 1));\n        }\n        \n        result.setNames(names);\n        return result;\n    }\n    \n    boolean isZMSService(String domain, String service) {\n        return (SYS_AUTH.equalsIgnoreCase(domain) && ZMSConsts.ZMS_SERVICE.equalsIgnoreCase(service));\n    }\n    \n    /**\n     * implements KeyStore getPublicKey\n     * @return String with PEM encoded key, which should be ybase64decoded prior\n     *         to return if ybase64encoded\n     **/\n    @Override\n    public String getPublicKey(String domain, String service, String keyId) {\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getPublicKey: service=\" + domain + \".\" + service + \" key-id=\" + keyId);\n        }\n        \n        if (service == null || keyId == null) {\n            return null;\n        }\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domain = domain.toLowerCase();\n        service = service.toLowerCase();\n        keyId = keyId.toLowerCase();\n        \n        // special handling for service sys.auth.zms which is ourselves\n        // so we'll just lookup our key in our map\n\n        String pubKey = null;\n        if (isZMSService(domain, service)) {\n            pubKey = serverPublicKeyMap.get(keyId);\n        }\n        \n        // if it's not the ZMS Server public key then lookup the \n        // public key from ZMS data\n        \n        if (pubKey == null) {\n            try {\n                PublicKeyEntry keyEntry = dbService.getServicePublicKeyEntry(domain, service, keyId, true);\n                if (keyEntry != null) {\n                    pubKey = keyEntry.getKey();\n                }\n            } catch (ResourceException ex) {\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"getPublicKey: unable to get public key: \" + ex.getMessage());\n                }\n                return null;\n            }\n        }\n\n        if (pubKey == null) {\n            if (LOG.isWarnEnabled()) {\n                LOG.warn(\"getPublicKey: service=\" + domain + \".\" + service + \" has no public key registered\");\n            }\n            return null;\n        }\n        \n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getPublicKey: service public key: \" + pubKey);\n        }\n        \n        return Crypto.ybase64DecodeString(pubKey);\n    }\n    \n    @Override\n    public void putDefaultAdmins(ResourceContext ctx, String domainName, String auditRef,\n            DefaultAdmins defaultAdmins) {\n\n        final String caller = \"putdefaultadmins\";\n        logPrincipal(ctx);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putDefaultAdmins: domain = \" + domainName);\n        }\n        \n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n\n        // verify that request is properly authenticated for this request\n        \n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n        \n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n        \n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"putDefaultAdmins: Domain not found: '\" + domainName + \"'\", caller);\n        }\n\n        // normalize and validate requested admin users\n\n        AthenzObject.DEFAULT_ADMINS.convertToLowerCase(defaultAdmins);\n        defaultAdmins.setAdmins(normalizedAdminUsers(defaultAdmins.getAdmins(), domain.getDomain().getUserAuthorityFilter(), caller));\n        \n        Role adminRole = null;\n        for (Role role : domain.getRoles()) {\n            if (ADMIN_ROLE_NAME.equals(ZMSUtils.removeDomainPrefix(role.getName(), domainName, ROLE_PREFIX))) {\n                adminRole = role;\n                break;\n            }\n        }\n        if (adminRole == null) {\n            // if the admin role does not exist in the role section then add it\n            // this typically should never happen since we have added the\n            // check to disallow deletion of the admin role but we'll keep\n            // the logic in place\n        \n            if (LOG.isInfoEnabled()) {\n                LOG.info(\"putDefaultAdmins: Adding domain admin role because no domain admin role was found for domain: \" + domainName);\n            }\n            adminRole = ZMSUtils.makeAdminRole(domainName, new ArrayList<>());\n            dbService.executePutRole(ctx, domainName, ADMIN_ROLE_NAME, adminRole, auditRef, caller);\n        }\n            \n        Policy adminPolicy = null;\n        for (Policy policy : domain.getPolicies()) {\n            if (ADMIN_POLICY_NAME.equals(ZMSUtils.removeDomainPrefix(policy.getName(), domainName, POLICY_PREFIX))) {\n                adminPolicy = policy;\n                break;\n            }\n        }\n        if (adminPolicy == null) {\n            // if the admin policy does not exist in the policy section then add it\n            // this typically should never happen since we have added the\n            // check to disallow deletion of the admin policy but we'll keep\n            // the logic in place\n            \n            if (LOG.isInfoEnabled()) {\n                LOG.info(\"putDefaultAdmins: Adding domain admin policy  because no domain admin policy  was found for domain: \" + domainName);\n            }\n            //Create and add the admin policy\n            adminPolicy = ZMSUtils.makeAdminPolicy(domainName, adminRole);\n            dbService.executePutPolicy(ctx, domainName, ADMIN_POLICY_NAME, adminPolicy, auditRef, caller);\n        }\n        \n        addDefaultAdminAssertion(ctx, domainName, adminPolicy, auditRef, caller);\n        \n        removeAdminDenyAssertions(ctx, domainName, domain.getPolicies(), domain.getRoles(), adminRole,\n                defaultAdmins, auditRef);\n        \n        addDefaultAdminMembers(ctx, domainName, adminRole, defaultAdmins, auditRef, caller);\n    }\n\n    void addDefaultAdminAssertion(ResourceContext ctx, String domainName, Policy adminPolicy,\n            String auditRef, String caller) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"addDefaultAdminAssertion\");\n        }\n        \n        String domainAllResources = domainName + \":*\";\n        String domainAdminRole = ZMSUtils.roleResourceName(domainName, ADMIN_ROLE_NAME);\n\n        boolean invalidAssertions = false;\n        List<Assertion> assertions = adminPolicy.getAssertions();\n        if (assertions != null) {\n            \n            for (Assertion assertion : assertions) {\n                String resource = assertion.getResource();\n                if (resource == null) {\n                    invalidAssertions = true;\n                    continue;\n                }\n            \n                String action = assertion.getAction();  \n                if (action == null) {\n                    invalidAssertions = true;\n                    continue;\n                }\n            \n                String role = assertion.getRole();\n                if (role == null) {\n                    invalidAssertions = true;\n                    continue;\n                }\n            \n                // default effect is no value is ALLOW\n                AssertionEffect effect = assertion.getEffect();\n                if (effect == null) {\n                    effect = AssertionEffect.ALLOW;\n                }\n            \n                if (resource.equals(domainAllResources) && action.equals(\"*\") && \n                        role.equals(domainAdminRole) && (effect == AssertionEffect.ALLOW)) {\n                    // found an assertion for resource = <domain>:*, with action = \"*\", \n                    // for role = <domainName>:role.admin and effect = \"ALLOW\" \n                    // (if effect is null then defaults to ALLOW) so no need to add it\n                    return;\n                }\n            }\n        }\n        \n        if (LOG.isInfoEnabled()) {\n            LOG.info(\"Adding default admin assertion to admin policy because no default admin assertion was found for admin policy for domain: \" + domainName);\n        }\n\n        // if we had invalid assertions then we're going to\n        // reset the assertion list otherwise we can't update\n\n        if (invalidAssertions) {\n            adminPolicy.setAssertions(new ArrayList<>());\n        }\n        ZMSUtils.addAssertion(adminPolicy, domainAllResources, \"*\", domainAdminRole, AssertionEffect.ALLOW);\n        dbService.executePutPolicy(ctx, domainName, ADMIN_POLICY_NAME, adminPolicy, auditRef, caller);\n    }\n    \n    void removeAdminDenyAssertions(ResourceContext ctx, final String domainName, List<Policy> policies,\n            List<Role> roles, Role adminRole, DefaultAdmins defaultAdmins, final String auditRef) {\n\n        final String caller = \"putdefaultadmins\";\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"removeAdminDenyAssertions\");\n        }\n        \n        for (Policy policy : policies) {\n            \n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"access: processing policy: \" + policy.getName());\n            }\n            \n            // Process all the assertions defined in this policy\n            // As soon as match for an assertion that \n            // denies access to the admin role is detected, remove it\n            \n            List<Assertion> assertions = policy.getAssertions();\n            if (assertions == null) {\n                continue;\n            }\n            List<Assertion> assertionsToDelete = new ArrayList<>();\n            \n            for (Assertion assertion : assertions) {\n\n                // If there is no \"effect\" in the assertion then default is ALLOW\n                // so continue because logic is looking for DENY\n                AssertionEffect effect = assertion.getEffect();\n                if (effect != AssertionEffect.DENY) {\n                    continue;\n                }\n                \n                // If there is no role in the assertion then admin is not being denied\n                String assertionRole = assertion.getRole();\n                if (assertionRole == null) {\n                    continue;\n                }\n\n                if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Found DENY assertion for role \" + assertionRole);\n                }\n                    \n                // role matches admin role then remove it\n                if (assertionRole.equals(adminRole.getName())) {\n                    assertionsToDelete.add(assertion);\n                } else {\n                    removeAdminMembers(ctx, domainName, roles, assertionRole, defaultAdmins, auditRef, caller);\n                }\n            }\n            \n            if (assertionsToDelete.isEmpty()) {\n                continue;\n            }\n            \n            if (LOG.isInfoEnabled()) {\n                LOG.info(\"Removing assertion from policy: \" + policy.getName() + \" because it was for the domain admin role.\");\n            }\n            \n            for (Assertion assertion : assertionsToDelete) {\n                assertions.remove(assertion);\n            }\n\n            String policyName = ZMSUtils.removeDomainPrefix(policy.getName(), domainName, POLICY_PREFIX);\n            if (assertions.size() == 0) {\n                if (LOG.isInfoEnabled()) {\n                    LOG.info(\"Removing  policy: \" + policyName +\n                            \" because it did not have any assertions after removing a DENY\" +\n                            \" assertion for the domain admin role.\");\n                }\n\n                dbService.executeDeletePolicy(ctx, domainName, policyName, auditRef, caller);\n            } else {\n                dbService.executePutPolicy(ctx, domainName, policyName, policy, auditRef, caller);\n            }\n        }\n    }\n    \n    void removeAdminMembers(ResourceContext ctx, String domainName, List<Role> roles,\n            String assertionRole, DefaultAdmins defaultAdmins, String auditRef, String caller) {\n            \n        \n        for (Role role : roles) {\n            \n            if (LOG.isDebugEnabled()) {\n                LOG.debug(\"removeAdminMembers: Removing admin members from role: \" + role.getName());\n            }\n            \n            if (!assertionRole.equals(role.getName())) {\n                continue;\n            }\n\n            String roleName = ZMSUtils.removeDomainPrefix(role.getName(), domainName, ROLE_PREFIX);\n            for (String adminName : defaultAdmins.getAdmins()) {\n                if (isMemberOfRole(role, adminName)) {\n                    if (LOG.isInfoEnabled()) {\n                        LOG.info(\"removeAdminMembers: removing member: \" + adminName + \" from role: \" +\n                                roleName + \" because there is a DENY assertion for this role in this domain.\");\n                    }\n                    \n                    dbService.executeDeleteMembership(ctx, domainName, roleName, adminName, auditRef, caller);\n                }\n            }\n        }\n    }\n\n    void addDefaultAdminMembers(ResourceContext ctx, String domainName, Role adminRole,\n            DefaultAdmins defaultAdmins, String auditRef, String caller) {\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"addDefaultAdminMembers\");\n        }\n        \n        for (String adminName : defaultAdmins.getAdmins()) {\n            if (!isMemberOfRole(adminRole, adminName)) {\n                if (LOG.isInfoEnabled()) {\n                    LOG.info(\"Adding member: \" + adminName + \" to admin role for domain: \" + domainName);\n                }\n                RoleMember roleMember = new RoleMember().setMemberName(adminName);\n                dbService.executePutMembership(ctx, domainName, ADMIN_ROLE_NAME,\n                        roleMember, auditRef, caller);\n            }\n        }\n    }\n\n    public ServicePrincipal getServicePrincipal(ResourceContext ctx) {\n\n        final String caller = \"getserviceprincipal\";\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n        \n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        final String principalDomain = principal.getDomain();\n        setRequestDomain(ctx, principalDomain);\n\n        Authority authority = principal.getAuthority();\n\n        // If the authority does not support authorization then we're going to\n        // generate a new ServiceToken signed by ZMS and send that back.\n\n        ServicePrincipal servicePrincipal = new ServicePrincipal();\n        servicePrincipal.setDomain(principal.getDomain());\n        servicePrincipal.setService(principal.getName());\n        \n        if (!authority.allowAuthorization()) {\n        \n            PrincipalToken sdToken = new PrincipalToken(principal.getCredentials());\n            PrincipalToken zmsToken = new PrincipalToken.Builder(\"S1\", sdToken.getDomain(), sdToken.getName())\n                .issueTime(sdToken.getTimestamp())\n                .expirationWindow(sdToken.getExpiryTime() - sdToken.getTimestamp())\n                .ip(sdToken.getIP()).keyId(privateKey.getId()).host(serverHostName)\n                .keyService(ZMSConsts.ZMS_SERVICE).build();\n            zmsToken.sign(privateKey.getKey());\n\n            servicePrincipal.setToken(zmsToken.getSignedToken());\n            \n        } else {\n            servicePrincipal.setToken(principal.getCredentials());\n        }\n\n        return servicePrincipal;\n    }\n\n    ArrayList<AllowedOperation> getAuthorizedServiceOperations(final String authorizedService, final String operationName) {\n\n        // lookup the authorized services struct and see if we have the\n        // service specified in the allowed list\n\n        AuthorizedService authzService = serverAuthorizedServices.get(authorizedService);\n        if (authzService == null) {\n            throw ZMSUtils.forbiddenError(\"Unauthorized Service \" + authorizedService,\n                    operationName);\n        }\n\n        // if the list is empty then we do not allow any operations\n\n        ArrayList<AllowedOperation> ops = authzService.getAllowedOperations();\n        if (ops == null || ops.isEmpty()) {\n            throw ZMSUtils.forbiddenError(\"Unauthorized Operation (\" + operationName\n                    + \") for Service \" + authorizedService, operationName);\n        }\n\n        return ops;\n    }\n\n    void verifyAuthorizedServiceOperation(final String authorizedService, final String operationName) {\n        verifyAuthorizedServiceOperation(authorizedService, operationName, null, null);\n    }\n\n    void verifyAuthorizedServiceRoleOperation(final String authorizedService, final String operationName,\n            final String roleName) {\n\n        // only process this request if we have an authorized service specified\n\n        if (authorizedService == null) {\n            return;\n        }\n\n        // lookup the authorized services struct and see if we have the\n        // service specified in the allowed list\n\n        ArrayList<AllowedOperation> ops = getAuthorizedServiceOperations(authorizedService, operationName);\n\n        // otherwise make sure the operation is allowed for this service\n\n        boolean opAllowed = false;\n        for (AllowedOperation op : ops) {\n            if (!op.getName().equalsIgnoreCase(operationName)) {\n                continue;\n            }\n\n            opAllowed = op.isOperationAllowedOn(\"role\", roleName, AllowedOperation.MatchType.EQUALS) ||\n                    op.isOperationAllowedOn(\"role-prefix\", roleName, AllowedOperation.MatchType.STARTS_WITH);\n            break;\n        }\n\n        if (!opAllowed) {\n            throw ZMSUtils.forbiddenError(\"Unauthorized Operation (\" + operationName\n                            + \") for Service \" + authorizedService\n                            + \" on role \" + roleName, operationName);\n        }\n    }\n\n    /**\n     * If opItemType and value are not defined in the authorized_services JSON file,\n     * you can simply pass NULL for these two values.\n     */\n    void verifyAuthorizedServiceOperation(final String authorizedService, final String operationName,\n            final String opItemType, final String opItemVal) {\n        \n        // only process this request if we have an authorized service specified\n\n        if (authorizedService == null) {\n            return;\n        }\n\n        // lookup the authorized services struct and see if we have the\n        // service specified in the allowed list\n\n        ArrayList<AllowedOperation> ops = getAuthorizedServiceOperations(authorizedService, operationName);\n\n        // otherwise make sure the operation is allowed for this service\n        \n        boolean opAllowed = false;\n        for (AllowedOperation op : ops) {\n            if (!op.getName().equalsIgnoreCase(operationName)) {\n                continue;\n            }\n            \n            opAllowed = op.isOperationAllowedOn(opItemType, opItemVal, AllowedOperation.MatchType.EQUALS);\n            break;\n        }\n        \n        if (!opAllowed) {\n            throw ZMSUtils.forbiddenError(\"Unauthorized Operation (\" + operationName\n                    + \") for Service \" + authorizedService\n                    + (opItemType != null && !opItemType.isEmpty() ? \" on opItemKey \" + opItemType + \" and opItemVal \" + opItemVal : \"\"),\n                    operationName);\n        }\n    }\n\n    @Override\n    public ResourceAccessList getResourceAccessList(ResourceContext ctx, String principal,\n            String action) {\n\n        final String caller = \"getresourceaccesslist\";\n\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        Principal ctxPrincipal = ((RsrcCtxWrapper) ctx).principal();\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getResourceAccessList:(\" + ctxPrincipal + \", \" + principal\n                    + \", \" + action + \")\");\n        }\n        \n        if (principal != null) {\n            validate(principal, TYPE_ENTITY_NAME, caller);\n            principal = normalizeDomainAliasUser(principal.toLowerCase());\n        }\n        if (action != null) {\n            validate(action, TYPE_COMPOUND_NAME, caller);\n            action = action.toLowerCase();\n        }\n        \n        // if principal is null then we it's a special case\n        // so we need to make sure the caller is authorized\n        // to make this request\n        \n        if (principal == null || principal.isEmpty()) {\n            if (!isAllowedResourceLookForAllUsers(ctxPrincipal)) {\n                throw ZMSUtils.forbiddenError(\"Principal: \" + ctxPrincipal.getFullName() +\n                        \" not authorized to lookup resources for all users in Athenz\", caller);\n            }\n        }\n        \n        return dbService.getResourceAccessList(principal, action);\n    }\n\n    @Override\n    public Status getStatus(ResourceContext ctx) {\n\n        final String caller = \"getstatus\";\n        logPrincipal(ctx);\n\n        // validate our request as status request\n        \n        validateRequest(ctx.request(), caller, true);\n        \n        // for now we're going to verify our database connectivity\n        // in case of failure we're going to return not found\n\n        DomainList dlist = listDomains(null, null, null, null, 0, false);\n        if (dlist.getNames() == null || dlist.getNames().isEmpty()) {\n            throw ZMSUtils.notFoundError(\"Error - no domains available\", caller);\n        }\n\n        // check if we're configured to check for the status file\n\n        if (healthCheckFile != null && !healthCheckFile.exists()) {\n            throw ZMSUtils.notFoundError(\"Error - no status available\", caller);\n        }\n\n        // if the StatusChecker is set, check the server status\n\n        if (statusChecker != null) {\n            try {\n                statusChecker.check();\n            } catch (StatusCheckException e) {\n                throw ZMSUtils.error(e.getCode(), e.getMsg(), caller);\n            }\n        }\n\n        return successServerStatus;\n    }\n\n    String getPrincipalDomain(ResourceContext ctx) {\n        if (ctx == null) {\n            return null;\n        }\n        final Principal ctxPrincipal = ((RsrcCtxWrapper) ctx).principal();\n        return ctxPrincipal == null ? null : ctxPrincipal.getDomain();\n    }\n\n    void setRequestDomain(ResourceContext ctx, String requestDomainName) {\n        ((RsrcCtxWrapper) ctx).setRequestDomain(requestDomainName);\n    }\n\n    String getRequestDomainName(ResourceContext ctx) {\n        if (ctx == null) {\n            return null;\n        }\n        return ((RsrcCtxWrapper) ctx).getRequestDomain();\n    }\n\n    Object getTimerMetric(ResourceContext ctx) {\n        if (ctx == null) {\n            return null;\n        }\n        return ((RsrcCtxWrapper) ctx).getTimerMetric();\n    }\n\n    void logPrincipal(ResourceContext ctx) {\n        \n        // we are going to log our principal and validate that it\n        // contains expected data\n        \n        final Principal ctxPrincipal = ((RsrcCtxWrapper) ctx).principal();\n        ((RsrcCtxWrapper) ctx).logPrincipal(ctxPrincipal);\n        if (ctxPrincipal != null && ctxPrincipal.getFullName() != null) {\n            validate(ctxPrincipal.getFullName(), TYPE_SERVICE_NAME, \"logPrincipal\");\n        }\n    }\n\n    public ResourceContext newResourceContext(HttpServletRequest request,\n                                              HttpServletResponse response) {\n        Object timerMetric = metric.startTiming(\"zms_api_latency\", null);\n        // check to see if we want to allow this URI to be available\n        // with optional authentication support\n\n        boolean optionalAuth = StringUtils.requestUriMatch(request.getRequestURI(),\n                authFreeUriSet, authFreeUriList);\n        return new RsrcCtxWrapper(request, response, authorities, optionalAuth, this, timerMetric);\n    }\n    \n    @Override\n    public Schema getRdlSchema(ResourceContext context) {\n        return schema;\n    }\n    \n    static String getServerHostName() {\n        \n        String serverHostName = System.getProperty(ZMSConsts.ZMS_PROP_HOSTNAME);\n        if (serverHostName == null || serverHostName.isEmpty()) {\n            try {\n                InetAddress localhost = java.net.InetAddress.getLocalHost();\n                serverHostName = localhost.getCanonicalHostName();\n            } catch (java.net.UnknownHostException e) {\n                LOG.info(\"Unable to determine local hostname: \" + e.getMessage());\n                serverHostName = \"localhost\";\n            }\n        }\n        \n        return serverHostName;\n    }\n    \n    Authority getAuthority(String className) {\n        \n        LOG.debug(\"Loading authority {}...\", className);\n        \n        Authority authority;\n        try {\n            authority = (Authority) Class.forName(className).newInstance();\n        } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n            LOG.error(\"Invalid Authority class: \" + className + \" error: \" + e.getMessage());\n            return null;\n        }\n        return authority;\n    }\n    \n    public static String getRootDir() {\n        \n        if (ROOT_DIR == null) {\n            ROOT_DIR = System.getProperty(ZMSConsts.ZMS_PROP_ROOT_DIR, ZMSConsts.STR_DEF_ROOT);\n        }\n\n        return ROOT_DIR;\n    }\n\n    boolean isAllowedSystemMetaDelete(Principal principal, final String reqDomain, final String attribute,\n            final String objectType) {\n\n        // the authorization policy resides in official sys.auth domain\n\n        AthenzDomain domain = getAthenzDomain(SYS_AUTH, true);\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n        // our action are always converted to lowercase\n\n        String resource = SYS_AUTH + \":meta.\" + objectType + \".\" + attribute + \".\" + reqDomain;\n        AccessStatus accessStatus = evaluateAccess(domain, principal.getFullName(), \"delete\",\n                resource, null, null);\n\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n\n    @Override\n    public void putRoleSystemMeta(ResourceContext ctx, String domainName, String roleName, String attribute,\n            String auditRef, RoleSystemMeta meta) {\n\n        final String caller = \"putrolesystemmeta\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(meta, TYPE_ROLE_SYSTEM_META, caller);\n        validate(attribute, TYPE_SIMPLE_NAME, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        attribute = attribute.toLowerCase();\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putRoleSystemMeta: name={}, role={} attribute={}, meta={}\",\n                    domainName, roleName, attribute, meta);\n        }\n\n        // if we are resetting the configured value then the caller\n        // must also have a delete action available for the same resource\n\n        boolean deleteAllowed = isAllowedSystemMetaDelete(principal, domainName, attribute, \"role\");\n\n        dbService.executePutRoleSystemMeta(ctx, domainName, roleName, meta, attribute, deleteAllowed, auditRef, caller);\n    }\n\n    @Override\n    public void putRoleMeta(ResourceContext ctx, String domainName, String roleName, String auditRef, RoleMeta meta) {\n\n        final String caller = \"putrolemeta\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n\n        // validate meta values - validator will enforce any patters\n        // defined in the schema and we need to validate the rest of the\n        // integer and string values. for now we're making sure we're not\n        // getting any negative values for our integer settings\n\n        validate(meta, TYPE_ROLE_META, caller);\n        validateRoleMetaValues(meta);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        AthenzObject.ROLE_META.convertToLowerCase(meta);\n\n        // validate the user authority settings if they're provided\n\n        validateRoleUserAuthorityAttributes(meta.getUserAuthorityFilter(), meta.getUserAuthorityExpiration(), caller);\n\n        // verify that request is properly authenticated for this request\n\n        Principal principal = ((RsrcCtxWrapper) ctx).principal();\n        verifyAuthorizedServiceOperation(principal.getAuthorizedService(), caller);\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"putRoleMeta: name={}, role={} meta={}\", domainName, roleName, meta);\n        }\n\n        dbService.executePutRoleMeta(ctx, domainName, roleName, meta, auditRef, caller);\n    }\n\n    @Override\n    public void putMembershipDecision(ResourceContext ctx, String domainName, String roleName,\n            String memberName, String auditRef, Membership membership) {\n\n        final String caller = \"putmembershipdecision\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(memberName, TYPE_MEMBER_NAME, caller);\n        validate(membership, TYPE_MEMBERSHIP, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        memberName = memberName.toLowerCase();\n        AthenzObject.MEMBERSHIP.convertToLowerCase(membership);\n\n        final Principal principal = ((RsrcCtxWrapper) ctx).principal();\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceRoleOperation(principal.getAuthorizedService(), caller, roleName);\n\n        // verify that the member name in the URI and object provided match\n\n        if (!memberName.equals(membership.getMemberName())) {\n            throw ZMSUtils.requestError(\"putMembershipDecision: Member name in URI and Membership object do not match\", caller);\n        }\n\n        // role name is optional so we'll verify only if the value is present in the object\n\n        if (membership.getRoleName() != null && !roleName.equals(membership.getRoleName())) {\n            throw ZMSUtils.requestError(\"putMembershipDecision: Role name in URI and Membership object do not match\", caller);\n        }\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        Role role = getRoleFromDomain(roleName, domain);\n        if (role == null) {\n            throw ZMSUtils.requestError(\"Invalid rolename specified\", caller);\n        }\n\n        // initially create the role member and only set the\n        // user name which is all we need in case we need to\n        // lookup the pending entry for review approval\n        // we'll set the state and expiration after the\n        // authorization check is successful\n\n        RoleMember roleMember = new RoleMember();\n        roleMember.setMemberName(normalizeDomainAliasUser(memberName));\n\n        // authorization check\n\n        validatePutMembershipDecisionAuthorization(principal, domain, role, roleMember);\n\n        roleMember.setApproved(membership.getApproved());\n        roleMember.setActive(membership.getActive());\n\n        // set the user state, expiration and review date values\n        // no need to update the review/expiration dates if the\n        // request is going to be rejected\n\n        if (roleMember.getApproved() == Boolean.TRUE) {\n\n            setRoleMemberExpiration(domain, role, roleMember, membership, caller);\n            setRoleMemberReview(role, roleMember, membership);\n\n            // check to see if we need to validate the principal\n            // but only if the decision is to approve. We don't\n            // want to block removal of rejected user requests\n\n            final String userAuthorityFilter = enforcedUserAuthorityFilter(role.getUserAuthorityFilter(),\n                    domain.getDomain().getUserAuthorityFilter());\n            if (shouldValidateRoleMembers(userAuthorityFilter)) {\n                validateRoleMemberPrincipal(roleMember.getMemberName(), userAuthorityFilter, caller);\n            }\n        }\n\n        dbService.executePutMembershipDecision(ctx, domainName, roleName,\n                roleMember, auditRef, caller);\n    }\n\n    private void validatePutMembershipDecisionAuthorization(final Principal principal, final AthenzDomain domain,\n            final Role role, final RoleMember roleMember) {\n\n        final String caller = \"putmembershipdecision\";\n\n        // if this is an audit enabled domain then we're going to carry\n        // out the authorization in the sys.auth.audit domains\n\n        if (role.getAuditEnabled() == Boolean.TRUE) {\n            if (!isAllowedAuditRoleMembershipApproval(principal, domain)) {\n                throw ZMSUtils.forbiddenError(\"principal \" + principal.getFullName()\n                        + \" is not authorized to approve / reject members\", caller);\n            }\n            return;\n        }\n\n        // otherwise we're going to do a standard check if the principal\n        // is authorized to update the domain role membership\n\n        if (!isAllowedPutMembershipAccess(principal, domain, role.getName())) {\n            throw ZMSUtils.forbiddenError(\"principal \" + principal.getFullName()\n                    + \" is not authorized to approve / reject members\", caller);\n        }\n\n        // if the user is allowed to make changes in the domain but\n        // the role is review enabled then we need to make sure\n        // the approver cannot be the same as the requester\n\n        if (role.getReviewEnabled() == Boolean.TRUE) {\n\n            Membership pendingMember = dbService.getMembership(domain.getName(),\n                    ZMSUtils.extractRoleName(domain.getName(), role.getName()),\n                    roleMember.getMemberName(), 0, true);\n\n            // if the member is not found then we're going to throw a not found exception\n\n            if (!pendingMember.getIsMember()) {\n                throw ZMSUtils.notFoundError(\"pending member \" + roleMember.getMemberName()\n                        + \" not found\", caller);\n            }\n\n            if (pendingMember.getRequestPrincipal().equalsIgnoreCase(principal.getFullName())) {\n                throw ZMSUtils.forbiddenError(\"principal \" + principal.getFullName()\n                        + \" cannot approve his/her own request\", caller);\n            }\n        }\n    }\n\n    boolean isAllowedAuditRoleMembershipApproval(Principal principal, final AthenzDomain reqDomain) {\n\n        // the authorization policy resides in official sys.auth.audit domains\n        // first we're going to check the per domain one and then we'll\n        // follow up with per org domain\n\n        AthenzDomain authDomain = getAthenzDomain(ZMSConsts.SYS_AUTH_AUDIT_BY_DOMAIN, true);\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n        // our action are always converted to lowercase\n\n        String resource = ZMSConsts.SYS_AUTH_AUDIT_BY_DOMAIN + \":audit.\" + reqDomain.getDomain().getName();\n        AccessStatus accessStatus = evaluateAccess(authDomain, principal.getFullName(),\n                \"update\", resource, null, null);\n        if (accessStatus == AccessStatus.ALLOWED) {\n            return true;\n        }\n\n        // if we didn't find any authorization for the per-domain setup\n        // we're going to look at the per-org setup\n\n        authDomain = getAthenzDomain(ZMSConsts.SYS_AUTH_AUDIT_BY_ORG, true);\n        resource = ZMSConsts.SYS_AUTH_AUDIT_BY_ORG + \":audit.\" + reqDomain.getDomain().getOrg();\n        accessStatus = evaluateAccess(authDomain, principal.getFullName(),\n                \"update\", resource, null, null);\n\n        return accessStatus == AccessStatus.ALLOWED;\n    }\n\n    Role getRoleFromDomain(final String roleName, AthenzDomain domain) {\n        if (domain != null && domain.getRoles() != null) {\n            for (Role role : domain.getRoles()) {\n                if (role.getName().equalsIgnoreCase(domain.getName() + AuthorityConsts.ROLE_SEP + roleName)) {\n                    return role;\n                }\n            }\n        }\n        return null;\n    }\n\n    boolean isAllowedPutMembershipAccess(Principal principal, final AthenzDomain domain, final String roleName) {\n\n        // evaluate our domain's roles and policies to see if access\n        // is allowed or not for the given operation and resource\n        // our action are always converted to lowercase\n\n        return evaluateAccess(domain, principal.getFullName(), \"update\", roleName, null, null) == AccessStatus.ALLOWED;\n    }\n\n    boolean isAllowedPutMembershipWithoutApproval(Principal principal, final AthenzDomain reqDomain, final Role role) {\n\n        if (role.getAuditEnabled() == Boolean.TRUE) {\n            return false;\n        }\n\n        return isAllowedPutMembershipAccess(principal, reqDomain, role.getName());\n    }\n\n    boolean isAllowedPutMembership(Principal principal, final AthenzDomain domain, final Role role,\n            final RoleMember member) {\n\n        // first lets check if the principal has update access on the role\n\n        if (isAllowedPutMembershipAccess(principal, domain, role.getName())) {\n\n            // even with update access, if the role is audit/review enabled, member status\n            // can not be set to active/approved. It has to be approved by audit/review admins.\n            // for all other roles, set member status to active/approved immediately\n\n            boolean auditEnabled = (role.getAuditEnabled() == Boolean.TRUE || role.getReviewEnabled() == Boolean.TRUE);\n            member.setActive(!auditEnabled);\n            member.setApproved(!auditEnabled);\n            return true;\n\n        } else if (role.getSelfServe() == Boolean.TRUE) {\n\n            // if the role is self-serve then users are allowed to add anyone\n            // since the request must be approved by someone else so we'll allow it\n            // but with member status set to inactive.\n\n            member.setActive(false);\n            member.setApproved(false);\n            return true;\n        }\n\n        return false;\n    }\n\n    boolean isAllowedDeletePendingMembership(Principal principal, final String domainName,\n            final String roleName, final String memberName) {\n\n        // first lets check if the principal has update access on the role\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"Domain not found: \" + domainName, \"deletePendingMembership\");\n        }\n        if (isAllowedPutMembershipAccess(principal, domain, ZMSUtils.roleResourceName(domainName, roleName))) {\n            return true;\n        }\n\n        // check of the requestor of the pending request is the principal\n\n        Membership pendingMember = dbService.getMembership(domainName, roleName, memberName, 0, true);\n        return pendingMember != null && principal.getFullName().equals(pendingMember.getRequestPrincipal());\n    }\n\n    @Override\n    public DomainRoleMembership getPendingDomainRoleMembersList(ResourceContext ctx, String principal) {\n\n        final String caller = \"getpendingdomainrolememberslist\";\n\n        final Principal ctxPrincipal = ((RsrcCtxWrapper) ctx).principal();\n        logPrincipal(ctx);\n\n        validateRequest(ctx.request(), caller);\n\n        String checkPrincipal;\n        if (principal != null && !principal.isEmpty()) {\n            validate(principal, TYPE_ENTITY_NAME, caller);\n            checkPrincipal = normalizeDomainAliasUser(principal.toLowerCase());\n        } else {\n            checkPrincipal = ctxPrincipal.getFullName();\n        }\n\n        if (LOG.isDebugEnabled()) {\n            LOG.debug(\"getpendingdomainrolememberslist principal: ({})\", checkPrincipal);\n        }\n\n        return dbService.getPendingDomainRoleMembers(checkPrincipal);\n    }\n\n    @Override\n    public void putRoleReview(ResourceContext ctx, String domainName, String roleName, String auditRef, Role role) {\n        final String caller = \"putrolereview\";\n        logPrincipal(ctx);\n\n        if (readOnlyMode) {\n            throw ZMSUtils.requestError(SERVER_READ_ONLY_MESSAGE, caller);\n        }\n\n        validateRequest(ctx.request(), caller);\n\n        validate(domainName, TYPE_DOMAIN_NAME, caller);\n        validate(roleName, TYPE_ENTITY_NAME, caller);\n        validate(role, TYPE_ROLE, caller);\n\n        // for consistent handling of all requests, we're going to convert\n        // all incoming object values into lower case (e.g. domain, role,\n        // policy, service, etc name)\n\n        domainName = domainName.toLowerCase();\n        setRequestDomain(ctx, domainName);\n        roleName = roleName.toLowerCase();\n        AthenzObject.ROLE.convertToLowerCase(role);\n\n        // verify that request is properly authenticated for this request\n\n        verifyAuthorizedServiceOperation(((RsrcCtxWrapper) ctx).principal().getAuthorizedService(), caller);\n\n        // verify the role name in the URI and request are consistent\n\n        if (!isConsistentRoleName(domainName, roleName, role)) {\n            throw ZMSUtils.requestError(caller + \": Inconsistent role names - expected: \"\n                    + ZMSUtils.roleResourceName(domainName, roleName) + \", actual: \"\n                    + role.getName(), caller);\n        }\n\n        AthenzDomain domain = getAthenzDomain(domainName, false);\n        if (domain == null) {\n            throw ZMSUtils.notFoundError(\"No such domain: \" + domainName, caller);\n        }\n\n        Role dbRole = getRoleFromDomain(roleName, domain);\n\n        if (configuredDueDateMillis(domain.getDomain().getMemberExpiryDays(), dbRole.getMemberExpiryDays()) == 0 &&\n                configuredDueDateMillis(domain.getDomain().getServiceExpiryDays(), dbRole.getServiceExpiryDays()) == 0) {\n            throw ZMSUtils.requestError(caller + \": Domain member expiry / Role member expiry must be set to review the role. \", caller);\n        }\n\n        // normalize and remove duplicate members\n\n        normalizeRoleMembers(role);\n\n        // update role expiry based on our configurations\n\n        updateRoleMemberExpiration(\n                domain.getDomain().getMemberExpiryDays(),\n                dbRole.getMemberExpiryDays(),\n                domain.getDomain().getServiceExpiryDays(),\n                dbRole.getServiceExpiryDays(),\n                role.getRoleMembers());\n\n        // update role review based on our configurations\n\n        updateRoleMemberReviewReminder(dbRole.getMemberReviewDays(), dbRole.getServiceReviewDays(), role.getRoleMembers());\n\n        // process our request\n\n        dbService.executePutRoleReview(ctx, domainName, roleName, role, auditRef, caller);\n    }\n\n    void validateUserAuthorityFilterAttribute(final String authorityFilter, final String caller)  {\n\n        if (authorityFilter != null && !authorityFilter.isEmpty()) {\n            if (userAuthority == null) {\n                throw ZMSUtils.requestError(\"Role User Authority filter specified without a valid user authority\", caller);\n            }\n\n            Set<String> attrSet = userAuthority.booleanAttributesSupported();\n            for (String attr : authorityFilter.split(\",\"))  {\n                if (!attrSet.contains(attr)) {\n                    throw ZMSUtils.requestError(attr + \" is not a valid user authority attribute\", caller);\n                }\n            }\n        }\n    }\n\n    void validateUserAuthorityDateAttribute(final String authorityExpiration, final String caller) {\n\n        if (authorityExpiration != null && !authorityExpiration.isEmpty()) {\n            if (userAuthority == null) {\n                throw ZMSUtils.requestError(\"Role User Authority expiry specified without a valid user authority\", caller);\n            }\n\n            Set<String> attrSet = userAuthority.dateAttributesSupported();\n            if (!attrSet.contains(authorityExpiration)) {\n                throw ZMSUtils.requestError(authorityExpiration + \" is not a valid user authority date attribute\", caller);\n            }\n        }\n    }\n\n    void validateRoleUserAuthorityAttributes(final String authorityFilter, final String authorityExpiration,\n                                             final String caller) {\n        validateUserAuthorityFilterAttribute(authorityFilter, caller);\n        validateUserAuthorityDateAttribute(authorityExpiration, caller);\n    }\n\n    class AutoApplyTemplate implements Runnable {\n        Map<String, Integer> eligibleTemplatesForAutoUpdate;\n\n        public AutoApplyTemplate(Map<String, Integer> eligibleTemplatesForAutoUpdate) {\n            this.eligibleTemplatesForAutoUpdate = eligibleTemplatesForAutoUpdate;\n        }\n\n        @Override\n        public void run() {\n            if (LOG.isInfoEnabled()) {\n                LOG.info(\"List of eligible templates with version to apply .. {}\", eligibleTemplatesForAutoUpdate);\n            }\n            Map<String, List<String>> domainTemplateUpdateMapping = dbService.applyTemplatesForListOfDomains(eligibleTemplatesForAutoUpdate);\n            if (LOG.isInfoEnabled()) {\n                for (String domainName : domainTemplateUpdateMapping.keySet()) {\n                    LOG.info(\"List of templates applied against domain {} {}\", domainName, domainTemplateUpdateMapping.get(domainName));\n                }\n            }\n        }\n    }\n\n    public void recordMetrics(ResourceContext ctx, String httpMethod, int httpStatus, String apiName) {\n        final String principalDomainName = getPrincipalDomain(ctx);\n        final String domainName = getRequestDomainName(ctx);\n        final Object timerMetric = getTimerMetric(ctx);\n        metric.increment(\"zms_api\", domainName, principalDomainName, httpMethod, httpStatus, apiName);\n        metric.stopTiming(timerMetric, domainName, principalDomainName, httpMethod, httpStatus, apiName + \"_timing\");\n    }\n}\n", "idx": 83, "id": 5353, "msg": "", "proj": "AthenZ-athenz", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -0,0 +1,34 @@\n+/*\n+ * Copyright Camunda Services GmbH and/or licensed to Camunda Services GmbH\n+ * under one or more contributor license agreements. See the NOTICE file\n+ * distributed with this work for additional information regarding copyright\n+ * ownership. Camunda licenses this file to you under the Apache License,\n+ * Version 2.0; you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.camunda.bpm.integrationtest.functional.scriptengine;\n+\n+import org.jboss.arquillian.container.test.api.Deployment;\n+import org.jboss.shrinkwrap.api.spec.WebArchive;\n+\n+/**\n+ * @author Sebastian Menski\n+ */\n+public class JavascriptScriptEngineSupportGraalJsTest extends AbstractScriptEngineSupportTest {\n+\n+  @Deployment\n+  public static WebArchive createProcessApplication() {\n+    return initWebArchiveDeployment()\n+      .addClass(AbstractScriptEngineSupportTest.class)\n+      .addAsResource(createScriptTaskProcess(\"graal.js\", EXAMPLE_SCRIPT), \"process.bpmn20.xml\");\n+  }\n+\n+}", "y": 1, "oldf": "", "idx": 1, "id": 11946, "msg": "From what I remember, we shouldn't add `author` docs in new classes.", "proj": "camunda-camunda-bpm-platform", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -5,9 +5,14 @@ import { pageY } from './../../helpers/dom/event';\n import { arrayEach } from './../../helpers/array';\n import { rangeEach } from './../../helpers/number';\n import { registerPlugin } from './../../plugins';\n+import { ValueMap } from './../../translations';\n \n // Developer note! Whenever you make a change in this file, make an analogous change in manualRowResize.js\n \n+const ROW_HEIGHTS_MAP_NAME = 'rowHeights';\n+const PERSISTENT_STATE_KEY = 'manualRowHeights';\n+const privatePool = new WeakMap();\n+\n /**\n  * @description\n  * This plugin allows to change rows height. To make rows height persistent the {@link Options#persistentState}", "y": 1, "oldf": "import BasePlugin from './../_base';\nimport { addClass, hasClass, removeClass, outerWidth } from './../../helpers/dom/element';\nimport EventManager from './../../eventManager';\nimport { pageY } from './../../helpers/dom/event';\nimport { arrayEach } from './../../helpers/array';\nimport { rangeEach } from './../../helpers/number';\nimport { registerPlugin } from './../../plugins';\n\n// Developer note! Whenever you make a change in this file, make an analogous change in manualRowResize.js\n\n/**\n * @description\n * This plugin allows to change rows height. To make rows height persistent the {@link Options#persistentState}\n * plugin should be enabled.\n *\n * The plugin creates additional components to make resizing possibly using user interface:\n * - handle - the draggable element that sets the desired height of the row.\n * - guide - the helper guide that shows the desired height as a horizontal guide.\n *\n * @plugin ManualRowResize\n */\nclass ManualRowResize extends BasePlugin {\n  constructor(hotInstance) {\n    super(hotInstance);\n\n    const { rootDocument } = this.hot;\n\n    this.currentTH = null;\n    this.currentRow = null;\n    this.selectedRows = [];\n    this.currentHeight = null;\n    this.newSize = null;\n    this.startY = null;\n    this.startHeight = null;\n    this.startOffset = null;\n    this.handle = rootDocument.createElement('DIV');\n    this.guide = rootDocument.createElement('DIV');\n    this.eventManager = new EventManager(this);\n    this.pressed = null;\n    this.dblclick = 0;\n    this.autoresizeTimeout = null;\n    this.manualRowHeights = [];\n\n    addClass(this.handle, 'manualRowResizer');\n    addClass(this.guide, 'manualRowResizerGuide');\n  }\n\n  /**\n   * Checks if the plugin is enabled in the handsontable settings. This method is executed in {@link Hooks#beforeInit}\n   * hook and if it returns `true` than the {@link ManualRowResize#enablePlugin} method is called.\n   *\n   * @returns {Boolean}\n   */\n  isEnabled() {\n    return this.hot.getSettings().manualRowResize;\n  }\n\n  /**\n   * Enables the plugin functionality for this Handsontable instance.\n   */\n  enablePlugin() {\n    if (this.enabled) {\n      return;\n    }\n\n    this.manualRowHeights = [];\n\n    const initialRowHeights = this.hot.getSettings().manualRowResize;\n    const loadedManualRowHeights = this.loadManualRowHeights();\n\n    if (typeof loadedManualRowHeights !== 'undefined') {\n      this.manualRowHeights = loadedManualRowHeights;\n    } else if (Array.isArray(initialRowHeights)) {\n      this.manualRowHeights = initialRowHeights;\n    } else {\n      this.manualRowHeights = [];\n    }\n\n    this.addHook('modifyRowHeight', (height, row) => this.onModifyRowHeight(height, row));\n\n    // Handsontable.hooks.register('beforeRowResize');\n    // Handsontable.hooks.register('afterRowResize');\n\n    this.bindEvents();\n\n    super.enablePlugin();\n  }\n\n  /**\n   * Updates the plugin state. This method is executed when {@link Core#updateSettings} is invoked.\n   */\n  updatePlugin() {\n    const initialRowHeights = this.hot.getSettings().manualRowResize;\n\n    if (Array.isArray(initialRowHeights)) {\n      this.manualRowHeights = initialRowHeights;\n\n    } else if (!initialRowHeights) {\n      this.manualRowHeights = [];\n    }\n  }\n\n  /**\n   * Disables the plugin functionality for this Handsontable instance.\n   */\n  disablePlugin() {\n    super.disablePlugin();\n  }\n\n  /**\n   * Saves the current sizes using the persistentState plugin (the {@link Options#persistentState} option has to be enabled).\n   * @fires Hooks#persistentStateSave\n   * @fires Hooks#manualRowHeights\n   */\n  saveManualRowHeights() {\n    this.hot.runHooks('persistentStateSave', 'manualRowHeights', this.manualRowHeights);\n  }\n\n  /**\n   * Loads the previously saved sizes using the persistentState plugin (the {@link Options#persistentState} option has to be enabled).\n   *\n   * @returns {Array}\n   * @fires Hooks#persistentStateLoad\n   * @fires Hooks#manualRowHeights\n   */\n  loadManualRowHeights() {\n    const storedState = {};\n\n    this.hot.runHooks('persistentStateLoad', 'manualRowHeights', storedState);\n\n    return storedState.value;\n  }\n\n  /**\n   * Sets the resize handle position.\n   *\n   * @private\n   * @param {HTMLCellElement} TH TH HTML element.\n   */\n  setupHandlePosition(TH) {\n    this.currentTH = TH;\n\n    const cellCoords = this.hot.getCoords(this.currentTH);\n    const row = cellCoords.row;\n    const headerWidth = outerWidth(this.currentTH);\n\n    if (row >= 0) { // if not col header\n      const box = this.currentTH.getBoundingClientRect();\n      const fixedRowTop = row < this.hot.getSettings().fixedRowsTop;\n      const fixedRowBottom = row >= this.hot.countRows() - this.hot.getSettings().fixedRowsBottom;\n      let parentOverlay = this.hot.view.wt.wtOverlays.leftOverlay;\n\n      if (fixedRowTop) {\n        parentOverlay = this.hot.view.wt.wtOverlays.topLeftCornerOverlay;\n\n      } else if (fixedRowBottom) {\n        parentOverlay = this.hot.view.wt.wtOverlays.bottomLeftCornerOverlay;\n      }\n\n      let relativeHeaderPosition = parentOverlay.getRelativeCellPosition(this.currentTH, cellCoords.row, cellCoords.col);\n\n      // If the TH is not a child of the left/top-left/bottom-left overlay, recalculate using the top-most header\n      if (!relativeHeaderPosition) {\n        const topMostHeader = parentOverlay.clone.wtTable.TBODY.children[+!!this.hot.getSettings().colHeaders + row].firstChild;\n        relativeHeaderPosition = parentOverlay.getRelativeCellPosition(topMostHeader, cellCoords.row, cellCoords.col);\n      }\n\n      this.currentRow = row;\n      this.selectedRows = [];\n\n      if (this.hot.selection.isSelected() && this.hot.selection.isSelectedByRowHeader()) {\n        const { from, to } = this.hot.getSelectedRangeLast();\n        let start = from.row;\n        let end = to.row;\n\n        if (start >= end) {\n          start = to.row;\n          end = from.row;\n        }\n\n        if (this.currentRow >= start && this.currentRow <= end) {\n          rangeEach(start, end, i => this.selectedRows.push(i));\n\n        } else {\n          this.selectedRows.push(this.currentRow);\n        }\n\n      } else {\n        this.selectedRows.push(this.currentRow);\n      }\n\n      this.startOffset = relativeHeaderPosition.top - 6;\n      this.startHeight = parseInt(box.height, 10);\n\n      this.handle.style.top = `${this.startOffset + this.startHeight}px`;\n      this.handle.style.left = `${relativeHeaderPosition.left}px`;\n\n      this.handle.style.width = `${headerWidth}px`;\n      this.hot.rootElement.appendChild(this.handle);\n    }\n  }\n\n  /**\n   * Refresh the resize handle position.\n   *\n   * @private\n   */\n  refreshHandlePosition() {\n    this.handle.style.top = `${this.startOffset + this.currentHeight}px`;\n  }\n\n  /**\n   * Sets the resize guide position.\n   *\n   * @private\n   */\n  setupGuidePosition() {\n    const handleWidth = parseInt(outerWidth(this.handle), 10);\n    const handleRightPosition = parseInt(this.handle.style.left, 10) + handleWidth;\n    const maximumVisibleElementWidth = parseInt(this.hot.view.maximumVisibleElementWidth(0), 10);\n    addClass(this.handle, 'active');\n    addClass(this.guide, 'active');\n\n    this.guide.style.top = this.handle.style.top;\n    this.guide.style.left = `${handleRightPosition}px`;\n    this.guide.style.width = `${maximumVisibleElementWidth - handleWidth}px`;\n    this.hot.rootElement.appendChild(this.guide);\n  }\n\n  /**\n   * Refresh the resize guide position.\n   *\n   * @private\n   */\n  refreshGuidePosition() {\n    this.guide.style.top = this.handle.style.top;\n  }\n\n  /**\n   * Hides both the resize handle and resize guide.\n   *\n   * @private\n   */\n  hideHandleAndGuide() {\n    removeClass(this.handle, 'active');\n    removeClass(this.guide, 'active');\n  }\n\n  /**\n   * Checks if provided element is considered as a row header.\n   *\n   * @private\n   * @param {HTMLElement} element HTML element.\n   * @returns {Boolean}\n   */\n  checkIfRowHeader(element) {\n    if (element !== this.hot.rootElement) {\n      const parent = element.parentNode;\n\n      if (parent.tagName === 'TBODY') {\n        return true;\n      }\n\n      return this.checkIfRowHeader(parent);\n    }\n\n    return false;\n  }\n\n  /**\n   * Gets the TH element from the provided element.\n   *\n   * @private\n   * @param {HTMLElement} element HTML element.\n   * @returns {HTMLElement}\n   */\n  getTHFromTargetElement(element) {\n    if (element.tagName !== 'TABLE') {\n      if (element.tagName === 'TH') {\n        return element;\n      }\n      return this.getTHFromTargetElement(element.parentNode);\n\n    }\n\n    return null;\n  }\n\n  /**\n   * 'mouseover' event callback - set the handle position.\n   *\n   * @private\n   * @param {MouseEvent} event\n   */\n  onMouseOver(event) {\n    if (this.checkIfRowHeader(event.target)) {\n      const th = this.getTHFromTargetElement(event.target);\n\n      if (th) {\n        if (!this.pressed) {\n          this.setupHandlePosition(th);\n        }\n      }\n    }\n  }\n\n  /**\n   * Auto-size row after doubleclick - callback.\n   *\n   * @private\n   * @fires Hooks#beforeRowResize\n   * @fires Hooks#afterRowResize\n   */\n  afterMouseDownTimeout() {\n    const render = () => {\n      this.hot.forceFullRender = true;\n      this.hot.view.render(); // updates all\n      this.hot.view.wt.wtOverlays.adjustElementsSize(true);\n    };\n    const resize = (selectedRow, forceRender) => {\n      const hookNewSize = this.hot.runHooks('beforeRowResize', selectedRow, this.newSize, true);\n\n      if (hookNewSize !== void 0) {\n        this.newSize = hookNewSize;\n      }\n\n      this.setManualSize(selectedRow, this.newSize); // double click sets auto row size\n\n      if (forceRender) {\n        render();\n      }\n\n      this.hot.runHooks('afterRowResize', selectedRow, this.newSize, true);\n    };\n\n    if (this.dblclick >= 2) {\n      const selectedRowsLength = this.selectedRows.length;\n\n      if (selectedRowsLength > 1) {\n        arrayEach(this.selectedRows, (selectedRow) => {\n          resize(selectedRow);\n        });\n        render();\n      } else {\n        arrayEach(this.selectedRows, (selectedRow) => {\n          resize(selectedRow, true);\n        });\n      }\n    }\n    this.dblclick = 0;\n    this.autoresizeTimeout = null;\n  }\n\n  /**\n   * 'mousedown' event callback.\n   *\n   * @private\n   * @param {MouseEvent} event\n   */\n  onMouseDown(event) {\n    if (hasClass(event.target, 'manualRowResizer')) {\n      this.setupGuidePosition();\n      this.pressed = this.hot;\n\n      if (this.autoresizeTimeout === null) {\n        this.autoresizeTimeout = setTimeout(() => this.afterMouseDownTimeout(), 500);\n\n        this.hot._registerTimeout(this.autoresizeTimeout);\n      }\n\n      this.dblclick += 1;\n      this.startY = pageY(event);\n      this.newSize = this.startHeight;\n    }\n  }\n\n  /**\n   * 'mousemove' event callback - refresh the handle and guide positions, cache the new row height.\n   *\n   * @private\n   * @param {MouseEvent} event\n   */\n  onMouseMove(event) {\n    if (this.pressed) {\n      this.currentHeight = this.startHeight + (pageY(event) - this.startY);\n\n      arrayEach(this.selectedRows, (selectedRow) => {\n        this.newSize = this.setManualSize(selectedRow, this.currentHeight);\n      });\n\n      this.refreshHandlePosition();\n      this.refreshGuidePosition();\n    }\n  }\n\n  /**\n   * 'mouseup' event callback - apply the row resizing.\n   *\n   * @private\n   *\n   * @fires Hooks#beforeRowResize\n   * @fires Hooks#afterRowResize\n   */\n  onMouseUp() {\n    const render = () => {\n      this.hot.forceFullRender = true;\n      this.hot.view.render(); // updates all\n      this.hot.view.wt.wtOverlays.adjustElementsSize(true);\n    };\n    const runHooks = (selectedRow, forceRender) => {\n      this.hot.runHooks('beforeRowResize', selectedRow, this.newSize);\n\n      if (forceRender) {\n        render();\n      }\n\n      this.saveManualRowHeights();\n\n      this.hot.runHooks('afterRowResize', selectedRow, this.newSize, false);\n    };\n    if (this.pressed) {\n      this.hideHandleAndGuide();\n      this.pressed = false;\n\n      if (this.newSize !== this.startHeight) {\n        const selectedRowsLength = this.selectedRows.length;\n\n        if (selectedRowsLength > 1) {\n          arrayEach(this.selectedRows, (selectedRow) => {\n            runHooks(selectedRow);\n          });\n          render();\n        } else {\n          arrayEach(this.selectedRows, (selectedRow) => {\n            runHooks(selectedRow, true);\n          });\n        }\n      }\n\n      this.setupHandlePosition(this.currentTH);\n    }\n  }\n\n  /**\n   * Binds the mouse events.\n   *\n   * @private\n   */\n  bindEvents() {\n    const { rootElement, rootWindow } = this.hot;\n    this.eventManager.addEventListener(rootElement, 'mouseover', e => this.onMouseOver(e));\n    this.eventManager.addEventListener(rootElement, 'mousedown', e => this.onMouseDown(e));\n    this.eventManager.addEventListener(rootWindow, 'mousemove', e => this.onMouseMove(e));\n    this.eventManager.addEventListener(rootWindow, 'mouseup', () => this.onMouseUp());\n  }\n\n  /**\n   * Sets the new height for specified row index.\n   *\n   * @param {Number} row Visual row index.\n   * @param {Number} height Row height.\n   * @returns {Number} Returns new height.\n   *\n   * @fires Hooks#modifyRow\n   */\n  setManualSize(row, height) {\n    const physicalRow = this.hot.toPhysicalRow(row);\n\n    this.manualRowHeights[physicalRow] = height;\n\n    return height;\n  }\n\n  /**\n   * Modifies the provided row height, based on the plugin settings.\n   *\n   * @private\n   * @param {Number} height Row height.\n   * @param {Number} row Visual row index.\n   * @returns {Number}\n   *\n   * @fires Hooks#modifyRow\n   */\n  onModifyRowHeight(height, row) {\n    if (this.enabled) {\n      const autoRowSizePlugin = this.hot.getPlugin('autoRowSize');\n      const autoRowHeightResult = autoRowSizePlugin ? autoRowSizePlugin.heights[row] : null;\n      const physicalRow = this.hot.toPhysicalRow(row);\n      const manualRowHeight = this.manualRowHeights[physicalRow];\n\n      if (manualRowHeight !== void 0 && (manualRowHeight === autoRowHeightResult || manualRowHeight > (height || 0))) {\n        return manualRowHeight;\n      }\n    }\n\n    return height;\n  }\n}\n\nregisterPlugin('manualRowResize', ManualRowResize);\n\nexport default ManualRowResize;\n", "idx": 1, "id": 15633, "msg": "Please change this name to the name of the plugin.", "proj": "handsontable-handsontable", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -3223,6 +3223,17 @@ class Series(_Frame, IndexOpsMixin, Generic[T]):\n         \"\"\"\n         return _col(DataFrame(self._internal.copy()))\n \n+    def keys(self):\n+        \"\"\"\n+        Return alias for index.\n+\n+        Returns\n+        -------\n+        Index\n+            Index of the Series.\n+        \"\"\"\n+        return self.index\n+\n     # TODO: 'regex', 'method' parameter\n     def replace(self, to_replace=None, value=None, regex=False) -> 'Series':\n         \"\"\"", "y": 1, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nA wrapper class for Spark Column to behave similar to pandas Series.\n\"\"\"\nimport re\nimport inspect\nfrom collections import Iterable\nfrom functools import partial, wraps, reduce\nfrom typing import Any, Generic, List, Optional, Tuple, TypeVar, Union\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.io.formats.printing import pprint_thing\n\nfrom databricks.koalas.typedef import as_python_type\nfrom pyspark import sql as spark\nfrom pyspark.sql import functions as F, Column\nfrom pyspark.sql.types import BooleanType, StructType\nfrom pyspark.sql.window import Window\n\nfrom databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.\nfrom databricks.koalas.config import get_option\nfrom databricks.koalas.base import IndexOpsMixin\nfrom databricks.koalas.frame import DataFrame\nfrom databricks.koalas.generic import _Frame\nfrom databricks.koalas.internal import IndexMap, _InternalFrame, SPARK_INDEX_NAME_FORMAT\nfrom databricks.koalas.missing.series import _MissingPandasLikeSeries\nfrom databricks.koalas.plot import KoalasSeriesPlotMethods\nfrom databricks.koalas.utils import validate_arguments_and_invoke_function, scol_for\nfrom databricks.koalas.datetimes import DatetimeMethods\nfrom databricks.koalas.strings import StringMethods\n\n\n# This regular expression pattern is complied and defined here to avoid to compile the same\n# pattern every time it is used in _repr_ in Series.\n# This pattern basically seeks the footer string from Pandas'\nREPR_PATTERN = re.compile(r\"Length: (?P<length>[0-9]+)\")\n\n_flex_doc_SERIES = \"\"\"\nReturn {desc} of series and other, element-wise (binary operator `{op_name}`).\n\nEquivalent to ``{equiv}``\n\nParameters\n----------\nother : Series or scalar value\n\nReturns\n-------\nSeries\n    The result of the operation.\n\nSee Also\n--------\nSeries.{reverse}\n\n{series_examples}\n\"\"\"\n\n_add_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.add(df.b)\na    4.0\nb    NaN\nc    6.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.radd(df.b)\na    4.0\nb    NaN\nc    6.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_sub_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.subtract(df.b)\na    0.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rsub(df.b)\na    0.0\nb    NaN\nc   -2.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_mul_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.multiply(df.b)\na    4.0\nb    NaN\nc    8.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rmul(df.b)\na    4.0\nb    NaN\nc    8.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_div_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.divide(df.b)\na    1.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rdiv(df.b)\na    1.0\nb    NaN\nc    0.5\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_pow_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.pow(df.b)\na     4.0\nb     NaN\nc    16.0\nd     NaN\nName: a, dtype: float64\n\n>>> df.a.rpow(df.b)\na     4.0\nb     NaN\nc    16.0\nd     NaN\nName: a, dtype: float64\n\"\"\"\n\n_mod_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.mod(df.b)\na    0.0\nb    NaN\nc    0.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rmod(df.b)\na    0.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_floordiv_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.floordiv(df.b)\na    1.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rfloordiv(df.b)\na    1.0\nb    NaN\nc    0.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\nT = TypeVar(\"T\")\n\n# Needed to disambiguate Series.str and str type\nstr_type = str\n\n\nclass Series(_Frame, IndexOpsMixin, Generic[T]):\n    \"\"\"\n    Koala Series that corresponds to Pandas Series logically. This holds Spark Column\n    internally.\n\n    :ivar _internal: an internal immutable Frame to manage metadata.\n    :type _internal: _InternalFrame\n    :ivar _kdf: Parent's Koalas DataFrame\n    :type _kdf: ks.DataFrame\n\n    Parameters\n    ----------\n    data : array-like, dict, or scalar value, Pandas Series\n        Contains data stored in Series\n        If data is a dict, argument order is maintained for Python 3.6\n        and later.\n        Note that if `data` is a Pandas Series, other arguments should not be used.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : numpy.dtype or None\n        If None, dtype will be inferred\n    copy : boolean, default False\n        Copy input data\n    \"\"\"\n\n    def __init__(self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False,\n                 anchor=None):\n        if isinstance(data, _InternalFrame):\n            assert dtype is None\n            assert name is None\n            assert not copy\n            assert not fastpath\n            IndexOpsMixin.__init__(self, data, anchor)\n        else:\n            if isinstance(data, pd.Series):\n                assert index is None\n                assert dtype is None\n                assert name is None\n                assert not copy\n                assert anchor is None\n                assert not fastpath\n                s = data\n            else:\n                s = pd.Series(\n                    data=data, index=index, dtype=dtype, name=name, copy=copy, fastpath=fastpath)\n            kdf = DataFrame(s)\n            IndexOpsMixin.__init__(self, kdf._internal.copy(scol=kdf._internal.data_scols[0]), kdf)\n\n    def _with_new_scol(self, scol: spark.Column) -> 'Series':\n        \"\"\"\n        Copy Koalas Series with the new Spark Column.\n\n        :param scol: the new Spark Column\n        :return: the copied Series\n        \"\"\"\n        return Series(self._internal.copy(scol=scol), anchor=self._kdf)\n\n    @property\n    def dtypes(self):\n        \"\"\"Return the dtype object of the underlying data.\n\n        >>> s = ks.Series(list('abc'))\n        >>> s.dtype == s.dtypes\n        True\n        \"\"\"\n        return self.dtype\n\n    @property\n    def spark_type(self):\n        \"\"\" Returns the data type as defined by Spark, as a Spark DataType object.\"\"\"\n        return self.schema.fields[-1].dataType\n\n    plot = CachedAccessor(\"plot\", KoalasSeriesPlotMethods)\n\n    # Arithmetic Operators\n    def add(self, other):\n        return (self + other).rename(self.name)\n\n    add.__doc__ = _flex_doc_SERIES.format(\n        desc='Addition',\n        op_name=\"+\",\n        equiv=\"series + other\",\n        reverse='radd',\n        series_examples=_add_example_SERIES)\n\n    def radd(self, other):\n        return (other + self).rename(self.name)\n\n    radd.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Addition',\n        op_name=\"+\",\n        equiv=\"other + series\",\n        reverse='add',\n        series_examples=_add_example_SERIES)\n\n    def div(self, other):\n        return (self / other).rename(self.name)\n\n    div.__doc__ = _flex_doc_SERIES.format(\n        desc='Floating division',\n        op_name=\"/\",\n        equiv=\"series / other\",\n        reverse='rdiv',\n        series_examples=_div_example_SERIES)\n\n    divide = div\n\n    def rdiv(self, other):\n        return (other / self).rename(self.name)\n\n    rdiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Floating division',\n        op_name=\"/\",\n        equiv=\"other / series\",\n        reverse='div',\n        series_examples=_div_example_SERIES)\n\n    def truediv(self, other):\n        return (self / other).rename(self.name)\n\n    truediv.__doc__ = _flex_doc_SERIES.format(\n        desc='Floating division',\n        op_name=\"/\",\n        equiv=\"series / other\",\n        reverse='rtruediv',\n        series_examples=_div_example_SERIES)\n\n    def rtruediv(self, other):\n        return (other / self).rename(self.name)\n\n    rtruediv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Floating division',\n        op_name=\"/\",\n        equiv=\"other / series\",\n        reverse='truediv',\n        series_examples=_div_example_SERIES)\n\n    def mul(self, other):\n        return (self * other).rename(self.name)\n\n    mul.__doc__ = _flex_doc_SERIES.format(\n        desc='Multiplication',\n        op_name=\"*\",\n        equiv=\"series * other\",\n        reverse='rmul',\n        series_examples=_mul_example_SERIES)\n\n    multiply = mul\n\n    def rmul(self, other):\n        return (other * self).rename(self.name)\n\n    rmul.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Multiplication',\n        op_name=\"*\",\n        equiv=\"other * series\",\n        reverse='mul',\n        series_examples=_mul_example_SERIES)\n\n    def sub(self, other):\n        return (self - other).rename(self.name)\n\n    sub.__doc__ = _flex_doc_SERIES.format(\n        desc='Subtraction',\n        op_name=\"-\",\n        equiv=\"series - other\",\n        reverse='rsub',\n        series_examples=_sub_example_SERIES)\n\n    subtract = sub\n\n    def rsub(self, other):\n        return (other - self).rename(self.name)\n\n    rsub.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Subtraction',\n        op_name=\"-\",\n        equiv=\"other - series\",\n        reverse='sub',\n        series_examples=_sub_example_SERIES)\n\n    def mod(self, other):\n        return (self % other).rename(self.name)\n\n    mod.__doc__ = _flex_doc_SERIES.format(\n        desc='Modulo',\n        op_name='%',\n        equiv='series % other',\n        reverse='rmod',\n        series_examples=_mod_example_SERIES)\n\n    def rmod(self, other):\n        return (other % self).rename(self.name)\n\n    rmod.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Modulo',\n        op_name='%',\n        equiv='other % series',\n        reverse='mod',\n        series_examples=_mod_example_SERIES)\n\n    def pow(self, other):\n        return (self ** other).rename(self.name)\n\n    pow.__doc__ = _flex_doc_SERIES.format(\n        desc='Exponential power of series',\n        op_name='**',\n        equiv='series ** other',\n        reverse='rpow',\n        series_examples=_pow_example_SERIES)\n\n    def rpow(self, other):\n        return (other ** self).rename(self.name)\n\n    rpow.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Exponential power',\n        op_name='**',\n        equiv='other ** series',\n        reverse='pow',\n        series_examples=_pow_example_SERIES)\n\n    def floordiv(self, other):\n        return (self // other).rename(self.name)\n\n    floordiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Integer division',\n        op_name='//',\n        equiv='series // other',\n        reverse='rfloordiv',\n        series_examples=_floordiv_example_SERIES)\n\n    def rfloordiv(self, other):\n        return (other // self).rename(self.name)\n\n    rfloordiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Integer division',\n        op_name='//',\n        equiv='other // series',\n        reverse='floordiv',\n        series_examples=_floordiv_example_SERIES)\n\n    # Comparison Operators\n    def eq(self, other):\n        \"\"\"\n        Compare if the current value is equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a == 1\n        a     True\n        b    False\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.eq(1)\n        a    True\n        b    None\n        c    True\n        d    None\n        Name: b, dtype: object\n        \"\"\"\n        return (self == other).rename(self.name)\n\n    equals = eq\n\n    def gt(self, other):\n        \"\"\"\n        Compare if the current value is greater than the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a > 1\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.gt(1)\n        a    False\n        b     None\n        c    False\n        d     None\n        Name: b, dtype: object\n        \"\"\"\n        return (self > other).rename(self.name)\n\n    def ge(self, other):\n        \"\"\"\n        Compare if the current value is greater than or equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a >= 2\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.ge(2)\n        a    False\n        b     None\n        c    False\n        d     None\n        Name: b, dtype: object\n        \"\"\"\n        return (self >= other).rename(self.name)\n\n    def lt(self, other):\n        \"\"\"\n        Compare if the current value is less than the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a < 1\n        a    False\n        b    False\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.lt(2)\n        a    True\n        b    None\n        c    True\n        d    None\n        Name: b, dtype: object\n        \"\"\"\n        return (self < other).rename(self.name)\n\n    def le(self, other):\n        \"\"\"\n        Compare if the current value is less than or equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a <= 2\n        a     True\n        b     True\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.le(2)\n        a    True\n        b    None\n        c    True\n        d    None\n        Name: b, dtype: object\n        \"\"\"\n        return (self <= other).rename(self.name)\n\n    def ne(self, other):\n        \"\"\"\n        Compare if the current value is not equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a != 1\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.ne(1)\n        a    False\n        b     None\n        c    False\n        d     None\n        Name: b, dtype: object\n        \"\"\"\n        return (self != other).rename(self.name)\n\n    # TODO: arg should support Series\n    # TODO: NaN and None\n    def map(self, arg):\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict``.\n\n        .. note:: make sure the size of the dictionary is not huge because it could\n            downgrade the performance or throw OutOfMemoryError due to a huge\n            expression within Spark. Consider the input as a functions as an\n            alternative instead in this case.\n\n        Parameters\n        ----------\n        arg : function or dict\n            Mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``None``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``None``.\n\n        Examples\n        --------\n        >>> s = ks.Series(['cat', 'dog', None, 'rabbit'])\n        >>> s\n        0       cat\n        1       dog\n        2      None\n        3    rabbit\n        Name: 0, dtype: object\n\n        ``map`` accepts a ``dict``. Values that are not found\n        in the ``dict`` are converted to ``None``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0    kitten\n        1     puppy\n        2      None\n        3      None\n        Name: 0, dtype: object\n\n        It also accepts a function:\n\n        >>> def format(x) -> str:\n        ...     return 'I am a {}'.format(x)\n\n        >>> s.map(format)\n        0       I am a cat\n        1       I am a dog\n        2      I am a None\n        3    I am a rabbit\n        Name: 0, dtype: object\n        \"\"\"\n        if isinstance(arg, dict):\n            is_start = True\n            # In case dictionary is empty.\n            current = F.when(F.lit(False), F.lit(None).cast(self.spark_type))\n\n            for to_replace, value in arg.items():\n                if is_start:\n                    current = F.when(self._scol == F.lit(to_replace), value)\n                    is_start = False\n                else:\n                    current = current.when(self._scol == F.lit(to_replace), value)\n\n            if hasattr(arg, \"__missing__\"):\n                tmp_val = arg[np._NoValue]\n                del arg[np._NoValue]  # Remove in case it's set in defaultdict.\n                current = current.otherwise(F.lit(tmp_val))\n            else:\n                current = current.otherwise(F.lit(None).cast(self.spark_type))\n            return self._with_new_scol(current).rename(self.name)\n        else:\n            return self.apply(arg)\n\n    def astype(self, dtype) -> 'Series':\n        \"\"\"\n        Cast a Koalas object to a specified dtype ``dtype``.\n\n        Parameters\n        ----------\n        dtype : data type\n            Use a numpy.dtype or Python type to cast entire pandas object to\n            the same type.\n\n        Returns\n        -------\n        casted : same type as caller\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n\n        Examples\n        --------\n        >>> ser = ks.Series([1, 2], dtype='int32')\n        >>> ser\n        0    1\n        1    2\n        Name: 0, dtype: int32\n\n        >>> ser.astype('int64')\n        0    1\n        1    2\n        Name: 0, dtype: int64\n        \"\"\"\n        from databricks.koalas.typedef import as_spark_type\n        spark_type = as_spark_type(dtype)\n        if not spark_type:\n            raise ValueError(\"Type {} not understood\".format(dtype))\n        return self._with_new_scol(self._scol.cast(spark_type))\n\n    def getField(self, name):\n        if not isinstance(self.schema, StructType):\n            raise AttributeError(\"Not a struct: {}\".format(self.schema))\n        else:\n            fnames = self.schema.fieldNames()\n            if name not in fnames:\n                raise AttributeError(\n                    \"Field {} not found, possible values are {}\".format(name, \", \".join(fnames)))\n            return self._with_new_scol(self._scol.getField(name))\n\n    def alias(self, name):\n        \"\"\"An alias for :meth:`Series.rename`.\"\"\"\n        return self.rename(name)\n\n    @property\n    def schema(self) -> StructType:\n        \"\"\"Return the underlying Spark DataFrame's schema.\"\"\"\n        return self.to_dataframe()._sdf.schema\n\n    @property\n    def shape(self):\n        \"\"\"Return a tuple of the shape of the underlying data.\"\"\"\n        return len(self),\n\n    @property\n    def ndim(self):\n        \"\"\"Returns number of dimensions of the Series.\"\"\"\n        return 1\n\n    @property\n    def name(self) -> Union[str, Tuple[str, ...]]:\n        \"\"\"Return name of the Series.\"\"\"\n        name = self._internal.column_index[0]\n        if name is not None and len(name) == 1:\n            return name[0]\n        else:\n            return name\n\n    @name.setter\n    def name(self, name: Union[str, Tuple[str, ...]]):\n        self.rename(name, inplace=True)\n\n    # TODO: Functionality and documentation should be matched. Currently, changing index labels\n    # taking dictionary and function to change index are not supported.\n    def rename(self, index: Union[str, Tuple[str, ...]] = None, **kwargs):\n        \"\"\"\n        Alter Series name.\n\n        Parameters\n        ----------\n        index : scalar\n            Scalar will alter the ``Series.name`` attribute.\n\n        inplace : bool, default False\n            Whether to return a new Series. If True then value of copy is\n            ignored.\n\n        Returns\n        -------\n        Series\n            Series with name altered.\n\n        Examples\n        --------\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        \"\"\"\n        if index is None:\n            scol = self._scol\n        else:\n            scol = self._scol.alias(str(index))\n        internal = self._internal.copy(\n            scol=scol,\n            column_index=[index if index is None or isinstance(index, tuple) else (index,)])\n        if kwargs.get('inplace', False):\n            self._internal = internal\n            return self\n        else:\n            return Series(internal, anchor=self._kdf)\n\n    @property\n    def index(self):\n        \"\"\"The index (axis labels) Column of the Series.\n\n        Currently not supported when the DataFrame has no index.\n\n        See Also\n        --------\n        Index\n        \"\"\"\n        return self._kdf.index\n\n    @property\n    def is_unique(self):\n        \"\"\"\n        Return boolean if values in the object are unique\n\n        Returns\n        -------\n        is_unique : boolean\n\n        >>> ks.Series([1, 2, 3]).is_unique\n        True\n        >>> ks.Series([1, 2, 2]).is_unique\n        False\n        >>> ks.Series([1, 2, 3, None]).is_unique\n        True\n        \"\"\"\n        sdf = self._kdf._sdf.select(self._scol)\n        col = self._scol\n\n        # Here we check:\n        #   1. the distinct count without nulls and count without nulls for non-null values\n        #   2. count null values and see if null is a distinct value.\n        #\n        # This workaround is in order to calculate the distinct count including nulls in\n        # single pass. Note that COUNT(DISTINCT expr) in Spark is designed to ignore nulls.\n        return sdf.select(\n            (F.count(col) == F.countDistinct(col)) &\n            (F.count(F.when(col.isNull(), 1).otherwise(None)) <= 1)\n        ).collect()[0][0]\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column,\n        or when the index is meaningless and needs to be reset\n        to the default before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels from the index.\n            Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series values.\n            Uses self.name by default. This argument is ignored when drop is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n        \"\"\"\n        if inplace and not drop:\n            raise TypeError('Cannot reset_index inplace on a Series to create a DataFrame')\n\n        if name is not None:\n            kdf = self.rename(name).to_dataframe()\n        else:\n            kdf = self.to_dataframe()\n        kdf = kdf.reset_index(level=level, drop=drop)\n        if drop:\n            kseries = _col(kdf)\n            if inplace:\n                self._internal = kseries._internal\n                self._kdf = kseries._kdf\n            else:\n                return kseries\n        else:\n            return kdf\n\n    def to_frame(self, name: Union[str, Tuple[str, ...]] = None) -> spark.DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = ks.Series([\"a\", \"b\", \"c\"])\n        >>> s.to_frame()\n           0\n        0  a\n        1  b\n        2  c\n\n        >>> s = ks.Series([\"a\", \"b\", \"c\"], name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is not None:\n            renamed = self.rename(name)\n        else:\n            renamed = self\n        sdf = renamed._internal.spark_internal_df\n        column_index = None  # type: Optional[List[Tuple[str, ...]]]\n        if renamed._internal.column_index[0] is None:\n            column_index = [('0',)]\n            column_index_names = None\n        else:\n            column_index = renamed._internal.column_index\n            column_index_names = renamed._internal.column_index_names\n        internal = _InternalFrame(sdf=sdf,\n                                  data_columns=[sdf.schema[-1].name],\n                                  index_map=renamed._internal.index_map,\n                                  column_index=column_index,\n                                  column_index_names=column_index_names)\n        return DataFrame(internal)\n\n    to_dataframe = to_frame\n\n    def to_string(self, buf=None, na_rep='NaN', float_format=None, header=True,\n                  index=True, length=False, dtype=False, name=False,\n                  max_rows=None):\n        \"\"\"\n        Render a string representation of the Series.\n\n        .. note:: This method should only be used if the resulting Pandas object is expected\n                  to be small, as all the data is loaded into the driver's memory. If the input\n                  is large, set max_rows parameter.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            buffer to write to\n        na_rep : string, optional\n            string representation of NAN to use, default 'NaN'\n        float_format : one-parameter function, optional\n            formatter function to apply to columns' elements if they are floats\n            default None\n        header : boolean, default True\n            Add the Series header (index name)\n        index : bool, optional\n            Add index (row) labels, default True\n        length : boolean, default False\n            Add the Series length\n        dtype : boolean, default False\n            Add the Series dtype\n        name : boolean, default False\n            Add the Series name if not None\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n\n        Returns\n        -------\n        formatted : string (if not buffer passed)\n\n        Examples\n        --------\n        >>> df = ks.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], columns=['dogs', 'cats'])\n        >>> print(df['dogs'].to_string())\n        0    0.2\n        1    0.0\n        2    0.6\n        3    0.2\n\n        >>> print(df['dogs'].to_string(max_rows=2))\n        0    0.2\n        1    0.0\n        \"\"\"\n        # Make sure locals() call is at the top of the function so we don't capture local variables.\n        args = locals()\n        if max_rows is not None:\n            kseries = self.head(max_rows)\n        else:\n            kseries = self\n\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_string, pd.Series.to_string, args)\n\n    def to_clipboard(self, excel=True, sep=None, **kwargs):\n        # Docstring defined below by reusing DataFrame.to_clipboard's.\n        args = locals()\n        kseries = self\n\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_clipboard, pd.Series.to_clipboard, args)\n\n    to_clipboard.__doc__ = DataFrame.to_clipboard.__doc__\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        .. note:: This method should only be used if the resulting Pandas DataFrame is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s_dict = s.to_dict()\n        >>> sorted(s_dict.items())\n        [(0, 1), (1, 2), (2, 3), (3, 4)]\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)  # doctest: +ELLIPSIS\n        defaultdict(<class 'list'>, {...})\n        \"\"\"\n        # Make sure locals() call is at the top of the function so we don't capture local variables.\n        args = locals()\n        kseries = self\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_dict, pd.Series.to_dict, args)\n\n    def to_latex(self, buf=None, columns=None, col_space=None, header=True, index=True,\n                 na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True,\n                 bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None,\n                 decimal='.', multicolumn=None, multicolumn_format=None, multirow=None):\n\n        args = locals()\n        kseries = self\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_latex, pd.Series.to_latex, args)\n\n    to_latex.__doc__ = DataFrame.to_latex.__doc__\n\n    def to_pandas(self):\n        \"\"\"\n        Return a pandas Series.\n\n        .. note:: This method should only be used if the resulting Pandas object is expected\n                  to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> df = ks.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], columns=['dogs', 'cats'])\n        >>> df['dogs'].to_pandas()\n        0    0.2\n        1    0.0\n        2    0.6\n        3    0.2\n        Name: dogs, dtype: float64\n        \"\"\"\n        return _col(self._internal.pandas_df.copy())\n\n    # Alias to maintain backward compatibility with Spark\n    toPandas = to_pandas\n\n    def to_list(self):\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        .. note:: This method should only be used if the resulting list is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        \"\"\"\n        return self._to_internal_pandas().to_list()\n\n    tolist = to_list\n\n    def drop_duplicates(self, inplace=False):\n        \"\"\"\n        Return koalas Series with duplicate values removed.\n\n        Parameters\n        ----------\n        inplace: bool, default False\n            If True, performs operation inpalce and returns None.\n\n        Returns\n        -------\n        Series\n            Series with deplicates dropped.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n        >>> s = ks.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        >>> s.drop_duplicates()\n        1       cow\n        0      lama\n        5     hippo\n        3    beetle\n        Name: animal, dtype: object\n        \"\"\"\n        kseries = _col(self.to_frame().drop_duplicates())\n\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def fillna(self, value=None, method=None, axis=None, inplace=False, limit=None):\n        \"\"\"Fill NA/NaN values.\n\n        .. note:: the current implementation of 'method' parameter in fillna uses Spark's Window\n            without specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series\n            Value to use to fill holes. alternately a dict/Series of values\n            specifying which value to use for each column.\n            DataFrame is not supported.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series pad / ffill: propagate last valid\n            observation forward to next valid backfill / bfill:\n            use NEXT valid observation to fill gap\n        axis : {0 or `index`}\n            1 and `columns` are not supported.\n        inplace : boolean, default False\n            Fill in place (do not create a new object)\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive NaN values to\n            forward/backward fill. In other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. If method is not specified,\n            this is the maximum number of entries along the entire axis where NaNs will be filled.\n            Must be greater than 0 if not None\n\n        Returns\n        -------\n        Series\n            Series with NA entries filled.\n\n        Examples\n        --------\n        >>> s = ks.Series([np.nan, 2, 3, 4, np.nan, 6], name='x')\n        >>> s\n        0    NaN\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        Name: x, dtype: float64\n\n        Replace all NaN elements with 0s.\n\n        >>> s.fillna(0)\n        0    0.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    0.0\n        5    6.0\n        Name: x, dtype: float64\n\n        We can also propagate non-null values forward or backward.\n\n        >>> s.fillna(method='ffill')\n        0    NaN\n        1    2.0\n        2    3.0\n        3    4.0\n        4    4.0\n        5    6.0\n        Name: x, dtype: float64\n\n        >>> s = ks.Series([np.nan, 'a', 'b', 'c', np.nan], name='x')\n        >>> s.fillna(method='ffill')\n        0    None\n        1       a\n        2       b\n        3       c\n        4       c\n        Name: x, dtype: object\n        \"\"\"\n        return self._fillna(value, method, axis, inplace, limit)\n\n    def _fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, part_cols=()):\n        if axis is None:\n            axis = 0\n        if not (axis == 0 or axis == \"index\"):\n            raise NotImplementedError(\"fillna currently only works for axis=0 or axis='index'\")\n        if (value is None) and (method is None):\n            raise ValueError(\"Must specify a fillna 'value' or 'method' parameter.\")\n        if (method is not None) and (method not in ['ffill', 'pad', 'backfill', 'bfill']):\n            raise ValueError(\"Expecting 'pad', 'ffill', 'backfill' or 'bfill'.\")\n        if self.isnull().sum() == 0:\n            if inplace:\n                self._internal = self._internal.copy()\n                self._kdf = self._kdf.copy()\n            else:\n                return self\n\n        column_name = self.name\n        scol = self._scol\n\n        if value is not None:\n            if not isinstance(value, (float, int, str, bool)):\n                raise TypeError(\"Unsupported type %s\" % type(value))\n            if limit is not None:\n                raise ValueError('limit parameter for value is not support now')\n            scol = F.when(scol.isNull(), value).otherwise(scol)\n        else:\n            if method in ['ffill', 'pad']:\n                func = F.last\n                end = (Window.currentRow - 1)\n                if limit is not None:\n                    begin = Window.currentRow - limit\n                else:\n                    begin = Window.unboundedPreceding\n            elif method in ['bfill', 'backfill']:\n                func = F.first\n                begin = Window.currentRow + 1\n                if limit is not None:\n                    end = Window.currentRow + limit\n                else:\n                    end = Window.unboundedFollowing\n\n            window = Window.partitionBy(*part_cols).orderBy(self._internal.index_scols)\\\n                .rowsBetween(begin, end)\n            scol = F.when(scol.isNull(), func(scol, True).over(window)).otherwise(scol)\n        kseries = self._with_new_scol(scol).rename(column_name)\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def dropna(self, axis=0, inplace=False, **kwargs):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        **kwargs\n            Not in use.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        Examples\n        --------\n        >>> ser = ks.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        Name: 0, dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        Name: 0, dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        Name: 0, dtype: float64\n        \"\"\"\n        # TODO: last two examples from Pandas produce different results.\n        kseries = _col(self.to_dataframe().dropna(axis=axis, inplace=False))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def clip(self, lower: Union[float, int] = None, upper: Union[float, int] = None) -> 'Series':\n        \"\"\"\n        Trim values at input threshold(s).\n\n        Assigns values outside boundary to boundary values.\n\n        Parameters\n        ----------\n        lower : float or int, default None\n            Minimum threshold value. All values below this threshold will be set to it.\n        upper : float or int, default None\n            Maximum threshold value. All values above this threshold will be set to it.\n\n        Returns\n        -------\n        Series\n            Series with the values outside the clip boundaries replaced\n\n        Examples\n        --------\n        >>> ks.Series([0, 2, 4]).clip(1, 3)\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        Notes\n        -----\n        One difference between this implementation and pandas is that running\n        `pd.Series(['a', 'b']).clip(0, 1)` will crash with \"TypeError: '<=' not supported between\n        instances of 'str' and 'int'\" while `ks.Series(['a', 'b']).clip(0, 1)` will output the\n        original Series, simply ignoring the incompatible types.\n        \"\"\"\n        return _col(self.to_dataframe().clip(lower, upper))\n\n    def drop(self,\n             labels=None,\n             index: Union[str, Tuple[str, ...], List[str], List[Tuple[str, ...]]] = None,\n             level=None):\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        index : None\n            Redundant for application on Series, but index can be used instead of labels.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        See Also\n        --------\n        Series.dropna\n\n        Examples\n        --------\n        >>> s = ks.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A    0\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        Drop single label A\n\n        >>> s.drop('A')\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        Drop labels B and C\n\n        >>> s.drop(labels=['B', 'C'])\n        A    0\n        Name: 0, dtype: int64\n\n        With 'index' rather than 'labels' returns exactly same result.\n\n        >>> s.drop(index='A')\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        >>> s.drop(index=['B', 'C'])\n        A    0\n        Name: 0, dtype: int64\n\n        Also support for MultiIndex\n\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop(('lama', 'weight'))\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop([('lama', 'speed'), ('falcon', 'weight')])\n        lama    weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        Name: 0, dtype: float64\n        \"\"\"\n        level_param = level\n        if labels is not None:\n            if index is not None:\n                raise ValueError(\"Cannot specify both 'labels' and 'index'\")\n            return self.drop(index=labels, level=level)\n        if index is not None:\n            if not isinstance(index, (str, tuple, list)):\n                raise ValueError(\"'index' type should be one of str, list, tuple\")\n            if level is None:\n                level = 0\n            if level >= len(self._internal.index_scols):\n                raise ValueError(\"'level' should be less than the number of indexes\")\n\n            if isinstance(index, str):\n                index = [(index,)]  # type: ignore\n            elif isinstance(index, tuple):\n                index = [index]\n            else:\n                if not (all((isinstance(idxes, str) for idxes in index)) or\n                        all((isinstance(idxes, tuple) for idxes in index))):\n                    raise ValueError(\"If the given index is a list, it \"\n                                     \"should only contains names as strings, \"\n                                     \"or a list of tuples that contain \"\n                                     \"index names as strings\")\n                index = [idxes if isinstance(idxes, tuple) else (idxes,)  # type: ignore\n                         for idxes in index]\n\n            drop_index_scols = []\n            for idxes in index:\n                try:\n                    index_scols = [self._internal.index_scols[lvl] == idx\n                                   for lvl, idx in enumerate(idxes, level)]\n                except IndexError:\n                    if level_param is None:\n                        raise KeyError(\"Key length ({}) exceeds index depth ({})\"\n                                       .format(len(self._internal.index_scols), len(idxes)))\n                    else:\n                        return self\n                drop_index_scols.append(reduce(lambda x, y: x & y, index_scols))\n\n            sdf = self._internal.sdf.where(~reduce(lambda x, y: x | y, drop_index_scols))\n            return _col(DataFrame(self._internal.copy(sdf=sdf)))\n        else:\n            raise ValueError(\"Need to specify at least one of 'labels' or 'index'\")\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first n rows.\n\n        This function returns the first n rows for the object based on position.\n        It is useful for quickly testing if your object has the right type of data in it.\n\n        Parameters\n        ----------\n        n : Integer, default =  5\n\n        Returns\n        -------\n        The first n rows of the caller object.\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'animal':['alligator', 'bee', 'falcon', 'lion']})\n        >>> df.animal.head(2)  # doctest: +NORMALIZE_WHITESPACE\n        0     alligator\n        1     bee\n        Name: animal, dtype: object\n        \"\"\"\n        return _col(self.to_dataframe().head(n))\n\n    # TODO: Categorical type isn't supported (due to PySpark's limitation) and\n    # some doctests related with timestamps were not added.\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        .. note:: This method returns newly creased Series whereas Pandas returns\n                  the unique values as a NumPy array.\n\n        Returns\n        -------\n        Returns the unique values as a Series.\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> kser = ks.Series([2, 1, 3, 3], name='A')\n        >>> kser.unique()\n        0    1\n        1    3\n        2    2\n        Name: A, dtype: int64\n\n        >>> ks.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        0   2016-01-01\n        Name: 0, dtype: datetime64[ns]\n\n        >>> kser.name = ('x', 'a')\n        >>> kser.unique()\n        0    1\n        1    3\n        2    2\n        Name: (x, a), dtype: int64\n        \"\"\"\n        sdf = self._internal.sdf.select(self._scol).distinct()\n        internal = _InternalFrame(sdf=sdf,\n                                  data_columns=[self._internal.data_columns[0]],\n                                  column_index=[self._internal.column_index[0]],\n                                  column_index_names=self._internal.column_index_names)\n        return _col(DataFrame(internal))\n\n    def nunique(self, dropna: bool = True, approx: bool = False, rsd: float = 0.05) -> int:\n        \"\"\"\n        Return number of unique elements in the object.\n\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don\u2019t include NaN in the count.\n        approx: bool, default False\n            If False, will use the exact algorithm and return the exact number of unique.\n            If True, it uses the HyperLogLog approximate algorithm, which is significantly faster\n            for large amount of data.\n            Note: This parameter is specific to Koalas and is not found in pandas.\n        rsd: float, default 0.05\n            Maximum estimation error allowed in the HyperLogLog algorithm.\n            Note: Just like ``approx`` this parameter is specific to Koalas.\n\n        Returns\n        -------\n        The number of unique values as an int.\n\n        Examples\n        --------\n        >>> ks.Series([1, 2, 3, np.nan]).nunique()\n        3\n\n        >>> ks.Series([1, 2, 3, np.nan]).nunique(dropna=False)\n        4\n\n        On big data, we recommend using the approximate algorithm to speed up this function.\n        The result will be very close to the exact unique count.\n\n        >>> ks.Series([1, 2, 3, np.nan]).nunique(approx=True)\n        3\n        \"\"\"\n        res = self._kdf._sdf.select([self._nunique(dropna, approx, rsd)])\n        return res.collect()[0][0]\n\n    def _nunique(self, dropna=True, approx=False, rsd=0.05):\n        name = self.name\n        count_fn = partial(F.approx_count_distinct, rsd=rsd) if approx else F.countDistinct\n        if dropna:\n            return count_fn(name).alias(name)\n        else:\n            return (count_fn(name) +\n                    F.when(F.count(F.when(self._internal.scol_for(name).isNull(), 1)\n                                   .otherwise(None)) >= 1, 1).otherwise(0)).alias(name)\n\n    # TODO: Update Documentation for Bins Parameter when its supported\n    def value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True):\n        \"\"\"\n        Return a Series containing counts of unique values.\n        The resulting object will be in descending order so that the\n        first element is the most frequently-occurring element.\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        normalize : boolean, default False\n            If True then the object returned will contain the relative\n            frequencies of the unique values.\n        sort : boolean, default True\n            Sort by values.\n        ascending : boolean, default False\n            Sort in ascending order.\n        bins : Not Yet Supported\n        dropna : boolean, default True\n            Don't include counts of NaN.\n\n        Returns\n        -------\n        counts : Series\n\n        See Also\n        --------\n        Series.count: Number of non-NA elements in a Series.\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'x':[0, 0, 1, 1, 1, np.nan]})\n        >>> df.x.value_counts()  # doctest: +NORMALIZE_WHITESPACE\n        1.0    3\n        0.0    2\n        Name: x, dtype: int64\n\n        With `normalize` set to `True`, returns the relative frequency by\n        dividing all values by the sum of values.\n\n        >>> df.x.value_counts(normalize=True)  # doctest: +NORMALIZE_WHITESPACE\n        1.0    0.6\n        0.0    0.4\n        Name: x, dtype: float64\n\n        **dropna**\n        With `dropna` set to `False` we can also see NaN index values.\n\n        >>> df.x.value_counts(dropna=False)  # doctest: +NORMALIZE_WHITESPACE\n        1.0    3\n        0.0    2\n        NaN    1\n        Name: x, dtype: int64\n        \"\"\"\n        if bins is not None:\n            raise NotImplementedError(\"value_counts currently does not support bins\")\n\n        if dropna:\n            sdf_dropna = self._kdf._sdf.filter(self.notna()._scol)\n        else:\n            sdf_dropna = self._kdf._sdf\n        index_name = SPARK_INDEX_NAME_FORMAT(0)\n        sdf = sdf_dropna.groupby(self._scol.alias(index_name)).count()\n        if sort:\n            if ascending:\n                sdf = sdf.orderBy(F.col('count'))\n            else:\n                sdf = sdf.orderBy(F.col('count').desc())\n\n        if normalize:\n            sum = sdf_dropna.count()\n            sdf = sdf.withColumn('count', F.col('count') / F.lit(sum))\n\n        internal = _InternalFrame(sdf=sdf,\n                                  data_columns=['count'],\n                                  index_map=[(index_name, None)],\n                                  column_index=self._internal.column_index,\n                                  column_index_names=self._internal.column_index_names)\n        return _col(DataFrame(internal))\n\n    def sort_values(self, ascending: bool = True, inplace: bool = False,\n                    na_position: str = 'last') -> Union['Series', None]:\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some criterion.\n\n        Parameters\n        ----------\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             if True, perform operation in-place\n        na_position : {'first', 'last'}, default 'last'\n             `first` puts NaNs at the beginning, `last` puts NaNs at the end\n\n        Returns\n        -------\n        sorted_obj : Series ordered by values.\n\n        Examples\n        --------\n        >>> s = ks.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3    10.0\n        4     5.0\n        Name: 0, dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        Name: 0, dtype: float64\n\n        Sort a series of strings\n\n        >>> s = ks.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        Name: 0, dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        Name: 0, dtype: object\n        \"\"\"\n        kseries = _col(self.to_dataframe().sort_values(by=self.name, ascending=ascending,\n                                                       na_position=na_position))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n            return None\n        else:\n            return kseries\n\n    def sort_index(self, axis: int = 0,\n                   level: Optional[Union[int, List[int]]] = None, ascending: bool = True,\n                   inplace: bool = False, kind: str = None, na_position: str = 'last') \\\n            -> Optional['Series']:\n        \"\"\"\n        Sort object by labels (along an axis)\n\n        Parameters\n        ----------\n        axis : index, columns to direct sorting. Currently, only axis = 0 is supported.\n        level : int or level name or list of ints or list of level names\n            if not None, sort on values in specified index level(s)\n        ascending : boolean, default True\n            Sort ascending vs. descending\n        inplace : bool, default False\n            if True, perform operation in-place\n        kind : str, default None\n            Koalas does not allow specifying the sorting algorithm at the moment, default None\n        na_position : {\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n            first puts NaNs at the beginning, last puts NaNs at the end. Not implemented for\n            MultiIndex.\n\n        Returns\n        -------\n        sorted_obj : Series\n\n        Examples\n        --------\n        >>> df = ks.Series([2, 1, np.nan], index=['b', 'a', np.nan])\n\n        >>> df.sort_index()\n        a      1.0\n        b      2.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(ascending=False)\n        b      2.0\n        a      1.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(na_position='first')\n        NaN    NaN\n        a      1.0\n        b      2.0\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(inplace=True)\n        >>> df\n        a      1.0\n        b      2.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df = ks.Series(range(4), index=[['b', 'b', 'a', 'a'], [1, 0, 1, 0]], name='0')\n\n        >>> df.sort_index()\n        a  0    3\n           1    2\n        b  0    1\n           1    0\n        Name: 0, dtype: int64\n\n        >>> df.sort_index(level=1)  # doctest: +SKIP\n        a  0    3\n        b  0    1\n        a  1    2\n        b  1    0\n        Name: 0, dtype: int64\n\n        >>> df.sort_index(level=[1, 0])\n        a  0    3\n        b  0    1\n        a  1    2\n        b  1    0\n        Name: 0, dtype: int64\n        \"\"\"\n        kseries = _col(self.to_dataframe().sort_index(axis=axis, level=level, ascending=ascending,\n                                                      kind=kind, na_position=na_position))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n            return None\n        else:\n            return kseries\n\n    def add_prefix(self, prefix):\n        \"\"\"\n        Prefix labels with string `prefix`.\n\n        For Series, the row labels are prefixed.\n        For DataFrame, the column labels are prefixed.\n\n        Parameters\n        ----------\n        prefix : str\n           The string to add before each label.\n\n        Returns\n        -------\n        Series\n           New Series with updated labels.\n\n        See Also\n        --------\n        Series.add_suffix: Suffix column labels with string `suffix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: 0, dtype: int64\n\n        >>> s.add_prefix('item_')\n        item_0    1\n        item_1    2\n        item_2    3\n        item_3    4\n        Name: 0, dtype: int64\n        \"\"\"\n        assert isinstance(prefix, str)\n        kdf = self.to_dataframe()\n        internal = kdf._internal\n        sdf = internal.sdf\n        sdf = sdf.select([F.concat(F.lit(prefix),\n                                   scol_for(sdf, index_column)).alias(index_column)\n                          for index_column in internal.index_columns] + internal.data_scols)\n        kdf._internal = internal.copy(sdf=sdf)\n        return _col(kdf)\n\n    def add_suffix(self, suffix):\n        \"\"\"\n        Suffix labels with string suffix.\n\n        For Series, the row labels are suffixed.\n        For DataFrame, the column labels are suffixed.\n\n        Parameters\n        ----------\n        suffix : str\n           The string to add after each label.\n\n        Returns\n        -------\n        Series\n           New Series with updated labels.\n\n        See Also\n        --------\n        Series.add_prefix: Prefix row labels with string `prefix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: 0, dtype: int64\n\n        >>> s.add_suffix('_item')\n        0_item    1\n        1_item    2\n        2_item    3\n        3_item    4\n        Name: 0, dtype: int64\n        \"\"\"\n        assert isinstance(suffix, str)\n        kdf = self.to_dataframe()\n        internal = kdf._internal\n        sdf = internal.sdf\n        sdf = sdf.select([F.concat(scol_for(sdf, index_column),\n                                   F.lit(suffix)).alias(index_column)\n                          for index_column in internal.index_columns] + internal.data_scols)\n        kdf._internal = internal.copy(sdf=sdf)\n        return _col(kdf)\n\n    def corr(self, other, method='pearson'):\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'spearman'}\n            * pearson : standard correlation coefficient\n            * spearman : Spearman rank correlation\n\n        Returns\n        -------\n        correlation : float\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'s1': [.2, .0, .6, .2],\n        ...                    's2': [.3, .6, .0, .1]})\n        >>> s1 = df.s1\n        >>> s2 = df.s2\n        >>> s1.corr(s2, method='pearson')  # doctest: +ELLIPSIS\n        -0.851064...\n\n        >>> s1.corr(s2, method='spearman')  # doctest: +ELLIPSIS\n        -0.948683...\n\n        Notes\n        -----\n        There are behavior differences between Koalas and pandas.\n\n        * the `method` argument only accepts 'pearson', 'spearman'\n        * the data should not contain NaNs. Koalas will return an error.\n        * Koalas doesn't support the following argument(s).\n\n          * `min_periods` argument is not supported\n        \"\"\"\n        # This implementation is suboptimal because it computes more than necessary,\n        # but it should be a start\n        df = self._kdf.assign(corr_arg1=self, corr_arg2=other)[[\"corr_arg1\", \"corr_arg2\"]]\n        c = df.corr(method=method)\n        return c.loc[\"corr_arg1\", \"corr_arg2\"]\n\n    def nsmallest(self, n: int = 5) -> 'Series':\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n        In Koalas, thanks to Spark's lazy execution and query optimizer,\n        the two would have same performance.\n\n        Examples\n        --------\n        >>> data = [1, 2, 3, 4, np.nan ,6, 7, 8]\n        >>> s = ks.Series(data)\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        6    7.0\n        7    8.0\n        Name: 0, dtype: float64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        5    6.0\n        Name: 0, dtype: float64\n\n        >>> s.nsmallest(3)\n        0    1.0\n        1    2.0\n        2    3.0\n        Name: 0, dtype: float64\n        \"\"\"\n        return _col(self.to_frame().nsmallest(n=n, columns=self.name))\n\n    def nlargest(self, n: int = 5) -> 'Series':\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        In Koalas, thanks to Spark's lazy execution and query optimizer,\n        the two would have same performance.\n\n        Examples\n        --------\n        >>> data = [1, 2, 3, 4, np.nan ,6, 7, 8]\n        >>> s = ks.Series(data)\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        6    7.0\n        7    8.0\n        Name: 0, dtype: float64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        7    8.0\n        6    7.0\n        5    6.0\n        3    4.0\n        2    3.0\n        Name: 0, dtype: float64\n\n        >>> s.nlargest(n=3)\n        7    8.0\n        6    7.0\n        5    6.0\n        Name: 0, dtype: float64\n\n\n        \"\"\"\n        return _col(self.to_frame().nlargest(n=n, columns=self.name))\n\n    def count(self):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Returns\n        -------\n        nobs : int\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = ks.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26]})\n\n        Notice the uncounted NA values:\n\n        >>> df['Person'].count()\n        5\n\n        >>> df['Age'].count()\n        4\n        \"\"\"\n        return self._reduce_for_stat_function(_Frame._count_expr, name=\"count\")\n\n    def append(self, to_append: 'Series', ignore_index: bool = False,\n               verify_integrity: bool = False) -> 'Series':\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n        ignore_index : boolean, default False\n            If True, do not use the index labels.\n        verify_integrity : boolean, default False\n            If True, raise Exception on creating index with duplicates\n\n        Returns\n        -------\n        appended : Series\n\n        Examples\n        --------\n        >>> s1 = ks.Series([1, 2, 3])\n        >>> s2 = ks.Series([4, 5, 6])\n        >>> s3 = ks.Series([4, 5, 6], index=[3,4,5])\n\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        Name: 0, dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        Name: 0, dtype: int64\n\n        With ignore_index set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        Name: 0, dtype: int64\n        \"\"\"\n        return _col(self.to_dataframe().append(to_append.to_dataframe(), ignore_index,\n                                               verify_integrity))\n\n    def sample(self, n: Optional[int] = None, frac: Optional[float] = None, replace: bool = False,\n               random_state: Optional[int] = None) -> 'Series':\n        return _col(self.to_dataframe().sample(\n            n=n, frac=frac, replace=replace, random_state=random_state))\n\n    sample.__doc__ = DataFrame.sample.__doc__\n\n    def hist(self, bins=10, **kwds):\n        return self.plot.hist(bins, **kwds)\n\n    hist.__doc__ = KoalasSeriesPlotMethods.hist.__doc__\n\n    def apply(self, func, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be a Python function that only works on the Series.\n\n        .. note:: this API executes the function once to infer the type which is\n             potentially expensive, for instance, when the dataset is created after\n             aggregations or sorting.\n\n             To avoid this, specify return type in ``func``, for instance, as below:\n\n             >>> def square(x) -> np.int32:\n             ...     return x ** 2\n\n             Koalas uses return type hint and does not try to infer the type.\n\n        Parameters\n        ----------\n        func : function\n            Python function to apply. Note that type hint for return type is required.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        Create a Series with typical summer temperatures for each city.\n\n        >>> s = ks.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        Name: 0, dtype: int64\n\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x) -> np.int64:\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        Name: 0, dtype: int64\n\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword\n\n        >>> def subtract_custom_value(x, custom_value) -> np.int64:\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        Name: 0, dtype: int64\n\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``\n\n        >>> def add_custom_values(x, **kwargs) -> np.int64:\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        Name: 0, dtype: int64\n\n\n        Use a function from the Numpy library\n\n        >>> def numpy_log(col) -> np.float64:\n        ...     return np.log(col)\n        >>> s.apply(numpy_log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        Name: 0, dtype: float64\n\n\n        You can omit the type hint and let Koalas infer its type.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        Name: 0, dtype: float64\n\n        \"\"\"\n        assert callable(func), \"the first argument should be a callable function.\"\n        try:\n            spec = inspect.getfullargspec(func)\n            return_sig = spec.annotations.get(\"return\", None)\n            should_infer_schema = return_sig is None\n        except TypeError:\n            # Falls back to schema inference if it fails to get signature.\n            should_infer_schema = True\n\n        apply_each = wraps(func)(lambda s, *a, **k: s.apply(func, args=a, **k))\n\n        if should_infer_schema:\n            # TODO: In this case, it avoids the shortcut for now (but only infers schema)\n            #  because it returns a series from a different DataFrame and it has a different\n            #  anchor. We should fix this to allow the shortcut or only allow to infer\n            #  schema.\n            limit = get_option(\"compute.shortcut_limit\")\n            pser = self.head(limit)._to_internal_pandas()\n            transformed = pser.apply(func, *args, **kwds)\n            kser = Series(transformed)\n\n            wrapped = ks.pandas_wraps(\n                return_col=as_python_type(kser.spark_type))(apply_each)\n        else:\n            wrapped = ks.pandas_wraps(return_col=return_sig)(apply_each)\n        return wrapped(self, *args, **kwds).rename(self.name)\n\n    # TODO: not all arguments are implemented comparing to Pandas' for now.\n    def aggregate(self, func: Union[str, List[str]]):\n        \"\"\"Aggregate using one or more operations over the specified axis.\n\n        Parameters\n        ----------\n        func : str or a list of str\n            function name(s) as string apply to series.\n\n        Returns\n        -------\n        scalar, Series\n            The return can be:\n            - scalar : when Series.agg is called with single function\n            - Series : when Series.agg is called with several functions\n\n        Notes\n        -----\n        `agg` is an alias for `aggregate`. Use the alias.\n\n        See Also\n        --------\n        databricks.koalas.Series.apply\n        databricks.koalas.Series.transform\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s.agg('min')\n        1\n\n        >>> s.agg(['min', 'max'])\n        max    4\n        min    1\n        Name: 0, dtype: int64\n        \"\"\"\n        if isinstance(func, list):\n            return self.to_frame().agg(func)[self.name]\n        elif isinstance(func, str):\n            return getattr(self, func)()\n        else:\n            raise ValueError(\"func must be a string or list of strings\")\n\n    agg = aggregate\n\n    def transpose(self, *args, **kwargs):\n        \"\"\"\n        Return the transpose, which is by definition self.\n\n        Examples\n        --------\n        It returns the same object as the transpose of the given series object, which is by\n        definition self.\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        >>> s.transpose()\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n        \"\"\"\n        return Series(self._internal.copy(), anchor=self._kdf)\n\n    T = property(transpose)\n\n    def transform(self, func, *args, **kwargs):\n        \"\"\"\n        Call ``func`` producing the same type as `self` with transformed values\n        and that has the same axis length as input.\n\n        .. note:: this API executes the function once to infer the type which is\n             potentially expensive, for instance, when the dataset is created after\n             aggregations or sorting.\n\n             To avoid this, specify return type in ``func``, for instance, as below:\n\n             >>> def square(x) -> np.int32:\n             ...     return x ** 2\n\n             Koalas uses return type hint and does not try to infer the type.\n\n        Parameters\n        ----------\n        func : function or list\n            A function or a list of functions to use for transforming the data.\n        *args\n            Positional arguments to pass to `func`.\n        **kwargs\n            Keyword arguments to pass to `func`.\n\n        Returns\n        -------\n        An instance of the same type with `self` that must have the same length as input.\n\n        See Also\n        --------\n        Series.apply : Invoke function on Series.\n\n        Examples\n        --------\n\n        >>> s = ks.Series(range(3))\n        >>> s\n        0    0\n        1    1\n        2    2\n        Name: 0, dtype: int64\n\n        >>> def sqrt(x) -> float:\n        ...    return np.sqrt(x)\n        >>> s.transform(sqrt)\n        0    0.000000\n        1    1.000000\n        2    1.414214\n        Name: 0, dtype: float32\n\n        Even though the resulting instance must have the same length as the\n        input, it is possible to provide several input functions:\n\n        >>> def exp(x) -> float:\n        ...    return np.exp(x)\n        >>> s.transform([sqrt, exp])\n               sqrt       exp\n        0  0.000000  1.000000\n        1  1.000000  2.718282\n        2  1.414214  7.389056\n\n        You can omit the type hint and let Koalas infer its type.\n\n        >>> s.transform([np.sqrt, np.exp])\n               sqrt       exp\n        0  0.000000  1.000000\n        1  1.000000  2.718282\n        2  1.414214  7.389056\n        \"\"\"\n        if isinstance(func, list):\n            applied = []\n            for f in func:\n                applied.append(self.apply(f, args=args, **kwargs).rename(f.__name__))\n\n            sdf = self._kdf._sdf.select(\n                self._internal.index_scols + [c._scol for c in applied])\n\n            internal = self.to_dataframe()._internal.copy(\n                sdf=sdf,\n                data_columns=[c._internal.data_columns[0] for c in applied],\n                column_index=[c._internal.column_index[0] for c in applied],\n                column_index_names=None)\n\n            return DataFrame(internal)\n        else:\n            return self.apply(func, args=args, **kwargs)\n\n    def round(self, decimals=0):\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Series object\n\n        See Also\n        --------\n        DataFrame.round\n\n        Examples\n        --------\n        >>> df = ks.Series([0.028208, 0.038683, 0.877076], name='x')\n        >>> df\n        0    0.028208\n        1    0.038683\n        2    0.877076\n        Name: x, dtype: float64\n\n        >>> df.round(2)\n        0    0.03\n        1    0.04\n        2    0.88\n        Name: x, dtype: float64\n        \"\"\"\n        if not isinstance(decimals, int):\n            raise ValueError(\"decimals must be an integer\")\n        column_name = self.name\n        scol = F.round(self._scol, decimals)\n        return self._with_new_scol(scol).rename(column_name)\n\n    # TODO: add 'interpolation' parameter.\n    def quantile(self, q=0.5, accuracy=10000):\n        \"\"\"\n        Return value at the given quantile.\n\n        .. note:: Unlike pandas', the quantile in Koalas is an approximated quantile based upon\n            approximate percentile computation because computing quantile across a large dataset\n            is extremely expensive.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            0 <= q <= 1, the quantile(s) to compute.\n        accuracy : int, optional\n            Default accuracy of approximation. Larger value means better accuracy.\n            The relative error can be deduced by 1.0 / accuracy.\n\n        Returns\n        -------\n        float or Series\n            If the current object is a Series and ``q`` is an array, a Series will be\n            returned where the index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4, 5])\n        >>> s.quantile(.5)\n        3\n\n        >>> s.quantile([.25, .5, .75])\n        0.25    2\n        0.5     3\n        0.75    4\n        Name: 0, dtype: int64\n        \"\"\"\n        if not isinstance(accuracy, int):\n            raise ValueError(\"accuracy must be an integer; however, got [%s]\" % type(accuracy))\n\n        if isinstance(q, Iterable):\n            q = list(q)\n\n        for v in q if isinstance(q, list) else [q]:\n            if not isinstance(v, float):\n                raise ValueError(\n                    \"q must be a float of an array of floats; however, [%s] found.\" % type(v))\n            if v < 0.0 or v > 1.0:\n                raise ValueError(\n                    \"percentiles should all be in the interval [0, 1].\")\n\n        if isinstance(q, list):\n            quantiles = q\n            # TODO: avoid to use dataframe. After this, anchor will be lost.\n\n            # First calculate the percentiles and map it to each `quantiles`\n            # by creating each entry as a struct. So, it becomes an array of\n            # structs as below:\n            #\n            # +--------------------------------+\n            # | arrays                         |\n            # +--------------------------------+\n            # |[[0.25, 2], [0.5, 3], [0.75, 4]]|\n            # +--------------------------------+\n            sdf = self._kdf._sdf\n            args = \", \".join(map(str, quantiles))\n            percentile_col = F.expr(\n                \"approx_percentile(`%s`, array(%s), %s)\" % (self.name, args, accuracy))\n            sdf = sdf.select(percentile_col.alias(\"percentiles\"))\n\n            internal_index_column = SPARK_INDEX_NAME_FORMAT(0)\n            value_column = \"value\"\n            cols = []\n            for i, quantile in enumerate(quantiles):\n                cols.append(F.struct(\n                    F.lit(\"%s\" % quantile).alias(internal_index_column),\n                    F.expr(\"percentiles[%s]\" % i).alias(value_column)))\n            sdf = sdf.select(F.array(*cols).alias(\"arrays\"))\n\n            # And then, explode it and manually set the index.\n            #\n            # +-----------------+-----+\n            # |__index_level_0__|value|\n            # +-----------------+-----+\n            # | 0.25            |    2|\n            # |  0.5            |    3|\n            # | 0.75            |    4|\n            # +-----------------+-----+\n            sdf = sdf.select(F.explode(F.col(\"arrays\"))).selectExpr(\"col.*\")\n\n            internal = self._kdf._internal.copy(\n                sdf=sdf,\n                data_columns=[value_column],\n                index_map=[(internal_index_column, None)],\n                column_index=None,\n                column_index_names=None)\n\n            return DataFrame(internal)[value_column].rename(self.name)\n        else:\n            return self._reduce_for_stat_function(\n                lambda _: F.expr(\"approx_percentile(`%s`, %s, %s)\" % (self.name, q, accuracy)),\n                name=\"median\")\n\n    # TODO: add axis, numeric_only, pct, na_option parameter\n    def rank(self, method='average', ascending=True):\n        \"\"\"\n        Compute numerical data ranks (1 through n) along axis. Equal values are\n        assigned a rank that is the average of the ranks of those values.\n\n        .. note:: the current implementation of rank uses Spark's Window without\n            specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}\n            * average: average rank of group\n            * min: lowest rank in group\n            * max: highest rank in group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n\n        Returns\n        -------\n        ranks : same type as caller\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'A': [1, 2, 2, 3], 'B': [4, 3, 2, 1]}, columns= ['A', 'B'])\n        >>> df\n           A  B\n        0  1  4\n        1  2  3\n        2  2  2\n        3  3  1\n\n        >>> df.rank().sort_index()\n             A    B\n        0  1.0  4.0\n        1  2.5  3.0\n        2  2.5  2.0\n        3  4.0  1.0\n\n        If method is set to 'min', it use lowest rank in group.\n\n        >>> df.rank(method='min').sort_index()\n             A    B\n        0  1.0  4.0\n        1  2.0  3.0\n        2  2.0  2.0\n        3  4.0  1.0\n\n        If method is set to 'max', it use highest rank in group.\n\n        >>> df.rank(method='max').sort_index()\n             A    B\n        0  1.0  4.0\n        1  3.0  3.0\n        2  3.0  2.0\n        3  4.0  1.0\n\n        If method is set to 'dense', it leaves no gaps in group.\n\n        >>> df.rank(method='dense').sort_index()\n             A    B\n        0  1.0  4.0\n        1  2.0  3.0\n        2  2.0  2.0\n        3  3.0  1.0\n        \"\"\"\n        return self._rank(method, ascending)\n\n    def _rank(self, method='average', ascending=True, part_cols=()):\n        if method not in ['average', 'min', 'max', 'first', 'dense']:\n            msg = \"method must be one of 'average', 'min', 'max', 'first', 'dense'\"\n            raise ValueError(msg)\n\n        if len(self._internal.index_columns) > 1:\n            raise ValueError('rank do not support index now')\n\n        if ascending:\n            asc_func = spark.functions.asc\n        else:\n            asc_func = spark.functions.desc\n\n        index_column = self._internal.index_columns[0]\n        column_name = self._internal.data_columns[0]\n\n        if method == 'first':\n            window = Window.orderBy(\n                asc_func(column_name), asc_func(index_column)\n            ).partitionBy(*part_cols).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            scol = F.row_number().over(window)\n        elif method == 'dense':\n            window = Window.orderBy(asc_func(column_name)).partitionBy(*part_cols) \\\n                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            scol = F.dense_rank().over(window)\n        else:\n            if method == 'average':\n                stat_func = F.mean\n            elif method == 'min':\n                stat_func = F.min\n            elif method == 'max':\n                stat_func = F.max\n            window1 = Window.orderBy(\n                asc_func(column_name)\n            ).partitionBy(*part_cols).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            window2 = Window.partitionBy(\n                *[column_name] + list(part_cols)\n            ).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n            scol = stat_func(F.row_number().over(window1)).over(window2)\n        kser = self._with_new_scol(scol).rename(self.name)\n        return kser.astype(np.float64)\n\n    def describe(self, percentiles: Optional[List[float]] = None) -> 'Series':\n        return _col(self.to_dataframe().describe(percentiles))\n\n    describe.__doc__ = DataFrame.describe.__doc__\n\n    def diff(self, periods=1):\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another element in the\n        DataFrame (default is the element in the same column of the previous row).\n\n        .. note:: the current implementation of diff uses Spark's Window without\n            specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative values.\n\n        Returns\n        -------\n        diffed : DataFrame\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]}, columns=['a', 'b', 'c'])\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.b.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        Name: b, dtype: float64\n\n        Difference with previous value\n\n        >>> df.c.diff(periods=3)\n        0     NaN\n        1     NaN\n        2     NaN\n        3    15.0\n        4    21.0\n        5    27.0\n        Name: c, dtype: float64\n\n        Difference with following value\n\n        >>> df.c.diff(periods=-1)\n        0    -3.0\n        1    -5.0\n        2    -7.0\n        3    -9.0\n        4   -11.0\n        5     NaN\n        Name: c, dtype: float64\n        \"\"\"\n        return self._diff(periods)\n\n    def _diff(self, periods, part_cols=()):\n        if not isinstance(periods, int):\n            raise ValueError('periods should be an int; however, got [%s]' % type(periods))\n        window = Window.partitionBy(*part_cols).orderBy(self._internal.index_scols)\\\n            .rowsBetween(-periods, -periods)\n        scol = self._scol - F.lag(self._scol, periods).over(window)\n        return self._with_new_scol(scol).rename(self.name)\n\n    def idxmax(self, skipna=True):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Examples\n        --------\n        >>> s = ks.Series(data=[1, None, 4, 3, 5],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    5.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmax()\n        'E'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n\n        In case of multi-index, you get a tuple:\n\n        >>> index = pd.MultiIndex.from_arrays([\n        ...     ['a', 'a', 'b', 'b'], ['c', 'd', 'e', 'f']], names=('first', 'second'))\n        >>> s = ks.Series(data=[1, None, 4, 5], index=index)\n        >>> s\n        first  second\n        a      c         1.0\n               d         NaN\n        b      e         4.0\n               f         5.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmax()\n        ('b', 'f')\n        \"\"\"\n        sdf = self._kdf._sdf\n        scol = self._scol\n        index_scols = self._kdf._internal.index_scols\n        # desc_nulls_(last|first) is used via Py4J directly because\n        # it's not supported in Spark 2.3.\n        if skipna:\n            sdf = sdf.orderBy(Column(scol._jc.desc_nulls_last()))\n        else:\n            sdf = sdf.orderBy(Column(scol._jc.desc_nulls_first()))\n        results = sdf.select([scol] + index_scols).take(1)\n        if len(results) == 0:\n            raise ValueError(\"attempt to get idxmin of an empty sequence\")\n        if results[0][0] is None:\n            # This will only happens when skipna is False because we will\n            # place nulls first.\n            return np.nan\n        values = list(results[0][1:])\n        if len(values) == 1:\n            return values[0]\n        else:\n            return tuple(values)\n\n    def idxmin(self, skipna=True):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = ks.Series(data=[1, None, 4, 0],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    0.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmin()\n        'D'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n\n        In case of multi-index, you get a tuple:\n\n        >>> index = pd.MultiIndex.from_arrays([\n        ...     ['a', 'a', 'b', 'b'], ['c', 'd', 'e', 'f']], names=('first', 'second'))\n        >>> s = ks.Series(data=[1, None, 4, 0], index=index)\n        >>> s\n        first  second\n        a      c         1.0\n               d         NaN\n        b      e         4.0\n               f         0.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmin()\n        ('b', 'f')\n        \"\"\"\n        sdf = self._kdf._sdf\n        scol = self._scol\n        index_scols = self._kdf._internal.index_scols\n        # asc_nulls_(list|first)is used via Py4J directly because\n        # it's not supported in Spark 2.3.\n        if skipna:\n            sdf = sdf.orderBy(Column(scol._jc.asc_nulls_last()))\n        else:\n            sdf = sdf.orderBy(Column(scol._jc.asc_nulls_first()))\n        results = sdf.select([scol] + index_scols).take(1)\n        if len(results) == 0:\n            raise ValueError(\"attempt to get idxmin of an empty sequence\")\n        if results[0][0] is None:\n            # This will only happens when skipna is False because we will\n            # place nulls first.\n            return np.nan\n        values = list(results[0][1:])\n        if len(values) == 1:\n            return values[0]\n        else:\n            return tuple(values)\n\n    def copy(self) -> 'Series':\n        \"\"\"\n        Make a copy of this object's indices and data.\n\n        Returns\n        -------\n        copy : Series\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2], index=[\"a\", \"b\"])\n        >>> s\n        a    1\n        b    2\n        Name: 0, dtype: int64\n        >>> s_copy = s.copy()\n        >>> s_copy\n        a    1\n        b    2\n        Name: 0, dtype: int64\n        \"\"\"\n        return _col(DataFrame(self._internal.copy()))\n\n    # TODO: 'regex', 'method' parameter\n    def replace(self, to_replace=None, value=None, regex=False) -> 'Series':\n        \"\"\"\n        Replace values given in to_replace with value.\n        Values of the Series are replaced with other values dynamically.\n\n        Parameters\n        ----------\n        to_replace : str, list, dict, Series, int, float, or None\n            How to find the values that will be replaced.\n            * numeric, str:\n\n                - numeric: numeric values equal to to_replace will be replaced with value\n                - str: string exactly matching to_replace will be replaced with value\n\n            * list of str or numeric:\n\n                - if to_replace and value are both lists, they must be the same length.\n                - str and numeric rules apply as above.\n\n            * dict:\n\n                - Dicts can be used to specify different replacement values for different\n                  existing values.\n                  For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019\n                  with \u2018z\u2019. To use a dict in this way the value parameter should be None.\n                - For a DataFrame a dict can specify that different values should be replaced\n                  in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1\n                  in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with\n                  whatever is specified in value.\n                  The value parameter should not be None in this case.\n                  You can treat this as a special case of passing two lists except that you are\n                  specifying the column to search in.\n\n            See the examples section for examples of each of these.\n\n        value : scalar, dict, list, str default None\n            Value to replace any values matching to_replace with.\n            For a DataFrame a dict of values can be used to specify which value to use\n            for each column (columns not in the dict will not be filled).\n            Regular expressions, strings and lists or dicts of such objects are also allowed.\n\n        Returns\n        -------\n        Series\n            Object after replacement.\n\n        Examples\n        --------\n\n        Scalar `to_replace` and `value`\n\n        >>> s = ks.Series([0, 1, 2, 3, 4])\n        >>> s\n        0    0\n        1    1\n        2    2\n        3    3\n        4    4\n        Name: 0, dtype: int64\n\n        >>> s.replace(0, 5)\n        0    5\n        1    1\n        2    2\n        3    3\n        4    4\n        Name: 0, dtype: int64\n\n        List-like `to_replace`\n\n        >>> s.replace([0, 4], 5000)\n        0    5000\n        1       1\n        2       2\n        3       3\n        4    5000\n        Name: 0, dtype: int64\n\n        >>> s.replace([1, 2, 3], [10, 20, 30])\n        0     0\n        1    10\n        2    20\n        3    30\n        4     4\n        Name: 0, dtype: int64\n\n        Dict-like `to_replace`\n\n        >>> s.replace({1: 1000, 2: 2000, 3: 3000, 4: 4000})\n        0       0\n        1    1000\n        2    2000\n        3    3000\n        4    4000\n        Name: 0, dtype: int64\n\n        Also support for MultiIndex\n\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace(45, 450)\n        lama    speed     450.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace([45, 30, 320], 500)\n        lama    speed     500.0\n                weight    200.0\n                length      1.2\n        cow     speed     500.0\n                weight    250.0\n                length      1.5\n        falcon  speed     500.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace({45: 450, 30: 300})\n        lama    speed     450.0\n                weight    200.0\n                length      1.2\n        cow     speed     300.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n        \"\"\"\n        if to_replace is None:\n            return self\n        if not isinstance(to_replace, (str, list, dict, int, float)):\n            raise ValueError(\n                \"'to_replace' should be one of str, list, dict, int, float\")\n        if regex:\n            raise NotImplementedError(\"replace currently not support for regex\")\n        if isinstance(to_replace, list) and isinstance(value, list):\n            if not len(to_replace) == len(value):\n                raise ValueError(\"Replacement lists must match in length. Expecting {} got {}\"\n                                 .format(len(to_replace), len(value)))\n            to_replace = {k: v for k, v in zip(to_replace, value)}\n        if isinstance(to_replace, dict):\n            is_start = True\n            if len(to_replace) == 0:\n                current = self._scol\n            else:\n                for to_replace_, value in to_replace.items():\n                    if is_start:\n                        current = F.when(self._scol == F.lit(to_replace_), value)\n                        is_start = False\n                    else:\n                        current = current.when(self._scol == F.lit(to_replace_), value)\n                current = current.otherwise(self._scol)\n        else:\n            current = F.when(self._scol.isin(to_replace), value).otherwise(self._scol)\n\n        return self._with_new_scol(current)\n\n    def _cum(self, func, skipna, part_cols=()):\n        # This is used to cummin, cummax, cumsum, etc.\n        index_columns = self._internal.index_columns\n        window = Window.orderBy(\n            index_columns).partitionBy(*part_cols).rowsBetween(\n                Window.unboundedPreceding, Window.currentRow)\n\n        if skipna:\n            # There is a behavior difference between pandas and PySpark. In case of cummax,\n            #\n            # Input:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  1.0  0.0\n            # 3  2.0  4.0\n            # 4  4.0  9.0\n            #\n            # pandas:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n            #\n            # PySpark:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  1.0\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n\n            scol = F.when(\n                # Manually sets nulls given the column defined above.\n                self._scol.isNull(), F.lit(None)\n            ).otherwise(func(self._scol).over(window))\n        else:\n            # Here, we use two Windows.\n            # One for real data.\n            # The other one for setting nulls after the first null it meets.\n            #\n            # There is a behavior difference between pandas and PySpark. In case of cummax,\n            #\n            # Input:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  1.0  0.0\n            # 3  2.0  4.0\n            # 4  4.0  9.0\n            #\n            # pandas:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  5.0  NaN\n            # 3  5.0  NaN\n            # 4  5.0  NaN\n            #\n            # PySpark:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  1.0\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n            scol = F.when(\n                # By going through with max, it sets True after the first time it meets null.\n                F.max(self._scol.isNull()).over(window),\n                # Manually sets nulls given the column defined above.\n                F.lit(None)\n            ).otherwise(func(self._scol).over(window))\n\n        # cumprod uses exp(sum(log(...))) trick.\n        if func.__name__ == \"cumprod\":\n            scol = F.exp(scol)\n\n        return self._with_new_scol(scol).rename(self.name)\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    dt = CachedAccessor(\"dt\", DatetimeMethods)\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # ----------------------------------------------------------------------\n\n    def _reduce_for_stat_function(self, sfun, name, axis=None, numeric_only=None):\n        \"\"\"\n        Applies sfun to the column and returns a scalar\n\n        Parameters\n        ----------\n        sfun : the stats function to be used for aggregation\n        name : original pandas API name.\n        axis : used only for sanity check because series only support index axis.\n        numeric_only : not used by this implementation, but passed down by stats functions\n        \"\"\"\n        from inspect import signature\n        if axis in ('columns', 1):\n            raise ValueError(\"Series does not support columns axis.\")\n        num_args = len(signature(sfun).parameters)\n        col_sdf = self._scol\n        col_type = self.spark_type\n        if isinstance(col_type, BooleanType) and sfun.__name__ not in ('min', 'max'):\n            # Stat functions cannot be used with boolean values by default\n            # Thus, cast to integer (true to 1 and false to 0)\n            # Exclude the min and max methods though since those work with booleans\n            col_sdf = col_sdf.cast('integer')\n        if num_args == 1:\n            # Only pass in the column if sfun accepts only one arg\n            col_sdf = sfun(col_sdf)\n        else:  # must be 2\n            assert num_args == 2\n            # Pass in both the column and its data type if sfun accepts two args\n            col_sdf = sfun(col_sdf, col_type)\n        return _unpack_scalar(self._kdf._sdf.select(col_sdf))\n\n    def __len__(self):\n        return len(self.to_dataframe())\n\n    def __getitem__(self, key):\n        return self._with_new_scol(self._scol.__getitem__(key))\n\n    def __getattr__(self, item: str_type) -> Any:\n        if item.startswith(\"__\") or item.startswith(\"_pandas_\") or item.startswith(\"_spark_\"):\n            raise AttributeError(item)\n        if hasattr(_MissingPandasLikeSeries, item):\n            property_or_func = getattr(_MissingPandasLikeSeries, item)\n            if isinstance(property_or_func, property):\n                return property_or_func.fget(self)  # type: ignore\n            else:\n                return partial(property_or_func, self)\n        return self.getField(item)\n\n    def __str__(self):\n        return self._pandas_orig_repr()\n\n    def _to_internal_pandas(self):\n        \"\"\"\n        Return a pandas Series directly from _internal to avoid overhead of copy.\n\n        This method is for internal use only.\n        \"\"\"\n        return _col(self._internal.pandas_df)\n\n    def __repr__(self):\n        max_display_count = get_option(\"display.max_rows\")\n        if max_display_count is None:\n            return self._to_internal_pandas().to_string(name=self.name, dtype=self.dtype)\n\n        pser = self.head(max_display_count + 1)._to_internal_pandas()\n        pser_length = len(pser)\n        pser = pser.iloc[:max_display_count]\n        if pser_length > max_display_count:\n            repr_string = pser.to_string(length=True)\n            rest, prev_footer = repr_string.rsplit(\"\\n\", 1)\n            match = REPR_PATTERN.search(prev_footer)\n            if match is not None:\n                length = match.group(\"length\")\n                name = str(self.dtype.name)\n                footer = (\"\\nName: {name}, dtype: {dtype}\\nShowing only the first {length}\"\n                          .format(length=length, name=self.name, dtype=pprint_thing(name)))\n                return rest + footer\n        return pser.to_string(name=self.name, dtype=self.dtype)\n\n    def __dir__(self):\n        if not isinstance(self.schema, StructType):\n            fields = []\n        else:\n            fields = [f for f in self.schema.fieldNames() if ' ' not in f]\n        return super(Series, self).__dir__() + fields\n\n    def __iter__(self):\n        return _MissingPandasLikeSeries.__iter__(self)\n\n    def _pandas_orig_repr(self):\n        # TODO: figure out how to reuse the original one.\n        return 'Column<%s>' % self._scol._jc.toString().encode('utf8')\n\n    def _equals(self, other: 'Series') -> bool:\n        return self._scol._jc.equals(other._scol._jc)\n\n\ndef _unpack_scalar(sdf):\n    \"\"\"\n    Takes a dataframe that is supposed to contain a single row with a single scalar value,\n    and returns this value.\n    \"\"\"\n    l = sdf.head(2)\n    assert len(l) == 1, (sdf, l)\n    row = l[0]\n    l2 = list(row.asDict().values())\n    assert len(l2) == 1, (row, l2)\n    return l2[0]\n\n\ndef _col(df):\n    assert isinstance(df, (DataFrame, pd.DataFrame)), type(df)\n    return df[df.columns[0]]\n", "idx": 1, "id": 12366, "msg": "Shall we have an example? The example in the PR description should be fine.", "proj": "databricks-koalas", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -56,6 +56,7 @@ type serverConfig struct {\n \tSVIDTTL             string             `hcl:\"svid_ttl\"`\n \tTrustDomain         string             `hcl:\"trust_domain\"`\n \tUpstreamBundle      bool               `hcl:\"upstream_bundle\"`\n+\tRegistrationPruning string             `hcl:\"registration_pruning\"`\n \n \tConfigPath string\n ", "y": 1, "oldf": "package run\n\nimport (\n\t\"context\"\n\t\"crypto/x509/pkix\"\n\t\"errors\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/hashicorp/hcl\"\n\t\"github.com/imdario/mergo\"\n\t\"github.com/spiffe/spire/pkg/common/catalog\"\n\t\"github.com/spiffe/spire/pkg/common/cli\"\n\t\"github.com/spiffe/spire/pkg/common/health\"\n\t\"github.com/spiffe/spire/pkg/common/idutil\"\n\t\"github.com/spiffe/spire/pkg/common/log\"\n\t\"github.com/spiffe/spire/pkg/common/telemetry\"\n\t\"github.com/spiffe/spire/pkg/common/util\"\n\t\"github.com/spiffe/spire/pkg/server\"\n\tbundleClient \"github.com/spiffe/spire/pkg/server/bundle/client\"\n\t\"github.com/spiffe/spire/pkg/server/endpoints/bundle\"\n)\n\nconst (\n\tdefaultConfigPath         = \"conf/server/server.conf\"\n\tdefaultSocketPath         = \"/tmp/spire-registration.sock\"\n\tdefaultLogLevel           = \"INFO\"\n\tdefaultBundleEndpointPort = 443\n)\n\n// config contains all available configurables, arranged by section\ntype config struct {\n\tServer       *serverConfig               `hcl:\"server\"`\n\tPlugins      *catalog.HCLPluginConfigMap `hcl:\"plugins\"`\n\tTelemetry    telemetry.FileConfig        `hcl:\"telemetry\"`\n\tHealthChecks health.Config               `hcl:\"health_checks\"`\n}\n\ntype serverConfig struct {\n\tBindAddress         string             `hcl:\"bind_address\"`\n\tBindPort            int                `hcl:\"bind_port\"`\n\tCASubject           *caSubjectConfig   `hcl:\"ca_subject\"`\n\tCATTL               string             `hcl:\"ca_ttl\"`\n\tDataDir             string             `hcl:\"data_dir\"`\n\tExperimental        experimentalConfig `hcl:\"experimental\"`\n\tLogFile             string             `hcl:\"log_file\"`\n\tLogLevel            string             `hcl:\"log_level\"`\n\tLogFormat           string             `hcl:\"log_format\"`\n\tRegistrationUDSPath string             `hcl:\"registration_uds_path\"`\n\tSVIDTTL             string             `hcl:\"svid_ttl\"`\n\tTrustDomain         string             `hcl:\"trust_domain\"`\n\tUpstreamBundle      bool               `hcl:\"upstream_bundle\"`\n\n\tConfigPath string\n\n\t// Undocumented configurables\n\tProfilingEnabled bool     `hcl:\"profiling_enabled\"`\n\tProfilingPort    int      `hcl:\"profiling_port\"`\n\tProfilingFreq    int      `hcl:\"profiling_freq\"`\n\tProfilingNames   []string `hcl:\"profiling_names\"`\n}\n\ntype experimentalConfig struct {\n\tAllowAgentlessNodeAttestors bool `hcl:\"allow_agentless_node_attestors\"`\n\n\tBundleEndpointEnabled bool                           `hcl:\"bundle_endpoint_enabled\"`\n\tBundleEndpointAddress string                         `hcl:\"bundle_endpoint_address\"`\n\tBundleEndpointPort    int                            `hcl:\"bundle_endpoint_port\"`\n\tBundleEndpointACME    *bundleEndpointACMEConfig      `hcl:\"bundle_endpoint_acme\"`\n\tFederatesWith         map[string]federatesWithConfig `hcl:\"federates_with\"`\n}\n\ntype caSubjectConfig struct {\n\tCountry      []string `hcl:\"country\"`\n\tOrganization []string `hcl:\"organization\"`\n\tCommonName   string   `hcl:\"common_name\"`\n}\n\ntype bundleEndpointACMEConfig struct {\n\tDirectoryURL string `hcl:\"directory_url\"`\n\tDomainName   string `hcl:\"domain_name\"`\n\tEmail        string `hcl:\"email\"`\n\tToSAccepted  bool   `hcl:\"tos_accepted\"`\n}\n\ntype federatesWithConfig struct {\n\tBundleEndpointAddress  string `hcl:\"bundle_endpoint_address\"`\n\tBundleEndpointPort     int    `hcl:\"bundle_endpoint_port\"`\n\tBundleEndpointSpiffeID string `hcl:\"bundle_endpoint_spiffe_id\"`\n}\n\n// Run CLI struct\ntype RunCLI struct{}\n\n//Help prints the server cmd usage\nfunc (*RunCLI) Help() string {\n\t_, err := parseFlags([]string{\"-h\"})\n\treturn err.Error()\n}\n\n// Run the SPIFFE Server\nfunc (*RunCLI) Run(args []string) int {\n\t// First parse the CLI flags so we can get the config\n\t// file path, if set\n\tcliInput, err := parseFlags(args)\n\tif err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\treturn 1\n\t}\n\n\t// Load and parse the config file using either the default\n\t// path or CLI-specified value\n\tfileInput, err := parseFile(cliInput.ConfigPath)\n\tif err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\treturn 1\n\t}\n\n\tinput, err := mergeInput(fileInput, cliInput)\n\tif err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\treturn 1\n\t}\n\n\tc, err := newServerConfig(input)\n\tif err != nil {\n\t\tfmt.Fprintln(os.Stderr, err)\n\t\treturn 1\n\t}\n\n\t// Set umask before starting up the server\n\tcli.SetUmask(c.Log)\n\n\ts := server.New(*c)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tutil.SignalListener(ctx, cancel)\n\n\terr = s.Run(ctx)\n\tif err != nil {\n\t\tc.Log.WithError(err).Error(\"server crashed\")\n\t\treturn 1\n\t}\n\n\tc.Log.Info(\"Server stopped gracefully\")\n\treturn 0\n}\n\n//Synopsis of the command\nfunc (*RunCLI) Synopsis() string {\n\treturn \"Runs the server\"\n}\n\nfunc parseFile(path string) (*config, error) {\n\tc := &config{}\n\n\tif path == \"\" {\n\t\tpath = defaultConfigPath\n\t}\n\n\t// Return a friendly error if the file is missing\n\tdata, err := ioutil.ReadFile(path)\n\tif os.IsNotExist(err) {\n\t\tabsPath, err := filepath.Abs(path)\n\t\tif err != nil {\n\t\t\tmsg := \"could not determine CWD; config file not found at %s: use -config\"\n\t\t\treturn nil, fmt.Errorf(msg, path)\n\t\t}\n\n\t\tmsg := \"could not find config file %s: please use the -config flag\"\n\t\treturn nil, fmt.Errorf(msg, absPath)\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to read configuration at %q: %v\", path, err)\n\t}\n\n\tif err := hcl.Decode(&c, string(data)); err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to decode configuration %q: %v\", path, err)\n\t}\n\n\treturn c, nil\n}\n\nfunc parseFlags(args []string) (*serverConfig, error) {\n\tflags := flag.NewFlagSet(\"run\", flag.ContinueOnError)\n\tc := &serverConfig{}\n\n\tflags.StringVar(&c.BindAddress, \"bindAddress\", \"\", \"IP address or DNS name of the SPIRE server\")\n\tflags.IntVar(&c.BindPort, \"serverPort\", 0, \"Port number of the SPIRE server\")\n\tflags.StringVar(&c.ConfigPath, \"config\", \"\", \"Path to a SPIRE config file\")\n\tflags.StringVar(&c.DataDir, \"dataDir\", \"\", \"Directory to store runtime data to\")\n\tflags.StringVar(&c.LogFile, \"logFile\", \"\", \"File to write logs to\")\n\tflags.StringVar(&c.LogFormat, \"logFormat\", \"\", \"'text' or 'json'\")\n\tflags.StringVar(&c.LogLevel, \"logLevel\", \"\", \"'debug', 'info', 'warn', or 'error'\")\n\tflags.StringVar(&c.RegistrationUDSPath, \"registrationUDSPath\", \"\", \"UDS Path to bind registration API\")\n\tflags.StringVar(&c.TrustDomain, \"trustDomain\", \"\", \"The trust domain that this server belongs to\")\n\tflags.BoolVar(&c.UpstreamBundle, \"upstreamBundle\", false, \"Include upstream CA certificates in the bundle\")\n\n\terr := flags.Parse(args)\n\tif err != nil {\n\t\treturn c, err\n\t}\n\n\treturn c, nil\n}\n\nfunc mergeInput(fileInput *config, cliInput *serverConfig) (*config, error) {\n\tc := &config{Server: &serverConfig{}}\n\n\t// Highest precedence first\n\terr := mergo.Merge(c.Server, cliInput)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = mergo.Merge(c, fileInput)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = mergo.Merge(c, defaultConfig())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn c, nil\n}\n\nfunc newServerConfig(c *config) (*server.Config, error) {\n\tsc := &server.Config{}\n\n\tif err := validateConfig(c); err != nil {\n\t\treturn nil, err\n\t}\n\n\tip := net.ParseIP(c.Server.BindAddress)\n\tif ip == nil {\n\t\treturn nil, fmt.Errorf(\"could not parse bind_address %q\", c.Server.BindAddress)\n\t}\n\tsc.BindAddress = &net.TCPAddr{\n\t\tIP:   ip,\n\t\tPort: c.Server.BindPort,\n\t}\n\n\tsc.BindUDSAddress = &net.UnixAddr{\n\t\tName: c.Server.RegistrationUDSPath,\n\t\tNet:  \"unix\",\n\t}\n\n\tsc.DataDir = c.Server.DataDir\n\n\ttd, err := idutil.ParseSpiffeID(\"spiffe://\"+c.Server.TrustDomain, idutil.AllowAnyTrustDomain())\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not parse trust_domain %q: %v\", c.Server.TrustDomain, err)\n\t}\n\tsc.TrustDomain = *td\n\n\tll := strings.ToUpper(c.Server.LogLevel)\n\tlf := strings.ToUpper(c.Server.LogFormat)\n\tlogger, err := log.NewLogger(ll, lf, c.Server.LogFile)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not start logger: %s\", err)\n\t}\n\tsc.Log = logger\n\n\tsc.UpstreamBundle = c.Server.UpstreamBundle\n\tsc.Experimental.AllowAgentlessNodeAttestors = c.Server.Experimental.AllowAgentlessNodeAttestors\n\tsc.Experimental.BundleEndpointEnabled = c.Server.Experimental.BundleEndpointEnabled\n\tsc.Experimental.BundleEndpointAddress = &net.TCPAddr{\n\t\tIP:   net.ParseIP(c.Server.Experimental.BundleEndpointAddress),\n\t\tPort: c.Server.Experimental.BundleEndpointPort,\n\t}\n\n\tif acme := c.Server.Experimental.BundleEndpointACME; acme != nil {\n\t\tsc.Experimental.BundleEndpointACME = &bundle.ACMEConfig{\n\t\t\tDirectoryURL: acme.DirectoryURL,\n\t\t\tDomainName:   acme.DomainName,\n\t\t\tCacheDir:     filepath.Join(sc.DataDir, \"bundle-acme\"),\n\t\t\tEmail:        acme.Email,\n\t\t\tToSAccepted:  acme.ToSAccepted,\n\t\t}\n\t}\n\n\tfederatesWith := map[string]bundleClient.TrustDomainConfig{}\n\tfor trustDomain, config := range c.Server.Experimental.FederatesWith {\n\t\tport := defaultBundleEndpointPort\n\t\tif config.BundleEndpointPort != 0 {\n\t\t\tport = config.BundleEndpointPort\n\t\t}\n\n\t\tfederatesWith[trustDomain] = bundleClient.TrustDomainConfig{\n\t\t\tEndpointAddress:  fmt.Sprintf(\"%s:%d\", config.BundleEndpointAddress, port),\n\t\t\tEndpointSpiffeID: config.BundleEndpointSpiffeID,\n\t\t}\n\t}\n\tsc.Experimental.FederatesWith = federatesWith\n\n\tsc.ProfilingEnabled = c.Server.ProfilingEnabled\n\tsc.ProfilingPort = c.Server.ProfilingPort\n\tsc.ProfilingFreq = c.Server.ProfilingFreq\n\tsc.ProfilingNames = c.Server.ProfilingNames\n\n\tif c.Server.SVIDTTL != \"\" {\n\t\tttl, err := time.ParseDuration(c.Server.SVIDTTL)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not parse default SVID ttl %q: %v\", c.Server.SVIDTTL, err)\n\t\t}\n\t\tsc.SVIDTTL = ttl\n\t}\n\n\tif c.Server.CATTL != \"\" {\n\t\tttl, err := time.ParseDuration(c.Server.CATTL)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not parse default CA ttl %q: %v\", c.Server.CATTL, err)\n\t\t}\n\t\tsc.CATTL = ttl\n\t}\n\n\tif subject := c.Server.CASubject; subject != nil {\n\t\tsc.CASubject = pkix.Name{\n\t\t\tOrganization: subject.Organization,\n\t\t\tCountry:      subject.Country,\n\t\t\tCommonName:   subject.CommonName,\n\t\t}\n\t}\n\n\tsc.PluginConfigs = *c.Plugins\n\tsc.Telemetry = c.Telemetry\n\tsc.HealthChecks = c.HealthChecks\n\n\treturn sc, nil\n}\n\nfunc validateConfig(c *config) error {\n\tif c.Server == nil {\n\t\treturn errors.New(\"server section must be configured\")\n\t}\n\n\tif c.Server.BindAddress == \"\" || c.Server.BindPort == 0 {\n\t\treturn errors.New(\"bind_address and bind_port must be configured\")\n\t}\n\n\tif c.Server.RegistrationUDSPath == \"\" {\n\t\treturn errors.New(\"registration_uds_path must be configured\")\n\t}\n\n\tif c.Server.TrustDomain == \"\" {\n\t\treturn errors.New(\"trust_domain must be configured\")\n\t}\n\n\tif c.Server.DataDir == \"\" {\n\t\treturn errors.New(\"data_dir must be configured\")\n\t}\n\n\tif c.Plugins == nil {\n\t\treturn errors.New(\"plugins section must be configured\")\n\t}\n\n\tif acme := c.Server.Experimental.BundleEndpointACME; acme != nil {\n\t\tif acme.DomainName == \"\" {\n\t\t\treturn errors.New(\"bundle_endpoint_acme domain_name must be configured\")\n\t\t}\n\n\t\tif acme.Email == \"\" {\n\t\t\treturn errors.New(\"bundle_endpoint_acme email must be configured\")\n\t\t}\n\t}\n\n\tfor td, tdConfig := range c.Server.Experimental.FederatesWith {\n\t\tif tdConfig.BundleEndpointAddress == \"\" {\n\t\t\treturn fmt.Errorf(\"%s bundle_endpoint_address must be configured\", td)\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc defaultConfig() *config {\n\treturn &config{\n\t\tServer: &serverConfig{\n\t\t\tBindAddress:         \"0.0.0.0\",\n\t\t\tBindPort:            8081,\n\t\t\tLogLevel:            defaultLogLevel,\n\t\t\tLogFormat:           log.DefaultFormat,\n\t\t\tRegistrationUDSPath: defaultSocketPath,\n\t\t\tExperimental: experimentalConfig{\n\t\t\t\tBundleEndpointAddress: \"0.0.0.0\",\n\t\t\t\tBundleEndpointPort:    defaultBundleEndpointPort,\n\t\t\t},\n\t\t},\n\t}\n}\n", "idx": 1, "id": 11733, "msg": "IMO this should be hardcoded. Other tunables like this one have turned out to be more harm than good because they raise questions and cause confusion (and sometimes actual problems!) way more often than they prove useful. Furthermore, they're easy to add but hard to remove... so the prudent step is to start with a hardcoded value. Based on the PR description, it sounds like you've already formed some opinions around what a reasonable value would/should be.", "proj": "spiffe-spire", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1370,13 +1370,13 @@ func (bc *blockchain) createPollGenesisStates(ctx context.Context, ws factory.Wo\n }\n \n func calculateReceiptRoot(receipts []*action.Receipt) hash.Hash256 {\n+\tif len(receipts) == 0 {\n+\t\treturn hash.ZeroHash256\n+\t}\n \th := make([]hash.Hash256, 0, len(receipts))\n \tfor _, receipt := range receipts {\n \t\th = append(h, receipt.Hash())\n \t}\n-\tif len(h) == 0 {\n-\t\treturn hash.ZeroHash256\n-\t}\n \tres := crypto.NewMerkleTree(h).HashTree()\n \treturn res\n }", "y": 0, "oldf": "// Copyright (c) 2018 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage blockchain\n\nimport (\n\t\"context\"\n\t\"math/big\"\n\t\"os\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/facebookgo/clock\"\n\t\"github.com/iotexproject/go-pkgs/bloom\"\n\t\"github.com/iotexproject/go-pkgs/hash\"\n\t\"github.com/iotexproject/iotex-address/address\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\n\t\"github.com/iotexproject/iotex-core/action\"\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/account\"\n\taccountutil \"github.com/iotexproject/iotex-core/action/protocol/account/util\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/execution/evm\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/poll\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rewarding\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rolldpos\"\n\t\"github.com/iotexproject/iotex-core/actpool/actioniterator\"\n\t\"github.com/iotexproject/iotex-core/blockchain/block\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/crypto\"\n\t\"github.com/iotexproject/iotex-core/db\"\n\t\"github.com/iotexproject/iotex-core/pkg/lifecycle\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/pkg/prometheustimer\"\n\t\"github.com/iotexproject/iotex-core/pkg/util/byteutil\"\n\t\"github.com/iotexproject/iotex-core/pkg/util/fileutil\"\n\t\"github.com/iotexproject/iotex-core/state\"\n\t\"github.com/iotexproject/iotex-core/state/factory\"\n)\n\nvar (\n\tblockMtc = prometheus.NewGaugeVec(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"iotex_block_metrics\",\n\n\t\t\tHelp: \"Block metrics.\",\n\t\t},\n\t\t[]string{\"type\"},\n\t)\n\terrDelegatesNotExist = errors.New(\"delegates cannot be found\")\n)\n\nfunc init() {\n\tprometheus.MustRegister(blockMtc)\n}\n\n// Blockchain represents the blockchain data structure and hosts the APIs to access it\ntype Blockchain interface {\n\tlifecycle.StartStopper\n\n\t// Balance returns balance of an account\n\tBalance(addr string) (*big.Int, error)\n\t// Nonce returns the nonce if the account exists\n\tNonce(addr string) (uint64, error)\n\t// CreateState adds a new account with initial balance to the factory\n\tCreateState(addr string, init *big.Int) (*state.Account, error)\n\t// CandidatesByHeight returns the candidate list by a given height\n\tCandidatesByHeight(height uint64) ([]*state.Candidate, error)\n\t// ProductivityByEpoch returns the number of produced blocks per delegate in an epoch\n\tProductivityByEpoch(epochNum uint64) (uint64, map[string]uint64, error)\n\t// For exposing blockchain states\n\t// GetHeightByHash returns Block's height by hash\n\tGetHeightByHash(h hash.Hash256) (uint64, error)\n\t// GetHashByHeight returns Block's hash by height\n\tGetHashByHeight(height uint64) (hash.Hash256, error)\n\t// GetBlockByHeight returns Block by height\n\tGetBlockByHeight(height uint64) (*block.Block, error)\n\t// GetBlockByHash returns Block by hash\n\tGetBlockByHash(h hash.Hash256) (*block.Block, error)\n\t// BlockHeaderByHeight return block header by height\n\tBlockHeaderByHeight(height uint64) (*block.Header, error)\n\t// BlockHeaderByHash return block header by hash\n\tBlockHeaderByHash(h hash.Hash256) (*block.Header, error)\n\t// BlockFooterByHeight return block footer by height\n\tBlockFooterByHeight(height uint64) (*block.Footer, error)\n\t// BlockFooterByHash return block footer by hash\n\tBlockFooterByHash(h hash.Hash256) (*block.Footer, error)\n\t// GetTotalActions returns the total number of actions\n\tGetTotalActions() (uint64, error)\n\t// GetReceiptByActionHash returns the receipt by action hash\n\tGetReceiptByActionHash(h hash.Hash256) (*action.Receipt, error)\n\t// GetActionsFromAddress returns actions from address\n\tGetActionsFromAddress(address string) ([]hash.Hash256, error)\n\t// GetActionsToAddress returns actions to address\n\tGetActionsToAddress(address string) ([]hash.Hash256, error)\n\t// GetActionCountByAddress returns action count by address\n\tGetActionCountByAddress(address string) (uint64, error)\n\t// GetActionByActionHash returns action by action hash\n\tGetActionByActionHash(h hash.Hash256) (action.SealedEnvelope, error)\n\t// GetBlockHashByActionHash returns Block hash by action hash\n\tGetBlockHashByActionHash(h hash.Hash256) (hash.Hash256, error)\n\t// GetReceiptsByHeight returns action receipts by block height\n\tGetReceiptsByHeight(height uint64) ([]*action.Receipt, error)\n\t// GetFactory returns the state factory\n\tGetFactory() factory.Factory\n\t// GetChainID returns the chain ID\n\tChainID() uint32\n\t// ChainAddress returns chain address on parent chain, the root chain return empty.\n\tChainAddress() string\n\t// TipHash returns tip block's hash\n\tTipHash() hash.Hash256\n\t// TipHeight returns tip block's height\n\tTipHeight() uint64\n\t// StateByAddr returns account of a given address\n\tStateByAddr(address string) (*state.Account, error)\n\t// RecoverChainAndState recovers the chain to target height and refresh state db if necessary\n\tRecoverChainAndState(targetHeight uint64) error\n\t// GenesisTimestamp returns the timestamp of genesis\n\tGenesisTimestamp() int64\n\n\t// For block operations\n\t// MintNewBlock creates a new block with given actions\n\t// Note: the coinbase transfer will be added to the given transfers when minting a new block\n\tMintNewBlock(\n\t\tactionMap map[string][]action.SealedEnvelope,\n\t\ttimestamp time.Time,\n\t) (*block.Block, error)\n\t// CommitBlock validates and appends a block to the chain\n\tCommitBlock(blk *block.Block) error\n\t// ValidateBlock validates a new block before adding it to the blockchain\n\tValidateBlock(blk *block.Block) error\n\n\t// For action operations\n\t// Validator returns the current validator object\n\tValidator() Validator\n\t// SetValidator sets the current validator object\n\tSetValidator(val Validator)\n\n\t// For smart contract operations\n\t// ExecuteContractRead runs a read-only smart contract operation, this is done off the network since it does not\n\t// cause any state change\n\tExecuteContractRead(caller address.Address, ex *action.Execution) ([]byte, *action.Receipt, error)\n\n\t// AddSubscriber make you listen to every single produced block\n\tAddSubscriber(BlockCreationSubscriber) error\n\n\t// RemoveSubscriber make you listen to every single produced block\n\tRemoveSubscriber(BlockCreationSubscriber) error\n}\n\n// blockchain implements the Blockchain interface\ntype blockchain struct {\n\tmu            sync.RWMutex // mutex to protect utk, tipHeight and tipHash\n\tdao           *blockDAO\n\tconfig        config.Config\n\ttipHeight     uint64\n\ttipHash       hash.Hash256\n\tvalidator     Validator\n\tlifecycle     lifecycle.Lifecycle\n\tclk           clock.Clock\n\tblocklistener []BlockCreationSubscriber\n\ttimerFactory  *prometheustimer.TimerFactory\n\n\t// used by account-based model\n\tsf factory.Factory\n\n\tregistry *protocol.Registry\n\n\tenableExperimentalActions bool\n}\n\n// Option sets blockchain construction parameter\ntype Option func(*blockchain, config.Config) error\n\n// DefaultStateFactoryOption sets blockchain's sf from config\nfunc DefaultStateFactoryOption() Option {\n\treturn func(bc *blockchain, cfg config.Config) (err error) {\n\t\tif cfg.Chain.EnableTrielessStateDB {\n\t\t\tbc.sf, err = factory.NewStateDB(cfg, factory.DefaultStateDBOption())\n\t\t} else {\n\t\t\tbc.sf, err = factory.NewFactory(cfg, factory.DefaultTrieOption())\n\t\t}\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"Failed to create state factory\")\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// PrecreatedStateFactoryOption sets blockchain's state.Factory to sf\nfunc PrecreatedStateFactoryOption(sf factory.Factory) Option {\n\treturn func(bc *blockchain, conf config.Config) error {\n\t\tbc.sf = sf\n\n\t\treturn nil\n\t}\n}\n\n// InMemStateFactoryOption sets blockchain's factory.Factory as in memory sf\nfunc InMemStateFactoryOption() Option {\n\treturn func(bc *blockchain, cfg config.Config) error {\n\t\tsf, err := factory.NewFactory(cfg, factory.InMemTrieOption())\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"Failed to create state factory\")\n\t\t}\n\t\tbc.sf = sf\n\n\t\treturn nil\n\t}\n}\n\n// PrecreatedDaoOption sets blockchain's dao\nfunc PrecreatedDaoOption(dao *blockDAO) Option {\n\treturn func(bc *blockchain, conf config.Config) error {\n\t\tbc.dao = dao\n\n\t\treturn nil\n\t}\n}\n\n// BoltDBDaoOption sets blockchain's dao with BoltDB from config.Chain.ChainDBPath\nfunc BoltDBDaoOption() Option {\n\treturn func(bc *blockchain, cfg config.Config) error {\n\t\tcfg.DB.DbPath = cfg.Chain.ChainDBPath // TODO: remove this after moving TrieDBPath from cfg.Chain to cfg.DB\n\t\t_, gateway := cfg.Plugins[config.GatewayPlugin]\n\t\tbc.dao = newBlockDAO(\n\t\t\tdb.NewOnDiskDB(cfg.DB),\n\t\t\tgateway && !cfg.Chain.EnableAsyncIndexWrite,\n\t\t\tcfg.Chain.CompressBlock,\n\t\t\tcfg.Chain.MaxCacheSize,\n\t\t)\n\t\treturn nil\n\t}\n}\n\n// InMemDaoOption sets blockchain's dao with MemKVStore\nfunc InMemDaoOption() Option {\n\treturn func(bc *blockchain, cfg config.Config) error {\n\t\t_, gateway := cfg.Plugins[config.GatewayPlugin]\n\t\tbc.dao = newBlockDAO(\n\t\t\tdb.NewMemKVStore(),\n\t\t\tgateway && !cfg.Chain.EnableAsyncIndexWrite,\n\t\t\tcfg.Chain.CompressBlock,\n\t\t\tcfg.Chain.MaxCacheSize,\n\t\t)\n\n\t\treturn nil\n\t}\n}\n\n// ClockOption overrides the default clock\nfunc ClockOption(clk clock.Clock) Option {\n\treturn func(bc *blockchain, conf config.Config) error {\n\t\tbc.clk = clk\n\n\t\treturn nil\n\t}\n}\n\n// RegistryOption sets the blockchain with the protocol registry\nfunc RegistryOption(registry *protocol.Registry) Option {\n\treturn func(bc *blockchain, conf config.Config) error {\n\t\tbc.registry = registry\n\t\treturn nil\n\t}\n}\n\n// EnableExperimentalActions enables the blockchain to process experimental actions\nfunc EnableExperimentalActions() Option {\n\treturn func(bc *blockchain, conf config.Config) error {\n\t\tbc.enableExperimentalActions = true\n\t\treturn nil\n\t}\n}\n\n// NewBlockchain creates a new blockchain and DB instance\nfunc NewBlockchain(cfg config.Config, opts ...Option) Blockchain {\n\t// create the Blockchain\n\tchain := &blockchain{\n\t\tconfig: cfg,\n\t\tclk:    clock.New(),\n\t}\n\tfor _, opt := range opts {\n\t\tif err := opt(chain, cfg); err != nil {\n\t\t\tlog.S().Panicf(\"Failed to execute blockchain creation option %p: %v\", opt, err)\n\t\t}\n\t}\n\ttimerFactory, err := prometheustimer.New(\n\t\t\"iotex_blockchain_perf\",\n\t\t\"Performance of blockchain module\",\n\t\t[]string{\"topic\", \"chainID\"},\n\t\t[]string{\"default\", strconv.FormatUint(uint64(cfg.Chain.ID), 10)},\n\t)\n\tif err != nil {\n\t\tlog.L().Panic(\"Failed to generate prometheus timer factory.\", zap.Error(err))\n\t}\n\tchain.timerFactory = timerFactory\n\t// Set block validator\n\tif err != nil {\n\t\tlog.L().Panic(\"Failed to get block producer address.\", zap.Error(err))\n\t}\n\tchain.validator = &validator{\n\t\tsf:                        chain.sf,\n\t\tvalidatorAddr:             cfg.ProducerAddress().String(),\n\t\tenableExperimentalActions: chain.enableExperimentalActions,\n\t}\n\n\tif chain.dao != nil {\n\t\tchain.lifecycle.Add(chain.dao)\n\t}\n\tif chain.sf != nil {\n\t\tchain.lifecycle.Add(chain.sf)\n\t}\n\treturn chain\n}\n\nfunc (bc *blockchain) ChainID() uint32 {\n\treturn atomic.LoadUint32(&bc.config.Chain.ID)\n}\n\nfunc (bc *blockchain) ChainAddress() string {\n\treturn bc.config.Chain.Address\n}\n\n// Start starts the blockchain\nfunc (bc *blockchain) Start(ctx context.Context) (err error) {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\tif err = bc.lifecycle.OnStart(ctx); err != nil {\n\t\treturn err\n\t}\n\t// get blockchain tip height\n\tif bc.tipHeight, err = bc.dao.getBlockchainHeight(); err != nil {\n\t\treturn err\n\t}\n\tif bc.tipHeight == 0 {\n\t\treturn bc.startEmptyBlockchain()\n\t}\n\t// get blockchain tip hash\n\tif bc.tipHash, err = bc.dao.getBlockHash(bc.tipHeight); err != nil {\n\t\treturn err\n\t}\n\treturn bc.startExistingBlockchain()\n}\n\n// Stop stops the blockchain.\nfunc (bc *blockchain) Stop(ctx context.Context) error {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\n\treturn bc.lifecycle.OnStop(ctx)\n}\n\n// Balance returns balance of address\nfunc (bc *blockchain) Balance(addr string) (*big.Int, error) {\n\treturn bc.sf.Balance(addr)\n}\n\n// Nonce returns the nonce if the account exists\nfunc (bc *blockchain) Nonce(addr string) (uint64, error) {\n\treturn bc.sf.Nonce(addr)\n}\n\n// CandidatesByHeight returns the candidate list by a given height\nfunc (bc *blockchain) CandidatesByHeight(height uint64) ([]*state.Candidate, error) {\n\treturn bc.candidatesByHeight(height)\n}\n\n// ProductivityByEpoch returns the map of the number of blocks produced per delegate in an epoch\nfunc (bc *blockchain) ProductivityByEpoch(epochNum uint64) (uint64, map[string]uint64, error) {\n\tp, ok := bc.registry.Find(rolldpos.ProtocolID)\n\tif !ok {\n\t\treturn 0, nil, errors.New(\"rolldpos protocol is not registered\")\n\t}\n\trp, ok := p.(*rolldpos.Protocol)\n\tif !ok {\n\t\treturn 0, nil, errors.New(\"fail to cast rolldpos protocol\")\n\t}\n\n\tvar isCurrentEpoch bool\n\tcurrentEpochNum := rp.GetEpochNum(bc.tipHeight)\n\tif epochNum > currentEpochNum {\n\t\treturn 0, nil, errors.New(\"epoch number is larger than current epoch number\")\n\t}\n\tif epochNum == currentEpochNum {\n\t\tisCurrentEpoch = true\n\t}\n\n\tepochStartHeight := rp.GetEpochHeight(epochNum)\n\tvar epochEndHeight uint64\n\tif isCurrentEpoch {\n\t\tepochEndHeight = bc.tipHeight\n\t} else {\n\t\tepochEndHeight = rp.GetEpochLastBlockHeight(epochNum)\n\t}\n\tnumBlks := epochEndHeight - epochStartHeight + 1\n\n\tp, ok = bc.registry.Find(poll.ProtocolID)\n\tif !ok {\n\t\treturn 0, nil, errors.New(\"poll protocol is not registered\")\n\t}\n\tctx := protocol.WithRunActionsCtx(context.Background(), protocol.RunActionsCtx{\n\t\tBlockHeight: bc.tipHeight,\n\t\tRegistry:    bc.registry,\n\t})\n\tws, err := bc.sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\ts, err := p.ReadState(ctx, ws, []byte(\"ActiveBlockProducersByEpoch\"),\n\t\tbyteutil.Uint64ToBytes(epochNum))\n\tif err != nil {\n\t\treturn 0, nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tvar activeConsensusBlockProducers state.CandidateList\n\tif err := activeConsensusBlockProducers.Deserialize(s); err != nil {\n\t\treturn 0, nil, err\n\t}\n\n\tproduce := make(map[string]uint64)\n\tfor _, bp := range activeConsensusBlockProducers {\n\t\tproduce[bp.Address] = 0\n\t}\n\tfor i := uint64(0); i < numBlks; i++ {\n\t\tblk, err := bc.blockHeaderByHeight(epochStartHeight + i)\n\t\tif err != nil {\n\t\t\treturn 0, nil, err\n\t\t}\n\t\tproduce[blk.ProducerAddress()]++\n\t}\n\treturn numBlks, produce, nil\n}\n\n// GetHeightByHash returns block's height by hash\nfunc (bc *blockchain) GetHeightByHash(h hash.Hash256) (uint64, error) {\n\treturn bc.dao.getBlockHeight(h)\n}\n\n// GetHashByHeight returns block's hash by height\nfunc (bc *blockchain) GetHashByHeight(height uint64) (hash.Hash256, error) {\n\treturn bc.dao.getBlockHash(height)\n}\n\n// GetBlockByHeight returns block from the blockchain hash by height\nfunc (bc *blockchain) GetBlockByHeight(height uint64) (*block.Block, error) {\n\tblk, err := bc.getBlockByHeight(height)\n\tif blk == nil || err != nil {\n\t\treturn blk, err\n\t}\n\tblk.HeaderLogger(log.L()).Debug(\"Get block.\")\n\treturn blk, err\n}\n\n// GetBlockByHash returns block from the blockchain hash by hash\nfunc (bc *blockchain) GetBlockByHash(h hash.Hash256) (*block.Block, error) {\n\treturn bc.dao.getBlock(h)\n}\n\nfunc (bc *blockchain) BlockHeaderByHeight(height uint64) (*block.Header, error) {\n\treturn bc.blockHeaderByHeight(height)\n}\n\nfunc (bc *blockchain) BlockHeaderByHash(h hash.Hash256) (*block.Header, error) {\n\treturn bc.dao.Header(h)\n}\n\nfunc (bc *blockchain) BlockFooterByHeight(height uint64) (*block.Footer, error) {\n\treturn bc.blockFooterByHeight(height)\n}\n\nfunc (bc *blockchain) BlockFooterByHash(h hash.Hash256) (*block.Footer, error) {\n\treturn bc.dao.Footer(h)\n}\n\n// GetTotalActions returns the total number of actions\nfunc (bc *blockchain) GetTotalActions() (uint64, error) {\n\treturn bc.dao.getTotalActions()\n}\n\n// GetReceiptByActionHash returns the receipt by action hash\nfunc (bc *blockchain) GetReceiptByActionHash(h hash.Hash256) (*action.Receipt, error) {\n\treturn bc.dao.getReceiptByActionHash(h)\n}\n\n// GetActionsFromAddress returns actions from address\nfunc (bc *blockchain) GetActionsFromAddress(addrStr string) ([]hash.Hash256, error) {\n\taddr, err := address.FromString(addrStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn getActionsBySenderAddress(bc.dao.kvstore, hash.BytesToHash160(addr.Bytes()))\n}\n\n// GetActionToAddress returns action to address\nfunc (bc *blockchain) GetActionsToAddress(addrStr string) ([]hash.Hash256, error) {\n\taddr, err := address.FromString(addrStr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn getActionsByRecipientAddress(bc.dao.kvstore, hash.BytesToHash160(addr.Bytes()))\n}\n\n// GetActionCountByAddress returns action count by address\nfunc (bc *blockchain) GetActionCountByAddress(addrStr string) (uint64, error) {\n\taddr, err := address.FromString(addrStr)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tfromCount, err := getActionCountBySenderAddress(bc.dao.kvstore, hash.BytesToHash160(addr.Bytes()))\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\ttoCount, err := getActionCountByRecipientAddress(bc.dao.kvstore, hash.BytesToHash160(addr.Bytes()))\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn fromCount + toCount, nil\n}\n\nfunc (bc *blockchain) getActionByActionHashHelper(h hash.Hash256) (hash.Hash256, error) {\n\treturn getBlockHashByActionHash(bc.dao.kvstore, h)\n}\n\n// GetActionByActionHash returns action by action hash\nfunc (bc *blockchain) GetActionByActionHash(h hash.Hash256) (action.SealedEnvelope, error) {\n\tblkHash, err := bc.getActionByActionHashHelper(h)\n\tif err != nil {\n\t\treturn action.SealedEnvelope{}, err\n\t}\n\n\tblk, err := bc.dao.getBlock(blkHash)\n\tif err != nil {\n\t\treturn action.SealedEnvelope{}, err\n\t}\n\tfor _, act := range blk.Actions {\n\t\tif act.Hash() == h {\n\t\t\treturn act, nil\n\t\t}\n\t}\n\treturn action.SealedEnvelope{}, errors.Errorf(\"block %x does not have transfer %x\", blkHash, h)\n}\n\n// GetBlockHashByActionHash returns Block hash by action hash\nfunc (bc *blockchain) GetBlockHashByActionHash(h hash.Hash256) (hash.Hash256, error) {\n\treturn getBlockHashByActionHash(bc.dao.kvstore, h)\n}\n\n// GetReceiptsByHeight returns action receipts by block height\nfunc (bc *blockchain) GetReceiptsByHeight(height uint64) ([]*action.Receipt, error) {\n\treturn bc.dao.getReceipts(height)\n}\n\n// GetFactory returns the state factory\nfunc (bc *blockchain) GetFactory() factory.Factory {\n\treturn bc.sf\n}\n\n// TipHash returns tip block's hash\nfunc (bc *blockchain) TipHash() hash.Hash256 {\n\tbc.mu.RLock()\n\tdefer bc.mu.RUnlock()\n\treturn bc.tipHash\n}\n\n// TipHeight returns tip block's height\nfunc (bc *blockchain) TipHeight() uint64 {\n\treturn atomic.LoadUint64(&bc.tipHeight)\n}\n\n// ValidateBlock validates a new block before adding it to the blockchain\nfunc (bc *blockchain) ValidateBlock(blk *block.Block) error {\n\tbc.mu.RLock()\n\tdefer bc.mu.RUnlock()\n\ttimer := bc.timerFactory.NewTimer(\"ValidateBlock\")\n\tdefer timer.End()\n\treturn bc.validateBlock(blk)\n}\n\nfunc (bc *blockchain) MintNewBlock(\n\tactionMap map[string][]action.SealedEnvelope,\n\ttimestamp time.Time,\n) (*block.Block, error) {\n\tbc.mu.RLock()\n\tdefer bc.mu.RUnlock()\n\tmintNewBlockTimer := bc.timerFactory.NewTimer(\"MintNewBlock\")\n\tdefer mintNewBlockTimer.End()\n\n\tnewblockHeight := bc.tipHeight + 1\n\t// run execution and update state trie root hash\n\tws, err := bc.sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"Failed to obtain working set from state factory\")\n\t}\n\n\tgasLimitForContext := bc.config.Genesis.BlockGasLimit\n\tctx := protocol.WithRunActionsCtx(context.Background(),\n\t\tprotocol.RunActionsCtx{\n\t\t\tBlockHeight:    newblockHeight,\n\t\t\tBlockTimeStamp: timestamp,\n\t\t\tProducer:       bc.config.ProducerAddress(),\n\t\t\tGasLimit:       gasLimitForContext,\n\t\t\tActionGasLimit: bc.config.Genesis.ActionGasLimit,\n\t\t\tRegistry:       bc.registry,\n\t\t})\n\t_, rc, actions, err := bc.pickAndRunActions(ctx, actionMap, ws)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"Failed to update state changes in new block %d\", newblockHeight)\n\t}\n\n\tblockMtc.WithLabelValues(\"numActions\").Set(float64(len(actions)))\n\n\tsk := bc.config.ProducerPrivateKey()\n\tra := block.NewRunnableActionsBuilder().\n\t\tSetHeight(newblockHeight).\n\t\tSetTimeStamp(timestamp).\n\t\tAddActions(actions...).\n\t\tBuild(sk.PublicKey())\n\n\tprevBlkHash := bc.tipHash\n\t// The first block's previous block hash is pointing to the digest of genesis config. This is to guarantee all nodes\n\t// could verify that they start from the same genesis\n\tif newblockHeight == 1 {\n\t\tprevBlkHash = bc.config.Genesis.Hash()\n\t}\n\tblk, err := block.NewBuilder(ra).\n\t\tSetPrevBlockHash(prevBlkHash).\n\t\tSetDeltaStateDigest(ws.Digest()).\n\t\tSetReceipts(rc).\n\t\tSetReceiptRoot(calculateReceiptRoot(rc)).\n\t\tSetLogsBloom(calculateLogsBloom(bc.config, newblockHeight, rc)).\n\t\tSignAndBuild(sk)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to create block\")\n\t}\n\tblk.WorkingSet = ws\n\n\treturn &blk, nil\n}\n\n//  CommitBlock validates and appends a block to the chain\nfunc (bc *blockchain) CommitBlock(blk *block.Block) error {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\ttimer := bc.timerFactory.NewTimer(\"CommitBlock\")\n\tdefer timer.End()\n\n\treturn bc.commitBlock(blk)\n}\n\n// StateByAddr returns the account of an address\nfunc (bc *blockchain) StateByAddr(address string) (*state.Account, error) {\n\tif bc.sf != nil {\n\t\ts, err := bc.sf.AccountState(address)\n\t\tif err != nil {\n\t\t\tlog.L().Warn(\"Failed to get account.\", zap.String(\"address\", address), zap.Error(err))\n\t\t\treturn nil, err\n\t\t}\n\t\treturn s, nil\n\t}\n\treturn nil, errors.New(\"state factory is nil\")\n}\n\n// SetValidator sets the current validator object\nfunc (bc *blockchain) SetValidator(val Validator) {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\tbc.validator = val\n}\n\n// Validator gets the current validator object\nfunc (bc *blockchain) Validator() Validator {\n\tbc.mu.RLock()\n\tdefer bc.mu.RUnlock()\n\treturn bc.validator\n}\n\nfunc (bc *blockchain) AddSubscriber(s BlockCreationSubscriber) error {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\tlog.L().Info(\"Add a subscriber.\")\n\tif s == nil {\n\t\treturn errors.New(\"subscriber could not be nil\")\n\t}\n\tbc.blocklistener = append(bc.blocklistener, s)\n\n\treturn nil\n}\n\nfunc (bc *blockchain) RemoveSubscriber(s BlockCreationSubscriber) error {\n\tbc.mu.Lock()\n\tdefer bc.mu.Unlock()\n\tfor i, sub := range bc.blocklistener {\n\t\tif sub == s {\n\t\t\tbc.blocklistener = append(bc.blocklistener[:i], bc.blocklistener[i+1:]...)\n\t\t\tlog.L().Info(\"Successfully unsubscribe block creation.\")\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn errors.New(\"cannot find subscription\")\n}\n\n//======================================\n// internal functions\n//=====================================\n\n// ExecuteContractRead runs a read-only smart contract operation, this is done off the network since it does not\n// cause any state change\nfunc (bc *blockchain) ExecuteContractRead(caller address.Address, ex *action.Execution) ([]byte, *action.Receipt, error) {\n\t// use latest block as carrier to run the offline execution\n\t// the block itself is not used\n\th := bc.TipHeight()\n\theader, err := bc.BlockHeaderByHeight(h)\n\tif err != nil {\n\t\treturn nil, nil, errors.Wrap(err, \"failed to get block in ExecuteContractRead\")\n\t}\n\tws, err := bc.sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn nil, nil, errors.Wrap(err, \"failed to obtain working set from state factory\")\n\t}\n\tproducer, err := address.FromString(header.ProducerAddress())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tgasLimit := bc.config.Genesis.BlockGasLimit\n\tctx := protocol.WithRunActionsCtx(context.Background(), protocol.RunActionsCtx{\n\t\tBlockHeight:    header.Height(),\n\t\tBlockTimeStamp: header.Timestamp(),\n\t\tProducer:       producer,\n\t\tCaller:         caller,\n\t\tGasLimit:       gasLimit,\n\t\tActionGasLimit: bc.config.Genesis.ActionGasLimit,\n\t\tGasPrice:       big.NewInt(0),\n\t\tIntrinsicGas:   0,\n\t})\n\treturn evm.ExecuteContract(\n\t\tctx,\n\t\tws,\n\t\tex,\n\t\tbc,\n\t\tbc.config.Genesis.PacificBlockHeight,\n\t)\n}\n\n// CreateState adds a new account with initial balance to the factory\nfunc (bc *blockchain) CreateState(addr string, init *big.Int) (*state.Account, error) {\n\tif bc.sf == nil {\n\t\treturn nil, errors.New(\"empty state factory\")\n\t}\n\tws, err := bc.sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to create clean working set\")\n\t}\n\taccount, err := accountutil.LoadOrCreateAccount(ws, addr, init)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to create new account %s\", addr)\n\t}\n\tgasLimit := bc.config.Genesis.BlockGasLimit\n\tcallerAddr, err := address.FromString(addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tctx := protocol.WithRunActionsCtx(context.Background(),\n\t\tprotocol.RunActionsCtx{\n\t\t\tGasLimit:       gasLimit,\n\t\t\tActionGasLimit: bc.config.Genesis.ActionGasLimit,\n\t\t\tCaller:         callerAddr,\n\t\t\tActionHash:     hash.ZeroHash256,\n\t\t\tNonce:          0,\n\t\t\tRegistry:       bc.registry,\n\t\t})\n\tif _, err = ws.RunActions(ctx, 0, nil); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to run the account creation\")\n\t}\n\tif err = bc.sf.Commit(ws); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to commit the account creation\")\n\t}\n\treturn account, nil\n}\n\n// RecoverChainAndState recovers the chain to target height and refresh state db if necessary\nfunc (bc *blockchain) RecoverChainAndState(targetHeight uint64) error {\n\tvar buildStateFromScratch bool\n\tstateHeight, err := bc.sf.Height()\n\tif err != nil {\n\t\tbuildStateFromScratch = true\n\t}\n\tif targetHeight > 0 {\n\t\tif err := bc.recoverToHeight(targetHeight); err != nil {\n\t\t\treturn errors.Wrapf(err, \"failed to recover blockchain to target height %d\", targetHeight)\n\t\t}\n\t\tif stateHeight > bc.tipHeight {\n\t\t\tbuildStateFromScratch = true\n\t\t}\n\t}\n\n\tif buildStateFromScratch {\n\t\treturn bc.refreshStateDB()\n\t}\n\treturn nil\n}\n\nfunc (bc *blockchain) GenesisTimestamp() int64 {\n\treturn bc.config.Genesis.Timestamp\n}\n\n//======================================\n// private functions\n//=====================================\n\nfunc (bc *blockchain) protocol(id string) (protocol.Protocol, bool) {\n\tif bc.registry == nil {\n\t\treturn nil, false\n\t}\n\treturn bc.registry.Find(id)\n}\n\nfunc (bc *blockchain) mustGetRollDPoSProtocol() *rolldpos.Protocol {\n\tp, ok := bc.protocol(rolldpos.ProtocolID)\n\tif !ok {\n\t\tlog.L().Panic(\"protocol rolldpos has not been registered\")\n\t}\n\trp, ok := p.(*rolldpos.Protocol)\n\tif !ok {\n\t\tlog.L().Panic(\"failed to cast to rolldpos protocol\")\n\t}\n\n\treturn rp\n}\n\nfunc (bc *blockchain) candidatesByHeight(height uint64) (state.CandidateList, error) {\n\tif bc.config.Genesis.EnableGravityChainVoting {\n\t\trp := bc.mustGetRollDPoSProtocol()\n\t\treturn bc.sf.CandidatesByHeight(rp.GetEpochHeight(rp.GetEpochNum(height)))\n\t}\n\tfor {\n\t\tcandidates, err := bc.sf.CandidatesByHeight(height)\n\t\tif err == nil {\n\t\t\treturn candidates, nil\n\t\t}\n\t\tif height == 0 {\n\t\t\treturn nil, err\n\t\t}\n\t\theight--\n\t}\n}\n\nfunc (bc *blockchain) getBlockByHeight(height uint64) (*block.Block, error) {\n\thash, err := bc.dao.getBlockHash(height)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn bc.dao.getBlock(hash)\n}\n\nfunc (bc *blockchain) blockHeaderByHeight(height uint64) (*block.Header, error) {\n\thash, err := bc.dao.getBlockHash(height)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn bc.dao.Header(hash)\n}\n\nfunc (bc *blockchain) blockFooterByHeight(height uint64) (*block.Footer, error) {\n\thash, err := bc.dao.getBlockHash(height)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn bc.dao.Footer(hash)\n}\n\nfunc (bc *blockchain) startEmptyBlockchain() error {\n\tvar ws factory.WorkingSet\n\tvar err error\n\tif ws, err = bc.sf.NewWorkingSet(); err != nil {\n\t\treturn errors.Wrap(err, \"failed to obtain working set from state factory\")\n\t}\n\tif !bc.config.Chain.EmptyGenesis {\n\t\t// Initialize the states before any actions happen on the blockchain\n\t\tif err := bc.createGenesisStates(ws); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_ = ws.UpdateBlockLevelInfo(0)\n\t}\n\t// add Genesis states\n\tif err := bc.sf.Commit(ws); err != nil {\n\t\treturn errors.Wrap(err, \"failed to commit Genesis states\")\n\t}\n\treturn nil\n}\n\nfunc (bc *blockchain) startExistingBlockchain() error {\n\tif bc.sf == nil {\n\t\treturn errors.New(\"statefactory cannot be nil\")\n\t}\n\n\tstateHeight, err := bc.sf.Height()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif stateHeight > bc.tipHeight {\n\t\treturn errors.New(\"factory is higher than blockchain\")\n\t}\n\n\tfor i := stateHeight + 1; i <= bc.tipHeight; i++ {\n\t\tblk, err := bc.getBlockByHeight(i)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tws, err := bc.sf.NewWorkingSet()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to obtain working set from state factory\")\n\t\t}\n\t\tif _, err := bc.runActions(blk.RunnableActions(), ws); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := bc.sf.Commit(ws); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tstateHeight, err = bc.sf.Height()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to get factory's height\")\n\t}\n\tlog.L().Info(\"Restarting blockchain.\",\n\t\tzap.Uint64(\"chainHeight\",\n\t\t\tbc.tipHeight),\n\t\tzap.Uint64(\"factoryHeight\", stateHeight))\n\treturn nil\n}\n\nfunc (bc *blockchain) validateBlock(blk *block.Block) error {\n\tvalidateTimer := bc.timerFactory.NewTimer(\"validate\")\n\tprevBlkHash := bc.tipHash\n\tif blk.Height() == 1 {\n\t\tprevBlkHash = bc.config.Genesis.Hash()\n\t}\n\terr := bc.validator.Validate(blk, bc.tipHeight, prevBlkHash)\n\tvalidateTimer.End()\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"error when validating block %d\", blk.Height())\n\t}\n\t// run actions and update state factory\n\tws, err := bc.sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"Failed to obtain working set from state factory\")\n\t}\n\trunTimer := bc.timerFactory.NewTimer(\"runActions\")\n\treceipts, err := bc.runActions(blk.RunnableActions(), ws)\n\trunTimer.End()\n\tif err != nil {\n\t\tlog.L().Panic(\"Failed to update state.\", zap.Uint64(\"tipHeight\", bc.tipHeight), zap.Error(err))\n\t}\n\n\tif err = blk.VerifyDeltaStateDigest(ws.Digest()); err != nil {\n\t\treturn err\n\t}\n\n\tif err = blk.VerifyReceiptRoot(calculateReceiptRoot(receipts)); err != nil {\n\t\treturn errors.Wrap(err, \"Failed to verify receipt root\")\n\t}\n\n\tblk.Receipts = receipts\n\n\t// attach working set to be committed to state factory\n\tblk.WorkingSet = ws\n\treturn nil\n}\n\n// commitBlock commits a block to the chain\nfunc (bc *blockchain) commitBlock(blk *block.Block) error {\n\t// Check if it is already exists, and return earlier\n\tblkHash, err := bc.dao.getBlockHash(blk.Height())\n\tif blkHash != hash.ZeroHash256 {\n\t\tlog.L().Debug(\"Block already exists.\", zap.Uint64(\"height\", blk.Height()))\n\t\treturn nil\n\t}\n\t// If it's a ready db io error, return earlier with the error\n\tif errors.Cause(err) != db.ErrNotExist {\n\t\treturn err\n\t}\n\t// write block into DB\n\tputTimer := bc.timerFactory.NewTimer(\"putBlock\")\n\terr = bc.dao.putBlock(blk)\n\tputTimer.End()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// update tip hash and height\n\tatomic.StoreUint64(&bc.tipHeight, blk.Height())\n\tbc.tipHash = blk.HashBlock()\n\n\tif bc.sf != nil {\n\t\tsfTimer := bc.timerFactory.NewTimer(\"sf.Commit\")\n\t\terr := bc.sf.Commit(blk.WorkingSet)\n\t\tsfTimer.End()\n\t\t// detach working set so it can be freed by GC\n\t\tblk.WorkingSet = nil\n\t\tif err != nil {\n\t\t\tlog.L().Panic(\"Error when committing states.\", zap.Error(err))\n\t\t}\n\n\t\t// write smart contract receipt into DB\n\t\treceiptTimer := bc.timerFactory.NewTimer(\"putReceipt\")\n\t\terr = bc.dao.putReceipts(blk.Height(), blk.Receipts)\n\t\treceiptTimer.End()\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"failed to put smart contract receipts into DB on height %d\", blk.Height())\n\t\t}\n\t}\n\tblk.HeaderLogger(log.L()).Info(\"Committed a block.\", log.Hex(\"tipHash\", bc.tipHash[:]))\n\n\t// emit block to all block subscribers\n\tbc.emitToSubscribers(blk)\n\treturn nil\n}\n\nfunc (bc *blockchain) runActions(\n\tacts block.RunnableActions,\n\tws factory.WorkingSet,\n) ([]*action.Receipt, error) {\n\tif bc.sf == nil {\n\t\treturn nil, errors.New(\"statefactory cannot be nil\")\n\t}\n\tgasLimit := bc.config.Genesis.BlockGasLimit\n\t// update state factory\n\tproducer, err := address.FromBytes(acts.BlockProducerPubKey().Hash())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tctx := protocol.WithRunActionsCtx(context.Background(),\n\t\tprotocol.RunActionsCtx{\n\t\t\tBlockHeight:    acts.BlockHeight(),\n\t\t\tBlockTimeStamp: acts.BlockTimeStamp(),\n\t\t\tProducer:       producer,\n\t\t\tGasLimit:       gasLimit,\n\t\t\tActionGasLimit: bc.config.Genesis.ActionGasLimit,\n\t\t\tRegistry:       bc.registry,\n\t\t})\n\n\treturn ws.RunActions(ctx, acts.BlockHeight(), acts.Actions())\n}\n\nfunc (bc *blockchain) pickAndRunActions(ctx context.Context, actionMap map[string][]action.SealedEnvelope,\n\tws factory.WorkingSet) (hash.Hash256, []*action.Receipt, []action.SealedEnvelope, error) {\n\tif bc.sf == nil {\n\t\treturn hash.ZeroHash256, nil, nil, errors.New(\"statefactory cannot be nil\")\n\t}\n\treceipts := make([]*action.Receipt, 0)\n\texecutedActions := make([]action.SealedEnvelope, 0)\n\n\traCtx := protocol.MustGetRunActionsCtx(ctx)\n\t// initial action iterator\n\tactionIterator := actioniterator.NewActionIterator(actionMap)\n\tfor {\n\t\tnextAction, ok := actionIterator.Next()\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\n\t\treceipt, err := ws.RunAction(raCtx, nextAction)\n\t\tif err != nil {\n\t\t\tif errors.Cause(err) == action.ErrHitGasLimit {\n\t\t\t\t// hit block gas limit, we should not process actions belong to this user anymore since we\n\t\t\t\t// need monotonically increasing nounce. But we can continue processing other actions\n\t\t\t\t// that belong other users\n\t\t\t\tactionIterator.PopAccount()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn hash.ZeroHash256, nil, nil, errors.Wrapf(err, \"Failed to update state changes for selp %x\", nextAction.Hash())\n\t\t}\n\t\tif receipt != nil {\n\t\t\traCtx.GasLimit -= receipt.GasConsumed\n\t\t\treceipts = append(receipts, receipt)\n\t\t}\n\t\texecutedActions = append(executedActions, nextAction)\n\n\t\t// To prevent loop all actions in act_pool, we stop processing action when remaining gas is below\n\t\t// than certain threshold\n\t\tif raCtx.GasLimit < bc.config.Chain.AllowedBlockGasResidue {\n\t\t\tbreak\n\t\t}\n\t}\n\tvar lastBlkHeight uint64\n\tif bc.config.Consensus.Scheme == config.RollDPoSScheme {\n\t\trp := bc.mustGetRollDPoSProtocol()\n\t\tepochNum := rp.GetEpochNum(raCtx.BlockHeight)\n\t\tlastBlkHeight = rp.GetEpochLastBlockHeight(epochNum)\n\t\t// generate delegates for next round\n\t\tskip, putPollResult, err := bc.createPutPollResultAction(raCtx.BlockHeight)\n\t\tswitch errors.Cause(err) {\n\t\tcase nil:\n\t\t\tif !skip {\n\t\t\t\treceipt, err := ws.RunAction(raCtx, putPollResult)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn hash.ZeroHash256, nil, nil, err\n\t\t\t\t}\n\t\t\t\tif receipt != nil {\n\t\t\t\t\treceipts = append(receipts, receipt)\n\t\t\t\t}\n\t\t\t\texecutedActions = append(executedActions, putPollResult)\n\t\t\t}\n\t\tcase errDelegatesNotExist:\n\t\t\tif raCtx.BlockHeight == lastBlkHeight {\n\t\t\t\t// TODO (zhi): if some bp by pass this condition, we need to reject block in validation step\n\t\t\t\treturn hash.ZeroHash256, nil, nil, errors.Wrapf(\n\t\t\t\t\terr,\n\t\t\t\t\t\"failed to prepare delegates for next epoch %d\",\n\t\t\t\t\tepochNum+1,\n\t\t\t\t)\n\t\t\t}\n\t\tdefault:\n\t\t\treturn hash.ZeroHash256, nil, nil, err\n\t\t}\n\t}\n\t// Process grant block reward action\n\tgrant, err := bc.createGrantRewardAction(action.BlockReward, raCtx.BlockHeight)\n\tif err != nil {\n\t\treturn hash.ZeroHash256, nil, nil, err\n\t}\n\treceipt, err := ws.RunAction(raCtx, grant)\n\tif err != nil {\n\t\treturn hash.ZeroHash256, nil, nil, err\n\t}\n\tif receipt != nil {\n\t\treceipts = append(receipts, receipt)\n\t}\n\texecutedActions = append(executedActions, grant)\n\n\t// Process grant epoch reward action if the block is the last one in an epoch\n\tif raCtx.BlockHeight == lastBlkHeight {\n\t\tgrant, err := bc.createGrantRewardAction(action.EpochReward, raCtx.BlockHeight)\n\t\tif err != nil {\n\t\t\treturn hash.ZeroHash256, nil, nil, err\n\t\t}\n\t\treceipt, err := ws.RunAction(raCtx, grant)\n\t\tif err != nil {\n\t\t\treturn hash.ZeroHash256, nil, nil, err\n\t\t}\n\t\tif receipt != nil {\n\t\t\treceipts = append(receipts, receipt)\n\t\t}\n\t\texecutedActions = append(executedActions, grant)\n\t}\n\n\tblockMtc.WithLabelValues(\"gasConsumed\").Set(float64(bc.config.Genesis.BlockGasLimit - raCtx.GasLimit))\n\n\treturn ws.UpdateBlockLevelInfo(raCtx.BlockHeight), receipts, executedActions, nil\n}\n\nfunc (bc *blockchain) createPutPollResultAction(height uint64) (skip bool, se action.SealedEnvelope, err error) {\n\tskip = true\n\tif !bc.config.Genesis.EnableGravityChainVoting {\n\t\treturn\n\t}\n\tpl, ok := bc.protocol(poll.ProtocolID)\n\tif !ok {\n\t\tlog.L().Panic(\"protocol poll has not been registered\")\n\t}\n\tpp, ok := pl.(poll.Protocol)\n\tif !ok {\n\t\tlog.L().Panic(\"Failed to cast to poll.Protocol\")\n\t}\n\trp := bc.mustGetRollDPoSProtocol()\n\tepochNum := rp.GetEpochNum(height)\n\tepochHeight := rp.GetEpochHeight(epochNum)\n\tnextEpochHeight := rp.GetEpochHeight(epochNum + 1)\n\tif height < epochHeight+(nextEpochHeight-epochHeight)/2 {\n\t\treturn\n\t}\n\tlog.L().Debug(\n\t\t\"createPutPollResultAction\",\n\t\tzap.Uint64(\"height\", height),\n\t\tzap.Uint64(\"epochNum\", epochNum),\n\t\tzap.Uint64(\"epochHeight\", epochHeight),\n\t\tzap.Uint64(\"nextEpochHeight\", nextEpochHeight),\n\t)\n\t_, err = bc.candidatesByHeight(nextEpochHeight)\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\treturn\n\tcase state.ErrStateNotExist:\n\t\tskip = false\n\tdefault:\n\t\treturn\n\t}\n\tl, err := pp.DelegatesByHeight(epochHeight)\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\tif len(l) == 0 {\n\t\t\terr = errors.Wrapf(\n\t\t\t\terrDelegatesNotExist,\n\t\t\t\t\"failed to fetch delegates by epoch height %d, empty list\",\n\t\t\t\tepochHeight,\n\t\t\t)\n\t\t\treturn\n\t\t}\n\tcase db.ErrNotExist:\n\t\terr = errors.Wrapf(\n\t\t\terrDelegatesNotExist,\n\t\t\t\"failed to fetch delegates by epoch height %d, original error %v\",\n\t\t\tepochHeight,\n\t\t\terr,\n\t\t)\n\t\treturn\n\tdefault:\n\t\treturn\n\t}\n\tsk := bc.config.ProducerPrivateKey()\n\tnonce := uint64(0)\n\tpollAction := action.NewPutPollResult(nonce, nextEpochHeight, l)\n\tbuilder := action.EnvelopeBuilder{}\n\tse, err = action.Sign(builder.SetNonce(nonce).SetAction(pollAction).Build(), sk)\n\treturn skip, se, err\n}\n\nfunc (bc *blockchain) emitToSubscribers(blk *block.Block) {\n\tif bc.blocklistener == nil {\n\t\treturn\n\t}\n\tfor _, s := range bc.blocklistener {\n\t\tgo func(bcs BlockCreationSubscriber, b *block.Block) {\n\t\t\tif err := bcs.HandleBlock(b); err != nil {\n\t\t\t\tlog.L().Error(\"Failed to handle new block.\", zap.Error(err))\n\t\t\t}\n\t\t}(s, blk)\n\t}\n}\n\n// RecoverToHeight recovers the blockchain to target height\nfunc (bc *blockchain) recoverToHeight(targetHeight uint64) error {\n\tfor bc.tipHeight > targetHeight {\n\t\tif err := bc.dao.deleteTipBlock(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbc.tipHeight--\n\t}\n\treturn nil\n}\n\n// RefreshStateDB deletes the existing state DB and creates a new one with state changes from genesis block\nfunc (bc *blockchain) refreshStateDB() error {\n\t// Delete existing state DB and reinitialize it\n\tif fileutil.FileExists(bc.config.Chain.TrieDBPath) && os.Remove(bc.config.Chain.TrieDBPath) != nil {\n\t\treturn errors.New(\"failed to delete existing state DB\")\n\t}\n\tif err := DefaultStateFactoryOption()(bc, bc.config); err != nil {\n\t\treturn errors.Wrap(err, \"failed to reinitialize state DB\")\n\t}\n\n\tfor _, p := range bc.registry.All() {\n\t\tbc.sf.AddActionHandlers(p)\n\t}\n\n\tif err := bc.sf.Start(context.Background()); err != nil {\n\t\treturn errors.Wrap(err, \"failed to start state factory\")\n\t}\n\tif err := bc.startEmptyBlockchain(); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.sf.Stop(context.Background()); err != nil {\n\t\treturn errors.Wrap(err, \"failed to stop state factory\")\n\t}\n\treturn nil\n}\n\nfunc (bc *blockchain) createGrantRewardAction(rewardType int, height uint64) (action.SealedEnvelope, error) {\n\tgb := action.GrantRewardBuilder{}\n\tgrant := gb.SetRewardType(rewardType).SetHeight(height).Build()\n\teb := action.EnvelopeBuilder{}\n\tenvelope := eb.SetNonce(0).\n\t\tSetGasPrice(big.NewInt(0)).\n\t\tSetGasLimit(grant.GasLimit()).\n\t\tSetAction(&grant).\n\t\tBuild()\n\tsk := bc.config.ProducerPrivateKey()\n\treturn action.Sign(envelope, sk)\n}\n\nfunc (bc *blockchain) createGenesisStates(ws factory.WorkingSet) error {\n\tif bc.registry == nil {\n\t\t// TODO: return nil to avoid test cases to blame on missing rewarding protocol\n\t\treturn nil\n\t}\n\tctx := protocol.WithRunActionsCtx(context.Background(), protocol.RunActionsCtx{\n\t\tBlockHeight:    0,\n\t\tBlockTimeStamp: time.Unix(bc.config.Genesis.Timestamp, 0),\n\t\tGasLimit:       0,\n\t\tActionGasLimit: bc.config.Genesis.ActionGasLimit,\n\t\tProducer:       nil,\n\t\tCaller:         nil,\n\t\tActionHash:     hash.ZeroHash256,\n\t\tNonce:          0,\n\t\tRegistry:       bc.registry,\n\t})\n\tif err := bc.createAccountGenesisStates(ctx, ws); err != nil {\n\t\treturn err\n\t}\n\tif bc.config.Consensus.Scheme == config.RollDPoSScheme {\n\t\tif err := bc.createPollGenesisStates(ctx, ws); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn bc.createRewardingGenesisStates(ctx, ws)\n}\n\nfunc (bc *blockchain) createAccountGenesisStates(ctx context.Context, ws factory.WorkingSet) error {\n\tp, ok := bc.registry.Find(account.ProtocolID)\n\tif !ok {\n\t\treturn nil\n\t}\n\tap, ok := p.(*account.Protocol)\n\tif !ok {\n\t\treturn errors.Errorf(\"error when casting protocol\")\n\t}\n\taddrs, balances := bc.config.Genesis.InitBalances()\n\treturn ap.Initialize(ctx, ws, addrs, balances)\n}\n\nfunc (bc *blockchain) createRewardingGenesisStates(ctx context.Context, ws factory.WorkingSet) error {\n\tp, ok := bc.registry.Find(rewarding.ProtocolID)\n\tif !ok {\n\t\treturn nil\n\t}\n\trp, ok := p.(*rewarding.Protocol)\n\tif !ok {\n\t\treturn errors.Errorf(\"error when casting protocol\")\n\t}\n\treturn rp.Initialize(\n\t\tctx,\n\t\tws,\n\t\tbc.config.Genesis.InitBalance(),\n\t\tbc.config.Genesis.BlockReward(),\n\t\tbc.config.Genesis.EpochReward(),\n\t\tbc.config.Genesis.NumDelegatesForEpochReward,\n\t\tbc.config.Genesis.ExemptAddrsFromEpochReward(),\n\t\tbc.config.Genesis.FoundationBonus(),\n\t\tbc.config.Genesis.NumDelegatesForFoundationBonus,\n\t\tbc.config.Genesis.FoundationBonusLastEpoch,\n\t\tbc.config.Genesis.ProductivityThreshold,\n\t)\n}\n\nfunc (bc *blockchain) createPollGenesisStates(ctx context.Context, ws factory.WorkingSet) error {\n\tif bc.config.Genesis.EnableGravityChainVoting {\n\t\tp, ok := bc.protocol(poll.ProtocolID)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"protocol %s is not found\", poll.ProtocolID)\n\t\t}\n\t\tpp, ok := p.(poll.Protocol)\n\t\tif !ok {\n\t\t\treturn errors.Errorf(\"error when casting poll protocol\")\n\t\t}\n\t\treturn pp.Initialize(\n\t\t\tctx,\n\t\t\tws,\n\t\t)\n\t}\n\treturn nil\n}\n\nfunc calculateReceiptRoot(receipts []*action.Receipt) hash.Hash256 {\n\th := make([]hash.Hash256, 0, len(receipts))\n\tfor _, receipt := range receipts {\n\t\th = append(h, receipt.Hash())\n\t}\n\tif len(h) == 0 {\n\t\treturn hash.ZeroHash256\n\t}\n\tres := crypto.NewMerkleTree(h).HashTree()\n\treturn res\n}\n\nfunc calculateLogsBloom(cfg config.Config, height uint64, receipts []*action.Receipt) bloom.BloomFilter {\n\tif height < cfg.Genesis.AleutianBlockHeight {\n\t\treturn nil\n\t}\n\tbloom, _ := bloom.NewBloomFilter(2048, 3)\n\tfor _, receipt := range receipts {\n\t\tfor _, log := range receipt.Logs {\n\t\t\tfor _, topic := range log.Topics {\n\t\t\t\tbloom.Add(topic[:])\n\t\t\t}\n\t\t}\n\t}\n\treturn bloom\n}\n", "idx": 6, "id": 18141, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -49,6 +49,31 @@ public interface Principal {\n         }\n     }\n \n+    /**\n+     * Principal state - active, authority filter disabled or authority system disabled\n+     */\n+    enum State {\n+        ACTIVE(0x00),\n+        AUTHORITY_FILTER_DISABLED(0x01),\n+        AUTHORITY_SYSTEM_DISABLED(0x02);\n+\n+        private final int principalState;\n+        State(int state) {\n+            principalState = state;\n+        }\n+        public int getValue() {\n+            return principalState;\n+        }\n+        public static State getState(int value) {\n+            for (State state : values()) {\n+                if (state.getValue() == value) {\n+                    return state;\n+                }\n+            }\n+            return ACTIVE;\n+        }\n+    }\n+\n     /** @return the domain of the authority over this principal, i.e. \"user\" */\n     String getDomain();\n ", "y": 1, "oldf": "/*\n * Copyright 2016 Yahoo Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.yahoo.athenz.auth;\n\nimport java.security.cert.X509Certificate;\nimport java.util.List;\n\n/**\n * A Principal is an authenticated entity that takes an action on a resource.\n */\npublic interface Principal {\n\n    /**\n     * Principal type - user, service, group or unknown\n     */\n    enum Type {\n        UNKNOWN(0),\n        USER(1),\n        SERVICE(2),\n        GROUP(3);\n\n        private final int principalType;\n        Type(int type) {\n            principalType = type;\n        }\n        public int getValue() {\n            return principalType;\n        }\n        public static Type getType(int value) {\n            for (Type type : values()) {\n                if (type.getValue() == value) {\n                    return type;\n                }\n            }\n            return UNKNOWN;\n        }\n    }\n\n    /** @return the domain of the authority over this principal, i.e. \"user\" */\n    String getDomain();\n\n    /** @return the name of the principal as a string, i.e. \"joe\" */\n    String getName();\n\n    /** @return the full name of the principal as a string, i.e. \"user.joe\" */\n    String getFullName();\n    \n    /** @return the credentials token as a string */\n    String getCredentials();\n    \n    /** @return the client certificate that the principal\n     * was authenticated with if using the certificate authority */\n    default X509Certificate getX509Certificate() {\n        return null;\n    }\n    \n    /** @return the credentials token as a string but will not contain a signature */\n    String getUnsignedCredentials();\n\n    /** @return the list of roles this principal is able to assume. This is null \n     * for user/service principals, but valid for a principal based on ZTokens. */\n    List<String> getRoles();\n    \n    /** @return the authority over this principal. Can be null, if not authenticated. */\n    Authority getAuthority();\n\n    /** @return the issue time for the credentials */\n    long getIssueTime();\n    \n    /** @return the service name that was authorized to use the Principal's UserToken */\n    String getAuthorizedService();\n    \n    /** @return the associated IP address provided in the principal token */\n    default String getIP() {\n        return null;\n    }\n    \n    /** @return the associated original requestor specified in the principal token */\n    default String getOriginalRequestor() {\n        return null;\n    }\n    \n    /** @return the associated original key service specified in the principal token */\n    default String getKeyService() {\n        return null;\n    }\n    \n    /** @return the private key identifier that was used to sign the service token */\n    default String getKeyId() {\n        return null;\n    }\n    \n    /** @return the application ID */\n    default String getApplicationId() {\n        return null;\n    }\n\n    /** @return True if the user certificate usage is restricted to mTLS authentication */\n    default boolean getMtlsRestricted() {\n        return false;\n    }\n}\n", "idx": 1, "id": 5437, "msg": "somewhat confusing to call this bit as system disabled since the full attribute with combined bits is also called system disabled. So maybe we should call this bit suspended since that's what we're implementing.", "proj": "AthenZ-athenz", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -198,7 +198,7 @@ public class LocalDistributor extends Distributor {\n       Host host = new Host(bus, node);\n       host.update(status);\n       hosts.add(host);\n-      LOG.info(String.format(\"Added node %s.\", node.getId()));\n+      LOG.finest(String.format(\"Added node %s.\", node.getId()));\n       host.runHealthCheck();\n \n       Runnable runnable = host::runHealthCheck;", "y": 1, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.selenium.grid.distributor.local;\n\nimport static com.google.common.collect.ImmutableSet.toImmutableSet;\nimport static org.openqa.selenium.grid.data.NodeStatusEvent.NODE_STATUS;\nimport static org.openqa.selenium.grid.distributor.local.Host.Status.UP;\nimport static org.openqa.selenium.remote.http.Contents.reader;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableSet;\n\nimport org.openqa.selenium.Beta;\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.SessionNotCreatedException;\nimport org.openqa.selenium.concurrent.Regularly;\nimport org.openqa.selenium.events.EventBus;\nimport org.openqa.selenium.grid.data.CreateSessionRequest;\nimport org.openqa.selenium.grid.data.CreateSessionResponse;\nimport org.openqa.selenium.grid.data.DistributorStatus;\nimport org.openqa.selenium.grid.data.NodeStatus;\nimport org.openqa.selenium.grid.distributor.Distributor;\nimport org.openqa.selenium.grid.node.Node;\nimport org.openqa.selenium.grid.node.remote.RemoteNode;\nimport org.openqa.selenium.grid.sessionmap.SessionMap;\nimport org.openqa.selenium.json.Json;\nimport org.openqa.selenium.json.JsonOutput;\nimport org.openqa.selenium.remote.NewSessionPayload;\nimport org.openqa.selenium.remote.http.HttpClient;\nimport org.openqa.selenium.remote.http.HttpRequest;\nimport org.openqa.selenium.remote.tracing.DistributedTracer;\nimport org.openqa.selenium.remote.tracing.Span;\n\nimport java.io.IOException;\nimport java.io.Reader;\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Comparator;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.locks.Lock;\nimport java.util.concurrent.locks.ReadWriteLock;\nimport java.util.concurrent.locks.ReentrantReadWriteLock;\nimport java.util.function.Supplier;\nimport java.util.logging.Logger;\nimport java.util.stream.Collectors;\n\npublic class LocalDistributor extends Distributor {\n\n  private static final Json JSON = new Json();\n  private static final Logger LOG = Logger.getLogger(\"Selenium Distributor\");\n  private final ReadWriteLock lock = new ReentrantReadWriteLock(/* fair */ true);\n  private final Set<Host> hosts = new HashSet<>();\n  private final DistributedTracer tracer;\n  private final EventBus bus;\n  private final HttpClient.Factory clientFactory;\n  private final SessionMap sessions;\n  private final Regularly hostChecker = new Regularly(\"distributor host checker\");\n  private final Map<UUID, Collection<Runnable>> allChecks = new ConcurrentHashMap<>();\n\n  public LocalDistributor(\n      DistributedTracer tracer,\n      EventBus bus,\n      HttpClient.Factory clientFactory,\n      SessionMap sessions) {\n    super(tracer, clientFactory);\n    this.tracer = Objects.requireNonNull(tracer);\n    this.bus = Objects.requireNonNull(bus);\n    this.clientFactory = Objects.requireNonNull(clientFactory);\n    this.sessions = Objects.requireNonNull(sessions);\n\n    bus.addListener(NODE_STATUS, event -> refresh(event.getData(NodeStatus.class)));\n  }\n\n  @Override\n  public CreateSessionResponse newSession(HttpRequest request)\n      throws SessionNotCreatedException {\n    try (Reader reader = reader(request);\n    NewSessionPayload payload = NewSessionPayload.create(reader)) {\n      Objects.requireNonNull(payload, \"Requests to process must be set.\");\n\n      Iterator<Capabilities> iterator = payload.stream().iterator();\n\n      if (!iterator.hasNext()) {\n        throw new SessionNotCreatedException(\"No capabilities found\");\n      }\n\n      Optional<Supplier<CreateSessionResponse>> selected;\n      CreateSessionRequest firstRequest = new CreateSessionRequest(\n          payload.getDownstreamDialects(),\n          iterator.next(),\n          ImmutableMap.of());\n\n      Lock writeLock = this.lock.writeLock();\n      writeLock.lock();\n      try {\n        selected = this.hosts.stream()\n            .filter(host -> host.getHostStatus() == UP)\n            // Find a host that supports this kind of thing\n            .filter(host -> host.hasCapacity(firstRequest.getCapabilities()))\n            .min(\n                // Now sort by node which has the lowest load (natural ordering)\n                Comparator.comparingDouble(Host::getLoad)\n                    // Then last session created (oldest first), so natural ordering again\n                    .thenComparingLong(Host::getLastSessionCreated)\n                    // And use the host id as a tie-breaker.\n                    .thenComparing(Host::getId))\n            // And reserve some space\n            .map(host -> host.reserve(firstRequest));\n      } finally {\n        writeLock.unlock();\n      }\n\n      CreateSessionResponse sessionResponse = selected\n          .orElseThrow(\n              () -> new SessionNotCreatedException(\n                  \"Unable to find provider for session: \" + payload.stream()\n                      .map(Capabilities::toString)\n                      .collect(Collectors.joining(\", \"))))\n          .get();\n\n      sessions.add(sessionResponse.getSession());\n\n      return sessionResponse;\n    } catch (IOException e) {\n      throw new SessionNotCreatedException(e.getMessage(), e);\n    }\n  }\n\n  private void refresh(NodeStatus status) {\n    Objects.requireNonNull(status);\n\n    // Iterate over the available nodes to find a match.\n    Lock writeLock = lock.writeLock();\n    writeLock.lock();\n    try {\n      Optional<Host> existing = hosts.stream()\n          .filter(host -> host.getId().equals(status.getNodeId()))\n          .findFirst();\n\n\n      if (existing.isPresent()) {\n        // Modify the state\n        existing.get().update(status);\n      } else {\n        // No match made. Add a new host.\n        Node node = new RemoteNode(\n            tracer,\n            clientFactory,\n            status.getNodeId(),\n            status.getUri(),\n            status.getStereotypes().keySet());\n        add(node, status);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }\n\n  @Override\n  public LocalDistributor add(Node node) {\n    return add(node, node.getStatus());\n  }\n\n  private LocalDistributor add(Node node, NodeStatus status) {\n    StringBuilder sb = new StringBuilder();\n\n    Lock writeLock = this.lock.writeLock();\n    writeLock.lock();\n    try (Span span = tracer.createSpan(\"distributor.add\", tracer.getActiveSpan());\n         JsonOutput out = JSON.newOutput(sb)) {\n      out.setPrettyPrint(false).write(node);\n      span.addTag(\"node\", sb.toString());\n\n      Host host = new Host(bus, node);\n      host.update(status);\n      hosts.add(host);\n      LOG.info(String.format(\"Added node %s.\", node.getId()));\n      host.runHealthCheck();\n\n      Runnable runnable = host::runHealthCheck;\n      Collection<Runnable> nodeRunnables = allChecks.getOrDefault(node.getId(), new ArrayList<>());\n      nodeRunnables.add(runnable);\n      allChecks.put(node.getId(), nodeRunnables);\n      hostChecker.submit(runnable, Duration.ofMinutes(5), Duration.ofSeconds(30));\n    } finally {\n      writeLock.unlock();\n    }\n\n    return this;\n  }\n\n  @Override\n  public void remove(UUID nodeId) {\n    Lock writeLock = lock.writeLock();\n    writeLock.lock();\n    try (Span span = tracer.createSpan(\"distributor.remove\", tracer.getActiveSpan())) {\n      span.addTag(\"node.id\", nodeId);\n      hosts.removeIf(host -> nodeId.equals(host.getId()));\n      allChecks.getOrDefault(nodeId, new ArrayList<>()).forEach(hostChecker::remove);\n    } finally {\n      writeLock.unlock();\n    }\n  }\n\n  @Override\n  public DistributorStatus getStatus() {\n    Lock readLock = this.lock.readLock();\n    readLock.lock();\n    try {\n      ImmutableSet<DistributorStatus.NodeSummary> summaries = this.hosts.stream()\n          .map(Host::asSummary)\n          .collect(toImmutableSet());\n\n      return new DistributorStatus(summaries);\n    } finally {\n      readLock.unlock();\n    }\n  }\n\n  @VisibleForTesting\n  @Beta\n  public void refresh() {\n    Lock writeLock = lock.writeLock();\n    writeLock.lock();\n    try {\n      hosts.forEach(Host::runHealthCheck);\n    } finally {\n      writeLock.unlock();\n    }\n  }\n}\n", "idx": 1, "id": 16468, "msg": "This change is incorrect.", "proj": "SeleniumHQ-selenium", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -59,7 +59,11 @@ func (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Reco\n \n \tif phase == waitForApplySync {\n \t\tpodhttpchaos := &v1alpha1.PodHttpChaos{}\n-\t\terr := impl.Client.Get(ctx, controller.ParseNamespacedName(record.Id), podhttpchaos)\n+\t\tNamespacedName, err := controller.ParseNamespacedName(record.Id)\n+\t\tif err != nil {\n+\t\t\treturn waitForApplySync, err\n+\t\t}\n+\t\terr = impl.Client.Get(ctx, NamespacedName, podhttpchaos)\n \t\tif err != nil {\n \t\t\treturn waitForApplySync, err\n \t\t}", "y": 1, "oldf": "// Copyright 2020 Chaos Mesh Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage httpchaos\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"strings\"\n\n\t\"github.com/go-logr/logr\"\n\t\"go.uber.org/fx\"\n\tv1 \"k8s.io/api/core/v1\"\n\tk8sError \"k8s.io/apimachinery/pkg/api/errors\"\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\n\t\"github.com/chaos-mesh/chaos-mesh/api/v1alpha1\"\n\t\"github.com/chaos-mesh/chaos-mesh/controllers/chaosimpl/httpchaos/podhttpchaosmanager\"\n\t\"github.com/chaos-mesh/chaos-mesh/controllers/chaosimpl/iochaos/podiochaosmanager\"\n\t\"github.com/chaos-mesh/chaos-mesh/controllers/common\"\n\t\"github.com/chaos-mesh/chaos-mesh/controllers/utils/controller\"\n)\n\nconst (\n\twaitForApplySync   v1alpha1.Phase = \"Not Injected/Wait\"\n\twaitForRecoverSync v1alpha1.Phase = \"Injected/Wait\"\n)\n\ntype Impl struct {\n\tclient.Client\n\tLog logr.Logger\n\n\tbuilder *podhttpchaosmanager.Builder\n}\n\nfunc (impl *Impl) Apply(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\n\t// The only possible phase to get in here is \"Not Injected\" or \"Not Injected/Wait\"\n\n\timpl.Log.Info(\"httpchaos Apply\", \"namespace\", obj.GetObjectMeta().Namespace, \"name\", obj.GetObjectMeta().Name)\n\thttpchaos := obj.(*v1alpha1.HTTPChaos)\n\tif httpchaos.Status.Instances == nil {\n\t\thttpchaos.Status.Instances = make(map[string]int64)\n\t}\n\n\trecord := records[index]\n\tphase := record.Phase\n\n\tif phase == waitForApplySync {\n\t\tpodhttpchaos := &v1alpha1.PodHttpChaos{}\n\t\terr := impl.Client.Get(ctx, controller.ParseNamespacedName(record.Id), podhttpchaos)\n\t\tif err != nil {\n\t\t\treturn waitForApplySync, err\n\t\t}\n\n\t\tif podhttpchaos.Status.FailedMessage != \"\" {\n\t\t\treturn waitForApplySync, errors.New(podhttpchaos.Status.FailedMessage)\n\t\t}\n\n\t\tif podhttpchaos.Status.ObservedGeneration >= httpchaos.Status.Instances[record.Id] {\n\t\t\treturn v1alpha1.Injected, nil\n\t\t}\n\n\t\treturn waitForApplySync, nil\n\t}\n\n\tpodId, _ := controller.ParseNamespacedNameContainer(records[index].Id)\n\tvar pod v1.Pod\n\terr := impl.Client.Get(ctx, podId, &pod)\n\tif err != nil {\n\t\treturn v1alpha1.NotInjected, err\n\t}\n\n\tsource := httpchaos.Namespace + \"/\" + httpchaos.Name\n\tm := impl.builder.WithInit(source, types.NamespacedName{\n\t\tNamespace: pod.Namespace,\n\t\tName:      pod.Name,\n\t})\n\n\tm.T.Append(v1alpha1.PodHttpChaosRule{\n\t\tSource: m.Source,\n\t\tPort:   httpchaos.Spec.Port,\n\t\tPodHttpChaosBaseRule: v1alpha1.PodHttpChaosBaseRule{\n\t\t\tTarget: httpchaos.Spec.Target,\n\t\t\tSelector: v1alpha1.PodHttpChaosSelector{\n\t\t\t\tPort:            &httpchaos.Spec.Port,\n\t\t\t\tPath:            httpchaos.Spec.Path,\n\t\t\t\tMethod:          httpchaos.Spec.Method,\n\t\t\t\tCode:            httpchaos.Spec.Code,\n\t\t\t\tRequestHeaders:  httpchaos.Spec.RequestHeaders,\n\t\t\t\tResponseHeaders: httpchaos.Spec.ResponseHeaders,\n\t\t\t},\n\t\t\tActions: httpchaos.Spec.PodHttpChaosActions,\n\t\t},\n\t})\n\tgenerationNumber, err := m.Commit(ctx)\n\tif err != nil {\n\t\treturn v1alpha1.NotInjected, err\n\t}\n\n\t// modify the custom status\n\thttpchaos.Status.Instances[record.Id] = generationNumber\n\treturn waitForApplySync, nil\n}\n\nfunc (impl *Impl) Recover(ctx context.Context, index int, records []*v1alpha1.Record, obj v1alpha1.InnerObject) (v1alpha1.Phase, error) {\n\t// The only possible phase to get in here is \"Injected\" or \"Injected/Wait\"\n\n\thttpchaos := obj.(*v1alpha1.HTTPChaos)\n\tif httpchaos.Status.Instances == nil {\n\t\thttpchaos.Status.Instances = make(map[string]int64)\n\t}\n\n\trecord := records[index]\n\tphase := record.Phase\n\tif phase == waitForRecoverSync {\n\t\tpodhttpchaos := &v1alpha1.PodHttpChaos{}\n\t\terr := impl.Client.Get(ctx, controller.ParseNamespacedName(record.Id), podhttpchaos)\n\t\tif err != nil {\n\t\t\t// TODO: handle this error\n\t\t\tif k8sError.IsNotFound(err) {\n\t\t\t\treturn v1alpha1.NotInjected, nil\n\t\t\t}\n\n\t\t\tif k8sError.IsForbidden(err) {\n\t\t\t\tif strings.Contains(err.Error(), \"because it is being terminated\") {\n\t\t\t\t\treturn v1alpha1.NotInjected, nil\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn waitForRecoverSync, err\n\t\t}\n\n\t\tif podhttpchaos.Status.FailedMessage != \"\" {\n\t\t\treturn waitForRecoverSync, errors.New(podhttpchaos.Status.FailedMessage)\n\t\t}\n\n\t\tif podhttpchaos.Status.ObservedGeneration >= httpchaos.Status.Instances[record.Id] {\n\t\t\treturn v1alpha1.NotInjected, nil\n\t\t}\n\n\t\treturn waitForRecoverSync, nil\n\t}\n\n\tpodId, _ := controller.ParseNamespacedNameContainer(records[index].Id)\n\tvar pod v1.Pod\n\terr := impl.Client.Get(ctx, podId, &pod)\n\tif err != nil {\n\t\t// TODO: handle this error\n\t\tif k8sError.IsNotFound(err) {\n\t\t\treturn v1alpha1.NotInjected, nil\n\t\t}\n\t\treturn v1alpha1.NotInjected, err\n\t}\n\n\tsource := httpchaos.Namespace + \"/\" + httpchaos.Name\n\tm := impl.builder.WithInit(source, types.NamespacedName{\n\t\tNamespace: pod.Namespace,\n\t\tName:      pod.Name,\n\t})\n\n\tgenerationNumber, err := m.Commit(ctx)\n\tif err != nil {\n\t\tif err == podiochaosmanager.ErrPodNotFound || err == podiochaosmanager.ErrPodNotRunning {\n\t\t\treturn v1alpha1.NotInjected, nil\n\t\t}\n\n\t\tif k8sError.IsForbidden(err) {\n\t\t\tif strings.Contains(err.Error(), \"because it is being terminated\") {\n\t\t\t\treturn v1alpha1.NotInjected, nil\n\t\t\t}\n\t\t}\n\t\treturn v1alpha1.Injected, err\n\t}\n\n\t// Now modify the custom status and phase\n\thttpchaos.Status.Instances[record.Id] = generationNumber\n\treturn waitForRecoverSync, nil\n}\n\nfunc NewImpl(c client.Client, b *podhttpchaosmanager.Builder, log logr.Logger) *common.ChaosImplPair {\n\treturn &common.ChaosImplPair{\n\t\tName:   \"httpchaos\",\n\t\tObject: &v1alpha1.HTTPChaos{},\n\t\tImpl: &Impl{\n\t\t\tClient:  c,\n\t\t\tLog:     log.WithName(\"httpchaos\"),\n\t\t\tbuilder: b,\n\t\t},\n\t\tObjectList: &v1alpha1.HTTPChaosList{},\n\t\tControlls:  []runtime.Object{&v1alpha1.PodHttpChaos{}},\n\t}\n}\n\nvar Module = fx.Provide(\n\tfx.Annotated{\n\t\tGroup:  \"impl\",\n\t\tTarget: NewImpl,\n\t},\n\tpodhttpchaosmanager.NewBuilder,\n)\n", "idx": 1, "id": 23450, "msg": "Please use little camel case variable name: `namespacedName`", "proj": "chaos-mesh-chaos-mesh", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -29,6 +29,7 @@ import java.util.Map;\n import java.util.stream.Stream;\n import org.apache.avro.generic.GenericData;\n import org.apache.avro.util.Utf8;\n+import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.encryption.EncryptedFiles;", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\npackage org.apache.iceberg.spark.source;\n\nimport java.io.Closeable;\nimport java.io.IOException;\nimport java.math.BigDecimal;\nimport java.nio.ByteBuffer;\nimport java.util.Collections;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.stream.Stream;\nimport org.apache.avro.generic.GenericData;\nimport org.apache.avro.util.Utf8;\nimport org.apache.iceberg.CombinedScanTask;\nimport org.apache.iceberg.FileScanTask;\nimport org.apache.iceberg.encryption.EncryptedFiles;\nimport org.apache.iceberg.encryption.EncryptedInputFile;\nimport org.apache.iceberg.encryption.EncryptionManager;\nimport org.apache.iceberg.io.CloseableIterator;\nimport org.apache.iceberg.io.FileIO;\nimport org.apache.iceberg.io.InputFile;\nimport org.apache.iceberg.relocated.com.google.common.base.Preconditions;\nimport org.apache.iceberg.relocated.com.google.common.collect.Maps;\nimport org.apache.iceberg.types.Type;\nimport org.apache.iceberg.util.ByteBuffers;\nimport org.apache.spark.rdd.InputFileBlockHolder;\nimport org.apache.spark.sql.types.Decimal;\nimport org.apache.spark.unsafe.types.UTF8String;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Base class of Spark readers.\n *\n * @param <T> is the Java class returned by this reader whose objects contain one or more rows.\n */\nabstract class BaseDataReader<T> implements Closeable {\n  private static final Logger LOG = LoggerFactory.getLogger(BaseDataReader.class);\n\n  private final Iterator<FileScanTask> tasks;\n  private final Map<String, InputFile> inputFiles;\n\n  private CloseableIterator<T> currentIterator;\n  private T current = null;\n  private FileScanTask currentTask = null;\n\n  BaseDataReader(CombinedScanTask task, FileIO io, EncryptionManager encryptionManager) {\n    this.tasks = task.files().iterator();\n    Map<String, ByteBuffer> keyMetadata = Maps.newHashMap();\n    task.files().stream()\n        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n        .forEach(file -> keyMetadata.put(file.path().toString(), file.keyMetadata()));\n    Stream<EncryptedInputFile> encrypted = keyMetadata.entrySet().stream()\n        .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));\n\n    // decrypt with the batch call to avoid multiple RPCs to a key server, if possible\n    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(encrypted::iterator);\n\n    Map<String, InputFile> files = Maps.newHashMapWithExpectedSize(task.files().size());\n    decryptedFiles.forEach(decrypted -> files.putIfAbsent(decrypted.location(), decrypted));\n    this.inputFiles = Collections.unmodifiableMap(files);\n\n    this.currentIterator = CloseableIterator.empty();\n  }\n\n  public boolean next() throws IOException {\n    try {\n      while (true) {\n        if (currentIterator.hasNext()) {\n          this.current = currentIterator.next();\n          return true;\n        } else if (tasks.hasNext()) {\n          this.currentIterator.close();\n          this.currentTask = tasks.next();\n          this.currentIterator = open(currentTask);\n        } else {\n          this.currentIterator.close();\n          return false;\n        }\n      }\n    } catch (IOException | RuntimeException e) {\n      if (currentTask != null && !currentTask.isDataTask()) {\n        LOG.error(\"Error reading file: {}\", getInputFile(currentTask).location(), e);\n      }\n      throw e;\n    }\n  }\n\n  public T get() {\n    return current;\n  }\n\n  abstract CloseableIterator<T> open(FileScanTask task);\n\n  @Override\n  public void close() throws IOException {\n    InputFileBlockHolder.unset();\n\n    // close the current iterator\n    this.currentIterator.close();\n\n    // exhaust the task iterator\n    while (tasks.hasNext()) {\n      tasks.next();\n    }\n  }\n\n  protected InputFile getInputFile(FileScanTask task) {\n    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n    return inputFiles.get(task.file().path().toString());\n  }\n\n  protected InputFile getInputFile(String location) {\n    return inputFiles.get(location);\n  }\n\n  protected static Object convertConstant(Type type, Object value) {\n    if (value == null) {\n      return null;\n    }\n\n    switch (type.typeId()) {\n      case DECIMAL:\n        return Decimal.apply((BigDecimal) value);\n      case STRING:\n        if (value instanceof Utf8) {\n          Utf8 utf8 = (Utf8) value;\n          return UTF8String.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n        }\n        return UTF8String.fromString(value.toString());\n      case FIXED:\n        if (value instanceof byte[]) {\n          return value;\n        } else if (value instanceof GenericData.Fixed) {\n          return ((GenericData.Fixed) value).bytes();\n        }\n        return ByteBuffers.toByteArray((ByteBuffer) value);\n      case BINARY:\n        return ByteBuffers.toByteArray((ByteBuffer) value);\n      default:\n    }\n    return value;\n  }\n}\n", "idx": 1, "id": 32835, "msg": "", "proj": "apache-iceberg", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -33,6 +33,7 @@ import (\n \t\"github.com/temporalio/temporal/common/log/tag\"\n \t\"github.com/temporalio/temporal/common/persistence\"\n \t\"github.com/temporalio/temporal/common/primitives\"\n+\ttasklistpb \"go.temporal.io/temporal-proto/tasklist\"\n )\n \n type (", "y": 1, "oldf": "// The MIT License\n//\n// Copyright (c) 2020 Temporal Technologies Inc.  All rights reserved.\n//\n// Copyright (c) 2020 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage matching\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/temporalio/temporal/.gen/proto/persistenceblobs\"\n\t\"github.com/temporalio/temporal/common/log\"\n\t\"github.com/temporalio/temporal/common/log/tag\"\n\t\"github.com/temporalio/temporal/common/persistence\"\n\t\"github.com/temporalio/temporal/common/primitives\"\n)\n\ntype (\n\ttaskListDB struct {\n\t\tsync.Mutex\n\t\tnamespaceID  primitives.UUID\n\t\ttaskListName string\n\t\ttaskListKind int32\n\t\ttaskType     int32\n\t\trangeID      int64\n\t\tackLevel     int64\n\t\tstore        persistence.TaskManager\n\t\tlogger       log.Logger\n\t}\n\ttaskListState struct {\n\t\trangeID  int64\n\t\tackLevel int64\n\t}\n)\n\n// newTaskListDB returns an instance of an object that represents\n// persistence view of a taskList. All mutations / reads to taskLists\n// wrt persistence go through this object.\n//\n// This class will serialize writes to persistence that do condition updates. There are\n// two reasons for doing this:\n// - To work around known Cassandra issue where concurrent LWT to the same partition cause timeout errors\n// - To provide the guarantee that there is only writer who updates taskList in persistence at any given point in time\n//   This guarantee makes some of the other code simpler and there is no impact to perf because updates to tasklist are\n//   spread out and happen in background routines\nfunc newTaskListDB(store persistence.TaskManager, namespaceID primitives.UUID, name string, taskType int32, kind int32, logger log.Logger) *taskListDB {\n\treturn &taskListDB{\n\t\tnamespaceID:  namespaceID,\n\t\ttaskListName: name,\n\t\ttaskListKind: kind,\n\t\ttaskType:     taskType,\n\t\tstore:        store,\n\t\tlogger:       logger,\n\t}\n}\n\n// RangeID returns the current persistence view of rangeID\nfunc (db *taskListDB) RangeID() int64 {\n\tdb.Lock()\n\tdefer db.Unlock()\n\treturn db.rangeID\n}\n\n// RenewLease renews the lease on a tasklist. If there is no previous lease,\n// this method will attempt to steal tasklist from current owner\nfunc (db *taskListDB) RenewLease() (taskListState, error) {\n\tdb.Lock()\n\tdefer db.Unlock()\n\tresp, err := db.store.LeaseTaskList(&persistence.LeaseTaskListRequest{\n\t\tNamespaceID:  db.namespaceID,\n\t\tTaskList:     db.taskListName,\n\t\tTaskType:     db.taskType,\n\t\tTaskListKind: db.taskListKind,\n\t\tRangeID:      atomic.LoadInt64(&db.rangeID),\n\t})\n\tif err != nil {\n\t\treturn taskListState{}, err\n\t}\n\tdb.ackLevel = resp.TaskListInfo.Data.AckLevel\n\tdb.rangeID = resp.TaskListInfo.RangeID\n\treturn taskListState{rangeID: db.rangeID, ackLevel: db.ackLevel}, nil\n}\n\n// UpdateState updates the taskList state with the given value\nfunc (db *taskListDB) UpdateState(ackLevel int64) error {\n\tdb.Lock()\n\tdefer db.Unlock()\n\t_, err := db.store.UpdateTaskList(&persistence.UpdateTaskListRequest{\n\t\tTaskListInfo: &persistenceblobs.TaskListInfo{\n\t\t\tNamespaceId: db.namespaceID,\n\t\t\tName:        db.taskListName,\n\t\t\tTaskType:    db.taskType,\n\t\t\tAckLevel:    ackLevel,\n\t\t\tKind:        db.taskListKind,\n\t\t},\n\t\tRangeID: db.rangeID,\n\t})\n\tif err == nil {\n\t\tdb.ackLevel = ackLevel\n\t}\n\treturn err\n}\n\n// CreateTasks creates a batch of given tasks for this task list\nfunc (db *taskListDB) CreateTasks(tasks []*persistenceblobs.AllocatedTaskInfo) (*persistence.CreateTasksResponse, error) {\n\tdb.Lock()\n\tdefer db.Unlock()\n\treturn db.store.CreateTasks(\n\t\t&persistence.CreateTasksRequest{\n\t\t\tTaskListInfo: &persistence.PersistedTaskListInfo{\n\t\t\t\tData: &persistenceblobs.TaskListInfo{\n\t\t\t\t\tNamespaceId: db.namespaceID,\n\t\t\t\t\tName:        db.taskListName,\n\t\t\t\t\tTaskType:    db.taskType,\n\t\t\t\t\tAckLevel:    db.ackLevel,\n\t\t\t\t\tKind:        db.taskListKind,\n\t\t\t\t},\n\t\t\t\tRangeID: db.rangeID,\n\t\t\t},\n\t\t\tTasks: tasks,\n\t\t})\n}\n\n// GetTasks returns a batch of tasks between the given range\nfunc (db *taskListDB) GetTasks(minTaskID int64, maxTaskID int64, batchSize int) (*persistence.GetTasksResponse, error) {\n\treturn db.store.GetTasks(&persistence.GetTasksRequest{\n\t\tNamespaceID:  db.namespaceID,\n\t\tTaskList:     db.taskListName,\n\t\tTaskType:     db.taskType,\n\t\tBatchSize:    batchSize,\n\t\tReadLevel:    minTaskID,  // exclusive\n\t\tMaxReadLevel: &maxTaskID, // inclusive\n\t})\n}\n\n// CompleteTask deletes a single task from this task list\nfunc (db *taskListDB) CompleteTask(taskID int64) error {\n\terr := db.store.CompleteTask(&persistence.CompleteTaskRequest{\n\t\tTaskList: &persistence.TaskListKey{\n\t\t\tNamespaceID: db.namespaceID,\n\t\t\tName:        db.taskListName,\n\t\t\tTaskType:    db.taskType,\n\t\t},\n\t\tTaskID: taskID,\n\t})\n\tif err != nil {\n\t\tdb.logger.Error(\"Persistent store operation failure\",\n\t\t\ttag.StoreOperationCompleteTask,\n\t\t\ttag.Error(err),\n\t\t\ttag.TaskID(taskID),\n\t\t\ttag.TaskType(db.taskType),\n\t\t\ttag.WorkflowTaskListName(db.taskListName))\n\t}\n\treturn err\n}\n\n// CompleteTasksLessThan deletes of tasks less than the given taskID. Limit is\n// the upper bound of number of tasks that can be deleted by this method. It may\n// or may not be honored\nfunc (db *taskListDB) CompleteTasksLessThan(taskID int64, limit int) (int, error) {\n\tn, err := db.store.CompleteTasksLessThan(&persistence.CompleteTasksLessThanRequest{\n\t\tNamespaceID:  db.namespaceID,\n\t\tTaskListName: db.taskListName,\n\t\tTaskType:     db.taskType,\n\t\tTaskID:       taskID,\n\t\tLimit:        limit,\n\t})\n\tif err != nil {\n\t\tdb.logger.Error(\"Persistent store operation failure\",\n\t\t\ttag.StoreOperationCompleteTasksLessThan,\n\t\t\ttag.Error(err),\n\t\t\ttag.TaskID(taskID),\n\t\t\ttag.TaskType(db.taskType),\n\t\t\ttag.WorkflowTaskListName(db.taskListName))\n\t}\n\treturn n, err\n}\n", "idx": 1, "id": 9539, "msg": "I think you need to run `make goimports` to fix ordering here.", "proj": "temporalio-temporal", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -552,8 +552,9 @@ Ex_Lob_Error ExLob::statSourceFile(char *srcfile, Int64 &sourceEOF)\n    if (srcType == HDFS_FILE)\n      {\n        hdfsFile sourceFile = hdfsOpenFile(fs_,srcfile,O_RDONLY,0,0,0);   \n-       if (!sourceFile)\t\t\t\t\t\t\t\t\n-\t  return LOB_SOURCE_FILE_OPEN_ERROR;\t\t\t\t\t\t\t\t\t\t \n+       if (!sourceFile)\t           \n+         return LOB_SOURCE_FILE_OPEN_ERROR;\n+         \t\t\t\t\t\t\t\t\t\t \n        hdfsFileInfo *sourceFileInfo = hdfsGetPathInfo(fs_,srcfile);\n        // get EOD from source hdfs file.\n        if (sourceFileInfo)", "y": 1, "oldf": "/**********************************************************************\n// @@@ START COPYRIGHT @@@\n//\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n//\n// @@@ END COPYRIGHT @@@\n**********************************************************************/\n/* -*-C++-*-\n *****************************************************************************\n *\n * File:         EXLOBaccess.cpp\n * Description:  class to store and retrieve LOB data.\n *               \n *               \n * Created:      10/29/2012\n * Language:     C++\n *\n *\n *\n *\n *****************************************************************************\n */\n\n#include <stdio.h>\n#include <unistd.h>\n#include <stdlib.h>\n#include <malloc.h>\n#include <string>\n#include <errno.h>\n#include <signal.h>\n#include <sys/file.h>\n#include <sys/stat.h>\n#include <sys/time.h>\n\n\n\n\n#include \"hdfs.h\"\n#include \"jni.h\"\n\n\n#include \"ExpLOBstats.h\"\n#include \"ExpLOBaccess.h\"\n#include \"ExpLOBinterface.h\"\n#include \"ExpLOBexternal.h\"\n#include \"NAVersionedObject.h\"\n#include \"ComQueue.h\"\n\n#include \"NAMemory.h\"\n#include <seabed/ms.h>\n#include <seabed/fserr.h>\n#include <curl/curl.h>\n#include <../../sqf/src/seabed/src/trans.h>\nextern int ms_transid_get(bool pv_supp,\n                          bool pv_trace,\n                          MS_Mon_Transid_Type *pp_transid,\n                          MS_Mon_Transseq_Type *pp_startid);\nextern int ms_transid_reinstate(MS_Mon_Transid_Type, MS_Mon_Transseq_Type);\n\n// short LobServerFNum;\nSB_Phandle_Type serverPhandle;\n\nExLob::ExLob() :\n    storage_(Lob_Invalid_Storage),\n    lobStorageLocation_(string()),\n    lobGlobalHeap_(NULL),\n    fs_(NULL),\n    fdData_(NULL),\n    openFlags_(0),\n    lobTrace_(FALSE)\n{\n  memset(lobDataFile_,'\\0',sizeof(lobDataFile_));\n    \n}\n\nExLob::~ExLob()\n{\n    \n    if (fdData_) {\n      hdfsCloseFile(fs_, fdData_);\n      fdData_ = NULL;\n    }\n   \n}\n\nEx_Lob_Error ExLob::initialize(char *lobFile, Ex_Lob_Mode mode, \n                               char *lobStorageLocation, \n\t\t\t       LobsStorage storage,\n                               char *hdfsServer, Int64 hdfsPort,\n                               char *lobLocation,\n                               int bufferSize , short replication ,\n                               int blockSize, Int64 lobMaxSize, ExLobGlobals *lobGlobals)\n{\n  int openFlags;\n  struct timespec startTime;\n  struct timespec endTime;\n  Int64 secs, nsecs, totalnsecs;\n \n  if (lobStorageLocation) \n    {\n      if (lobStorageLocation_.empty()) \n\t{\n\t  lobStorageLocation_ = string(lobStorageLocation);\n\t}\n\n      if (lobFile)\n        snprintf(lobDataFile_, MAX_LOB_FILE_NAME_LEN, \"%s/%s\", \n                 lobStorageLocation_.c_str(), \n                 lobFile);\n      \n    } \n  else \n    { \n      if (lobFile)\n        snprintf(lobDataFile_, MAX_LOB_FILE_NAME_LEN, \"%s\", lobFile);\n      \n    }\n\n  if (storage_ != Lob_Invalid_Storage) \n    {\n      return LOB_INIT_ERROR;\n    } else \n    {\n      storage_ = storage;\n    }\n\n  stats_.init(); \n\n  hdfsServer_ = hdfsServer;\n  hdfsPort_ = hdfsPort;\n  // lobLocation_ = lobLocation;\n  clock_gettime(CLOCK_MONOTONIC, &startTime);\n  \n  if (lobGlobals->getHdfsFs() == NULL)     \n    return LOB_HDFS_CONNECT_ERROR;\n  else \n    fs_ = lobGlobals->getHdfsFs();\n    \n\n  clock_gettime(CLOCK_MONOTONIC, &endTime);\n\n  secs = endTime.tv_sec - startTime.tv_sec;\n  nsecs = endTime.tv_nsec - startTime.tv_nsec;\n  if (nsecs < 0) \n    {\n      secs--;\n      nsecs += NUM_NSECS_IN_SEC;\n    }\n  totalnsecs = (secs * NUM_NSECS_IN_SEC) + nsecs;\n  stats_.hdfsConnectionTime += totalnsecs;\n    \n  if (mode == EX_LOB_CREATE) \n    { \n      // check if file is already created\n      hdfsFileInfo *fInfo = hdfsGetPathInfo(fs_, lobDataFile_);\n      if (fInfo != NULL) \n\t{\n\t  hdfsFreeFileInfo(fInfo, 1);\n\t  return LOB_DATA_FILE_CREATE_ERROR;\n\t} \n      openFlags = O_WRONLY | O_CREAT;   \n      fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags, bufferSize, replication, blockSize);\n      if (!fdData_) \n\t{\n          return LOB_DATA_FILE_CREATE_ERROR;\n\t}\n      hdfsCloseFile(fs_, fdData_);\n      fdData_ = NULL;\n     \n    }\n  lobGlobalHeap_ = lobGlobals->getHeap();    \n  return LOB_OPER_OK;\n    \n}\n\nEx_Lob_Error ExLob::fetchCursor(char *handleIn, Int32 handleLenIn, Int64 &outOffset, Int64 &outSize,NABoolean &isEOD, Int64 transId) \n{\n  Ex_Lob_Error err = LOB_OPER_OK;\n  Int64 dummyParam;\n  int cliErr=0;\n  Int64 offset = 0;\n  Int64 size = 0;\n  lobCursors_it it = lobCursors_.find(string(handleIn, handleLenIn));\n  char logBuf[4096];\n  lobDebugInfo(\"In ExLob::fetchCursor\",0,__LINE__,lobTrace_);\n  char *blackBox = new(getLobGlobalHeap()) char[MAX_LOB_FILE_NAME_LEN+6];\n  Int32 blackBoxLen = 0;\n\n   if (it == lobCursors_.end())\n   {\n      return LOB_CURSOR_NOT_OPEN;                         \n   }\n\n   void *cliInterface = it->second.cliInterface_;\n   \n   \n    cliErr = SQL_EXEC_LOBcliInterface(handleIn, handleLenIn, \n                                      blackBox, &blackBoxLen,\t\t       \n                                      (char *)&dummyParam, (Lng32 *)&dummyParam,\n                                     LOB_CLI_SELECT_FETCH, LOB_CLI_ExecImmed,\n                                     &offset, &size,\n                                     &dummyParam, &dummyParam, \n\t\t\t\t     &cliInterface,\n                                      transId,lobTrace_);\n    if (cliErr <0 ) \n      {\n        str_sprintf(logBuf, \"LOB_CLI_SELECT_FETCH Returned cli error  %d\",cliErr);\n        lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n\n        err = LOB_DESC_READ_ERROR;\n        return err;\n      }\n    if (cliErr == 100 )\n      {\n        isEOD= TRUE;\n        cliErr = SQL_EXEC_LOBcliInterface(handleIn, handleLenIn, \n\t\t\t\t     NULL, NULL,\n                                     (char *)&dummyParam, (Lng32 *)&dummyParam,\n                                     LOB_CLI_SELECT_CLOSE, LOB_CLI_ExecImmed,\n                                     &dummyParam, &dummyParam,\n                                     &dummyParam, &dummyParam, \n\t\t\t\t     &cliInterface,\n                                          transId,lobTrace_);\n        if (cliErr <0 ) \n          {\n            str_sprintf(logBuf, \"LOB_CLI_SELECT_CLOSE Returned cli error  %d\",cliErr);\n            lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n            err = LOB_DESC_READ_ERROR;\n            return err;\n          }\n        \n      }\n    else\n      {\n        if (blackBox && blackBoxLen >0 )\n          {\n            // we have received the external data file name from the descriptor table\n            // replace the contents of the lobDataFile with this name \n            str_cpy_and_null(lobDataFile_, blackBox, blackBoxLen,'\\0','0',TRUE);\n       \n          }\n        outOffset = offset;\n        outSize = size;\n      }\n\n    str_sprintf(logBuf, \" Returned after ::fetchCursor %Ld,%Ld\",outOffset,outSize);\n    lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n\n    return err;\n}\n\n\n\nEx_Lob_Error ExLob::getDesc(ExLobDesc &desc,char * handleIn, Int32 handleInLen, char *blackBox, Int32 *blackBoxLen, char *handleOut, Int32 &handleOutLen, Int64 transId) \n{\n    Ex_Lob_Error err = LOB_OPER_OK; \n    NABoolean multipleChunks = FALSE;\n    Int32 clierr = 0;\n    Int64 size,offset,dummyParam = 0;\n    char logBuf[4096];\n    lobDebugInfo(\"In ExLob::getDesc\",0,__LINE__,lobTrace_);\n \n\n     clierr = SQL_EXEC_LOBcliInterface(handleIn, \n                                       handleInLen, \n                                       blackBox, blackBoxLen,\n                                       handleOut, &handleOutLen,\n                                       LOB_CLI_SELECT_UNIQUE, LOB_CLI_ExecImmed,\n                                       &offset, &size,\n                                       &dummyParam, &dummyParam, \n                                       0,\n                                       transId,lobTrace_);\n     \n     if (clierr < 0) \n       return LOB_DESC_READ_ERROR;\n    desc.setOffset(offset);\n    desc.setSize(size);\n   \n    str_sprintf(logBuf,\"After Cli LOB_CLI_SELECT_UNIQUE:descOffset:%Ld, descSize: %Ld\",offset,size);\n    lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n    return err;\n}\n\n\nEx_Lob_Error ExLob::writeData(Int64 offset, char *data, Int32 size, Int64 &operLen)\n{ \n    Ex_Lob_Error err;\n    lobDebugInfo(\"In ExLob::writeData\",0,__LINE__,lobTrace_);\n    if (!fdData_ || (openFlags_ != (O_WRONLY | O_APPEND))) // file is not open for write\n    {\n      // get file info\n      hdfsFileInfo *fInfo = hdfsGetPathInfo(fs_, lobDataFile_);\n      if (fInfo == NULL) {\n         return LOB_DATA_FILE_NOT_FOUND_ERROR;\n      }\n    }\n     hdfsCloseFile(fs_, fdData_);\n     fdData_=NULL;\n     openFlags_ = O_WRONLY | O_APPEND; \n     fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, 0, 0, 0);\n     if (!fdData_) {\n       openFlags_ = -1;\n       return LOB_DATA_FILE_OPEN_ERROR;\n     }\n\n     if ((operLen = hdfsWrite(fs_, fdData_, data, size)) == -1) {\n       return LOB_DATA_WRITE_ERROR;\n     }\n     if (hdfsFlush(fs_, fdData_)) {\n       return LOB_DATA_FLUSH_ERROR;\n     }\n\n    \n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::writeDataSimple(char *data, Int64 size, LobsSubOper subOperation, Int64 &operLen,\n                                    int bufferSize , short replication , int blockSize)\n{ \n    Ex_Lob_Error err;\n\n\n    if (!fdData_ || (openFlags_ != (O_WRONLY | O_APPEND))) // file is not open for write\n    {\n      // get file info\n      hdfsFileInfo *fInfo = hdfsGetPathInfo(fs_, lobDataFile_);\n      if (fInfo == NULL) {\n         return LOB_DATA_FILE_NOT_FOUND_ERROR;\n      } else { \n         // file exists, check the size\n         if (fInfo->mSize != 0) {\n            hdfsFreeFileInfo(fInfo, 1);\n            return LOB_DATA_FILE_NOT_EMPTY_ERROR;\n         }\n      }\n      hdfsCloseFile(fs_, fdData_);\n      fdData_=NULL;\n      openFlags_ = O_WRONLY | O_APPEND ; \n      fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, bufferSize, replication, blockSize);\n      if (!fdData_) {\n         openFlags_ = -1;\n         return LOB_DATA_FILE_OPEN_ERROR;\n      }\n    }\n    if (hdfsWrite(fs_, fdData_, data, size) == -1) {\n      return LOB_DATA_WRITE_ERROR;\n    }\n    if (hdfsFlush(fs_, fdData_)) {\n      return LOB_DATA_FLUSH_ERROR;\n    }\n\n\n    operLen = size;\n\n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::dataModCheck2(\n       char * dirPath, \n       Int64  inputModTS,\n       Lng32  numOfPartLevels)\n{\n  if (numOfPartLevels == 0)\n    return LOB_OPER_OK;\n\n  Lng32 currNumFilesInDir = 0;\n  hdfsFileInfo * fileInfos = \n    hdfsListDirectory(fs_, dirPath, &currNumFilesInDir);\n  if ((currNumFilesInDir > 0) && (fileInfos == NULL))\n    {\n      return LOB_DATA_FILE_NOT_FOUND_ERROR;\n    }\n\n  NABoolean failed = FALSE;\n  for (Lng32 i = 0; ((NOT failed) && (i < currNumFilesInDir)); i++)\n    {\n      hdfsFileInfo &fileInfo = fileInfos[i];\n      if (fileInfo.mKind == kObjectKindDirectory)\n        {\n          Int64 currModTS = fileInfo.mLastMod;\n          if ((inputModTS > 0) &&\n              (currModTS > inputModTS))\n            failed = TRUE;\n        }\n    }\n\n  hdfsFreeFileInfo(fileInfos, currNumFilesInDir);\n  if (failed)\n    return LOB_DATA_MOD_CHECK_ERROR;\n\n  numOfPartLevels--;\n  Ex_Lob_Error err = LOB_OPER_OK;\n  if (numOfPartLevels > 0)\n    {\n      for (Lng32 i = 0; ((NOT failed) && (i < currNumFilesInDir)); i++)\n        {\n          hdfsFileInfo &fileInfo = fileInfos[i];\n          err = dataModCheck2(fileInfo.mName, inputModTS, numOfPartLevels);\n          if (err != LOB_OPER_OK)\n            return err;\n        }\n    }\n\n  return LOB_OPER_OK;\n}\n\n// numOfPartLevels: 0, if not partitioned\n//                  N, number of partitioning cols\nEx_Lob_Error ExLob::dataModCheck(\n       char * dirPath, \n       Int64  inputModTS,\n       Lng32  numOfPartLevels,\n       ExLobGlobals *lobGlobals)\n{\n  // find mod time of root dir\n  hdfsFileInfo *fileInfos = hdfsGetPathInfo(fs_, dirPath);\n  if (fileInfos == NULL)\n    {\n      return LOB_DATA_FILE_NOT_FOUND_ERROR;\n    }\n    \n  Int64 currModTS = fileInfos[0].mLastMod;\n  hdfsFreeFileInfo(fileInfos, 1);\n  if ((inputModTS > 0) &&\n      (currModTS > inputModTS))\n    return LOB_DATA_MOD_CHECK_ERROR;\n\n  if (numOfPartLevels > 0)\n    {\n      return dataModCheck2(dirPath, inputModTS, numOfPartLevels);\n    }\n\n  return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::emptyDirectory(char *dirPath,\n                                   ExLobGlobals *lobGlobals)\n{\n  int retcode = 0;\n\n  hdfsFileInfo *fileInfos = hdfsGetPathInfo(fs_, dirPath);\n  if (fileInfos == NULL)\n    {\n      return LOB_DIR_NAME_ERROR;\n    }\n  \n  Lng32 currNumFilesInDir = 0;\n  fileInfos = hdfsListDirectory(fs_, dirPath, &currNumFilesInDir);\n  if ((currNumFilesInDir > 0) && (fileInfos == NULL))\n    {\n      return LOB_DATA_FILE_NOT_FOUND_ERROR;\n    }\n\n  if ((currNumFilesInDir == 0) && (fileInfos == NULL)) // empty directory\n    {\n      return LOB_OPER_OK;\n    }\n\n  // delete all files in this directory\n  NABoolean error = FALSE;\n  for (Lng32 i = 0; i < currNumFilesInDir; i++)\n    {\n      hdfsFileInfo &fileInfo = fileInfos[i];\n      if (fileInfo.mKind == kObjectKindFile)\n        {\n          retcode = hdfsDelete(fs_, fileInfo.mName, 0);\n          if (retcode != 0)\n            error = TRUE;\n        }\n    } // for\n\n  // recursively delete all files in sub-dirs\n  for (Lng32 i = 0; i < currNumFilesInDir; i++)\n    {\n      hdfsFileInfo &fileInfo = fileInfos[i];\n      if (fileInfo.mKind == kObjectKindDirectory)\n        {\n          retcode = emptyDirectory(fileInfo.mName, lobGlobals);\n          if (retcode != LOB_OPER_OK)\n            error = TRUE;\n        }\n    } // for\n\n  if (fileInfos)\n    {\n      hdfsFreeFileInfo(fileInfos, currNumFilesInDir);\n    }\n\n  if (error)\n    return LOB_DATA_FILE_DELETE_ERROR;\n  \n  return LOB_OPER_OK;\n}\n\nstruct MemoryStruct {\n  char *memory;\n  size_t size;\n  NAHeap *heap;\n};\n\n// callback for writing from http file to memory while dynamically growing the size.\nstatic size_t\nWriteMemoryCallback(void *contents, size_t size, size_t nmemb, void *userp)\n{\n  size_t realsize = size * nmemb;\n  struct MemoryStruct *mem = (struct MemoryStruct *)userp;\n\n  \n  mem->memory =  (char *)(mem->heap)->allocateMemory(mem->size + realsize + 1 );\n\n  if(mem->memory == NULL) {\n    /* out of memory! */\n    return 0;\n  }\n\n  memcpy(&(mem->memory[mem->size]), contents, realsize);\n  mem->size += realsize;\n  mem->memory[mem->size] = 0;\n\n  return realsize;\n}\n//Call back for retrieving http file header info\nstatic size_t header_throw_away(void *ptr, size_t size, size_t nmemb, void *data)\n{\n  /* we are not interested in the headers itself,\n     so we only return the size we would have saved ... */\n  return (size_t)(size * nmemb);\n}\n\n\nEx_Lob_Error ExLob::statSourceFile(char *srcfile, Int64 &sourceEOF)\n{\n   char logBuf[4096];\n   lobDebugInfo(\"In ExLob::statSourceFile\",0,__LINE__,lobTrace_);\n   // check if the source file is a hdfs file or from local file system.\n  LobInputOutputFileType srcType = fileType(srcfile);\n   if (srcType == HDFS_FILE)\n     {\n       hdfsFile sourceFile = hdfsOpenFile(fs_,srcfile,O_RDONLY,0,0,0);   \n       if (!sourceFile)\t\t\t\t\t\t\t\t\n\t  return LOB_SOURCE_FILE_OPEN_ERROR;\t\t\t\t\t\t\t\t\t\t \n       hdfsFileInfo *sourceFileInfo = hdfsGetPathInfo(fs_,srcfile);\n       // get EOD from source hdfs file.\n       if (sourceFileInfo)\n\t sourceEOF = sourceFileInfo->mSize;\n       else\n\t return LOB_SOURCE_FILE_OPEN_ERROR;\n       \n       str_sprintf(logBuf,\"Returning EOF of %Ld for file %s\", sourceEOF,srcfile);\n       lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n     }\n\n   else if (srcType == LOCAL_FILE)\n     {\n       int openFlags = O_RDONLY;\n       int fdSrcFile = open(srcfile, openFlags);\n       if (fdSrcFile < 0) {\n\t return LOB_SOURCE_FILE_OPEN_ERROR;\n       }\n\n       if (flock(fdSrcFile, LOCK_EX) == -1) {\n\t return LOB_SOURCE_FILE_LOCK_ERROR;\n       }\n\n       struct stat statbuf;\n       if (stat(srcfile, &statbuf) != 0) {\n\t return LOB_SOURCE_FILE_STAT_ERROR;\n       }\n\n       sourceEOF = statbuf.st_size;\n\n       \n\n       flock(fdSrcFile, LOCK_UN);\n       close(fdSrcFile);\n       str_sprintf(logBuf,\"Returning EOF of %Ld for file %s\", sourceEOF,srcfile);\n       lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n       \n     }\n   else if (srcType == CURL_FILE)\n     {\n       // This is an http/ftp file. Use curl interface to determine size\n       CURL *curl;\n       CURLcode res;\n       const time_t filetime = 0;\n        double filesize = 0;\n       curl = curl_easy_init();\n       if(curl) {\n\t curl_easy_setopt(curl, CURLOPT_URL, srcfile);\n\n\t /* find file size from header */\n\t /* No download if the file */\n\t curl_easy_setopt(curl, CURLOPT_NOBODY, 1L);\n\t /* Ask for filetime */\n\t curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1L);\n\t curl_easy_setopt(curl, CURLOPT_FILETIME, 1L);\n\t /* No header output: TODO 14.1 http-style HEAD output for ftp */\n\t curl_easy_setopt(curl, CURLOPT_HEADERFUNCTION,header_throw_away);\n\t curl_easy_setopt(curl, CURLOPT_HEADER, 0L);\n\t res = curl_easy_perform(curl);\n\t if(CURLE_OK == res) {\n           res = curl_easy_getinfo(curl, CURLINFO_CONTENT_LENGTH_DOWNLOAD, &filesize);\n\t   if (res == CURLE_OK)\n\t     {\n\t       Int64 temp_fs = 0;\n\t      \n               temp_fs = filesize;\n\t       \n\t      sourceEOF = temp_fs;\n\t     }\n\t   else\n\t      return LOB_SOURCE_FILE_STAT_ERROR;\n\t }\n\t curl_easy_cleanup(curl);\n       }\n\n     }\n\n   \n   return LOB_OPER_OK;\n}\n\n\n\nEx_Lob_Error ExLob::readSourceFile(char *srcfile, char *&fileData, Int32 &size, Int64 offset)\n {\n   Ex_Lob_Error lobErr = LOB_OPER_OK;\n   // check if the source file is a hdfs file or from local file system.\n \n   LobInputOutputFileType srcType = fileType(srcfile);\n   if (srcType == HDFS_FILE)\n     {\n       lobErr = readHdfsSourceFile(srcfile, fileData, size, offset);\n     }\n   else if (srcType == LOCAL_FILE)\n     {\n       lobErr = readLocalSourceFile(srcfile, fileData, size, offset);\n     }\n   else if(srcType == CURL_FILE)\n     {\n       lobErr = readExternalSourceFile((char *)srcfile, fileData, size, offset);\n     }\n   else\n     return LOB_SOURCE_FILE_OPEN_ERROR;\n  \n  return lobErr;\n }\nEx_Lob_Error ExLob::readHdfsSourceFile(char *srcfile, char *&fileData, Int32 &size, Int64 offset)\n {\n     char logBuf[4096];\n     str_sprintf(logBuf,\"Calling ::readHdfsSourceFile: %s Offset:%Ld, Size: %Ld\",srcfile, offset,size);\n     lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n  \n   \n     int openFlags = O_RDONLY;\n     hdfsFile fdSrcFile = hdfsOpenFile(fs_,srcfile, openFlags,0,0,0);\n     if (fdSrcFile == NULL) {\n       return LOB_SOURCE_FILE_OPEN_ERROR;\n     }\n\n     \n     fileData = (char *) (getLobGlobalHeap())->allocateMemory(size);\n     if (fileData == (char *)-1) {\n       return LOB_SOURCE_DATA_ALLOC_ERROR;\n     }\n\n     if (hdfsPread(fs_,fdSrcFile, offset,fileData, size) == -1) {\n       hdfsCloseFile(fs_,fdSrcFile);\n       getLobGlobalHeap()->deallocateMemory(fileData);\n       fileData = NULL;\n       return LOB_SOURCE_FILE_READ_ERROR;\n     }\n\n     \n     hdfsCloseFile(fs_,fdSrcFile);\n     \n     return LOB_OPER_OK;\n }\nEx_Lob_Error ExLob::readLocalSourceFile(char *srcfile, char *&fileData, Int32 &size, Int64 offset)\n   {  \n     char logBuf[4096];\n     str_sprintf(logBuf,\"Calling ::readLocalSourceFile: %s Offset:%Ld, Size: %Ld\",srcfile, offset,size);\n     lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n  \n     int openFlags = O_RDONLY;\n     int fdSrcFile = open(srcfile, openFlags);\n     if (fdSrcFile < 0 ) {\n       return LOB_SOURCE_FILE_OPEN_ERROR;\n     }\n\n     if (flock(fdSrcFile, LOCK_EX) == -1) {\n       return LOB_SOURCE_FILE_LOCK_ERROR;\n     }\n\n     struct stat statbuf;\n     if (stat(srcfile, &statbuf) != 0) {\n       return LOB_SOURCE_FILE_STAT_ERROR;\n     }\n\n     fileData = (char *) (getLobGlobalHeap())->allocateMemory(size);\n     if (fileData == (char *)-1) {\n       return LOB_SOURCE_DATA_ALLOC_ERROR;\n     }\n\n     if (pread(fdSrcFile, fileData, size, offset) == -1) {\n       close(fdSrcFile);\n       getLobGlobalHeap()->deallocateMemory(fileData);\n       fileData = NULL;\n       return LOB_SOURCE_FILE_READ_ERROR;\n     }\n\n     flock(fdSrcFile, LOCK_UN);\n     close(fdSrcFile);\n     \n     return LOB_OPER_OK ;\n   }\n\nEx_Lob_Error ExLob::readExternalSourceFile(char *srcfile, char *&fileData, Int32 &size,Int64 offset)\n{\n    CURL *curl;\n    CURLcode res;\n    struct MemoryStruct chunk;\n    chunk.memory = (char *) (getLobGlobalHeap())->allocateMemory(size);\n    chunk.size = 0;    /* no data at this point */\n    chunk.heap = getLobGlobalHeap();\n\n   curl = curl_easy_init();\n   if(curl) {\n     \n          curl_easy_setopt(curl, CURLOPT_URL, srcfile);\n\n          /* send all data to this function  */\n          curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteMemoryCallback);\n          /* we pass our 'chunk' struct to the callback function */\n          curl_easy_setopt(curl, CURLOPT_WRITEDATA, (void *)&chunk);\n\t  curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1L);\n          res = curl_easy_perform(curl);\n          curl_easy_cleanup(curl);\n\t  fileData = chunk.memory;\n          \n     }\n\n  return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::writeDesc(Int64 &sourceLen, char *source, LobsSubOper subOper, Int64 &descNumOut, Int64 &operLen, Int64 lobMaxSize,Int64 lobMaxChunkMemSize,Int64 lobGCLimit, char * handleIn, Int32 handleInLen, char *blackBox, Int32 *blackBoxLen, char *handleOut, Int32 &handleOutLen, void *lobGlobals)\n{\n  Ex_Lob_Error err=LOB_OPER_OK; \n    Int64 dataOffset = 0;\n    Int64 outDescPartnKey = 0;\n    Int64 outDescSyskey = 0;\n    Int32 clierr = 0;\n    char logBuf[4096];\n  \n    lobDebugInfo(\"In ExLob::writeDesc\",0,__LINE__,lobTrace_);\n    // Calculate sourceLen for each subOper.\n    if ((subOper == Lob_File)||(subOper == Lob_External))\n      {\n\terr = statSourceFile(source, sourceLen); \n\tif (err != LOB_OPER_OK)\n\t  return err;\n      }\n    if (sourceLen <= 0 || sourceLen > lobMaxSize)\n      {\n\treturn LOB_MAX_LIMIT_ERROR; //exceeded the size of the max lob size\n        //TBD trigger compaction\n      }\n    if (subOper != Lob_External) \n      {\n        lobDebugInfo(\"Calling ExLob::allocateDesc\",0,__LINE__,lobTrace_);\n        err = allocateDesc((unsigned int)sourceLen, descNumOut, dataOffset, lobMaxSize, lobMaxChunkMemSize,handleIn, handleInLen, lobGCLimit,lobGlobals);\n      }\n   \n    operLen = 0; \n    if (err != LOB_OPER_OK)\n      return err;\n   lobDebugInfo(\"Calling cli LOB_CLI_INSERT\",0,__LINE__,lobTrace_);\n   \n   clierr = SQL_EXEC_LOBcliInterface(handleIn, \n                                     handleInLen, \n\t\t\t\t     blackBox,\n                                     blackBoxLen,\n                                     handleOut, &handleOutLen,\n                                     LOB_CLI_INSERT, LOB_CLI_ExecImmed,\n                                     &dataOffset, &sourceLen,\n                                     &outDescPartnKey, &outDescSyskey, \n\t\t\t\t     0,\n\t\t\t\t     0,lobTrace_);\n    if (clierr < 0 ) {\n      str_sprintf(logBuf,\"CLI LOB_CLI_INSERT returned error %d\",clierr);\n      lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n  \n      return LOB_DESC_WRITE_ERROR;\n    }\n    return err;\n}\n\n\nEx_Lob_Error ExLob::insertDesc(Int64 offset, Int64 size,  char *handleIn, Int32 handleInLen,  char *handleOut, Int32 &handleOutLen, char *blackBox, Int32 blackBoxLen,void *lobGlobals) \n{\n  \n   Lng32 clierr;\n   Int64 dummyParam;\n   Int64 outDescSyskey = 0;\n   Int64 outDescPartnKey = 0;\n   handleOutLen = 0;\n   Int32 chunkNum = 1;\n \n   NABoolean foundUnused = FALSE;\n   char logBuf[4096];\n   lobDebugInfo(\"In ExLob::InsertDesc\",0,__LINE__,lobTrace_);\n   str_sprintf(logBuf,\"Calling Cli LOB_CLI_INSERT: Offset:%Ld, Size: %Ld\",\n               offset,size);\n   lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n   clierr = SQL_EXEC_LOBcliInterface(handleIn, \n                                     handleInLen, \n\t\t\t\t     NULL, &chunkNum,\n                                     handleOut, &handleOutLen,\n                                     LOB_CLI_INSERT, LOB_CLI_ExecImmed,\n                                     &offset, &size,\n                                     &outDescPartnKey, &outDescSyskey, \n\t\t\t\t     0,\n\t\t\t\t     0,lobTrace_);\n   str_sprintf(logBuf,\"After LOB_CLI_INSERT: ChunkNum: OutSyskey:%Ld\",\n               chunkNum,outDescSyskey);\n   lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n   lobDebugInfo(\"Leaving ExLob::InsertDesc\",0,__LINE__,lobTrace_);\n   if (clierr < 0 ) {\n     lobDebugInfo(\"LOB_CLI_INSERT cli call returned error :\",clierr,__LINE__,TRUE);\n     return LOB_DESC_WRITE_ERROR;\n    }\n   return LOB_OPER_OK;\n}\n\n\nEx_Lob_Error ExLob::writeLobData(char *source, Int64 sourceLen, LobsSubOper subOperation, Int64 tgtOffset,Int64 &operLen, Int64 lobMaxChunkMemSize)\n{\n    Ex_Lob_Error err=LOB_OPER_OK; \n    char logBuf[4096];\n    lobDebugInfo(\"In ExLob::writeLobData\",0,__LINE__,lobTrace_);\n    char *inputAddr = source;\n    Int64 readOffset = 0;\n    Int32 allocMemSize = 0;\n    Int64 inputSize = sourceLen;\n    Int64 writeOffset = tgtOffset;\n    if (subOperation == Lob_External)\n      return LOB_OPER_OK;\n    while(inputSize > 0)\n      {\n        allocMemSize = MINOF(lobMaxChunkMemSize, inputSize);\n\tif (subOperation == Lob_File) \n\t  {\n            str_sprintf(logBuf,\"reading source file %s allocMemSize : %Ld, readOffset:%Ld\", source,allocMemSize,readOffset);\n            lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n\t    err = readSourceFile(source, inputAddr, allocMemSize, readOffset);\n\t    if (err != LOB_OPER_OK)\n              {\n                lobDebugInfo(\"readSouceFile returned an error\",0,__LINE__,lobTrace_);\n                return err;    \n              } \n\t  }    \n\telse \n\t  { // in memory\n\t   \n\t  }\n\terr = writeData(writeOffset, inputAddr, allocMemSize, operLen);\n\tif (err != LOB_OPER_OK)\n\t  {\n            str_sprintf(logBuf,\"::writeData returned error .writeOffset:%Ld, allocMemSize:%Ld, operLen %Ld \", writeOffset,allocMemSize,operLen);\n            lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n\t    //handle errors that happen in one of the chunks.\n\t   return err;\n\t  }\n\tif (subOperation == Lob_File) {\n\t  writeOffset = writeOffset+allocMemSize;\n\t  readOffset = readOffset+allocMemSize;\n\t  inputSize = inputSize-lobMaxChunkMemSize;\n\t  getLobGlobalHeap()->deallocateMemory(inputAddr);\n          str_sprintf(logBuf,\"Bookkeeping for Lob_File source.writeOffset:%Ld, readOffset:%Ld, inputSize: %Ld \", writeOffset,readOffset, inputSize);\n          lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n\t}\n\telse\n\t  {\n\t    writeOffset = writeOffset+allocMemSize;\n\t    inputSize = inputSize-lobMaxChunkMemSize;\n\t    inputAddr = inputAddr+allocMemSize;\n            str_sprintf(logBuf,\"Bookkeeping for Lob_Memory source. writeOffset:%Ld,  inputSize: %Ld \", writeOffset, inputSize);\n            lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n\t  }\n      }\n    lobDebugInfo(\"Leaving ExLob::writeLobData\",0,__LINE__,lobTrace_);\t\n    hdfsCloseFile(fs_, fdData_);\n    fdData_=NULL;\n    return err;\n}\n\nEx_Lob_Error ExLob::readToMem(char *memAddr, Int64 size,  Int64 &operLen,char * handleIn, Int32 handleInLen, char *blackBox, Int32 blackBoxLen, char * handleOut, Int32 &handleOutLen, Int64 transId)\n{\n   Ex_Lob_Error err = LOB_OPER_OK; \n   NABoolean multipleChunks = FALSE;\n  \n   int cliErr;\n\n   operLen = 0;\n   ExLobDesc desc;\n   Int64 sizeToRead = 0;\n   char logBuf[4096];\n   lobDebugInfo(\"In ExLob::readToMem\",0,__LINE__,lobTrace_);\n   \n   err = getDesc(desc,handleIn,handleInLen,blackBox, &blackBoxLen,handleOut,handleOutLen,transId);\n   if (err != LOB_OPER_OK)\n     {\t   \n       return err;\n     }\n   sizeToRead = MINOF(size,desc.getSize());\n   if (blackBox)\n     {\n       \n       // we have received the external data file name from the descriptor table\n       // replace the contents of the lobDataFile with this name \n       str_cpy_and_null(lobDataFile_, blackBox, blackBoxLen,'\\0','0',TRUE);\n       \n     }\n   if (blackBoxLen == -1)\n     {\n       lobDebugInfo(\"Reading multiple chunks\",0,__LINE__,lobTrace_);\n       sizeToRead = size;\n       multipleChunks = TRUE;\n     }\n   str_sprintf(logBuf,\"sizeToRead:%Ld, desc.size :%Ld\", sizeToRead, desc.getSize());\n   lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n   err = readDataToMem(memAddr, desc.getOffset(),sizeToRead, operLen, handleIn,handleInLen, multipleChunks,transId);\n\n   return err;\n}\nLobInputOutputFileType ExLob::fileType(char *ioFileName)\n{\n  std::string fileTgt(ioFileName);\n  std:string hdfsDirStr(\"hdfs://\");\n  std::string httpStr(\"http://\");\n  std:: string fileDirStr(\"file://\");\n  short found = 0;\n  LobInputOutputFileType  filetype;\n  bool isHdfs = FALSE;\n  bool isLocal = FALSE;\n  bool isExternal = FALSE;\n  bool isHdfsDir = FALSE;\n  bool isFileDir = FALSE;\n  if (((found = fileTgt.find(hdfsDirStr)) != std::string::npos) && (found == 0))\n    {\n      return HDFS_FILE;\n      \n    }      \n    else if (((found = fileTgt.find(fileDirStr)) != std::string::npos) &&(found == 0))\n      return LOCAL_FILE;\n      \n     \n    else if (((found = fileTgt.find(httpStr)) != std::string::npos) && (found == 0))\n      return CURL_FILE;\n    \n    else\n      return LOCAL_FILE;\n}\nEx_Lob_Error ExLob::readToFile(char *tgtFileName, Int64 tgtLength, Int64 &operLen, Int64 lobMaxChunkMemLen, Int32 fileflags,char *handleIn,Int32 handleInLen, char *blackBox, Int32 blackBoxLen, char * handleOut, Int32 &handleOutLen, Int64 transId)\n{\n  char logBuf[4096];\n  lobDebugInfo(\"In ExLob::readToFile\",0,__LINE__,lobTrace_);\n  Ex_Lob_Error err = LOB_OPER_OK; \n  Int64 srcOffset = 0;\n  Int64 srcLength = 0;\n  LobInputOutputFileType tgtType = fileType(tgtFileName);\n  ExLobDesc desc;\n  NABoolean multipleChunks = FALSE;\n  err = getDesc(desc,handleIn,handleInLen,blackBox, &blackBoxLen,handleOut,handleOutLen,transId);\n  if (err != LOB_OPER_OK)\n    return err;\n  if (blackBoxLen == -1)  // mxlobsrvr returned -1 indicating multiple chunks for this particular lob handle\n    {\n      lobDebugInfo(\"Reading multiple chunks\",0,__LINE__,lobTrace_);\n      multipleChunks = TRUE;\n      //the data retrieval in chunks is handled in readDataToMem.\n    }\n  else if (tgtLength <=0 )\n    {\n      return LOB_SOURCE_FILE_READ_ERROR;\n    }\n  else\n    {\n      srcOffset = desc.getOffset();\n      \n    }\n  if (blackBox)\n     {\n       // we have received the external data file name from the descriptor table\n       // replace the contents of the lobDataFile with this name \n        str_cpy_and_null(lobDataFile_, blackBox, blackBoxLen,'\\0','0',TRUE);\n       \n     }\n  if (tgtType == HDFS_FILE)\n    {\n      err = readDataToHdfsFile(tgtFileName,  srcOffset , tgtLength,operLen, lobMaxChunkMemLen, fileflags,handleIn,handleInLen,multipleChunks,transId);\n      if (err != LOB_OPER_OK)\n\treturn err;\n    }\n  else if(tgtType == CURL_FILE)\n    {\n      err = readDataToExternalFile(tgtFileName, srcOffset, tgtLength, operLen, lobMaxChunkMemLen, fileflags,handleIn, handleInLen,multipleChunks,transId);\n      if (err != LOB_OPER_OK)\n\treturn err;\n    }\n  else if (tgtType == LOCAL_FILE)\n    { \n      err = readDataToLocalFile(tgtFileName,srcOffset, tgtLength,operLen, lobMaxChunkMemLen, fileflags,handleIn,handleInLen,multipleChunks,transId);\n      if (err != LOB_OPER_OK)\n\treturn err;\n    }\n  else\n    return LOB_TARGET_FILE_OPEN_ERROR; //unknown format\n\n  return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::append(char *data, Int64 size, LobsSubOper so, Int64 headDescNum, Int64 &operLen, Int64 lobMaxSize,Int64 lobMaxChunkMemSize, Int64 lobGCLimit, char *handleIn, Int32 handleInLen,  char * handleOut, Int32 &handleOutLen,void *lobGlobals)\n{\n    Ex_Lob_Error err = LOB_OPER_OK;\n    Int64 dummyParam;\n    Int64 dataOffset=0;\n    Int64 sourceLen = size;\n    Int32 clierr = 0;\n    Int32 chunkNum = 0;\n    Int64 outDescPartnKey, outDescSyskey = 0;\n    char logBuf[4096];\n    char *blackBox = NULL;\n    Int32 blackBoxLen = 0;\n\n    if (so ==Lob_External)\n      {\n        blackBox = data;\n        blackBoxLen = (Int32)size;\n      }\n    lobDebugInfo(\"In ExLob::append\",0,__LINE__,lobTrace_);\n\n    if ((so == Lob_File))\n      {\n\terr = statSourceFile(data, sourceLen); \n\tif (err != LOB_OPER_OK)\n\t  return err;\n      }\n   \n      \n    if (sourceLen <= 0 || sourceLen > lobMaxSize)\n      {\n        return LOB_MAX_LIMIT_ERROR; //exceeded the size of the max lob size\n      }\n    err = allocateDesc((unsigned int)sourceLen, dummyParam, dataOffset, lobMaxSize,lobMaxChunkMemSize,handleIn, handleInLen,lobGCLimit,lobGlobals);\n    if (err != LOB_OPER_OK)\n      return err;\n\n    lobDebugInfo(\"Calling cli LOB_CLI_INSERT_APPEND\",0,__LINE__,lobTrace_);\n    clierr = SQL_EXEC_LOBcliInterface(handleIn, handleInLen, \n\t\t\t\t      blackBox, &blackBoxLen,\n                                      handleOut, &handleOutLen,\n                                      LOB_CLI_INSERT_APPEND, LOB_CLI_ExecImmed,\n                                      &dataOffset, &sourceLen,\n                                      &outDescPartnKey, &outDescSyskey, \n\t\t\t\t      0,\n\t\t\t\t      0,lobTrace_);\n    \n    \n    if (clierr < 0 || clierr == 100) { // some error or EOD.\n      str_sprintf(logBuf,\"cli LOB_CLI_INSERT_APPEND returned :%d\", clierr);\n      lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n      return LOB_DESC_APPEND_ERROR;\n    }\n\n    char *inputAddr = data;\n    if (so == Lob_Buffer)\n      {\n\tinputAddr = (char *)(*(long *)data);\n      }\n     str_sprintf(logBuf,\"Calling writeLobData: inputAddr: %Ld, InputSize%Ld, tgtOffset:%Ld\",(long)inputAddr,sourceLen,dataOffset);\n    err = writeLobData(inputAddr, sourceLen,so,dataOffset,operLen,lobMaxChunkMemSize);\n    if (err != LOB_OPER_OK)\n      {\n        lobDebugInfo(\"writeLobData returned error\",0,__LINE__,lobTrace_);\n        return err;\n      }\n    return LOB_OPER_OK;\n}\nEx_Lob_Error ExLob::insertData(char *data, Int64 size, LobsSubOper so,Int64 headDescNum, Int64 &operLen, Int64 lobMaxSize, Int64 lobMaxChunkMemSize,char * handleIn, Int32 handleInLen, char *blackBox, Int32 blackBoxLen, char * handleOut, Int32 &handleOutLen, void *lobGlobals)\n{\n   Ex_Lob_Error err=LOB_OPER_OK; \n   ExLobDesc desc;\n   int clierr = 0;\n   operLen = 0;\n   char logBuf[4096];\n   lobDebugInfo(\"In ExLob::InsertData\",0,__LINE__,lobTrace_);\n   str_sprintf(logBuf,\"data:%Ld, size %Ld, lobMaxSize:%Ld, lobMaxChunkMemSize:%Ld\", (long)data, size,lobMaxSize,lobMaxChunkMemSize);\n  \n   // get offset and input size from desc (the one that was just  \n   // inserted into the descriptor handle table)\n   \n   err = getDesc(desc,handleIn,handleInLen,blackBox, &blackBoxLen,handleOut,handleOutLen,0);\n     \n    if (err !=LOB_OPER_OK) { // some error or EOD.\n       lobDebugInfo(\"getDesc returned error\",0,__LINE__,lobTrace_);\n       return LOB_DESC_READ_ERROR;\n    }\n    \n    if ((data == NULL)) { \n       return LOB_SOURCE_DATA_ERROR;\n    }\n\n    char *inputAddr = data;\n    if (so == Lob_Buffer)\n      {\n\tinputAddr = (char *)(*(long *)data);\n      }\n    Int64 inputSize = desc.getSize();\n    Int64 tgtOffset = desc.getOffset();\n    str_sprintf(logBuf,\"Calling writeLobData: inputAddr: %Ld, InputSize%Ld, tgtOffset:%Ld\",(long)inputAddr,inputSize,tgtOffset);\n\n    lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n    err = writeLobData(inputAddr, inputSize,so, tgtOffset, \n\t\t       operLen,lobMaxChunkMemSize);\n    if (err != LOB_OPER_OK){\n      lobDebugInfo(\"writeLobData returned error\",0,__LINE__,lobTrace_);\n      return err;\n    }\n    return LOB_OPER_OK;\n}\nEx_Lob_Error ExLob::update(char *data, Int64 size, LobsSubOper so,Int64 headDescNum, Int64 &operLen, Int64 lobMaxSize, Int64 lobMaxChunkMemSize, Int64 lobGCLimit, char *handleIn, Int32 handleInLen,  char *handleOut, Int32 &handleOutLen, void *lobGlobals)\n{\n    Ex_Lob_Error err = LOB_OPER_OK;\n    Int64 dummyParam;\n    Int64 dataOffset = 0;\n    Int64 sourceLen = size;\n    Int32 clierr = 0;\n    Int64 outDescPartnKey,outDescSyskey = 0;\n    Int32 chunkNum = 0;\n    char logBuf[4096];\n    char *blackBox = NULL;\n    Int32 blackBoxLen = 0;\n\n    if (so == Lob_External)\n      {\n        blackBox = data;\n        blackBoxLen = (Int32)size;\n      }\n    lobDebugInfo(\"In ExLob::update\",0,__LINE__,lobTrace_);\n    if ((so == Lob_File) || (so == Lob_External))\n      {\n        str_sprintf(logBuf,\"Calling statSourceFile: source:%s, sourceLen: %Ld\",\n               data,sourceLen);\n        lobDebugInfo(logBuf, 0,__LINE__,lobTrace_);\n\terr = statSourceFile(data, sourceLen); \n\tif (err != LOB_OPER_OK)\n\t  return err;\n      }\n    if(so != Lob_External)\n      {\n        if (sourceLen <= 0 || sourceLen > lobMaxSize)\n          {\n            return LOB_MAX_LIMIT_ERROR; //exceeded the size of the max lob size\n          }\n        lobDebugInfo(\"Calling allocateDesc\",0,__LINE__,lobTrace_);\n        err = allocateDesc((unsigned int)sourceLen, dummyParam, dataOffset, lobMaxSize, lobMaxChunkMemSize, handleIn, handleInLen, lobGCLimit,lobGlobals);\n        if (err != LOB_OPER_OK)\n          return err;\n      }\n    lobDebugInfo(\"Calling CLI LOB_CLI_UPDATE_UNIQUE\",0,__LINE__,lobTrace_);\n    clierr = SQL_EXEC_LOBcliInterface(handleIn, \n                                      handleInLen, \n\t\t\t\t      blackBox, &blackBoxLen,\n                                      handleOut, &handleOutLen,\n                                      LOB_CLI_UPDATE_UNIQUE, LOB_CLI_ExecImmed,\n                                      &dataOffset, &sourceLen,\n                                      &outDescPartnKey, &outDescSyskey, \n\t\t\t\t      0,\n\t\t\t\t      0,lobTrace_);\n    \n    if (clierr < 0 || clierr == 100) { // some error or EOD.\n      \n       return LOB_DESC_UPDATE_ERROR;\n    }\n    char *inputAddr = data;\n    if (so == Lob_Buffer)\n      {\n\tinputAddr = (char *)(*(long *)data);\n      }\n    str_sprintf(logBuf,\"Calling writeLobData.sourceLen:%Ld, dataOffset:%Ld\",sourceLen,dataOffset);\n    lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n           \n    err = writeLobData(inputAddr, sourceLen,so,dataOffset,operLen,lobMaxChunkMemSize);\n    str_sprintf(logBuf,\"writeLobData returned. operLen:%Ld\",operLen);\n    lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n    if (err != LOB_OPER_OK){\n       lobDebugInfo(\"writeLobData Failed\",0,__LINE__,lobTrace_); \n       return err;\n    }\n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::delDesc(char *handleIn, Int32 handleInLen, Int64 transId)\n{\n    Ex_Lob_Error err;   \n    Int64 offset=0;\n    Int64 dummyParam=0;  \n    Lng32 clierr=0;\n\n    clierr = SQL_EXEC_LOBcliInterface(handleIn, handleInLen, \n\t\t\t\t      0, 0,\n                                      (char *)&dummyParam, (Lng32 *)&dummyParam,\n                                      LOB_CLI_DELETE, LOB_CLI_ExecImmed,\n                                      &dummyParam, &dummyParam,\n                                      &dummyParam, &dummyParam, \n\t\t\t\t      0,\n\t\t\t\t      transId,lobTrace_);\n\n    if (clierr < 0)\n      return LOB_DESC_FILE_DELETE_ERROR;\n    \n    return LOB_OPER_OK;\n \n}\n\nEx_Lob_Error ExLob::purgeLob()\n{\n    char logBuf[4096];\n     if (hdfsDelete(fs_, lobDataFile_, 0) != 0)\n       {\n         str_sprintf(logBuf,\"hdfsDelete of %s returned error\",lobDataFile_);\n         lobDebugInfo(lobDataFile_,0,__LINE__,lobTrace_);\n\t return LOB_DATA_FILE_DELETE_ERROR;\n       }\n    \n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::openCursor(char *handleIn, Int32 handleInLen,Int64 transId)\n{\n    Ex_Lob_Error err;\n    cursor_t cursor;\n    Int32 clierr;\n    Int64 dummyParam = 0;\n    void *cliInterface = NULL;\n    char logBuf[4096];\n   \n    lobDebugInfo(\"In ExLob::openCursor\",0,__LINE__,lobTrace_);\n    clierr = SQL_EXEC_LOBcliInterface(handleIn, \n                                      handleInLen,\n\t\t\t\t      0,0,\n                                      (char *)&dummyParam, (Lng32 *)&dummyParam,\n                                      LOB_CLI_SELECT_CURSOR, LOB_CLI_ExecImmed,\n                                      &dummyParam, &dummyParam,\n                                      &dummyParam, &dummyParam, \n                                      &cliInterface,\n                                      transId,lobTrace_);\n\n    if (clierr <0 ) {\n      str_sprintf(logBuf,\"openCursor returned cliErr %d\",clierr);\n      return LOB_DESC_READ_ERROR;\n    }\n\n  \n    cursor.bytesRead_ = -1;\n    cursor.descOffset_ = -1;\n    cursor.descSize_ = -1;\n    cursor.cliInterface_ = cliInterface; // used only in lob process\n    cursor.eod_ = false;\n    cursor.eor_ = false; \n    cursor.eol_ = false;\n\n    \n    lobCursors_it it = lobCursors_.find(string(handleIn, handleInLen));\n\n    if (it == lobCursors_.end())\n    {\n      lobCursors_.insert(pair<string, cursor_t>\n                        (string(handleIn, handleInLen), cursor));\n    }\n    else\n    {\n       it->second = cursor;\n    } \n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::openDataCursor(char *file, LobsCursorType type, Int64 range, Int64 bufMaxSize, \n                                   Int64 maxBytes, Int64 waited, ExLobGlobals *lobGlobals)\n{\n    Ex_Lob_Error err;\n    cursor_t cursor;\n\n    clock_gettime(CLOCK_MONOTONIC, &cursor.openTime_);\n      \n    // check to see if cursor is already open. \n    // occurs for pre-open cases\n    lobCursorLock_.lock();\n\n    lobCursors_it it = lobCursors_.find(string(file, strlen(file)));\n    if (it != lobCursors_.end()) {\n      clock_gettime(CLOCK_MONOTONIC, &cursor.openTime_);\n      lobCursorLock_.unlock();\n      return LOB_OPER_OK;\n    }\n    \n    union ranges_t {\n      Int64 range64;\n      struct {\n        Lng32 beginRange;\n        Lng32 numRanges;\n      }r;\n    } ranges;\n        \n    cursor.bytesRead_ = -1;\n    cursor.descOffset_ = -1;\n    cursor.descSize_ = -1;\n    cursor.cliInterface_ = NULL; // used only in lob process\n    cursor.eod_ = false;\n    cursor.eor_ = false;\n    cursor.eol_ = false;\n    cursor.type_ = type;\n    cursor.bufMaxSize_ = bufMaxSize;\n    cursor.maxBytes_ = maxBytes;  \n    cursor.prefetch_ = !waited;\n    cursor.bufferHits_ = 0;\n    cursor.bufferMisses_ = 0;\n    strcpy(cursor.name_, file);\n\n    cursor.currentRange_ = -1;\n    cursor.endRange_ = -1;\n    cursor.currentStartOffset_ = -1;\n    cursor.descOffset_ = range;\n    \n    cursor.currentFd_ = NULL;\n    cursor.currentBytesToRead_ = -1;\n    cursor.currentBytesRead_ = 0;\n    cursor.currentEod_ = false;\n\n    lobCursors_.insert(pair<string, cursor_t> \n                     (string(file, strlen(file)), cursor));\n\n    it = lobCursors_.find(string(file, strlen(file))); // to get the actual cursor object in the map\n\n    if (!fdData_ || (openFlags_ != O_RDONLY)) \n    {\n      hdfsCloseFile(fs_, fdData_);\n      fdData_ = NULL;\n      openFlags_ = O_RDONLY;\n      fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, 0, 0, 0);\n      if (!fdData_) {\n        openFlags_ = -1;\n        lobCursorLock_.unlock();\n        return LOB_DATA_FILE_OPEN_ERROR;\n      }\n    }\n\n    if (hdfsSeek(fs_, fdData_, (it->second).descOffset_) == -1) {\n      lobCursorLock_.unlock();\n      return LOB_DATA_FILE_POSITION_ERROR;\n    }\n\n    // start reading in a worker thread\n    lobGlobals->enqueuePrefetchRequest(this, &(it->second));\n\n    lobCursorLock_.unlock();\n    \n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::readCursor(char *tgt, Int64 tgtSize, char *handleIn, Int32 handleInLen, Int64 &operLen,Int64 transId)\n{\n    int dataOffset;\n    Ex_Lob_Error result;\n    cursor_t cursor;\n    char logBuf[4096];\n    lobCursors_it it = lobCursors_.find(string(handleIn, handleInLen));\n    lobDebugInfo(\"In ExLob::readCursor\",0,__LINE__,lobTrace_);\n    if (it == lobCursors_.end())\n    {\n       return LOB_CURSOR_NOT_OPEN;\n    }\n    else\n    {\n       cursor = it->second; \n    } \n    str_sprintf(logBuf,\"ExLob::readCursor:: cliInterface:%Ld,bytesRead_:%Ld,descOffset_:%LddescSize_:%Ld,eod_:%d,eor_:%d,eol_:%d,\",(long)cursor.cliInterface_,cursor.bytesRead_,cursor.descOffset_,cursor.descSize_,cursor.eod_,cursor.eor_,cursor.eol_);\n    lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n    if (cursor.eod_) {\n       // remove cursor from the map.\n       // server has already closed the cursor. \n       closeCursor(handleIn, handleInLen); \n       // indicate EOD to SQL\n       operLen = 0; \n       return LOB_OPER_OK;\n    }\n    \n    result = readCursorData(tgt, tgtSize, cursor, operLen, handleIn,handleInLen,transId); // increments cursor\n        \n    if (result != LOB_OPER_OK)\n      return result;\n\n    it->second = cursor;\n\n    return LOB_OPER_OK;\n}\n\n\n\n\nEx_Lob_Error ExLob::closeCursor(char *handleIn, Int32 handleInLen)\n{\n  char logBuf[4096];\n  lobCursors_it it = lobCursors_.find(string(handleIn, handleInLen));\n  if (it != lobCursors_.end())\n    {\n      str_sprintf(logBuf,\"closing cursor for handle\");\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);    \n      lobCursors_.erase(it);\n    }\n    return LOB_OPER_OK;\n}\n\n#ifdef __ignore\nEx_Lob_Error ExLob::doSanityChecks(char *dir, LobsStorage storage,\n                                   Int32 handleInLen, Int32 handleOutLen, \n                                   Int32 blackBoxLen)\n{\n\n#ifdef SQ_USE_HDFS\n    if (!fs_)\n      return LOB_HDFS_CONNECT_ERROR;\n#else\n    if (fdData_ == -1)\n      return LOB_DATA_FILE_OPEN_ERROR;\n#endif\n\n    if (dir_.compare(dir) != 0)\n      return LOB_DIR_NAME_ERROR;\n\n    if (storage_ != storage)\n      return LOB_STORAGE_TYPE_ERROR;\n\n    if (handleInLen > MAX_HANDLE_IN_LEN) {\n      return LOB_HANDLE_IN_LEN_ERROR;\n    }\n\n    if (handleOutLen > MAX_HANDLE_IN_LEN) {\n      return LOB_HANDLE_OUT_LEN_ERROR;\n    }\n\n    if (blackBoxLen > MAX_HANDLE_IN_LEN) {\n      return LOB_BLACK_BOX_LEN_ERROR;\n    }\n\n    return LOB_OPER_OK;\n}\n#endif\nEx_Lob_Error ExLob::allocateDesc(ULng32 size, Int64 &descNum, Int64 &dataOffset, Int64 lobMaxSize, Int64 lobMaxChunkMemLen, char *handleIn, Int32 handleInLen, Int64 lobGCLimit, void *lobGlobals)\n{\n  NABoolean GCDone = FALSE;\n    Ex_Lob_Error err = LOB_OPER_OK;\n    Lng32 retval = 0;\n    Int64 numRead = 0;\n    Int64 numWritten = 0;\n    dataOffset = 0;\n    Int64 dummyParam = 0;\n    if (size > lobMaxSize)\n      return LOB_MAX_LIMIT_ERROR;\n    char logBuf[4096];\n    lobDebugInfo(\"In ExLob::allocateDesc\",0,__LINE__,lobTrace_);\n    Int32 openFlags = O_RDONLY ;   \n    \n    hdfsFileInfo *fInfo = hdfsGetPathInfo(fs_, lobDataFile_);\n    if (fInfo)\n      dataOffset = fInfo->mSize;\n\n    if (dataOffset > lobGCLimit) // 5 GB default\n      {\n        str_sprintf(logBuf,\"Starting GC. Current Offset : %Ld\",dataOffset);\n        lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n           \n         \n        Int32 rc = SQL_EXEC_LOB_GC_Interface(lobGlobals,handleIn,handleInLen,\n                                             hdfsServer_,hdfsPort_,\n                                             (char *)lobStorageLocation_.c_str(),\n                                             lobMaxChunkMemLen,lobTrace_);\n       \n        if (rc<0)\n          {\n            lobDebugInfo(\"GC failed\",0,__LINE__,lobTrace_); \n            GCDone = FALSE;\n          }\n        else\n          GCDone = TRUE;\n           \n           \n      }\n      if (GCDone) // recalculate the new offset  \n        {  \n          hdfsFreeFileInfo(fInfo, 1);\n          fInfo = hdfsGetPathInfo(fs_, lobDataFile_);\n        }\n        \n      if (fInfo)\n        dataOffset = fInfo->mSize;\n      if (GCDone)\n        str_sprintf(logBuf,\"Done GC. Allocating new Offset %Ld in %s\",\n                    dataOffset,lobDataFile_);\n      else\n        str_sprintf(logBuf,\"Allocating new Offset %Ld in %s \",\n                    dataOffset,lobDataFile_);\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n      //Find the last offset in the file\n      // dataOffset = hdfsTell(fs_,fdData_);  //commenting out.hdfsTell always returns 0 !!\n     \n      return LOB_OPER_OK;    \n}\nEx_Lob_Error ExLob::compactLobDataFile(ExLobInMemoryDescChunksEntry *dcArray,Int32 numEntries)\n{\n  Ex_Lob_Error rc = LOB_OPER_OK;\n  char logBuf[4096];\n  lobDebugInfo(\"In ExLob::compactLobDataFile\",0,__LINE__,lobTrace_);\n  Int64 maxMemChunk = 1024*1024*1024; //1GB limit for intermediate buffer for transfering data\n  char * saveLobDataFile = new(getLobGlobalHeap()) char[MAX_LOB_FILE_NAME_LEN+6];\n  str_sprintf(saveLobDataFile, \"%s_save\",lobDataFile_);\n  char * tmpLobDataFile = new(getLobGlobalHeap()) char[MAX_LOB_FILE_NAME_LEN+5];\n  str_sprintf(tmpLobDataFile, \"%s_tmp\",lobDataFile_);\n\n  str_sprintf(logBuf,\"DataFile %s, TempDataFile : %s, SaveDataFile : %s \",lobDataFile_,tmpLobDataFile, saveLobDataFile);\n\n  lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n  hdfsFS fs = hdfsConnect(hdfsServer_,hdfsPort_);\n  if (fs == NULL)\n    return LOB_DATA_FILE_OPEN_ERROR;\n  \n \n  hdfsFile  fdData = hdfsOpenFile(fs, lobDataFile_, O_RDONLY, 0, 0,0);\n  if (!fdData) \n    {   \n      str_sprintf(logBuf,\"Could not open file:%s\",lobDataFile_);\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n      hdfsCloseFile(fs,fdData);\n      fdData = NULL;\n      return LOB_DATA_FILE_OPEN_ERROR;\n    }\n  \n  hdfsFile fdTemp = hdfsOpenFile(fs, tmpLobDataFile,O_WRONLY|O_CREAT,0,0,0);\n   if (!fdTemp) \n    {\n      str_sprintf(logBuf,\"Could not open file:%s\",tmpLobDataFile);\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n      hdfsCloseFile(fs,fdTemp);\n      fdTemp = NULL;\n      return LOB_DATA_FILE_OPEN_ERROR;\n    }\n\n   Int32 i = 0;\n   Int64 bytesRead = 0;\n   Int64 bytesWritten = 0;\n   Int64 size = 0;\n   Int64 chunkLen = 0;\n   char * tgt = NULL;\n   while (i < numEntries)\n     {\n       chunkLen = dcArray[i].getChunkLen();\n       if (chunkLen > maxMemChunk)\n         {\n           tgt = (char *)(getLobGlobalHeap())->allocateMemory(maxMemChunk);\n           while (chunkLen > maxMemChunk)\n             {             \n               bytesRead = hdfsPread(fs,fdData,dcArray[i].getCurrentOffset(),tgt,maxMemChunk);\n               if (bytesRead != maxMemChunk)\n                 {\n                   lobDebugInfo(\"Problem reading from  data file\",0,__LINE__,lobTrace_);\n                   getLobGlobalHeap()->deallocateMemory(tgt);\n                   return LOB_DATA_READ_ERROR;\n                 }\n               bytesWritten = hdfsWrite(fs,fdTemp, tgt,maxMemChunk);\n               if (bytesWritten != size)\n                 {\n                   lobDebugInfo(\"Problem writing temp data file\",0,__LINE__,lobTrace_);\n                   getLobGlobalHeap()->deallocateMemory(tgt);\n                   return LOB_TARGET_FILE_WRITE_ERROR;\n                 }\n               chunkLen -= maxMemChunk;\n             }\n          \n         }\n       else\n         {\n           tgt = (char *)(getLobGlobalHeap())->allocateMemory(chunkLen);\n            bytesRead = hdfsPread(fs,fdData,dcArray[i].getCurrentOffset(),tgt,chunkLen);\n               if (bytesRead != chunkLen)\n                 {\n                   lobDebugInfo(\"Problem reading from  data file\",0,__LINE__,lobTrace_);\n                   getLobGlobalHeap()->deallocateMemory(tgt);\n                   return LOB_DATA_READ_ERROR;\n                 }\n               bytesWritten = hdfsWrite(fs,fdTemp, tgt,chunkLen);\n               if (bytesWritten != chunkLen)\n                 {\n                   lobDebugInfo(\"Problem writing to temp data file\",0,__LINE__,lobTrace_);\n                   getLobGlobalHeap()->deallocateMemory(tgt);\n                   return LOB_TARGET_FILE_WRITE_ERROR;\n                 }\n         }\n       if (hdfsFlush(fs, fdTemp)) {\n          lobDebugInfo(\"Problem flushing to temp data file\",0,__LINE__,lobTrace_);\n         return LOB_DATA_FLUSH_ERROR;\n       }\n       getLobGlobalHeap()->deallocateMemory(tgt);\n       i++;\n     }\n   hdfsCloseFile(fs,fdTemp);\n   hdfsCloseFile(fs,fdData);\n  \n   //Now save the data file and rename the tempfile to the original datafile\n\n   Int32 rc2 = hdfsRename(fs,lobDataFile_,saveLobDataFile);\n   if (rc2 == -1)\n     {\n       lobDebugInfo(\"Problem renaming datafile to save data file\",0,__LINE__,lobTrace_);\n       NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n       NADELETEBASIC(tmpLobDataFile,getLobGlobalHeap());\n       return LOB_DATA_FILE_WRITE_ERROR;\n     }\n   rc2 = hdfsRename(fs,tmpLobDataFile, lobDataFile_);\n   if (rc2 == -1)\n     {\n       lobDebugInfo(\"Problem renaming temp datafile to data file\",0,__LINE__,lobTrace_);\n       NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n       NADELETEBASIC(tmpLobDataFile,getLobGlobalHeap());\n       return LOB_DATA_FILE_WRITE_ERROR;\n     }\n   NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n   NADELETEBASIC(tmpLobDataFile,getLobGlobalHeap());\n   return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::restoreLobDataFile()\n{\n  Ex_Lob_Error rc = LOB_OPER_OK;\n  lobDebugInfo(\"In ExLob::restoreLobDataFile\",0,__LINE__,lobTrace_);\n  \n  hdfsFS fs = hdfsConnect(hdfsServer_,hdfsPort_);\n  if (fs == NULL)\n    return LOB_DATA_FILE_OPEN_ERROR;\n   char * saveLobDataFile = new(getLobGlobalHeap()) char[MAX_LOB_FILE_NAME_LEN+6];\n   str_sprintf(saveLobDataFile, \"%s_save\",lobDataFile_);\n   Int32 rc2 = hdfsDelete(fs,lobDataFile_,FALSE);//ok to ignore error.\n   rc2 = hdfsRename(fs,saveLobDataFile, lobDataFile_);\n   if (rc2)\n     {\n       lobDebugInfo(\"Problem renaming savedatafile to data file\",0,__LINE__,lobTrace_);\n       NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n       return LOB_OPER_ERROR; \n     }\n   NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n   return rc;\n\n} \n\nEx_Lob_Error ExLob::purgeBackupLobDataFile()\n{\n  Ex_Lob_Error rc = LOB_OPER_OK;\n   lobDebugInfo(\"In ExLob::purgeBackupLobDataFile\",0,__LINE__,lobTrace_);\n  hdfsFS fs = hdfsConnect(hdfsServer_,hdfsPort_);\n  if (fs == NULL)\n    return LOB_DATA_FILE_OPEN_ERROR;\n   char * saveLobDataFile = new(getLobGlobalHeap()) char[MAX_LOB_FILE_NAME_LEN+6];\n   str_sprintf(saveLobDataFile, \"%s_save\",lobDataFile_);\n   Int32 rc2 = hdfsDelete(fs,saveLobDataFile,FALSE);//ok to ignore error.\n   \n   NADELETEBASIC(saveLobDataFile,getLobGlobalHeap());\n   return rc;\n}\n///////////////////////////////////////////////////////////////////////////////\n// ExLobDescHeader definitions\n///////////////////////////////////////////////////////////////////////////////\n\nExLobDescHeader::ExLobDescHeader(unsigned int size) :\n    freeDesc_(0),\n    dataOffset_(0),\n    availSize_(size)\n{\n}\n\nExLobDescHeader::~ExLobDescHeader()\n{\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// ExLobDesc definitions\n///////////////////////////////////////////////////////////////////////////////\n\nExLobDesc::ExLobDesc(int offset, int size, int tail) :\n    dataOffset_(offset),\n    dataSize_(size),\n    dataState_(EX_LOB_DATA_INITIALIZING),\n    tail_(tail),\n    next_(-1),\n    prev_(-1),\n    nextFree_(-1)\n{\n}\n\nExLobDesc::~ExLobDesc()\n{\n}\n\nEx_Lob_Error ExLob::readCursorData(char *tgt, Int64 tgtSize, cursor_t &cursor, Int64 &operLen, char *handleIn, Int32 handleLenIn, Int64 transId)\n{\n   ExLobDesc desc;\n   Ex_Lob_Error err;\n   Int64 bytesAvailable = 0;\n   Int64 bytesToCopy = 0;\n   Int64 bytesRead = 0;\n   operLen = 0;\n   tOffset offset; \n   struct timespec startTime; \n   struct timespec endTime;\n   NABoolean isEOD=FALSE;\n   Int64 outOffset = 0;\n   Int64 outSize = 0;\n   char logBuf[4096];\n   lobDebugInfo(\"In ExLob::readCursorData\",0,__LINE__,lobTrace_);\n\n   while ( (operLen < tgtSize) && !cursor.eod_ )\n   {\n    \n      if (cursor.bytesRead_ == cursor.descSize_) // time to read next chunck\n      {\n        err = fetchCursor(handleIn, handleLenIn,outOffset, outSize,isEOD,transId);\n         if (err != LOB_OPER_OK) {\n            return err;\n         }\n\n         if (isEOD) {\n            cursor.eod_ = true; // subsequent call will return 100 and close the cursor\n            continue;\n         } else {\n            cursor.descSize_ = outSize;\n            cursor.descOffset_ = outOffset;\n            cursor.bytesRead_ = 0;\n         }\n      }\n\n      bytesAvailable = cursor.descSize_ - cursor.bytesRead_;\n      bytesToCopy = min(bytesAvailable, tgtSize - operLen);\n      offset = cursor.descOffset_ + cursor.bytesRead_;\n      // #endif\n\n      if (!fdData_ || (openFlags_ != O_RDONLY)) \n      {\n         hdfsCloseFile(fs_, fdData_);\n\t fdData_=NULL;\n         openFlags_ = O_RDONLY;\n         fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, 0, 0, 0);\n         if (!fdData_) {\n            openFlags_ = -1;\n            return LOB_DATA_FILE_OPEN_ERROR;\n         }\n      }\n\n      clock_gettime(CLOCK_MONOTONIC, &startTime);\n\n      bytesRead = hdfsPread(fs_, fdData_, offset, tgt, bytesToCopy);\n      str_sprintf(logBuf,\"After hdfsPread: BytesToCopy:%Ld, Offset:%Ld, tgt:%Ld, BytesRead :%Ld\",\n                  bytesToCopy,offset,(long)tgt,bytesRead);\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n      clock_gettime(CLOCK_MONOTONIC, &endTime);\n\n      Int64 secs = endTime.tv_sec - startTime.tv_sec;\n      Int64 nsecs = endTime.tv_nsec - startTime.tv_nsec;\n      if (nsecs < 0) {\n        secs--;\n        nsecs += NUM_NSECS_IN_SEC;\n      }\n      Int64 totalnsecs = (secs * NUM_NSECS_IN_SEC) + nsecs;\n      stats_.CumulativeReadTime += totalnsecs;\n\n      if (bytesRead == -1) {\n         return LOB_DATA_READ_ERROR;\n      } else if (bytesRead == 0) {\n         cursor.eod_ = true;         \n         continue;\n      }\n\n\n      cursor.bytesRead_ += bytesRead;\n      operLen += bytesRead;\n      tgt += bytesRead;\n   }\n   hdfsCloseFile(fs_, fdData_);\n   fdData_ = NULL;\n   return LOB_OPER_OK;\n}\n\n\n\nEx_Lob_Error ExLob::readDataToMem(char *memAddr,\n                                  Int64 offset, Int64 size, Int64 &operLen,\n                                  char *handleIn, Int32 handleLenIn, \n                                  NABoolean multipleChunks, Int64 transId)\n\n{ \n  Ex_Lob_Error err = LOB_OPER_OK;\n  operLen = 0;\n  Int64 bytesRead = 0;\n  char logBuf[4096];\n  lobDebugInfo(\"In ExLob::readToMem\",0,__LINE__,lobTrace_);\n  if (multipleChunks) \n    {\n      lobDebugInfo(\"Reading in multiple chunks\",0,__LINE__,lobTrace_);\n      err = openCursor(handleIn, \n\t\t       handleLenIn,transId);\n      //now we can fetch the descriptors for each chunk\n    }\n   \n  if (err != LOB_OPER_OK)\n    return err;\n  \n\n  if (fdData_)// we may have a stale handle. close and open to refresh \n    {\n      hdfsCloseFile(fs_, fdData_);\n      fdData_=NULL;\n      openFlags_ = O_RDONLY;\n      fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, 0, 0, 0);\n      if (!fdData_) {\n\topenFlags_ = -1;\n\treturn LOB_DATA_FILE_OPEN_ERROR;\n      }\n    }\n  else\n    {\n      fdData_ = hdfsOpenFile(fs_, lobDataFile_, openFlags_, 0, 0, 0);\n      if (!fdData_) {\n\topenFlags_ = -1;\n\treturn LOB_DATA_FILE_OPEN_ERROR;\n      }\n    }\n  \n  if (!multipleChunks)\n    {\n      lobDebugInfo(\"Reading in single chunk\",0,__LINE__,lobTrace_);\n      if ((bytesRead = hdfsPread(fs_, fdData_, offset, \n\t\t\t\t memAddr, size)) == -1) {\n\t  \n\treturn LOB_DATA_READ_ERROR;\n      } \n      str_sprintf(logBuf,\"After hdfsPread: File:%s, Offset:%Ld, Size:%Ld,Target Mem Addr:%Ld\",lobDataFile_,offset,size,memAddr);\n      lobDebugInfo(logBuf,0,__LINE__,lobTrace_);\n      operLen = bytesRead;\n      return LOB_OPER_OK;\n    }\n  else\n    {\n      //handle reading the multiple chunks like a cursor\n      err = readCursor(memAddr,size, handleIn,\n\t\t       handleLenIn, operLen, transId);\n\t \t \n      if (err==LOB_OPER_OK)\n\tcloseCursor(handleIn, \n\t\t    handleLenIn);\n      else\n\treturn err;\n    }\n  return LOB_OPER_OK;\n}\n\n \n\nEx_Lob_Error ExLob::readDataToLocalFile(char *fileName,  Int64 offset, Int64 size, Int64 &writeOperLen, Int64 lobMaxChunkMemSize, Int32 fileflags,char *handleIn,Int32 handleInLen, NABoolean multipleChunks,Int64 transId)\n{ \n    Ex_Lob_Error err;\n    Int64 operLen = 0;\n   \n    Int64 srcLen = size;\n    Int64 srcOffset = offset;\n    Int64 tgtOffset = 0;\n    char *lobData = 0;\n    Int64 chunkSize = 0;\n    char logBuf[4096];\n    lobDebugInfo(\"In ExLob::readDataToLocalFile\",0,__LINE__,lobTrace_);\n    if (srcLen <=0)\n       return LOB_SOURCE_DATA_ALLOC_ERROR;\n    // open the target file for writing \n    int filePerms = S_IRUSR | S_IWUSR | S_IRGRP | S_IWGRP | S_IROTH | S_IWOTH;\n    int openFlags = O_RDWR ; // O_DIRECT needs mem alignment\n    if (((LobTgtFileFlags)fileflags == Lob_Append_Or_Error ) ||\n\t((LobTgtFileFlags)fileflags == Lob_Error_Or_Create ) ||\n\t((LobTgtFileFlags)fileflags == Lob_Append_Or_Create))\n      openFlags |= O_APPEND;\n    else\n      openFlags |= O_TRUNC;\n    int fdDestFile = open(fileName, openFlags, filePerms);\n\t  \n    if (fdDestFile >=0 )\n      {\n\tif ((LobTgtFileFlags)fileflags == Lob_Error_Or_Create)\n\t    return LOB_TARGET_FILE_EXISTS_ERROR;\t  \n      }\n    if (fdDestFile == -1) \n      {\n\tif (((LobTgtFileFlags)fileflags == Lob_Append_Or_Error) ||\n\t    ((LobTgtFileFlags)fileflags == Lob_Truncate_Or_Error))\n\t  return LOB_TARGET_FILE_OPEN_ERROR;\t  \n\telse\n\t  {\n\t    openFlags = O_CREAT | O_RDWR ;\n\t    fdDestFile = open(fileName, openFlags, filePerms);\n\t    if (fdDestFile == -1)\t  \n\t      return LOB_TARGET_FILE_OPEN_ERROR; \n\t  }\n      }\n    if ((srcLen < lobMaxChunkMemSize) && (multipleChunks ==FALSE)) // simple single I/O case\n      {\n        lobDebugInfo(\"Reading in single chunk\",0,__LINE__,lobTrace_);\n\tlobData = (char *) (getLobGlobalHeap())->allocateMemory(srcLen);\n\n\tif (lobData == NULL) \n\t  {\n\t    return LOB_SOURCE_DATA_ALLOC_ERROR;\n\t  }\n\terr = readDataToMem(lobData, srcOffset,srcLen,operLen, handleIn,handleInLen, multipleChunks,transId);\n\tif (err != LOB_OPER_OK)\n\t  {\n\t    getLobGlobalHeap()->deallocateMemory(lobData);\n\t    return err;\n\t  }\n       \n\twriteOperLen += pwrite(fdDestFile, lobData, srcLen, tgtOffset) ; \n\tif (writeOperLen <= 0)\n\t  {\n\t    getLobGlobalHeap()->deallocateMemory(lobData);\n\t    return LOB_TARGET_FILE_WRITE_ERROR;\n\t  }\n\tgetLobGlobalHeap()->deallocateMemory(lobData);\n      }\n    else // multiple chunks to read\n      {\n        lobDebugInfo(\"Reading in multiple chunks into local file\",0,__LINE__,lobTrace_);\n\terr = openCursor(handleIn, \n                         handleInLen,transId);\n\tif (err != LOB_OPER_OK)\n\t  return err;\n\twhile ( srcLen > 0)\n\t  {\n\t    chunkSize = MINOF(srcLen, lobMaxChunkMemSize);\n\t    lobData = (char *) (getLobGlobalHeap())->allocateMemory(chunkSize);\n\n\t    if (lobData == NULL) \n\t      {\n\t\tgetLobGlobalHeap()->deallocateMemory(lobData);\n\t\treturn LOB_SOURCE_DATA_ALLOC_ERROR;\n\t      }\n\t    //handle reading the multiple chunks like a cursor\n\t    err = readCursor(lobData,chunkSize, handleIn,\n\t\t\t     handleInLen, operLen, transId);\n\n\t    if ((err != LOB_OPER_OK) || (operLen != chunkSize))\n\t      {\n\t\tgetLobGlobalHeap()->deallocateMemory(lobData);\n\t\treturn err;\n\t      }\n       \n\t    writeOperLen += pwrite(fdDestFile, lobData, chunkSize, tgtOffset) ; \n\t    if (writeOperLen <= 0)\n\t      {\n\t\tgetLobGlobalHeap()->deallocateMemory(lobData);\n\t\treturn LOB_TARGET_FILE_WRITE_ERROR;\n\t      }     \n\t    getLobGlobalHeap()->deallocateMemory(lobData);\n\t    srcLen -= chunkSize;\n\t    tgtOffset += chunkSize;     \n\t  }\n\tcloseCursor(handleIn, \n\t\t    handleInLen);\n      }\n    close(fdDestFile);\n    return LOB_OPER_OK;\n}\n\n\nEx_Lob_Error ExLob::readDataToHdfsFile(char *tgtFileName,  Int64 offset, Int64 size, Int64 &writeOperLen, Int64 lobMaxChunkMemLen, Int32 fileflags,char *handleIn, Int32 handleInLen, NABoolean multipleChunks,Int64 transId)\n{ \n  Ex_Lob_Error err;\n  Int64 operLen = 0;\n \n  Int64 srcLen = size;\n  Int64 srcOffset = offset;\n  Int64 tgtOffset = 0;\n  char *lobData = 0;\n  Int64 chunkSize = 0;\t\n  hdfsFile  fdTgtFile;\n  char logBuf[4096];\n  lobDebugInfo(\"In ExLob::readDataToHdfsFile\",0,__LINE__,lobTrace_);\n  // open and write to the target file\n  int openFlags = O_WRONLY;\n  if ((LobTgtFileFlags)fileflags == Lob_Append_Or_Error )\n    openFlags |= O_APPEND;\n\n  //hdfsFile fdTgtFile = hdfsOpenFile(fs_,tgtFileName, openFlags, 0,0,0);\n  if (hdfsExists(fs_,tgtFileName) == 0)\n    {\n      if ((LobTgtFileFlags)fileflags == Lob_Error_Or_Create)\n\treturn LOB_TARGET_FILE_EXISTS_ERROR;\n      else\n\t{\n\t  \n\t  openFlags =  O_WRONLY ;\n\t  if ((LobTgtFileFlags)fileflags == Lob_Append_Or_Error )\n\t    openFlags |= O_APPEND;\n\t  fdTgtFile = hdfsOpenFile(fs_, tgtFileName, openFlags, 0,0,0);\n\t  if (fdTgtFile == NULL)\n\t    return LOB_TARGET_FILE_OPEN_ERROR;\n\t}\n    }\n  else\n    { \n      if (((LobTgtFileFlags)fileflags == Lob_Append_Or_Error) ||\n\t  ((LobTgtFileFlags)fileflags == Lob_Truncate_Or_Error))\n\treturn LOB_TARGET_FILE_OPEN_ERROR;\t  \n      else\n\t{ \n\t  openFlags =  O_WRONLY ;\n\t  fdTgtFile = hdfsOpenFile(fs_, tgtFileName, openFlags, 0,0,0);\n\t  if (fdTgtFile == NULL)\n\t    return LOB_TARGET_FILE_OPEN_ERROR;\n\t}\n    } \n\n  if ((srcLen < lobMaxChunkMemLen) && (multipleChunks ==FALSE)) // simple single I/O case\n    {\n      lobDebugInfo(\"Reading in single chunk\",0,__LINE__,lobTrace_);\n      lobData = (char *) (getLobGlobalHeap())->allocateMemory(srcLen);\n\n      if (lobData == NULL) \n\t{\n\t  return LOB_SOURCE_DATA_ALLOC_ERROR;\n\t}\n      err = readDataToMem(lobData, srcOffset,srcLen,operLen,handleIn,handleInLen, multipleChunks,transId);\n      if (err != LOB_OPER_OK)\n\t{\n\t  getLobGlobalHeap()->deallocateMemory(lobData);\n\t  return err;\n\t}\n     \n      writeOperLen += hdfsWrite(fs_,fdTgtFile,lobData, srcLen);\n      if (writeOperLen <= 0)\n\t{\n\t  getLobGlobalHeap()->deallocateMemory(lobData);  \n\t  return LOB_TARGET_FILE_WRITE_ERROR;\n\t}\n      if (hdfsFlush(fs_, fdTgtFile)) \n\t{\n\t  getLobGlobalHeap()->deallocateMemory(lobData);\n\t  return LOB_DATA_FLUSH_ERROR;\n\t} \n      getLobGlobalHeap()->deallocateMemory(lobData);\n    }\n  else\n    {// multiple chunks to read\n      lobDebugInfo(\"Reading in multiple chunks into local file\",0,__LINE__,lobTrace_);\n      err = openCursor(handleIn, \n\t\t       handleInLen,\n                       transId);\n      if (err != LOB_OPER_OK)\n\treturn err;\n      while ( srcLen > 0)\n\t{\n\t  chunkSize = MINOF(srcLen, lobMaxChunkMemLen);\n\t  lobData = (char *) (getLobGlobalHeap())->allocateMemory(chunkSize);\t      \n\t  if (lobData == NULL) \n\t    {\n\t      getLobGlobalHeap()->deallocateMemory(lobData);\n\t      return LOB_SOURCE_DATA_ALLOC_ERROR;\n\t    }\n\t  //handle reading the multiple chunks like a cursor\n\t  err = readCursor(lobData,chunkSize, handleIn,\n\t\t\t   handleInLen, operLen, transId);\n\n\t  if ((err != LOB_OPER_OK) || (operLen != chunkSize))\n\t    {\n\t      getLobGlobalHeap()->deallocateMemory(lobData);\n\t      return LOB_DATA_READ_ERROR;\n\t    }\n\t  writeOperLen += hdfsWrite(fs_,fdTgtFile,lobData, chunkSize);\n\t  if (writeOperLen <= 0)\n\t    {\n\t      getLobGlobalHeap()->deallocateMemory(lobData);  \n\t      return LOB_TARGET_FILE_WRITE_ERROR;\n\t    }\n\t  if (hdfsFlush(fs_, fdTgtFile)) \n\t    {\n\t      getLobGlobalHeap()->deallocateMemory(lobData);\n\t      return LOB_DATA_FLUSH_ERROR;\n\t    } \n\t  getLobGlobalHeap()->deallocateMemory(lobData);\n\t  srcLen -= chunkSize;\n\n\t}\n      closeCursor(handleIn, \n\t\t  handleInLen);\t    \n    }\n  hdfsCloseFile(fs_, fdTgtFile);\n  fdTgtFile=NULL;\n  hdfsCloseFile(fs_,fdData_);\n  fdData_=NULL;\n  \n  return LOB_OPER_OK;\n}\n\n\n\n\nEx_Lob_Error ExLob::readDataToExternalFile(char *tgtFileName,  Int64 offset, Int64 size, Int64 &operLen,Int64 lobMaxChunkMemLen,Int32 fileflags,char *handleIn,Int32 handleInLen, NABoolean multipleChunks,Int64 transId)\n{ \n  //TBD\n  return LOB_OPER_OK;\n}\nEx_Lob_Error ExLob::closeFile()\n{\n    if (fdData_) \n    {\n      hdfsCloseFile(fs_, fdData_);\n      fdData_ = NULL;\n    }\n\n    return LOB_OPER_OK;\n}\n\n\n\nEx_Lob_Error ExLob::readStats(char *statsBuffer)\n{\n    memcpy(statsBuffer, (char *)&stats_, sizeof(stats_));\n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::initStats()\n{\n    stats_.init();\n    return LOB_OPER_OK;\n}\n//Main driver of any LOB related operation \n\nEx_Lob_Error ExLobsOper (\n\t\t\t char        *lobName,          // lob name\n\t\t\t char        *handleIn,         // input handle (for cli calls)\n\t\t\t Int32       handleInLen,       // input handle len\n\t\t\t char        *hdfsServer,       // server where hdfs fs resides\n\t\t\t Int64       hdfsPort,          // port number to access hdfs server\n\t\t\t char        *handleOut,        // output handle (for cli calls)\n\t\t\t Int32       &handleOutLen,     // output handle len\n\t\t\t Int64       descNumIn,         // input desc Num (for flat files only)\n\t\t\t Int64       &descNumOut,       // output desc Num (for flat files only)\n\t\t\t Int64       &retOperLen,       // length of data involved in this operation\n\t\t\t Int64       requestTagIn,      // only for checking status\n\t\t\t Int64       &requestTagOut,    // returned with every request other than check status\n\t\t\t Ex_Lob_Error  &requestStatus,  // returned req status\n\t\t\t Int64       &cliError,         // err returned by cli call\n\t\t\t char        *lobStorageLocation,              // directory in the storage\n\t\t\t LobsStorage storage,           // storage type\n\t\t\t char        *source,           // source (memory addr, filename, foreign lob etc)\n\t\t\t Int64       sourceLen,         // source len (memory len, foreign desc offset etc)\n\t\t\t Int64       cursorBytes,\n\t\t\t char        *cursorId,\n\t\t\t LobsOper    operation,         // LOB operation\n\t\t\t LobsSubOper subOperation,      // LOB sub operation\n\t\t\t Int64       waited,            // waited or nowaited\n\t\t\t void        *&globPtr,         // ptr to the Lob objects. \n\t\t\t Int64       transId,\n\t\t\t void        *blackBox,         // black box to be sent to cli\n\t\t\t Int32       blackBoxLen,       // length of black box\n\t\t\t Int64       lobMaxSize,\n\t\t\t Int64       lobMaxChunkMemSize,\n                         Int64       lobGCLimit,\n\t\t\t int         bufferSize ,\n\t\t\t short       replication ,\n\t\t\t int         blockSize,\n\t\t\t Lng32       openType)\n{ \n  Ex_Lob_Error err = LOB_OPER_OK;\n  ExLob *lobPtr = NULL;\n  char fn[MAX_LOB_FILE_NAME_LEN];\n  struct timespec startTime;\n  struct timespec endTime;\n  Int64 secs, nsecs, totalnsecs;\n  ExLobPreOpen *preOpenObj;\n  ExLobGlobals *lobGlobals = NULL;\n  transId = 0;\n  retOperLen = 0;\n  ExLobDesc desc;\n    \n  lobMap_t *lobMap = NULL;\n  lobMap_it it;\n\n  clock_gettime(CLOCK_MONOTONIC, &startTime);\n\n  char *fileName = lobName;\n\n  if (globPtr == NULL)\n    {\n      if ((operation == Lob_Init))\n\t{\n          \n          globPtr = new ExLobGlobals();\n\t  if (globPtr == NULL) \n\t    return LOB_INIT_ERROR;\n\n\t  lobGlobals = (ExLobGlobals *)globPtr;\n\n\t  err = lobGlobals->initialize(); \n          if (err != LOB_OPER_OK)\n            return err;\n\t}\n      else\n\t{\n\t  return LOB_GLOB_PTR_ERROR;\n\t}\n    }\n\n  if ((globPtr != NULL) && (operation != Lob_Init))\n    {\n      lobGlobals = (ExLobGlobals *)globPtr;\n\n      lobMap = lobGlobals->getLobMap();\n\n      it = lobMap->find(string(fileName));\n\n      if (it == lobMap->end())\n\t{\n\t  //lobPtr = new (lobGlobals->getHeap())ExLob();\n\t  lobPtr = new ExLob();\n\t  if (lobPtr == NULL) \n\t    return LOB_ALLOC_ERROR;\n\n\t  err = lobPtr->initialize(fileName, (operation == Lob_Create) ? EX_LOB_CREATE : EX_LOB_RW, lobStorageLocation, storage, hdfsServer, hdfsPort, lobStorageLocation,bufferSize, replication, blockSize,lobMaxSize,lobGlobals);\n\t  if (err != LOB_OPER_OK)\n            {\n              char buf[5000];\n              str_sprintf(buf,\"Lob initialization failed;filename:%s;location:%s;hdfsserver:%s;hdfsPort:%d;lobMaxSize:%Ld\",fileName,lobStorageLocation,hdfsServer,lobMaxSize);\n              lobDebugInfo(buf,err,__LINE__,lobGlobals->lobTrace_);\n              return err;\n            }\n\n\t  lobMap->insert(pair<string, ExLob*>(string(fileName), lobPtr));\n\t}\n      else\n\t{\n\t  lobPtr = it->second;\n        \n\t}\n      lobPtr->lobTrace_ = lobGlobals->lobTrace_;\n    }\n  /* \n// **Note** This is code that needs to get called before sneding a request to the \n//mxlobsrvr process. It's inactive code currently   \n  MS_Mon_Transid_Type transIdBig;\n  MS_Mon_Transseq_Type transStartId;\n  if (!lobGlobals->isHive())\n    {\n      // get current transaction\n   \n      int transIdErr = ms_transid_get(false, false, &transIdBig, &transStartId);\n      // set the pass thru request object values in the lob\n    \n      lobPtr->getRequest()->setValues(lobPtr->getDescFileName(),\n                                     descNumIn, handleInLen, handleIn, storage,\n                                     transId, transIdBig, transStartId,\n                                     (char *)blackBox, blackBoxLen);\n    }\n  */\n  switch(operation)\n    {\n    case Lob_Init:\n    case Lob_Create:\n      break;\n\n    case Lob_InsertDesc:\n      err = lobPtr->writeDesc(sourceLen, source, subOperation, descNumOut, retOperLen, lobMaxSize, lobMaxChunkMemSize,lobGCLimit,handleIn,handleInLen,(char *)blackBox, &blackBoxLen,handleOut,handleOutLen,lobGlobals);\n      if (err != LOB_OPER_OK)\n        {\n          lobDebugInfo(\"writeDesc failed \",err,__LINE__,lobGlobals->lobTrace_);\n        }\n      break;\n\n    case Lob_InsertData:\n      err = lobPtr->insertData(source, sourceLen, subOperation, descNumIn, retOperLen, lobMaxSize,lobMaxChunkMemSize,handleIn,handleInLen,(char *)blackBox, blackBoxLen,handleOut,handleOutLen,lobGlobals);\n      if (err != LOB_OPER_OK)\n        {\n          lobDebugInfo(\"insertData failed \",err,__LINE__,lobGlobals->lobTrace_);\n        }\n      break;\n\n    case Lob_InsertDataSimple:\n      err = lobPtr->writeDataSimple(source, sourceLen, subOperation, retOperLen,\n\t\t\t\t    bufferSize , replication , blockSize);\n      break;\n\n    case Lob_Read:\n      if (storage == Lob_External_HDFS_File)   \n        //Allocate storage to read the lob external file name from the \n        //descriptor tables  to get the data from.        \n        blackBox = new(lobGlobals->getHeap()) char[MAX_LOB_FILE_NAME_LEN+6];\n          \n      if (subOperation == Lob_Memory)\n        {\n          err = lobPtr->readToMem(source,sourceLen,retOperLen,handleIn,handleInLen,(char *)blackBox, blackBoxLen,handleOut,handleOutLen,transId);\n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"readToMem failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else if (subOperation == Lob_File)\n        {\n          err = lobPtr->readToFile(source, sourceLen, retOperLen, lobMaxChunkMemSize,  openType,handleIn,handleInLen,(char *)blackBox, blackBoxLen,handleOut,handleOutLen,transId);\n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"readToFile failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else  \n\terr = LOB_SUBOPER_ERROR;\n      if (blackBox)\n        (lobGlobals->getHeap())->deallocateMemory((char*) blackBox);\n      break;\n\n    case Lob_ReadDesc: // read desc only. Needed for pass thru.\n      err = lobPtr->getDesc(desc,handleIn,handleInLen,(char *)blackBox, &blackBoxLen,handleOut,handleOutLen,transId); \n      retOperLen = 0;\n      break;\n    case Lob_OpenCursor:\n      err = lobPtr->openCursor(handleIn, handleInLen,transId);\n      break;\n\n    case Lob_OpenDataCursorSimple:  \n      if (openType == 1) { // preopen\n\tsprintf(fn,\"%s:%Lx:%s\",lobPtr->getDataFileName(), (long long unsigned int)lobName, cursorId);\n\tpreOpenObj = new (lobGlobals->getHeap()) ExLobPreOpen(lobPtr, fn, descNumIn, sourceLen, cursorBytes, waited);\n\tlobGlobals->addToPreOpenList(preOpenObj);\n      } else if (openType == 2) { // must open\n\tsprintf(fn,\"%s:%Lx:%s\",lobPtr->getDataFileName(), (long long unsigned int)lobName, cursorId);\n\tfileName = fn;\n\terr = lobPtr->openDataCursor(fileName, Lob_Cursor_Simple, descNumIn, sourceLen, cursorBytes, waited, lobGlobals);\n      } else\n\terr = LOB_SUBOPER_ERROR;\n      break;\n\n    case Lob_ReadCursor:\n      if ((subOperation == Lob_Memory) || (subOperation == Lob_Buffer))\n\terr = lobPtr->readCursor(source, sourceLen, handleIn, handleInLen, retOperLen,transId);\n      else if (subOperation == Lob_File)\n\terr = lobPtr->readCursor(source, -1, handleIn, handleInLen, retOperLen,transId);\n      else  \n\terr = LOB_SUBOPER_ERROR;\n      break;\n\n    case Lob_ReadDataCursorSimple:\n      sprintf(fn,\"%s:%Lx:%s\",lobPtr->getDataFileName(), (long long unsigned int)lobName, cursorId);\n      fileName = fn;       \n      err = lobPtr->readDataCursorSimple(fileName, source, sourceLen, retOperLen, lobGlobals);\n      break;\n\n    case Lob_CloseFile:\n      if (lobPtr->hasNoOpenCursors()) {\n\tlobGlobals->traceMessage(\"Lob_CloseFile\",NULL,__LINE__);\n\terr = lobPtr->closeFile();\n\tit = lobMap->find(string(lobName));\n\tlobMap->erase(it);\n\tdelete lobPtr;\n\tlobPtr = NULL;\n      }  \n      break;\n\n    case Lob_CloseCursor:\n      err = lobPtr->closeCursor(handleIn, handleInLen);\n      break;\n\n    case Lob_CloseDataCursorSimple:\n      sprintf(fn,\"%s:%Lx:%s\",lobPtr->getDataFileName(), (long long unsigned int)lobName, cursorId);\n      fileName = fn;\n      err = lobPtr->closeDataCursorSimple(fileName, lobGlobals);\n      break;\n\n    case Lob_Append:\n      if ((subOperation == Lob_Memory) ||(subOperation == Lob_Buffer) || (subOperation ==Lob_External))\n        {\n          err = lobPtr->append(source, sourceLen, subOperation, descNumIn, retOperLen,lobMaxSize, lobMaxChunkMemSize,lobGCLimit,handleIn,handleInLen,handleOut,handleOutLen,lobGlobals);\n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"append(Memory,Buffer) failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else if (subOperation == Lob_File)\n        {\n          err = lobPtr->append(source, -1, subOperation, descNumIn, retOperLen,lobMaxSize, lobMaxChunkMemSize,lobGCLimit,handleIn,handleInLen,handleOut,handleOutLen,lobGlobals);\n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"append(File) failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else  \n\terr = LOB_SUBOPER_ERROR;\n      break;\n\n    case Lob_Update:\n      if ((subOperation == Lob_Memory)||(subOperation == Lob_Buffer)||(subOperation ==Lob_External))\n        {\n          err = lobPtr->update(source, sourceLen, subOperation, descNumIn, retOperLen, lobMaxSize, lobMaxChunkMemSize,lobGCLimit,handleIn,handleInLen,handleOut,handleOutLen,lobGlobals);\n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"update(Memory,Buffer) failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else if (subOperation == Lob_File)\n        {\n          err = lobPtr->update(source, -1, subOperation,descNumIn, retOperLen,lobMaxSize, lobMaxChunkMemSize,lobGCLimit,handleIn,handleInLen,handleOut,handleOutLen,lobGlobals); \n          if (err != LOB_OPER_OK)\n            {\n              lobDebugInfo(\"update(Memory,Buffer) failed \",err,__LINE__,lobGlobals->lobTrace_);\n            }\n        }\n      else\n\terr = LOB_SUBOPER_ERROR;\n      break;\n\n    case Lob_Delete:\n      err = lobPtr->delDesc(handleIn, handleInLen,transId);\n      break;\n\n    case Lob_Drop:\n      err = lobPtr->purgeLob();\n      it = lobMap->find(string(lobName));\n      lobMap->erase(it);\n      delete lobPtr;\n      lobPtr = NULL;\n      if (err != LOB_OPER_OK)           \n        lobDebugInfo(\"purgeLob failed \",err,__LINE__,lobGlobals->lobTrace_);\n      break;\n\n    case Lob_Purge:\n      err = lobPtr->purgeLob();\n      it = lobMap->find(string(lobName));\n      lobMap->erase(it);\n      delete lobPtr;\n      lobPtr = NULL;\n      if (err != LOB_OPER_OK)           \n        lobDebugInfo(\"purgeLob failed \",err,__LINE__,lobGlobals->lobTrace_);\n      break;\n\n    case Lob_Stats:\n      err = lobPtr->readStats(source);\n      lobPtr->initStats(); // because file may remain open across cursors\n      break;\n\n    case Lob_Empty_Directory:    \n      err = lobPtr->emptyDirectory(lobStorageLocation, lobGlobals);\n\n      break;\n\n    case Lob_Data_Mod_Check:\n      {       \n        Int64 inputModTS = *(Int64*)blackBox;\n        Int32 inputNumOfPartLevels = \n          *(Lng32*)&((char*)blackBox)[sizeof(inputModTS)];\n        err = lobPtr->dataModCheck(lobStorageLocation, inputModTS, inputNumOfPartLevels,\n                                   lobGlobals);\n      }\n      break;\n\n    case Lob_Cleanup:\n        delete lobGlobals;\n        break;\n     \n    case Lob_PerformGC:\n      err = lobPtr->compactLobDataFile((ExLobInMemoryDescChunksEntry *)source,sourceLen);\n      if (err != LOB_OPER_OK)           \n        lobDebugInfo(\"compactLobDataFile failed \",err,__LINE__,lobGlobals->lobTrace_);\n      break;\n    case Lob_RestoreLobDataFile:\n      err = lobPtr->restoreLobDataFile();\n      if (err != LOB_OPER_OK)           \n        lobDebugInfo(\"restoreLobDataFile failed \",err,__LINE__,lobGlobals->lobTrace_);\n      break;\n    case Lob_PurgeBackupLobDataFile:\n      err = lobPtr->purgeBackupLobDataFile();\n      if (err != LOB_OPER_OK)           \n        lobDebugInfo(\"purgeBackupLobDataFile failed \",err,__LINE__,lobGlobals->lobTrace_);\n      break;\n    default:\n      err = LOB_OPER_ERROR;\n      break;\n    }\n  /*\n//**Note ** This code is needed to reinstate the master transaction after \n// returning from the mxlobsrvr process. This is inactive code for now\nif (!lobGlobals->isHive() )\n    {\n      if (lobPtr)\n       // set the pass thru request object values from the lob\n       lobPtr->getRequest()->getValues(descNumOut, handleOutLen, handleOut, \n                                       requestStatus, cliError,\n                                       (char *)blackBox, blackBoxLen);    // reinstate the transaction\n      if (TRANSID_IS_VALID(transIdBig)) {\n       ms_transid_reinstate(transIdBig, transStartId);\n      }\n    }\n\n  */\n  clock_gettime(CLOCK_MONOTONIC, &endTime);\n\n  secs = endTime.tv_sec - startTime.tv_sec;\n  nsecs = endTime.tv_nsec - startTime.tv_nsec;\n  if (nsecs < 0) {\n    secs--;\n    nsecs += NUM_NSECS_IN_SEC;\n  }\n  totalnsecs = (secs * NUM_NSECS_IN_SEC) + nsecs;\n  if (lobPtr && lobPtr->getStats())\n    lobPtr->getStats()->hdfsAccessLayerTime += totalnsecs; \n       \n  return err;\n}\n\nvoid cleanupLOBDataDescFiles(const char *lobHdfsServer,int lobHdfsPort,const char *lobHdfsLoc)\n{ \n  int numExistingFiles=0;\n  hdfsFS fs;\n  int err = 0;\n  fs = hdfsConnect(lobHdfsServer, lobHdfsPort);\n  if (fs == NULL)\n    return;\n  // Get this list of all data and desc files in the lob sotrage location\n  hdfsFileInfo *fileInfos = hdfsListDirectory(fs, lobHdfsLoc, &numExistingFiles);\n  if (fileInfos == NULL)\n      return ;\n    \n  //Delete each one in a loop\n  for (int i = 0; i < numExistingFiles; i++)  \n    {    \n      err = hdfsDelete(fs, fileInfos[i].mName, 0);\n    }\n    \n  // *Note* : delete the memory allocated by libhdfs for the file info array  \n  if (fileInfos)\n    {\n      hdfsFreeFileInfo(fileInfos, numExistingFiles);\n    }\n}\n\n\n// The following methods are used for hive access\n/* \nMain thread issues an open to open a range of 128 MB and wakes up a \nworker thread. It doesn\u2019t wait.It calls pre open on the next range. This is \ndone in method ::readDataCursorSimple.\n\nThe worker threads do their work in ::doWorkInThread and ::performRequests, ::readCursorDataSimple.(note the  diff from the method the mainthread calls above) \n\nMain thread then issues a read. Since worker thread had already begun fetching \n16KB buffers in (1), the main thread most likely will not need to wait and the \ndata will be ready. It keeps consuming the buffers, recycling them back into \npostFetchBufList. \nWhen done, the main thread closes the cursor(::closeDataCursorSimple). This is determined by whether we \nhave reached the end of range or the end of data for that file.\nThe worker threads on the other hand  read 16KB of data and buffers them in a \nprefetchBufList. It continues doing this until end of range is reached or the \nbuffer limit (128MB) has been reached. \n*/\n\nEx_Lob_Error ExLob::readDataCursorSimple(char *file, char *tgt, Int64 tgtSize, \n                                         Int64 &operLen, ExLobGlobals *lobGlobals)\n{\n    int dataOffset;\n    Ex_Lob_Error result = LOB_OPER_OK;\n    cursor_t *cursor;\n    ExLobCursor::bufferList_t::iterator c_it;\n    ExLobCursorBuffer *buf = NULL;\n    Int64 bytesToCopy = 0;\n    operLen = 0;\n    Int64 len;\n    char *target = tgt;\n    bool done = false;\n\n    struct timespec startTime;\n    struct timespec endTime;\n\n    lobCursorLock_.lock();\n\n    lobCursors_it it = lobCursors_.find(string(file, strlen(file)));\n\n    if (it == lobCursors_.end())\n    {\n       lobCursorLock_.unlock();\n       return LOB_CURSOR_NOT_OPEN;\n    }\n    else\n    {\n       cursor = &(it->second); \n    } \n\n    lobCursorLock_.unlock();\n\n    while ((operLen < tgtSize) && !done && !cursor->eol_)\n    {\n      lobGlobals->traceMessage(\"locking cursor\",cursor,__LINE__);\n      cursor->lock_.lock();\n\n      // if no buffers to read and is eor or eod, we are done.\n      // else wait for prefetch thread to wake us up. \n      if (cursor->prefetchBufList_.size() == 0) {\n        if (cursor->eor_ || cursor->eod_) {\n          done = true;\n        } else {\n          cursor->bufferMisses_++;\n\t  lobGlobals->traceMessage(\"wait on condition cursor\",cursor,__LINE__);\n          cursor->lock_.wait();\n        }\n\tlobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n        cursor->lock_.unlock();\n        continue;\n      } \n\n      // a buffer is available\n      c_it = cursor->prefetchBufList_.begin();\n      buf = *c_it;\n      lobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n      cursor->lock_.unlock();\n\n      bytesToCopy = min(buf->bytesRemaining_, tgtSize - operLen);\n      memcpy(target, buf->data_ + buf->bytesUsed_, bytesToCopy);\n      target += bytesToCopy;\n      if (bytesToCopy == buf->bytesRemaining_) { // buffer is now empty\n        buf->bytesRemaining_ = -1;\n        buf->bytesUsed_ = -1;\n        lobGlobals->postfetchBufListLock_.lock();\n        lobGlobals->postfetchBufList_.push_back(buf);\n        lobGlobals->postfetchBufListLock_.unlock();\n\tlobGlobals->traceMessage(\"locking cursor\",cursor,__LINE__);\n        cursor->lock_.lock();\n        c_it = cursor->prefetchBufList_.erase(c_it);\n\tlobGlobals->traceMessage(\"signal condition cursor\",cursor,__LINE__);\n        cursor->lock_.wakeOne(); // wake up prefetch thread if it was waiting for an empty buffer.\n\tlobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n        cursor->lock_.unlock();\n      } else {\n        buf->bytesUsed_ += bytesToCopy;\n        buf->bytesRemaining_ -= bytesToCopy;\n      }\n      stats_.bytesPrefetched += bytesToCopy;\n      operLen += bytesToCopy;\n    } \n\n    // update stats\n    stats_.bytesRead += operLen;\n    stats_.bytesToRead += tgtSize;\n    stats_.numReadReqs++;\n\n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLob::closeDataCursorSimple(char *fileName, ExLobGlobals *lobGlobals)\n{\n    cursor_t *cursor = NULL;\n    Int64 secs = 0;\n    Int64 nsecs = 0;\n\n    lobCursorLock_.lock();\n\n    lobCursors_it it = lobCursors_.find(string(fileName, strlen(fileName)));\n    if (it != lobCursors_.end())\n    {\n      cursor = &(it->second);\n      lobGlobals->traceMessage(\"locking cursor\",cursor,__LINE__);\n      cursor->lock_.lock();\n\n      clock_gettime(CLOCK_MONOTONIC, &cursor->closeTime_);\n      secs = cursor->closeTime_.tv_sec - cursor->openTime_.tv_sec;\n      nsecs = cursor->closeTime_.tv_nsec - cursor->openTime_.tv_nsec;\n\n      if (cursor->eod_ || cursor->eor_) { // prefetch thread already done,\n        cursor->emptyPrefetchList(lobGlobals);\n\tlobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n\tcursor->lock_.unlock();\n        lobCursors_.erase(it);            // so erase it here. \n        // no need to unlock as cursor object is gone.\n      } else {\n        cursor->eol_ = true;     // prefetch thread will do the eol rituals\n\tlobGlobals->traceMessage(\"signal condition cursor\",cursor,__LINE__);\n        cursor->lock_.wakeOne(); // wakeup prefetch thread\n\tlobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n        cursor->lock_.unlock();\n      }\n    }\n\n    lobCursorLock_.unlock();\n\n    if (nsecs < 0) {\n      secs--;\n      nsecs += NUM_NSECS_IN_SEC;\n    }\n    Int64 totalnsecs = (secs * NUM_NSECS_IN_SEC) + nsecs;\n    stats_.cursorElapsedTime += totalnsecs;\n\n    return LOB_OPER_OK;\n}\n\n\nEx_Lob_Error ExLobGlobals::performRequest(ExLobHdfsRequest *request)\n{\n  Ex_Lob_Error err = LOB_OPER_OK;\n  ExLob *lobPtr;\n  ExLobCursorBuffer *buf;\n  ExLobCursor *cursor;\n  Int64 size;\n  NABoolean seenEOR = false;\n  NABoolean seenEOD = false;\n  ExLobCursor::bufferList_t::iterator c_it;\n  Int64 totalBufSize;\n\n  switch (request->reqType_) \n  {\n    case Lob_Hdfs_Cursor_Prefetch :\n      lobPtr = request->lobPtr_;\n      cursor = request->cursor_;\n      traceMessage(\"locking cursor\",cursor,__LINE__);\n      cursor->lock_.lock();\n      while (!cursor->eod_ && !cursor->eor_ && !cursor->eol_) \n      {\n        postfetchBufListLock_.lock();\n        c_it = postfetchBufList_.begin();\n        if (c_it != postfetchBufList_.end()) {\n          buf = *c_it;\n          postfetchBufList_.erase(c_it);\n          postfetchBufListLock_.unlock();\n\t  traceMessage(\"unlocking cursor\",cursor,__LINE__);\n          cursor->lock_.unlock();\n        } else { \n          postfetchBufListLock_.unlock();\n          // there are no empty buffers. \n          // if prefetch list already has the max, wait for one to free up.\n          totalBufSize =  cursor->prefetchBufList_.size() * cursor->bufMaxSize_;\n          if (totalBufSize > LOB_CURSOR_PREFETCH_BYTES_MAX) {\n\t    traceMessage(\"wait on condition cursor\",cursor,__LINE__);\n            cursor->lock_.wait();\n            char buffer2[2048];\n            sprintf(buffer2, \"cursor->eod_ %d cursor->eor_ %d \"\n                             \"cursor->eol_ %d\", cursor->eod_,\n                              cursor->eor_, cursor->eol_);\n            traceMessage(buffer2, cursor, __LINE__);\n            continue;\n          }\n          // create a new buffer\n\t  traceMessage(\"unlocking cursor\",cursor,__LINE__);\n          cursor->lock_.unlock();\n          buf = new (getHeap()) ExLobCursorBuffer();\n          buf->data_ = (char *) (getHeap())->allocateMemory( cursor->bufMaxSize_);\n          lobPtr->stats_.buffersUsed++;\n        }\n        size = min(cursor->bufMaxSize_, (cursor->maxBytes_ - cursor->bytesRead_ + (16 * 1024)));\n        if (buf->data_) {\n          lobPtr->readCursorDataSimple(buf->data_, size, *cursor, buf->bytesRemaining_);\n          buf->bytesUsed_ = 0;\n\t  traceMessage(\"locking cursor\",cursor,__LINE__);\n          cursor->lock_.lock();\n          if (size < (cursor->bufMaxSize_)) {\n            cursor->eor_ = true;\n\t    seenEOR = true;\n          }\n          if (buf->bytesRemaining_) {\n            cursor->prefetchBufList_.push_back(buf);\n\t    traceMessage(\"signal condition cursor\",cursor,__LINE__);\n            cursor->lock_.wakeOne();\n\t    traceMessage(\"unlocking cursor\",cursor,__LINE__);\n            cursor->lock_.unlock();\n          } else {\n            cursor->eod_ = true;\n            seenEOD = true;\n\t    traceMessage(\"signal condition cursor\",cursor,__LINE__);\n            cursor->lock_.wakeOne();\n\t    traceMessage(\"unlocking cursor\",cursor,__LINE__);\n            cursor->lock_.unlock();\n            postfetchBufListLock_.lock();\n            postfetchBufList_.push_back(buf);\n            postfetchBufListLock_.unlock();\n          }\n        } else {\n          assert(\"data_ is null\"); \n        }\n\t// Important! Break and do not access cursor object if we have reached\n\t// end of data or range.\n\t// The main thread could have destroyed the cursor \n\t// in ::closeDataCursorSimple\n\tif (seenEOD || seenEOR)\n        {\n          char buffer2[2048];\n          sprintf(buffer2, \"seenEOD %d seenEOR %d\",\n                               seenEOD, seenEOR);\n          traceMessage(buffer2, cursor, __LINE__);\n          break;\n        }\n\ttraceMessage(\"locking cursor\",cursor,__LINE__);\n\tcursor->lock_.lock();\n      } // while\n\n      if (!seenEOD && !seenEOR)\n\t{\n          traceMessage(\"locking cursor\",cursor,__LINE__);\n\t  cursor->lock_.unlock();\n\t  if (cursor->eol_) { // never reaches here ??  \n\t    lobPtr->deleteCursor(cursor->name_, this);\n\t  }\n\t}\n      processPreOpens();\n      break;\n\n    default:\n      request->error_ = LOB_HDFS_REQUEST_UNKNOWN;\n  }\n\n  return LOB_OPER_OK;\n}\n\n\n\nEx_Lob_Error ExLob::readCursorDataSimple(char *tgt, Int64 tgtSize, cursor_t &cursor, Int64 &operLen)\n{\n   ExLobDesc desc;\n   Ex_Lob_Error err;\n   Int64 bytesAvailable = 0;\n   Int64 bytesToCopy = 0;\n   Int64 bytesRead = 0;\n   operLen = 0;\n   tOffset offset; \n   struct timespec startTime; \n   struct timespec endTime;\n   bool done = false;\n\n   if (!fdData_) {\n      return LOB_CURSOR_NOT_OPEN_ERROR;\n   }\n\n   if (cursor.bytesRead_ == -1) {  // starting\n      cursor.bytesRead_ = 0;\n   }\n\n   clock_gettime(CLOCK_MONOTONIC, &startTime);\n   \n   while ( (operLen < tgtSize) && !done )\n   {\n      //offset = cursor.descOffset_ + cursor.bytesRead_;\n      bytesToCopy = tgtSize - operLen;\n      offset = cursor.descOffset_ + cursor.bytesRead_;\n\n      // gets chunks of 64KB. Uses readDirect internally.\n      // bytesRead = hdfsPread(fs_, fdData_, offset, tgt, bytesToCopy);\n      bytesRead = hdfsRead(fs_, fdData_, tgt, bytesToCopy);\n\n      stats_.numHdfsReqs++;\n\n      if (bytesRead == -1) {\n         return LOB_DATA_READ_ERROR;\n      } else if (bytesRead == 0) {\n         done = true; \n      }\n\n      cursor.bytesRead_ += bytesRead;\n      operLen += bytesRead;\n      tgt += bytesRead;\n   }\n\n   clock_gettime(CLOCK_MONOTONIC, &endTime);\n\n   Int64 secs = endTime.tv_sec - startTime.tv_sec;\n   Int64 nsecs = endTime.tv_nsec - startTime.tv_nsec;\n   if (nsecs < 0) {\n    secs--; \n    nsecs += NUM_NSECS_IN_SEC;\n   }\n\n   Int64 totalnsecs = (secs * NUM_NSECS_IN_SEC) + nsecs;\n   stats_.CumulativeReadTime += totalnsecs;\n\n   return LOB_OPER_OK;\n}\nvoid ExLobCursor::emptyPrefetchList(ExLobGlobals *lobGlobals)\n{\n    ExLobCursor::bufferList_t::iterator c_it;\n    ExLobCursorBuffer *buf = NULL;\n\n    c_it = prefetchBufList_.begin();\n    while (c_it != prefetchBufList_.end())\n    {\n      buf = *c_it;\n      lobGlobals->postfetchBufListLock_.lock();\n      lobGlobals->postfetchBufList_.push_back(buf);\n      lobGlobals->postfetchBufListLock_.unlock();\n      c_it = prefetchBufList_.erase(c_it);\n    }\n}\n\n// Seems like this is currently unused. \n// closeDataCusrorSimple takes care of destroying the cursor.But addign code\n// similar to closeDataCursorSimple for correctness in case it is used in future\nEx_Lob_Error ExLob::deleteCursor(char *cursorName, ExLobGlobals *lobGlobals)\n{\n    cursor_t *cursor = NULL;\n\n    lobCursorLock_.lock();\n\n    lobCursors_it it = lobCursors_.find(string(cursorName, strlen(cursorName)));\n    if (it != lobCursors_.end())\n    {\n      cursor = &(it->second);\n      lobGlobals->traceMessage(\"locking cursor\",cursor,__LINE__);\n      cursor->lock_.lock();\n      cursor->emptyPrefetchList(lobGlobals);\n      lobGlobals->traceMessage(\"unlocking cursor\",cursor,__LINE__);\n      cursor->lock_.unlock();\n      lobCursors_.erase(it);\n    }\n\n    lobCursorLock_.unlock();\n\n    return LOB_OPER_OK;\n}\n\n//*** Note - sample code to send and receive  \nEx_Lob_Error ExLob::sendReqToLobServer() \n{\n\n    Ex_Lob_Error err; \n#ifdef __ignore\n    request_.setType(Lob_Req_Get_Desc);\n\n    err = request_.send();\n\n    if (err != LOB_OPER_OK) {\n       return err;\n    }\n\n    err = request_.getError();\n#endif\n    return err;\n}\n\n///////////////////////////////////////////////////////////////////////////////\n// ExLobGlobals definitions\n///////////////////////////////////////////////////////////////////////////////\n\nExLobGlobals::ExLobGlobals() :\n    lobMap_(NULL), \n    fs_(NULL),\n    isCliInitialized_(FALSE),\n    isHive_(FALSE),\n    threadTraceFile_(NULL),\n    lobTrace_(FALSE),\n    heap_(NULL)\n{\n  //initialize the log file\n  if (getenv(\"TRACE_HDFS_THREAD_ACTIONS\"))\n    {\n      char logFileName[50]= \"\";\n      sprintf(logFileName,\"trace_threads.%d\",getpid());\n      threadTraceFile_ = fopen(logFileName,\"a\");\n    }\n  if(getenv(\"TRACE_LOB_ACTIONS\"))\n    lobTrace_ = TRUE;\n}\n\nExLobGlobals::~ExLobGlobals()\n{\n    ExLobCursor::bufferList_t::iterator c_it;\n    ExLobCursorBuffer *buf = NULL;\n\n    preOpenListLock_.lock();\n    preOpenList_.clear();\n    preOpenListLock_.unlock();\n\n    \n    if (lobMap_) \n      delete lobMap_;\n\n    for (int i=0; i<NUM_WORKER_THREADS; i++) {\n      enqueueShutdownRequest();\n    }\n\n    for (int i=0; i<NUM_WORKER_THREADS; i++) {\n      pthread_join(threadId_[i], NULL);\n    }\n    // Free the post fetch bugf list AFTER the worker threads have left to \n    // avoid slow worker thread being stuck and master deallocating these \n    // buffers and not consuming the buffers which could cause a  lock.\n \n    postfetchBufListLock_.lock();\n    c_it = postfetchBufList_.begin();\n    while (c_it != postfetchBufList_.end()) {\n      buf = *c_it;\n      if (buf->data_) {\n        heap_->deallocateMemory( buf->data_);\n      }\n      c_it = postfetchBufList_.erase(c_it);\n    }\n    postfetchBufListLock_.unlock();\n    \n    //msg_mon_close_process(&serverPhandle);\n    if (threadTraceFile_)\n      fclose(threadTraceFile_);\n    threadTraceFile_ = NULL;\n\n   \n}\n\n\n\n// called once per process\nEx_Lob_Error ExLobGlobals::initialize()\n{\n    Ex_Lob_Error err = LOB_OPER_OK;\n\n    lobMap_ = (lobMap_t *) new (getHeap())lobMap_t;  \n    if (lobMap_ == NULL)\n      return LOB_INIT_ERROR;\n    // No need to start them here for LOB usage.These worker threads are needed \n    // only for hive access so moving them to the ExpLOBInterfaceInit function \n    // where they will get started only in case of hive access.\n    // start the worker threads\n    //startWorkerThreads();\n\n    return err;\n}\n\nstatic void *workerThreadMain(void *arg)\n{\n   // parameter passed to the thread is an instance of the ExLobHdfs object\n   ExLobGlobals *glob = (ExLobGlobals *)arg;\n\n   glob->doWorkInThread();\n\n   return NULL;\n}\n\nEx_Lob_Error ExLobGlobals::startWorkerThreads()\n{\n   int rc;\n   for (int i=0; i<NUM_WORKER_THREADS; i++) {\n     rc = pthread_create(&threadId_[i], NULL, workerThreadMain, this);\n     if (rc != 0)\n      return LOB_HDFS_THREAD_CREATE_ERROR;\n   }\n   \n   return LOB_OPER_OK;\n}\n\n\n\n///////////////////////////////////////////////////////////////////////////////\n// ExLobHdfs definitions\n///////////////////////////////////////////////////////////////////////////////\n\nExLobLock::ExLobLock()\n    : bellRang_(false),\n      waiters_(0)\n{\n   pthread_mutexattr_t mutexAttr;\n   pthread_mutexattr_init( &mutexAttr );\n   pthread_mutex_init( &mutex_, &mutexAttr );\n   pthread_cond_init( &workBell_, NULL );\n}\n\nExLobLock::~ExLobLock()\n{\n   pthread_mutex_unlock( &mutex_ );\n   pthread_mutex_destroy(&mutex_);\n   pthread_cond_destroy(&workBell_);\n}\n\nvoid ExLobLock::lock()\n{\n   pthread_mutex_lock( &mutex_ );\n}\n\nvoid ExLobLock::unlock()\n{\n   pthread_mutex_unlock( &mutex_ );\n}\n\nvoid ExLobLock::wakeOne()\n{\n   pthread_cond_signal(&workBell_);\n}\n\nvoid ExLobLock::wakeAll()\n{\n   pthread_cond_broadcast(&workBell_);\n}\n\nvoid ExLobLock::wait()\n{\n    waiters_++;\n    pthread_cond_wait(&workBell_, &mutex_);\n    waiters_--;\n}\n\n#ifdef __ignore\nExLobHdfsRequest::ExLobHdfsRequest(LobsHdfsRequestType reqType, hdfsFS fs, \n                                   hdfsFile file, char *buffer, int size) :\n   reqType_(reqType),\n   fs_(fs),\n   file_(file),\n   buffer_(buffer),\n   size_(size)\n{\n  lobPtr_ = 0;\n  error_ = LOB_OPER_OK;\n}\n#endif\nExLobHdfsRequest::ExLobHdfsRequest(LobsHdfsRequestType reqType, ExLobCursor *cursor) :\n   reqType_(reqType),\n   cursor_(cursor)\n{\n  buffer_=0;\n  lobPtr_=0;\n  size_=0;\n  error_=LOB_OPER_OK;\n}\n\nExLobHdfsRequest::ExLobHdfsRequest(LobsHdfsRequestType reqType, ExLob *lobPtr, ExLobCursor *cursor) :\n   reqType_(reqType),\n   lobPtr_(lobPtr),\n   cursor_(cursor)\n{\n  buffer_=0;\n  size_=0;\n  error_=LOB_OPER_OK;\n}\n\nExLobHdfsRequest::ExLobHdfsRequest(LobsHdfsRequestType reqType) :\n   reqType_(reqType)\n{\n\n  buffer_=0;\n  cursor_=0;\n  lobPtr_=0;\n  size_=0;\n  error_=LOB_OPER_OK;\n}\n\nExLobHdfsRequest::~ExLobHdfsRequest()\n{\n}\n\nEx_Lob_Error ExLobGlobals::enqueueRequest(ExLobHdfsRequest *request)\n{\n   char buffer2[2048];\n   sprintf(buffer2, \"enqueue request %d\", request->reqType_);\n   traceMessage(buffer2, NULL, __LINE__);\n   reqQueueLock_.lock();\n   reqQueue_.push_back(request);\n   reqQueueLock_.wakeOne();\n   reqQueueLock_.unlock();\n\n   return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLobGlobals::enqueuePrefetchRequest(ExLob *lobPtr, ExLobCursor *cursor)\n{// Leaving this allocated from system heap. Since this class contains hdfsFS unable to derive from LOB heap\n  ExLobHdfsRequest *request = new  ExLobHdfsRequest(Lob_Hdfs_Cursor_Prefetch, lobPtr, cursor);\n   \n   if (!request) {\n     // return error\n   }\n\n   enqueueRequest(request);\n\n   return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLobGlobals::enqueueShutdownRequest()\n{\n // Leaving this allocated from system heap. Since this class contains hdfsFS unable to derive from LOB heap\n  ExLobHdfsRequest *request = new ExLobHdfsRequest(Lob_Hdfs_Shutdown);\n   \n   if (!request) {\n     // return error\n   }\n\n   enqueueRequest(request);\n\n   return LOB_OPER_OK;\n}\n\n\nExLobHdfsRequest* ExLobGlobals::getHdfsRequest()\n{\n   ExLobHdfsRequest *request;\n   reqList_t::iterator it;\n\n   reqQueueLock_.lock();\n   it = reqQueue_.begin();\n\n   request = NULL;\n\n   while(request == NULL) \n   {\n      if (it != reqQueue_.end()) \n      {\n         request = *it;\n         it = reqQueue_.erase(it);\n      } else {\n         reqQueueLock_.wait();\n         it = reqQueue_.begin();\n      }\n   }\n\n   reqQueueLock_.unlock();\n   char buffer2[2048];\n   sprintf(buffer2, \"got request %d\", request->reqType_);\n   traceMessage(buffer2, NULL, __LINE__);\n   return request;\n}\n\nvoid ExLobGlobals::doWorkInThread()\n{\n   ExLobHdfsRequest *request;\n\n   // mask all signals\n   struct sigaction act;\n   sigemptyset(&act.sa_mask);\n\n   sigset_t mask;\n   sigfillset(&mask);\n   int rc = pthread_sigmask(SIG_SETMASK, &mask, NULL);\n   if (rc != 0) {\n      return;\n   }\n\n   // enter processing zone\n   for (;;) \n   {\n      request = getHdfsRequest(); // will wait until new req arrives\n\n      if (request->isShutDown()) { \n\t//we are asked to shutdown\n\t//wake up next worker before going away\n\treqQueueLock_.lock();\n\treqQueueLock_.wakeOne();\n\treqQueueLock_.unlock();\n\tbreak; \n\n      }\n      else {\n         performRequest(request);\n         delete request;\n      }\n   }\n\n   pthread_exit(0);\n}\n\nEx_Lob_Error ExLobGlobals::addToPreOpenList(ExLobPreOpen *preOpenObj)\n{\n    preOpenListLock_.lock();\n    preOpenList_.push_back(preOpenObj);\n    preOpenListLock_.unlock();\n    return LOB_OPER_OK;\n}\n\nEx_Lob_Error ExLobGlobals::processPreOpens() \n{\n    ExLobPreOpen *preOpenObj = NULL; \n    preOpenList_t::iterator p_it;\n\n    preOpenListLock_.lock();\n    if (!preOpenList_.empty()) \n    {\n        p_it = preOpenList_.begin();\n        preOpenObj = *p_it;\n        preOpenList_.erase(p_it);\n    }\n    preOpenListLock_.unlock();\n\n    if (preOpenObj != NULL) \n    {\n        ExLob *lobPtr = preOpenObj->lobPtr_;\n\n        lobPtr->openDataCursor(preOpenObj->cursorName_, Lob_Cursor_Simple, preOpenObj->range_, \n                               preOpenObj->bufMaxSize_, preOpenObj->maxBytes_, \n                               preOpenObj->waited_, this);\n    }\n\n    return LOB_OPER_OK;\n}\n\n//Enable envvar TRACE_HDFS_THREAD_ACTIONS to enable tracing. \n//The output file will be named trace_threads.<pid> on ech node\n\nvoid ExLobGlobals::traceMessage(const char *logMessage, ExLobCursor *cursor,\n                                int line)\n{\n  if ( threadTraceFile_ && logMessage)\n  {\n    fprintf(threadTraceFile_, \n    \"Thread: 0x%lx Line:  %d %s 0x%lx\\n\" ,\n       (unsigned long)pthread_self(), line, logMessage, \n       (unsigned long) cursor);\n    fflush(threadTraceFile_);\n  }\n    \n}\n\n//Enable envvar TRACE_LOB_ACTIONS to enable tracing. \n//The output file will be in the masterexec.<pid> logs in the \n//$MY_SQROOT/logs directory on each node\n\nvoid lobDebugInfo(const char *logMessage,Int32 errorcode,\n                         Int32 line, NABoolean lobTrace)\n{\n  if ( lobTrace) \n    {\n      NAString logString(\"LOB : \");\n      logString += logMessage;\n      \n      SQLMXLoggingArea::logSQLMXDebugEvent(logString.data(),(short)errorcode,line);\n    }\n  \n    \n}\n\n// ExLobRequest definitions\n///////////////////////////////////////////////////////////////////////////////\n\nExLobRequest::ExLobRequest() :\n    reqNum_(0),\n    descNumIn_(-1),\n    descNumOut_(-1),\n    handleInLen_(-1),\n    handleOutLen_(-1),\n    dataOffset_(-1),\n    type_(Lob_Req_Invalid),\n    storage_(Lob_Invalid_Storage),\n    operLen_(-1),\n    error_(LOB_INVALID_ERROR_VAL),\n    cliError_(-1),\n    status_(LOB_INVALID_ERROR_VAL),\n    transId_(0)\n{\n   TRANSID_SET_NULL(transIdBig_);\n}\nvoid ExLobRequest::setValues(char *descFileName, Int64 descNumIn, Int64 handleInLen, \n                             char *handleIn, LobsStorage storage, Int64 transId,\n                             SB_Transid_Type transIdBig,\n                             SB_Transseq_Type transStartId,\n                             char *blackBox, Int64 blackBoxLen)\n{\n  \n    descNumIn_ = descNumIn;\n    handleInLen_ = handleInLen;\n    storage_ = storage;\n    strcpy(descFileName_, descFileName);\n    if (handleIn != NULL && handleInLen > 0) {\n       memcpy(handleIn_, handleIn, handleInLen);\n    }\n    cliError_ = -1;\n    error_ = LOB_INVALID_ERROR_VAL;\n    status_ = LOB_INVALID_ERROR_VAL;\n\n    transId_ = transId;\n    transIdBig_ = transIdBig;\n    transStartId_ = transStartId;\n    blackBoxLen_ = blackBoxLen;\n    if (blackBox != NULL && blackBoxLen > 0) {\n       memcpy(blackBox_, blackBox, blackBoxLen);\n    }\n    \n}\n\nvoid ExLobRequest::getValues(Int64 &descNumOut, Int64 &handleOutLen, \n                             char *handleOut, Ex_Lob_Error &requestStatus, \n                             Int64 &cliError,\n                             char *blackBox, Int64 &blackBoxLen)\n{\n  \n    descNumOut = descNumOut_;\n    handleOutLen = handleOutLen_;\n    requestStatus = error_;\n    cliError = cliError_;\n    if (handleOut != NULL && handleOutLen_ > 0) {\n       memcpy(handleOut, handleOut_, handleOutLen_);\n    }\n    blackBoxLen = blackBoxLen_;\n    if (blackBox != NULL && blackBoxLen_ > 0) {\n       memcpy(blackBox, blackBox_, blackBoxLen_);\n    }\n    // #endif\n}\n\nExLobRequest::~ExLobRequest()\n{\n}\n\nEx_Lob_Error ExLobRequest::send()\n{\n \n\n    int msgid; \n    int oid;\n    MS_Result_Type result;\n    short req_ctrl[BUFSIZ];\n    short rep_ctrl[BUFSIZ];\n    char *req_data = (char *)this;\n    ExLobRequest rep_data;\n    short req_data_len = sizeof(ExLobRequest);\n    short rep_data_max = sizeof(ExLobRequest);\n    int err=0;\n    int inx=0;\n    int retries = 3;\n\n    incrReqNum();\n\n    status_ = LOB_OPER_REQ_IN_PROGRESS;\n\n    do \n    {\n       err = BMSG_LINK_(&serverPhandle,\n                        &msgid, \n                        req_ctrl,\n                        (ushort) (inx &1),\n                        rep_ctrl,\n                        1,\n                        req_data,\n                        req_data_len,\n                        (char *)&rep_data, \n                        rep_data_max,\n                        0,0,0,0); \n       retries--;\n\n       err = BMSG_BREAK_(msgid, (short *) &result, &serverPhandle);\n\n       if (err == -XZFIL_ERR_PATHDOWN) {\n        //lobGlobals>resetServerPhandle();\n       }\n\n    } while ( (err == XZFIL_ERR_PATHDOWN) && (retries > 0) ); // 201 if lobserver got restared\n\n    status_ = LOB_OPER_REQ_DONE;\n\n    if (err != XZFIL_ERR_OK)\n      return LOB_SEND_MSG_ERROR;\n\n    memcpy(this, &rep_data, rep_data_max);\n  \n    return LOB_OPER_OK;\n}\n", "idx": 1, "id": 12811, "msg": "Is this where we are expecting errno to be set? We could capture its value here.", "proj": "apache-trafodion", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -0,0 +1,27 @@\n+package session\n+\n+type Manager struct {\n+\tGenerator GeneratorInterface\n+\tsessions  []SessionId\n+}\n+\n+func (manager *Manager) Add(sid SessionId) {\n+\tmanager.sessions = append(manager.sessions, sid)\n+}\n+\n+func (manager *Manager) Create() SessionId {\n+\tid := manager.Generator.Generate()\n+\tmanager.Add(id)\n+\n+\treturn id\n+}\n+\n+func (manager *Manager) Get(sid SessionId) (sessionId SessionId) {\n+\tfor i := range manager.sessions {\n+\t\tif manager.sessions[i] == sid {\n+\t\t\treturn manager.sessions[i]\n+\t\t}\n+\t}\n+\n+\treturn\n+}", "y": 1, "oldf": "", "idx": 1, "id": 9534, "msg": "Impossible to distinguish if session was found/not-found. Test would help You to prove usability.", "proj": "mysteriumnetwork-node", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -39,11 +39,8 @@ public class ViewsConfigurationLibrary {\n \n     @Bean\n     public ViewLibrary viewLibrary() {\n-        return new ViewLibrary(\n-                applicationName,\n-                themeConfiguration.themeManager(),\n-                phoenicisGlobalConfiguration.objectMapper()\n-        );\n+        return new ViewLibrary(applicationName, themeConfiguration.themeManager(),\n+                phoenicisGlobalConfiguration.objectMapper());\n     }\n \n     @Bean", "y": 1, "oldf": "/*\n * Copyright (C) 2015-2017 P\u00c2RIS Quentin\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License along\n * with this program; if not, write to the Free Software Foundation, Inc.,\n * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n */\n\npackage org.phoenicis.javafx.views.mainwindow.library;\n\nimport org.phoenicis.configuration.PhoenicisGlobalConfiguration;\nimport org.phoenicis.javafx.views.common.ThemeConfiguration;\nimport org.phoenicis.javafx.views.mainwindow.console.ConsoleTabFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class ViewsConfigurationLibrary {\n    @Value(\"${application.name}\")\n    private String applicationName;\n\n    @Autowired\n    private ThemeConfiguration themeConfiguration;\n\n    @Autowired\n    private PhoenicisGlobalConfiguration phoenicisGlobalConfiguration;\n\n    @Bean\n    public ViewLibrary viewLibrary() {\n        return new ViewLibrary(\n                applicationName,\n                themeConfiguration.themeManager(),\n                phoenicisGlobalConfiguration.objectMapper()\n        );\n    }\n\n    @Bean\n    public ConsoleTabFactory consoleTabFactory() {\n        return new ConsoleTabFactory();\n    }\n}\n", "idx": 1, "id": 10142, "msg": "It would be great if the code formatter kept the other way of presenting beans parameters. It is a lot more readable", "proj": "PhoenicisOrg-phoenicis", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -458,7 +458,7 @@ class BrowseModeTreeInterceptor(treeInterceptorHandler.TreeInterceptor):\n \t\t\tobj.setFocus()\n \t\t\tself.passThrough = True\n \t\t\treportPassThrough(self)\n-\t\telif obj.role == controlTypes.ROLE_EMBEDDEDOBJECT or obj.role in self.APPLICATION_ROLES:\n+\t\telif obj.role == controlTypes.ROLE_EMBEDDEDOBJECT or self._isApplicationObject(obj):\n \t\t\tobj.setFocus()\n \t\t\tspeech.speakObject(obj, reason=controlTypes.REASON_FOCUS)\n \t\telse:", "y": 0, "oldf": "#browseMode.py\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#Copyright (C) 2007-2017 NV Access Limited, Babbage B.V.\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n\r\nimport itertools\r\nimport collections\r\nimport winsound\r\nimport time\r\nimport weakref\r\nimport wx\r\nfrom logHandler import log\r\nimport documentBase\r\nimport review\r\nimport scriptHandler\r\nimport eventHandler\r\nimport nvwave\r\nimport queueHandler\r\nimport gui\r\nimport ui\r\nimport cursorManager\r\nfrom scriptHandler import isScriptWaiting, willSayAllResume\r\nimport aria\r\nimport controlTypes\r\nimport config\r\nimport textInfos\r\nimport braille\r\nimport speech\r\nimport sayAllHandler\r\nimport treeInterceptorHandler\r\nimport inputCore\r\nimport api\r\nimport gui.guiHelper\r\nfrom NVDAObjects import NVDAObject\r\n\r\nREASON_QUICKNAV = \"quickNav\"\r\n\r\ndef reportPassThrough(treeInterceptor,onlyIfChanged=True):\r\n\t\"\"\"Reports the pass through mode if it has changed.\r\n\t@param treeInterceptor: The current Browse Mode treeInterceptor.\r\n\t@type treeInterceptor: L{BrowseModeTreeInterceptor}\r\n\t@param onlyIfChanged: if true reporting will not happen if the last reportPassThrough reported the same thing.\r\n\t@type onlyIfChanged: bool\r\n\t\"\"\"\r\n\tif not onlyIfChanged or treeInterceptor.passThrough != reportPassThrough.last:\r\n\t\tif config.conf[\"virtualBuffers\"][\"passThroughAudioIndication\"]:\r\n\t\t\tsound = r\"waves\\focusMode.wav\" if treeInterceptor.passThrough else r\"waves\\browseMode.wav\"\r\n\t\t\tnvwave.playWaveFile(sound)\r\n\t\telse:\r\n\t\t\tif treeInterceptor.passThrough:\r\n\t\t\t\t# Translators: The mode to interact with controls in documents\r\n\t\t\t\tui.message(_(\"Focus mode\"))\r\n\t\t\telse:\r\n\t\t\t\t# Translators: The mode that presents text in a flat representation\r\n\t\t\t\t# that can be navigated with the cursor keys like in a text document\r\n\t\t\t\tui.message(_(\"Browse mode\"))\r\n\t\treportPassThrough.last = treeInterceptor.passThrough\r\nreportPassThrough.last = False\r\n\r\ndef mergeQuickNavItemIterators(iterators,direction=\"next\"):\r\n\t\"\"\"\r\n\tMerges multiple iterators that emit L{QuickNavItem} objects, yielding them from first to last. \r\n\tThey are sorted using min or max (__lt__ should be implemented on the L{QuickNavItem} objects).\r\n\t@param iters: the iterators you want to merge. \r\n\t@type iters: sequence of iterators that emit L{QuicknavItem} objects.\r\n\t@param direction: the direction these iterators are searching (e.g. next, previous)\r\n\t@type direction: string\r\n\t\"\"\"\r\n\tfinder=min if direction==\"next\" else max\r\n\tcurValues=[]\r\n\t# Populate a list with all iterators and their corisponding first value\r\n\tfor it in iterators:\r\n\t\ttry:\r\n\t\t\tval=next(it)\r\n\t\texcept StopIteration:\r\n\t\t\tcontinue\r\n\t\tcurValues.append((it,val))\r\n\t# Until all iterators have been used up,\r\n\t# Find the first (minimum or maximum) of all the values,\r\n\t# emit that, and update the list with the next available value for the iterator whose value was emitted.\r\n\twhile len(curValues)>0:\r\n\t\tfirst=finder(curValues,key=lambda x: x[1])\r\n\t\tcurValues.remove(first)\r\n\t\tit,val=first\r\n\t\tyield val\r\n\t\ttry:\r\n\t\t\tnewVal=next(it)\r\n\t\texcept StopIteration:\r\n\t\t\tcontinue\r\n\t\tcurValues.append((it,newVal))\r\n\r\nclass QuickNavItem(object):\r\n\t\"\"\" Emitted by L{BrowseModeTreeInterceptor._iterNodesByType}, this represents one of many positions in a browse mode document, based on the type of item being searched for (e.g. link, heading, table etc).\"\"\"  \r\n\r\n\titemType=None #: The type of items searched for (e.g. link, heading, table etc) \r\n\tlabel=None #: The label that should represent this item in the Elements list.\r\n\tisAfterSelection=False #: Is this item positioned after the caret in the document? Used by the elements list to place its own selection.\r\n\r\n\tdef __init__(self,itemType,document):\r\n\t\t\"\"\"\r\n\t\t@param itemType: the type that was searched for (e.g. link, heading, table etc)\r\n\t\t@ type itemType: string\r\n\t\t@ param document: the browse mode document this item is a part of.\r\n\t\t@type document: L{BrowseModeTreeInterceptor}\r\n\t\t\"\"\"\r\n\t\tself.itemType=itemType\r\n\t\tself.document=document\r\n\r\n\tdef isChild(self,parent):\r\n\t\t\"\"\"\r\n\t\tIs this item a child of the given parent?\r\n\t\tThis is used when representing items in a hierarchical tree structure, such as the Elements List.\r\n\t\t@param parent: the item of whom this item may be a child of.\r\n\t\t@type parent: L{QuickNavItem}\r\n\t\t@return: True if this item is a child, false otherwise.\r\n\t\t@rtype: bool\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef report(self,readUnit=None):\r\n\t\t\"\"\"\r\n\t\tReports the contents of this item.\r\n\t\t@param readUnit: the optional unit (e.g. line, paragraph) that should be used to announce the item position when moved to. If not given, then the full sise of the item is used.\r\n\t\t@type readUnit: a L{textInfos}.UNIT_* constant.\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef moveTo(self):\r\n\t\t\"\"\"\r\n\t\tMoves the browse mode caret or focus to this item.\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef activate(self):\r\n\t\t\"\"\"\r\n\t\tActivates this item's position. E.g. follows a link, presses a button etc.\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef rename(self,newName):\r\n\t\t\"\"\"\r\n\t\tRenames this item with the new name.\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\t@property\r\n\tdef isRenameAllowed(self):\r\n\t\treturn False\r\n\r\nclass TextInfoQuickNavItem(QuickNavItem):\r\n\t\"\"\" Represents a quick nav item in a browse mode document who's positions are represented by a L{textInfos.TextInfo}. \"\"\"\r\n\r\n\tdef __init__(self,itemType,document,textInfo):\r\n\t\t\"\"\"\r\n\t\tSee L{QuickNavItem.__init__} for itemType and document argument definitions.\r\n\t\t@param textInfo: the textInfo position this item represents.\r\n\t\t@type textInfo: L{textInfos.TextInfo}\r\n\t\t\"\"\"\r\n\t\tself.textInfo=textInfo\r\n\t\tsuper(TextInfoQuickNavItem,self).__init__(itemType,document)\r\n\r\n\tdef __lt__(self,other):\r\n\t\treturn self.textInfo.compareEndPoints(other.textInfo,\"startToStart\")<0\r\n\r\n\t@property\r\n\tdef obj(self):\r\n\t\treturn self.textInfo.basePosition if isinstance(self.textInfo.basePosition,NVDAObject) else None\r\n\r\n\t@property\r\n\tdef label(self):\r\n\t\treturn self.textInfo.text.strip()\r\n\r\n\tdef isChild(self,parent):\r\n\t\tif parent.textInfo.isOverlapping(self.textInfo):\r\n\t\t\treturn True\r\n\t\treturn False\r\n\r\n\tdef report(self,readUnit=None):\r\n\t\tinfo=self.textInfo\r\n\t\tif readUnit:\r\n\t\t\tfieldInfo = info.copy()\r\n\t\t\tinfo.collapse()\r\n\t\t\tinfo.move(readUnit, 1, endPoint=\"end\")\r\n\t\t\tif info.compareEndPoints(fieldInfo, \"endToEnd\") > 0:\r\n\t\t\t\t# We've expanded past the end of the field, so limit to the end of the field.\r\n\t\t\t\tinfo.setEndPoint(fieldInfo, \"endToEnd\")\r\n\t\tspeech.speakTextInfo(info, reason=controlTypes.REASON_FOCUS)\r\n\r\n\tdef activate(self):\r\n\t\tself.textInfo.obj._activatePosition(self.textInfo)\r\n\r\n\tdef moveTo(self):\r\n\t\tinfo=self.textInfo.copy()\r\n\t\tinfo.collapse()\r\n\t\tself.document._set_selection(info,reason=REASON_QUICKNAV)\r\n\r\n\t@property\r\n\tdef isAfterSelection(self):\r\n\t\tcaret=self.document.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\treturn self.textInfo.compareEndPoints(caret, \"startToStart\") > 0\r\n\r\n\tdef _getLabelForProperties(self, labelPropertyGetter):\r\n\t\t\"\"\"\r\n\t\tFetches required properties for this L{TextInfoQuickNavItem} and constructs a label to be shown in an elements list.\r\n\t\tThis can be used by subclasses to implement the L{label} property.\r\n\t\t@Param labelPropertyGetter: A callable taking 1 argument, specifying the property to fetch.\r\n\t\t\tFor example, if L{itemType} is landmark, the callable must return the landmark type when \"landmark\" is passed as the property argument.\r\n\t\t\tAlternative property names might be name or value.\r\n\t\t\tThe callable must return None if the property doesn't exist.\r\n\t\t\tAn expected callable might be get method on a L{Dict},\r\n\t\t\tor \"lambda property: getattr(self.obj, property, None)\" for an L{NVDAObject}.\r\n\t\t\"\"\"\r\n\t\tcontent = self.textInfo.text.strip()\r\n\t\tif self.itemType is \"heading\":\r\n\t\t\t# Output: displayed text of the heading.\r\n\t\t\treturn content\r\n\t\tlabelParts = None\r\n\t\tname = labelPropertyGetter(\"name\")\r\n\t\tif self.itemType is \"landmark\":\r\n\t\t\tlandmark = aria.landmarkRoles.get(labelPropertyGetter(\"landmark\"))\r\n\t\t\t# Example output: main menu; navigation\r\n\t\t\tlabelParts = (name, landmark)\r\n\t\telse: \r\n\t\t\trole = labelPropertyGetter(\"role\")\r\n\t\t\troleText = controlTypes.roleLabels[role]\r\n\t\t\t# Translators: Reported label in the elements list for an element which which has no name and value\r\n\t\t\tunlabeled = _(\"Unlabeled\")\r\n\t\t\trealStates = labelPropertyGetter(\"states\")\r\n\t\t\tlabeledStates = \" \".join(controlTypes.processAndLabelStates(role, realStates, controlTypes.REASON_FOCUS))\r\n\t\t\tif self.itemType is \"formField\":\r\n\t\t\t\tif role in (controlTypes.ROLE_BUTTON,controlTypes.ROLE_DROPDOWNBUTTON,controlTypes.ROLE_TOGGLEBUTTON,controlTypes.ROLE_SPLITBUTTON,controlTypes.ROLE_MENUBUTTON,controlTypes.ROLE_DROPDOWNBUTTONGRID,controlTypes.ROLE_SPINBUTTON,controlTypes.ROLE_TREEVIEWBUTTON):\r\n\t\t\t\t\t# Example output: Mute; toggle button; pressed\r\n\t\t\t\t\tlabelParts = (content or name or unlabeled, roleText, labeledStates)\r\n\t\t\t\telse:\r\n\t\t\t\t\t# Example output: Find a repository...; edit; has auto complete; NVDA\r\n\t\t\t\t\tlabelParts = (name or unlabeled, roleText, labeledStates, content)\r\n\t\t\telif self.itemType in (\"link\", \"button\"):\r\n\t\t\t\t# Example output: You have unread notifications; visited\r\n\t\t\t\tlabelParts = (content or name or unlabeled, labeledStates)\r\n\t\tif labelParts:\r\n\t\t\tlabel = \"; \".join(lp for lp in labelParts if lp)\r\n\t\telse:\r\n\t\t\tlabel = content\r\n\t\treturn label\r\n\r\nclass BrowseModeTreeInterceptor(treeInterceptorHandler.TreeInterceptor):\r\n\tscriptCategory = inputCore.SCRCAT_BROWSEMODE\r\n\tdisableAutoPassThrough = False\r\n\tAPPLICATION_ROLES = (controlTypes.ROLE_APPLICATION, controlTypes.ROLE_DIALOG)\r\n\r\n\tdef _get_currentNVDAObject(self):\r\n\t\traise NotImplementedError\r\n\r\n\tALWAYS_SWITCH_TO_PASS_THROUGH_ROLES = frozenset({\r\n\t\tcontrolTypes.ROLE_COMBOBOX,\r\n\t\tcontrolTypes.ROLE_EDITABLETEXT,\r\n\t\tcontrolTypes.ROLE_LIST,\r\n\t\tcontrolTypes.ROLE_SLIDER,\r\n\t\tcontrolTypes.ROLE_TABCONTROL,\r\n\t\tcontrolTypes.ROLE_MENUBAR,\r\n\t\tcontrolTypes.ROLE_POPUPMENU,\r\n\t\tcontrolTypes.ROLE_TREEVIEW,\r\n\t\tcontrolTypes.ROLE_TREEVIEWITEM,\r\n\t\tcontrolTypes.ROLE_SPINBUTTON,\r\n\t\tcontrolTypes.ROLE_TABLEROW,\r\n\t\tcontrolTypes.ROLE_TABLECELL,\r\n\t\tcontrolTypes.ROLE_TABLEROWHEADER,\r\n\t\tcontrolTypes.ROLE_TABLECOLUMNHEADER,\r\n\t\t})\r\n\r\n\tSWITCH_TO_PASS_THROUGH_ON_FOCUS_ROLES = frozenset({\r\n\t\tcontrolTypes.ROLE_LISTITEM,\r\n\t\tcontrolTypes.ROLE_RADIOBUTTON,\r\n\t\tcontrolTypes.ROLE_TAB,\r\n\t\tcontrolTypes.ROLE_MENUITEM,\r\n\t\tcontrolTypes.ROLE_RADIOMENUITEM,\r\n\t\tcontrolTypes.ROLE_CHECKMENUITEM,\r\n\t\t})\r\n\r\n\tdef shouldPassThrough(self, obj, reason=None):\r\n\t\t\"\"\"Determine whether pass through mode should be enabled (focus mode) or disabled (browse mode) for a given object.\r\n\t\t@param obj: The object in question.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t@param reason: The reason for this query; one of the output reasons, L{REASON_QUICKNAV}, or C{None} for manual pass through mode activation by the user.\r\n\t\t@return: C{True} if pass through mode (focus mode) should be enabled, C{False} if it should be disabled (browse mode).\r\n\t\t\"\"\"\r\n\t\tif reason and (\r\n\t\t\tself.disableAutoPassThrough\r\n\t\t\tor (reason == controlTypes.REASON_FOCUS and not config.conf[\"virtualBuffers\"][\"autoPassThroughOnFocusChange\"])\r\n\t\t\tor (reason == controlTypes.REASON_CARET and not config.conf[\"virtualBuffers\"][\"autoPassThroughOnCaretMove\"])\r\n\t\t):\r\n\t\t\t# This check relates to auto pass through and auto pass through is disabled, so don't change the pass through state.\r\n\t\t\treturn self.passThrough\r\n\t\tif reason == REASON_QUICKNAV:\r\n\t\t\treturn False\r\n\t\tstates = obj.states\r\n\t\trole = obj.role\r\n\t\tif controlTypes.STATE_EDITABLE in states and controlTypes.STATE_UNAVAILABLE not in states:\r\n\t\t\treturn True\r\n\t\t# Menus sometimes get focus due to menuStart events even though they don't report as focused/focusable.\r\n\t\tif not obj.isFocusable and controlTypes.STATE_FOCUSED not in states and role != controlTypes.ROLE_POPUPMENU:\r\n\t\t\treturn False\r\n\t\t# many controls that are read-only should not switch to passThrough. \r\n\t\t# However, certain controls such as combo boxes and readonly edits are read-only but still interactive.\r\n\t\t# #5118: read-only ARIA grids should also be allowed (focusable table cells, rows and headers).\r\n\t\tif controlTypes.STATE_READONLY in states and role not in (controlTypes.ROLE_EDITABLETEXT, controlTypes.ROLE_COMBOBOX, controlTypes.ROLE_TABLEROW, controlTypes.ROLE_TABLECELL, controlTypes.ROLE_TABLEROWHEADER, controlTypes.ROLE_TABLECOLUMNHEADER):\r\n\t\t\treturn False\r\n\t\t# Any roles or states for which we always switch to passThrough\r\n\t\tif role in self.ALWAYS_SWITCH_TO_PASS_THROUGH_ROLES or controlTypes.STATE_EDITABLE in states:\r\n\t\t\treturn True\r\n\t\t# focus is moving to this control. Perhaps after pressing tab or clicking a button that brings up a menu (via javascript)\r\n\t\tif reason == controlTypes.REASON_FOCUS:\r\n\t\t\tif role in self.SWITCH_TO_PASS_THROUGH_ON_FOCUS_ROLES:\r\n\t\t\t\treturn True\r\n\t\t\t# If this is a focus change, pass through should be enabled for certain ancestor containers.\r\n\t\t\t# this is done last for performance considerations. Walking up the through the parents could be costly\r\n\t\t\twhile obj and obj != self.rootNVDAObject:\r\n\t\t\t\tif obj.role == controlTypes.ROLE_TOOLBAR:\r\n\t\t\t\t\treturn True\r\n\t\t\t\tobj = obj.parent\r\n\t\treturn False\r\n\r\n\tdef _get_shouldTrapNonCommandGestures(self):\r\n\t\treturn config.conf['virtualBuffers']['trapNonCommandGestures']\r\n\r\n\tdef script_trapNonCommandGesture(self,gesture):\r\n\t\twinsound.PlaySound(\"default\",1)\r\n\r\n\tsingleLetterNavEnabled=True #: Whether single letter navigation scripts should be active (true) or if these letters should fall to the application.\r\n\r\n\tdef getAlternativeScript(self,gesture,script):\r\n\t\tif self.passThrough or not gesture.isCharacter:\r\n\t\t\treturn script\r\n\t\tif not self.singleLetterNavEnabled:\r\n\t\t\treturn None\r\n\t\tif not script and self.shouldTrapNonCommandGestures: \r\n\t\t\tscript=self.script_trapNonCommandGesture\r\n\t\treturn script\r\n\r\n\tdef script_toggleSingleLetterNav(self,gesture):\r\n\t\tif self.singleLetterNavEnabled:\r\n\t\t\tself.singleLetterNavEnabled=False\r\n\t\t\t# Translators: Reported when single letter navigation in browse mode is turned off.\r\n\t\t\tui.message(_(\"Single letter navigation off\"))\r\n\t\telse:\r\n\t\t\tself.singleLetterNavEnabled=True\r\n\t\t\t# Translators: Reported when single letter navigation in browse mode is turned on.\r\n\t\t\tui.message(_(\"Single letter navigation on\"))\r\n\t# Translators: the description for the toggleSingleLetterNavigation command in browse mode.\r\n\tscript_toggleSingleLetterNav.__doc__=_(\"Toggles single letter navigation on and off. When on, single letter keys in browse mode jump to various kinds of elements on the page. When off, these keys are passed to the application\")\r\n\r\n\tdef _get_ElementsListDialog(self):\r\n\t\treturn ElementsListDialog\r\n\r\n\tdef _iterNodesByType(self,itemType,direction=\"next\",pos=None):\r\n\t\t\"\"\"\r\n\t\tYields L{QuickNavItem} objects representing the ordered positions in this document according to the type being searched for (e.g. link, heading, table etc).\r\n\t\t@param itemType: the type being searched for (e.g. link, heading, table etc)\r\n\t\t@type itemType: string\r\n\t\t@param direction: the direction in which to search (next, previous, up)\r\n\t\t@ type direction: string\r\n\t\t@param pos: the position in the document from where to start the search.\r\n\t\t@type pos: Usually an L{textInfos.TextInfo} \r\n\t\t@raise NotImplementedError: This type is not supported by this BrowseMode implementation\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef _iterNotLinkBlock(self, direction=\"next\", pos=None):\r\n\t\traise NotImplementedError\r\n\r\n\tdef _quickNavScript(self,gesture, itemType, direction, errorMessage, readUnit):\r\n\t\tif itemType==\"notLinkBlock\":\r\n\t\t\titerFactory=self._iterNotLinkBlock\r\n\t\telse:\r\n\t\t\titerFactory=lambda direction,info: self._iterNodesByType(itemType,direction,info)\r\n\t\tinfo=self.selection\r\n\t\ttry:\r\n\t\t\titem = next(iterFactory(direction, info))\r\n\t\texcept NotImplementedError:\r\n\t\t\t# Translators: a message when a particular quick nav command is not supported in the current document.\r\n\t\t\tui.message(_(\"Not supported in this document\"))\r\n\t\t\treturn\r\n\t\texcept StopIteration:\r\n\t\t\tui.message(errorMessage)\r\n\t\t\treturn\r\n\t\titem.moveTo()\r\n\t\tif not gesture or not willSayAllResume(gesture):\r\n\t\t\titem.report(readUnit=readUnit)\r\n\r\n\t@classmethod\r\n\tdef addQuickNav(cls, itemType, key, nextDoc, nextError, prevDoc, prevError, readUnit=None):\r\n\t\t\"\"\"Adds a script for the given quick nav item.\r\n\t\t@param itemType: The type of item, I.E. \"heading\" \"Link\" ...\r\n\t\t@param key: The quick navigation key to bind to the script. Shift is automatically added for the previous item gesture. E.G. h for heading\r\n\t\t@param nextDoc: The command description to bind to the script that yields the next quick nav item.\r\n\t\t@param nextError: The error message if there are no more quick nav items of type itemType in this direction.\r\n\t\t@param prevDoc: The command description to bind to the script that yields the previous quick nav item.\r\n\t\t@param prevError: The error message if there are no more quick nav items of type itemType in this direction.\r\n\t\t@param readUnit: The unit (one of the textInfos.UNIT_* constants) to announce when moving to this type of item. \r\n\t\t\tFor example, only the line is read when moving to tables to avoid reading a potentially massive table. \r\n\t\t\tIf None, the entire item will be announced.\r\n\t\t\"\"\"\r\n\t\tscriptSuffix = itemType[0].upper() + itemType[1:]\r\n\t\tscriptName = \"next%s\" % scriptSuffix\r\n\t\tfuncName = \"script_%s\" % scriptName\r\n\t\tscript = lambda self,gesture: self._quickNavScript(gesture, itemType, \"next\", nextError, readUnit)\r\n\t\tscript.__doc__ = nextDoc\r\n\t\tscript.__name__ = funcName\r\n\t\tscript.resumeSayAllMode=sayAllHandler.CURSOR_CARET\r\n\t\tsetattr(cls, funcName, script)\r\n\t\tcls.__gestures[\"kb:%s\" % key] = scriptName\r\n\t\tscriptName = \"previous%s\" % scriptSuffix\r\n\t\tfuncName = \"script_%s\" % scriptName\r\n\t\tscript = lambda self,gesture: self._quickNavScript(gesture, itemType, \"previous\", prevError, readUnit)\r\n\t\tscript.__doc__ = prevDoc\r\n\t\tscript.__name__ = funcName\r\n\t\tscript.resumeSayAllMode=sayAllHandler.CURSOR_CARET\r\n\t\tsetattr(cls, funcName, script)\r\n\t\tcls.__gestures[\"kb:shift+%s\" % key] = scriptName\r\n\r\n\tdef script_elementsList(self,gesture):\r\n\t\t# We need this to be a modal dialog, but it mustn't block this script.\r\n\t\tdef run():\r\n\t\t\tgui.mainFrame.prePopup()\r\n\t\t\td = self.ElementsListDialog(self)\r\n\t\t\td.ShowModal()\r\n\t\t\td.Destroy()\r\n\t\t\tgui.mainFrame.postPopup()\r\n\t\twx.CallAfter(run)\r\n\t# Translators: the description for the Elements List command in browse mode.\r\n\tscript_elementsList.__doc__ = _(\"Lists various types of elements in this document\")\r\n\r\n\tdef _activateNVDAObject(self, obj):\r\n\t\t\"\"\"Activate an object in response to a user request.\r\n\t\tThis should generally perform the default action or click on the object.\r\n\t\t@param obj: The object to activate.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t\"\"\"\r\n\t\ttry:\r\n\t\t\tobj.doAction()\r\n\t\texcept NotImplementedError:\r\n\t\t\tlog.debugWarning(\"doAction not implemented\")\r\n\r\n\tdef _activatePosition(self,obj=None):\r\n\t\tif not obj:\r\n\t\t\tobj=self.currentNVDAObject\r\n\t\t\tif not obj:\r\n\t\t\t\treturn\r\n\t\tif obj.role == controlTypes.ROLE_MATH:\r\n\t\t\timport mathPres\r\n\t\t\ttry:\r\n\t\t\t\treturn mathPres.interactWithMathMl(obj.mathMl)\r\n\t\t\texcept (NotImplementedError, LookupError):\r\n\t\t\t\tpass\r\n\t\t\treturn\r\n\t\tif self.shouldPassThrough(obj):\r\n\t\t\tobj.setFocus()\r\n\t\t\tself.passThrough = True\r\n\t\t\treportPassThrough(self)\r\n\t\telif obj.role == controlTypes.ROLE_EMBEDDEDOBJECT or obj.role in self.APPLICATION_ROLES:\r\n\t\t\tobj.setFocus()\r\n\t\t\tspeech.speakObject(obj, reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\tself._activateNVDAObject(obj)\r\n\r\n\tdef script_activatePosition(self,gesture):\r\n\t\tself._activatePosition()\r\n\t# Translators: the description for the activatePosition script on browseMode documents.\r\n\tscript_activatePosition.__doc__ = _(\"Activates the current object in the document\")\r\n\r\n\tdef script_disablePassThrough(self, gesture):\r\n\t\tif not self.passThrough or self.disableAutoPassThrough:\r\n\t\t\treturn gesture.send()\r\n\t\tself.passThrough = False\r\n\t\tself.disableAutoPassThrough = False\r\n\t\treportPassThrough(self)\r\n\tscript_disablePassThrough.ignoreTreeInterceptorPassThrough = True\r\n\r\n\t__gestures={\r\n\t\t\"kb:NVDA+f7\": \"elementsList\",\r\n\t\t\"kb:enter\": \"activatePosition\",\r\n\t\t\"kb:numpadEnter\": \"activatePosition\",\r\n\t\t\"kb:space\": \"activatePosition\",\r\n\t\t\"kb:NVDA+shift+space\":\"toggleSingleLetterNav\",\r\n\t\t\"kb:escape\": \"disablePassThrough\",\r\n\t}\r\n\r\n# Add quick navigation scripts.\r\nqn = BrowseModeTreeInterceptor.addQuickNav\r\nqn(\"heading\", key=\"h\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading\"))\r\nqn(\"heading1\", key=\"1\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 1\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 1\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 1\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 1\"))\r\nqn(\"heading2\", key=\"2\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 2\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 2\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 2\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 2\"))\r\nqn(\"heading3\", key=\"3\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 3\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 3\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 3\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 3\"))\r\nqn(\"heading4\", key=\"4\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 4\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 4\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 4\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 4\"))\r\nqn(\"heading5\", key=\"5\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 5\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 5\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 5\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 5\"))\r\nqn(\"heading6\", key=\"6\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next heading at level 6\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next heading at level 6\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous heading at level 6\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous heading at level 6\"))\r\nqn(\"table\", key=\"t\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next table\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next table\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous table\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous table\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"link\", key=\"k\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next link\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next link\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous link\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous link\"))\r\nqn(\"visitedLink\", key=\"v\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next visited link\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next visited link\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous visited link\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous visited link\"))\r\nqn(\"unvisitedLink\", key=\"u\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next unvisited link\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next unvisited link\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous unvisited link\"), \r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous unvisited link\"))\r\nqn(\"formField\", key=\"f\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next form field\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next form field\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous form field\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous form field\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"list\", key=\"l\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next list\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next list\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous list\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous list\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"listItem\", key=\"i\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next list item\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next list item\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous list item\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous list item\"))\r\nqn(\"button\", key=\"b\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next button\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next button\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous button\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous button\"))\r\nqn(\"edit\", key=\"e\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next edit field\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next edit field\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous edit field\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous edit field\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"frame\", key=\"m\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next frame\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next frame\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous frame\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous frame\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"separator\", key=\"s\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next separator\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next separator\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous separator\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous separator\"))\r\nqn(\"radioButton\", key=\"r\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next radio button\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next radio button\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous radio button\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous radio button\"))\r\nqn(\"comboBox\", key=\"c\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next combo box\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next combo box\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous combo box\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous combo box\"))\r\nqn(\"checkBox\", key=\"x\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next check box\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next check box\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous check box\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous check box\"))\r\nqn(\"graphic\", key=\"g\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next graphic\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next graphic\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous graphic\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous graphic\"))\r\nqn(\"blockQuote\", key=\"q\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next block quote\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next block quote\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous block quote\"), \r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous block quote\"))\r\nqn(\"notLinkBlock\", key=\"n\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"skips forward past a block of links\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no more text after a block of links\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"skips backward past a block of links\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no more text before a block of links\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"landmark\", key=\"d\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next landmark\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next landmark\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous landmark\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous landmark\"),\r\n\treadUnit=textInfos.UNIT_LINE)\r\nqn(\"embeddedObject\", key=\"o\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next embedded object\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next embedded object\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous embedded object\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous embedded object\"))\r\nqn(\"annotation\", key=\"a\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next annotation\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next annotation\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous annotation\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous annotation\"))\r\nqn(\"error\", key=\"w\",\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tnextDoc=_(\"moves to the next error\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tnextError=_(\"no next error\"),\r\n\t# Translators: Input help message for a quick navigation command in browse mode.\r\n\tprevDoc=_(\"moves to the previous error\"),\r\n\t# Translators: Message presented when the browse mode element is not found.\r\n\tprevError=_(\"no previous error\"))\r\ndel qn\r\n\r\nclass ElementsListDialog(wx.Dialog):\r\n\tELEMENT_TYPES = (\r\n\t\t# Translators: The label of a radio button to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\t(\"link\", _(\"Lin&ks\")),\r\n\t\t# Translators: The label of a radio button to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\t(\"heading\", _(\"&Headings\")),\r\n\t\t# Translators: The label of a radio button to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\t(\"formField\", _(\"&Form fields\")),\r\n\t\t# Translators: The label of a radio button to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\t(\"button\", _(\"&Buttons\")),\r\n\t\t# Translators: The label of a radio button to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\t(\"landmark\", _(\"Lan&dmarks\")),\r\n\t)\r\n\r\n\tElement = collections.namedtuple(\"Element\", (\"item\", \"parent\"))\r\n\r\n\tlastSelectedElementType=0\r\n\r\n\tdef __init__(self, document):\r\n\t\tself.document = document\r\n\t\t# Translators: The title of the browse mode Elements List dialog.\r\n\t\tsuper(ElementsListDialog, self).__init__(gui.mainFrame, wx.ID_ANY, _(\"Elements List\"))\r\n\t\tmainSizer = wx.BoxSizer(wx.VERTICAL)\r\n\t\tcontentsSizer = wx.BoxSizer(wx.VERTICAL)\r\n\r\n\t\t# Translators: The label of a group of radio buttons to select the type of element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\tchild = wx.RadioBox(self, wx.ID_ANY, label=_(\"Type:\"), choices=tuple(et[1] for et in self.ELEMENT_TYPES))\r\n\t\tchild.SetSelection(self.lastSelectedElementType)\r\n\t\tchild.Bind(wx.EVT_RADIOBOX, self.onElementTypeChange)\r\n\t\tcontentsSizer.Add(child, flag=wx.EXPAND)\r\n\t\tcontentsSizer.AddSpacer(gui.guiHelper.SPACE_BETWEEN_VERTICAL_DIALOG_ITEMS)\r\n\r\n\t\tself.tree = wx.TreeCtrl(self, size=wx.Size(500, 600), style=wx.TR_HAS_BUTTONS | wx.TR_HIDE_ROOT | wx.TR_LINES_AT_ROOT | wx.TR_SINGLE | wx.TR_EDIT_LABELS)\r\n\t\tself.tree.Bind(wx.EVT_SET_FOCUS, self.onTreeSetFocus)\r\n\t\tself.tree.Bind(wx.EVT_CHAR, self.onTreeChar)\r\n\t\tself.tree.Bind(wx.EVT_TREE_BEGIN_LABEL_EDIT, self.onTreeLabelEditBegin)\r\n\t\tself.tree.Bind(wx.EVT_TREE_END_LABEL_EDIT, self.onTreeLabelEditEnd)\r\n\t\tself.treeRoot = self.tree.AddRoot(\"root\")\r\n\t\tcontentsSizer.Add(self.tree,flag=wx.EXPAND)\r\n\t\tcontentsSizer.AddSpacer(gui.guiHelper.SPACE_BETWEEN_VERTICAL_DIALOG_ITEMS)\r\n\r\n\t\t# Translators: The label of an editable text field to filter the elements\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\tfilterText = _(\"Filt&er by:\")\r\n\t\tlabeledCtrl = gui.guiHelper.LabeledControlHelper(self, filterText, wx.TextCtrl)\r\n\t\tself.filterEdit = labeledCtrl.control\r\n\t\tself.filterEdit.Bind(wx.EVT_TEXT, self.onFilterEditTextChange)\r\n\t\tcontentsSizer.Add(labeledCtrl.sizer)\r\n\t\tcontentsSizer.AddSpacer(gui.guiHelper.SPACE_BETWEEN_VERTICAL_DIALOG_ITEMS)\r\n\r\n\t\tbHelper = gui.guiHelper.ButtonHelper(wx.HORIZONTAL)\r\n\t\t# Translators: The label of a button to activate an element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\tself.activateButton = bHelper.addButton(self, label=_(\"&Activate\"))\r\n\t\tself.activateButton.Bind(wx.EVT_BUTTON, lambda evt: self.onAction(True))\r\n\t\t\r\n\t\t# Translators: The label of a button to move to an element\r\n\t\t# in the browse mode Elements List dialog.\r\n\t\tself.moveButton = bHelper.addButton(self, label=_(\"&Move to\"))\r\n\t\tself.moveButton.Bind(wx.EVT_BUTTON, lambda evt: self.onAction(False))\r\n\t\tbHelper.addButton(self, id=wx.ID_CANCEL)\r\n\r\n\t\tcontentsSizer.Add(bHelper.sizer, flag=wx.ALIGN_RIGHT)\r\n\r\n\t\tmainSizer.Add(contentsSizer, border=gui.guiHelper.BORDER_FOR_DIALOGS, flag=wx.ALL)\r\n\t\tmainSizer.Fit(self)\r\n\t\tself.SetSizer(mainSizer)\r\n\r\n\t\tself.tree.SetFocus()\r\n\t\tself.initElementType(self.ELEMENT_TYPES[self.lastSelectedElementType][0])\r\n\t\tself.Center(wx.BOTH | wx.CENTER_ON_SCREEN)\r\n\r\n\tdef onElementTypeChange(self, evt):\r\n\t\telementType=evt.GetInt()\r\n\t\t# We need to make sure this gets executed after the focus event.\r\n\t\t# Otherwise, NVDA doesn't seem to get the event.\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue, self.initElementType, self.ELEMENT_TYPES[elementType][0])\r\n\t\tself.lastSelectedElementType=elementType\r\n\r\n\tdef initElementType(self, elType):\r\n\t\tif elType in (\"link\",\"button\"):\r\n\t\t\t# Links and buttons can be activated.\r\n\t\t\tself.activateButton.Enable()\r\n\t\t\tself.SetAffirmativeId(self.activateButton.GetId())\r\n\t\telse:\r\n\t\t\t# No other element type can be activated.\r\n\t\t\tself.activateButton.Disable()\r\n\t\t\tself.SetAffirmativeId(self.moveButton.GetId())\r\n\r\n\t\t# Gather the elements of this type.\r\n\t\tself._elements = []\r\n\t\tself._initialElement = None\r\n\r\n\t\tparentElements = []\r\n\t\tisAfterSelection=False\r\n\t\tfor item in self.document._iterNodesByType(elType):\r\n\t\t\t# Find the parent element, if any.\r\n\t\t\tfor parent in reversed(parentElements):\r\n\t\t\t\tif item.isChild(parent.item):\r\n\t\t\t\t\tbreak\r\n\t\t\t\telse:\r\n\t\t\t\t\t# We're not a child of this parent, so this parent has no more children and can be removed from the stack.\r\n\t\t\t\t\tparentElements.pop()\r\n\t\t\telse:\r\n\t\t\t\t# No parent found, so we're at the root.\r\n\t\t\t\t# Note that parentElements will be empty at this point, as all parents are no longer relevant and have thus been removed from the stack.\r\n\t\t\t\tparent = None\r\n\r\n\t\t\telement=self.Element(item,parent)\r\n\t\t\tself._elements.append(element)\r\n\r\n\t\t\tif not isAfterSelection:\r\n\t\t\t\tisAfterSelection=item.isAfterSelection\r\n\t\t\t\tif not isAfterSelection:\r\n\t\t\t\t\t# The element immediately preceding or overlapping the caret should be the initially selected element.\r\n\t\t\t\t\t# Since we have not yet passed the selection, use this as the initial element. \r\n\t\t\t\t\ttry:\r\n\t\t\t\t\t\tself._initialElement = self._elements[-1]\r\n\t\t\t\t\texcept IndexError:\r\n\t\t\t\t\t\t# No previous element.\r\n\t\t\t\t\t\tpass\r\n\r\n\t\t\t# This could be the parent of a subsequent element, so add it to the parents stack.\r\n\t\t\tparentElements.append(element)\r\n\r\n\t\t# Start with no filtering.\r\n\t\tself.filterEdit.ChangeValue(\"\")\r\n\t\tself.filter(\"\", newElementType=True)\r\n\r\n\tdef filter(self, filterText, newElementType=False):\r\n\t\t# If this is a new element type, use the element nearest the cursor.\r\n\t\t# Otherwise, use the currently selected element.\r\n\t\tdefaultElement = self._initialElement if newElementType else self.tree.GetItemPyData(self.tree.GetSelection())\r\n\t\t# Clear the tree.\r\n\t\tself.tree.DeleteChildren(self.treeRoot)\r\n\r\n\t\t# Populate the tree with elements matching the filter text.\r\n\t\telementsToTreeItems = {}\r\n\t\tdefaultItem = None\r\n\t\tmatched = False\r\n\t\t#Do case-insensitive matching by lowering both filterText and each element's text.\r\n\t\tfilterText=filterText.lower()\r\n\t\tfor element in self._elements:\r\n\t\t\tlabel=element.item.label\r\n\t\t\tif filterText and filterText not in label.lower():\r\n\t\t\t\tcontinue\r\n\t\t\tmatched = True\r\n\t\t\tparent = element.parent\r\n\t\t\tif parent:\r\n\t\t\t\tparent = elementsToTreeItems.get(parent)\r\n\t\t\titem = self.tree.AppendItem(parent or self.treeRoot, label)\r\n\t\t\tself.tree.SetItemPyData(item, element)\r\n\t\t\telementsToTreeItems[element] = item\r\n\t\t\tif element == defaultElement:\r\n\t\t\t\tdefaultItem = item\r\n\r\n\t\tself.tree.ExpandAll()\r\n\r\n\t\tif not matched:\r\n\t\t\t# No items, so disable the buttons.\r\n\t\t\tself.activateButton.Disable()\r\n\t\t\tself.moveButton.Disable()\r\n\t\t\treturn\r\n\r\n\t\t# If there's no default item, use the first item in the tree.\r\n\t\tself.tree.SelectItem(defaultItem or self.tree.GetFirstChild(self.treeRoot)[0])\r\n\t\t# Enable the button(s).\r\n\t\t# If the activate button isn't the default button, it is disabled for this element type and shouldn't be enabled here.\r\n\t\tif self.AffirmativeId == self.activateButton.Id:\r\n\t\t\tself.activateButton.Enable()\r\n\t\tself.moveButton.Enable()\r\n\r\n\tdef onTreeSetFocus(self, evt):\r\n\t\t# Start with no search.\r\n\t\tself._searchText = \"\"\r\n\t\tself._searchCallLater = None\r\n\t\tevt.Skip()\r\n\r\n\tdef onTreeChar(self, evt):\r\n\t\tkey = evt.KeyCode\r\n\r\n\t\tif key == wx.WXK_RETURN:\r\n\t\t\t# The enter key should be propagated to the dialog and thus activate the default button,\r\n\t\t\t# but this is broken (wx ticket #3725).\r\n\t\t\t# Therefore, we must catch the enter key here.\r\n\t\t\t# Activate the current default button.\r\n\t\t\tevt = wx.CommandEvent(wx.wxEVT_COMMAND_BUTTON_CLICKED, wx.ID_ANY)\r\n\t\t\tbutton = self.FindWindowById(self.AffirmativeId)\r\n\t\t\tif button.Enabled:\r\n\t\t\t\tbutton.ProcessEvent(evt)\r\n\t\t\telse:\r\n\t\t\t\twx.Bell()\r\n\r\n\t\telif key == wx.WXK_F2:\r\n\t\t\titem=self.tree.GetSelection()\r\n\t\t\tif item:\r\n\t\t\t\tselectedItemType=self.tree.GetItemPyData(item).item\r\n\t\t\t\tself.tree.EditLabel(item)\r\n\t\t\t\tevt.Skip()\r\n\r\n\t\telif key >= wx.WXK_START or key == wx.WXK_BACK:\r\n\t\t\t# Non-printable character.\r\n\t\t\tself._searchText = \"\"\r\n\t\t\tevt.Skip()\r\n\r\n\t\telse:\r\n\t\t\t# Search the list.\r\n\t\t\t# We have to implement this ourselves, as tree views don't accept space as a search character.\r\n\t\t\tchar = unichr(evt.UnicodeKey).lower()\r\n\t\t\t# IF the same character is typed twice, do the same search.\r\n\t\t\tif self._searchText != char:\r\n\t\t\t\tself._searchText += char\r\n\t\t\tif self._searchCallLater:\r\n\t\t\t\tself._searchCallLater.Restart()\r\n\t\t\telse:\r\n\t\t\t\tself._searchCallLater = wx.CallLater(1000, self._clearSearchText)\r\n\t\t\tself.search(self._searchText)\r\n\r\n\tdef onTreeLabelEditBegin(self,evt):\r\n\t\titem=self.tree.GetSelection()\r\n\t\tselectedItemType = self.tree.GetItemPyData(item).item\r\n\t\tif not selectedItemType.isRenameAllowed:\r\n\t\t\tevt.Veto()\r\n\r\n\tdef onTreeLabelEditEnd(self,evt):\r\n\t\t\tselectedItemNewName=evt.GetLabel()\r\n\t\t\titem=self.tree.GetSelection()\r\n\t\t\tselectedItemType = self.tree.GetItemPyData(item).item\r\n\t\t\tselectedItemType.rename(selectedItemNewName)\r\n\r\n\tdef _clearSearchText(self):\r\n\t\tself._searchText = \"\"\r\n\r\n\tdef search(self, searchText):\r\n\t\titem = self.tree.GetSelection()\r\n\t\tif not item:\r\n\t\t\t# No items.\r\n\t\t\treturn\r\n\r\n\t\t# First try searching from the current item.\r\n\t\t# Failing that, search from the first item.\r\n\t\titems = itertools.chain(self._iterReachableTreeItemsFromItem(item), self._iterReachableTreeItemsFromItem(self.tree.GetFirstChild(self.treeRoot)[0]))\r\n\t\tif len(searchText) == 1:\r\n\t\t\t# If only a single character has been entered, skip (search after) the current item.\r\n\t\t\tnext(items)\r\n\r\n\t\tfor item in items:\r\n\t\t\tif self.tree.GetItemText(item).lower().startswith(searchText):\r\n\t\t\t\tself.tree.SelectItem(item)\r\n\t\t\t\treturn\r\n\r\n\t\t# Not found.\r\n\t\twx.Bell()\r\n\r\n\tdef _iterReachableTreeItemsFromItem(self, item):\r\n\t\twhile item:\r\n\t\t\tyield item\r\n\r\n\t\t\tchildItem = self.tree.GetFirstChild(item)[0]\r\n\t\t\tif childItem and self.tree.IsExpanded(item):\r\n\t\t\t\t# Has children and is reachable, so recurse.\r\n\t\t\t\tfor childItem in self._iterReachableTreeItemsFromItem(childItem):\r\n\t\t\t\t\tyield childItem\r\n\r\n\t\t\titem = self.tree.GetNextSibling(item)\r\n\r\n\tdef onFilterEditTextChange(self, evt):\r\n\t\tself.filter(self.filterEdit.GetValue())\r\n\t\tevt.Skip()\r\n\r\n\tdef onAction(self, activate):\r\n\t\tself.Close()\r\n\t\t# Save off the last selected element type on to the class so its used in initialization next time.\r\n\t\tself.__class__.lastSelectedElementType=self.lastSelectedElementType\r\n\t\titem = self.tree.GetSelection()\r\n\t\titem = self.tree.GetItemPyData(item).item\r\n\t\tif activate:\r\n\t\t\titem.activate()\r\n\t\telse:\r\n\t\t\tdef move():\r\n\t\t\t\tspeech.cancelSpeech()\r\n\t\t\t\titem.moveTo()\r\n\t\t\t\titem.report()\r\n\t\t\twx.CallLater(100, move)\r\n\r\nclass BrowseModeDocumentTextInfo(textInfos.TextInfo):\r\n\r\n\tdef getControlFieldSpeech(self, attrs, ancestorAttrs, fieldType, formatConfig=None, extraDetail=False, reason=None):\r\n\t\ttextList = []\r\n\t\tlandmark = attrs.get(\"landmark\")\r\n\t\tif formatConfig[\"reportLandmarks\"] and fieldType == \"start_addedToControlFieldStack\" and landmark:\r\n\t\t\ttry:\r\n\t\t\t\ttextList.append(attrs[\"name\"])\r\n\t\t\texcept KeyError:\r\n\t\t\t\tpass\r\n\t\t\tif landmark == \"region\":\r\n\t\t\t\t# The word landmark is superfluous for regions.\r\n\t\t\t\ttextList.append(aria.landmarkRoles[landmark])\r\n\t\t\telse:\r\n\t\t\t\ttextList.append(_(\"%s landmark\") % aria.landmarkRoles[landmark])\r\n\t\ttextList.append(super(BrowseModeDocumentTextInfo, self).getControlFieldSpeech(attrs, ancestorAttrs, fieldType, formatConfig, extraDetail, reason))\r\n\t\treturn \" \".join(textList)\r\n\r\n\tdef getControlFieldBraille(self, field, ancestors, reportStart, formatConfig):\r\n\t\ttextList = []\r\n\t\tlandmark = field.get(\"landmark\")\r\n\t\tif formatConfig[\"reportLandmarks\"] and reportStart and landmark and field.get(\"_startOfNode\"):\r\n\t\t\ttry:\r\n\t\t\t\ttextList.append(field[\"name\"])\r\n\t\t\texcept KeyError:\r\n\t\t\t\tpass\r\n\t\t\tif landmark == \"region\":\r\n\t\t\t\t# The word landmark is superfluous for regions.\r\n\t\t\t\ttextList.append(braille.landmarkLabels[landmark])\r\n\t\t\telse:\r\n\t\t\t\t# Translators: This is brailled to indicate a landmark (example output: lmk main).\r\n\t\t\t\ttextList.append(_(\"lmk %s\") % braille.landmarkLabels[landmark])\r\n\t\ttext = super(BrowseModeDocumentTextInfo, self).getControlFieldBraille(field, ancestors, reportStart, formatConfig)\r\n\t\tif text:\r\n\t\t\ttextList.append(text)\r\n\t\treturn \" \".join(textList)\r\n\r\n\tdef _get_focusableNVDAObjectAtStart(self):\r\n\t\ttry:\r\n\t\t\titem = next(self.obj._iterNodesByType(\"focusable\", \"up\", self))\r\n\t\texcept StopIteration:\r\n\t\t\treturn self.obj.rootNVDAObject\r\n\t\tif not item:\r\n\t\t\treturn self.obj.rootNVDAObject\r\n\t\treturn item.obj\r\n\r\nclass BrowseModeDocumentTreeInterceptor(documentBase.DocumentWithTableNavigation,cursorManager.CursorManager,BrowseModeTreeInterceptor,treeInterceptorHandler.DocumentTreeInterceptor):\r\n\r\n\tprogrammaticScrollMayFireEvent = False\r\n\r\n\tdef __init__(self,obj):\r\n\t\tsuper(BrowseModeDocumentTreeInterceptor,self).__init__(obj)\r\n\t\tself._lastProgrammaticScrollTime = None\r\n\t\tself.documentConstantIdentifier = self.documentConstantIdentifier\r\n\t\tself._lastFocusObj = None\r\n\t\tself._hadFirstGainFocus = False\r\n\t\tself._enteringFromOutside = True\r\n\t\t# We need to cache this because it will be unavailable once the document dies.\r\n\t\tif not hasattr(self.rootNVDAObject.appModule, \"_browseModeRememberedCaretPositions\"):\r\n\t\t\tself.rootNVDAObject.appModule._browseModeRememberedCaretPositions = {}\r\n\t\tself._lastCaretPosition = None\r\n\t\t#: True if the last caret move was due to a focus change.\r\n\t\tself._lastCaretMoveWasFocus = False\r\n\r\n\tdef terminate(self):\r\n\t\tif self.shouldRememberCaretPositionAcrossLoads and self._lastCaretPosition:\r\n\t\t\ttry:\r\n\t\t\t\tself.rootNVDAObject.appModule._browseModeRememberedCaretPositions[self.documentConstantIdentifier] = self._lastCaretPosition\r\n\t\t\texcept AttributeError:\r\n\t\t\t\t# The app module died.\r\n\t\t\t\tpass\r\n\r\n\tdef _get_currentNVDAObject(self):\r\n\t\treturn self.makeTextInfo(textInfos.POSITION_CARET).NVDAObjectAtStart\r\n\r\n\tdef event_treeInterceptor_gainFocus(self):\r\n\t\t\"\"\"Triggered when this browse mode document gains focus.\r\n\t\tThis event is only fired upon entering this treeInterceptor when it was not the current treeInterceptor before.\r\n\t\tThis is different to L{event_gainFocus}, which is fired when an object inside this treeInterceptor gains focus, even if that object is in the same treeInterceptor.\r\n\t\t\"\"\"\r\n\t\tdoSayAll=False\r\n\t\thadFirstGainFocus=self._hadFirstGainFocus\r\n\t\tif not hadFirstGainFocus:\r\n\t\t\t# This treeInterceptor is gaining focus for the first time.\r\n\t\t\t# Fake a focus event on the focus object, as the treeInterceptor may have missed the actual focus event.\r\n\t\t\tfocus = api.getFocusObject()\r\n\t\t\tself.event_gainFocus(focus, lambda: focus.event_gainFocus())\r\n\t\t\tif not self.passThrough:\r\n\t\t\t\t# We only set the caret position if in browse mode.\r\n\t\t\t\t# If in focus mode, the document must have forced the focus somewhere,\r\n\t\t\t\t# so we don't want to override it.\r\n\t\t\t\tinitialPos = self._getInitialCaretPos()\r\n\t\t\t\tif initialPos:\r\n\t\t\t\t\tself.selection = self.makeTextInfo(initialPos)\r\n\t\t\t\treportPassThrough(self)\r\n\t\t\t\tdoSayAll=config.conf['virtualBuffers']['autoSayAllOnPageLoad']\r\n\t\t\tself._hadFirstGainFocus = True\r\n\r\n\t\tif not self.passThrough:\r\n\t\t\tif doSayAll:\r\n\t\t\t\tspeech.speakObjectProperties(self.rootNVDAObject,name=True,states=True,reason=controlTypes.REASON_FOCUS)\r\n\t\t\t\tsayAllHandler.readText(sayAllHandler.CURSOR_CARET)\r\n\t\t\telse:\r\n\t\t\t\t# Speak it like we would speak focus on any other document object.\r\n\t\t\t\t# This includes when entering the treeInterceptor for the first time:\r\n\t\t\t\tif not hadFirstGainFocus:\r\n\t\t\t\t\tspeech.speakObject(self.rootNVDAObject, reason=controlTypes.REASON_FOCUS)\r\n\t\t\t\telse:\r\n\t\t\t\t\t# And when coming in from an outside object\r\n\t\t\t\t\t# #4069 But not when coming up from a non-rendered descendant.\r\n\t\t\t\t\tancestors=api.getFocusAncestors()\r\n\t\t\t\t\tfdl=api.getFocusDifferenceLevel()\r\n\t\t\t\t\ttry:\r\n\t\t\t\t\t\ttl=ancestors.index(self.rootNVDAObject)\r\n\t\t\t\t\texcept ValueError:\r\n\t\t\t\t\t\ttl=len(ancestors)\r\n\t\t\t\t\tif fdl<=tl:\r\n\t\t\t\t\t\tspeech.speakObject(self.rootNVDAObject, reason=controlTypes.REASON_FOCUS)\r\n\t\t\t\tinfo = self.selection\r\n\t\t\t\tif not info.isCollapsed:\r\n\t\t\t\t\tspeech.speakSelectionMessage(_(\"selected %s\"), info.text)\r\n\t\t\t\telse:\r\n\t\t\t\t\tinfo.expand(textInfos.UNIT_LINE)\r\n\t\t\t\t\tspeech.speakTextInfo(info, reason=controlTypes.REASON_CARET, unit=textInfos.UNIT_LINE)\r\n\r\n\t\treportPassThrough(self)\r\n\t\tbraille.handler.handleGainFocus(self)\r\n\r\n\tdef event_caret(self, obj, nextHandler):\r\n\t\tif self.passThrough:\r\n\t\t\tnextHandler()\r\n\r\n\tdef _activateLongDesc(self,controlField):\r\n\t\t\"\"\"\r\n\t\tActivates (presents) the long description for a particular field (usually a graphic).\r\n\t\t@param controlField: the field who's long description should be activated. This field is guaranteed to have states containing HASLONGDESC state. \r\n\t\t@type controlField: dict\r\n\t\t\"\"\"\r\n\t\traise NotImplementedError\r\n\r\n\tdef _activatePosition(self, info=None):\r\n\t\tobj=None\r\n\t\tif info:\r\n\t\t\tobj=info.NVDAObjectAtStart\r\n\t\t\tif not obj:\r\n\t\t\t\treturn\r\n\t\tsuper(BrowseModeDocumentTreeInterceptor,self)._activatePosition(obj)\r\n\r\n\tdef _set_selection(self, info, reason=controlTypes.REASON_CARET):\r\n\t\tsuper(BrowseModeDocumentTreeInterceptor, self)._set_selection(info)\r\n\t\tif isScriptWaiting() or not info.isCollapsed:\r\n\t\t\treturn\r\n\t\t# Save the last caret position for use in terminate().\r\n\t\t# This must be done here because the buffer might be cleared just before terminate() is called,\r\n\t\t# causing the last caret position to be lost.\r\n\t\tcaret = info.copy()\r\n\t\tcaret.collapse()\r\n\t\tself._lastCaretPosition = caret.bookmark\r\n\t\treview.handleCaretMove(caret)\r\n\t\tif reason == controlTypes.REASON_FOCUS:\r\n\t\t\tself._lastCaretMoveWasFocus = True\r\n\t\t\tfocusObj = api.getFocusObject()\r\n\t\t\tif focusObj==self.rootNVDAObject:\r\n\t\t\t\treturn\r\n\t\telse:\r\n\t\t\tself._lastCaretMoveWasFocus = False\r\n\t\t\tfocusObj=info.focusableNVDAObjectAtStart\r\n\t\t\tobj=info.NVDAObjectAtStart\r\n\t\t\tif not obj:\r\n\t\t\t\tlog.debugWarning(\"Invalid NVDAObjectAtStart\")\r\n\t\t\t\treturn\r\n\t\t\tif obj==self.rootNVDAObject:\r\n\t\t\t\treturn\r\n\t\t\tif focusObj and not eventHandler.isPendingEvents(\"gainFocus\") and focusObj!=self.rootNVDAObject and focusObj != api.getFocusObject() and self._shouldSetFocusToObj(focusObj):\r\n\t\t\t\tfocusObj.setFocus()\r\n\t\t\tobj.scrollIntoView()\r\n\t\t\tif self.programmaticScrollMayFireEvent:\r\n\t\t\t\tself._lastProgrammaticScrollTime = time.time()\r\n\t\tself.passThrough=self.shouldPassThrough(focusObj,reason=reason)\r\n\t\t# Queue the reporting of pass through mode so that it will be spoken after the actual content.\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue, reportPassThrough, self)\r\n\r\n\tdef _shouldSetFocusToObj(self, obj):\r\n\t\t\"\"\"Determine whether an object should receive focus.\r\n\t\tSubclasses may extend or override this method.\r\n\t\t@param obj: The object in question.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t\"\"\"\r\n\t\treturn obj.role not in self.APPLICATION_ROLES and obj.isFocusable and obj.role!=controlTypes.ROLE_EMBEDDEDOBJECT\r\n\r\n\tdef script_activateLongDesc(self,gesture):\r\n\t\tinfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\tinfo.expand(\"character\")\r\n\t\tfor field in reversed(info.getTextWithFields()):\r\n\t\t\tif isinstance(field,textInfos.FieldCommand) and field.command==\"controlStart\":\r\n\t\t\t\tstates=field.field.get('states')\r\n\t\t\t\tif states and controlTypes.STATE_HASLONGDESC in states:\r\n\t\t\t\t\tself._activateLongDesc(field.field)\r\n\t\t\t\t\tbreak\r\n\t\telse:\r\n\t\t\t# Translators: the message presented when the activateLongDescription script cannot locate a long description to activate.\r\n\t\t\tui.message(_(\"No long description\"))\r\n\t# Translators: the description for the activateLongDescription script on browseMode documents.\r\n\tscript_activateLongDesc.__doc__=_(\"Shows the long description at this position if one is found.\")\r\n\r\n\tdef event_caretMovementFailed(self, obj, nextHandler, gesture=None):\r\n\t\tif not self.passThrough or not gesture or not config.conf[\"virtualBuffers\"][\"autoPassThroughOnCaretMove\"]:\r\n\t\t\treturn nextHandler()\r\n\t\tif gesture.mainKeyName in (\"home\", \"end\"):\r\n\t\t\t# Home, end, control+home and control+end should not disable pass through.\r\n\t\t\treturn nextHandler()\r\n\t\tscript = self.getScript(gesture)\r\n\t\tif not script:\r\n\t\t\treturn nextHandler()\r\n\r\n\t\t# We've hit the edge of the focused control.\r\n\t\t# Therefore, move the virtual caret to the same edge of the field.\r\n\t\tinfo = self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\tinfo.expand(info.UNIT_CONTROLFIELD)\r\n\t\tif gesture.mainKeyName in (\"leftArrow\", \"upArrow\", \"pageUp\"):\r\n\t\t\tinfo.collapse()\r\n\t\telse:\r\n\t\t\tinfo.collapse(end=True)\r\n\t\t\tinfo.move(textInfos.UNIT_CHARACTER, -1)\r\n\t\tinfo.updateCaret()\r\n\r\n\t\tscriptHandler.queueScript(script, gesture)\r\n\r\n\tdef script_collapseOrExpandControl(self, gesture):\r\n\t\toldFocus = api.getFocusObject()\r\n\t\toldFocusStates = oldFocus.states\r\n\t\tgesture.send()\r\n\t\tif controlTypes.STATE_COLLAPSED in oldFocusStates:\r\n\t\t\tself.passThrough = True\r\n\t\telif not self.disableAutoPassThrough:\r\n\t\t\tself.passThrough = False\r\n\t\treportPassThrough(self)\r\n\tscript_collapseOrExpandControl.ignoreTreeInterceptorPassThrough = True\r\n\r\n\tdef _tabOverride(self, direction):\r\n\t\t\"\"\"Override the tab order if the virtual  caret is not within the currently focused node.\r\n\t\tThis is done because many nodes are not focusable and it is thus possible for the virtual caret to be unsynchronised with the focus.\r\n\t\tIn this case, we want tab/shift+tab to move to the next/previous focusable node relative to the virtual caret.\r\n\t\tIf the virtual caret is within the focused node, the tab/shift+tab key should be passed through to allow normal tab order navigation.\r\n\t\tNote that this method does not pass the key through itself if it is not overridden. This should be done by the calling script if C{False} is returned.\r\n\t\t@param direction: The direction in which to move.\r\n\t\t@type direction: str\r\n\t\t@return: C{True} if the tab order was overridden, C{False} if not.\r\n\t\t@rtype: bool\r\n\t\t\"\"\"\r\n\t\tif self._lastCaretMoveWasFocus:\r\n\t\t\t# #5227: If the caret was last moved due to a focus change, don't override tab.\r\n\t\t\t# This ensures that tabbing behaves as expected after tabbing hits an iframe document.\r\n\t\t\treturn False\r\n\t\tfocus = api.getFocusObject()\r\n\t\ttry:\r\n\t\t\tfocusInfo = self.makeTextInfo(focus)\r\n\t\texcept:\r\n\t\t\treturn False\r\n\t\t# We only want to override the tab order if the caret is not within the focused node.\r\n\t\tcaretInfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\t#Only check that the caret is within the focus for things that ar not documents\r\n\t\t#As for documents we should always override\r\n\t\tif focus.role!=controlTypes.ROLE_DOCUMENT or controlTypes.STATE_EDITABLE in focus.states:\r\n\t\t\t# Expand to one character, as isOverlapping() doesn't yield the desired results with collapsed ranges.\r\n\t\t\tcaretInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\t\tif focusInfo.isOverlapping(caretInfo):\r\n\t\t\t\treturn False\r\n\t\t# If we reach here, we do want to override tab/shift+tab if possible.\r\n\t\t# Find the next/previous focusable node.\r\n\t\ttry:\r\n\t\t\titem = next(self._iterNodesByType(\"focusable\", direction, caretInfo))\r\n\t\texcept StopIteration:\r\n\t\t\treturn False\r\n\t\tobj=item.obj\r\n\t\tnewInfo=item.textInfo\r\n\t\tif obj == api.getFocusObject():\r\n\t\t\t# This node is already focused, so we need to move to and speak this node here.\r\n\t\t\tnewCaret = newInfo.copy()\r\n\t\t\tnewCaret.collapse()\r\n\t\t\tself._set_selection(newCaret,reason=controlTypes.REASON_FOCUS)\r\n\t\t\tif self.passThrough:\r\n\t\t\t\tobj.event_gainFocus()\r\n\t\t\telse:\r\n\t\t\t\tspeech.speakTextInfo(newInfo,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# This node doesn't have the focus, so just set focus to it. The gainFocus event will handle the rest.\r\n\t\t\tobj.setFocus()\r\n\t\treturn True\r\n\r\n\tdef script_tab(self, gesture):\r\n\t\tif not self._tabOverride(\"next\"):\r\n\t\t\tgesture.send()\r\n\r\n\tdef script_shiftTab(self, gesture):\r\n\t\tif not self._tabOverride(\"previous\"):\r\n\t\t\tgesture.send()\r\n\r\n\tdef event_focusEntered(self,obj,nextHandler):\r\n\t\tif obj==self.rootNVDAObject:\r\n\t\t\tself._enteringFromOutside = True\r\n\t\t# Even if passThrough is enabled, we still completely drop focusEntered events here. \r\n\t\t# In order to get them back when passThrough is enabled, we replay them with the _replayFocusEnteredEvents method in event_gainFocus.\r\n\t\t# The reason for this is to ensure that focusEntered events are delayed until a focus event has had a chance to disable passthrough mode.\r\n\t\t# As in this case we would  not want them.\r\n\r\n\tdef _shouldIgnoreFocus(self, obj):\r\n\t\t\"\"\"Determines whether focus on a given object should be ignored.\r\n\t\t@param obj: The object in question.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t@return: C{True} if focus on L{obj} should be ignored, C{False} otherwise.\r\n\t\t@rtype: bool\r\n\t\t\"\"\"\r\n\t\treturn False\r\n\r\n\tdef _postGainFocus(self, obj):\r\n\t\t\"\"\"Executed after a gainFocus within the browseMode document.\r\n\t\tThis will not be executed if L{event_gainFocus} determined that it should abort and call nextHandler.\r\n\t\t@param obj: The object that gained focus.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t\"\"\"\r\n\r\n\tdef _replayFocusEnteredEvents(self):\r\n\t\t# We blocked the focusEntered events because we were in browse mode,\r\n\t\t# but now that we've switched to focus mode, we need to fire them.\r\n\t\tfor parent in api.getFocusAncestors()[api.getFocusDifferenceLevel():]:\r\n\t\t\ttry:\r\n\t\t\t\tparent.event_focusEntered()\r\n\t\t\texcept:\r\n\t\t\t\tlog.exception(\"Error executing focusEntered event: %s\" % parent)\r\n\r\n\tdef event_gainFocus(self, obj, nextHandler):\r\n\t\tenteringFromOutside=self._enteringFromOutside\r\n\t\tself._enteringFromOutside=False\r\n\t\tif not self.isReady:\r\n\t\t\tif self.passThrough:\r\n\t\t\t\tself._replayFocusEnteredEvents()\r\n\t\t\t\tnextHandler()\r\n\t\t\treturn\r\n\t\tif enteringFromOutside and not self.passThrough and self._lastFocusObj==obj:\r\n\t\t\t# We're entering the document from outside (not returning from an inside object/application; #3145)\r\n\t\t\t# and this was the last non-root node with focus, so ignore this focus event.\r\n\t\t\t# Otherwise, if the user switches away and back to this document, the cursor will jump to this node.\r\n\t\t\t# This is not ideal if the user was positioned over a node which cannot receive focus.\r\n\t\t\treturn\r\n\t\tif obj==self.rootNVDAObject:\r\n\t\t\tif self.passThrough:\r\n\t\t\t\tself._replayFocusEnteredEvents()\r\n\t\t\t\treturn nextHandler()\r\n\t\t\treturn \r\n\t\tif not self.passThrough and self._shouldIgnoreFocus(obj):\r\n\t\t\treturn\r\n\t\tself._lastFocusObj=obj\r\n\r\n\t\ttry:\r\n\t\t\tfocusInfo = self.makeTextInfo(obj)\r\n\t\texcept:\r\n\t\t\t# This object is not in the treeInterceptor, even though it resides beneath the document.\r\n\t\t\t# Automatic pass through should be enabled in certain circumstances where this occurs.\r\n\t\t\tif not self.passThrough and self.shouldPassThrough(obj,reason=controlTypes.REASON_FOCUS):\r\n\t\t\t\tself.passThrough=True\r\n\t\t\t\treportPassThrough(self)\r\n\t\t\t\tself._replayFocusEnteredEvents()\r\n\t\t\treturn nextHandler()\r\n\r\n\t\t#We only want to update the caret and speak the field if we're not in the same one as before\r\n\t\tcaretInfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\t# Expand to one character, as isOverlapping() doesn't treat, for example, (4,4) and (4,5) as overlapping.\r\n\t\tcaretInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tif not self._hadFirstGainFocus or not focusInfo.isOverlapping(caretInfo):\r\n\t\t\t# The virtual caret is not within the focus node.\r\n\t\t\toldPassThrough=self.passThrough\r\n\t\t\tpassThrough=self.shouldPassThrough(obj,reason=controlTypes.REASON_FOCUS)\r\n\t\t\tif not oldPassThrough and (passThrough or sayAllHandler.isRunning()):\r\n\t\t\t\t# If pass-through is disabled, cancel speech, as a focus change should cause page reading to stop.\r\n\t\t\t\t# This must be done before auto-pass-through occurs, as we want to stop page reading even if pass-through will be automatically enabled by this focus change.\r\n\t\t\t\tspeech.cancelSpeech()\r\n\t\t\tself.passThrough=passThrough\r\n\t\t\tif not self.passThrough:\r\n\t\t\t\t# We read the info from the browseMode document  instead of the control itself.\r\n\t\t\t\tspeech.speakTextInfo(focusInfo,reason=controlTypes.REASON_FOCUS)\r\n\t\t\t\t# However, we still want to update the speech property cache so that property changes will be spoken properly.\r\n\t\t\t\tspeech.speakObject(obj,controlTypes.REASON_ONLYCACHE)\r\n\t\t\telse:\r\n\t\t\t\t# Although we are going to speak the object rather than textInfo content, we still need to silently speak the textInfo content so that the textInfo speech cache is updated correctly.\r\n\t\t\t\t# Not doing this would cause  later browseMode speaking to either not speak controlFields it had entered, or speak controlField exits after having already exited.\r\n\t\t\t\t# See #7435 for a discussion on this.\r\n\t\t\t\tspeech.speakTextInfo(focusInfo,reason=controlTypes.REASON_ONLYCACHE)\r\n\t\t\t\tself._replayFocusEnteredEvents()\r\n\t\t\t\tnextHandler()\r\n\t\t\tfocusInfo.collapse()\r\n\t\t\tself._set_selection(focusInfo,reason=controlTypes.REASON_FOCUS)\r\n\t\telse:\r\n\t\t\t# The virtual caret was already at the focused node.\r\n\t\t\tif not self.passThrough:\r\n\t\t\t\t# This focus change was caused by a virtual caret movement, so don't speak the focused node to avoid double speaking.\r\n\t\t\t\t# However, we still want to update the speech property cache so that property changes will be spoken properly.\r\n\t\t\t\tspeech.speakObject(obj,controlTypes.REASON_ONLYCACHE)\r\n\t\t\telse:\r\n\t\t\t\tself._replayFocusEnteredEvents()\r\n\t\t\t\treturn nextHandler()\r\n\r\n\t\tself._postGainFocus(obj)\r\n\r\n\tevent_gainFocus.ignoreIsReady=True\r\n\r\n\tdef _handleScrollTo(self, obj):\r\n\t\t\"\"\"Handle scrolling the browseMode document to a given object in response to an event.\r\n\t\tSubclasses should call this from an event which indicates that the document has scrolled.\r\n\t\t@postcondition: The virtual caret is moved to L{obj} and the buffer content for L{obj} is reported.\r\n\t\t@param obj: The object to which the document should scroll.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t@return: C{True} if the document was scrolled, C{False} if not.\r\n\t\t@rtype: bool\r\n\t\t@note: If C{False} is returned, calling events should probably call their nextHandler.\r\n\t\t\"\"\"\r\n\t\tif self.programmaticScrollMayFireEvent and self._lastProgrammaticScrollTime and time.time() - self._lastProgrammaticScrollTime < 0.4:\r\n\t\t\t# This event was probably caused by this browseMode document's call to scrollIntoView().\r\n\t\t\t# Therefore, ignore it. Otherwise, the cursor may bounce back to the scroll point.\r\n\t\t\t# However, pretend we handled it, as we don't want it to be passed on to the object either.\r\n\t\t\treturn True\r\n\r\n\t\ttry:\r\n\t\t\tscrollInfo = self.makeTextInfo(obj)\r\n\t\texcept:\r\n\t\t\treturn False\r\n\r\n\t\t#We only want to update the caret and speak the field if we're not in the same one as before\r\n\t\tcaretInfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\t# Expand to one character, as isOverlapping() doesn't treat, for example, (4,4) and (4,5) as overlapping.\r\n\t\tcaretInfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tif not scrollInfo.isOverlapping(caretInfo):\r\n\t\t\tif scrollInfo.isCollapsed:\r\n\t\t\t\tscrollInfo.expand(textInfos.UNIT_LINE)\r\n\t\t\tspeech.speakTextInfo(scrollInfo,reason=controlTypes.REASON_CARET)\r\n\t\t\tscrollInfo.collapse()\r\n\t\t\tself.selection = scrollInfo\r\n\t\t\treturn True\r\n\r\n\t\treturn False\r\n\r\n\tdef _isNVDAObjectInApplication(self, obj):\r\n\t\t\"\"\"Determine whether a given object is within an application.\r\n\t\tThe object is considered to be within an application if it or one of its ancestors has an application role.\r\n\t\tThis should only be called on objects beneath the treeInterceptor's root NVDAObject.\r\n\t\t@param obj: The object in question.\r\n\t\t@type obj: L{NVDAObjects.NVDAObject}\r\n\t\t@return: C{True} if L{obj} is within an application, C{False} otherwise.\r\n\t\t@rtype: bool\r\n\t\t\"\"\"\r\n\t\t# We cache the result for each object we walk.\r\n\t\t# There can be browse mode documents within other documents and the result might be different between these,\r\n\t\t# so the cache must be maintained on the TreeInterceptor rather than the object itself.\r\n\t\ttry:\r\n\t\t\tcache = self._isInAppCache\r\n\t\texcept AttributeError:\r\n\t\t\t# Create this lazily, as this method isn't used by all browse mode implementations.\r\n\t\t\tcache = self._isInAppCache = weakref.WeakKeyDictionary()\r\n\t\tobjs = []\r\n\t\tdef doResult(result):\r\n\t\t\t# Cache this on descendants we've walked over.\r\n\t\t\tfor obj in objs:\r\n\t\t\t\tcache[obj] = result\r\n\t\t\treturn result\r\n\r\n\t\twhile obj and obj != self.rootNVDAObject:\r\n\t\t\tinApp = cache.get(obj)\r\n\t\t\tif inApp is not None:\r\n\t\t\t\t# We found a cached result.\r\n\t\t\t\treturn doResult(inApp)\r\n\t\t\tobjs.append(obj)\r\n\t\t\tif obj.role in self.APPLICATION_ROLES:\r\n\t\t\t\treturn doResult(True)\r\n\t\t\t# Cache container.\r\n\t\t\tcontainer = obj.container\r\n\t\t\tobj.container = container\r\n\t\t\tobj = container\r\n\t\treturn doResult(False)\r\n\r\n\tdef _get_documentConstantIdentifier(self):\r\n\t\t\"\"\"Get the constant identifier for this document.\r\n\t\tThis identifier should uniquely identify all instances (not just one instance) of a document for at least the current session of the hosting application.\r\n\t\tGenerally, the document URL should be used.\r\n\t\t@return: The constant identifier for this document, C{None} if there is none.\r\n\t\t\"\"\"\r\n\t\treturn None\r\n\r\n\tdef _get_shouldRememberCaretPositionAcrossLoads(self):\r\n\t\t\"\"\"Specifies whether the position of the caret should be remembered when this document is loaded again.\r\n\t\tThis is useful when the browser remembers the scroll position for the document,\r\n\t\tbut does not communicate this information via APIs.\r\n\t\tThe remembered caret position is associated with this document using L{documentConstantIdentifier}.\r\n\t\t@return: C{True} if the caret position should be remembered, C{False} if not.\r\n\t\t@rtype: bool\r\n\t\t\"\"\"\r\n\t\tdocConstId = self.documentConstantIdentifier\r\n\t\t# Return True if the URL indicates that this is probably a web browser document.\r\n\t\t# We do this check because we don't want to remember caret positions for email messages, etc.\r\n\t\treturn isinstance(docConstId, basestring) and docConstId.split(\"://\", 1)[0] in (\"http\", \"https\", \"ftp\", \"ftps\", \"file\")\r\n\r\n\tdef _getInitialCaretPos(self):\r\n\t\t\"\"\"Retrieve the initial position of the caret after the buffer has been loaded.\r\n\t\tThis position, if any, will be passed to L{makeTextInfo}.\r\n\t\tSubclasses should extend this method.\r\n\t\t@return: The initial position of the caret, C{None} if there isn't one.\r\n\t\t@rtype: TextInfo position\r\n\t\t\"\"\"\r\n\t\tif self.shouldRememberCaretPositionAcrossLoads:\r\n\t\t\ttry:\r\n\t\t\t\treturn self.rootNVDAObject.appModule._browseModeRememberedCaretPositions[self.documentConstantIdentifier]\r\n\t\t\texcept KeyError:\r\n\t\t\t\tpass\r\n\t\treturn None\r\n\r\n\tdef getEnclosingContainerRange(self,range):\r\n\t\trange=range.copy()\r\n\t\trange.collapse()\r\n\t\ttry:\r\n\t\t\titem = next(self._iterNodesByType(\"container\", \"up\", range))\r\n\t\texcept (NotImplementedError,StopIteration):\r\n\t\t\ttry:\r\n\t\t\t\titem = next(self._iterNodesByType(\"landmark\", \"up\", range))\r\n\t\t\texcept (NotImplementedError,StopIteration):\r\n\t\t\t\treturn\r\n\t\treturn item.textInfo\r\n\r\n\tdef script_moveToStartOfContainer(self,gesture):\r\n\t\tinfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tcontainer=self.getEnclosingContainerRange(info)\r\n\t\tif not container:\r\n\t\t\t# Translators: Reported when the user attempts to move to the start or end of a container\r\n\t\t\t# (list, table, etc.) but there is no container. \r\n\t\t\tui.message(_(\"Not in a container\"))\r\n\t\t\treturn\r\n\t\tcontainer.collapse()\r\n\t\tself._set_selection(container, reason=REASON_QUICKNAV)\r\n\t\tif not willSayAllResume(gesture):\r\n\t\t\tcontainer.expand(textInfos.UNIT_LINE)\r\n\t\t\tspeech.speakTextInfo(container, reason=controlTypes.REASON_FOCUS)\r\n\tscript_moveToStartOfContainer.resumeSayAllMode=sayAllHandler.CURSOR_CARET\r\n\t# Translators: Description for the Move to start of container command in browse mode. \r\n\tscript_moveToStartOfContainer.__doc__=_(\"Moves to the start of the container element, such as a list or table\")\r\n\r\n\tdef script_movePastEndOfContainer(self,gesture):\r\n\t\tinfo=self.makeTextInfo(textInfos.POSITION_CARET)\r\n\t\tinfo.expand(textInfos.UNIT_CHARACTER)\r\n\t\tcontainer=self.getEnclosingContainerRange(info)\r\n\t\tif not container:\r\n\t\t\t# Translators: Reported when the user attempts to move to the start or end of a container\r\n\t\t\t# (list, table, etc.) but there is no container. \r\n\t\t\tui.message(_(\"Not in a container\"))\r\n\t\t\treturn\r\n\t\tcontainer.collapse(end=True)\r\n\t\tdocEnd=container.obj.makeTextInfo(textInfos.POSITION_LAST)\r\n\t\tif container.compareEndPoints(docEnd,\"endToEnd\")>=0:\r\n\t\t\tcontainer=docEnd\r\n\t\t\t# Translators: a message reported when:\r\n\t\t\t# Review cursor is at the bottom line of the current navigator object.\r\n\t\t\t# Landing at the end of a browse mode document when trying to jump to the end of the current container. \r\n\t\t\tui.message(_(\"Bottom\"))\r\n\t\tself._set_selection(container, reason=REASON_QUICKNAV)\r\n\t\tif not willSayAllResume(gesture):\r\n\t\t\tcontainer.expand(textInfos.UNIT_LINE)\r\n\t\t\tspeech.speakTextInfo(container, reason=controlTypes.REASON_FOCUS)\r\n\tscript_movePastEndOfContainer.resumeSayAllMode=sayAllHandler.CURSOR_CARET\r\n\t# Translators: Description for the Move past end of container command in browse mode. \r\n\tscript_movePastEndOfContainer.__doc__=_(\"Moves past the end  of the container element, such as a list or table\")\r\n\r\n\tNOT_LINK_BLOCK_MIN_LEN = 30\r\n\tdef _isSuitableNotLinkBlock(self,range):\r\n\t\treturn len(range.text)>=self.NOT_LINK_BLOCK_MIN_LEN\r\n\r\n\tdef _iterNotLinkBlock(self, direction=\"next\", pos=None):\r\n\t\tlinks = self._iterNodesByType(\"link\", direction=direction, pos=pos)\r\n\t\t# We want to compare each link against the next link.\r\n\t\titem1 = next(links)\r\n\t\twhile True:\r\n\t\t\titem2 = next(links)\r\n\t\t\t# If the distance between the links is small, this is probably just a piece of non-link text within a block of links; e.g. an inactive link of a nav bar.\r\n\t\t\tif direction==\"previous\":\r\n\t\t\t\trange=item1.textInfo.copy()\r\n\t\t\t\trange.collapse()\r\n\t\t\t\trange.setEndPoint(item2.textInfo,\"startToEnd\")\r\n\t\t\telse:\r\n\t\t\t\trange=item2.textInfo.copy()\r\n\t\t\t\trange.collapse()\r\n\t\t\t\trange.setEndPoint(item1.textInfo,\"startToEnd\")\r\n\t\t\tif self._isSuitableNotLinkBlock(range):\r\n\t\t\t\tyield TextInfoQuickNavItem(\"notLinkBlock\",self,range)\r\n\t\t\titem1=item2\r\n\r\n\t__gestures={\r\n\t\t\"kb:NVDA+d\": \"activateLongDesc\",\r\n\t\t\"kb:alt+upArrow\": \"collapseOrExpandControl\",\r\n\t\t\"kb:alt+downArrow\": \"collapseOrExpandControl\",\r\n\t\t\"kb:tab\": \"tab\",\r\n\t\t\"kb:shift+tab\": \"shiftTab\",\r\n\t\t\"kb:shift+,\": \"moveToStartOfContainer\",\r\n\t\t\"kb:,\": \"movePastEndOfContainer\",\r\n\t}\r\n", "idx": 2, "id": 22046, "msg": "", "proj": "nvaccess-nvda", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.solr.cluster.placement;\n+\n+import java.util.Set;\n+\n+/**\n+ * Delete shards request.\n+ */\n+public interface DeleteShardsRequest extends ModificationRequest {\n+  Set<String> getShardNames();\n+}", "y": 1, "oldf": "", "idx": 1, "id": 39380, "msg": "If we don't use this interface (i.e. the class that implements it) I suggest we do not include either in this PR. Or at least define and call the corresponding method in `AssignStrategy` from the appropriate `*Cmd` even if nothing does a real implementation and vetting based on it (but it would be ready to be consumed maybe by another plugin written by some user).", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -5527,8 +5527,8 @@ const instr_info_t e_vex_extensions[][3] = {\n     {INVALID, 0x0fae33, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   }, { /* e_vex ext 63 */\n     {INVALID,   0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n-    {OP_vcvtph2ps, 0x66381318, \"vcvtph2ps\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n-    {INVALID, 0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n+    {OP_vcvtph2ps, 0x66381318, \"vcvtph2ps\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, tvex[63][2]},\n+    {OP_vcvtph2ps, 0x66381318, \"vcvtph2ps\", Ve, xx, KEw, We, xx, mrm|evex|reqp, x, END_LIST},\n   }, { /* e_vex ext 64 */\n     {INVALID,   0x66381818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n     {OP_vbroadcastss, 0x66381818, \"vbroadcastss\", Vx, xx, Wd_dq, xx, xx, mrm|vex|reqp, x, END_LIST},", "y": 0, "oldf": "/* **********************************************************\n * Copyright (c) 2011-2019 Google, Inc.  All rights reserved.\n * Copyright (c) 2001-2010 VMware, Inc.  All rights reserved.\n * **********************************************************/\n\n/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and/or other materials provided with the distribution.\n *\n * * Neither the name of VMware, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL VMWARE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/* Copyright (c) 2003-2007 Determina Corp. */\n/* Copyright (c) 2001-2003 Massachusetts Institute of Technology */\n/* Copyright (c) 2001 Hewlett-Packard Company */\n\n/* decode_table.c -- tables for decoding x86 instructions\n */\n\n#include \"../globals.h\" /* need this to include decode.h (uint, etc.) */\n#include \"arch.h\"       /* need this to include decode.h (byte, etc. */\n#include \"instr.h\"      /* for REG_ constants */\n#include \"decode.h\"\n#include \"decode_private.h\"\n\n/****************************************************************************\n * All code below based on tables in the ``Intel Architecture Software\n * Developer's Manual,'' Volume 2: Instruction Set Reference, 2001.\n * Updated with information from later Intel manuals and AMD manuals.\n *\n * I added many new types not present in the Intel tables: see decode.h\n *\n * I don't list %eflags as a source or dest operand, but the particular\n * flags written are encoded.\n *\n * XXX: some day it may be worth adding flags indicating which instrs\n * are valid on which models of which processors (probably best to just add\n * which cpuid flag must be set for the instr to be supported): for\n * now though we do not rely on being able to predict which instrs are\n * invalid.\n */\n\n// We skip auto-formatting for the entire file to keep our aligned op_instr\n// entries and our single-line table entries:\n/* clang-format off */\n\n/****************************************************************************\n * Operand pointers into tables\n * When there are multiple encodings of an opcode, this points to the first\n * entry in a linked list.\n * This array corresponds with the enum in opcode.h\n * IF YOU CHANGE ONE YOU MUST CHANGE THE OTHER\n */\nconst instr_info_t * const op_instr[] =\n{\n    /* OP_INVALID */   NULL,\n    /* OP_UNDECODED */ NULL,\n    /* OP_CONTD   */   NULL,\n    /* OP_LABEL   */   NULL,\n\n    /* OP_add     */   &first_byte[0x05],\n    /* OP_or      */   &first_byte[0x0d],\n    /* OP_adc     */   &first_byte[0x15],\n    /* OP_sbb     */   &first_byte[0x1d],\n    /* OP_and     */   &first_byte[0x25],\n    /* OP_daa     */   &first_byte[0x27],\n    /* OP_sub     */   &first_byte[0x2d],\n    /* OP_das     */   &first_byte[0x2f],\n    /* OP_xor     */   &first_byte[0x35],\n    /* OP_aaa     */   &first_byte[0x37],\n    /* OP_cmp     */   &first_byte[0x3d],\n    /* OP_aas     */   &first_byte[0x3f],\n    /* OP_inc     */   &x64_extensions[0][0],\n    /* OP_dec     */   &x64_extensions[8][0],\n    /* OP_push    */   &first_byte[0x50],\n    /* OP_push_imm*/   &first_byte[0x68],\n    /* OP_pop     */   &first_byte[0x58],\n    /* OP_pusha   */   &first_byte[0x60],\n    /* OP_popa    */   &first_byte[0x61],\n    /* OP_bound   */   &evex_prefix_extensions[0][0],\n    /* OP_arpl    */   &x64_extensions[16][0],\n    /* OP_imul    */   &base_extensions[10][5],\n\n    /* OP_jo_short    */   &first_byte[0x70],\n    /* OP_jno_short   */   &first_byte[0x71],\n    /* OP_jb_short    */   &first_byte[0x72],\n    /* OP_jnb_short   */   &first_byte[0x73],\n    /* OP_jz_short    */   &first_byte[0x74],\n    /* OP_jnz_short   */   &first_byte[0x75],\n    /* OP_jbe_short   */   &first_byte[0x76],\n    /* OP_jnbe_short  */   &first_byte[0x77],\n    /* OP_js_short    */   &first_byte[0x78],\n    /* OP_jns_short   */   &first_byte[0x79],\n    /* OP_jp_short    */   &first_byte[0x7a],\n    /* OP_jnp_short   */   &first_byte[0x7b],\n    /* OP_jl_short    */   &first_byte[0x7c],\n    /* OP_jnl_short   */   &first_byte[0x7d],\n    /* OP_jle_short   */   &first_byte[0x7e],\n    /* OP_jnle_short  */   &first_byte[0x7f],\n\n    /* OP_call          */   &first_byte[0xe8],\n    /* OP_call_ind      */   &base_extensions[12][2],\n    /* OP_call_far      */   &first_byte[0x9a],\n    /* OP_call_far_ind  */   &base_extensions[12][3],\n    /* OP_jmp           */   &first_byte[0xe9],\n    /* OP_jmp_short     */   &first_byte[0xeb],\n    /* OP_jmp_ind       */   &base_extensions[12][4],\n    /* OP_jmp_far       */   &first_byte[0xea],\n    /* OP_jmp_far_ind   */   &base_extensions[12][5],\n\n    /* OP_loopne  */   &first_byte[0xe0],\n    /* OP_loope   */   &first_byte[0xe1],\n    /* OP_loop    */   &first_byte[0xe2],\n    /* OP_jecxz   */   &first_byte[0xe3],\n\n    /* point ld & st at eAX & al instrs, they save 1 byte (no modrm),\n     * hopefully time taken considering them doesn't offset that */\n    /* OP_mov_ld     */   &first_byte[0xa1],\n    /* OP_mov_st     */   &first_byte[0xa3],\n    /* PR 250397: store of immed is mov_st not mov_imm, even though can be immed->reg,\n     * which we address by sharing part of the mov_st template chain */\n    /* OP_mov_imm    */   &first_byte[0xb8],\n    /* OP_mov_seg    */   &first_byte[0x8e],\n    /* OP_mov_priv   */   &second_byte[0x20],\n\n    /* OP_test    */   &first_byte[0xa9],\n    /* OP_lea     */   &first_byte[0x8d],\n    /* OP_xchg    */   &first_byte[0x91],\n    /* OP_cwde    */   &first_byte[0x98],\n    /* OP_cdq     */   &first_byte[0x99],\n    /* OP_fwait   */   &first_byte[0x9b],\n    /* OP_pushf   */   &first_byte[0x9c],\n    /* OP_popf    */   &first_byte[0x9d],\n    /* OP_sahf    */   &first_byte[0x9e],\n    /* OP_lahf    */   &first_byte[0x9f],\n\n    /* OP_ret      */   &first_byte[0xc2],\n    /* OP_ret_far  */   &first_byte[0xca],\n\n    /* OP_les     */   &vex_prefix_extensions[0][0],\n    /* OP_lds     */   &vex_prefix_extensions[1][0],\n    /* OP_enter   */   &first_byte[0xc8],\n    /* OP_leave   */   &first_byte[0xc9],\n    /* OP_int3    */   &first_byte[0xcc],\n    /* OP_int     */   &first_byte[0xcd],\n    /* OP_into    */   &first_byte[0xce],\n    /* OP_iret    */   &first_byte[0xcf],\n    /* OP_aam     */   &first_byte[0xd4],\n    /* OP_aad     */   &first_byte[0xd5],\n    /* OP_xlat    */   &first_byte[0xd7],\n    /* OP_in      */   &first_byte[0xe5],\n    /* OP_out     */   &first_byte[0xe7],\n    /* OP_hlt     */   &first_byte[0xf4],\n    /* OP_cmc     */   &first_byte[0xf5],\n    /* OP_clc     */   &first_byte[0xf8],\n    /* OP_stc     */   &first_byte[0xf9],\n    /* OP_cli     */   &first_byte[0xfa],\n    /* OP_sti     */   &first_byte[0xfb],\n    /* OP_cld     */   &first_byte[0xfc],\n    /* OP_std     */   &first_byte[0xfd],\n\n\n    /* OP_lar         */   &second_byte[0x02],\n    /* OP_lsl         */   &second_byte[0x03],\n    /* OP_syscall     */   &second_byte[0x05],\n    /* OP_clts        */   &second_byte[0x06],\n    /* OP_sysret      */   &second_byte[0x07],\n    /* OP_invd        */   &second_byte[0x08],\n    /* OP_wbinvd      */   &second_byte[0x09],\n    /* OP_ud2a        */   &second_byte[0x0b],\n    /* OP_nop_modrm   */   &second_byte[0x1f],\n    /* OP_movntps     */   &prefix_extensions[11][0],\n    /* OP_movntpd     */   &prefix_extensions[11][2],\n    /* OP_wrmsr       */   &second_byte[0x30],\n    /* OP_rdtsc       */   &second_byte[0x31],\n    /* OP_rdmsr       */   &second_byte[0x32],\n    /* OP_rdpmc       */   &second_byte[0x33],\n    /* OP_sysenter    */   &second_byte[0x34],\n    /* OP_sysexit     */   &second_byte[0x35],\n\n    /* OP_cmovo       */   &second_byte[0x40],\n    /* OP_cmovno      */   &e_vex_extensions[83][0],\n    /* OP_cmovb       */   &e_vex_extensions[84][0],\n    /* OP_cmovnb      */   &second_byte[0x43],\n    /* OP_cmovz       */   &e_vex_extensions[86][0],\n    /* OP_cmovnz      */   &e_vex_extensions[87][0],\n    /* OP_cmovbe      */   &e_vex_extensions[88][0],\n    /* OP_cmovnbe     */   &e_vex_extensions[89][0],\n    /* OP_cmovs       */   &second_byte[0x48],\n    /* OP_cmovns      */   &second_byte[0x49],\n    /* OP_cmovp       */   &e_vex_extensions[90][0],\n    /* OP_cmovnp      */   &e_vex_extensions[85][0],\n    /* OP_cmovl       */   &second_byte[0x4c],\n    /* OP_cmovnl      */   &second_byte[0x4d],\n    /* OP_cmovle      */   &second_byte[0x4e],\n    /* OP_cmovnle     */   &second_byte[0x4f],\n\n    /* OP_punpcklbw   */   &prefix_extensions[32][0],\n    /* OP_punpcklwd   */   &prefix_extensions[33][0],\n    /* OP_punpckldq   */   &prefix_extensions[34][0],\n    /* OP_packsswb    */   &prefix_extensions[35][0],\n    /* OP_pcmpgtb     */   &prefix_extensions[36][0],\n    /* OP_pcmpgtw     */   &prefix_extensions[37][0],\n    /* OP_pcmpgtd     */   &prefix_extensions[38][0],\n    /* OP_packuswb    */   &prefix_extensions[39][0],\n    /* OP_punpckhbw   */   &prefix_extensions[40][0],\n    /* OP_punpckhwd   */   &prefix_extensions[41][0],\n    /* OP_punpckhdq   */   &prefix_extensions[42][0],\n    /* OP_packssdw    */   &prefix_extensions[43][0],\n    /* OP_punpcklqdq  */   &prefix_extensions[44][2],\n    /* OP_punpckhqdq  */   &prefix_extensions[45][2],\n    /* OP_movd        */   &prefix_extensions[46][0],\n    /* OP_movq        */   &prefix_extensions[112][0],\n    /* OP_movdqu      */   &prefix_extensions[112][1],\n    /* OP_movdqa      */   &prefix_extensions[112][2],\n    /* OP_pshufw      */   &prefix_extensions[47][0],\n    /* OP_pshufd      */   &prefix_extensions[47][2],\n    /* OP_pshufhw     */   &prefix_extensions[47][1],\n    /* OP_pshuflw     */   &prefix_extensions[47][3],\n    /* OP_pcmpeqb     */   &prefix_extensions[48][0],\n    /* OP_pcmpeqw     */   &prefix_extensions[49][0],\n    /* OP_pcmpeqd     */   &prefix_extensions[50][0],\n    /* OP_emms        */   &vex_L_extensions[0][0],\n\n    /* OP_jo      */   &second_byte[0x80],\n    /* OP_jno     */   &second_byte[0x81],\n    /* OP_jb      */   &second_byte[0x82],\n    /* OP_jnb     */   &second_byte[0x83],\n    /* OP_jz      */   &second_byte[0x84],\n    /* OP_jnz     */   &second_byte[0x85],\n    /* OP_jbe     */   &second_byte[0x86],\n    /* OP_jnbe    */   &second_byte[0x87],\n    /* OP_js      */   &second_byte[0x88],\n    /* OP_jns     */   &second_byte[0x89],\n    /* OP_jp      */   &second_byte[0x8a],\n    /* OP_jnp     */   &second_byte[0x8b],\n    /* OP_jl      */   &second_byte[0x8c],\n    /* OP_jnl     */   &second_byte[0x8d],\n    /* OP_jle     */   &second_byte[0x8e],\n    /* OP_jnle    */   &second_byte[0x8f],\n\n    /* OP_seto        */   &e_vex_extensions[79][0],\n    /* OP_setno       */   &e_vex_extensions[80][0],\n    /* OP_setb        */   &e_vex_extensions[81][0],\n    /* OP_setnb       */   &e_vex_extensions[82][0],\n    /* OP_setz        */   &second_byte[0x94],\n    /* OP_setnz       */   &second_byte[0x95],\n    /* OP_setbe       */   &second_byte[0x96],\n    /* OP_setnbe      */   &second_byte[0x97],\n    /* OP_sets        */   &e_vex_extensions[91][0],\n    /* OP_setns       */   &e_vex_extensions[92][0],\n    /* OP_setp        */   &second_byte[0x9a],\n    /* OP_setnp       */   &second_byte[0x9b],\n    /* OP_setl        */   &second_byte[0x9c],\n    /* OP_setnl       */   &second_byte[0x9d],\n    /* OP_setle       */   &second_byte[0x9e],\n    /* OP_setnle        */   &second_byte[0x9f],\n\n    /* OP_cpuid       */   &second_byte[0xa2],\n    /* OP_bt          */   &second_byte[0xa3],\n    /* OP_shld        */   &second_byte[0xa4],\n    /* OP_rsm         */   &second_byte[0xaa],\n    /* OP_bts         */   &second_byte[0xab],\n    /* OP_shrd        */   &second_byte[0xac],\n    /* OP_cmpxchg     */   &second_byte[0xb1],\n    /* OP_lss         */   &second_byte[0xb2],\n    /* OP_btr         */   &second_byte[0xb3],\n    /* OP_lfs         */   &second_byte[0xb4],\n    /* OP_lgs         */   &second_byte[0xb5],\n    /* OP_movzx       */   &second_byte[0xb7],\n    /* OP_ud2b        */   &second_byte[0xb9],\n    /* OP_btc         */   &second_byte[0xbb],\n    /* OP_bsf         */   &prefix_extensions[140][0],\n    /* OP_bsr         */   &prefix_extensions[136][0],\n    /* OP_movsx       */   &second_byte[0xbf],\n    /* OP_xadd        */   &second_byte[0xc1],\n    /* OP_movnti      */   &second_byte[0xc3],\n    /* OP_pinsrw      */   &prefix_extensions[53][0],\n    /* OP_pextrw      */   &prefix_extensions[54][0],\n    /* OP_bswap       */   &second_byte[0xc8],\n    /* OP_psrlw       */   &prefix_extensions[56][0],\n    /* OP_psrld       */   &prefix_extensions[57][0],\n    /* OP_psrlq       */   &prefix_extensions[58][0],\n    /* OP_paddq       */   &prefix_extensions[59][0],\n    /* OP_pmullw      */   &prefix_extensions[60][0],\n    /* OP_pmovmskb    */   &prefix_extensions[62][0],\n    /* OP_psubusb     */   &prefix_extensions[63][0],\n    /* OP_psubusw     */   &prefix_extensions[64][0],\n    /* OP_pminub      */   &prefix_extensions[65][0],\n    /* OP_pand        */   &prefix_extensions[66][0],\n    /* OP_paddusb     */   &prefix_extensions[67][0],\n    /* OP_paddusw     */   &prefix_extensions[68][0],\n    /* OP_pmaxub      */   &prefix_extensions[69][0],\n    /* OP_pandn       */   &prefix_extensions[70][0],\n    /* OP_pavgb       */   &prefix_extensions[71][0],\n    /* OP_psraw       */   &prefix_extensions[72][0],\n    /* OP_psrad       */   &prefix_extensions[73][0],\n    /* OP_pavgw       */   &prefix_extensions[74][0],\n    /* OP_pmulhuw     */   &prefix_extensions[75][0],\n    /* OP_pmulhw      */   &prefix_extensions[76][0],\n    /* OP_movntq      */   &prefix_extensions[78][0],\n    /* OP_movntdq     */   &prefix_extensions[78][2],\n    /* OP_psubsb      */   &prefix_extensions[79][0],\n    /* OP_psubsw      */   &prefix_extensions[80][0],\n    /* OP_pminsw      */   &prefix_extensions[81][0],\n    /* OP_por         */   &prefix_extensions[82][0],\n    /* OP_paddsb      */   &prefix_extensions[83][0],\n    /* OP_paddsw      */   &prefix_extensions[84][0],\n    /* OP_pmaxsw      */   &prefix_extensions[85][0],\n    /* OP_pxor        */   &prefix_extensions[86][0],\n    /* OP_psllw       */   &prefix_extensions[87][0],\n    /* OP_pslld       */   &prefix_extensions[88][0],\n    /* OP_psllq       */   &prefix_extensions[89][0],\n    /* OP_pmuludq     */   &prefix_extensions[90][0],\n    /* OP_pmaddwd     */   &prefix_extensions[91][0],\n    /* OP_psadbw      */   &prefix_extensions[92][0],\n    /* OP_maskmovq    */   &prefix_extensions[93][0],\n    /* OP_maskmovdqu  */   &prefix_extensions[93][2],\n    /* OP_psubb       */   &prefix_extensions[94][0],\n    /* OP_psubw       */   &prefix_extensions[95][0],\n    /* OP_psubd       */   &prefix_extensions[96][0],\n    /* OP_psubq       */   &prefix_extensions[97][0],\n    /* OP_paddb       */   &prefix_extensions[98][0],\n    /* OP_paddw       */   &prefix_extensions[99][0],\n    /* OP_paddd       */   &prefix_extensions[100][0],\n    /* OP_psrldq      */   &prefix_extensions[101][2],\n    /* OP_pslldq      */   &prefix_extensions[102][2],\n\n\n    /* OP_rol          */   &base_extensions[ 4][0],\n    /* OP_ror          */   &base_extensions[ 4][1],\n    /* OP_rcl          */   &base_extensions[ 4][2],\n    /* OP_rcr          */   &base_extensions[ 4][3],\n    /* OP_shl          */   &base_extensions[ 4][4],\n    /* OP_shr          */   &base_extensions[ 4][5],\n    /* OP_sar          */   &base_extensions[ 4][7],\n    /* OP_not          */   &base_extensions[10][2],\n    /* OP_neg          */   &base_extensions[10][3],\n    /* OP_mul          */   &base_extensions[10][4],\n    /* OP_div          */   &base_extensions[10][6],\n    /* OP_idiv         */   &base_extensions[10][7],\n    /* OP_sldt         */   &base_extensions[13][0],\n    /* OP_str          */   &base_extensions[13][1],\n    /* OP_lldt         */   &base_extensions[13][2],\n    /* OP_ltr          */   &base_extensions[13][3],\n    /* OP_verr         */   &base_extensions[13][4],\n    /* OP_verw         */   &base_extensions[13][5],\n    /* OP_sgdt         */   &mod_extensions[0][0],\n    /* OP_sidt         */   &mod_extensions[1][0],\n    /* OP_lgdt         */   &mod_extensions[5][0],\n    /* OP_lidt         */   &mod_extensions[4][0],\n    /* OP_smsw         */   &base_extensions[14][4],\n    /* OP_lmsw         */   &base_extensions[14][6],\n    /* OP_invlpg       */   &mod_extensions[2][0],\n    /* OP_cmpxchg8b    */   &base_extensions[16][1],\n    /* OP_fxsave32     */   &rex_w_extensions[0][0],\n    /* OP_fxrstor32    */   &rex_w_extensions[1][0],\n    /* OP_ldmxcsr      */   &e_vex_extensions[61][0],\n    /* OP_stmxcsr      */   &e_vex_extensions[62][0],\n    /* OP_lfence       */   &mod_extensions[6][1],\n    /* OP_mfence       */   &mod_extensions[7][1],\n    /* OP_clflush      */   &mod_extensions[3][0],\n    /* OP_sfence       */   &mod_extensions[3][1],\n    /* OP_prefetchnta  */   &base_extensions[23][0],\n    /* OP_prefetcht0   */   &base_extensions[23][1],\n    /* OP_prefetcht1   */   &base_extensions[23][2],\n    /* OP_prefetcht2   */   &base_extensions[23][3],\n    /* OP_prefetch     */   &base_extensions[24][0],\n    /* OP_prefetchw    */   &base_extensions[24][1],\n\n\n    /* OP_movups     */   &prefix_extensions[ 0][0],\n    /* OP_movss      */   &mod_extensions[18][0],\n    /* OP_movupd     */   &prefix_extensions[ 0][2],\n    /* OP_movsd      */   &mod_extensions[19][0],\n    /* OP_movlps     */   &prefix_extensions[ 2][0],\n    /* OP_movlpd     */   &prefix_extensions[ 2][2],\n    /* OP_unpcklps   */   &prefix_extensions[ 4][0],\n    /* OP_unpcklpd   */   &prefix_extensions[ 4][2],\n    /* OP_unpckhps   */   &prefix_extensions[ 5][0],\n    /* OP_unpckhpd   */   &prefix_extensions[ 5][2],\n    /* OP_movhps     */   &prefix_extensions[ 6][0],\n    /* OP_movhpd     */   &prefix_extensions[ 6][2],\n    /* OP_movaps     */   &prefix_extensions[ 8][0],\n    /* OP_movapd     */   &prefix_extensions[ 8][2],\n    /* OP_cvtpi2ps   */   &prefix_extensions[10][0],\n    /* OP_cvtsi2ss   */   &prefix_extensions[10][1],\n    /* OP_cvtpi2pd   */   &prefix_extensions[10][2],\n    /* OP_cvtsi2sd   */   &prefix_extensions[10][3],\n    /* OP_cvttps2pi  */   &prefix_extensions[12][0],\n    /* OP_cvttss2si  */   &prefix_extensions[12][1],\n    /* OP_cvttpd2pi  */   &prefix_extensions[12][2],\n    /* OP_cvttsd2si  */   &prefix_extensions[12][3],\n    /* OP_cvtps2pi   */   &prefix_extensions[13][0],\n    /* OP_cvtss2si   */   &prefix_extensions[13][1],\n    /* OP_cvtpd2pi   */   &prefix_extensions[13][2],\n    /* OP_cvtsd2si   */   &prefix_extensions[13][3],\n    /* OP_ucomiss    */   &prefix_extensions[14][0],\n    /* OP_ucomisd    */   &prefix_extensions[14][2],\n    /* OP_comiss     */   &prefix_extensions[15][0],\n    /* OP_comisd     */   &prefix_extensions[15][2],\n    /* OP_movmskps   */   &prefix_extensions[16][0],\n    /* OP_movmskpd   */   &prefix_extensions[16][2],\n    /* OP_sqrtps     */   &prefix_extensions[17][0],\n    /* OP_sqrtss     */   &prefix_extensions[17][1],\n    /* OP_sqrtpd     */   &prefix_extensions[17][2],\n    /* OP_sqrtsd     */   &prefix_extensions[17][3],\n    /* OP_rsqrtps    */   &prefix_extensions[18][0],\n    /* OP_rsqrtss    */   &prefix_extensions[18][1],\n    /* OP_rcpps      */   &prefix_extensions[19][0],\n    /* OP_rcpss      */   &prefix_extensions[19][1],\n    /* OP_andps      */   &prefix_extensions[20][0],\n    /* OP_andpd      */   &prefix_extensions[20][2],\n    /* OP_andnps     */   &prefix_extensions[21][0],\n    /* OP_andnpd     */   &prefix_extensions[21][2],\n    /* OP_orps       */   &prefix_extensions[22][0],\n    /* OP_orpd       */   &prefix_extensions[22][2],\n    /* OP_xorps      */   &prefix_extensions[23][0],\n    /* OP_xorpd      */   &prefix_extensions[23][2],\n    /* OP_addps      */   &prefix_extensions[24][0],\n    /* OP_addss      */   &prefix_extensions[24][1],\n    /* OP_addpd      */   &prefix_extensions[24][2],\n    /* OP_addsd      */   &prefix_extensions[24][3],\n    /* OP_mulps      */   &prefix_extensions[25][0],\n    /* OP_mulss      */   &prefix_extensions[25][1],\n    /* OP_mulpd      */   &prefix_extensions[25][2],\n    /* OP_mulsd      */   &prefix_extensions[25][3],\n    /* OP_cvtps2pd   */   &prefix_extensions[26][0],\n    /* OP_cvtss2sd   */   &prefix_extensions[26][1],\n    /* OP_cvtpd2ps   */   &prefix_extensions[26][2],\n    /* OP_cvtsd2ss   */   &prefix_extensions[26][3],\n    /* OP_cvtdq2ps   */   &prefix_extensions[27][0],\n    /* OP_cvttps2dq  */   &prefix_extensions[27][1],\n    /* OP_cvtps2dq   */   &prefix_extensions[27][2],\n    /* OP_subps      */   &prefix_extensions[28][0],\n    /* OP_subss      */   &prefix_extensions[28][1],\n    /* OP_subpd      */   &prefix_extensions[28][2],\n    /* OP_subsd      */   &prefix_extensions[28][3],\n    /* OP_minps      */   &prefix_extensions[29][0],\n    /* OP_minss      */   &prefix_extensions[29][1],\n    /* OP_minpd      */   &prefix_extensions[29][2],\n    /* OP_minsd      */   &prefix_extensions[29][3],\n    /* OP_divps      */   &prefix_extensions[30][0],\n    /* OP_divss      */   &prefix_extensions[30][1],\n    /* OP_divpd      */   &prefix_extensions[30][2],\n    /* OP_divsd      */   &prefix_extensions[30][3],\n    /* OP_maxps      */   &prefix_extensions[31][0],\n    /* OP_maxss      */   &prefix_extensions[31][1],\n    /* OP_maxpd      */   &prefix_extensions[31][2],\n    /* OP_maxsd      */   &prefix_extensions[31][3],\n    /* OP_cmpps      */   &prefix_extensions[52][0],\n    /* OP_cmpss      */   &prefix_extensions[52][1],\n    /* OP_cmppd      */   &prefix_extensions[52][2],\n    /* OP_cmpsd      */   &prefix_extensions[52][3],\n    /* OP_shufps     */   &prefix_extensions[55][0],\n    /* OP_shufpd     */   &prefix_extensions[55][2],\n    /* OP_cvtdq2pd   */   &prefix_extensions[77][1],\n    /* OP_cvttpd2dq  */   &prefix_extensions[77][2],\n    /* OP_cvtpd2dq   */   &prefix_extensions[77][3],\n    /* OP_nop        */   &rex_b_extensions[0][0],\n    /* OP_pause      */   &prefix_extensions[103][1],\n\n    /* OP_ins         */   &rep_extensions[1][0],\n    /* OP_rep_ins     */   &rep_extensions[1][2],\n    /* OP_outs        */   &rep_extensions[3][0],\n    /* OP_rep_outs    */   &rep_extensions[3][2],\n    /* OP_movs        */   &rep_extensions[5][0],\n    /* OP_rep_movs    */   &rep_extensions[5][2],\n    /* OP_stos        */   &rep_extensions[7][0],\n    /* OP_rep_stos    */   &rep_extensions[7][2],\n    /* OP_lods        */   &rep_extensions[9][0],\n    /* OP_rep_lods    */   &rep_extensions[9][2],\n    /* OP_cmps        */   &repne_extensions[1][0],\n    /* OP_rep_cmps    */   &repne_extensions[1][2],\n    /* OP_repne_cmps  */   &repne_extensions[1][4],\n    /* OP_scas        */   &repne_extensions[3][0],\n    /* OP_rep_scas    */   &repne_extensions[3][2],\n    /* OP_repne_scas  */   &repne_extensions[3][4],\n\n\n    /* OP_fadd     */   &float_low_modrm[0x00],\n    /* OP_fmul     */   &float_low_modrm[0x01],\n    /* OP_fcom     */   &float_low_modrm[0x02],\n    /* OP_fcomp    */   &float_low_modrm[0x03],\n    /* OP_fsub     */   &float_low_modrm[0x04],\n    /* OP_fsubr    */   &float_low_modrm[0x05],\n    /* OP_fdiv     */   &float_low_modrm[0x06],\n    /* OP_fdivr    */   &float_low_modrm[0x07],\n    /* OP_fld      */   &float_low_modrm[0x08],\n    /* OP_fst      */   &float_low_modrm[0x0a],\n    /* OP_fstp     */   &float_low_modrm[0x0b],\n    /* OP_fldenv   */   &float_low_modrm[0x0c],\n    /* OP_fldcw    */   &float_low_modrm[0x0d],\n    /* OP_fnstenv  */   &float_low_modrm[0x0e],\n    /* OP_fnstcw   */   &float_low_modrm[0x0f],\n    /* OP_fiadd    */   &float_low_modrm[0x10],\n    /* OP_fimul    */   &float_low_modrm[0x11],\n    /* OP_ficom    */   &float_low_modrm[0x12],\n    /* OP_ficomp   */   &float_low_modrm[0x13],\n    /* OP_fisub    */   &float_low_modrm[0x14],\n    /* OP_fisubr   */   &float_low_modrm[0x15],\n    /* OP_fidiv    */   &float_low_modrm[0x16],\n    /* OP_fidivr   */   &float_low_modrm[0x17],\n    /* OP_fild     */   &float_low_modrm[0x18],\n    /* OP_fist     */   &float_low_modrm[0x1a],\n    /* OP_fistp    */   &float_low_modrm[0x1b],\n    /* OP_frstor   */   &float_low_modrm[0x2c],\n    /* OP_fnsave   */   &float_low_modrm[0x2e],\n    /* OP_fnstsw   */   &float_low_modrm[0x2f],\n\n    /* OP_fbld     */   &float_low_modrm[0x3c],\n    /* OP_fbstp    */   &float_low_modrm[0x3e],\n\n\n    /* OP_fxch      */   &float_high_modrm[1][0x08],\n    /* OP_fnop      */   &float_high_modrm[1][0x10],\n    /* OP_fchs      */   &float_high_modrm[1][0x20],\n    /* OP_fabs      */   &float_high_modrm[1][0x21],\n    /* OP_ftst      */   &float_high_modrm[1][0x24],\n    /* OP_fxam      */   &float_high_modrm[1][0x25],\n    /* OP_fld1      */   &float_high_modrm[1][0x28],\n    /* OP_fldl2t    */   &float_high_modrm[1][0x29],\n    /* OP_fldl2e    */   &float_high_modrm[1][0x2a],\n    /* OP_fldpi     */   &float_high_modrm[1][0x2b],\n    /* OP_fldlg2    */   &float_high_modrm[1][0x2c],\n    /* OP_fldln2    */   &float_high_modrm[1][0x2d],\n    /* OP_fldz      */   &float_high_modrm[1][0x2e],\n    /* OP_f2xm1     */   &float_high_modrm[1][0x30],\n    /* OP_fyl2x     */   &float_high_modrm[1][0x31],\n    /* OP_fptan     */   &float_high_modrm[1][0x32],\n    /* OP_fpatan    */   &float_high_modrm[1][0x33],\n    /* OP_fxtract   */   &float_high_modrm[1][0x34],\n    /* OP_fprem1    */   &float_high_modrm[1][0x35],\n    /* OP_fdecstp   */   &float_high_modrm[1][0x36],\n    /* OP_fincstp   */   &float_high_modrm[1][0x37],\n    /* OP_fprem     */   &float_high_modrm[1][0x38],\n    /* OP_fyl2xp1   */   &float_high_modrm[1][0x39],\n    /* OP_fsqrt     */   &float_high_modrm[1][0x3a],\n    /* OP_fsincos   */   &float_high_modrm[1][0x3b],\n    /* OP_frndint   */   &float_high_modrm[1][0x3c],\n    /* OP_fscale    */   &float_high_modrm[1][0x3d],\n    /* OP_fsin      */   &float_high_modrm[1][0x3e],\n    /* OP_fcos      */   &float_high_modrm[1][0x3f],\n    /* OP_fcmovb    */   &float_high_modrm[2][0x00],\n    /* OP_fcmove    */   &float_high_modrm[2][0x08],\n    /* OP_fcmovbe   */   &float_high_modrm[2][0x10],\n    /* OP_fcmovu    */   &float_high_modrm[2][0x18],\n    /* OP_fucompp   */   &float_high_modrm[2][0x29],\n    /* OP_fcmovnb   */   &float_high_modrm[3][0x00],\n    /* OP_fcmovne   */   &float_high_modrm[3][0x08],\n    /* OP_fcmovnbe  */   &float_high_modrm[3][0x10],\n    /* OP_fcmovnu   */   &float_high_modrm[3][0x18],\n    /* OP_fnclex    */   &float_high_modrm[3][0x22],\n    /* OP_fninit    */   &float_high_modrm[3][0x23],\n    /* OP_fucomi    */   &float_high_modrm[3][0x28],\n    /* OP_fcomi     */   &float_high_modrm[3][0x30],\n    /* OP_ffree     */   &float_high_modrm[5][0x00],\n    /* OP_fucom     */   &float_high_modrm[5][0x20],\n    /* OP_fucomp    */   &float_high_modrm[5][0x28],\n    /* OP_faddp     */   &float_high_modrm[6][0x00],\n    /* OP_fmulp     */   &float_high_modrm[6][0x08],\n    /* OP_fcompp    */   &float_high_modrm[6][0x19],\n    /* OP_fsubrp    */   &float_high_modrm[6][0x20],\n    /* OP_fsubp     */   &float_high_modrm[6][0x28],\n    /* OP_fdivrp    */   &float_high_modrm[6][0x30],\n    /* OP_fdivp     */   &float_high_modrm[6][0x38],\n    /* OP_fucomip   */   &float_high_modrm[7][0x28],\n    /* OP_fcomip    */   &float_high_modrm[7][0x30],\n\n    /* SSE3 instructions */\n    /* OP_fisttp      */   &float_low_modrm[0x29],\n    /* OP_haddpd      */   &prefix_extensions[114][2],\n    /* OP_haddps      */   &prefix_extensions[114][3],\n    /* OP_hsubpd      */   &prefix_extensions[115][2],\n    /* OP_hsubps      */   &prefix_extensions[115][3],\n    /* OP_addsubpd    */   &prefix_extensions[116][2],\n    /* OP_addsubps    */   &prefix_extensions[116][3],\n    /* OP_lddqu       */   &prefix_extensions[117][3],\n    /* OP_monitor     */    &rm_extensions[1][0],\n    /* OP_mwait       */    &rm_extensions[1][1],\n    /* OP_movsldup    */   &prefix_extensions[ 2][1],\n    /* OP_movshdup    */   &prefix_extensions[ 6][1],\n    /* OP_movddup     */   &prefix_extensions[ 2][3],\n\n    /* 3D-Now! instructions */\n    /* OP_femms         */   &second_byte[0x0e],\n    /* OP_unknown_3dnow */   &suffix_extensions[0],\n    /* OP_pavgusb       */   &suffix_extensions[1],\n    /* OP_pfadd         */   &suffix_extensions[2],\n    /* OP_pfacc         */   &suffix_extensions[3],\n    /* OP_pfcmpge       */   &suffix_extensions[4],\n    /* OP_pfcmpgt       */   &suffix_extensions[5],\n    /* OP_pfcmpeq       */   &suffix_extensions[6],\n    /* OP_pfmin         */   &suffix_extensions[7],\n    /* OP_pfmax         */   &suffix_extensions[8],\n    /* OP_pfmul         */   &suffix_extensions[9],\n    /* OP_pfrcp         */   &suffix_extensions[10],\n    /* OP_pfrcpit1      */   &suffix_extensions[11],\n    /* OP_pfrcpit2      */   &suffix_extensions[12],\n    /* OP_pfrsqrt       */   &suffix_extensions[13],\n    /* OP_pfrsqit1      */   &suffix_extensions[14],\n    /* OP_pmulhrw       */   &suffix_extensions[15],\n    /* OP_pfsub         */   &suffix_extensions[16],\n    /* OP_pfsubr        */   &suffix_extensions[17],\n    /* OP_pi2fd         */   &suffix_extensions[18],\n    /* OP_pf2id         */   &suffix_extensions[19],\n    /* OP_pi2fw         */   &suffix_extensions[20],\n    /* OP_pf2iw         */   &suffix_extensions[21],\n    /* OP_pfnacc        */   &suffix_extensions[22],\n    /* OP_pfpnacc       */   &suffix_extensions[23],\n    /* OP_pswapd        */   &suffix_extensions[24],\n\n    /* SSSE3 */\n    /* OP_pshufb        */   &prefix_extensions[118][0],\n    /* OP_phaddw        */   &prefix_extensions[119][0],\n    /* OP_phaddd        */   &prefix_extensions[120][0],\n    /* OP_phaddsw       */   &prefix_extensions[121][0],\n    /* OP_pmaddubsw     */   &prefix_extensions[122][0],\n    /* OP_phsubw        */   &prefix_extensions[123][0],\n    /* OP_phsubd        */   &prefix_extensions[124][0],\n    /* OP_phsubsw       */   &prefix_extensions[125][0],\n    /* OP_psignb        */   &prefix_extensions[126][0],\n    /* OP_psignw        */   &prefix_extensions[127][0],\n    /* OP_psignd        */   &prefix_extensions[128][0],\n    /* OP_pmulhrsw      */   &prefix_extensions[129][0],\n    /* OP_pabsb         */   &prefix_extensions[130][0],\n    /* OP_pabsw         */   &prefix_extensions[131][0],\n    /* OP_pabsd         */   &prefix_extensions[132][0],\n    /* OP_palignr       */   &prefix_extensions[133][0],\n\n    /* SSE4 (incl AMD (SSE4A) and Intel-specific (SSE4.1, SSE4.2) extensions */\n    /* OP_popcnt        */   &second_byte[0xb8],\n    /* OP_movntss       */   &prefix_extensions[11][1],\n    /* OP_movntsd       */   &prefix_extensions[11][3],\n    /* OP_extrq         */   &prefix_extensions[134][2],\n    /* OP_insertq       */   &prefix_extensions[134][3],\n    /* OP_lzcnt         */   &prefix_extensions[136][1],\n    /* OP_pblendvb      */   &third_byte_38[16],\n    /* OP_blendvps      */   &third_byte_38[17],\n    /* OP_blendvpd      */   &third_byte_38[18],\n    /* OP_ptest         */   &e_vex_extensions[3][0],\n    /* OP_pmovsxbw      */   &e_vex_extensions[4][0],\n    /* OP_pmovsxbd      */   &e_vex_extensions[5][0],\n    /* OP_pmovsxbq      */   &e_vex_extensions[6][0],\n    /* OP_pmovsxwd      */   &e_vex_extensions[7][0],\n    /* OP_pmovsxwq      */   &e_vex_extensions[8][0],\n    /* OP_pmovsxdq      */   &e_vex_extensions[9][0],\n    /* OP_pmuldq        */   &e_vex_extensions[10][0],\n    /* OP_pcmpeqq       */   &e_vex_extensions[11][0],\n    /* OP_movntdqa      */   &e_vex_extensions[12][0],\n    /* OP_packusdw      */   &e_vex_extensions[13][0],\n    /* OP_pmovzxbw      */   &e_vex_extensions[14][0],\n    /* OP_pmovzxbd      */   &e_vex_extensions[15][0],\n    /* OP_pmovzxbq      */   &e_vex_extensions[16][0],\n    /* OP_pmovzxwd      */   &e_vex_extensions[17][0],\n    /* OP_pmovzxwq      */   &e_vex_extensions[18][0],\n    /* OP_pmovzxdq      */   &e_vex_extensions[19][0],\n    /* OP_pcmpgtq       */   &e_vex_extensions[20][0],\n    /* OP_pminsb        */   &e_vex_extensions[21][0],\n    /* OP_pminsd        */   &e_vex_extensions[22][0],\n    /* OP_pminuw        */   &e_vex_extensions[23][0],\n    /* OP_pminud        */   &e_vex_extensions[24][0],\n    /* OP_pmaxsb        */   &e_vex_extensions[25][0],\n    /* OP_pmaxsd        */   &e_vex_extensions[26][0],\n    /* OP_pmaxuw        */   &e_vex_extensions[27][0],\n    /* OP_pmaxud        */   &e_vex_extensions[28][0],\n    /* OP_pmulld        */   &e_vex_extensions[29][0],\n    /* OP_phminposuw    */   &e_vex_extensions[30][0],\n    /* OP_crc32         */   &prefix_extensions[139][3],\n    /* OP_pextrb        */   &e_vex_extensions[36][0],\n    /* OP_pextrd        */   &e_vex_extensions[38][0],\n    /* OP_extractps     */   &e_vex_extensions[39][0],\n    /* OP_roundps       */   &e_vex_extensions[40][0],\n    /* OP_roundpd       */   &e_vex_extensions[41][0],\n    /* OP_roundss       */   &e_vex_extensions[42][0],\n    /* OP_roundsd       */   &e_vex_extensions[43][0],\n    /* OP_blendps       */   &e_vex_extensions[44][0],\n    /* OP_blendpd       */   &e_vex_extensions[45][0],\n    /* OP_pblendw       */   &e_vex_extensions[46][0],\n    /* OP_pinsrb        */   &e_vex_extensions[47][0],\n    /* OP_insertps      */   &e_vex_extensions[48][0],\n    /* OP_pinsrd        */   &e_vex_extensions[49][0],\n    /* OP_dpps          */   &e_vex_extensions[50][0],\n    /* OP_dppd          */   &e_vex_extensions[51][0],\n    /* OP_mpsadbw       */   &e_vex_extensions[52][0],\n    /* OP_pcmpestrm     */   &e_vex_extensions[53][0],\n    /* OP_pcmpestri     */   &e_vex_extensions[54][0],\n    /* OP_pcmpistrm     */   &e_vex_extensions[55][0],\n    /* OP_pcmpistri     */   &e_vex_extensions[56][0],\n\n    /* x64 */\n    /* OP_movsxd        */   &x64_extensions[16][1],\n    /* OP_swapgs        */   &rm_extensions[2][0],\n\n    /* VMX */\n    /* OP_vmcall        */   &rm_extensions[0][1],\n    /* OP_vmlaunch      */   &rm_extensions[0][2],\n    /* OP_vmresume      */   &rm_extensions[0][3],\n    /* OP_vmxoff        */   &rm_extensions[0][4],\n    /* OP_vmptrst       */   &mod_extensions[13][0],\n    /* OP_vmptrld       */   &prefix_extensions[137][0],\n    /* OP_vmxon         */   &prefix_extensions[137][1],\n    /* OP_vmclear       */   &prefix_extensions[137][2],\n    /* OP_vmread        */   &prefix_extensions[134][0],\n    /* OP_vmwrite       */   &prefix_extensions[135][0],\n\n    /* undocumented */\n    /* OP_int1          */   &first_byte[0xf1],\n    /* OP_salc          */   &first_byte[0xd6],\n    /* OP_ffreep        */   &float_high_modrm[7][0x00],\n\n    /* AMD SVM */\n    /* OP_vmrun         */   &rm_extensions[3][0],\n    /* OP_vmmcall       */   &rm_extensions[3][1],\n    /* OP_vmload        */   &rm_extensions[3][2],\n    /* OP_vmsave        */   &rm_extensions[3][3],\n    /* OP_stgi          */   &rm_extensions[3][4],\n    /* OP_clgi          */   &rm_extensions[3][5],\n    /* OP_skinit        */   &rm_extensions[3][6],\n    /* OP_invlpga       */   &rm_extensions[3][7],\n    /* AMD though not part of SVM */\n    /* OP_rdtscp        */   &rm_extensions[2][1],\n\n    /* Intel VMX additions */\n    /* OP_invept        */   &third_byte_38[49],\n    /* OP_invvpid       */   &third_byte_38[50],\n\n    /* added in Intel Westmere */\n    /* OP_pclmulqdq     */   &e_vex_extensions[57][0],\n    /* OP_aesimc        */   &e_vex_extensions[31][0],\n    /* OP_aesenc        */   &e_vex_extensions[32][0],\n    /* OP_aesenclast    */   &e_vex_extensions[33][0],\n    /* OP_aesdec        */   &e_vex_extensions[34][0],\n    /* OP_aesdeclast    */   &e_vex_extensions[35][0],\n    /* OP_aeskeygenassist*/  &e_vex_extensions[58][0],\n\n    /* added in Intel Atom */\n    /* OP_movbe         */   &prefix_extensions[138][0],\n\n    /* added in Intel Sandy Bridge */\n    /* OP_xgetbv        */   &rm_extensions[4][0],\n    /* OP_xsetbv        */   &rm_extensions[4][1],\n    /* OP_xsave32       */   &rex_w_extensions[2][0],\n    /* OP_xrstor32      */   &rex_w_extensions[3][0],\n    /* OP_xsaveopt32    */   &rex_w_extensions[4][0],\n\n    /* AVX */\n    /* OP_vmovss        */  &mod_extensions[ 8][0],\n    /* OP_vmovsd        */  &mod_extensions[ 9][0],\n    /* OP_vmovups       */  &prefix_extensions[ 0][4],\n    /* OP_vmovupd       */  &prefix_extensions[ 0][6],\n    /* OP_vmovlps       */  &prefix_extensions[ 2][4],\n    /* OP_vmovsldup     */  &prefix_extensions[ 2][5],\n    /* OP_vmovlpd       */  &prefix_extensions[ 2][6],\n    /* OP_vmovddup      */  &prefix_extensions[ 2][7],\n    /* OP_vunpcklps     */  &prefix_extensions[ 4][4],\n    /* OP_vunpcklpd     */  &prefix_extensions[ 4][6],\n    /* OP_vunpckhps     */  &prefix_extensions[ 5][4],\n    /* OP_vunpckhpd     */  &prefix_extensions[ 5][6],\n    /* OP_vmovhps       */  &prefix_extensions[ 6][4],\n    /* OP_vmovshdup     */  &prefix_extensions[ 6][5],\n    /* OP_vmovhpd       */  &prefix_extensions[ 6][6],\n    /* OP_vmovaps       */  &prefix_extensions[ 8][4],\n    /* OP_vmovapd       */  &prefix_extensions[ 8][6],\n    /* OP_vcvtsi2ss     */  &prefix_extensions[10][5],\n    /* OP_vcvtsi2sd     */  &prefix_extensions[10][7],\n    /* OP_vmovntps      */  &prefix_extensions[11][4],\n    /* OP_vmovntpd      */  &prefix_extensions[11][6],\n    /* OP_vcvttss2si    */  &prefix_extensions[12][5],\n    /* OP_vcvttsd2si    */  &prefix_extensions[12][7],\n    /* OP_vcvtss2si     */  &prefix_extensions[13][5],\n    /* OP_vcvtsd2si     */  &prefix_extensions[13][7],\n    /* OP_vucomiss      */  &prefix_extensions[14][4],\n    /* OP_vucomisd      */  &prefix_extensions[14][6],\n    /* OP_vcomiss       */  &prefix_extensions[15][4],\n    /* OP_vcomisd       */  &prefix_extensions[15][6],\n    /* OP_vmovmskps     */  &prefix_extensions[16][4],\n    /* OP_vmovmskpd     */  &prefix_extensions[16][6],\n    /* OP_vsqrtps       */  &prefix_extensions[17][4],\n    /* OP_vsqrtss       */  &prefix_extensions[17][5],\n    /* OP_vsqrtpd       */  &prefix_extensions[17][6],\n    /* OP_vsqrtsd       */  &prefix_extensions[17][7],\n    /* OP_vrsqrtps      */  &prefix_extensions[18][4],\n    /* OP_vrsqrtss      */  &prefix_extensions[18][5],\n    /* OP_vrcpps        */  &prefix_extensions[19][4],\n    /* OP_vrcpss        */  &prefix_extensions[19][5],\n    /* OP_vandps        */  &prefix_extensions[20][4],\n    /* OP_vandpd        */  &prefix_extensions[20][6],\n    /* OP_vandnps       */  &prefix_extensions[21][4],\n    /* OP_vandnpd       */  &prefix_extensions[21][6],\n    /* OP_vorps         */  &prefix_extensions[22][4],\n    /* OP_vorpd         */  &prefix_extensions[22][6],\n    /* OP_vxorps        */  &prefix_extensions[23][4],\n    /* OP_vxorpd        */  &prefix_extensions[23][6],\n    /* OP_vaddps        */  &prefix_extensions[24][4],\n    /* OP_vaddss        */  &prefix_extensions[24][5],\n    /* OP_vaddpd        */  &prefix_extensions[24][6],\n    /* OP_vaddsd        */  &prefix_extensions[24][7],\n    /* OP_vmulps        */  &prefix_extensions[25][4],\n    /* OP_vmulss        */  &prefix_extensions[25][5],\n    /* OP_vmulpd        */  &prefix_extensions[25][6],\n    /* OP_vmulsd        */  &prefix_extensions[25][7],\n    /* OP_vcvtps2pd     */  &prefix_extensions[26][4],\n    /* OP_vcvtss2sd     */  &prefix_extensions[26][5],\n    /* OP_vcvtpd2ps     */  &prefix_extensions[26][6],\n    /* OP_vcvtsd2ss     */  &prefix_extensions[26][7],\n    /* OP_vcvtdq2ps     */  &prefix_extensions[27][4],\n    /* OP_vcvttps2dq    */  &prefix_extensions[27][5],\n    /* OP_vcvtps2dq     */  &prefix_extensions[27][6],\n    /* OP_vsubps        */  &prefix_extensions[28][4],\n    /* OP_vsubss        */  &prefix_extensions[28][5],\n    /* OP_vsubpd        */  &prefix_extensions[28][6],\n    /* OP_vsubsd        */  &prefix_extensions[28][7],\n    /* OP_vminps        */  &prefix_extensions[29][4],\n    /* OP_vminss        */  &prefix_extensions[29][5],\n    /* OP_vminpd        */  &prefix_extensions[29][6],\n    /* OP_vminsd        */  &prefix_extensions[29][7],\n    /* OP_vdivps        */  &prefix_extensions[30][4],\n    /* OP_vdivss        */  &prefix_extensions[30][5],\n    /* OP_vdivpd        */  &prefix_extensions[30][6],\n    /* OP_vdivsd        */  &prefix_extensions[30][7],\n    /* OP_vmaxps        */  &prefix_extensions[31][4],\n    /* OP_vmaxss        */  &prefix_extensions[31][5],\n    /* OP_vmaxpd        */  &prefix_extensions[31][6],\n    /* OP_vmaxsd        */  &prefix_extensions[31][7],\n    /* OP_vpunpcklbw    */  &prefix_extensions[32][6],\n    /* OP_vpunpcklwd    */  &prefix_extensions[33][6],\n    /* OP_vpunpckldq    */  &prefix_extensions[34][6],\n    /* OP_vpacksswb     */  &prefix_extensions[35][6],\n    /* OP_vpcmpgtb      */  &prefix_extensions[36][6],\n    /* OP_vpcmpgtw      */  &prefix_extensions[37][6],\n    /* OP_vpcmpgtd      */  &prefix_extensions[38][6],\n    /* OP_vpackuswb     */  &prefix_extensions[39][6],\n    /* OP_vpunpckhbw    */  &prefix_extensions[40][6],\n    /* OP_vpunpckhwd    */  &prefix_extensions[41][6],\n    /* OP_vpunpckhdq    */  &prefix_extensions[42][6],\n    /* OP_vpackssdw     */  &prefix_extensions[43][6],\n    /* OP_vpunpcklqdq   */  &prefix_extensions[44][6],\n    /* OP_vpunpckhqdq   */  &prefix_extensions[45][6],\n    /* OP_vmovd         */  &prefix_extensions[46][6],\n    /* OP_vpshufhw      */  &prefix_extensions[47][5],\n    /* OP_vpshufd       */  &prefix_extensions[47][6],\n    /* OP_vpshuflw      */  &prefix_extensions[47][7],\n    /* OP_vpcmpeqb      */  &prefix_extensions[48][6],\n    /* OP_vpcmpeqw      */  &prefix_extensions[49][6],\n    /* OP_vpcmpeqd      */  &prefix_extensions[50][6],\n    /* OP_vmovq         */  &prefix_extensions[51][5],\n    /* OP_vcmpps        */  &prefix_extensions[52][4],\n    /* OP_vcmpss        */  &prefix_extensions[52][5],\n    /* OP_vcmppd        */  &prefix_extensions[52][6],\n    /* OP_vcmpsd        */  &prefix_extensions[52][7],\n    /* OP_vpinsrw       */  &prefix_extensions[53][6],\n    /* OP_vpextrw       */  &prefix_extensions[54][6],\n    /* OP_vshufps       */  &prefix_extensions[55][4],\n    /* OP_vshufpd       */  &prefix_extensions[55][6],\n    /* OP_vpsrlw        */  &prefix_extensions[56][6],\n    /* OP_vpsrld        */  &prefix_extensions[57][6],\n    /* OP_vpsrlq        */  &prefix_extensions[58][6],\n    /* OP_vpaddq        */  &prefix_extensions[59][6],\n    /* OP_vpmullw       */  &prefix_extensions[60][6],\n    /* OP_vpmovmskb     */  &prefix_extensions[62][6],\n    /* OP_vpsubusb      */  &prefix_extensions[63][6],\n    /* OP_vpsubusw      */  &prefix_extensions[64][6],\n    /* OP_vpminub       */  &prefix_extensions[65][6],\n    /* OP_vpand         */  &prefix_extensions[66][6],\n    /* OP_vpaddusb      */  &prefix_extensions[67][6],\n    /* OP_vpaddusw      */  &prefix_extensions[68][6],\n    /* OP_vpmaxub       */  &prefix_extensions[69][6],\n    /* OP_vpandn        */  &prefix_extensions[70][6],\n    /* OP_vpavgb        */  &prefix_extensions[71][6],\n    /* OP_vpsraw        */  &prefix_extensions[72][6],\n    /* OP_vpsrad        */  &prefix_extensions[73][6],\n    /* OP_vpavgw        */  &prefix_extensions[74][6],\n    /* OP_vpmulhuw      */  &prefix_extensions[75][6],\n    /* OP_vpmulhw       */  &prefix_extensions[76][6],\n    /* OP_vcvtdq2pd     */  &prefix_extensions[77][5],\n    /* OP_vcvttpd2dq    */  &prefix_extensions[77][6],\n    /* OP_vcvtpd2dq     */  &prefix_extensions[77][7],\n    /* OP_vmovntdq      */  &prefix_extensions[78][6],\n    /* OP_vpsubsb       */  &prefix_extensions[79][6],\n    /* OP_vpsubsw       */  &prefix_extensions[80][6],\n    /* OP_vpminsw       */  &prefix_extensions[81][6],\n    /* OP_vpor          */  &prefix_extensions[82][6],\n    /* OP_vpaddsb       */  &prefix_extensions[83][6],\n    /* OP_vpaddsw       */  &prefix_extensions[84][6],\n    /* OP_vpmaxsw       */  &prefix_extensions[85][6],\n    /* OP_vpxor         */  &prefix_extensions[86][6],\n    /* OP_vpsllw        */  &prefix_extensions[87][6],\n    /* OP_vpslld        */  &prefix_extensions[88][6],\n    /* OP_vpsllq        */  &prefix_extensions[89][6],\n    /* OP_vpmuludq      */  &prefix_extensions[90][6],\n    /* OP_vpmaddwd      */  &prefix_extensions[91][6],\n    /* OP_vpsadbw       */  &prefix_extensions[92][6],\n    /* OP_vmaskmovdqu   */  &prefix_extensions[93][6],\n    /* OP_vpsubb        */  &prefix_extensions[94][6],\n    /* OP_vpsubw        */  &prefix_extensions[95][6],\n    /* OP_vpsubd        */  &prefix_extensions[96][6],\n    /* OP_vpsubq        */  &prefix_extensions[97][6],\n    /* OP_vpaddb        */  &prefix_extensions[98][6],\n    /* OP_vpaddw        */  &prefix_extensions[99][6],\n    /* OP_vpaddd        */  &prefix_extensions[100][6],\n    /* OP_vpsrldq       */  &prefix_extensions[101][6],\n    /* OP_vpslldq       */  &prefix_extensions[102][6],\n    /* OP_vmovdqu       */  &prefix_extensions[112][5],\n    /* OP_vmovdqa       */  &prefix_extensions[112][6],\n    /* OP_vhaddpd       */  &prefix_extensions[114][6],\n    /* OP_vhaddps       */  &prefix_extensions[114][7],\n    /* OP_vhsubpd       */  &prefix_extensions[115][6],\n    /* OP_vhsubps       */  &prefix_extensions[115][7],\n    /* OP_vaddsubpd     */  &prefix_extensions[116][6],\n    /* OP_vaddsubps     */  &prefix_extensions[116][7],\n    /* OP_vlddqu        */  &prefix_extensions[117][7],\n    /* OP_vpshufb       */  &prefix_extensions[118][6],\n    /* OP_vphaddw       */  &prefix_extensions[119][6],\n    /* OP_vphaddd       */  &prefix_extensions[120][6],\n    /* OP_vphaddsw      */  &prefix_extensions[121][6],\n    /* OP_vpmaddubsw    */  &prefix_extensions[122][6],\n    /* OP_vphsubw       */  &prefix_extensions[123][6],\n    /* OP_vphsubd       */  &prefix_extensions[124][6],\n    /* OP_vphsubsw      */  &prefix_extensions[125][6],\n    /* OP_vpsignb       */  &prefix_extensions[126][6],\n    /* OP_vpsignw       */  &prefix_extensions[127][6],\n    /* OP_vpsignd       */  &prefix_extensions[128][6],\n    /* OP_vpmulhrsw     */  &prefix_extensions[129][6],\n    /* OP_vpabsb        */  &prefix_extensions[130][6],\n    /* OP_vpabsw        */  &prefix_extensions[131][6],\n    /* OP_vpabsd        */  &prefix_extensions[132][6],\n    /* OP_vpalignr      */  &prefix_extensions[133][6],\n    /* OP_vpblendvb     */  &e_vex_extensions[ 2][1],\n    /* OP_vblendvps     */  &e_vex_extensions[ 0][1],\n    /* OP_vblendvpd     */  &e_vex_extensions[ 1][1],\n    /* OP_vptest        */  &e_vex_extensions[ 3][1],\n    /* OP_vpmovsxbw     */  &e_vex_extensions[ 4][1],\n    /* OP_vpmovsxbd     */  &e_vex_extensions[ 5][1],\n    /* OP_vpmovsxbq     */  &e_vex_extensions[ 6][1],\n    /* OP_vpmovsxwd     */  &e_vex_extensions[ 7][1],\n    /* OP_vpmovsxwq     */  &e_vex_extensions[ 8][1],\n    /* OP_vpmovsxdq     */  &e_vex_extensions[ 9][1],\n    /* OP_vpmuldq       */  &e_vex_extensions[10][1],\n    /* OP_vpcmpeqq      */  &e_vex_extensions[11][1],\n    /* OP_vmovntdqa     */  &e_vex_extensions[12][1],\n    /* OP_vpackusdw     */  &e_vex_extensions[13][1],\n    /* OP_vpmovzxbw     */  &e_vex_extensions[14][1],\n    /* OP_vpmovzxbd     */  &e_vex_extensions[15][1],\n    /* OP_vpmovzxbq     */  &e_vex_extensions[16][1],\n    /* OP_vpmovzxwd     */  &e_vex_extensions[17][1],\n    /* OP_vpmovzxwq     */  &e_vex_extensions[18][1],\n    /* OP_vpmovzxdq     */  &e_vex_extensions[19][1],\n    /* OP_vpcmpgtq      */  &e_vex_extensions[20][1],\n    /* OP_vpminsb       */  &e_vex_extensions[21][1],\n    /* OP_vpminsd       */  &e_vex_extensions[22][1],\n    /* OP_vpminuw       */  &e_vex_extensions[23][1],\n    /* OP_vpminud       */  &e_vex_extensions[24][1],\n    /* OP_vpmaxsb       */  &e_vex_extensions[25][1],\n    /* OP_vpmaxsd       */  &e_vex_extensions[26][1],\n    /* OP_vpmaxuw       */  &e_vex_extensions[27][1],\n    /* OP_vpmaxud       */  &e_vex_extensions[28][1],\n    /* OP_vpmulld       */  &e_vex_extensions[29][1],\n    /* OP_vphminposuw   */  &e_vex_extensions[30][1],\n    /* OP_vaesimc       */  &e_vex_extensions[31][1],\n    /* OP_vaesenc       */  &e_vex_extensions[32][1],\n    /* OP_vaesenclast   */  &e_vex_extensions[33][1],\n    /* OP_vaesdec       */  &e_vex_extensions[34][1],\n    /* OP_vaesdeclast   */  &e_vex_extensions[35][1],\n    /* OP_vpextrb       */  &e_vex_extensions[36][1],\n    /* OP_vpextrd       */  &e_vex_extensions[38][1],\n    /* OP_vextractps    */  &e_vex_extensions[39][1],\n    /* OP_vroundps      */  &e_vex_extensions[40][1],\n    /* OP_vroundpd      */  &e_vex_extensions[41][1],\n    /* OP_vroundss      */  &e_vex_extensions[42][1],\n    /* OP_vroundsd      */  &e_vex_extensions[43][1],\n    /* OP_vblendps      */  &e_vex_extensions[44][1],\n    /* OP_vblendpd      */  &e_vex_extensions[45][1],\n    /* OP_vpblendw      */  &e_vex_extensions[46][1],\n    /* OP_vpinsrb       */  &e_vex_extensions[47][1],\n    /* OP_vinsertps     */  &e_vex_extensions[48][1],\n    /* OP_vpinsrd       */  &e_vex_extensions[49][1],\n    /* OP_vdpps         */  &e_vex_extensions[50][1],\n    /* OP_vdppd         */  &e_vex_extensions[51][1],\n    /* OP_vmpsadbw      */  &e_vex_extensions[52][1],\n    /* OP_vpcmpestrm    */  &e_vex_extensions[53][1],\n    /* OP_vpcmpestri    */  &e_vex_extensions[54][1],\n    /* OP_vpcmpistrm    */  &e_vex_extensions[55][1],\n    /* OP_vpcmpistri    */  &e_vex_extensions[56][1],\n    /* OP_vpclmulqdq    */  &e_vex_extensions[57][1],\n    /* OP_vaeskeygenassist*/ &e_vex_extensions[58][1],\n    /* OP_vtestps       */  &e_vex_extensions[59][1],\n    /* OP_vtestpd       */  &e_vex_extensions[60][1],\n    /* OP_vzeroupper    */  &vex_L_extensions[0][1],\n    /* OP_vzeroall      */  &vex_L_extensions[0][2],\n    /* OP_vldmxcsr      */  &e_vex_extensions[61][1],\n    /* OP_vstmxcsr      */  &e_vex_extensions[62][1],\n    /* OP_vbroadcastss  */  &e_vex_extensions[64][1],\n    /* OP_vbroadcastsd  */  &e_vex_extensions[65][1],\n    /* OP_vbroadcastf128*/  &e_vex_extensions[66][1],\n    /* OP_vmaskmovps    */  &e_vex_extensions[67][1],\n    /* OP_vmaskmovpd    */  &e_vex_extensions[68][1],\n    /* OP_vpermilps     */  &e_vex_extensions[71][1],\n    /* OP_vpermilpd     */  &e_vex_extensions[72][1],\n    /* OP_vperm2f128    */  &e_vex_extensions[73][1],\n    /* OP_vinsertf128   */  &e_vex_extensions[74][1],\n    /* OP_vextractf128  */  &e_vex_extensions[75][1],\n\n    /* added in Ivy Bridge I believe, and covered by F16C cpuid flag */\n    /* OP_vcvtph2ps     */  &e_vex_extensions[63][1],\n    /* OP_vcvtps2ph     */  &e_vex_extensions[76][1],\n\n    /* FMA */\n    /* OP_vfmadd132ps   */  &vex_W_extensions[ 0][0],\n    /* OP_vfmadd132pd   */  &vex_W_extensions[ 0][1],\n    /* OP_vfmadd213ps   */  &vex_W_extensions[ 1][0],\n    /* OP_vfmadd213pd   */  &vex_W_extensions[ 1][1],\n    /* OP_vfmadd231ps   */  &vex_W_extensions[ 2][0],\n    /* OP_vfmadd231pd   */  &vex_W_extensions[ 2][1],\n    /* OP_vfmadd132ss   */  &vex_W_extensions[ 3][0],\n    /* OP_vfmadd132sd   */  &vex_W_extensions[ 3][1],\n    /* OP_vfmadd213ss   */  &vex_W_extensions[ 4][0],\n    /* OP_vfmadd213sd   */  &vex_W_extensions[ 4][1],\n    /* OP_vfmadd231ss   */  &vex_W_extensions[ 5][0],\n    /* OP_vfmadd231sd   */  &vex_W_extensions[ 5][1],\n    /* OP_vfmaddsub132ps*/  &vex_W_extensions[ 6][0],\n    /* OP_vfmaddsub132pd*/  &vex_W_extensions[ 6][1],\n    /* OP_vfmaddsub213ps*/  &vex_W_extensions[ 7][0],\n    /* OP_vfmaddsub213pd*/  &vex_W_extensions[ 7][1],\n    /* OP_vfmaddsub231ps*/  &vex_W_extensions[ 8][0],\n    /* OP_vfmaddsub231pd*/  &vex_W_extensions[ 8][1],\n    /* OP_vfmsubadd132ps*/  &vex_W_extensions[ 9][0],\n    /* OP_vfmsubadd132pd*/  &vex_W_extensions[ 9][1],\n    /* OP_vfmsubadd213ps*/  &vex_W_extensions[10][0],\n    /* OP_vfmsubadd213pd*/  &vex_W_extensions[10][1],\n    /* OP_vfmsubadd231ps*/  &vex_W_extensions[11][0],\n    /* OP_vfmsubadd231pd*/  &vex_W_extensions[11][1],\n    /* OP_vfmsub132ps   */  &vex_W_extensions[12][0],\n    /* OP_vfmsub132pd   */  &vex_W_extensions[12][1],\n    /* OP_vfmsub213ps   */  &vex_W_extensions[13][0],\n    /* OP_vfmsub213pd   */  &vex_W_extensions[13][1],\n    /* OP_vfmsub231ps   */  &vex_W_extensions[14][0],\n    /* OP_vfmsub231pd   */  &vex_W_extensions[14][1],\n    /* OP_vfmsub132ss   */  &vex_W_extensions[15][0],\n    /* OP_vfmsub132sd   */  &vex_W_extensions[15][1],\n    /* OP_vfmsub213ss   */  &vex_W_extensions[16][0],\n    /* OP_vfmsub213sd   */  &vex_W_extensions[16][1],\n    /* OP_vfmsub231ss   */  &vex_W_extensions[17][0],\n    /* OP_vfmsub231sd   */  &vex_W_extensions[17][1],\n    /* OP_vfnmadd132ps  */  &vex_W_extensions[18][0],\n    /* OP_vfnmadd132pd  */  &vex_W_extensions[18][1],\n    /* OP_vfnmadd213ps  */  &vex_W_extensions[19][0],\n    /* OP_vfnmadd213pd  */  &vex_W_extensions[19][1],\n    /* OP_vfnmadd231ps  */  &vex_W_extensions[20][0],\n    /* OP_vfnmadd231pd  */  &vex_W_extensions[20][1],\n    /* OP_vfnmadd132ss  */  &vex_W_extensions[21][0],\n    /* OP_vfnmadd132sd  */  &vex_W_extensions[21][1],\n    /* OP_vfnmadd213ss  */  &vex_W_extensions[22][0],\n    /* OP_vfnmadd213sd  */  &vex_W_extensions[22][1],\n    /* OP_vfnmadd231ss  */  &vex_W_extensions[23][0],\n    /* OP_vfnmadd231sd  */  &vex_W_extensions[23][1],\n    /* OP_vfnmsub132ps  */  &vex_W_extensions[24][0],\n    /* OP_vfnmsub132pd  */  &vex_W_extensions[24][1],\n    /* OP_vfnmsub213ps  */  &vex_W_extensions[25][0],\n    /* OP_vfnmsub213pd  */  &vex_W_extensions[25][1],\n    /* OP_vfnmsub231ps  */  &vex_W_extensions[26][0],\n    /* OP_vfnmsub231pd  */  &vex_W_extensions[26][1],\n    /* OP_vfnmsub132ss  */  &vex_W_extensions[27][0],\n    /* OP_vfnmsub132sd  */  &vex_W_extensions[27][1],\n    /* OP_vfnmsub213ss  */  &vex_W_extensions[28][0],\n    /* OP_vfnmsub213sd  */  &vex_W_extensions[28][1],\n    /* OP_vfnmsub231ss  */  &vex_W_extensions[29][0],\n    /* OP_vfnmsub231sd  */  &vex_W_extensions[29][1],\n\n    /* SSE2 that were omitted before */\n    /* OP_movq2dq       */  &prefix_extensions[61][1],\n    /* OP_movdq2q       */  &prefix_extensions[61][3],\n\n    /* OP_fxsave64      */   &rex_w_extensions[0][1],\n    /* OP_fxrstor64     */   &rex_w_extensions[1][1],\n    /* OP_xsave64       */   &rex_w_extensions[2][1],\n    /* OP_xrstor64      */   &rex_w_extensions[3][1],\n    /* OP_xsaveopt64    */   &rex_w_extensions[4][1],\n\n    /* added in Intel Ivy Bridge: RDRAND and FSGSBASE cpuid flags */\n    /* OP_rdrand        */   &mod_extensions[12][1],\n    /* OP_rdfsbase      */   &mod_extensions[14][1],\n    /* OP_rdgsbase      */   &mod_extensions[15][1],\n    /* OP_wrfsbase      */   &mod_extensions[16][1],\n    /* OP_wrgsbase      */   &mod_extensions[17][1],\n\n    /* coming in the future but adding now since enough details are known */\n    /* OP_rdseed        */   &mod_extensions[13][1],\n\n    /* AMD FMA4 */\n    /* OP_vfmaddsubps   */   &vex_W_extensions[30][0],\n    /* OP_vfmaddsubpd   */   &vex_W_extensions[31][0],\n    /* OP_vfmsubaddps   */   &vex_W_extensions[32][0],\n    /* OP_vfmsubaddpd   */   &vex_W_extensions[33][0],\n    /* OP_vfmaddps      */   &vex_W_extensions[34][0],\n    /* OP_vfmaddpd      */   &vex_W_extensions[35][0],\n    /* OP_vfmaddss      */   &vex_W_extensions[36][0],\n    /* OP_vfmaddsd      */   &vex_W_extensions[37][0],\n    /* OP_vfmsubps      */   &vex_W_extensions[38][0],\n    /* OP_vfmsubpd      */   &vex_W_extensions[39][0],\n    /* OP_vfmsubss      */   &vex_W_extensions[40][0],\n    /* OP_vfmsubsd      */   &vex_W_extensions[41][0],\n    /* OP_vfnmaddps     */   &vex_W_extensions[42][0],\n    /* OP_vfnmaddpd     */   &vex_W_extensions[43][0],\n    /* OP_vfnmaddss     */   &vex_W_extensions[44][0],\n    /* OP_vfnmaddsd     */   &vex_W_extensions[45][0],\n    /* OP_vfnmsubps     */   &vex_W_extensions[46][0],\n    /* OP_vfnmsubpd     */   &vex_W_extensions[47][0],\n    /* OP_vfnmsubss     */   &vex_W_extensions[48][0],\n    /* OP_vfnmsubsd     */   &vex_W_extensions[49][0],\n\n    /* AMD XOP */\n    /* OP_vfrczps       */   &xop_extensions[27],\n    /* OP_vfrczpd       */   &xop_extensions[28],\n    /* OP_vfrczss       */   &xop_extensions[29],\n    /* OP_vfrczsd       */   &xop_extensions[30],\n    /* OP_vpcmov        */   &vex_W_extensions[50][0],\n    /* OP_vpcomb        */   &xop_extensions[19],\n    /* OP_vpcomw        */   &xop_extensions[20],\n    /* OP_vpcomd        */   &xop_extensions[21],\n    /* OP_vpcomq        */   &xop_extensions[22],\n    /* OP_vpcomub       */   &xop_extensions[23],\n    /* OP_vpcomuw       */   &xop_extensions[24],\n    /* OP_vpcomud       */   &xop_extensions[25],\n    /* OP_vpcomuq       */   &xop_extensions[26],\n    /* OP_vpermil2pd    */   &vex_W_extensions[65][0],\n    /* OP_vpermil2ps    */   &vex_W_extensions[64][0],\n    /* OP_vphaddbw      */   &xop_extensions[43],\n    /* OP_vphaddbd      */   &xop_extensions[44],\n    /* OP_vphaddbq      */   &xop_extensions[45],\n    /* OP_vphaddwd      */   &xop_extensions[46],\n    /* OP_vphaddwq      */   &xop_extensions[47],\n    /* OP_vphadddq      */   &xop_extensions[48],\n    /* OP_vphaddubw     */   &xop_extensions[49],\n    /* OP_vphaddubd     */   &xop_extensions[50],\n    /* OP_vphaddubq     */   &xop_extensions[51],\n    /* OP_vphadduwd     */   &xop_extensions[52],\n    /* OP_vphadduwq     */   &xop_extensions[53],\n    /* OP_vphaddudq     */   &xop_extensions[54],\n    /* OP_vphsubbw      */   &xop_extensions[55],\n    /* OP_vphsubwd      */   &xop_extensions[56],\n    /* OP_vphsubdq      */   &xop_extensions[57],\n    /* OP_vpmacssww     */   &xop_extensions[ 1],\n    /* OP_vpmacsswd     */   &xop_extensions[ 2],\n    /* OP_vpmacssdql    */   &xop_extensions[ 3],\n    /* OP_vpmacssdd     */   &xop_extensions[ 4],\n    /* OP_vpmacssdqh    */   &xop_extensions[ 5],\n    /* OP_vpmacsww      */   &xop_extensions[ 6],\n    /* OP_vpmacswd      */   &xop_extensions[ 7],\n    /* OP_vpmacsdql     */   &xop_extensions[ 8],\n    /* OP_vpmacsdd      */   &xop_extensions[ 9],\n    /* OP_vpmacsdqh     */   &xop_extensions[10],\n    /* OP_vpmadcsswd    */   &xop_extensions[13],\n    /* OP_vpmadcswd     */   &xop_extensions[14],\n    /* OP_vpperm        */   &vex_W_extensions[51][0],\n    /* OP_vprotb        */   &xop_extensions[15],\n    /* OP_vprotw        */   &xop_extensions[16],\n    /* OP_vprotd        */   &xop_extensions[17],\n    /* OP_vprotq        */   &xop_extensions[18],\n    /* OP_vpshlb        */   &vex_W_extensions[56][0],\n    /* OP_vpshlw        */   &vex_W_extensions[57][0],\n    /* OP_vpshld        */   &vex_W_extensions[58][0],\n    /* OP_vpshlq        */   &vex_W_extensions[59][0],\n    /* OP_vpshab        */   &vex_W_extensions[60][0],\n    /* OP_vpshaw        */   &vex_W_extensions[61][0],\n    /* OP_vpshad        */   &vex_W_extensions[62][0],\n    /* OP_vpshaq        */   &vex_W_extensions[63][0],\n\n    /* AMD TBM */\n    /* OP_bextr         */   &prefix_extensions[141][4],\n    /* OP_blcfill       */   &base_extensions[27][1],\n    /* OP_blci          */   &base_extensions[28][6],\n    /* OP_blcic         */   &base_extensions[27][5],\n    /* OP_blcmsk        */   &base_extensions[28][1],\n    /* OP_blcs          */   &base_extensions[27][3],\n    /* OP_blsfill       */   &base_extensions[27][2],\n    /* OP_blsic         */   &base_extensions[27][6],\n    /* OP_t1mskc        */   &base_extensions[27][7],\n    /* OP_tzmsk         */   &base_extensions[27][4],\n\n    /* AMD LWP */\n    /* OP_llwpcb        */   &base_extensions[29][0],\n    /* OP_slwpcb        */   &base_extensions[29][1],\n    /* OP_lwpins        */   &base_extensions[30][0],\n    /* OP_lwpval        */   &base_extensions[30][1],\n\n    /* Intel BMI1 */\n    /* (includes non-immed form of OP_bextr) */\n    /* OP_andn          */   &third_byte_38[100],\n    /* OP_blsr          */   &base_extensions[31][1],\n    /* OP_blsmsk        */   &base_extensions[31][2],\n    /* OP_blsi          */   &base_extensions[31][3],\n    /* OP_tzcnt         */   &prefix_extensions[140][1],\n\n    /* Intel BMI2 */\n    /* OP_bzhi          */   &prefix_extensions[142][4],\n    /* OP_pext          */   &prefix_extensions[142][6],\n    /* OP_pdep          */   &prefix_extensions[142][7],\n    /* OP_sarx          */   &prefix_extensions[141][5],\n    /* OP_shlx          */   &prefix_extensions[141][6],\n    /* OP_shrx          */   &prefix_extensions[141][7],\n    /* OP_rorx          */   &third_byte_3a[56],\n    /* OP_mulx          */   &prefix_extensions[143][7],\n\n    /* Intel Safer Mode Extensions */\n    /* OP_getsec        */   &second_byte[0x37],\n\n    /* Misc Intel additions */\n    /* OP_vmfunc        */   &rm_extensions[4][4],\n    /* OP_invpcid       */   &third_byte_38[103],\n\n    /* Intel TSX */\n    /* OP_xabort        */   &base_extensions[17][7],\n    /* OP_xbegin        */   &base_extensions[18][7],\n    /* OP_xend          */   &rm_extensions[4][5],\n    /* OP_xtest         */   &rm_extensions[4][6],\n\n    /* AVX2 */\n    /* OP_vpgatherdd    */   &vex_W_extensions[66][0],\n    /* OP_vpgatherdq    */   &vex_W_extensions[66][1],\n    /* OP_vpgatherqd    */   &vex_W_extensions[67][0],\n    /* OP_vpgatherqq    */   &vex_W_extensions[67][1],\n    /* OP_vgatherdps    */   &vex_W_extensions[68][0],\n    /* OP_vgatherdpd    */   &vex_W_extensions[68][1],\n    /* OP_vgatherqps    */   &vex_W_extensions[69][0],\n    /* OP_vgatherqpd    */   &vex_W_extensions[69][1],\n    /* OP_vbroadcasti128 */  &third_byte_38[108],\n    /* OP_vinserti128   */   &third_byte_3a[57],\n    /* OP_vextracti128  */   &third_byte_3a[58],\n    /* OP_vpmaskmovd    */   &vex_W_extensions[70][0],\n    /* OP_vpmaskmovq    */   &vex_W_extensions[70][1],\n    /* OP_vperm2i128    */   &third_byte_3a[62],\n    /* OP_vpermd        */   &third_byte_38[112],\n    /* OP_vpermps       */   &third_byte_38[111],\n    /* OP_vpermq        */   &third_byte_3a[59],\n    /* OP_vpermpd       */   &third_byte_3a[60],\n    /* OP_vpblendd      */   &third_byte_3a[61],\n    /* OP_vpsllvd       */   &vex_W_extensions[73][0],\n    /* OP_vpsllvq       */   &vex_W_extensions[73][1],\n    /* OP_vpsravd       */   &third_byte_38[114],\n    /* OP_vpsrlvd       */   &vex_W_extensions[72][0],\n    /* OP_vpsrlvq       */   &vex_W_extensions[72][1],\n    /* OP_vpbroadcastb  */   &third_byte_38[116],\n    /* OP_vpbroadcastw  */   &third_byte_38[117],\n    /* OP_vpbroadcastd  */   &third_byte_38[118],\n    /* OP_vpbroadcastq  */   &third_byte_38[119],\n\n    /* added in Intel Skylake */\n    /* OP_xsavec32      */   &rex_w_extensions[5][0],\n    /* OP_xsavec64      */   &rex_w_extensions[5][1],\n\n    /* Intel ADX */\n    /* OP_adox          */   &prefix_extensions[143][1],\n    /* OP_adcx          */   &prefix_extensions[143][2],\n\n    /* AVX-512 VEX encoded (scalar opmask instructions) */\n    /* OP_kmovw         */  &vex_W_extensions[74][0],\n    /* OP_kmovb         */  &vex_W_extensions[75][0],\n    /* OP_kmovq         */  &vex_W_extensions[74][1],\n    /* OP_kmovd         */  &vex_W_extensions[75][1],\n    /* OP_kandw         */  &vex_W_extensions[82][0],\n    /* OP_kandb         */  &vex_W_extensions[83][0],\n    /* OP_kandq         */  &vex_W_extensions[82][1],\n    /* OP_kandd         */  &vex_W_extensions[83][1],\n    /* OP_kandnw        */  &vex_W_extensions[84][0],\n    /* OP_kandnb        */  &vex_W_extensions[85][0],\n    /* OP_kandnq        */  &vex_W_extensions[84][1],\n    /* OP_kandnd        */  &vex_W_extensions[85][1],\n    /* OP_kunpckbw      */  &vex_W_extensions[87][0],\n    /* OP_kunpckwd      */  &vex_W_extensions[86][0],\n    /* OP_kunpckdq      */  &vex_W_extensions[86][1],\n    /* OP_knotw         */  &vex_W_extensions[88][0],\n    /* OP_knotb         */  &vex_W_extensions[89][0],\n    /* OP_knotq         */  &vex_W_extensions[88][1],\n    /* OP_knotd         */  &vex_W_extensions[89][1],\n    /* OP_korw          */  &vex_W_extensions[90][0],\n    /* OP_korb          */  &vex_W_extensions[91][0],\n    /* OP_korq          */  &vex_W_extensions[90][1],\n    /* OP_kord          */  &vex_W_extensions[91][1],\n    /* OP_kxnorw        */  &vex_W_extensions[92][0],\n    /* OP_kxnorb        */  &vex_W_extensions[93][0],\n    /* OP_kxnorq        */  &vex_W_extensions[92][1],\n    /* OP_kxnord        */  &vex_W_extensions[93][1],\n    /* OP_kxorw         */  &vex_W_extensions[94][0],\n    /* OP_kxorb         */  &vex_W_extensions[95][0],\n    /* OP_kxorq         */  &vex_W_extensions[94][1],\n    /* OP_kxord         */  &vex_W_extensions[95][1],\n    /* OP_kaddw         */  &vex_W_extensions[96][0],\n    /* OP_kaddb         */  &vex_W_extensions[97][0],\n    /* OP_kaddq         */  &vex_W_extensions[96][1],\n    /* OP_kaddd         */  &vex_W_extensions[97][1],\n    /* OP_kortestw      */  &vex_W_extensions[98][0],\n    /* OP_kortestb      */  &vex_W_extensions[99][0],\n    /* OP_kortestq      */  &vex_W_extensions[98][1],\n    /* OP_kortestd      */  &vex_W_extensions[99][1],\n    /* OP_kshiftlw      */  &vex_W_extensions[100][1],\n    /* OP_kshiftlb      */  &vex_W_extensions[100][0],\n    /* OP_kshiftlq      */  &vex_W_extensions[101][1],\n    /* OP_kshiftld      */  &vex_W_extensions[101][0],\n    /* OP_kshiftrw      */  &vex_W_extensions[102][1],\n    /* OP_kshiftrb      */  &vex_W_extensions[102][0],\n    /* OP_kshiftrq      */  &vex_W_extensions[103][1],\n    /* OP_kshiftrd      */  &vex_W_extensions[103][0],\n    /* OP_ktestw        */  &vex_W_extensions[104][0],\n    /* OP_ktestb        */  &vex_W_extensions[105][0],\n    /* OP_ktestq        */  &vex_W_extensions[104][1],\n    /* OP_ktestd        */  &vex_W_extensions[105][1],\n\n    /* AVX-512 EVEX encoded */\n    /* OP_vmovdqa32     */  &evex_W_extensions[8][0],\n    /* OP_vmovdqa64     */  &evex_W_extensions[8][1],\n    /* OP_vmovdqu8      */  &evex_W_extensions[10][0],\n    /* OP_vmovdqu16     */  &evex_W_extensions[10][1],\n    /* OP_vmovdqu32     */  &evex_W_extensions[11][0],\n    /* OP_vmovdqu64     */  &evex_W_extensions[11][1],\n    /* OP_vpandd        */  &evex_W_extensions[41][0],\n    /* OP_vpandq        */  &evex_W_extensions[41][1],\n    /* OP_vpandnd       */  &evex_W_extensions[42][0],\n    /* OP_vpandnq       */  &evex_W_extensions[42][1],\n    /* OP_vpmullq        */  &evex_W_extensions[45][1],\n    /* OP_vpord         */  &evex_W_extensions[43][0],\n    /* OP_vporq         */  &evex_W_extensions[43][1],\n    /* OP_vpxord        */  &evex_W_extensions[44][0],\n    /* OP_vpxorq        */  &evex_W_extensions[44][1],\n    /* TODO i#1312. */\n};\n\n\n/****************************************************************************\n * Macros to make tables legible\n */\n\n/* Jb is defined in dynamo.h, undefine it for this file */\n#undef Jb\n\n#define xx  TYPE_NONE, OPSZ_NA\n\n/* from Intel tables, using our corresponding OPSZ constants */\n#define Ap  TYPE_A, OPSZ_6_irex10_short4 /* NOTE - not legal for 64-bit instructions */\n#define By  TYPE_B, OPSZ_4_rex8\n#define Cr  TYPE_C, OPSZ_4x8\n#define Dr  TYPE_D, OPSZ_4x8\n#define Eb  TYPE_E, OPSZ_1\n#define Ew  TYPE_E, OPSZ_2\n#define Ev  TYPE_E, OPSZ_4_rex8_short2\n#define Esv TYPE_E, OPSZ_4x8_short2 /* \"stack v\", or \"d64\" in Intel tables */\n#define Ed  TYPE_E, OPSZ_4\n#define Ep  TYPE_E, OPSZ_6_irex10_short4\n#define Ed_q TYPE_E, OPSZ_4_rex8\n#define Ey  TYPE_E, OPSZ_4_rex8\n#define Rd_Mb TYPE_E, OPSZ_1_reg4\n#define Rd_Mw TYPE_E, OPSZ_2_reg4\n#define Gb  TYPE_G, OPSZ_1\n#define Gw  TYPE_G, OPSZ_2\n#define Gv  TYPE_G, OPSZ_4_rex8_short2\n#define Gz  TYPE_G, OPSZ_4_short2\n#define Gd  TYPE_G, OPSZ_4\n#define Gd_q TYPE_G, OPSZ_4_rex8\n#define Gr  TYPE_G, OPSZ_4x8\n#define Gy  TYPE_G, OPSZ_4_rex8\n#define Ib  TYPE_I, OPSZ_1\n#define Iw  TYPE_I, OPSZ_2\n#define Id  TYPE_I, OPSZ_4\n#define Iv  TYPE_I, OPSZ_4_rex8_short2\n#define Iz  TYPE_I, OPSZ_4_short2\n#define Jb  TYPE_J, OPSZ_1\n#define Jz  TYPE_J, OPSZ_4_short2xi4\n#define Ma  TYPE_M, OPSZ_8_short4\n#define Mp  TYPE_M, OPSZ_6_irex10_short4\n#define Ms  TYPE_M, OPSZ_6x10\n#define Ob  TYPE_O, OPSZ_1\n#define Ov  TYPE_O, OPSZ_4_rex8_short2\n#define Pd  TYPE_P, OPSZ_4\n#define Pq  TYPE_P, OPSZ_8\n#define Pw_q TYPE_P, OPSZ_2_of_8\n#define Pd_q TYPE_P, OPSZ_4_of_8\n#define Ppi TYPE_P, OPSZ_8\n#define Nw_q  TYPE_P_MODRM, OPSZ_2_of_8\n#define Nq  TYPE_P_MODRM, OPSZ_8\n#define Qd  TYPE_Q, OPSZ_4\n#define Qq  TYPE_Q, OPSZ_8\n#define Qpi TYPE_Q, OPSZ_8\n#define Rr  TYPE_R, OPSZ_4x8\n#define Rv  TYPE_R, OPSZ_4_rex8_short2\n#define Ry  TYPE_R, OPSZ_4_rex8\n#define Sw  TYPE_S, OPSZ_2\n#define Vq  TYPE_V, OPSZ_8\n#define Vdq TYPE_V, OPSZ_16\n#define Vb_dq TYPE_V, OPSZ_1_of_16\n#define Vw_dq TYPE_V, OPSZ_2_of_16\n#define Vd_dq TYPE_V, OPSZ_4_of_16\n#define Vd_q_dq TYPE_V, OPSZ_4_rex8_of_16\n#define Vq_dq TYPE_V, OPSZ_8_of_16\n#define Vps TYPE_V, OPSZ_16\n#define Vpd TYPE_V, OPSZ_16\n#define Vss TYPE_V, OPSZ_4_of_16\n#define Vsd TYPE_V, OPSZ_8_of_16\n#define Ups TYPE_V_MODRM, OPSZ_16\n#define Upd TYPE_V_MODRM, OPSZ_16\n#define Udq TYPE_V_MODRM, OPSZ_16\n#define Uw_dq TYPE_V_MODRM, OPSZ_2_of_16\n#define Uq_dq TYPE_V_MODRM, OPSZ_8_of_16\n#define Wq  TYPE_W, OPSZ_8\n#define Wdq TYPE_W, OPSZ_16\n#define Wb_dq TYPE_W, OPSZ_1_of_16\n#define Ww_dq TYPE_W, OPSZ_2_of_16\n#define Wd_dq TYPE_W, OPSZ_4_of_16\n#define Wq_dq TYPE_W, OPSZ_8_of_16\n#define Wps TYPE_W, OPSZ_16\n#define Wpd TYPE_W, OPSZ_16\n#define Wss TYPE_W, OPSZ_4_of_16\n#define Wsd TYPE_W, OPSZ_8_of_16\n#define Udq_Md TYPE_W, OPSZ_4_reg16\n#define Xb  TYPE_X, OPSZ_1\n#define Xv  TYPE_X, OPSZ_4_rex8_short2\n#define Xz  TYPE_X, OPSZ_4_short2\n#define Yb  TYPE_Y, OPSZ_1\n#define Yv  TYPE_Y, OPSZ_4_rex8_short2\n#define Yz  TYPE_Y, OPSZ_4_short2\n\n/* AVX additions */\n#define Vvs TYPE_V, OPSZ_16_vex32\n#define Vvd TYPE_V, OPSZ_16_vex32\n#define Vx TYPE_V, OPSZ_16_vex32\n#define Vqq TYPE_V, OPSZ_32\n#define Vdq_qq TYPE_V, OPSZ_16_of_32\n#define Wvs TYPE_W, OPSZ_16_vex32\n#define Wvd TYPE_W, OPSZ_16_vex32\n#define Wx TYPE_W, OPSZ_16_vex32\n#define Uvs TYPE_V_MODRM, OPSZ_16_vex32\n#define Uvd TYPE_V_MODRM, OPSZ_16_vex32\n#define Uss TYPE_V_MODRM, OPSZ_4_of_16\n#define Usd TYPE_V_MODRM, OPSZ_8_of_16\n#define Ux TYPE_V_MODRM, OPSZ_16_vex32\n#define Udq TYPE_V_MODRM, OPSZ_16\n#define Hvs TYPE_H, OPSZ_16_vex32\n#define Hvd TYPE_H, OPSZ_16_vex32\n#define Hss TYPE_H, OPSZ_4_of_16\n#define Hsd TYPE_H, OPSZ_8_of_16\n#define Hq_dq TYPE_H, OPSZ_8_of_16\n#define Hdq TYPE_H, OPSZ_16\n#define H12_dq TYPE_H, OPSZ_12_of_16\n#define H12_8_dq TYPE_H, OPSZ_12_rex8_of_16\n#define H14_dq TYPE_H, OPSZ_14_of_16\n#define H15_dq TYPE_H, OPSZ_15_of_16\n#define Hqq TYPE_H, OPSZ_32\n#define Hx TYPE_H, OPSZ_16_vex32\n#define Hh_x TYPE_H, OPSZ_half_16_vex32\n#define Wvq_dq TYPE_W, OPSZ_8_of_16_vex32\n#define Wh_x TYPE_W, OPSZ_half_16_vex32\n#define Wqq TYPE_W, OPSZ_32\n#define Mvs TYPE_M, OPSZ_16_vex32\n#define Mvd TYPE_M, OPSZ_16_vex32\n#define Mx TYPE_M, OPSZ_16_vex32\n#define Ldq TYPE_L, OPSZ_16 /* immed is 1 byte but reg is xmm */\n#define Lx TYPE_L, OPSZ_16_vex32 /* immed is 1 byte but reg is xmm/ymm */\n#define Lvs TYPE_L, OPSZ_16_vex32 /* immed is 1 byte but reg is xmm/ymm */\n#define Lss TYPE_L, OPSZ_4_of_16 /* immed is 1 byte but reg is xmm/ymm */\n#define Lsd TYPE_L, OPSZ_8_of_16 /* immed is 1 byte but reg is xmm/ymm */\n\n/* AVX-512 additions */\n#define KPb TYPE_K_REG, OPSZ_1\n#define KPw TYPE_K_REG, OPSZ_2\n#define KPd TYPE_K_REG, OPSZ_4\n#define KPq TYPE_K_REG, OPSZ_8\n#define KRb TYPE_K_MODRM_R, OPSZ_1\n#define KRw TYPE_K_MODRM_R, OPSZ_2\n#define KRd TYPE_K_MODRM_R, OPSZ_4\n#define KRq TYPE_K_MODRM_R, OPSZ_8\n#define KQb TYPE_K_MODRM, OPSZ_1\n#define KQw TYPE_K_MODRM, OPSZ_2\n#define KQd TYPE_K_MODRM, OPSZ_4\n#define KQq TYPE_K_MODRM, OPSZ_8\n#define KVb TYPE_K_VEX, OPSZ_1\n#define KVw TYPE_K_VEX, OPSZ_2\n#define KVd TYPE_K_VEX, OPSZ_4\n#define KVq TYPE_K_VEX, OPSZ_8\n#define KEb TYPE_K_EVEX, OPSZ_1\n#define KEw TYPE_K_EVEX, OPSZ_2\n#define KEd TYPE_K_EVEX, OPSZ_4\n#define KEq TYPE_K_EVEX, OPSZ_8\n#define Ves TYPE_V, OPSZ_16_vex32_evex64\n#define Ved TYPE_V, OPSZ_16_vex32_evex64\n#define Wes TYPE_W, OPSZ_16_vex32_evex64\n#define Wed TYPE_W, OPSZ_16_vex32_evex64\n#define We TYPE_W, OPSZ_16_vex32_evex64\n#define Ve TYPE_V, OPSZ_16_vex32_evex64\n#define We TYPE_W, OPSZ_16_vex32_evex64\n#define Wh_e TYPE_W, OPSZ_half_16_vex32_evex64\n#define Hes TYPE_H, OPSZ_16_vex32_evex64\n#define Hed TYPE_H, OPSZ_16_vex32_evex64\n#define He TYPE_H, OPSZ_16_vex32_evex64\n#define Hh_e TYPE_H, OPSZ_half_16_vex32_evex64\n#define Mes TYPE_M, OPSZ_16_vex32_evex64\n#define Med TYPE_M, OPSZ_16_vex32_evex64\n\n/* my own codes\n * size m = 32 or 16 bit depending on addr size attribute\n * B=ds:eDI, Z=xlat's mem, K=float in mem, i_==indirect\n */\n#define Mb  TYPE_M, OPSZ_1\n#define Md  TYPE_M, OPSZ_4\n#define Md_q  TYPE_M, OPSZ_4_rex8\n#define Mw  TYPE_M, OPSZ_2\n#define Mm  TYPE_M, OPSZ_lea\n#define Me  TYPE_M, OPSZ_512\n#define Mxsave TYPE_M, OPSZ_xsave\n#define Mps  TYPE_M, OPSZ_16\n#define Mpd  TYPE_M, OPSZ_16\n#define Mss  TYPE_M, OPSZ_4\n#define Msd  TYPE_M, OPSZ_8\n#define Mq  TYPE_M, OPSZ_8\n#define Mdq  TYPE_M, OPSZ_16\n#define Mq_dq TYPE_M, OPSZ_8_rex16\n#define Mv  TYPE_M, OPSZ_4_rex8_short2\n#define MVd TYPE_VSIB, OPSZ_4\n#define MVq TYPE_VSIB, OPSZ_8\n#define Zb  TYPE_XLAT, OPSZ_1\n#define Bq  TYPE_MASKMOVQ, OPSZ_8\n#define Bdq  TYPE_MASKMOVQ, OPSZ_16\n#define Fw  TYPE_FLOATMEM, OPSZ_2\n#define Fd  TYPE_FLOATMEM, OPSZ_4\n#define Fq  TYPE_FLOATMEM, OPSZ_8\n#define Fx  TYPE_FLOATMEM, OPSZ_10\n#define Fy  TYPE_FLOATMEM, OPSZ_28_short14 /* _14_ if data16 */\n#define Fz  TYPE_FLOATMEM, OPSZ_108_short94 /* _98_ if data16 */\n#define i_dx  TYPE_INDIR_REG, REG_DX\n#define i_Ev  TYPE_INDIR_E, OPSZ_4_rex8_short2\n#define i_Exi  TYPE_INDIR_E, OPSZ_4x8_short2xi8\n#define i_Ep  TYPE_INDIR_E, OPSZ_6_irex10_short4\n#define i_xSP TYPE_INDIR_VAR_XREG, REG_ESP\n#define i_iSP TYPE_INDIR_VAR_XIREG, REG_ESP\n#define i_xBP TYPE_INDIR_VAR_XREG, REG_EBP\n/* negative offset from (%xsp) for pushes */\n#define i_iSPo1 TYPE_INDIR_VAR_XIREG_OFFS_1, REG_ESP\n#define i_vSPo2 TYPE_INDIR_VAR_REG_OFFS_2, REG_ESP\n#define i_xSPo1 TYPE_INDIR_VAR_XREG_OFFS_1, REG_ESP\n#define i_xSPo8 TYPE_INDIR_VAR_XREG_OFFS_8, REG_ESP\n#define i_xSPs8 TYPE_INDIR_VAR_XREG_SIZEx8, REG_ESP\n#define i_vSPs2 TYPE_INDIR_VAR_REG_SIZEx2, REG_ESP\n#define i_vSPs3 TYPE_INDIR_VAR_REG_SIZEx3x5, REG_ESP\n/* pop but unusual size */\n#define i_xSPoN TYPE_INDIR_VAR_XREG_OFFS_N, REG_ESP\n#define c1  TYPE_1, OPSZ_0\n/* we pick the right constant based on the opcode */\n#define cF  TYPE_FLOATCONST, OPSZ_0\n\n/* registers that are base 32 but vary down or up */\n#define eAX TYPE_VAR_REG, REG_EAX\n#define eCX TYPE_VAR_REG, REG_ECX\n#define eDX TYPE_VAR_REG, REG_EDX\n#define eBX TYPE_VAR_REG, REG_EBX\n#define eSP TYPE_VAR_REG, REG_ESP\n#define eBP TYPE_VAR_REG, REG_EBP\n#define eSI TYPE_VAR_REG, REG_ESI\n#define eDI TYPE_VAR_REG, REG_EDI\n\n/* registers that are base 32 and can vary down but not up */\n#define zAX TYPE_VARZ_REG, REG_EAX\n#define zCX TYPE_VARZ_REG, REG_ECX\n#define zDX TYPE_VARZ_REG, REG_EDX\n#define zBX TYPE_VARZ_REG, REG_EBX\n#define zSP TYPE_VARZ_REG, REG_ESP\n#define zBP TYPE_VARZ_REG, REG_EBP\n#define zSI TYPE_VARZ_REG, REG_ESI\n#define zDI TYPE_VARZ_REG, REG_EDI\n\n/* registers whose base matches the mode, and can vary down but not up.\n * we use the 32-bit versions but expand in resolve_var_reg()\n */\n#define xAX TYPE_VAR_XREG, REG_EAX\n#define xCX TYPE_VAR_XREG, REG_ECX\n#define xDX TYPE_VAR_XREG, REG_EDX\n#define xBX TYPE_VAR_XREG, REG_EBX\n#define xSP TYPE_VAR_XREG, REG_ESP\n#define xBP TYPE_VAR_XREG, REG_EBP\n#define xSI TYPE_VAR_XREG, REG_ESI\n#define xDI TYPE_VAR_XREG, REG_EDI\n\n/* jecxz and loop* vary by addr16 */\n#define axCX TYPE_VAR_ADDR_XREG, REG_ECX\n/* string ops also use addr16 */\n#define axSI TYPE_VAR_ADDR_XREG, REG_ESI\n#define axDI TYPE_VAR_ADDR_XREG, REG_EDI\n#define axAX TYPE_VAR_ADDR_XREG, REG_EAX\n\n/* 8-bit implicit registers (not from modrm) that can be exteded via rex.r */\n#define al_x TYPE_REG_EX, REG_AL\n#define cl_x TYPE_REG_EX, REG_CL\n#define dl_x TYPE_REG_EX, REG_DL\n#define bl_x TYPE_REG_EX, REG_BL\n#define ah_x TYPE_REG_EX, REG_AH\n#define ch_x TYPE_REG_EX, REG_CH\n#define dh_x TYPE_REG_EX, REG_DH\n#define bh_x TYPE_REG_EX, REG_BH\n\n/* 4_rex8_short2 implicit registers (not from modrm) that can be exteded via rex.r */\n#define eAX_x TYPE_VAR_REG_EX, REG_EAX\n#define eCX_x TYPE_VAR_REG_EX, REG_ECX\n#define eDX_x TYPE_VAR_REG_EX, REG_EDX\n#define eBX_x TYPE_VAR_REG_EX, REG_EBX\n#define eSP_x TYPE_VAR_REG_EX, REG_ESP\n#define eBP_x TYPE_VAR_REG_EX, REG_EBP\n#define eSI_x TYPE_VAR_REG_EX, REG_ESI\n#define eDI_x TYPE_VAR_REG_EX, REG_EDI\n\n/* 4x8_short2 implicit registers (not from modrm) that can be exteded via rex.r */\n#define xAX_x TYPE_VAR_XREG_EX, REG_EAX\n#define xCX_x TYPE_VAR_XREG_EX, REG_ECX\n#define xDX_x TYPE_VAR_XREG_EX, REG_EDX\n#define xBX_x TYPE_VAR_XREG_EX, REG_EBX\n#define xSP_x TYPE_VAR_XREG_EX, REG_ESP\n#define xBP_x TYPE_VAR_XREG_EX, REG_EBP\n#define xSI_x TYPE_VAR_XREG_EX, REG_ESI\n#define xDI_x TYPE_VAR_XREG_EX, REG_EDI\n\n/* 4_rex8 implicit registers (not from modrm) that can be exteded via rex.r */\n#define uAX_x TYPE_VAR_REGX_EX, REG_EAX\n#define uCX_x TYPE_VAR_REGX_EX, REG_ECX\n#define uDX_x TYPE_VAR_REGX_EX, REG_EDX\n#define uBX_x TYPE_VAR_REGX_EX, REG_EBX\n#define uSP_x TYPE_VAR_REGX_EX, REG_ESP\n#define uBP_x TYPE_VAR_REGX_EX, REG_EBP\n#define uSI_x TYPE_VAR_REGX_EX, REG_ESI\n#define uDI_x TYPE_VAR_REGX_EX, REG_EDI\n\n/* 4_rex8 implicit registers (not from modrm) */\n#define uDX TYPE_VAR_REGX, REG_EDX\n\n#define ax TYPE_REG, REG_AX\n#define cx TYPE_REG, REG_CX\n#define dx TYPE_REG, REG_DX\n#define bx TYPE_REG, REG_BX\n#define sp TYPE_REG, REG_SP\n#define bp TYPE_REG, REG_BP\n#define si TYPE_REG, REG_SI\n#define di TYPE_REG, REG_DI\n\n#define al TYPE_REG, REG_AL\n#define cl TYPE_REG, REG_CL\n#define dl TYPE_REG, REG_DL\n#define bl TYPE_REG, REG_BL\n#define ah TYPE_REG, REG_AH\n#define ch TYPE_REG, REG_CH\n#define dh TYPE_REG, REG_DH\n#define bh TYPE_REG, REG_BH\n\n#define eax TYPE_REG, REG_EAX\n#define ecx TYPE_REG, REG_ECX\n#define edx TYPE_REG, REG_EDX\n#define ebx TYPE_REG, REG_EBX\n#define esp TYPE_REG, REG_ESP\n#define ebp TYPE_REG, REG_EBP\n#define esi TYPE_REG, REG_ESI\n#define edi TYPE_REG, REG_EDI\n\n#define xsp TYPE_XREG, REG_ESP\n#define xbp TYPE_XREG, REG_EBP\n#define xcx TYPE_XREG, REG_ECX\n\n#define cs  TYPE_REG, SEG_CS\n#define ss  TYPE_REG, SEG_SS\n#define ds  TYPE_REG, SEG_DS\n#define es  TYPE_REG, SEG_ES\n#define fs  TYPE_REG, SEG_FS\n#define gs  TYPE_REG, SEG_GS\n\n#define st0 TYPE_REG, REG_ST0\n#define st1 TYPE_REG, REG_ST1\n#define st2 TYPE_REG, REG_ST2\n#define st3 TYPE_REG, REG_ST3\n#define st4 TYPE_REG, REG_ST4\n#define st5 TYPE_REG, REG_ST5\n#define st6 TYPE_REG, REG_ST6\n#define st7 TYPE_REG, REG_ST7\n\n#define xmm0 TYPE_REG, REG_XMM0\n\n/* flags */\n#define no       0\n#define mrm      HAS_MODRM\n#define xop      (HAS_EXTRA_OPERANDS|EXTRAS_IN_CODE_FIELD)\n#define mrm_xop  (HAS_MODRM|HAS_EXTRA_OPERANDS|EXTRAS_IN_CODE_FIELD)\n#define xop_next (HAS_EXTRA_OPERANDS)\n#define i64      X64_INVALID\n#define o64      X86_INVALID\n#define reqp     REQUIRES_PREFIX\n#define vex      REQUIRES_VEX\n#define rex      REQUIRES_REX\n#define reqL0    REQUIRES_VEX_L_0\n#define reqL1    REQUIRES_VEX_L_1\n#define predcc   HAS_PRED_CC\n#define predcx   HAS_PRED_COMPLEX\n#define evex     REQUIRES_EVEX\n#define reqLL0   REQUIRES_EVEX_LL_0\n#define reqLL1   REQUIRES_EVEX_LL_1\n\n/* eflags */\n#define x     0\n#define fRC   EFLAGS_READ_CF\n#define fRP   EFLAGS_READ_PF\n#define fRA   EFLAGS_READ_AF\n#define fRZ   EFLAGS_READ_ZF\n#define fRS   EFLAGS_READ_SF\n#define fRT   EFLAGS_READ_TF\n#define fRI   EFLAGS_READ_IF\n#define fRD   EFLAGS_READ_DF\n#define fRO   EFLAGS_READ_OF\n#define fRN   EFLAGS_READ_NT\n#define fRR   EFLAGS_READ_RF\n#define fRX   EFLAGS_READ_ALL\n#define fR6   EFLAGS_READ_6\n#define fWC   EFLAGS_WRITE_CF\n#define fWP   EFLAGS_WRITE_PF\n#define fWA   EFLAGS_WRITE_AF\n#define fWZ   EFLAGS_WRITE_ZF\n#define fWS   EFLAGS_WRITE_SF\n#define fWT   EFLAGS_WRITE_TF\n#define fWI   EFLAGS_WRITE_IF\n#define fWD   EFLAGS_WRITE_DF\n#define fWO   EFLAGS_WRITE_OF\n#define fWN   EFLAGS_WRITE_NT\n#define fWR   EFLAGS_WRITE_RF\n#define fWX   EFLAGS_WRITE_ALL\n#define fW6   EFLAGS_WRITE_6\n/* flags affected by OP_int*\n * FIXME: should we add AC and VM flags?\n */\n#define fINT  (fRX|fWT|fWN|fWI|fWR)\n\n/* for constructing linked lists of table entries */\n#define NA 0\n#define END_LIST  0\n#define tfb (ptr_int_t)&first_byte\n#define tsb (ptr_int_t)&second_byte\n#define tex (ptr_int_t)&base_extensions\n#define t38 (ptr_int_t)&third_byte_38\n#define t3a (ptr_int_t)&third_byte_3a\n#define tpe (ptr_int_t)&prefix_extensions\n#define tvex (ptr_int_t)&e_vex_extensions\n#define modx (ptr_int_t)&mod_extensions\n#define tre (ptr_int_t)&rep_extensions\n#define tne (ptr_int_t)&repne_extensions\n#define tfl (ptr_int_t)&float_low_modrm\n#define tfh (ptr_int_t)&float_high_modrm\n#define exop (ptr_int_t)&extra_operands\n#define t64e (ptr_int_t)&x64_extensions\n#define trexb (ptr_int_t)&rex_b_extensions\n#define trexw (ptr_int_t)&rex_w_extensions\n#define tvex (ptr_int_t)&e_vex_extensions\n#define tvexw (ptr_int_t)&vex_W_extensions\n#define txop (ptr_int_t)&xop_extensions\n#define tevexw (ptr_int_t)&evex_W_extensions\n\n/****************************************************************************\n * One-byte opcodes\n * This is from Tables A-2 & A-3\n */\nconst instr_info_t first_byte[] = {\n    /* {op/type, op encoding, name, dst1, dst2, src1, src2, src3, modrm?, eflags, code} */\n    /* 00 */\n    {OP_add,  0x000000, \"add\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][0]},\n    {OP_add,  0x010000, \"add\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x00]},\n    {OP_add,  0x020000, \"add\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x01]},\n    {OP_add,  0x030000, \"add\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x02]},\n    {OP_add,  0x040000, \"add\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x03]},\n    {OP_add,  0x050000, \"add\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x04]},\n    {OP_push, 0x060000, \"push\", xsp, i_xSPo1, es, xsp, xx, i64, x, tfb[0x0e]},\n    {OP_pop,  0x070000, \"pop\", es, xsp, xsp, i_xSP, xx, i64, x, tsb[0xa1]},\n    /* 08 */\n    {OP_or,  0x080000, \"or\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][1]},\n    {OP_or,  0x090000, \"or\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x08]},\n    {OP_or,  0x0a0000, \"or\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x09]},\n    {OP_or,  0x0b0000, \"or\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x0a]},\n    {OP_or,  0x0c0000, \"or\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x0b]},\n    {OP_or,  0x0d0000, \"or\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x0c]},\n    {OP_push,0x0e0000, \"push\", xsp, i_xSPo1, cs, xsp, xx, i64, x, tfb[0x16]},\n    {ESCAPE, 0x0f0000, \"(escape)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* 10 */\n    {OP_adc,  0x100000, \"adc\",  Eb, xx, Gb, Eb, xx, mrm, (fW6|fRC), tex[1][2]},\n    {OP_adc,  0x110000, \"adc\",  Ev, xx, Gv, Ev, xx, mrm, (fW6|fRC), tfb[0x10]},\n    {OP_adc,  0x120000, \"adc\",  Gb, xx, Eb, Gb, xx, mrm, (fW6|fRC), tfb[0x11]},\n    {OP_adc,  0x130000, \"adc\",  Gv, xx, Ev, Gv, xx, mrm, (fW6|fRC), tfb[0x12]},\n    {OP_adc,  0x140000, \"adc\",  al, xx, Ib, al, xx, no,  (fW6|fRC), tfb[0x13]},\n    {OP_adc,  0x150000, \"adc\", eAX, xx, Iz, eAX, xx, no,  (fW6|fRC), tfb[0x14]},\n    {OP_push, 0x160000, \"push\", xsp, i_xSPo1, ss, xsp, xx, i64, x, tfb[0x1e]},\n    {OP_pop,  0x170000, \"pop\", ss, xsp, xsp, i_xSP, xx, i64, x, tfb[0x1f]},\n    /* 18 */\n    {OP_sbb,  0x180000, \"sbb\",  Eb, xx, Gb, Eb, xx, mrm, (fW6|fRC), tex[1][3]},\n    {OP_sbb,  0x190000, \"sbb\",  Ev, xx, Gv, Ev, xx, mrm, (fW6|fRC), tfb[0x18]},\n    {OP_sbb,  0x1a0000, \"sbb\",  Gb, xx, Eb, Gb, xx, mrm, (fW6|fRC), tfb[0x19]},\n    {OP_sbb,  0x1b0000, \"sbb\",  Gv, xx, Ev, Gv, xx, mrm, (fW6|fRC), tfb[0x1a]},\n    {OP_sbb,  0x1c0000, \"sbb\",  al, xx, Ib, al, xx, no,  (fW6|fRC), tfb[0x1b]},\n    {OP_sbb,  0x1d0000, \"sbb\", eAX, xx, Iz, eAX, xx, no,  (fW6|fRC), tfb[0x1c]},\n    {OP_push, 0x1e0000, \"push\", xsp, i_xSPo1, ds, xsp, xx, i64, x, tsb[0xa0]},\n    {OP_pop,  0x1f0000, \"pop\", ds, xsp, xsp, i_xSP, xx, i64, x, tfb[0x07]},\n    /* 20 */\n    {OP_and,  0x200000, \"and\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][4]},\n    {OP_and,  0x210000, \"and\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x20]},\n    {OP_and,  0x220000, \"and\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x21]},\n    {OP_and,  0x230000, \"and\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x22]},\n    {OP_and,  0x240000, \"and\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x23]},\n    {OP_and,  0x250000, \"and\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x24]},\n    {PREFIX,  0x260000, \"es\", xx, xx, xx, xx, xx, no, x, SEG_ES},\n    {OP_daa,  0x270000, \"daa\", al, xx, al, xx, xx, i64, (fW6|fRC|fRA), END_LIST},\n    /* 28 */\n    {OP_sub,  0x280000, \"sub\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][5]},\n    {OP_sub,  0x290000, \"sub\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x28]},\n    {OP_sub,  0x2a0000, \"sub\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x29]},\n    {OP_sub,  0x2b0000, \"sub\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x2a]},\n    {OP_sub,  0x2c0000, \"sub\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x2b]},\n    {OP_sub,  0x2d0000, \"sub\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x2c]},\n    {PREFIX,  0x2e0000, \"cs\", xx, xx, xx, xx, xx, no, x, SEG_CS},\n    {OP_das,  0x2f0000, \"das\", al, xx, al, xx, xx, i64, (fW6|fRC|fRA), END_LIST},\n    /* 30 */\n    {OP_xor,  0x300000, \"xor\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][6]},\n    {OP_xor,  0x310000, \"xor\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x30]},\n    {OP_xor,  0x320000, \"xor\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x31]},\n    {OP_xor,  0x330000, \"xor\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x32]},\n    {OP_xor,  0x340000, \"xor\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x33]},\n    {OP_xor,  0x350000, \"xor\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x34]},\n    {PREFIX,  0x360000, \"ss\", xx, xx, xx, xx, xx, no, x, SEG_SS},\n    {OP_aaa,  0x370000, \"aaa\", ax, xx, ax, xx, xx, i64, (fW6|fRA), END_LIST},\n    /* 38 */\n    {OP_cmp,  0x380000, \"cmp\", xx, xx,  Eb, Gb, xx, mrm, fW6, tex[1][7]},\n    {OP_cmp,  0x390000, \"cmp\", xx, xx,  Ev, Gv, xx, mrm, fW6, tfb[0x38]},\n    {OP_cmp,  0x3a0000, \"cmp\", xx, xx,  Gb, Eb, xx, mrm, fW6, tfb[0x39]},\n    {OP_cmp,  0x3b0000, \"cmp\", xx, xx,  Gv, Ev, xx, mrm, fW6, tfb[0x3a]},\n    {OP_cmp,  0x3c0000, \"cmp\", xx, xx,  al, Ib, xx, no,  fW6, tfb[0x3b]},\n    {OP_cmp,  0x3d0000, \"cmp\", xx, xx, eAX, Iz, xx, no,  fW6, tfb[0x3c]},\n    {PREFIX,  0x3e0000, \"ds\", xx, xx, xx, xx, xx, no, x, SEG_DS},\n    {OP_aas,  0x3f0000, \"aas\", ax, xx, ax, xx, xx, i64, (fW6|fRA), END_LIST},\n    /* 40 */\n    {X64_EXT, 0x400000, \"(x64_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {X64_EXT, 0x410000, \"(x64_ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {X64_EXT, 0x420000, \"(x64_ext 2)\", xx, xx, xx, xx, xx, no, x, 2},\n    {X64_EXT, 0x430000, \"(x64_ext 3)\", xx, xx, xx, xx, xx, no, x, 3},\n    {X64_EXT, 0x440000, \"(x64_ext 4)\", xx, xx, xx, xx, xx, no, x, 4},\n    {X64_EXT, 0x450000, \"(x64_ext 5)\", xx, xx, xx, xx, xx, no, x, 5},\n    {X64_EXT, 0x460000, \"(x64_ext 6)\", xx, xx, xx, xx, xx, no, x, 6},\n    {X64_EXT, 0x470000, \"(x64_ext 7)\", xx, xx, xx, xx, xx, no, x, 7},\n    /* 48 */\n    {X64_EXT, 0x480000, \"(x64_ext 8)\", xx, xx, xx, xx, xx, no, x, 8},\n    {X64_EXT, 0x490000, \"(x64_ext 9)\", xx, xx, xx, xx, xx, no, x, 9},\n    {X64_EXT, 0x4a0000, \"(x64_ext 10)\", xx, xx, xx, xx, xx, no, x, 10},\n    {X64_EXT, 0x4b0000, \"(x64_ext 11)\", xx, xx, xx, xx, xx, no, x, 11},\n    {X64_EXT, 0x4c0000, \"(x64_ext 12)\", xx, xx, xx, xx, xx, no, x, 12},\n    {X64_EXT, 0x4d0000, \"(x64_ext 13)\", xx, xx, xx, xx, xx, no, x, 13},\n    {X64_EXT, 0x4e0000, \"(x64_ext 14)\", xx, xx, xx, xx, xx, no, x, 14},\n    {X64_EXT, 0x4f0000, \"(x64_ext 15)\", xx, xx, xx, xx, xx, no, x, 15},\n    /* 50 */\n    {OP_push,  0x500000, \"push\", xsp, i_xSPo1, xAX_x, xsp, xx, no, x, tfb[0x51]},\n    {OP_push,  0x510000, \"push\", xsp, i_xSPo1, xCX_x, xsp, xx, no, x, tfb[0x52]},\n    {OP_push,  0x520000, \"push\", xsp, i_xSPo1, xDX_x, xsp, xx, no, x, tfb[0x53]},\n    {OP_push,  0x530000, \"push\", xsp, i_xSPo1, xBX_x, xsp, xx, no, x, tfb[0x54]},\n    {OP_push,  0x540000, \"push\", xsp, i_xSPo1, xSP_x, xsp, xx, no, x, tfb[0x55]},\n    {OP_push,  0x550000, \"push\", xsp, i_xSPo1, xBP_x, xsp, xx, no, x, tfb[0x56]},\n    {OP_push,  0x560000, \"push\", xsp, i_xSPo1, xSI_x, xsp, xx, no, x, tfb[0x57]},\n    {OP_push,  0x570000, \"push\", xsp, i_xSPo1, xDI_x, xsp, xx, no, x, tex[12][6]},\n    /* 58 */\n    {OP_pop,  0x580000, \"pop\", xAX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x59]},\n    {OP_pop,  0x590000, \"pop\", xCX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5a]},\n    {OP_pop,  0x5a0000, \"pop\", xDX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5b]},\n    {OP_pop,  0x5b0000, \"pop\", xBX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5c]},\n    {OP_pop,  0x5c0000, \"pop\", xSP_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5d]},\n    {OP_pop,  0x5d0000, \"pop\", xBP_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5e]},\n    {OP_pop,  0x5e0000, \"pop\", xSI_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5f]},\n    {OP_pop,  0x5f0000, \"pop\", xDI_x, xsp, xsp, i_xSP, xx, no, x, tex[26][0]},\n    /* 60 */\n    {OP_pusha, 0x600000, \"pusha\", xsp, i_xSPo8, xsp, eAX, eBX, xop|i64, x, exop[0x00]},\n    {OP_popa,  0x610000, \"popa\", xsp, eAX, xsp, i_xSPs8, xx, xop|i64, x, exop[0x02]},\n    {EVEX_PREFIX_EXT, 0x620000, \"(evex_prefix_ext)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {X64_EXT,  0x630000, \"(x64_ext 16)\", xx, xx, xx, xx, xx, no, x, 16},\n    {PREFIX, 0x640000, \"fs\", xx, xx, xx, xx, xx, no, x, SEG_FS},\n    {PREFIX, 0x650000, \"gs\", xx, xx, xx, xx, xx, no, x, SEG_GS},\n    {PREFIX, 0x660000, \"data size\", xx, xx, xx, xx, xx, no, x, PREFIX_DATA},\n    {PREFIX, 0x670000, \"addr size\", xx, xx, xx, xx, xx, no, x, PREFIX_ADDR},\n    /* 68 */\n    {OP_push_imm, 0x680000, \"push\", xsp, i_xSPo1, Iz, xsp, xx, no, x, tfb[0x6a]},\n    {OP_imul,  0x690000, \"imul\", Gv, xx, Ev, Iz, xx, mrm, fW6, tfb[0x6b]},\n    {OP_push_imm, 0x6a0000, \"push\", xsp, i_xSPo1, Ib, xsp, xx, no, x, END_LIST},/* sign-extend to push 2/4/8 bytes */\n    {OP_imul,  0x6b0000, \"imul\", Gv, xx, Ev, Ib, xx, mrm, fW6, END_LIST},\n    {REP_EXT,  0x6c0000, \"((rep) ins)\", Yb, xx, i_dx, xx, xx, no, fRD, 0},\n    {REP_EXT,  0x6d0000, \"((rep) ins)\", Yz, xx, i_dx, xx, xx, no, fRD, 1},\n    {REP_EXT,  0x6e0000, \"((rep) outs)\", i_dx, xx, Xb, xx, xx, no, fRD, 2},\n    {REP_EXT,  0x6f0000, \"((rep) outs)\", i_dx, xx, Xz, xx, xx, no, fRD, 3},\n    /* 70 */\n    {OP_jo_short,  0x700000, \"jo\",  xx, xx, Jb, xx, xx, no, fRO, END_LIST},\n    {OP_jno_short, 0x710000, \"jno\", xx, xx, Jb, xx, xx, no, fRO, END_LIST},\n    {OP_jb_short,  0x720000, \"jb\",  xx, xx, Jb, xx, xx, no, fRC, END_LIST},\n    {OP_jnb_short, 0x730000, \"jnb\", xx, xx, Jb, xx, xx, no, fRC, END_LIST},\n    {OP_jz_short,  0x740000, \"jz\",  xx, xx, Jb, xx, xx, no, fRZ, END_LIST},\n    {OP_jnz_short, 0x750000, \"jnz\", xx, xx, Jb, xx, xx, no, fRZ, END_LIST},\n    {OP_jbe_short, 0x760000, \"jbe\", xx, xx, Jb, xx, xx, no, (fRC|fRZ), END_LIST},\n    {OP_jnbe_short,0x770000, \"jnbe\",xx, xx, Jb, xx, xx, no, (fRC|fRZ), END_LIST},\n    /* 78 */\n    {OP_js_short,  0x780000, \"js\",  xx, xx, Jb, xx, xx, no, fRS, END_LIST},\n    {OP_jns_short, 0x790000, \"jns\", xx, xx, Jb, xx, xx, no, fRS, END_LIST},\n    {OP_jp_short,  0x7a0000, \"jp\",  xx, xx, Jb, xx, xx, no, fRP, END_LIST},\n    {OP_jnp_short, 0x7b0000, \"jnp\", xx, xx, Jb, xx, xx, no, fRP, END_LIST},\n    {OP_jl_short,  0x7c0000, \"jl\",  xx, xx, Jb, xx, xx, no, (fRS|fRO), END_LIST},\n    {OP_jnl_short, 0x7d0000, \"jnl\", xx, xx, Jb, xx, xx, no, (fRS|fRO), END_LIST},\n    {OP_jle_short, 0x7e0000, \"jle\", xx, xx, Jb, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n    {OP_jnle_short,0x7f0000, \"jnle\",xx, xx, Jb, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n    /* 80 */\n    {EXTENSION, 0x800000, \"(group 1a)\", Eb, xx, Ib, xx, xx, mrm, x, 0},\n    {EXTENSION, 0x810000, \"(group 1b)\", Ev, xx, Iz, xx, xx, mrm, x, 1},\n    {EXTENSION, 0x820000, \"(group 1c*)\", Ev, xx, Ib, xx, xx, mrm|i64, x, 25}, /* PR 235092: gnu tools (gdb, objdump) think this is a bad opcode but windbg and the hardware disagree */\n    {EXTENSION, 0x830000, \"(group 1c)\", Ev, xx, Ib, xx, xx, mrm, x, 2},\n    {OP_test,  0x840000, \"test\", xx, xx, Eb, Gb, xx, mrm, fW6, tex[10][0]},\n    {OP_test,  0x850000, \"test\", xx, xx, Ev, Gv, xx, mrm, fW6, tfb[0x84]},\n    {OP_xchg,  0x860000, \"xchg\", Eb, Gb, Eb, Gb, xx, mrm, x, END_LIST},\n    {OP_xchg,  0x870000, \"xchg\", Ev, Gv, Ev, Gv, xx, mrm, x, tfb[0x86]},\n    /* 88 */\n    {OP_mov_st,  0x880000, \"mov\", Eb, xx, Gb, xx, xx, mrm, x, tex[18][0]},\n    {OP_mov_st,  0x890000, \"mov\", Ev, xx, Gv, xx, xx, mrm, x, tfb[0x88]},\n    {OP_mov_ld,  0x8a0000, \"mov\", Gb, xx, Eb, xx, xx, mrm, x, END_LIST},\n    {OP_mov_ld,  0x8b0000, \"mov\", Gv, xx, Ev, xx, xx, mrm, x, tfb[0x8a]},\n    {OP_mov_seg, 0x8c0000, \"mov\", Ev, xx, Sw, xx, xx, mrm, x, END_LIST},\n    {OP_lea,  0x8d0000, \"lea\", Gv, xx, Mm, xx, xx, mrm, x, END_LIST}, /* Intel has just M */\n    {OP_mov_seg, 0x8e0000, \"mov\", Sw, xx, Ev, xx, xx, mrm, x, tfb[0x8c]},\n    {XOP_PREFIX_EXT, 0x8f0000, \"(xop_prefix_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    /* 90 */\n    {PREFIX_EXT, 0x900000, \"(prefix ext 103)\", xx, xx, xx, xx, xx, no, x, 103},\n    {OP_xchg, 0x910000, \"xchg\", eCX_x, eAX, eCX_x, eAX, xx, no, x, tfb[0x92]},\n    {OP_xchg, 0x920000, \"xchg\", eDX_x, eAX, eDX_x, eAX, xx, no, x, tfb[0x93]},\n    {OP_xchg, 0x930000, \"xchg\", eBX_x, eAX, eBX_x, eAX, xx, no, x, tfb[0x94]},\n    {OP_xchg, 0x940000, \"xchg\", eSP_x, eAX, eSP_x, eAX, xx, no, x, tfb[0x95]},\n    {OP_xchg, 0x950000, \"xchg\", eBP_x, eAX, eBP_x, eAX, xx, no, x, tfb[0x96]},\n    {OP_xchg, 0x960000, \"xchg\", eSI_x, eAX, eSI_x, eAX, xx, no, x, tfb[0x97]},\n    {OP_xchg, 0x970000, \"xchg\", eDI_x, eAX, eDI_x, eAX, xx, no, x, tfb[0x87]},\n    /* 98 */\n    {OP_cwde, 0x980000, \"cwde\", eAX, xx, ax, xx, xx, no, x, END_LIST},/*16-bit==\"cbw\", src is al not ax; FIXME: newer gdb calls it \"cwtl\"?!?*/\n    /* PR 354096: does not write to ax/eax/rax: sign-extends into dx/edx/rdx */\n    {OP_cdq,  0x990000, \"cdq\", eDX, xx, eAX, xx, xx, no, x, END_LIST},/*16-bit==\"cwd\";64-bit==\"cqo\"*/\n    {OP_call_far, 0x9a0000, \"lcall\",  xsp, i_vSPo2, Ap, xsp, xx, i64, x, END_LIST},\n    {OP_fwait, 0x9b0000, \"fwait\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pushf, 0x9c0000, \"pushf\", xsp, i_xSPo1, xsp, xx, xx, no, fRX, END_LIST},\n    {OP_popf,  0x9d0000, \"popf\", xsp, xx, xsp, i_xSP, xx, no, fWX, END_LIST},\n    {OP_sahf,  0x9e0000, \"sahf\", xx, xx, ah, xx, xx, no, (fW6&(~fWO)), END_LIST},\n    {OP_lahf,  0x9f0000, \"lahf\", ah, xx, xx, xx, xx, no, (fR6&(~fRO)), END_LIST},\n    /* a0 */\n    {OP_mov_ld,  0xa00000, \"mov\", al, xx, Ob, xx, xx, no, x, tfb[0x8b]},\n    {OP_mov_ld,  0xa10000, \"mov\", eAX, xx, Ov, xx, xx, no, x, tfb[0xa0]},\n    {OP_mov_st,  0xa20000, \"mov\", Ob, xx, al, xx, xx, no, x, tfb[0x89]},\n    {OP_mov_st,  0xa30000, \"mov\", Ov, xx, eAX, xx, xx, no, x, tfb[0xa2]},\n    {REP_EXT, 0xa40000, \"((rep) movs)\", Yb, xx, Xb, xx, xx, no, fRD, 4},\n    {REP_EXT, 0xa50000, \"((rep) movs)\", Yv, xx, Xv, xx, xx, no, fRD, 5},\n    {REPNE_EXT, 0xa60000, \"((rep/ne) cmps)\", Xb, xx, Yb, xx, xx, no, (fW6|fRD|fRZ), 0},\n    {REPNE_EXT, 0xa70000, \"((rep/ne) cmps)\", Xv, xx, Yv, xx, xx, no, (fW6|fRD|fRZ), 1},\n    /* a8 */\n    {OP_test,  0xa80000, \"test\", xx, xx,  al, Ib, xx, no, fW6, tfb[0x85]},\n    {OP_test,  0xa90000, \"test\", xx, xx, eAX, Iz, xx, no, fW6, tfb[0xa8]},\n    {REP_EXT, 0xaa0000, \"((rep) stos)\", Yb, xx, al, xx, xx, no, fRD, 6},\n    {REP_EXT, 0xab0000, \"((rep) stos)\", Yv, xx, eAX, xx, xx, no, fRD, 7},\n    {REP_EXT, 0xac0000, \"((rep) lods)\", al, xx, Xb, xx, xx, no, fRD, 8},\n    {REP_EXT, 0xad0000, \"((rep) lods)\", eAX, xx, Xv, xx, xx, no, fRD, 9},\n    {REPNE_EXT, 0xae0000, \"((rep/ne) scas)\", al, xx, Yb, xx, xx, no, (fW6|fRD|fRZ), 2},\n    {REPNE_EXT, 0xaf0000, \"((rep/ne) scas)\", eAX, xx, Yv, xx, xx, no, (fW6|fRD|fRZ), 3},\n    /* b0 */\n    {OP_mov_imm, 0xb00000, \"mov\", al_x, xx, Ib, xx, xx, no, x, tfb[0xb1]},\n    {OP_mov_imm, 0xb10000, \"mov\", cl_x, xx, Ib, xx, xx, no, x, tfb[0xb2]},\n    {OP_mov_imm, 0xb20000, \"mov\", dl_x, xx, Ib, xx, xx, no, x, tfb[0xb3]},\n    {OP_mov_imm, 0xb30000, \"mov\", bl_x, xx, Ib, xx, xx, no, x, tfb[0xb4]},\n    {OP_mov_imm, 0xb40000, \"mov\", ah_x, xx, Ib, xx, xx, no, x, tfb[0xb5]},\n    {OP_mov_imm, 0xb50000, \"mov\", ch_x, xx, Ib, xx, xx, no, x, tfb[0xb6]},\n    {OP_mov_imm, 0xb60000, \"mov\", dh_x, xx, Ib, xx, xx, no, x, tfb[0xb7]},\n    /* PR 250397: we point at the tail end of the mov_st templates */\n    {OP_mov_imm, 0xb70000, \"mov\", bh_x, xx, Ib, xx, xx, no, x, tex[18][0]},\n    /* b8 */\n    {OP_mov_imm, 0xb80000, \"mov\", eAX_x, xx, Iv, xx, xx, no, x, tfb[0xb9]},\n    {OP_mov_imm, 0xb90000, \"mov\", eCX_x, xx, Iv, xx, xx, no, x, tfb[0xba]},\n    {OP_mov_imm, 0xba0000, \"mov\", eDX_x, xx, Iv, xx, xx, no, x, tfb[0xbb]},\n    {OP_mov_imm, 0xbb0000, \"mov\", eBX_x, xx, Iv, xx, xx, no, x, tfb[0xbc]},\n    {OP_mov_imm, 0xbc0000, \"mov\", eSP_x, xx, Iv, xx, xx, no, x, tfb[0xbd]},\n    {OP_mov_imm, 0xbd0000, \"mov\", eBP_x, xx, Iv, xx, xx, no, x, tfb[0xbe]},\n    {OP_mov_imm, 0xbe0000, \"mov\", eSI_x, xx, Iv, xx, xx, no, x, tfb[0xbf]},\n    {OP_mov_imm, 0xbf0000, \"mov\", eDI_x, xx, Iv, xx, xx, no, x, tfb[0xb0]},\n    /* c0 */\n    {EXTENSION, 0xc00000, \"(group 2a)\", Eb, xx, Ib, xx, xx, mrm, x, 3},\n    {EXTENSION, 0xc10000, \"(group 2b)\", Ev, xx, Ib, xx, xx, mrm, x, 4},\n    {OP_ret,  0xc20000, \"ret\", xsp, xx, Iw, xsp, i_iSP, no, x, tfb[0xc3]},\n    {OP_ret,  0xc30000, \"ret\", xsp, xx, xsp, i_iSP, xx, no, x, END_LIST},\n    {VEX_PREFIX_EXT, 0xc40000, \"(vex_prefix_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {VEX_PREFIX_EXT, 0xc50000, \"(vex_prefix_ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {EXTENSION, 0xc60000, \"(group 11a)\", Eb, xx, Ib, xx, xx, mrm, x, 17},\n    {EXTENSION, 0xc70000, \"(group 11b)\", Ev, xx, Iz, xx, xx, mrm, x, 18},\n    /* c8 */\n    {OP_enter,  0xc80000, \"enter\", xsp, i_xSPoN, Iw, Ib, xsp, xop, x, exop[0x05]},\n    {OP_leave,  0xc90000, \"leave\", xsp, xbp, xbp, xsp, i_xBP, no, x, END_LIST},\n    {OP_ret_far,  0xca0000, \"lret\", xsp, xx, Iw, xsp, i_vSPs2, no, x, tfb[0xcb]},\n    {OP_ret_far,  0xcb0000, \"lret\", xsp, xx, xsp, i_vSPs2, xx, no, x, END_LIST},\n    /* we ignore the operations on the kernel stack */\n    {OP_int3, 0xcc0000, \"int3\", xx, xx, xx, xx, xx, no, fINT, END_LIST},\n    {OP_int,  0xcd0000, \"int\",  xx, xx, Ib, xx, xx, no, fINT, END_LIST},\n    {OP_into, 0xce0000, \"into\", xx, xx, xx, xx, xx, i64, fINT, END_LIST},\n    {OP_iret, 0xcf0000, \"iret\", xsp, xx, xsp, i_vSPs3, xx, no, fWX, END_LIST},\n    /* d0 */\n    {EXTENSION, 0xd00000, \"(group 2c)\", Eb, xx, c1,  xx, xx, mrm, x, 5},\n    {EXTENSION, 0xd10000, \"(group 2d)\", Ev, xx, c1,  xx, xx, mrm, x, 6},\n    {EXTENSION, 0xd20000, \"(group 2e)\", Eb, xx, cl, xx, xx, mrm, x, 7},\n    {EXTENSION, 0xd30000, \"(group 2f)\", Ev, xx, cl, xx, xx, mrm, x, 8},\n    {OP_aam,  0xd40000, \"aam\", ax, xx, Ib, ax, xx, i64, fW6, END_LIST},\n    {OP_aad,  0xd50000, \"aad\", ax, xx, Ib, ax, xx, i64, fW6, END_LIST},\n    {OP_salc,  0xd60000, \"salc\", al, xx, xx, xx, xx, i64, fRC, END_LIST},/*undocumented*/\n    {OP_xlat,  0xd70000, \"xlat\", al, xx, Zb, xx, xx, no, x, END_LIST},\n    /* d8 */\n    {FLOAT_EXT, 0xd80000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},/* all floats need modrm */\n    {FLOAT_EXT, 0xd90000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xda0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdb0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdc0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdd0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xde0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdf0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    /* e0 */\n    {OP_loopne,0xe00000, \"loopne\", axCX, xx, Jb, axCX, xx, no, fRZ, END_LIST},\n    {OP_loope, 0xe10000, \"loope\",  axCX, xx, Jb, axCX, xx, no, fRZ, END_LIST},\n    {OP_loop,  0xe20000, \"loop\",   axCX, xx, Jb, axCX, xx, no, x, END_LIST},\n    {OP_jecxz, 0xe30000, \"jecxz\",  xx, xx, Jb, axCX, xx, no, x, END_LIST},/*16-bit==\"jcxz\",64-bit=\"jrcxz\"*/\n    /* FIXME: in & out access \"I/O ports\", are these memory addresses?\n     * if so, change Ib to Ob and change dx to i_dx (move to dest for out)\n     */\n    {OP_in,  0xe40000, \"in\", al, xx, Ib, xx, xx, no, x, tfb[0xed]},\n    {OP_in,  0xe50000, \"in\", zAX, xx, Ib, xx, xx, no, x, tfb[0xe4]},\n    {OP_out,  0xe60000, \"out\", xx, xx, Ib, al, xx, no, x, tfb[0xef]},\n    {OP_out,  0xe70000, \"out\", xx, xx, Ib, zAX, xx, no, x, tfb[0xe6]},\n    /* e8 */\n    {OP_call,     0xe80000, \"call\",  xsp, i_iSPo1, Jz, xsp, xx, no, x, END_LIST},\n    {OP_jmp,       0xe90000, \"jmp\", xx, xx, Jz, xx, xx, no, x, END_LIST},\n    {OP_jmp_far,   0xea0000, \"ljmp\", xx, xx, Ap, xx, xx, i64, x, END_LIST},\n    {OP_jmp_short, 0xeb0000, \"jmp\", xx, xx, Jb, xx, xx, no, x, END_LIST},\n    {OP_in,  0xec0000, \"in\", al, xx, dx, xx, xx, no, x, END_LIST},\n    {OP_in,  0xed0000, \"in\", zAX, xx, dx, xx, xx, no, x, tfb[0xec]},\n    {OP_out,  0xee0000, \"out\", xx, xx, al, dx, xx, no, x, END_LIST},\n    {OP_out,  0xef0000, \"out\", xx, xx, zAX, dx, xx, no, x, tfb[0xee]},\n    /* f0 */\n    {PREFIX, 0xf00000, \"lock\", xx, xx, xx, xx, xx, no, x, PREFIX_LOCK},\n    /* Also called OP_icebp.  Undocumented.  I'm assuming looks like OP_int* */\n    {OP_int1, 0xf10000, \"int1\", xx, xx, xx, xx, xx, no, fINT, END_LIST},\n    {PREFIX, 0xf20000, \"repne\", xx, xx, xx, xx, xx, no, x, PREFIX_REPNE},\n    {PREFIX, 0xf30000, \"rep\", xx, xx, xx, xx, xx, no, x, PREFIX_REP},\n    {OP_hlt,  0xf40000, \"hlt\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_cmc,  0xf50000, \"cmc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {EXTENSION, 0xf60000, \"(group 3a)\", Eb, xx, xx, xx, xx, mrm, x, 9},\n    {EXTENSION, 0xf70000, \"(group 3b)\", Ev, xx, xx, xx, xx, mrm, x, 10},\n    /* f8 */\n    {OP_clc,  0xf80000, \"clc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {OP_stc,  0xf90000, \"stc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {OP_cli,  0xfa0000, \"cli\", xx, xx, xx, xx, xx, no, fWI, END_LIST},\n    {OP_sti,  0xfb0000, \"sti\", xx, xx, xx, xx, xx, no, fWI, END_LIST},\n    {OP_cld,  0xfc0000, \"cld\", xx, xx, xx, xx, xx, no, fWD, END_LIST},\n    {OP_std,  0xfd0000, \"std\", xx, xx, xx, xx, xx, no, fWD, END_LIST},\n    {EXTENSION, 0xfe0000, \"(group 4)\", xx, xx, xx, xx, xx, mrm, x, 11},\n    {EXTENSION, 0xff0000, \"(group 5)\", xx, xx, xx, xx, xx, mrm, x, 12},\n};\n/****************************************************************************\n * Two-byte opcodes\n * This is from Tables A-4 & A-5\n */\nconst instr_info_t second_byte[] = {\n  /* 00 */\n  {EXTENSION, 0x0f0010, \"(group 6)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  {EXTENSION, 0x0f0110, \"(group 7)\", xx, xx, xx, xx, xx, mrm, x, 14},\n  {OP_lar, 0x0f0210, \"lar\", Gv, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n  {OP_lsl, 0x0f0310, \"lsl\", Gv, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n  {INVALID, 0x0f0410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* XXX: writes ss and cs */\n  {OP_syscall, 0x0f0510, \"syscall\", xcx, xx, xx, xx, xx, no, x, NA}, /* AMD/x64 only */\n  {OP_clts, 0x0f0610, \"clts\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  /* XXX: writes ss and cs */\n  {OP_sysret, 0x0f0710, \"sysret\", xx, xx, xx, xx, xx, no, x, NA}, /* AMD/x64 only */\n  /* 08 */\n  {OP_invd, 0x0f0810, \"invd\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {OP_wbinvd, 0x0f0910, \"wbinvd\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {INVALID, 0x0f0a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {OP_ud2a, 0x0f0b10, \"ud2a\", xx, xx, xx, xx, xx, no, x, END_LIST}, /* \"undefined instr\" instr */\n  {INVALID, 0x0f0c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {EXTENSION, 0x0f0d10, \"(group amd)\", xx, xx, xx, xx, xx, mrm, x, 24}, /* AMD only */\n  {OP_femms, 0x0f0e10, \"femms\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {SUFFIX_EXT, 0x0f0f10, \"(group 3DNow!)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  /* 10 */\n  {PREFIX_EXT, 0x0f1010, \"(prefix ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  {PREFIX_EXT, 0x0f1110, \"(prefix ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n  {PREFIX_EXT, 0x0f1210, \"(prefix ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n  {PREFIX_EXT, 0x0f1310, \"(prefix ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n  {PREFIX_EXT, 0x0f1410, \"(prefix ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n  {PREFIX_EXT, 0x0f1510, \"(prefix ext 5)\", xx, xx, xx, xx, xx, mrm, x, 5},\n  {PREFIX_EXT, 0x0f1610, \"(prefix ext 6)\", xx, xx, xx, xx, xx, mrm, x, 6},\n  {PREFIX_EXT, 0x0f1710, \"(prefix ext 7)\", xx, xx, xx, xx, xx, mrm, x, 7},\n  /* 18 */\n  {EXTENSION, 0x0f1810, \"(group 16)\", xx, xx, xx, xx, xx, mrm, x, 23},\n  /* xref case 9862/PR 214297 : 0f19-0f1e are \"HINT_NOP\": valid on P6+.\n   * we treat them the same as 0f1f but do not put on encoding chain.\n   * The operand is ignored but to support encoding it we must list it.\n   * i453: analysis routines now special case nop_modrm to ignore src opnd */\n  {OP_nop_modrm, 0x0f1910, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1a10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1b10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1c10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1d10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1e10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1f10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  /* 20 */\n  {OP_mov_priv, 0x0f2010, \"mov\", Rr, xx, Cr, xx, xx, mrm, fW6, tsb[0x21]},\n  {OP_mov_priv, 0x0f2110, \"mov\", Rr, xx, Dr, xx, xx, mrm, fW6, tsb[0x22]},\n  {OP_mov_priv, 0x0f2210, \"mov\", Cr, xx, Rr, xx, xx, mrm, fW6, tsb[0x23]},\n  {OP_mov_priv, 0x0f2310, \"mov\", Dr, xx, Rr, xx, xx, mrm, fW6, END_LIST},\n  {INVALID, 0x0f2410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* FIXME: gdb thinks ok! */\n  {INVALID, 0x0f2510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f2610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* FIXME: gdb thinks ok! */\n  {INVALID, 0x0f2710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* 28 */\n  {PREFIX_EXT, 0x0f2810, \"(prefix ext 8)\", xx, xx, xx, xx, xx, mrm, x, 8},\n  {PREFIX_EXT, 0x0f2910, \"(prefix ext 9)\", xx, xx, xx, xx, xx, mrm, x, 9},\n  {PREFIX_EXT, 0x0f2a10, \"(prefix ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},\n  {PREFIX_EXT, 0x0f2b10, \"(prefix ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},\n  {PREFIX_EXT, 0x0f2c10, \"(prefix ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},\n  {PREFIX_EXT, 0x0f2d10, \"(prefix ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  {PREFIX_EXT, 0x0f2e10, \"(prefix ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},\n  {PREFIX_EXT, 0x0f2f10, \"(prefix ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},\n  /* 30 */\n  {OP_wrmsr, 0x0f3010, \"wrmsr\", xx, xx, edx, eax, ecx, no, x, END_LIST},\n  {OP_rdtsc, 0x0f3110, \"rdtsc\", edx, eax, xx, xx, xx, no, x, END_LIST},\n  {OP_rdmsr, 0x0f3210, \"rdmsr\", edx, eax, ecx, xx, xx, no, x, END_LIST},\n  {OP_rdpmc, 0x0f3310, \"rdpmc\", edx, eax, ecx, xx, xx, no, x, END_LIST},\n  /* XXX: sysenter writes cs and ss */\n  {OP_sysenter, 0x0f3410, \"sysenter\", xsp, xx, xx, xx, xx, no, x, END_LIST},\n  /* XXX: sysexit writes cs and ss */\n  {OP_sysexit, 0x0f3510, \"sysexit\", xsp, xx, xcx, xx, xx, no, x, END_LIST},\n  {INVALID, 0x0f3610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* XXX i#1313: various getsec leaf funcs at CPL 0 write to all kinds of\n   * processor state including eflags and eip.  Leaf funcs are indicated by eax\n   * value, though.  Here we only model the CPL > 0 effects, which conditionally\n   * write to ebx + ecx.\n   */\n  {OP_getsec, 0x0f3710, \"getsec\", eax, ebx, eax, ebx, xx, xop|predcx, x, exop[13]},\n  /* 38 */\n  {ESCAPE_3BYTE_38, 0x0f3810, \"(3byte 38)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {ESCAPE_3BYTE_3a, 0x0f3a10, \"(3byte 3a)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* 40 */\n  {OP_cmovo,   0x0f4010, \"cmovo\",  Gv, xx, Ev, xx, xx, mrm|predcc, fRO, END_LIST},\n  {E_VEX_EXT, 0x0f4110, \"(e_vex ext 83)\", xx, xx, xx, xx, xx, mrm, x, 83},\n  {E_VEX_EXT, 0x0f4210, \"(e_vex ext 84)\", xx, xx, xx, xx, xx, mrm, x, 84},\n  {OP_cmovnb,  0x0f4310, \"cmovnb\", Gv, xx, Ev, xx, xx, mrm|predcc, fRC, END_LIST},\n  {E_VEX_EXT, 0x0f4410, \"(e_vex ext 86)\", xx, xx, xx, xx, xx, mrm, x, 86},\n  {E_VEX_EXT, 0x0f4510, \"(e_vex ext 87)\", xx, xx, xx, xx, xx, mrm, x, 87},\n  {E_VEX_EXT, 0x0f4610, \"(e_vex ext 88)\", xx, xx, xx, xx, xx, mrm, x, 88},\n  {E_VEX_EXT, 0x0f4710, \"(e_vex ext 89)\", xx, xx, xx, xx, xx, mrm, x, 89},\n  /* 48 */\n  {OP_cmovs,  0x0f4810, \"cmovs\",  Gv, xx, Ev, xx, xx, mrm|predcc, fRS, END_LIST},\n  {OP_cmovns, 0x0f4910, \"cmovns\", Gv, xx, Ev, xx, xx, mrm|predcc, fRS, END_LIST},\n  {E_VEX_EXT, 0x0f4a10, \"(e_vex ext 90)\", xx, xx, xx, xx, xx, mrm, x, 90},\n  {E_VEX_EXT, 0x0f4b10, \"(e_vex ext 85)\", xx, xx, xx, xx, xx, mrm, x, 85},\n  {OP_cmovl,  0x0f4c10, \"cmovl\",  Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO), END_LIST},\n  {OP_cmovnl, 0x0f4d10, \"cmovnl\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO), END_LIST},\n  {OP_cmovle, 0x0f4e10, \"cmovle\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO|fRZ), END_LIST},\n  {OP_cmovnle,0x0f4f10, \"cmovnle\",Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO|fRZ), END_LIST},\n  /* 50 */\n  {PREFIX_EXT, 0x0f5010, \"(prefix ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},\n  {PREFIX_EXT, 0x0f5110, \"(prefix ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},\n  {PREFIX_EXT, 0x0f5210, \"(prefix ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},\n  {PREFIX_EXT, 0x0f5310, \"(prefix ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},\n  {PREFIX_EXT, 0x0f5410, \"(prefix ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},\n  {PREFIX_EXT, 0x0f5510, \"(prefix ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},\n  {PREFIX_EXT, 0x0f5610, \"(prefix ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},\n  {PREFIX_EXT, 0x0f5710, \"(prefix ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},\n  /* 58 */\n  {PREFIX_EXT, 0x0f5810, \"(prefix ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},\n  {PREFIX_EXT, 0x0f5910, \"(prefix ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},\n  {PREFIX_EXT, 0x0f5a10, \"(prefix ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},\n  {PREFIX_EXT, 0x0f5b10, \"(prefix ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},\n  {PREFIX_EXT, 0x0f5c10, \"(prefix ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},\n  {PREFIX_EXT, 0x0f5d10, \"(prefix ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},\n  {PREFIX_EXT, 0x0f5e10, \"(prefix ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},\n  {PREFIX_EXT, 0x0f5f10, \"(prefix ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},\n  /* 60 */\n  {PREFIX_EXT, 0x0f6010, \"(prefix ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},\n  {PREFIX_EXT, 0x0f6110, \"(prefix ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},\n  {PREFIX_EXT, 0x0f6210, \"(prefix ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},\n  {PREFIX_EXT, 0x0f6310, \"(prefix ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},\n  {PREFIX_EXT, 0x0f6410, \"(prefix ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},\n  {PREFIX_EXT, 0x0f6510, \"(prefix ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},\n  {PREFIX_EXT, 0x0f6610, \"(prefix ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},\n  {PREFIX_EXT, 0x0f6710, \"(prefix ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},\n  /* 68 */\n  {PREFIX_EXT, 0x0f6810, \"(prefix ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},\n  {PREFIX_EXT, 0x0f6910, \"(prefix ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},\n  {PREFIX_EXT, 0x0f6a10, \"(prefix ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},\n  {PREFIX_EXT, 0x0f6b10, \"(prefix ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},\n  {PREFIX_EXT, 0x0f6c10, \"(prefix ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},\n  {PREFIX_EXT, 0x0f6d10, \"(prefix ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},\n  {PREFIX_EXT, 0x0f6e10, \"(prefix ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},\n  {PREFIX_EXT, 0x0f6f10, \"(prefix ext 112)\", xx, xx, xx, xx, xx, mrm, x, 112},\n  /* 70 */\n  {PREFIX_EXT, 0x0f7010, \"(prefix ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},\n  {EXTENSION, 0x0f7110, \"(group 12)\", xx, xx, xx, xx, xx, mrm, x, 19},\n  {EXTENSION, 0x0f7210, \"(group 13)\", xx, xx, xx, xx, xx, mrm, x, 20},\n  {EXTENSION, 0x0f7310, \"(group 14)\", xx, xx, xx, xx, xx, mrm, x, 21},\n  {PREFIX_EXT, 0x0f7410, \"(prefix ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},\n  {PREFIX_EXT, 0x0f7510, \"(prefix ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},\n  {PREFIX_EXT, 0x0f7610, \"(prefix ext 50)\", xx, xx, xx, xx, xx, mrm, x, 50},\n  {VEX_L_EXT,  0x0f7710, \"(vex L ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n  /* 78 */\n  {PREFIX_EXT, 0x0f7810, \"(prefix ext 134)\", xx, xx, xx, xx, xx, mrm, x, 134},\n  {PREFIX_EXT, 0x0f7910, \"(prefix ext 135)\", xx, xx, xx, xx, xx, mrm, x, 135},\n  {INVALID, 0x0f7a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f7b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {PREFIX_EXT, 0x0f7c10, \"(prefix ext 114)\", xx, xx, xx, xx, xx, mrm, x, 114},\n  {PREFIX_EXT, 0x0f7d10, \"(prefix ext 115)\", xx, xx, xx, xx, xx, mrm, x, 115},\n  {PREFIX_EXT, 0x0f7e10, \"(prefix ext 51)\", xx, xx, xx, xx, xx, mrm, x, 51},\n  {PREFIX_EXT, 0x0f7f10, \"(prefix ext 113)\", xx, xx, xx, xx, xx, mrm, x, 113},\n  /* 80 */\n  {OP_jo,  0x0f8010, \"jo\",  xx, xx, Jz, xx, xx, no, fRO, END_LIST},\n  {OP_jno, 0x0f8110, \"jno\", xx, xx, Jz, xx, xx, no, fRO, END_LIST},\n  {OP_jb,  0x0f8210, \"jb\",  xx, xx, Jz, xx, xx, no, fRC, END_LIST},\n  {OP_jnb, 0x0f8310, \"jnb\", xx, xx, Jz, xx, xx, no, fRC, END_LIST},\n  {OP_jz,  0x0f8410, \"jz\",  xx, xx, Jz, xx, xx, no, fRZ, END_LIST},\n  {OP_jnz, 0x0f8510, \"jnz\", xx, xx, Jz, xx, xx, no, fRZ, END_LIST},\n  {OP_jbe, 0x0f8610, \"jbe\", xx, xx, Jz, xx, xx, no, (fRC|fRZ), END_LIST},\n  {OP_jnbe,0x0f8710, \"jnbe\",xx, xx, Jz, xx, xx, no, (fRC|fRZ), END_LIST},\n  /* 88 */\n  {OP_js,  0x0f8810, \"js\",  xx, xx, Jz, xx, xx, no, fRS, END_LIST},\n  {OP_jns, 0x0f8910, \"jns\", xx, xx, Jz, xx, xx, no, fRS, END_LIST},\n  {OP_jp,  0x0f8a10, \"jp\",  xx, xx, Jz, xx, xx, no, fRP, END_LIST},\n  {OP_jnp, 0x0f8b10, \"jnp\", xx, xx, Jz, xx, xx, no, fRP, END_LIST},\n  {OP_jl,  0x0f8c10, \"jl\",  xx, xx, Jz, xx, xx, no, (fRS|fRO), END_LIST},\n  {OP_jnl, 0x0f8d10, \"jnl\", xx, xx, Jz, xx, xx, no, (fRS|fRO), END_LIST},\n  {OP_jle, 0x0f8e10, \"jle\", xx, xx, Jz, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n  {OP_jnle,0x0f8f10, \"jnle\",xx, xx, Jz, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n  /* 90 */\n  {E_VEX_EXT, 0x0f9010, \"(e_vex ext 79)\", xx, xx, xx, xx, xx, mrm, x, 79},\n  {E_VEX_EXT, 0x0f9110, \"(e_vex ext 80)\", xx, xx, xx, xx, xx, mrm, x, 80},\n  {E_VEX_EXT, 0x0f9210, \"(e_vex ext 81)\", xx, xx, xx, xx, xx, mrm, x, 81},\n  {E_VEX_EXT, 0x0f9310, \"(e_vex ext 82)\", xx, xx, xx, xx, xx, mrm, x, 82},\n  {OP_setz,  0x0f9410, \"setz\",  Eb, xx, xx, xx, xx, mrm, fRZ, END_LIST},\n  {OP_setnz, 0x0f9510, \"setnz\", Eb, xx, xx, xx, xx, mrm, fRZ, END_LIST},\n  {OP_setbe, 0x0f9610, \"setbe\", Eb, xx, xx, xx, xx, mrm, (fRC|fRZ), END_LIST},\n  {OP_setnbe,0x0f9710, \"setnbe\",Eb, xx, xx, xx, xx, mrm, (fRC|fRZ), END_LIST},\n  /* 98 */\n  {E_VEX_EXT, 0x0f9810, \"(e_vex ext 91)\", xx, xx, xx, xx, xx, mrm, x, 91},\n  {E_VEX_EXT, 0x0f9910, \"(e_vex ext 92)\", xx, xx, xx, xx, xx, mrm, x, 92},\n  {OP_setp,  0x0f9a10, \"setp\",  Eb, xx, xx, xx, xx, mrm, fRP, END_LIST},\n  {OP_setnp, 0x0f9b10, \"setnp\", Eb, xx, xx, xx, xx, mrm, fRP, END_LIST},\n  {OP_setl,  0x0f9c10, \"setl\",  Eb, xx, xx, xx, xx, mrm, (fRS|fRO), END_LIST},\n  {OP_setnl, 0x0f9d10, \"setnl\", Eb, xx, xx, xx, xx, mrm, (fRS|fRO), END_LIST},\n  {OP_setle, 0x0f9e10, \"setle\", Eb, xx, xx, xx, xx, mrm, (fRS|fRO|fRZ), END_LIST},\n  {OP_setnle,0x0f9f10, \"setnle\",Eb, xx, xx, xx, xx, mrm, (fRS|fRO|fRZ), END_LIST},\n  /* a0 */\n  {OP_push, 0x0fa010, \"push\", xsp, i_xSPo1, fs, xsp, xx, no, x, tsb[0xa8]},\n  {OP_pop,  0x0fa110, \"pop\", fs, xsp, xsp, i_xSP, xx, no, x, tsb[0xa9]},\n  {OP_cpuid, 0x0fa210, \"cpuid\", eax, ebx, eax, ecx, xx, xop, x, exop[0x06]},\n  {OP_bt,   0x0fa310, \"bt\",   xx, xx, Ev, Gv, xx, mrm, fW6, tex[15][4]},\n  {OP_shld, 0x0fa410, \"shld\", Ev, xx, Gv, Ib, Ev, mrm, fW6, tsb[0xa5]},\n  {OP_shld, 0x0fa510, \"shld\", Ev, xx, Gv, cl, Ev, mrm, fW6, END_LIST},\n  {INVALID, 0x0fa610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0fa710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* a8 */\n  {OP_push, 0x0fa810, \"push\", xsp, i_xSPo1, gs, xsp, xx, no, x, END_LIST},\n  {OP_pop,  0x0fa910, \"pop\", gs, xsp, xsp, i_xSP, xx, no, x, END_LIST},\n  {OP_rsm,  0x0faa10, \"rsm\", xx, xx, xx, xx, xx, no, fWX, END_LIST},\n  {OP_bts,  0x0fab10, \"bts\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][5]},\n  {OP_shrd, 0x0fac10, \"shrd\", Ev, xx, Gv, Ib, Ev, mrm, fW6, tsb[0xad]},\n  {OP_shrd, 0x0fad10, \"shrd\", Ev, xx, Gv, cl, Ev, mrm, fW6, END_LIST},\n  {EXTENSION, 0x0fae10, \"(group 15)\", xx, xx, xx, xx, xx, mrm, x, 22},\n  {OP_imul, 0x0faf10, \"imul\", Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x69]},\n  /* b0 */\n  {OP_cmpxchg, 0x0fb010, \"cmpxchg\", Eb, al, Gb, Eb, al, mrm, fW6, END_LIST},\n  {OP_cmpxchg, 0x0fb110, \"cmpxchg\", Ev, eAX, Gv, Ev, eAX, mrm, fW6, tsb[0xb0]},\n  {OP_lss, 0x0fb210, \"lss\", Gv, ss, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_btr, 0x0fb310, \"btr\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][6]},\n  {OP_lfs, 0x0fb410, \"lfs\", Gv, fs, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_lgs, 0x0fb510, \"lgs\", Gv, gs, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_movzx, 0x0fb610, \"movzx\", Gv, xx, Eb, xx, xx, mrm, x, END_LIST},\n  {OP_movzx, 0x0fb710, \"movzx\", Gv, xx, Ew, xx, xx, mrm, x, tsb[0xb6]},\n  /* b8 */\n  {OP_popcnt, 0xf30fb810, \"popcnt\", Gv, xx, Ev, xx, xx, mrm|reqp, fW6, END_LIST},\n  /* This is Group 10, but all identical (ud2b) so no reason to split opcode by /reg */\n  {OP_ud2b, 0x0fb910, \"ud2b\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {EXTENSION, 0x0fba10, \"(group 8)\", xx, xx, xx, xx, xx, mrm, x, 15},\n  {OP_btc, 0x0fbb10, \"btc\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][7]},\n  {PREFIX_EXT, 0x0fbc10, \"(prefix ext 140)\", xx, xx, xx, xx, xx, mrm, x, 140},\n  {PREFIX_EXT, 0x0fbd10, \"(prefix ext 136)\", xx, xx, xx, xx, xx, mrm, x, 136},\n  {OP_movsx, 0x0fbe10, \"movsx\", Gv, xx, Eb, xx, xx, mrm, x, END_LIST},\n  {OP_movsx, 0x0fbf10, \"movsx\", Gv, xx, Ew, xx, xx, mrm, x, tsb[0xbe]},\n  /* c0 */\n  {OP_xadd, 0x0fc010, \"xadd\", Eb, Gb, Eb, Gb, xx, mrm, fW6, END_LIST},\n  {OP_xadd, 0x0fc110, \"xadd\", Ev, Gv, Ev, Gv, xx, mrm, fW6, tsb[0xc0]},\n  {PREFIX_EXT, 0x0fc210, \"(prefix ext 52)\", xx, xx, xx, xx, xx, mrm, x, 52},\n  {OP_movnti, 0x0fc310, \"movnti\", Md_q, xx, Gd_q, xx, xx, mrm, x, END_LIST},\n  {PREFIX_EXT, 0x0fc410, \"(prefix ext 53)\", xx, xx, xx, xx, xx, mrm, x, 53},\n  {PREFIX_EXT, 0x0fc510, \"(prefix ext 54)\", xx, xx, xx, xx, xx, mrm, x, 54},\n  {PREFIX_EXT, 0x0fc610, \"(prefix ext 55)\", xx, xx, xx, xx, xx, mrm, x, 55},\n  {EXTENSION, 0x0fc710, \"(group 9)\", xx, xx, xx, xx, xx, mrm, x, 16},\n  /* c8 */\n  {OP_bswap, 0x0fc810, \"bswap\", uAX_x, xx, uAX_x, xx, xx, no, x, tsb[0xc9]},\n  {OP_bswap, 0x0fc910, \"bswap\", uCX_x, xx, uCX_x, xx, xx, no, x, tsb[0xca]},\n  {OP_bswap, 0x0fca10, \"bswap\", uDX_x, xx, uDX_x, xx, xx, no, x, tsb[0xcb]},\n  {OP_bswap, 0x0fcb10, \"bswap\", uBX_x, xx, uBX_x, xx, xx, no, x, tsb[0xcc]},\n  {OP_bswap, 0x0fcc10, \"bswap\", uSP_x, xx, uSP_x, xx, xx, no, x, tsb[0xcd]},\n  {OP_bswap, 0x0fcd10, \"bswap\", uBP_x, xx, uBP_x, xx, xx, no, x, tsb[0xce]},\n  {OP_bswap, 0x0fce10, \"bswap\", uSI_x, xx, uSI_x, xx, xx, no, x, tsb[0xcf]},\n  {OP_bswap, 0x0fcf10, \"bswap\", uDI_x, xx, uDI_x, xx, xx, no, x, END_LIST},\n  /* d0 */\n  {PREFIX_EXT, 0x0fd010, \"(prefix ext 116)\", xx, xx, xx, xx, xx, mrm, x, 116},\n  {PREFIX_EXT, 0x0fd110, \"(prefix ext 56)\", xx, xx, xx, xx, xx, mrm, x, 56},\n  {PREFIX_EXT, 0x0fd210, \"(prefix ext 57)\", xx, xx, xx, xx, xx, mrm, x, 57},\n  {PREFIX_EXT, 0x0fd310, \"(prefix ext 58)\", xx, xx, xx, xx, xx, mrm, x, 58},\n  {PREFIX_EXT, 0x0fd410, \"(prefix ext 59)\", xx, xx, xx, xx, xx, mrm, x, 59},\n  {PREFIX_EXT, 0x0fd510, \"(prefix ext 60)\", xx, xx, xx, xx, xx, mrm, x, 60},\n  {PREFIX_EXT, 0x0fd610, \"(prefix ext 61)\", xx, xx, xx, xx, xx, mrm, x, 61},\n  {PREFIX_EXT, 0x0fd710, \"(prefix ext 62)\", xx, xx, xx, xx, xx, mrm, x, 62},\n  /* d8 */\n  {PREFIX_EXT, 0x0fd810, \"(prefix ext 63)\", xx, xx, xx, xx, xx, mrm, x, 63},\n  {PREFIX_EXT, 0x0fd910, \"(prefix ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},\n  {PREFIX_EXT, 0x0fda10, \"(prefix ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},\n  {PREFIX_EXT, 0x0fdb10, \"(prefix ext 66)\", xx, xx, xx, xx, xx, mrm, x, 66},\n  {PREFIX_EXT, 0x0fdc10, \"(prefix ext 67)\", xx, xx, xx, xx, xx, mrm, x, 67},\n  {PREFIX_EXT, 0x0fdd10, \"(prefix ext 68)\", xx, xx, xx, xx, xx, mrm, x, 68},\n  {PREFIX_EXT, 0x0fde10, \"(prefix ext 69)\", xx, xx, xx, xx, xx, mrm, x, 69},\n  {PREFIX_EXT, 0x0fdf10, \"(prefix ext 70)\", xx, xx, xx, xx, xx, mrm, x, 70},\n  /* e0 */\n  {PREFIX_EXT, 0x0fe010, \"(prefix ext 71)\", xx, xx, xx, xx, xx, mrm, x, 71},\n  {PREFIX_EXT, 0x0fe110, \"(prefix ext 72)\", xx, xx, xx, xx, xx, mrm, x, 72},\n  {PREFIX_EXT, 0x0fe210, \"(prefix ext 73)\", xx, xx, xx, xx, xx, mrm, x, 73},\n  {PREFIX_EXT, 0x0fe310, \"(prefix ext 74)\", xx, xx, xx, xx, xx, mrm, x, 74},\n  {PREFIX_EXT, 0x0fe410, \"(prefix ext 75)\", xx, xx, xx, xx, xx, mrm, x, 75},\n  {PREFIX_EXT, 0x0fe510, \"(prefix ext 76)\", xx, xx, xx, xx, xx, mrm, x, 76},\n  {PREFIX_EXT, 0x0fe610, \"(prefix ext 77)\", xx, xx, xx, xx, xx, mrm, x, 77},\n  {PREFIX_EXT, 0x0fe710, \"(prefix ext 78)\", xx, xx, xx, xx, xx, mrm, x, 78},\n  /* e8 */\n  {PREFIX_EXT, 0x0fe810, \"(prefix ext 79)\", xx, xx, xx, xx, xx, mrm, x, 79},\n  {PREFIX_EXT, 0x0fe910, \"(prefix ext 80)\", xx, xx, xx, xx, xx, mrm, x, 80},\n  {PREFIX_EXT, 0x0fea10, \"(prefix ext 81)\", xx, xx, xx, xx, xx, mrm, x, 81},\n  {PREFIX_EXT, 0x0feb10, \"(prefix ext 82)\", xx, xx, xx, xx, xx, mrm, x, 82},\n  {PREFIX_EXT, 0x0fec10, \"(prefix ext 83)\", xx, xx, xx, xx, xx, mrm, x, 83},\n  {PREFIX_EXT, 0x0fed10, \"(prefix ext 84)\", xx, xx, xx, xx, xx, mrm, x, 84},\n  {PREFIX_EXT, 0x0fee10, \"(prefix ext 85)\", xx, xx, xx, xx, xx, mrm, x, 85},\n  {PREFIX_EXT, 0x0fef10, \"(prefix ext 86)\", xx, xx, xx, xx, xx, mrm, x, 86},\n  /* f0 */\n  {PREFIX_EXT, 0x0ff010, \"(prefix ext 117)\", xx, xx, xx, xx, xx, mrm, x, 117},\n  {PREFIX_EXT, 0x0ff110, \"(prefix ext 87)\", xx, xx, xx, xx, xx, mrm, x, 87},\n  {PREFIX_EXT, 0x0ff210, \"(prefix ext 88)\", xx, xx, xx, xx, xx, mrm, x, 88},\n  {PREFIX_EXT, 0x0ff310, \"(prefix ext 89)\", xx, xx, xx, xx, xx, mrm, x, 89},\n  {PREFIX_EXT, 0x0ff410, \"(prefix ext 90)\", xx, xx, xx, xx, xx, mrm, x, 90},\n  {PREFIX_EXT, 0x0ff510, \"(prefix ext 91)\", xx, xx, xx, xx, xx, mrm, x, 91},\n  {PREFIX_EXT, 0x0ff610, \"(prefix ext 92)\", xx, xx, xx, xx, xx, mrm, x, 92},\n  {PREFIX_EXT, 0x0ff710, \"(prefix ext 93)\", xx, xx, xx, xx, xx, mrm, x, 93},\n  /* f8 */\n  {PREFIX_EXT, 0x0ff810, \"(prefix ext 94)\", xx, xx, xx, xx, xx, mrm, x, 94},\n  {PREFIX_EXT, 0x0ff910, \"(prefix ext 95)\", xx, xx, xx, xx, xx, mrm, x, 95},\n  {PREFIX_EXT, 0x0ffa10, \"(prefix ext 96)\", xx, xx, xx, xx, xx, mrm, x, 96},\n  {PREFIX_EXT, 0x0ffb10, \"(prefix ext 97)\", xx, xx, xx, xx, xx, mrm, x, 97},\n  {PREFIX_EXT, 0x0ffc10, \"(prefix ext 98)\", xx, xx, xx, xx, xx, mrm, x, 98},\n  {PREFIX_EXT, 0x0ffd10, \"(prefix ext 99)\", xx, xx, xx, xx, xx, mrm, x, 99},\n  {PREFIX_EXT, 0x0ffe10, \"(prefix ext 100)\", xx, xx, xx, xx, xx, mrm, x, 100},\n  {INVALID, 0x0fff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n};\n\n/****************************************************************************\n * Opcode extensions\n * This is from Table A-6\n */\nconst instr_info_t base_extensions[][8] = {\n  /* group 1a -- first opcode byte 80: all assumed to have Ib */\n  { /* extensions[0] */\n    {OP_add, 0x800020, \"add\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][0]},\n    {OP_or,  0x800021, \"or\",  Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][1]},\n    {OP_adc, 0x800022, \"adc\", Eb, xx, Ib, Eb, xx, mrm, (fW6|fRC), tex[25][2]},\n    {OP_sbb, 0x800023, \"sbb\", Eb, xx, Ib, Eb, xx, mrm, (fW6|fRC), tex[25][3]},\n    {OP_and, 0x800024, \"and\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][4]},\n    {OP_sub, 0x800025, \"sub\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][5]},\n    {OP_xor, 0x800026, \"xor\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][6]},\n    {OP_cmp, 0x800027, \"cmp\", xx, xx, Eb, Ib, xx, mrm, fW6,  tex[25][7]},\n },\n  /* group 1b -- first opcode byte 81: all assumed to have Iz */\n  { /* extensions[1] */\n    {OP_add, 0x810020, \"add\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][0]},\n    {OP_or,  0x810021, \"or\",  Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][1]},\n    {OP_adc, 0x810022, \"adc\", Ev, xx, Iz, Ev, xx, mrm, (fW6|fRC), tex[2][2]},\n    {OP_sbb, 0x810023, \"sbb\", Ev, xx, Iz, Ev, xx, mrm, (fW6|fRC), tex[2][3]},\n    {OP_and, 0x810024, \"and\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][4]},\n    {OP_sub, 0x810025, \"sub\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][5]},\n    {OP_xor, 0x810026, \"xor\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][6]},\n    {OP_cmp, 0x810027, \"cmp\", xx, xx, Ev, Iz, xx, mrm, fW6,  tex[2][7]},\n },\n  /* group 1c -- first opcode byte 83 (for 82, see below \"group 1c*\"):\n   * all assumed to have Ib */\n  { /* extensions[2] */\n    {OP_add, 0x830020, \"add\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][0]},\n    {OP_or,  0x830021, \"or\",  Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][1]},\n    {OP_adc, 0x830022, \"adc\", Ev, xx, Ib, Ev, xx, mrm, (fW6|fRC), tex[0][2]},\n    {OP_sbb, 0x830023, \"sbb\", Ev, xx, Ib, Ev, xx, mrm, (fW6|fRC), tex[0][3]},\n    {OP_and, 0x830024, \"and\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][4]},\n    {OP_sub, 0x830025, \"sub\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][5]},\n    {OP_xor, 0x830026, \"xor\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][6]},\n    {OP_cmp, 0x830027, \"cmp\", xx, xx, Ev, Ib, xx, mrm, fW6,  tex[0][7]},\n },\n  /* group 2a -- first opcode byte c0: all assumed to have Ib */\n  { /* extensions[3] */\n    {OP_rol, 0xc00020, \"rol\", Eb, xx, Ib, Eb, xx, mrm, (fWC|fWO),  tex[5][0]},\n    {OP_ror, 0xc00021, \"ror\", Eb, xx, Ib, Eb, xx, mrm, (fWC|fWO),  tex[5][1]},\n    {OP_rcl, 0xc00022, \"rcl\", Eb, xx, Ib, Eb, xx, mrm, (fRC|fWC|fWO), tex[5][2]},\n    {OP_rcr, 0xc00023, \"rcr\", Eb, xx, Ib, Eb, xx, mrm, (fRC|fWC|fWO), tex[5][3]},\n    {OP_shl, 0xc00024, \"shl\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][4]},\n    {OP_shr, 0xc00025, \"shr\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xc00026, \"shl\", Eb, xx, Ib, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xc00027, \"sar\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][7]},\n },\n  /* group 2b -- first opcode byte c1: all assumed to have Ib */\n  { /* extensions[4] */\n    {OP_rol, 0xc10020, \"rol\", Ev, xx, Ib, Ev, xx, mrm, (fWC|fWO),  tex[6][0]},\n    {OP_ror, 0xc10021, \"ror\", Ev, xx, Ib, Ev, xx, mrm, (fWC|fWO),  tex[6][1]},\n    {OP_rcl, 0xc10022, \"rcl\", Ev, xx, Ib, Ev, xx, mrm, (fRC|fWC|fWO), tex[6][2]},\n    {OP_rcr, 0xc10023, \"rcr\", Ev, xx, Ib, Ev, xx, mrm, (fRC|fWC|fWO), tex[6][3]},\n    {OP_shl, 0xc10024, \"shl\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][4]},\n    {OP_shr, 0xc10025, \"shr\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xc10026, \"shl\", Ev, xx, Ib, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xc10027, \"sar\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][7]},\n },\n  /* group 2c -- first opcode byte d0 */\n  { /* extensions[5] */\n    {OP_rol, 0xd00020, \"rol\", Eb, xx, c1, Eb, xx, mrm, (fWC|fWO),  tex[8][0]},\n    {OP_ror, 0xd00021, \"ror\", Eb, xx, c1, Eb, xx, mrm, (fWC|fWO),  tex[8][1]},\n    {OP_rcl, 0xd00022, \"rcl\", Eb, xx, c1, Eb, xx, mrm, (fRC|fWC|fWO), tex[8][2]},\n    {OP_rcr, 0xd00023, \"rcr\", Eb, xx, c1, Eb, xx, mrm, (fRC|fWC|fWO), tex[8][3]},\n    {OP_shl, 0xd00024, \"shl\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][4]},\n    {OP_shr, 0xd00025, \"shr\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd00026, \"shl\", Eb, xx, c1, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd00027, \"sar\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][7]},\n },\n  /* group 2d -- first opcode byte d1 */\n  { /* extensions[6] */\n    {OP_rol, 0xd10020, \"rol\", Ev, xx, c1, Ev, xx, mrm, (fWC|fWO),  tex[3][0]},\n    {OP_ror, 0xd10021, \"ror\", Ev, xx, c1, Ev, xx, mrm, (fWC|fWO),  tex[3][1]},\n    {OP_rcl, 0xd10022, \"rcl\", Ev, xx, c1, Ev, xx, mrm, (fRC|fWC|fWO), tex[3][2]},\n    {OP_rcr, 0xd10023, \"rcr\", Ev, xx, c1, Ev, xx, mrm, (fRC|fWC|fWO), tex[3][3]},\n    {OP_shl, 0xd10024, \"shl\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][4]},\n    {OP_shr, 0xd10025, \"shr\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd10026, \"shl\", Ev, xx, c1, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd10027, \"sar\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][7]},\n },\n  /* group 2e -- first opcode byte d2 */\n  { /* extensions[7] */\n    {OP_rol, 0xd20020, \"rol\", Eb, xx, cl, Eb, xx, mrm, (fWC|fWO),  END_LIST},\n    {OP_ror, 0xd20021, \"ror\", Eb, xx, cl, Eb, xx, mrm, (fWC|fWO),  END_LIST},\n    {OP_rcl, 0xd20022, \"rcl\", Eb, xx, cl, Eb, xx, mrm, (fRC|fWC|fWO), END_LIST},\n    {OP_rcr, 0xd20023, \"rcr\", Eb, xx, cl, Eb, xx, mrm, (fRC|fWC|fWO), END_LIST},\n    {OP_shl, 0xd20024, \"shl\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_shr, 0xd20025, \"shr\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd20026, \"shl\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd20027, \"sar\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n },\n  /* group 2f -- first opcode byte d3 */\n  { /* extensions[8] */\n    {OP_rol, 0xd30020, \"rol\", Ev, xx, cl, Ev, xx, mrm, (fWC|fWO),  tex[7][0]},\n    {OP_ror, 0xd30021, \"ror\", Ev, xx, cl, Ev, xx, mrm, (fWC|fWO),  tex[7][1]},\n    {OP_rcl, 0xd30022, \"rcl\", Ev, xx, cl, Ev, xx, mrm, (fRC|fWC|fWO), tex[7][2]},\n    {OP_rcr, 0xd30023, \"rcr\", Ev, xx, cl, Ev, xx, mrm, (fRC|fWC|fWO), tex[7][3]},\n    {OP_shl, 0xd30024, \"shl\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][4]},\n    {OP_shr, 0xd30025, \"shr\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd30026, \"shl\", Ev, xx, cl, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd30027, \"sar\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][7]},\n },\n  /* group 3a -- first opcode byte f6 */\n  { /* extensions[9] */\n    {OP_test, 0xf60020, \"test\", xx, xx, Eb, Ib, xx, mrm, fW6, END_LIST},\n    /* PR 332254: /1 is an alias for /0; we do not add to encoding chain though */\n    {OP_test, 0xf60021, \"test\", xx, xx, Eb, Ib, xx, mrm, fW6, END_LIST},\n    {OP_not,  0xf60022, \"not\", Eb, xx, Eb, xx, xx, mrm, x, END_LIST},\n    {OP_neg,  0xf60023, \"neg\", Eb, xx, Eb, xx, xx, mrm, fW6, END_LIST},\n    {OP_mul,  0xf60024, \"mul\", ax, xx, Eb, al, xx, mrm, fW6, END_LIST},\n    {OP_imul, 0xf60025, \"imul\", ax, xx, Eb, al, xx, mrm, fW6, tsb[0xaf]},\n    {OP_div,  0xf60026, \"div\", ah, al, Eb, ax, xx, mrm, fW6, END_LIST},\n    {OP_idiv, 0xf60027, \"idiv\", ah, al, Eb, ax, xx, mrm, fW6, END_LIST},\n },\n  /* group 3b -- first opcode byte f7 */\n  { /* extensions[10] */\n    {OP_test, 0xf70020, \"test\", xx,  xx, Ev, Iz, xx, mrm, fW6, tex[9][0]},\n    /* PR 332254: /1 is an alias for /0; we do not add to encoding chain though */\n    {OP_test, 0xf70021, \"test\", xx,  xx, Ev, Iz, xx, mrm, fW6, END_LIST},\n    {OP_not,  0xf70022, \"not\", Ev,  xx, Ev, xx, xx, mrm, x, tex[9][2]},\n    {OP_neg,  0xf70023, \"neg\", Ev,  xx, Ev, xx, xx, mrm, fW6, tex[9][3]},\n    {OP_mul,  0xf70024, \"mul\",   eDX, eAX, Ev, eAX, xx, mrm, fW6, tex[9][4]},\n    {OP_imul, 0xf70025, \"imul\",  eDX, eAX, Ev, eAX, xx, mrm, fW6, tex[9][5]},\n    {OP_div,  0xf70026, \"div\",   eDX, eAX, Ev, eDX, eAX, mrm, fW6, tex[9][6]},\n    {OP_idiv, 0xf70027, \"idiv\",  eDX, eAX, Ev, eDX, eAX, mrm, fW6, tex[9][7]},\n },\n  /* group 4 (first byte fe) */\n  { /* extensions[11] */\n    {OP_inc, 0xfe0020, \"inc\", Eb, xx, Eb, xx, xx, mrm, (fW6&(~fWC)), END_LIST},\n    {OP_dec, 0xfe0021, \"dec\", Eb, xx, Eb, xx, xx, mrm, (fW6&(~fWC)), END_LIST},\n    {INVALID, 0xfe0022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 5 (first byte ff) */\n  { /* extensions[12] */\n    {OP_inc, 0xff0020, \"inc\", Ev, xx, Ev, xx, xx, mrm, (fW6&(~fWC)), tex[11][0]},\n    {OP_dec, 0xff0021, \"dec\", Ev, xx, Ev, xx, xx, mrm, (fW6&(~fWC)), tex[11][1]},\n    {OP_call_ind,     0xff0022, \"call\",  xsp, i_iSPo1, i_Exi, xsp, xx, mrm, x, END_LIST},\n    /* Note how a far call's stack operand size matches far ret rather than call */\n    {OP_call_far_ind, 0xff0023, \"lcall\",  xsp, i_vSPo2, i_Ep, xsp, xx, mrm, x, END_LIST},\n    {OP_jmp_ind,      0xff0024, \"jmp\",  xx, xx, i_Exi, xx, xx, mrm, x, END_LIST},\n    {OP_jmp_far_ind,  0xff0025, \"ljmp\",  xx, xx, i_Ep, xx, xx, mrm, x, END_LIST},\n    {OP_push, 0xff0026, \"push\", xsp, i_xSPo1, Esv, xsp, xx, mrm, x, tfb[0x06]},\n    {INVALID, 0xff0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 6 (first bytes 0f 00) */\n  { /* extensions[13] */\n    {OP_sldt, 0x0f0030, \"sldt\", Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_str,  0x0f0031, \"str\", Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_lldt, 0x0f0032, \"lldt\", xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {OP_ltr,  0x0f0033, \"ltr\", xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {OP_verr, 0x0f0034, \"verr\", xx, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n    {OP_verw, 0x0f0035, \"verw\", xx, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n    {INVALID, 0x0f0036, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0037, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 7 (first bytes 0f 01) */\n  { /* extensions[14] */\n    {MOD_EXT, 0x0f0130, \"(group 7 mod ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {MOD_EXT, 0x0f0131, \"(group 7 mod ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {MOD_EXT, 0x0f0132, \"(group 7 mod ext 5)\", xx, xx, xx, xx, xx, no, x, 5},\n    {MOD_EXT, 0x0f0133, \"(group 7 mod ext 4)\", xx, xx, xx, xx, xx, no, x, 4},\n    {OP_smsw, 0x0f0134, \"smsw\",  Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f0135, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_lmsw, 0x0f0136, \"lmsw\",  xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {MOD_EXT, 0x0f0137, \"(group 7 mod ext 2)\", xx, xx, xx, xx, xx, no, x, 2},\n  },\n  /* group 8 (first bytes 0f ba): all assumed to have Ib */\n  { /* extensions[15] */\n    {INVALID, 0x0fba30, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba31, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba32, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba33, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bt,  0x0fba34, \"bt\",    xx, xx, Ev, Ib, xx, mrm, fW6, END_LIST},\n    {OP_bts, 0x0fba35, \"bts\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n    {OP_btr, 0x0fba36, \"btr\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n    {OP_btc, 0x0fba37, \"btc\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n  },\n  /* group 9 (first bytes 0f c7) */\n  { /* extensions[16] */\n    {INVALID, 0x0fc730, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_cmpxchg8b, 0x0fc731, \"cmpxchg8b\", Mq_dq, eAX, Mq_dq, eAX, eDX, mrm_xop, fWZ, exop[0x07]},/*\"cmpxchg16b\" w/ rex.w*/\n    {INVALID, 0x0fc732, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fc733, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {REX_W_EXT, 0x0fc734, \"(rex.w ext 5)\", xx, xx, xx, xx, xx, mrm, x, 5},\n    {INVALID, 0x0fc735, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {MOD_EXT, 0x0fc736, \"(group 9 mod ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},\n    {MOD_EXT, 0x0fc737, \"(mod ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  },\n  /* group 10 is all ud2b and is not used by us since identical */\n  /* group 11a (first byte c6) */\n  { /* extensions[17] */\n    {OP_mov_st, 0xc60020, \"mov\", Eb, xx, Ib, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xc60021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* XXX i#1314: this also sets eip */\n    {OP_xabort, 0xf8c60067, \"xabort\", eax, xx, Ib, xx, xx, mrm, x, END_LIST},\n  },\n  /* group 11b (first byte c7) */\n  { /* extensions[18] */\n    /* PR 250397: be aware that mov_imm shares this tail end of mov_st templates */\n    {OP_mov_st, 0xc70020, \"mov\", Ev, xx, Iz, xx, xx, mrm, x, tex[17][0]},\n    {INVALID, 0xc70021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_xbegin, 0xf8c70067, \"xbegin\", xx, xx, Jz, xx, xx, mrm, x, END_LIST},\n  },\n  /* group 12 (first bytes 0f 71): all assumed to have Ib */\n  { /* extensions[19] */\n    {INVALID, 0x0f7130, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7132, \"(prefix ext 104)\", xx, xx, xx, xx, xx, no, x, 104},\n    {INVALID, 0x0f7133, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7134, \"(prefix ext 105)\", xx, xx, xx, xx, xx, no, x, 105},\n    {INVALID, 0x0f7135, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7136, \"(prefix ext 106)\", xx, xx, xx, xx, xx, no, x, 106},\n    {INVALID, 0x0f7137, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 13 (first bytes 0f 72): all assumed to have Ib */\n  { /* extensions[20] */\n    {INVALID, 0x0f7230, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7231, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7232, \"(prefix ext 107)\", xx, xx, xx, xx, xx, no, x, 107},\n    {INVALID, 0x0f7233, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7234, \"(prefix ext 108)\", xx, xx, xx, xx, xx, no, x, 108},\n    {INVALID, 0x0f7235, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7236, \"(prefix ext 109)\", xx, xx, xx, xx, xx, no, x, 109},\n    {INVALID, 0x0f7237, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 14 (first bytes 0f 73): all assumed to have Ib */\n  { /* extensions[21] */\n    {INVALID, 0x0f7330, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7331, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7332, \"(prefix ext 110)\", xx, xx, xx, xx, xx, no, x, 110},\n    {PREFIX_EXT, 0x0f7333, \"(prefix ext 101)\", xx, xx, xx, xx, xx, no, x, 101},\n    {INVALID, 0x0f7334, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7335, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7336, \"(prefix ext 111)\", xx, xx, xx, xx, xx, no, x, 111},\n    {PREFIX_EXT, 0x0f7337, \"(prefix ext 102)\", xx, xx, xx, xx, xx, no, x, 102},\n  },\n  /* group 15 (first bytes 0f ae) */\n  { /* extensions[22] */\n    /* Intel tables imply they may add opcodes in the mod=3 (non-mem) space in future */\n    {MOD_EXT,    0x0fae30, \"(group 15 mod ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},\n    {MOD_EXT,    0x0fae31, \"(group 15 mod ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},\n    {MOD_EXT,    0x0fae32, \"(group 15 mod ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},\n    {MOD_EXT,    0x0fae33, \"(group 15 mod ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},\n    {REX_W_EXT,  0x0fae34, \"(rex.w ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n    {MOD_EXT,    0x0fae35, \"(group 15 mod ext 6)\", xx, xx, xx, xx, xx, no, x, 6},\n    {MOD_EXT,    0x0fae36, \"(group 15 mod ext 7)\", xx, xx, xx, xx, xx, no, x, 7},\n    {MOD_EXT,    0x0fae37, \"(group 15 mod ext 3)\", xx, xx, xx, xx, xx, no, x, 3},\n },\n  /* group 16 (first bytes 0f 18) */\n  { /* extensions[23] */\n    /* Intel tables imply they may add opcodes in the mod=3 (non-mem) space in future */\n    {OP_prefetchnta, 0x0f1830, \"prefetchnta\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht0,  0x0f1831, \"prefetcht0\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht1,  0x0f1832, \"prefetcht1\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht2,  0x0f1833, \"prefetcht2\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1834, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1835, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1836, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1837, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n },\n  /* group AMD (first bytes 0f 0d) */\n  { /* extensions[24] */\n    {OP_prefetch,  0x0f0d30, \"prefetch\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetchw, 0x0f0d31, \"prefetchw\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f0d32, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d33, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d34, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d35, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d36, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d37, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* group 1c* -- first opcode byte 82\n   * see PR 235092 for the discrepancies in what 0x82 should be: empirically\n   * and according to recent Intel manuals it matches 0x80, not 0x83 (as old\n   * Intel manuals implied) or invalid (as gnu tools claim).\n   * not linked into any encode chain.\n   */\n  { /* extensions[25]: all assumed to have Ib */\n    {OP_add, 0x820020, \"add\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_or,  0x820021, \"or\",  Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_adc, 0x820022, \"adc\", Eb, xx, Ib, Eb, xx, mrm|i64, (fW6|fRC), END_LIST},\n    {OP_sbb, 0x820023, \"sbb\", Eb, xx, Ib, Eb, xx, mrm|i64, (fW6|fRC), END_LIST},\n    {OP_and, 0x820024, \"and\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_sub, 0x820025, \"sub\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_xor, 0x820026, \"xor\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_cmp, 0x820027, \"cmp\", xx, xx, Eb, Ib, xx, mrm|i64, fW6,  END_LIST},\n  },\n  /* group 1d (Intel now calling Group 1A) -- first opcode byte 8f */\n  { /* extensions[26] */\n    {OP_pop,  0x8f0020, \"pop\", Esv, xsp, xsp, i_xSP, xx, mrm, x, tfb[0x17]},\n    /* we shouldn't ever get here for these, as this becomes an XOP prefix */\n    {INVALID, 0x8f0021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 1 */\n  { /* extensions[27] */\n    {INVALID,     0x090138, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blcfill,  0x090139, \"blcfill\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsfill,  0x09013a, \"blsfill\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blcs,     0x09013b, \"blcs\",    By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_tzmsk,    0x09013c, \"tzmsk\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blcic,    0x09013d, \"blcic\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsic,    0x09013e, \"blsic\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_t1mskc,   0x09013f, \"t1mskc\",  By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n  },\n  /* XOP group 2 */\n  { /* extensions[28] */\n    {INVALID,     0x090238, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blcmsk,   0x090239, \"blcmsk\",By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x09023a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blci,     0x09023e, \"blci\",  By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x09023f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 3 */\n  { /* extensions[29] */\n    /* XXX i#1311: these instrs implicitly write to memory which we should\n     * find a way to encode into the IR.\n     */\n    {OP_llwpcb,   0x091238, \"llwpcb\", xx, xx, Ry, xx, xx, mrm|vex, x, END_LIST},\n    {OP_slwpcb,   0x091239, \"slwpcb\", Ry, xx, xx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0x09123a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123e, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 4: all assumed to have a 4-byte immediate by xop_a_extra[] */\n  { /* extensions[30] */\n    /* XXX i#1311: these instrs implicitly write to memory which we should\n     * find a way to encode into the IR.\n     */\n    {OP_lwpins,   0x0a1238, \"lwpins\", xx, xx, By, Ed, Id, mrm|vex, fWC, END_LIST},\n    {OP_lwpval,   0x0a1239, \"lwpval\", xx, xx, By, Ed, Id, mrm|vex, x, END_LIST},\n    {INVALID,     0x0a123a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123e, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* group 17 */\n  { /* extensions[31] */\n    {INVALID,     0x38f338, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blsr,     0x38f339, \"blsr\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsmsk,   0x38f33a, \"blsmsk\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsi,     0x38f33b, \"blsi\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x38f33c, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33d, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33e, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33f, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Two-byte instructions that differ depending on presence of\n * prefixes, indexed in this order:\n *   none, 0xf3, 0x66, 0xf2\n * A second set is used for vex-encoded instructions, indexed in the\n * same order by prefix.\n * A third set is used for evex-encoded instructions, indexed in the\n * same order by prefix.\n *\n * N.B.: to avoid having a full entry here when there is only one\n * valid opcode prefix, use |reqp in the original entry instead of\n * pointing to this table.\n */\nconst instr_info_t prefix_extensions[][12] = {\n  /* prefix extension 0 */\n  {\n    {OP_movups, 0x0f1010, \"movups\", Vps, xx, Wps, xx, xx, mrm, x, tpe[1][0]},\n    {MOD_EXT,   0xf30f1010, \"(mod ext 18)\",  xx, xx, xx, xx, xx, mrm, x, 18},\n    {OP_movupd, 0x660f1010, \"movupd\", Vpd, xx, Wpd, xx, xx, mrm, x, tpe[1][2]},\n    {MOD_EXT,   0xf20f1010, \"(mod ext 19)\",  xx, xx, xx, xx, xx, mrm, x, 19},\n    {OP_vmovups,   0x0f1010, \"vmovups\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tpe[1][4]},\n    {MOD_EXT,    0xf30f1010, \"(mod ext 8)\", xx, xx, xx, xx, xx, mrm|vex, x, 8},\n    {OP_vmovupd, 0x660f1010, \"vmovupd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, tpe[1][6]},\n    {MOD_EXT,    0xf20f1010, \"(mod ext 9)\", xx, xx, xx, xx, xx, mrm|vex, x, 9},\n    {EVEX_W_EXT, 0x0f1010, \"(evex_W ext 0)\", xx, xx, xx, xx, xx, mrm|evex, x, 0},\n    {MOD_EXT,    0xf30f1010, \"(mod ext 20)\", xx, xx, xx, xx, xx, mrm|evex, x, 20},\n    {EVEX_W_EXT, 0x660f1010, \"(evex_W ext 2)\", xx, xx, xx, xx, xx, mrm|evex, x, 2},\n    {MOD_EXT,    0xf20f1010, \"(mod ext 21)\", xx, xx, xx, xx, xx, mrm|evex, x, 21},\n  },\n  /* prefix extension 1 */\n  {\n    {OP_movups, 0x0f1110, \"movups\", Wps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {OP_movss,  0xf30f1110, \"movss\",  Wss, xx, Vss, xx, xx, mrm, x, END_LIST},\n    {OP_movupd, 0x660f1110, \"movupd\", Wpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {OP_movsd,  0xf20f1110, \"movsd\",  Wsd, xx, Vsd, xx, xx, mrm, x, END_LIST},\n    {OP_vmovups,   0x0f1110, \"vmovups\", Wvs, xx, Vvs, xx, xx, mrm|vex, x, tevexw[0][0]},\n    {MOD_EXT,    0xf30f1110, \"(mod ext 10)\", xx, xx, xx, xx, xx, mrm|vex, x, 10},\n    {OP_vmovupd, 0x660f1110, \"vmovupd\", Wvd, xx, Vvd, xx, xx, mrm|vex, x, tevexw[2][1]},\n    {MOD_EXT,    0xf20f1110, \"(mod ext 11)\", xx, xx, xx, xx, xx, mrm|vex, x, 11},\n    {EVEX_W_EXT, 0x0f1110, \"(evex_W ext 1)\", xx, xx, xx, xx, xx, mrm|evex, x, 1},\n    {MOD_EXT,    0xf30f1110, \"(mod ext 22)\", xx, xx, xx, xx, xx, mrm|evex, x, 22},\n    {EVEX_W_EXT, 0x660f1110, \"(evex_W ext 3)\", xx, xx, xx, xx, xx, mrm|evex, x, 3},\n    {MOD_EXT,    0xf20f1110, \"(mod ext 23)\", xx, xx, xx, xx, xx, mrm|evex, x, 23},\n  },\n  /* prefix extension 2 */\n  {\n    /* i#319: note that the reg-reg form of the load version (0f12) is legal\n     * and has a separate pneumonic (\"movhlps\"), yet the reg-reg form of\n     * the store version (0f13) is illegal\n     */\n    {OP_movlps, 0x0f1210, \"movlps\", Vq_dq, xx, Wq_dq, xx, xx, mrm, x, tpe[3][0]}, /*\"movhlps\" if reg-reg */\n    {OP_movsldup, 0xf30f1210, \"movsldup\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_movlpd, 0x660f1210, \"movlpd\", Vq_dq, xx, Mq, xx, xx, mrm, x, tpe[3][2]},\n    {OP_movddup, 0xf20f1210, \"movddup\", Vpd, xx, Wq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_vmovlps,    0x0f1210, \"vmovlps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|vex|reqL0, x, tpe[3][4]}, /*\"vmovhlps\" if reg-reg */\n    {OP_vmovsldup,0xf30f1210, \"vmovsldup\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tevexw[18][0]},\n    {OP_vmovlpd,  0x660f1210, \"vmovlpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|vex, x, tpe[3][6]},\n    {OP_vmovddup, 0xf20f1210, \"vmovddup\", Vvd, xx, Wh_x, xx, xx, mrm|vex, x, tevexw[19][1]},\n    {EVEX_W_EXT, 0x0f1210, \"(evex_W ext 14)\", xx, xx, xx, xx, xx, mrm|evex, x, 14},\n    {EVEX_W_EXT, 0xf30f1210, \"(evex_W ext 18)\", xx, xx, xx, xx, xx, mrm|evex, x, 18},\n    {EVEX_W_EXT, 0x660f1210, \"(evex_W ext 16)\", xx, xx, xx, xx, xx, mrm|evex, x, 16},\n    {EVEX_W_EXT, 0xf20f1210, \"(evex_W ext 19)\", xx, xx, xx, xx, xx, mrm|evex, x, 19},\n  },\n  /* prefix extension 3 */\n  {\n    {OP_movlps, 0x0f1310, \"movlps\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movlpd, 0x660f1310, \"movlpd\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovlps, 0x0f1310, \"vmovlps\", Mq, xx, Vq_dq, xx, xx, mrm|vex, x, tevexw[14][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovlpd, 0x660f1310, \"vmovlpd\", Mq, xx, Vq_dq, xx, xx, mrm|vex, x, tevexw[16][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f1310, \"(evex_W ext 15)\", xx, xx, xx, xx, xx, mrm|evex, x, 15},\n    {INVALID, 0xf30f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1310, \"(evex_W ext 17)\", xx, xx, xx, xx, xx, mrm|evex, x, 17},\n    {INVALID, 0xf20f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 4 */\n  {\n    {OP_unpcklps, 0x0f1410, \"unpcklps\", Vps, xx, Wq_dq, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_unpcklpd, 0x660f1410, \"unpcklpd\", Vpd, xx, Wq_dq, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpcklps, 0x0f1410, \"vunpcklps\", Vvs, xx, Hh_x, Wh_x, xx, mrm|vex, x, tevexw[25][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpcklpd, 0x660f1410, \"vunpcklpd\", Vvd, xx, Hh_x, Wh_x, xx, mrm|vex, x, tevexw[26][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f1410, \"(evex_W ext 25)\", xx, xx, xx, xx, xx, mrm|evex, x, 25},\n    {INVALID, 0xf30f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1410, \"(evex_W ext 26)\", xx, xx, xx, xx, xx, mrm|evex, x, 26},\n    {INVALID, 0xf20f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 5 */\n  {\n    {OP_unpckhps, 0x0f1510, \"unpckhps\", Vps, xx, Wq_dq, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_unpckhpd, 0x660f1510, \"unpckhpd\", Vpd, xx, Wq_dq, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpckhps, 0x0f1510, \"vunpckhps\", Vvs, xx, Hh_x, Wh_x, xx, mrm|vex, x, tevexw[27][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpckhpd, 0x660f1510, \"vunpckhpd\", Vvd, xx, Hh_x, Wh_x, xx, mrm|vex, x, tevexw[28][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f1510, \"(evex_W ext 27)\", xx, xx, xx, xx, xx, mrm|evex, x, 27},\n    {INVALID, 0xf30f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1510, \"(evex_W ext 28)\", xx, xx, xx, xx, xx, mrm|evex, x, 28},\n    {INVALID, 0xf20f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 6 */\n  {\n    /* i#319: note that the reg-reg form of the load version (0f16) is legal\n     * and has a separate pneumonic (\"movhlps\"), yet the reg-reg form of\n     * the store version (0f17) is illegal\n     */\n    {OP_movhps, 0x0f1610, \"movhps\", Vq_dq, xx, Wq_dq, xx, xx, mrm, x, tpe[7][0]}, /*\"movlhps\" if reg-reg */\n    {OP_movshdup, 0xf30f1610, \"movshdup\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_movhpd, 0x660f1610, \"movhpd\", Vq_dq, xx, Mq, xx, xx, mrm, x, tpe[7][2]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhps, 0x0f1610, \"vmovhps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|vex|reqL0, x, tpe[7][4]}, /*\"vmovlhps\" if reg-reg */\n    {OP_vmovshdup, 0xf30f1610, \"vmovshdup\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tevexw[24][0]},\n    {OP_vmovhpd, 0x660f1610, \"vmovhpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|vex|reqL0, x, tpe[7][6]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f1610, \"(evex_W ext 20)\", xx, xx, xx, xx, xx, mrm|evex, x, 20},\n    {EVEX_W_EXT, 0xf30f1610, \"(evex_W ext 24)\", xx, xx, xx, xx, xx, mrm|evex, x, 24},\n    {EVEX_W_EXT, 0x660f1610, \"(evex_W ext 22)\", xx, xx, xx, xx, xx, mrm|evex, x, 22},\n    {INVALID, 0xf20f1610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 7 */\n  {\n    {OP_movhps, 0x0f1710, \"movhps\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movhpd, 0x660f1710, \"movhpd\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhps, 0x0f1710, \"vmovhps\", Mq, xx, Vq_dq, xx, xx, mrm|vex|reqL0, x, tevexw[20][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhpd, 0x660f1710, \"vmovhpd\", Mq, xx, Vq_dq, xx, xx, mrm|vex|reqL0, x, tevexw[22][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f1710, \"(evex_W ext 21)\", xx, xx, xx, xx, xx, mrm|evex, x, 21},\n    {INVALID, 0xf30f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1710, \"(evex_W ext 23)\", xx, xx, xx, xx, xx, mrm|evex, x, 23},\n    {INVALID, 0xf20f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 8 */\n  {\n    {OP_movaps, 0x0f2810, \"movaps\", Vps, xx, Wps, xx, xx, mrm, x, tpe[9][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movapd, 0x660f2810, \"movapd\", Vpd, xx, Wpd, xx, xx, mrm, x, tpe[9][2]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovaps, 0x0f2810, \"vmovaps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tpe[9][4]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovapd, 0x660f2810, \"vmovapd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, tpe[9][6]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT,   0x0f2810, \"(evex_W ext 4)\", xx, xx, xx, xx, xx, mrm|evex, x, 4},\n    {INVALID,    0xf30f2810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2810, \"(evex_W ext 6)\", xx, xx, xx, xx, xx, mrm|evex, x, 6},\n    {INVALID,    0xf20f2810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 9 */\n  {\n    {OP_movaps, 0x0f2910, \"movaps\", Wps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movapd, 0x660f2910, \"movapd\", Wpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovaps, 0x0f2910, \"vmovaps\", Wvs, xx, Vvs, xx, xx, mrm|vex, x, tevexw[4][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovapd, 0x660f2910, \"vmovapd\", Wvd, xx, Vvd, xx, xx, mrm|vex, x, tevexw[6][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT,   0x0f2910, \"(evex_W ext 5)\", xx, xx, xx, xx, xx, mrm|evex, x, 5},\n    {INVALID,    0xf30f2910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2910, \"(evex_W ext 7)\", xx, xx, xx, xx, xx, mrm|evex, x, 7},\n    {INVALID,    0xf20f2910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 10 */\n  {\n    {OP_cvtpi2ps,  0x0f2a10, \"cvtpi2ps\", Vq_dq, xx, Qq, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsi2ss, 0xf30f2a10, \"cvtsi2ss\", Vss, xx, Ed_q, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpi2pd, 0x660f2a10, \"cvtpi2pd\", Vpd, xx, Qq, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsi2sd, 0xf20f2a10, \"cvtsi2sd\", Vsd, xx, Ed_q, xx, xx, mrm, x, END_LIST},\n    {INVALID,  0x0f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsi2ss, 0xf30f2a10, \"vcvtsi2ss\", Vdq, xx, H12_dq, Ed_q, xx, mrm|vex, x, tevexw[31][0]},\n    {INVALID, 0x660f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsi2sd, 0xf20f2a10, \"vcvtsi2sd\", Vdq, xx, Hsd, Ed_q, xx, mrm|vex, x, tevexw[32][0]},\n    {INVALID,   0x0f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf30f2a10, \"(evex_W ext 31)\", xx, xx, xx, xx, xx, mrm|evex, x, 31},\n    {INVALID, 0x660f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf20f2a10, \"(evex_W ext 32)\", xx, xx, xx, xx, xx, mrm|evex, x, 32},\n  },\n  /* prefix extension 11 */\n  {\n    {OP_movntps,   0x0f2b10, \"movntps\", Mps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {OP_movntss, 0xf30f2b10, \"movntss\", Mss, xx, Vss, xx, xx, mrm, x, END_LIST},\n    {OP_movntpd, 0x660f2b10, \"movntpd\", Mpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {OP_movntsd, 0xf20f2b10, \"movntsd\", Msd, xx, Vsd, xx, xx, mrm, x, END_LIST},\n    {OP_vmovntps,   0x0f2b10, \"vmovntps\", Mvs, xx, Vvs, xx, xx, mrm|vex, x, tevexw[33][0]},\n    /* XXX: AMD doesn't list movntss in their new manual => assuming no vex version */\n    {INVALID, 0xf30f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovntpd, 0x660f2b10, \"vmovntpd\", Mvd, xx, Vvd, xx, xx, mrm|vex, x, tevexw[34][1]},\n    {INVALID, 0xf20f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f2b10, \"(evex_W ext 33)\", xx, xx, xx, xx, xx, mrm|evex, x, 33},\n    {INVALID, 0xf30f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2b10, \"(evex_W ext 34)\", xx, xx, xx, xx, xx, mrm|evex, x, 34},\n    {INVALID, 0xf20f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 12 */\n  {\n    {OP_cvttps2pi, 0x0f2c10, \"cvttps2pi\", Pq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvttss2si, 0xf30f2c10, \"cvttss2si\", Gd_q, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvttpd2pi, 0x660f2c10, \"cvttpd2pi\", Pq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvttsd2si, 0xf20f2c10, \"cvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvttss2si, 0xf30f2c10, \"vcvttss2si\", Gd_q, xx, Wss, xx, xx, mrm|vex, x, tevexw[35][0]},\n    {INVALID, 0x660f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvttsd2si, 0xf20f2c10, \"vcvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|vex, x, tevexw[36][0]},\n    {INVALID,   0x0f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf30f2c10, \"(evex_W ext 35)\", xx, xx, xx, xx, xx, mrm|evex, x, 35},\n    {INVALID, 0x660f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf20f2c10, \"(evex_W ext 36)\", xx, xx, xx, xx, xx, mrm|evex, x, 36},\n  },\n  /* prefix extension 13 */\n  {\n    {OP_cvtps2pi, 0x0f2d10, \"cvtps2pi\", Pq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtss2si, 0xf30f2d10, \"cvtss2si\", Gd_q, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2pi, 0x660f2d10, \"cvtpd2pi\", Pq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsd2si, 0xf20f2d10, \"cvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtss2si, 0xf30f2d10, \"vcvtss2si\", Gd_q, xx, Wss, xx, xx, mrm|vex, x, tevexw[29][0]},\n    {INVALID, 0x660f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsd2si, 0xf20f2d10, \"vcvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|vex, x, tevexw[30][0]},\n    {INVALID,   0x0f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf30f2d10, \"(evex_W ext 29)\", xx, xx, xx, xx, xx, mrm|evex, x, 29},\n    {INVALID, 0x660f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf20f2d10, \"(evex_W ext 30)\", xx, xx, xx, xx, xx, mrm|evex, x, 30},\n  },\n  /* prefix extension 14 */\n  {\n    {OP_ucomiss, 0x0f2e10, \"ucomiss\", xx, xx, Vss, Wss, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_ucomisd, 0x660f2e10, \"ucomisd\", xx, xx, Vsd, Wsd, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vucomiss, 0x0f2e10, \"vucomiss\", xx, xx, Vss, Wss, xx, mrm|vex, fW6, tevexw[37][0]},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vucomisd, 0x660f2e10, \"vucomisd\", xx, xx, Vsd, Wsd, xx, mrm|vex, fW6, tevexw[38][1]},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f2e10, \"(evex_W ext 37)\", xx, xx, xx, xx, xx, mrm|evex, x, 37},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2e10, \"(evex_W ext 38)\", xx, xx, xx, xx, xx, mrm|evex, x, 38},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 15 */\n  {\n    {OP_comiss,  0x0f2f10, \"comiss\",  xx, xx, Vss, Wss, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_comisd,  0x660f2f10, \"comisd\",  xx, xx, Vsd, Wsd, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcomiss,  0x0f2f10, \"vcomiss\",  xx, xx, Vss, Wss, xx, mrm|vex, fW6, tevexw[39][0]},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vcomisd,  0x660f2f10, \"vcomisd\",  xx, xx, Vsd, Wsd, xx, mrm|vex, fW6, tevexw[40][1]},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x0f2e10, \"(evex_W ext 39)\", xx, xx, xx, xx, xx, mrm|evex, x, 39},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2e10, \"(evex_W ext 40)\", xx, xx, xx, xx, xx, mrm|evex, x, 40},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 16 */\n  {\n    {OP_movmskps, 0x0f5010, \"movmskps\", Gr, xx, Ups, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_movmskpd, 0x660f5010, \"movmskpd\", Gr, xx, Upd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovmskps, 0x0f5010, \"vmovmskps\", Gr, xx, Uvs, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovmskpd, 0x660f5010, \"vmovmskpd\", Gr, xx, Uvd, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 17 */\n  {\n    {OP_sqrtps, 0x0f5110, \"sqrtps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtss, 0xf30f5110, \"sqrtss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtpd, 0x660f5110, \"sqrtpd\", Vpd, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtsd, 0xf20f5110, \"sqrtsd\", Vsd, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {OP_vsqrtps, 0x0f5110, \"vsqrtps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtss, 0xf30f5110, \"vsqrtss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtpd, 0x660f5110, \"vsqrtpd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtsd, 0xf20f5110, \"vsqrtsd\", Vdq, xx, Hsd, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 18 */\n  {\n    {OP_rsqrtps, 0x0f5210, \"rsqrtps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_rsqrtss, 0xf30f5210, \"rsqrtss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vrsqrtps, 0x0f5210, \"vrsqrtps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vrsqrtss, 0xf30f5210, \"vrsqrtss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 19 */\n  {\n    {OP_rcpps, 0x0f5310, \"rcpps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_rcpss, 0xf30f5310, \"rcpss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vrcpps, 0x0f5310, \"vrcpps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vrcpss, 0xf30f5310, \"vrcpss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 20 */\n  {\n    {OP_andps,  0x0f5410, \"andps\",  Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_andpd,  0x660f5410, \"andpd\",  Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandps,  0x0f5410, \"vandps\",  Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[20][8]},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vandpd,  0x660f5410, \"vandpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[20][10]},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandps,  0x0f5410, \"vandps\",  Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandpd,  0x660f5450, \"vandpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 21 */\n  {\n    {OP_andnps, 0x0f5510, \"andnps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_andnpd, 0x660f5510, \"andnpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandnps, 0x0f5510, \"vandnps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[21][8]},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vandnpd, 0x660f5510, \"vandnpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[21][10]},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandnps, 0x0f5510, \"vandnps\", Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandnpd, 0x660f5550, \"vandnpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 22 */\n  {\n    {OP_orps,   0x0f5610, \"orps\",   Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_orpd,   0x660f5610, \"orpd\",   Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vorps,   0x0f5610, \"vorps\",   Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[22][8]},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vorpd,   0x660f5610, \"vorpd\",   Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[22][10]},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vorps, 0x0f5610, \"vorps\", Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vorpd, 0x660f5650, \"vorpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 23 */\n  {\n    {OP_xorps,  0x0f5710, \"xorps\",  Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_xorpd,  0x660f5710, \"xorpd\",  Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vxorps,  0x0f5710, \"vxorps\",  Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[23][8]},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vxorpd,  0x660f5710, \"vxorpd\",  Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[23][10]},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vxorps, 0x0f5710, \"vxorps\",  Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vxorpd, 0x660f5750, \"vxorpd\",  Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 24 */\n  {\n    {OP_addps, 0x0f5810, \"addps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_addss, 0xf30f5810, \"addss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_addpd, 0x660f5810, \"addpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_addsd, 0xf20f5810, \"addsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vaddps, 0x0f5810, \"vaddps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[24][8]},\n    {OP_vaddss, 0xf30f5810, \"vaddss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, tpe[24][9]},\n    {OP_vaddpd, 0x660f5810, \"vaddpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[24][10]},\n    {OP_vaddsd, 0xf20f5810, \"vaddsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, tpe[24][11]},\n    {OP_vaddps, 0x0f5810, \"vaddps\", Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {OP_vaddss, 0xf30f5810, \"vaddss\", Vdq, xx, KEw, Hdq, Wss, mrm|evex, x, END_LIST},\n    {OP_vaddpd, 0x660f5850, \"vaddpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {OP_vaddsd, 0xf20f5850, \"vaddsd\", Vdq, xx, KEb, Hdq, Wsd, mrm|evex, x, END_LIST},\n  },\n  /* prefix extension 25 */\n  {\n    {OP_mulps, 0x0f5910, \"mulps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_mulss, 0xf30f5910, \"mulss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_mulpd, 0x660f5910, \"mulpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_mulsd, 0xf20f5910, \"mulsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vmulps, 0x0f5910, \"vmulps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[25][8]},\n    {OP_vmulss, 0xf30f5910, \"vmulss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, tpe[25][9]},\n    {OP_vmulpd, 0x660f5910, \"vmulpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[25][10]},\n    {OP_vmulsd, 0xf20f5910, \"vmulsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, tpe[25][11]},\n    {OP_vmulps, 0x0f5910, \"vmulps\", Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {OP_vmulss, 0xf30f5910, \"vmulss\", Vdq, xx, KEw, Hdq, Wss, mrm|evex, x, END_LIST},\n    {OP_vmulpd, 0x660f5910, \"vmulpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {OP_vmulsd, 0xf20f5910, \"vmulsd\", Vdq, xx, KEb, Hdq, Wsd, mrm|evex, x, END_LIST},\n  },\n  /* prefix extension 26 */\n  {\n    {OP_cvtps2pd, 0x0f5a10, \"cvtps2pd\", Vpd, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtss2sd, 0xf30f5a10, \"cvtss2sd\", Vsd, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2ps, 0x660f5a10, \"cvtpd2ps\", Vps, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsd2ss, 0xf20f5a10, \"cvtsd2ss\", Vss, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {OP_vcvtps2pd, 0x0f5a10, \"vcvtps2pd\", Vvd, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtss2sd, 0xf30f5a10, \"vcvtss2sd\", Vdq, xx, Hsd, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtpd2ps, 0x660f5a10, \"vcvtpd2ps\", Vvs, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtsd2ss, 0xf20f5a10, \"vcvtsd2ss\", Vdq, xx, H12_dq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 27 */\n  {\n    {OP_cvtdq2ps, 0x0f5b10, \"cvtdq2ps\", Vps, xx, Wdq, xx, xx, mrm, x, END_LIST},\n    {OP_cvttps2dq, 0xf30f5b10, \"cvttps2dq\", Vdq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtps2dq, 0x660f5b10, \"cvtps2dq\", Vdq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtdq2ps, 0x0f5b10, \"vcvtdq2ps\", Vvs, xx, Wx, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvttps2dq, 0xf30f5b10, \"vcvttps2dq\", Vx, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtps2dq, 0x660f5b10, \"vcvtps2dq\", Vx, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 28 */\n  {\n    {OP_subps, 0x0f5c10, \"subps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_subss, 0xf30f5c10, \"subss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_subpd, 0x660f5c10, \"subpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_subsd, 0xf20f5c10, \"subsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vsubps, 0x0f5c10, \"vsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, tpe[28][8]},\n    {OP_vsubss, 0xf30f5c10, \"vsubss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, tpe[28][9]},\n    {OP_vsubpd, 0x660f5c10, \"vsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, tpe[28][10]},\n    {OP_vsubsd, 0xf20f5c10, \"vsubsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, tpe[28][11]},\n    {OP_vsubps, 0x0f5c10, \"vsubps\", Ves, xx, KEw, Hes, Wes, mrm|evex, x, END_LIST},\n    {OP_vsubss, 0xf30f5c10, \"vsubss\", Vdq, xx, KEw, Hdq, Wss, mrm|evex, x, END_LIST},\n    {OP_vsubpd, 0x660f5c10, \"vsubpd\", Ved, xx, KEb, Hed, Wed, mrm|evex, x, END_LIST},\n    {OP_vsubsd, 0xf20f5c10, \"vsubsd\", Vdq, xx, KEb, Hdq, Wsd, mrm|evex, x, END_LIST},\n  },\n  /* prefix extension 29 */\n  {\n    {OP_minps, 0x0f5d10, \"minps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_minss, 0xf30f5d10, \"minss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_minpd, 0x660f5d10, \"minpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_minsd, 0xf20f5d10, \"minsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vminps, 0x0f5d10, \"vminps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vminss, 0xf30f5d10, \"vminss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vminpd, 0x660f5d10, \"vminpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vminsd, 0xf20f5d10, \"vminsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 30 */\n  {\n    {OP_divps, 0x0f5e10, \"divps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_divss, 0xf30f5e10, \"divss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_divpd, 0x660f5e10, \"divpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_divsd, 0xf20f5e10, \"divsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vdivps, 0x0f5e10, \"vdivps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vdivss, 0xf30f5e10, \"vdivss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vdivpd, 0x660f5e10, \"vdivpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vdivsd, 0xf20f5e10, \"vdivsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 31 */\n  {\n    {OP_maxps, 0x0f5f10, \"maxps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_maxss, 0xf30f5f10, \"maxss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_maxpd, 0x660f5f10, \"maxpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_maxsd, 0xf20f5f10, \"maxsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vmaxps, 0x0f5f10, \"vmaxps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxss, 0xf30f5f10, \"vmaxss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxpd, 0x660f5f10, \"vmaxpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxsd, 0xf20f5f10, \"vmaxsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 32 */\n  {\n    {OP_punpcklbw,   0x0f6010, \"punpcklbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[32][2]},\n    {INVALID,      0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklbw, 0x660f6010, \"punpcklbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6010,   \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklbw, 0x660f6010, \"vpunpcklbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 33 */\n  {\n    {OP_punpcklwd,   0x0f6110, \"punpcklwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[33][2]},\n    {INVALID,      0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklwd, 0x660f6110, \"punpcklwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklwd, 0x660f6110, \"vpunpcklwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 34 */\n  {\n    {OP_punpckldq,   0x0f6210, \"punpckldq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[34][2]},\n    {INVALID,      0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckldq, 0x660f6210, \"punpckldq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckldq, 0x660f6210, \"vpunpckldq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 35 */\n  {\n    {OP_packsswb,   0x0f6310, \"packsswb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[35][2]},\n    {INVALID,     0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packsswb, 0x660f6310, \"packsswb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpacksswb, 0x660f6310, \"vpacksswb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 36 */\n  {\n    {OP_pcmpgtb,   0x0f6410, \"pcmpgtb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[36][2]},\n    {INVALID,    0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtb, 0x660f6410, \"pcmpgtb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtb, 0x660f6410, \"vpcmpgtb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 37 */\n  {\n    {OP_pcmpgtw,   0x0f6510, \"pcmpgtw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[37][2]},\n    {INVALID,    0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtw, 0x660f6510, \"pcmpgtw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtw, 0x660f6510, \"vpcmpgtw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 38 */\n  {\n    {OP_pcmpgtd,   0x0f6610, \"pcmpgtd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[38][2]},\n    {INVALID,    0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtd, 0x660f6610, \"pcmpgtd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtd, 0x660f6610, \"vpcmpgtd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 39 */\n  {\n    {OP_packuswb,   0x0f6710, \"packuswb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[39][2]},\n    {INVALID,     0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packuswb, 0x660f6710, \"packuswb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpackuswb, 0x660f6710, \"vpackuswb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 40 */\n  {\n    {OP_punpckhbw,   0x0f6810, \"punpckhbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[40][2]},\n    {INVALID,      0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhbw, 0x660f6810, \"punpckhbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhbw, 0x660f6810, \"vpunpckhbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 41 */\n  {\n    {OP_punpckhwd,   0x0f6910, \"punpckhwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[41][2]},\n    {INVALID,      0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhwd, 0x660f6910, \"punpckhwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhwd, 0x660f6910, \"vpunpckhwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 42 */\n  {\n    {OP_punpckhdq,   0x0f6a10, \"punpckhdq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[42][2]},\n    {INVALID,      0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhdq, 0x660f6a10, \"punpckhdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhdq, 0x660f6a10, \"vpunpckhdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 43 */\n  {\n    {OP_packssdw,   0x0f6b10, \"packssdw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[43][2]},\n    {INVALID,     0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packssdw, 0x660f6b10, \"packssdw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpackssdw, 0x660f6b10, \"vpackssdw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 44 */\n  {\n    {INVALID,         0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklqdq, 0x660f6c10, \"punpcklqdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,       0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklqdq, 0x660f6c10, \"vpunpcklqdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,       0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 45 */\n  {\n    {INVALID,         0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhqdq, 0x660f6d10, \"punpckhqdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,       0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhqdq, 0x660f6d10, \"vpunpckhqdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,       0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 46 */\n  {\n    /* movd zeroes the top bits when the destination is an mmx or xmm reg */\n    {OP_movd,   0x0f6e10, \"movd\", Pq, xx, Ed_q, xx, xx, mrm, x, tpe[46][2]},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movd, 0x660f6e10, \"movd\", Vdq, xx, Ed_q, xx, xx, mrm, x, tpe[51][0]},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovd, 0x660f6e10, \"vmovd\", Vdq, xx, Ed_q, xx, xx, mrm|vex, x, tpe[51][6]},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 47: all assumed to have Ib */\n  {\n    {OP_pshufw,   0x0f7010, \"pshufw\",   Pq, xx, Qq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshufhw, 0xf30f7010, \"pshufhw\", Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshufd,  0x660f7010, \"pshufd\",  Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshuflw, 0xf20f7010, \"pshuflw\", Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {INVALID,       0x0f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpshufhw, 0xf30f7010, \"vpshufhw\", Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    {OP_vpshufd,  0x660f7010, \"vpshufd\",  Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    {OP_vpshuflw, 0xf20f7010, \"vpshuflw\", Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 48 */\n  {\n    {OP_pcmpeqb,   0x0f7410, \"pcmpeqb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[48][2]},\n    {INVALID,    0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqb, 0x660f7410, \"pcmpeqb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqb, 0x660f7410, \"vpcmpeqb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 49 */\n  {\n    {OP_pcmpeqw,   0x0f7510, \"pcmpeqw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[49][2]},\n    {INVALID,    0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqw, 0x660f7510, \"pcmpeqw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqw, 0x660f7510, \"vpcmpeqw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 50 */\n  {\n    {OP_pcmpeqd,   0x0f7610, \"pcmpeqd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[50][2]},\n    {INVALID,    0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqd, 0x660f7610, \"pcmpeqd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqd, 0x660f7610, \"vpcmpeqd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 51 */\n  {\n    {OP_movd,   0x0f7e10, \"movd\", Ed_q, xx, Pd_q, xx, xx, mrm, x, tpe[51][2]},\n    /* movq zeroes the top bits when the destination is an mmx or xmm reg */\n    {OP_movq, 0xf30f7e10, \"movq\", Vdq, xx, Wq_dq, xx, xx, mrm, x, tpe[61][2]},\n    {OP_movd, 0x660f7e10, \"movd\", Ed_q, xx, Vd_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovq, 0xf30f7e10, \"vmovq\", Vdq, xx, Wq_dq, xx, xx, mrm|vex, x, tpe[61][6]},\n    {OP_vmovd, 0x660f7e10, \"vmovd\", Ed_q, xx, Vd_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 52: all assumed to have Ib */\n  {\n    {OP_cmpps, 0x0fc210, \"cmpps\", Vps, xx, Wps, Ib, Vps, mrm, x, END_LIST},\n    {OP_cmpss, 0xf30fc210, \"cmpss\", Vss, xx, Wss, Ib, Vss, mrm, x, END_LIST},\n    {OP_cmppd, 0x660fc210, \"cmppd\", Vpd, xx, Wpd, Ib, Vpd, mrm, x, END_LIST},\n    {OP_cmpsd, 0xf20fc210, \"cmpsd\", Vsd, xx, Wsd, Ib, Vsd, mrm, x, END_LIST},\n    {OP_vcmpps, 0x0fc210, \"vcmpps\", Vvs, xx, Hvs, Wvs, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmpss, 0xf30fc210, \"vcmpss\", Vdq, xx, Hdq, Wss, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmppd, 0x660fc210, \"vcmppd\", Vvd, xx, Hvd, Wvd, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmpsd, 0xf20fc210, \"vcmpsd\", Vdq, xx, Hdq, Wsd, Ib, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 53: all assumed to have Ib */\n  { /* note that gnu tools print immed first: pinsrw $0x0,(%esp),%xmm0 */\n    /* FIXME i#1388: pinsrw actually reads only bottom word of reg */\n    {OP_pinsrw,   0x0fc410, \"pinsrw\", Pw_q, xx, Rd_Mw, Ib, xx, mrm, x, tpe[53][2]},\n    {INVALID,   0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pinsrw, 0x660fc410, \"pinsrw\", Vw_dq, xx, Rd_Mw, Ib, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x0fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpinsrw, 0x660fc410, \"vpinsrw\", Vdq, xx, H14_dq, Rd_Mw, Ib, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 54: all assumed to have Ib */\n  { /* note that gnu tools print immed first: pextrw $0x7,%xmm7,%edx */\n    {OP_pextrw,   0x0fc510, \"pextrw\", Gd, xx, Nw_q, Ib, xx, mrm, x, tpe[54][2]},\n    {INVALID,   0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pextrw, 0x660fc510, \"pextrw\", Gd, xx, Uw_dq, Ib, xx, mrm, x, tvex[37][0]},\n    {INVALID,   0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x0fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpextrw, 0x660fc510, \"vpextrw\", Gd, xx, Uw_dq, Ib, xx, mrm|vex, x, tvex[37][1]},\n    {INVALID,   0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 55: all assumed to have Ib */\n  {\n    {OP_shufps, 0x0fc610, \"shufps\", Vps, xx, Wps, Ib, Vps, mrm, x, END_LIST},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_shufpd, 0x660fc610, \"shufpd\", Vpd, xx, Wpd, Ib, Vpd, mrm, x, END_LIST},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vshufps, 0x0fc610, \"vshufps\", Vvs, xx, Hvs, Wvs, Ib, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vshufpd, 0x660fc610, \"vshufpd\", Vvd, xx, Hvd, Wvd, Ib, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 56 */\n  {\n    {OP_psrlw,   0x0fd110, \"psrlw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[56][2]},\n    {INVALID,  0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrlw, 0x660fd110, \"psrlw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[104][0]},\n    {INVALID,  0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrlw,  0x660fd110, \"vpsrlw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[104][6]},\n    {INVALID,  0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 57 */\n  {\n    {OP_psrld,   0x0fd210, \"psrld\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[57][2]},\n    {INVALID,  0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrld, 0x660fd210, \"psrld\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[107][0]},\n    {INVALID,  0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrld, 0x660fd210, \"vpsrld\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[107][6]},\n    {INVALID,  0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 58 */\n  {\n    {OP_psrlq,   0x0fd310, \"psrlq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[58][2]},\n    {INVALID,  0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrlq, 0x660fd310, \"psrlq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[110][0]},\n    {INVALID,  0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrlq, 0x660fd310, \"vpsrlq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[110][6]},\n    {INVALID,  0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 59 */\n  {\n    {OP_paddq,   0x0fd410, \"paddq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[59][2]},\n    {INVALID,  0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_paddq, 0x660fd410, \"paddq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,  0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddq, 0x660fd410, \"vpaddq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[59][10]},\n    {INVALID,  0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddq, 0x660fd450, \"vpaddq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 60 */\n  {\n    {OP_pmullw,   0x0fd510, \"pmullw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[60][2]},\n    {INVALID,   0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmullw, 0x660fd510, \"pmullw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmullw, 0x660fd510, \"vpmullw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[60][10]},\n    {INVALID,   0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmullw, 0x660fd510, \"vpmullw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 61 */\n  {\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movq2dq, 0xf30fd610, \"movq2dq\", Vdq, xx, Nq, xx, xx, mrm, x, END_LIST},\n    {OP_movq, 0x660fd610, \"movq\", Wq_dq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_movdq2q, 0xf20fd610, \"movdq2q\", Pq, xx, Uq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovq, 0x660fd610, \"vmovq\", Wq_dq, xx, Vq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 62 */\n  {\n    {OP_pmovmskb,   0x0fd710, \"pmovmskb\", Gd, xx, Nq, xx, xx, mrm, x, tpe[62][2]},\n    {INVALID,     0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmovmskb, 0x660fd710, \"pmovmskb\", Gd, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,       0x0fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmovmskb, 0x660fd710, \"vpmovmskb\", Gd, xx, Ux, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 63 */\n  {\n    {OP_psubusb,   0x0fd810, \"psubusb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[63][2]},\n    {INVALID,    0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubusb, 0x660fd810, \"psubusb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubusb, 0x660fd810, \"vpsubusb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[63][10]},\n    {INVALID,    0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubusb, 0x660fd810, \"vpsubusb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 64 */\n  {\n    {OP_psubusw,   0x0fd910, \"psubusw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[64][2]},\n    {INVALID,    0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubusw, 0x660fd910, \"psubusw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubusw, 0x660fd910, \"vpsubusw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[64][10]},\n    {INVALID,    0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubusw, 0x660fd910, \"vpsubusw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 65 */\n  {\n    {OP_pminub,   0x0fda10, \"pminub\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[65][2]},\n    {INVALID,    0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pminub, 0x660fda10, \"pminub\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpminub, 0x660fda10, \"vpminub\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 66 */\n  {\n    {OP_pand,   0x0fdb10, \"pand\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[66][2]},\n    {INVALID,    0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pand, 0x660fdb10, \"pand\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpand, 0x660fdb10, \"vpand\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660fdb10, \"(evex_W ext 41)\", xx, xx, xx, xx, xx, mrm|evex, x, 41},\n    {INVALID, 0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 67 */\n  {\n    {OP_paddusb,   0x0fdc10, \"paddusb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[67][2]},\n    {INVALID,    0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddusb, 0x660fdc10, \"paddusb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddusb, 0x660fdc10, \"vpaddusb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[67][10]},\n    {INVALID,    0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddusb, 0x660fdc10, \"vpaddusb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 68 */\n  {\n    {OP_paddusw,   0x0fdd10, \"paddusw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[68][2]},\n    {INVALID,    0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddusw, 0x660fdd10, \"paddusw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddusw, 0x660fdd10, \"vpaddusw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[68][10]},\n    {INVALID,    0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddusw, 0x660fdd10, \"vpaddusw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 69 */\n  {\n    {OP_pmaxub,   0x0fde10, \"pmaxub\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[69][2]},\n    {INVALID,    0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaxub, 0x660fde10, \"pmaxub\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaxub, 0x660fde10, \"vpmaxub\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 70 */\n  {\n    {OP_pandn,   0x0fdf10, \"pandn\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[70][2]},\n    {INVALID,    0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pandn, 0x660fdf10, \"pandn\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpandn, 0x660fdf10, \"vpandn\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660fdf10, \"(evex_W ext 42)\", xx, xx, xx, xx, xx, mrm|evex, x, 42},\n    {INVALID, 0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 71 */\n  {\n    {OP_pavgb,   0x0fe010, \"pavgb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[71][2]},\n    {INVALID,    0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pavgb, 0x660fe010, \"pavgb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpavgb, 0x660fe010, \"vpavgb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 72 */\n  {\n    {OP_psraw,   0x0fe110, \"psraw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[72][2]},\n    {INVALID,    0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psraw, 0x660fe110, \"psraw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[105][0]},\n    {INVALID,    0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsraw, 0x660fe110, \"vpsraw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[105][6]},\n    {INVALID,    0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 73 */\n  {\n    {OP_psrad,   0x0fe210, \"psrad\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[73][2]},\n    {INVALID,    0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrad, 0x660fe210, \"psrad\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[108][0]},\n    {INVALID,    0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrad, 0x660fe210, \"vpsrad\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[108][6]},\n    {INVALID,    0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 74 */\n  {\n    {OP_pavgw,   0x0fe310, \"pavgw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[74][2]},\n    {INVALID,    0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pavgw, 0x660fe310, \"pavgw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpavgw, 0x660fe310, \"vpavgw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 75 */\n  {\n    {OP_pmulhuw,   0x0fe410, \"pmulhuw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[75][2]},\n    {INVALID,    0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmulhuw, 0x660fe410, \"pmulhuw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmulhuw, 0x660fe410, \"vpmulhuw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[75][10]},\n    {INVALID,    0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmulhuw, 0x660fe410, \"vpmulhuw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 76 */\n  {\n    {OP_pmulhw,   0x0fe510, \"pmulhw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[76][2]},\n    {INVALID,    0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmulhw, 0x660fe510, \"pmulhw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmulhw, 0x660fe510, \"vpmulhw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[76][10]},\n    {INVALID,    0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmulhw, 0x660fe510, \"vpmulhw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 77 */\n  {\n    {INVALID, 0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_cvtdq2pd, 0xf30fe610, \"cvtdq2pd\",  Vpd, xx, Wq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_cvttpd2dq,0x660fe610, \"cvttpd2dq\", Vdq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2dq, 0xf20fe610, \"cvtpd2dq\",  Vdq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {INVALID,        0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vcvtdq2pd, 0xf30fe610, \"vcvtdq2pd\",  Vvd, xx, Wvq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvttpd2dq,0x660fe610, \"vcvttpd2dq\", Vx, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtpd2dq, 0xf20fe610, \"vcvtpd2dq\",  Vx, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 78 */\n  {\n    {OP_movntq,    0x0fe710, \"movntq\",  Mq, xx, Pq, xx, xx, mrm, x, END_LIST},\n    {INVALID,    0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_movntdq, 0x660fe710, \"movntdq\", Mdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovntdq, 0x660fe710, \"vmovntdq\", Mx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 79 */\n  {\n    {OP_psubsb,   0x0fe810, \"psubsb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[79][2]},\n    {INVALID,    0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubsb, 0x660fe810, \"psubsb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubsb, 0x660fe810, \"vpsubsb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[79][10]},\n    {INVALID,    0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubsb, 0x660fe810, \"vpsubsb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 80 */\n  {\n    {OP_psubsw,   0x0fe910, \"psubsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[80][2]},\n    {INVALID,    0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubsw, 0x660fe910, \"psubsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubsw, 0x660fe910, \"vpsubsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[80][10]},\n    {INVALID,    0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubsw, 0x660fe910, \"vpsubsw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 81 */\n  {\n    {OP_pminsw,   0x0fea10, \"pminsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[81][2]},\n    {INVALID,    0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pminsw, 0x660fea10, \"pminsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpminsw, 0x660fea10, \"vpminsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 82 */\n  {\n    {OP_por,   0x0feb10, \"por\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[82][2]},\n    {INVALID,    0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_por, 0x660feb10, \"por\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpor, 0x660feb10, \"vpor\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660feb10, \"(evex_W ext 43)\", xx, xx, xx, xx, xx, mrm|evex, x, 43},\n    {INVALID, 0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 83 */\n  {\n    {OP_paddsb,   0x0fec10, \"paddsb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[83][2]},\n    {INVALID,    0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddsb, 0x660fec10, \"paddsb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddsb, 0x660fec10, \"vpaddsb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[83][10]},\n    {INVALID,    0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddsb, 0x660fec10, \"vpaddsb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 84 */\n  {\n    {OP_paddsw,   0x0fed10, \"paddsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[84][2]},\n    {INVALID,    0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddsw, 0x660fed10, \"paddsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddsw, 0x660fed10, \"vpaddsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[84][10]},\n    {INVALID,    0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddsw, 0x660fed10, \"vpaddsw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 85 */\n  {\n    {OP_pmaxsw,   0x0fee10, \"pmaxsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[85][2]},\n    {INVALID,    0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaxsw, 0x660fee10, \"pmaxsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaxsw, 0x660fee10, \"vpmaxsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 86 */\n  {\n    {OP_pxor,   0x0fef10, \"pxor\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[86][2]},\n    {INVALID,    0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pxor, 0x660fef10, \"pxor\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpxor, 0x660fef10, \"vpxor\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660fef10, \"(evex_W ext 44)\", xx, xx, xx, xx, xx, mrm|evex, x, 44},\n    {INVALID, 0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 87 */\n  {\n    {OP_psllw,   0x0ff110, \"psllw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[87][2]},\n    {INVALID,    0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psllw, 0x660ff110, \"psllw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[106][0]},\n    {INVALID,    0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsllw,  0x660ff110, \"vpsllw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[106][6]},\n    {INVALID,    0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 88 */\n  {\n    {OP_pslld,   0x0ff210, \"pslld\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[88][2]},\n    {INVALID,    0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pslld, 0x660ff210, \"pslld\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[109][0]},\n    {INVALID,    0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpslld, 0x660ff210, \"vpslld\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[109][6]},\n    {INVALID,    0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 89 */\n  {\n    {OP_psllq,   0x0ff310, \"psllq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[89][2]},\n    {INVALID,    0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psllq, 0x660ff310, \"psllq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[111][0]},\n    {INVALID,    0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsllq, 0x660ff310, \"vpsllq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[111][6]},\n    {INVALID,    0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 90 */\n  {\n    {OP_pmuludq,   0x0ff410, \"pmuludq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[90][2]},\n    {INVALID,    0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmuludq, 0x660ff410, \"pmuludq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmuludq, 0x660ff410, \"vpmuludq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[90][10]},\n    {INVALID,    0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmuludq, 0x660ff450, \"vpmuludq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 91 */\n  {\n    {OP_pmaddwd,   0x0ff510, \"pmaddwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[91][2]},\n    {INVALID,    0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaddwd, 0x660ff510, \"pmaddwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaddwd, 0x660ff510, \"vpmaddwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[91][10]},\n    {INVALID,    0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmaddwd, 0x660ff510, \"vpmaddwd\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 92 */\n  {\n    {OP_psadbw,   0x0ff610, \"psadbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[92][2]},\n    {INVALID,    0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psadbw, 0x660ff610, \"psadbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsadbw, 0x660ff610, \"vpsadbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 93 */\n  {\n    {OP_maskmovq,     0x0ff710, \"maskmovq\", Bq, xx, Pq, Nq, xx, mrm|predcx, x, END_LIST}, /* Intel table says \"Ppi, Qpi\" */\n    {INVALID,       0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_maskmovdqu, 0x660ff710, \"maskmovdqu\", Bdq, xx, Vdq, Udq, xx, mrm|predcx, x, END_LIST},\n    {INVALID,       0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmaskmovdqu, 0x660ff710, \"vmaskmovdqu\", Bdq, xx, Vdq, Udq, xx, mrm|vex|reqL0|predcx, x, END_LIST},\n    {INVALID,       0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 94 */\n  {\n    {OP_psubb,   0x0ff810, \"psubb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[94][2]},\n    {INVALID,    0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubb, 0x660ff810, \"psubb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubb, 0x660ff810, \"vpsubb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[94][10]},\n    {INVALID,    0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubb, 0x660ff810, \"vpsubb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 95 */\n  {\n    {OP_psubw,   0x0ff910, \"psubw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[95][2]},\n    {INVALID,    0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubw, 0x660ff910, \"psubw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubw, 0x660ff910, \"vpsubw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[95][10]},\n    {INVALID,    0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubw, 0x660ff910, \"vpsubw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 96 */\n  {\n    {OP_psubd,   0x0ffa10, \"psubd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[96][2]},\n    {INVALID,    0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubd, 0x660ffa10, \"psubd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubd, 0x660ffa10, \"vpsubd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[96][10]},\n    {INVALID,    0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubd, 0x660ffa10, \"vpsubd\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 97 */\n  {\n    {OP_psubq,   0x0ffb10, \"psubq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[97][2]},\n    {INVALID,  0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psubq, 0x660ffb10, \"psubq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,  0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,    0x0ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubq, 0x660ffb10, \"vpsubq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[97][10]},\n    {INVALID,  0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubq, 0x660ffb50, \"vpsubq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 98 */\n  {\n    {OP_paddb,   0x0ffc10, \"paddb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[98][2]},\n    {INVALID,    0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddb, 0x660ffc10, \"paddb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddb, 0x660ffc10, \"vpaddb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[98][10]},\n    {INVALID,    0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddb, 0x660ffc10, \"vpaddb\", Ve, xx, KEq, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 99 */\n  {\n    {OP_paddw,   0x0ffd10, \"paddw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[99][2]},\n    {INVALID,    0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddw, 0x660ffd10, \"paddw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddw, 0x660ffd10, \"vpaddw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[99][10]},\n    {INVALID,    0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddw, 0x660ffd10, \"vpaddw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 100 */\n  {\n    {OP_paddd,   0x0ffe10, \"paddd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[100][2]},\n    {INVALID,    0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddd, 0x660ffe10, \"paddd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddd, 0x660ffe10, \"vpaddd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[100][10]},\n    {INVALID,    0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddd, 0x660ffe10, \"vpaddd\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 101: all assumed to have Ib */\n  {\n    {INVALID,     0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrldq, 0x660f7333, \"psrldq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrldq, 0x660f7333, \"vpsrldq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 102: all assumed to have Ib */\n  {\n    {INVALID,     0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pslldq, 0x660f7337, \"pslldq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpslldq, 0x660f7337, \"vpslldq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 103 */\n  {\n    {REX_B_EXT,  0x900000, \"(rex.b ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {OP_pause,0xf3900000, \"pause\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* we chain these even though encoding won't find them */\n    {OP_nop, 0x66900000, \"nop\", xx, xx, xx, xx, xx, no, x, tpe[103][3]},\n    /* windbg displays as \"repne nop\" */\n    {OP_nop, 0xf2900000, \"nop\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf3900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x66900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf2900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 104: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrlw,    0x0f7132, \"psrlw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[104][2]},\n    {INVALID,   0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrlw,  0x660f7132, \"psrlw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrlw,  0x660f7132, \"vpsrlw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 105: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psraw,    0x0f7134, \"psraw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[105][2]},\n    {INVALID,   0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psraw,  0x660f7134, \"psraw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsraw,  0x660f7134, \"vpsraw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 106: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psllw,    0x0f7136, \"psllw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[106][2]},\n    {INVALID,   0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psllw,  0x660f7136, \"psllw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsllw,  0x660f7136, \"vpsllw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 107: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrld,    0x0f7232, \"psrld\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[107][2]},\n    {INVALID,   0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrld,  0x660f7232, \"psrld\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrld,  0x660f7232, \"vpsrld\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 108: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrad,    0x0f7234, \"psrad\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[108][2]},\n    {INVALID,   0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrad,  0x660f7234, \"psrad\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrad,  0x660f7234, \"vpsrad\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 109: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_pslld,    0x0f7236, \"pslld\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[109][2]},\n    {INVALID,   0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pslld,  0x660f7236, \"pslld\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpslld,  0x660f7236, \"vpslld\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 110: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrlq,    0x0f7332, \"psrlq\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[110][2]},\n    {INVALID,   0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrlq,  0x660f7332, \"psrlq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrlq,  0x660f7332, \"vpsrlq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 111: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psllq,    0x0f7336, \"psllq\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[111][2]},\n    {INVALID,   0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psllq,  0x660f7336, \"psllq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsllq,  0x660f7336, \"vpsllq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 112 */\n  {\n    {OP_movq,     0x0f6f10, \"movq\", Pq, xx, Qq, xx, xx, mrm, x, tpe[113][0]},\n    {OP_movdqu, 0xf30f6f10, \"movdqu\", Vdq, xx, Wdq, xx, xx, mrm, x, tpe[113][1]},\n    {OP_movdqa, 0x660f6f10, \"movdqa\", Vdq, xx, Wdq, xx, xx, mrm, x, tpe[113][2]},\n    {INVALID,   0xf20f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovdqu, 0xf30f6f10, \"vmovdqu\", Vx, xx, Wx, xx, xx, mrm|vex, x, tpe[113][5]},\n    {OP_vmovdqa, 0x660f6f10, \"vmovdqa\", Vx, xx, Wx, xx, xx, mrm|vex, x, tpe[113][6]},\n    {INVALID,   0xf20f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf30f6f10, \"(evex_W ext 11)\", xx, xx, xx, xx, xx, mrm|evex, x, 11},\n    {EVEX_W_EXT, 0x660f6f10, \"(evex_W ext 8)\", xx, xx, xx, xx, xx, mrm|evex, x, 8},\n    {EVEX_W_EXT, 0xf20f6f10, \"(evex_W ext 10)\", xx, xx, xx, xx, xx, mrm|evex, x, 10},\n  },\n  /* prefix extension 113 */\n  {\n    {OP_movq,     0x0f7f10, \"movq\", Qq, xx, Pq, xx, xx, mrm, x, tpe[51][1]},\n    {OP_movdqu, 0xf30f7f10, \"movdqu\", Wdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {OP_movdqa, 0x660f7f10, \"movdqa\", Wdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovdqu, 0xf30f7f10, \"vmovdqu\", Wx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vmovdqa, 0x660f7f10, \"vmovdqa\", Wx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0xf30f7f10, \"(evex_W ext 13)\", xx, xx, xx, xx, xx, mrm|evex, x, 13},\n    {EVEX_W_EXT, 0x660f7f10, \"(evex_W ext 9)\", xx, xx, xx, xx, xx, mrm|evex, x, 9},\n    {EVEX_W_EXT, 0xf20f7f10, \"(evex_W ext 12)\", xx, xx, xx, xx, xx, mrm|evex, x, 12},\n  },\n  /* prefix extension 114 */\n  {\n    {INVALID,     0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_haddpd, 0x660f7c10, \"haddpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_haddps, 0xf20f7c10, \"haddps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vhaddpd, 0x660f7c10, \"vhaddpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vhaddps, 0xf20f7c10, \"vhaddps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 115 */\n  {\n    {INVALID,     0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_hsubpd, 0x660f7d10, \"hsubpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_hsubps, 0xf20f7d10, \"hsubps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vhsubpd, 0x660f7d10, \"vhsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vhsubps, 0xf20f7d10, \"vhsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 116 */\n  {\n    {INVALID,     0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_addsubpd, 0x660fd010, \"addsubpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_addsubps, 0xf20fd010, \"addsubps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vaddsubpd, 0x660fd010, \"vaddsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vaddsubps, 0xf20fd010, \"vaddsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 117 */\n  {\n    {INVALID,     0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_lddqu,  0xf20ff010, \"lddqu\", Vdq, xx, Mdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,     0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vlddqu,  0xf20ff010, \"vlddqu\", Vx, xx, Mx, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /***************************************************\n   * SSSE3\n   */\n  { /* prefix extension 118 */\n    {OP_pshufb,     0x380018, \"pshufb\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[118][2]},\n    {INVALID,     0xf3380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pshufb,   0x66380018, \"pshufb\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf2380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf3380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpshufb,   0x66380018, \"vpshufb\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf2380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 119 */\n    {OP_phaddw,      0x380118, \"phaddw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[119][2]},\n    {INVALID,      0xf3380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddw,    0x66380118, \"phaddw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddw,    0x66380118, \"vphaddw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 120 */\n    {OP_phaddd,      0x380218, \"phaddd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[120][2]},\n    {INVALID,      0xf3380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddd,    0x66380218, \"phaddd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddd,    0x66380218, \"vphaddd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 121 */\n    {OP_phaddsw,     0x380318, \"phaddsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[121][2]},\n    {INVALID,      0xf3380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddsw,   0x66380318, \"phaddsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddsw,   0x66380318, \"vphaddsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 122 */\n    {OP_pmaddubsw,   0x380418, \"pmaddubsw\",Pq, xx, Qq, Pq, xx, mrm, x, tpe[122][2]},\n    {INVALID,      0xf3380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmaddubsw, 0x66380418, \"pmaddubsw\",Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmaddubsw, 0x66380418, \"vpmaddubsw\",Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[122][10]},\n    {INVALID,      0xf2380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmaddubsw, 0x66380418, \"vpmaddubsw\",Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf2380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 123 */\n    {OP_phsubw,      0x380518, \"phsubw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[123][2]},\n    {INVALID,      0xf3380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubw,    0x66380518, \"phsubw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubw,    0x66380518, \"vphsubw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 124 */\n    {OP_phsubd,      0x380618, \"phsubd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[124][2]},\n    {INVALID,      0xf3380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubd,    0x66380618, \"phsubd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubd,    0x66380618, \"vphsubd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 125 */\n    {OP_phsubsw,     0x380718, \"phsubsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[125][2]},\n    {INVALID,      0xf3380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubsw,   0x66380718, \"phsubsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubsw,   0x66380718, \"vphsubsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 126 */\n    {OP_psignb,      0x380818, \"psignb\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[126][2]},\n    {INVALID,      0xf3380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignb,    0x66380818, \"psignb\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignb,    0x66380818, \"vpsignb\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 127 */\n    {OP_psignw,      0x380918, \"psignw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[127][2]},\n    {INVALID,      0xf3380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignw,    0x66380918, \"psignw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignw,    0x66380918, \"vpsignw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 128 */\n    {OP_psignd,      0x380a18, \"psignd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[128][2]},\n    {INVALID,      0xf3380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignd,    0x66380a18, \"psignd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignd,    0x66380a18, \"vpsignd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 129 */\n    {OP_pmulhrsw,    0x380b18, \"pmulhrsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[129][2]},\n    {INVALID,      0xf3380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmulhrsw,  0x66380b18, \"pmulhrsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmulhrsw,  0x66380b18, \"vpmulhrsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[129][10]},\n    {INVALID,      0xf2380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmulhrsw,  0x66380b18, \"vpmulhrsw\", Ve, xx, KEd, He, We, mrm|evex, x, END_LIST},\n    {INVALID, 0xf2380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 130 */\n    {OP_pabsb,       0x381c18, \"pabsb\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[130][2]},\n    {INVALID,      0xf3381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsb,     0x66381c18, \"pabsb\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsb,     0x66381c18, \"vpabsb\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 131 */\n    {OP_pabsw,       0x381d18, \"pabsw\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[131][2]},\n    {INVALID,      0xf3381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsw,     0x66381d18, \"pabsw\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsw,     0x66381d18, \"vpabsw\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 132 */\n    {OP_pabsd,       0x381e18, \"pabsd\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[132][2]},\n    {INVALID,      0xf3381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsd,     0x66381e18, \"pabsd\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsd,     0x66381e18, \"vpabsd\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 133: all assumed to have Ib */\n    {OP_palignr,     0x3a0f18, \"palignr\", Pq, xx, Qq, Ib, Pq, mrm, x, tpe[133][2]},\n    {INVALID,      0xf33a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_palignr,   0x663a0f18, \"palignr\", Vdq, xx, Wdq, Ib, Vdq, mrm, x, END_LIST},\n    {INVALID,      0xf23a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x3a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf33a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpalignr,   0x663a0f18, \"vpalignr\", Vx, xx, Hx, Wx, Ib, mrm|vex, x, END_LIST},\n    {INVALID,      0xf23a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x3a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf33a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x663a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf23a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 134 */\n    {OP_vmread,      0x0f7810, \"vmread\",  Ed_q, xx, Gd_q, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf30f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* FIXME PR 338279: this is listed as /0 but I'm not going to chain it into\n     * the reg extensions table until I can verify, since gdb thinks it\n     * does NOT need /0.  Waiting for a processor that actually supports it.\n     * It's ok for DR proper to think a non-cti instr is valid when really it's not,\n     * though for our decoding library use we should get it right.\n     */\n    {OP_extrq,     0x660f7810, \"extrq\",   Udq, xx, Ib, Ib, xx, mrm, x, tpe[135][2]},\n    /* FIXME: is src or dst Udq? */\n    {OP_insertq,   0xf20f7810, \"insertq\", Vdq, xx, Udq, Ib, Ib, mrm, x, tpe[135][3]},\n    {INVALID,        0x0f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 135 */\n    {OP_vmwrite,     0x0f7910, \"vmwrite\", Gd_q, xx, Ed_q, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf30f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* FIXME: is src or dst Udq? */\n    {OP_extrq,     0x660f7910, \"extrq\",   Vdq, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {OP_insertq,   0xf20f7910, \"insertq\", Vdq, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {INVALID,        0x0f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 136 */\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, END_LIST},\n    /* XXX: if cpuid doesn't show lzcnt support, this is treated as bsr */\n    {OP_lzcnt,     0xf30fbd10, \"lzcnt\",   Gv, xx, Ev, xx, xx, mrm, fW6, END_LIST},\n    /* This is bsr w/ DATA_PREFIX, which we indicate by omitting 0x66 (i#1118).\n     * It's not in the encoding chain.  Ditto for 0xf2.  If we keep the \"all\n     * prefix ext marked invalid are really treated valid\" we don't need these,\n     * but better to be explicit where we have to so we can easily remove that.\n     */\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {INVALID,        0x0fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 137 */\n    {OP_vmptrld,     0x0fc736, \"vmptrld\", xx, xx, Mq, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmxon,     0xf30fc736, \"vmxon\",   xx, xx, Mq, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmclear,   0x660fc736, \"vmclear\", Mq, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf20fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x0fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 138 */\n    {OP_movbe,   0x38f018, \"movbe\", Gv, xx, Mv, xx, xx, mrm, x, tpe[139][0]},\n    {INVALID,  0xf338f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* really this is regular data-size prefix */\n    {OP_movbe, 0x6638f018, \"movbe\", Gw, xx, Mw, xx, xx, mrm, x, tpe[139][2]},\n    {OP_crc32, 0xf238f018, \"crc32\", Gv, xx, Eb, Gv, xx, mrm, x, END_LIST},\n    {INVALID,    0x38f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf338f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0x6638f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf238f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 139 */\n    {OP_movbe,   0x38f118, \"movbe\", Mv, xx, Gv, xx, xx, mrm, x, tpe[138][2]},\n    {INVALID,  0xf338f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* really this is regular data-size prefix */\n    {OP_movbe, 0x6638f118, \"movbe\", Mw, xx, Gw, xx, xx, mrm, x, END_LIST},\n    {OP_crc32, 0xf238f118, \"crc32\", Gv, xx, Ev, Gv, xx, mrm, x, tpe[138][3]},\n    {INVALID,    0x38f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf338f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0x6638f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf238f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* XXX: Intel Vol2B Sep2010 decode table claims crc32 has Gd\n     * instead of Gv, and that f2 f1 has Ey instead of Ev, and that\n     * there is a separate instruction with both 66 and f2 prefixes!\n     * But detail page doesn't corroborate that...\n     */\n  },\n  { /* prefix extension 140 */\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, END_LIST},\n    /* XXX: if cpuid doesn't show tzcnt support, this is treated as bsf */\n    {OP_tzcnt,     0xf30fbc10, \"tzcnt\",   Gv, xx, Ev, xx, xx, mrm, fW6, END_LIST},\n    /* see OP_bsr comments above -- this is the same but for bsf: */\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {INVALID,        0x0fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 141 */\n    {INVALID,        0x38f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf238f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bextr,       0x38f718, \"bextr\",   Gy, xx, Ey, By, xx, mrm|vex, fW6, txop[60]},\n    {OP_sarx,      0xf338f718, \"sarx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_shlx,      0x6638f718, \"shlx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_shrx,      0xf238f718, \"shrx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 142 */\n    {INVALID,        0x38f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf238f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bzhi,        0x38f518, \"bzhi\",    Gy, xx, Ey, By, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,      0xf338f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pext,      0x6638f518, \"pext\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_pdep,      0xf238f518, \"pdep\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 143 */\n    {INVALID,        0x38f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_adox,      0xf338f618, \"adox\",    Gy, xx, Ey, Gy, xx, mrm, (fWO|fRO), END_LIST},\n    {OP_adcx,      0x6638f618, \"adcx\",    Gy, xx, Ey, Gy, xx, mrm, (fWC|fRC), END_LIST},\n    {INVALID,      0xf238f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x38f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_mulx,      0xf238f618, \"mulx\",    By, Gy, Ey, uDX, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 144 */\n    {INVALID,        0x0f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9010, \"(vex_W ext 74)\", xx, xx, xx, xx, xx, mrm|vex, x, 74},\n    {INVALID,      0xf30f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9010, \"(vex_W ext 75)\", xx, xx, xx, xx, xx, mrm|vex, x, 75},\n    {INVALID,      0xf20f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 145 */\n    {INVALID,        0x0f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9110, \"(vex_W ext 76)\", xx, xx, xx, xx, xx, mrm|vex, x, 76},\n    {INVALID,      0xf30f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9110, \"(vex_W ext 77)\", xx, xx, xx, xx, xx, mrm|vex, x, 77},\n    {INVALID,      0xf20f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 146 */\n    {INVALID,        0x0f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9210, \"(vex_W ext 78)\", xx, xx, xx, xx, xx, mrm|vex, x, 78},\n    {INVALID,      0xf30f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9210, \"(vex_W ext 79)\", xx, xx, xx, xx, xx, mrm|vex, x, 79},\n    {VEX_W_EXT,    0xf20f9210, \"(vex_W ext 106)\",xx, xx, xx, xx, xx, mrm|vex, x, 106},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 147 */\n    {INVALID,        0x0f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9310, \"(vex_W ext 80)\", xx, xx, xx, xx, xx, mrm|vex, x, 80},\n    {INVALID,      0xf30f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9310, \"(vex_W ext 81)\", xx, xx, xx, xx, xx, mrm|vex, x, 81},\n    {VEX_W_EXT,    0xf20f9310, \"(vex_W ext 107)\",xx, xx, xx, xx, xx, mrm|vex, x, 107},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 148 */\n    {INVALID,        0x0f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4110, \"(vex_W ext 82)\", xx, xx, xx, xx, xx, mrm|vex, x, 82},\n    {INVALID,      0xf30f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4110, \"(vex_W ext 83)\", xx, xx, xx, xx, xx, mrm|vex, x, 83},\n    {INVALID,      0xf20f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 149 */\n    {INVALID,        0x0f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4210, \"(vex_W ext 84)\", xx, xx, xx, xx, xx, mrm|vex, x, 84},\n    {INVALID,      0xf30f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4210, \"(vex_W ext 85)\", xx, xx, xx, xx, xx, mrm|vex, x, 85},\n    {INVALID,      0xf20f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 150 */\n    {INVALID,        0x0f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4b10, \"(vex_W ext 86)\", xx, xx, xx, xx, xx, mrm|vex, x, 86},\n    {INVALID,      0xf30f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4b10, \"(vex_W ext 87)\", xx, xx, xx, xx, xx, mrm|vex, x, 87},\n    {INVALID,      0xf20f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 151 */\n    {INVALID,        0x0f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4410, \"(vex_W ext 88)\", xx, xx, xx, xx, xx, mrm|vex, x, 88},\n    {INVALID,      0xf30f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4410, \"(vex_W ext 89)\", xx, xx, xx, xx, xx, mrm|vex, x, 89},\n    {INVALID,      0xf20f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 152 */\n    {INVALID,        0x0f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4510, \"(vex_W ext 90)\", xx, xx, xx, xx, xx, mrm|vex, x, 90},\n    {INVALID,      0xf30f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4510, \"(vex_W ext 91)\", xx, xx, xx, xx, xx, mrm|vex, x, 91},\n    {INVALID,      0xf20f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 153 */\n    {INVALID,        0x0f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4610, \"(vex_W ext 92)\", xx, xx, xx, xx, xx, mrm|vex, x, 92},\n    {INVALID,      0xf30f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4610, \"(vex_W ext 93)\", xx, xx, xx, xx, xx, mrm|vex, x, 93},\n    {INVALID,      0xf20f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 154 */\n    {INVALID,        0x0f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4710, \"(vex_W ext 94)\", xx, xx, xx, xx, xx, mrm|vex, x, 94},\n    {INVALID,      0xf30f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4710, \"(vex_W ext 95)\", xx, xx, xx, xx, xx, mrm|vex, x, 95},\n    {INVALID,      0xf20f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 155 */\n    {INVALID,        0x0f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4a10, \"(vex_W ext 96)\", xx, xx, xx, xx, xx, mrm|vex, x, 96},\n    {INVALID,      0xf30f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4a10, \"(vex_W ext 97)\", xx, xx, xx, xx, xx, mrm|vex, x, 97},\n    {INVALID,      0xf20f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 156 */\n    {INVALID,        0x0f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9810, \"(vex_W ext 98)\", xx, xx, xx, xx, xx, mrm|vex, x, 98},\n    {INVALID,      0xf30f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9810, \"(vex_W ext 99)\", xx, xx, xx, xx, xx, mrm|vex, x, 99},\n    {INVALID,      0xf20f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 157 */\n    {INVALID,        0x0f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9910, \"(vex_W ext 104)\", xx, xx, xx, xx, xx, mrm|vex, x, 104},\n    {INVALID,      0xf30f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9910, \"(vex_W ext 105)\", xx, xx, xx, xx, xx, mrm|vex, x, 105},\n    {INVALID,      0xf20f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n/****************************************************************************\n * Instructions that differ based on whether vex-encoded or not.\n * Most of these require an 0x66 prefix but we use reqp for that\n * so there's nothing inherent here about prefixes.\n * TODO i#1312: A third row has been added for AVX-512 w/ EVEX prefix. Most or all\n * EVEX instructions seem to resemble their corresponding VEX version. If we add\n * a decode_table entry here, we currently can't test them throgh instr_create macros,\n * unless we force the creation of EVEX versions.\n */\nconst instr_info_t e_vex_extensions[][3] = {\n  {    /* e_vex ext  0 */\n    {INVALID, 0x663a4a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vblendvps, 0x663a4a18, \"vblendvps\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  1 */\n    {INVALID, 0x663a4b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vblendvpd, 0x663a4b18, \"vblendvpd\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  2 */\n    {INVALID, 0x663a4c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpblendvb, 0x663a4c18, \"vpblendvb\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  3 */\n    {OP_ptest,  0x66381718, \"ptest\",    xx, xx,  Vdq,Wdq, xx, mrm|reqp, fW6, END_LIST},\n    {OP_vptest, 0x66381718, \"vptest\",   xx, xx,    Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66381718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  4 */\n    {OP_pmovsxbw,  0x66382018, \"pmovsxbw\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbw, 0x66382018, \"vpmovsxbw\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  5 */\n    {OP_pmovsxbd,  0x66382118, \"pmovsxbd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbd, 0x66382118, \"vpmovsxbd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  6 */\n    {OP_pmovsxbq,  0x66382218, \"pmovsxbq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbq, 0x66382218, \"vpmovsxbq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  7 */\n    {OP_pmovsxwd,  0x66382318, \"pmovsxwd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxwd, 0x66382318, \"vpmovsxwd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  8 */\n    {OP_pmovsxwq,  0x66382418, \"pmovsxwq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxwq, 0x66382418, \"vpmovsxwq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  9 */\n    {OP_pmovsxdq, 0x66382518, \"pmovsxdq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxdq,0x66382518, \"vpmovsxdq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 10 */\n    {OP_pmuldq,   0x66382818, \"pmuldq\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmuldq,  0x66382818, \"vpmuldq\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, tvex[10][2]},\n    {OP_vpmuldq,  0x66382858, \"vpmuldq\",   Ve, xx, KEb,He,We, mrm|evex|reqp, x, END_LIST},\n  }, { /* e_vex ext 11 */\n    {OP_pcmpeqq,  0x66382918, \"pcmpeqq\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpcmpeqq, 0x66382918, \"vpcmpeqq\",  Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 12 */\n    {OP_movntdqa,  0x66382a18, \"movntdqa\", Mdq, xx, Vdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vmovntdqa, 0x66382a18, \"vmovntdqa\", Mx, xx, Vx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 13 */\n    {OP_packusdw,  0x66382b18, \"packusdw\", Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpackusdw, 0x66382b18, \"vpackusdw\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 14 */\n    {OP_pmovzxbw,  0x66383018, \"pmovzxbw\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbw, 0x66383018, \"vpmovzxbw\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 15 */\n    {OP_pmovzxbd,  0x66383118, \"pmovzxbd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbd, 0x66383118, \"vpmovzxbd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 16 */\n    {OP_pmovzxbq,  0x66383218, \"pmovzxbq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbq, 0x66383218, \"vpmovzxbq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 17 */\n    {OP_pmovzxwd,  0x66383318, \"pmovzxwd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxwd, 0x66383318, \"vpmovzxwd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 18 */\n    {OP_pmovzxwq,  0x66383418, \"pmovzxwq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxwq, 0x66383418, \"vpmovzxwq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 19 */\n    {OP_pmovzxdq,  0x66383518, \"pmovzxdq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxdq, 0x66383518, \"vpmovzxdq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 20 */\n    {OP_pcmpgtq,  0x66383718, \"pcmpgtq\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpcmpgtq, 0x66383718, \"vpcmpgtq\",  Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 21 */\n    {OP_pminsb,   0x66383818, \"pminsb\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminsb,  0x66383818, \"vpminsb\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 22 */\n    {OP_pminsd,   0x66383918, \"pminsd\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminsd,  0x66383918, \"vpminsd\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 23 */\n    {OP_pminuw,   0x66383a18, \"pminuw\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminuw,  0x66383a18, \"vpminuw\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 24 */\n    {OP_pminud,   0x66383b18, \"pminud\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminud,  0x66383b18, \"vpminud\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 25 */\n    {OP_pmaxsb,   0x66383c18, \"pmaxsb\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxsb,  0x66383c18, \"vpmaxsb\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 26 */\n    {OP_pmaxsd,   0x66383d18, \"pmaxsd\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxsd,  0x66383d18, \"vpmaxsd\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 27 */\n    {OP_pmaxuw,   0x66383e18, \"pmaxuw\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxuw,  0x66383e18, \"vpmaxuw\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 28 */\n    {OP_pmaxud,   0x66383f18, \"pmaxud\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxud,  0x66383f18, \"vpmaxud\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 29 */\n    {OP_pmulld,   0x66384018, \"pmulld\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmulld,  0x66384018, \"vpmulld\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, tevexw[45][0]},\n    {EVEX_W_EXT, 0x66384018, \"(evex_W ext 45)\", xx, xx, xx, xx, xx, mrm|evex, x, 45},\n  }, { /* e_vex ext 30 */\n    {OP_phminposuw,  0x66384118,\"phminposuw\",Vdq,xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vphminposuw, 0x66384118,\"vphminposuw\",Vdq,xx, Wdq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66384118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 31 */\n    {OP_aesimc,  0x6638db18, \"aesimc\",  Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesimc, 0x6638db18, \"vaesimc\",  Vdq, xx, Wdq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638db18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 32 */\n    {OP_aesenc,  0x6638dc18, \"aesenc\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesenc, 0x6638dc18, \"vaesenc\",  Vdq, xx, Hdq,Wdq, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638dc18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 33 */\n    {OP_aesenclast,  0x6638dd18,\"aesenclast\",Vdq,xx,Wdq,Vdq,xx, mrm|reqp, x, END_LIST},\n    {OP_vaesenclast, 0x6638dd18,\"vaesenclast\",Vdq,xx,Hdq,Wdq,xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638dd18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 34 */\n    {OP_aesdec,  0x6638de18, \"aesdec\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesdec, 0x6638de18, \"vaesdec\",  Vdq, xx, Hdq,Wdq, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638de18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 35 */\n    {OP_aesdeclast,  0x6638df18,\"aesdeclast\",Vdq,xx,Wdq,Vdq,xx, mrm|reqp, x, END_LIST},\n    {OP_vaesdeclast, 0x6638df18,\"vaesdeclast\",Vdq,xx,Hdq,Wdq,xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638df18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 36 */\n    {OP_pextrb,   0x663a1418, \"pextrb\", Rd_Mb, xx, Vb_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpextrb,  0x663a1418, \"vpextrb\", Rd_Mb, xx, Vb_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 37 */\n    {OP_pextrw,   0x663a1518, \"pextrw\", Rd_Mw, xx, Vw_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpextrw,  0x663a1518, \"vpextrw\", Rd_Mw, xx, Vw_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 38 */\n    {OP_pextrd,   0x663a1618, \"pextrd\",  Ed_q, xx, Vd_q_dq, Ib, xx, mrm|reqp, x, END_LIST},/*\"pextrq\" with rex.w*/\n    {OP_vpextrd,  0x663a1618, \"vpextrd\",  Ed_q, xx, Vd_q_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},/*\"vpextrq\" with rex.w*/\n    {INVALID, 0x663a1618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 39 */\n    {OP_extractps,  0x663a1718, \"extractps\", Ed, xx, Vd_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vextractps, 0x663a1718, \"vextractps\", Ed, xx, Vd_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 40 */\n    {OP_roundps,  0x663a0818, \"roundps\",  Vdq, xx, Wdq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundps, 0x663a0818, \"vroundps\",  Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 41 */\n    {OP_roundpd,  0x663a0918, \"roundpd\",  Vdq, xx, Wdq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundpd, 0x663a0918, \"vroundpd\",  Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 42 */\n    {OP_roundss,  0x663a0a18, \"roundss\",  Vss, xx, Wss, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundss, 0x663a0a18, \"vroundss\",  Vdq, xx, H12_dq, Wss, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 43 */\n    {OP_roundsd,  0x663a0b18, \"roundsd\",  Vsd, xx, Wsd, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundsd, 0x663a0b18, \"vroundsd\",  Vdq, xx, Hsd, Wsd, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 44 */\n    {OP_blendps,  0x663a0c18, \"blendps\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vblendps, 0x663a0c18, \"vblendps\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 45 */\n    {OP_blendpd,  0x663a0d18, \"blendpd\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vblendpd, 0x663a0d18, \"vblendpd\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 46 */\n    {OP_pblendw,  0x663a0e18, \"pblendw\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vpblendw, 0x663a0e18, \"vpblendw\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 47 */\n    /* FIXME i#1388: pinsrb actually reads only bottom byte of reg */\n    {OP_pinsrb,   0x663a2018, \"pinsrb\",   Vb_dq, xx, Rd_Mb,  Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpinsrb,  0x663a2018, \"vpinsrb\",   Vdq, xx, H15_dq, Rd_Mb, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a2018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 48 */\n    {OP_insertps, 0x663a2118, \"insertps\", Vdq,xx,Udq_Md,Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vinsertps,0x663a2118, \"vinsertps\", Vdq,xx,Hdq,Udq_Md,Ib, mrm|vex|reqp|reqL0, x, END_LIST},\n    {INVALID, 0x663a2118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 49 */\n    {OP_pinsrd,   0x663a2218, \"pinsrd\",   Vd_q_dq, xx, Ed_q,Ib, xx, mrm|reqp, x, END_LIST},/*\"pinsrq\" with rex.w*/\n    {OP_vpinsrd,  0x663a2218, \"vpinsrd\",   Vdq, xx, H12_8_dq, Ed_q, Ib, mrm|vex|reqp, x, END_LIST},/*\"vpinsrq\" with rex.w*/\n    {INVALID, 0x663a2218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 50 */\n    {OP_dpps,     0x663a4018, \"dpps\",     Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vdpps,    0x663a4018, \"vdpps\",     Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 51 */\n    {OP_dppd,     0x663a4118, \"dppd\",     Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vdppd,    0x663a4118, \"vdppd\",     Vdq, xx, Hdq, Wdq, Ib, mrm|vex|reqp|reqL0, x, END_LIST},\n    {INVALID, 0x663a4118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 52 */\n    {OP_mpsadbw,  0x663a4218, \"mpsadbw\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vmpsadbw, 0x663a4218, \"vmpsadbw\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 53 */\n    {OP_pcmpestrm, 0x663a6018, \"pcmpestrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|reqp|xop, fW6, exop[8]},\n    {OP_vpcmpestrm,0x663a6018, \"vpcmpestrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|vex|reqp|xop, fW6, exop[11]},\n    {INVALID, 0x663a6018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 54 */\n    {OP_pcmpestri, 0x663a6118, \"pcmpestri\",ecx, xx, Vdq, Wdq, Ib, mrm|reqp|xop, fW6, exop[9]},\n    {OP_vpcmpestri,0x663a6118, \"vpcmpestri\",ecx, xx, Vdq, Wdq, Ib, mrm|vex|reqp|xop, fW6, exop[12]},\n    {INVALID, 0x663a6118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 55 */\n    {OP_pcmpistrm, 0x663a6218, \"pcmpistrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|reqp, fW6, END_LIST},\n    {OP_vpcmpistrm,0x663a6218, \"vpcmpistrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x663a6218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 56 */\n    {OP_pcmpistri, 0x663a6318, \"pcmpistri\",ecx, xx, Vdq, Wdq, Ib, mrm|reqp, fW6, END_LIST},\n    {OP_vpcmpistri,0x663a6318, \"vpcmpistri\",ecx, xx, Vdq, Wdq, Ib, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x663a6318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 57 */\n    {OP_pclmulqdq, 0x663a4418, \"pclmulqdq\", Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vpclmulqdq,0x663a4418, \"vpclmulqdq\", Vdq, xx, Hdq, Wdq, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 58 */\n    {OP_aeskeygenassist, 0x663adf18, \"aeskeygenassist\",Vdq,xx,Wdq,Ib,xx,mrm|reqp,x,END_LIST},\n    {OP_vaeskeygenassist,0x663adf18, \"vaeskeygenassist\",Vdq,xx,Wdq,Ib,xx,mrm|vex|reqp,x,END_LIST},\n    {INVALID, 0x663adf18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 59 */\n    {INVALID,   0x66380e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vtestps, 0x66380e18, \"vtestps\", xx, xx, Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66380e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 60 */\n    {INVALID,   0x66380f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vtestpd, 0x66380f18, \"vtestpd\", xx, xx, Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66380f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 61 */\n    {OP_ldmxcsr, 0x0fae32, \"ldmxcsr\", xx, xx, Md, xx, xx, mrm, x, END_LIST},\n    {OP_vldmxcsr, 0x0fae32, \"vldmxcsr\", xx, xx, Md, xx, xx, mrm|vex|reqL0, x, END_LIST},\n    {INVALID, 0x0fae32, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 62 */\n    {OP_stmxcsr, 0x0fae33, \"stmxcsr\", Md, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_vstmxcsr, 0x0fae33, \"vstmxcsr\", Md, xx, xx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x0fae33, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 63 */\n    {INVALID,   0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtph2ps, 0x66381318, \"vcvtph2ps\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 64 */\n    {INVALID,   0x66381818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastss, 0x66381818, \"vbroadcastss\", Vx, xx, Wd_dq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66381818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 65 */\n    {INVALID,   0x66381918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastsd, 0x66381918, \"vbroadcastsd\", Vqq, xx, Wq_dq, xx, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x66381918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 66 */\n    {INVALID,   0x66381a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastf128, 0x66381a18, \"vbroadcastf128\", Vqq, xx, Mdq, xx, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x66381a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 67 */\n    {INVALID,   0x66382c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovps, 0x66382c18, \"vmaskmovps\", Vx, xx, Hx,Mx, xx, mrm|vex|reqp|predcx, x, tvex[69][1]},\n    {INVALID, 0x66382c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 68 */\n    {INVALID,   0x66382d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovpd, 0x66382d18, \"vmaskmovpd\", Vx, xx, Hx,Mx, xx, mrm|vex|reqp|predcx, x, tvex[70][1]},\n    {INVALID, 0x66382d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 69 */\n    {INVALID,   0x66382e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovps, 0x66382e18, \"vmaskmovps\", Mx, xx, Hx,Vx, xx, mrm|vex|reqp|predcx, x, END_LIST},\n    {INVALID, 0x66382e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 70 */\n    {INVALID,   0x66382f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovpd, 0x66382f18, \"vmaskmovpd\", Mx, xx, Hx,Vx, xx, mrm|vex|reqp|predcx, x, END_LIST},\n    {INVALID, 0x66382f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 71 */\n    {INVALID,   0x663a0418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilps, 0x663a0418, \"vpermilps\", Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, tvex[77][1]},\n    {INVALID, 0x663a0418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 72 */\n    {INVALID,   0x663a0518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilpd, 0x663a0518, \"vpermilpd\", Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, tvex[78][1]},\n    {INVALID, 0x663a0518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 73 */\n    {INVALID,   0x663a0618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vperm2f128, 0x663a0618, \"vperm2f128\", Vx, xx, Hx,Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 74 */\n    {INVALID,   0x663a1818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vinsertf128, 0x663a1818, \"vinsertf128\", Vx, xx, Hx,Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 75 */\n    {INVALID,   0x663a1918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vextractf128, 0x663a1918, \"vextractf128\", Wdq, xx, Vdq_qq, Ib, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x663a1918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 76 */\n    {INVALID,   0x663a1d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtps2ph, 0x663a1d18, \"vcvtps2ph\", Wx, xx, Vx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 77 */\n    {INVALID,   0x66380c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilps, 0x66380c18, \"vpermilps\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66380c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 78 */\n    {INVALID,   0x66380d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilpd, 0x66380d18, \"vpermilpd\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66380d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 79 */\n    {OP_seto,    0x0f9010,             \"seto\", Eb, xx, xx, xx, xx, mrm, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f9010, \"(prefix ext 144)\", xx, xx, xx, xx, xx, mrm,   x, 144},\n    {INVALID, 0x0f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 80 */\n    {OP_setno,   0x0f9110,            \"setno\", Eb, xx, xx, xx, xx, mrm, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f9110, \"(prefix ext 145)\", xx, xx, xx, xx, xx, mrm,   x, 145},\n    {INVALID, 0x0f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 81 */\n    {OP_setb,    0x0f9210,             \"setb\", Eb, xx, xx, xx, xx, mrm, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f9210, \"(prefix ext 146)\", xx, xx, xx, xx, xx, mrm,   x, 146},\n    {INVALID, 0x0f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 82 */\n    {OP_setnb,   0x0f9310,            \"setnb\", Eb, xx, xx, xx, xx, mrm, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f9310, \"(prefix ext 147)\", xx, xx, xx, xx, xx, mrm,   x, 147},\n    {INVALID, 0x0f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 83 */\n    {OP_cmovno,  0x0f4110,           \"cmovno\", Gv, xx, Ev, xx, xx, mrm|predcc, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f4110, \"(prefix ext 148)\", xx, xx, xx, xx, xx, mrm,         x, 148},\n    {INVALID, 0x0f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 84 */\n    {OP_cmovb,   0x0f4210,            \"cmovb\", Gv, xx, Ev, xx, xx, mrm|predcc, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f4210, \"(prefix ext 149)\", xx, xx, xx, xx, xx, mrm,          x, 149},\n    {INVALID, 0x0f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 85 */\n    {OP_cmovnp,  0x0f4b10,           \"cmovnp\", Gv, xx, Ev, xx, xx, mrm|predcc, fRP, END_LIST},\n    {PREFIX_EXT, 0x0f4b10, \"(prefix ext 150)\", xx, xx, xx, xx, xx, mrm,          x, 150},\n    {INVALID, 0x0f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 86 */\n    {OP_cmovz,   0x0f4410,            \"cmovz\", Gv, xx, Ev, xx, xx, mrm|predcc, fRZ, END_LIST},\n    {PREFIX_EXT, 0x0f4410, \"(prefix ext 151)\", xx, xx, xx, xx, xx, mrm,          x, 151},\n    {INVALID, 0x0f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 87 */\n    {OP_cmovnz,  0x0f4510,           \"cmovnz\", Gv, xx, Ev, xx, xx, mrm|predcc, fRZ, END_LIST},\n    {PREFIX_EXT, 0x0f4510, \"(prefix ext 152)\", xx, xx, xx, xx, xx, mrm,          x, 152},\n    {INVALID, 0x0f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 88 */\n    {OP_cmovbe,  0x0f4610,           \"cmovbe\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRC|fRZ), END_LIST},\n    {PREFIX_EXT, 0x0f4610, \"(prefix ext 153)\", xx, xx, xx, xx, xx, mrm,                x, 153},\n    {INVALID, 0x0f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 89 */\n    {OP_cmovnbe, 0x0f4710,          \"cmovnbe\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRC|fRZ), END_LIST},\n    {PREFIX_EXT, 0x0f4710, \"(prefix ext 154)\", xx, xx, xx, xx, xx, mrm,                x, 154},\n    {INVALID, 0x0f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 90 */\n    {OP_cmovp,   0x0f4a10,            \"cmovp\", Gv, xx, Ev, xx, xx, mrm|predcc, fRP, END_LIST},\n    {PREFIX_EXT, 0x0f4a10, \"(prefix ext 155)\", xx, xx, xx, xx, xx, mrm,          x, 155},\n    {INVALID, 0x0f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 91 */\n    {OP_sets,    0x0f9810,             \"sets\", Eb, xx, xx, xx, xx, mrm, fRS, END_LIST},\n    {PREFIX_EXT, 0x0f9810, \"(prefix ext 156)\", xx, xx, xx, xx, xx, mrm,    x, 156},\n    {INVALID, 0x0f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 92 */\n    {OP_setns,   0x0f9910,            \"setns\", Eb, xx, xx, xx, xx, mrm, fRS, END_LIST},\n    {PREFIX_EXT, 0x0f9910, \"(prefix ext 157)\", xx, xx, xx, xx, xx, mrm,   x, 157},\n    {INVALID, 0x0f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on mod and rm bits in modrm byte\n * For mod, entry 0 is all mem ref mod values (0,1,2) while entry 1 is 3.\n * For the mem ref, we give just one of the 3 possible modrm bytes\n * (we only use it when encoding so we don't need all 3).\n */\nconst instr_info_t mod_extensions[][2] = {\n  { /* mod extension 0 */\n    {OP_sgdt, 0x0f0130, \"sgdt\", Ms, xx, xx, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,  0x0f0171, \"(group 7 mod + rm ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  },\n  { /* mod extension 1 */\n    {OP_sidt, 0x0f0131, \"sidt\",  Ms, xx, xx, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,  0x0f0171, \"(group 7 mod + rm ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n  },\n  { /* mod extension 2 */\n    {OP_invlpg, 0x0f0137, \"invlpg\", xx, xx, Mm, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0177, \"(group 7 mod + rm ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n  },\n  { /* mod extension 3 */\n    {OP_clflush, 0x0fae37, \"clflush\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_sfence,  0xf80fae77, \"sfence\",  xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 4 */\n    {OP_lidt,   0x0f0133, \"lidt\",  xx, xx, Ms, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0173, \"(group 7 mod + rm ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n  },\n  { /* mod extension 5 */\n    {OP_lgdt,   0x0f0132, \"lgdt\",  xx, xx, Ms, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0172, \"(group 7 mod + rm ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n  },\n  { /* mod extension 6 */\n    {REX_W_EXT, 0x0fae35, \"(rex.w ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n    /* note that gdb thinks e9-ef are \"lfence (bad)\" (PR 239920) */\n    {OP_lfence, 0xe80fae75, \"lfence\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 7 */\n    {REX_W_EXT,   0x0fae36, \"(rex.w ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n    {OP_mfence,   0xf00fae76, \"mfence\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 8 */\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vdq, xx, Wss,  xx, xx, mrm|vex, x, modx[10][0]},\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vdq, xx, H12_dq, Uss, xx, mrm|vex, x, modx[10][1]},\n  },\n  { /* mod extension 9 */\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vdq, xx, Wsd,  xx, xx, mrm|vex, x, modx[11][0]},\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vdq, xx, Hsd, Usd, xx, mrm|vex, x, modx[11][1]},\n  },\n  { /* mod extension 10 */\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Wss, xx, Vss,  xx, xx, mrm|vex, x, modx[ 8][1]},\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Udq, xx, H12_dq, Vss, xx, mrm|vex, x, modx[20][0]},\n  },\n  { /* mod extension 11 */\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Wsd, xx, Vsd,  xx, xx, mrm|vex, x, modx[ 9][1]},\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Udq, xx, Hsd, Vsd, xx, mrm|vex, x, modx[21][0]},\n  },\n  { /* mod extension 12 */\n    {PREFIX_EXT, 0x0fc736, \"(prefix ext 137)\", xx, xx, xx, xx, xx, no, x, 137},\n    {OP_rdrand,  0x0fc736, \"rdrand\", Rv, xx, xx, xx, xx, mrm, fW6, END_LIST},\n  },\n  { /* mod extension 13 */\n    /* The latest Intel table implies 0x66 prefix makes invalid instr but not worth\n     * explicitly encoding that until we have more information.\n     */\n    {OP_vmptrst, 0x0fc737, \"vmptrst\", Mq, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_rdseed,  0x0fc737, \"rdseed\", Rv, xx, xx, xx, xx, mrm, fW6, END_LIST},\n  },\n  { /* mod extension 14 */\n    {REX_W_EXT,  0x0fae30, \"(rex.w ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n    /* Using reqp to avoid having to create a whole prefix_ext entry for one opcode.\n     * Ditto below.\n     */\n    {OP_rdfsbase,0xf30fae30, \"rdfsbase\", Ry, xx, xx, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 15 */\n    {REX_W_EXT,  0x0fae31, \"(rex.w ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n    {OP_rdgsbase,0xf30fae31, \"rdgsbase\", Ry, xx, xx, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 16 */\n    {E_VEX_EXT,    0x0fae32, \"(e_vex ext 61)\", xx, xx, xx, xx, xx, mrm, x, 61},\n    {OP_wrfsbase,0xf30fae32, \"wrfsbase\", xx, xx, Ry, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 17 */\n    {E_VEX_EXT,    0x0fae33, \"(e_vex ext 62)\", xx, xx, xx, xx, xx, mrm, x, 62},\n    {OP_wrgsbase,0xf30fae33, \"wrgsbase\", xx, xx, Ry, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 18 */\n    /* load from memory zeroes top bits */\n    {OP_movss,  0xf30f1010, \"movss\",  Vdq, xx, Mss, xx, xx, mrm, x, modx[18][1]},\n    {OP_movss,  0xf30f1010, \"movss\",  Vss, xx, Uss, xx, xx, mrm, x, tpe[1][1]},\n  },\n  { /* mod extension 19 */\n    /* load from memory zeroes top bits */\n    {OP_movsd,  0xf20f1010, \"movsd\",  Vdq, xx, Msd, xx, xx, mrm, x, modx[19][1]},\n    {OP_movsd,  0xf20f1010, \"movsd\",  Vsd, xx, Usd, xx, xx, mrm, x, tpe[1][3]},\n  },\n  { /* mod extension 20 */\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vdq, xx, KEb, Wss,  xx, mrm|evex, x, modx[22][0]},\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vdq, xx, KEb, H12_dq, Uss, mrm|evex, x, modx[22][1]},\n  },\n  { /* mod extension 21 */\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vdq, xx, KEb, Wsd,  xx, mrm|evex, x, modx[23][0]},\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vdq, xx, KEb, Hsd, Usd, mrm|evex, x, modx[23][1]},\n  },\n  { /* mod extension 22 */\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Wss, xx, KEb, Vss,  xx, mrm|evex, x, modx[20][1]},\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Udq, xx, KEb, H12_dq, Vss, mrm|evex, x, END_LIST},\n  },\n  { /* mod extension 23 */\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Wsd, xx, KEb, Vsd,  xx, mrm|evex, x, modx[21][1]},\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Udq, xx, KEb, Hsd, Vsd, mrm|evex, x, END_LIST},\n  },\n};\n\n/* Naturally all of these have modrm bytes even if they have no explicit operands */\nconst instr_info_t rm_extensions[][8] = {\n  { /* rm extension 0 */\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmcall,   0xc10f0171, \"vmcall\",   xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmlaunch, 0xc20f0171, \"vmlaunch\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmresume, 0xc30f0171, \"vmresume\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmxoff,   0xc40f0171, \"vmxoff\",   xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 1 */\n    {OP_monitor, 0xc80f0171, \"monitor\",  xx, xx, eax, ecx, edx, mrm, x, END_LIST},\n    {OP_mwait,   0xc90f0171, \"mwait\",  xx, xx, eax, ecx, xx, mrm, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 2 */\n    {OP_swapgs, 0xf80f0177, \"swapgs\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_rdtscp, 0xf90f0177, \"rdtscp\", edx, eax, xx, xx, xx, mrm|xop, x, exop[10]},/*AMD-only*/\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 3 */\n    {OP_vmrun,  0xd80f0173, \"vmrun\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_vmmcall,0xd90f0173, \"vmmcall\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_vmload, 0xda0f0173, \"vmload\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_vmsave, 0xdb0f0173, \"vmsave\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_stgi,   0xdc0f0173, \"stgi\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_clgi,   0xdd0f0173, \"clgi\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_skinit, 0xde0f0173, \"skinit\", xx, xx, eax, xx, xx, mrm, x, END_LIST},\n    {OP_invlpga,0xdf0f0173, \"invlpga\", xx, xx, axAX, ecx, xx, mrm, x, END_LIST},\n  },\n  { /* rm extension 4 */\n    {OP_xgetbv, 0xd00f0172, \"xgetbv\", edx, eax, ecx, xx, xx, mrm, x, END_LIST},\n    {OP_xsetbv, 0xd10f0172, \"xsetbv\", xx, xx, ecx, edx, eax, mrm, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmfunc, 0xd40f0172, \"vmfunc\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    /* Only if the transaction fails does xend write to eax => predcx.\n     * XXX i#1314: on failure eip is also written to.\n     */\n    {OP_xend,   0xd50f0172, \"xend\", eax, xx, xx, xx, xx, mrm|predcx, x, NA},\n    {OP_xtest,  0xd60f0172, \"xtest\", xx, xx, xx, xx, xx, mrm, fW6, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether in 64-bit mode\n */\n\nconst instr_info_t x64_extensions[][2] = {\n  {    /* x64_ext 0 */\n    {OP_inc,  0x400000, \"inc\", zAX, xx, zAX, xx, xx, i64, (fW6&(~fWC)), t64e[1][0]},\n    {PREFIX,  0x400000, \"rex\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_GENERAL},\n  }, { /* x64_ext 1 */\n    {OP_inc,  0x410000, \"inc\", zCX, xx, zCX, xx, xx, i64, (fW6&(~fWC)), t64e[2][0]},\n    {PREFIX,  0x410000, \"rex.b\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_B},\n  }, { /* x64_ext 2 */\n    {OP_inc,  0x420000, \"inc\", zDX, xx, zDX, xx, xx, i64, (fW6&(~fWC)), t64e[3][0]},\n    {PREFIX,  0x420000, \"rex.x\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_X},\n  }, { /* x64_ext 3 */\n    {OP_inc,  0x430000, \"inc\", zBX, xx, zBX, xx, xx, i64, (fW6&(~fWC)), t64e[4][0]},\n    {PREFIX,  0x430000, \"rex.xb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 4 */\n    {OP_inc,  0x440000, \"inc\", zSP, xx, zSP, xx, xx, i64, (fW6&(~fWC)), t64e[5][0]},\n    {PREFIX,  0x440000, \"rex.r\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R},\n  }, { /* x64_ext 5 */\n    {OP_inc,  0x450000, \"inc\", zBP, xx, zBP, xx, xx, i64, (fW6&(~fWC)), t64e[6][0]},\n    {PREFIX,  0x450000, \"rex.rb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_B},\n  }, { /* x64_ext 6 */\n    {OP_inc,  0x460000, \"inc\", zSI, xx, zSI, xx, xx, i64, (fW6&(~fWC)), t64e[7][0]},\n    {PREFIX,  0x460000, \"rex.rx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_X},\n  }, { /* x64_ext 7 */\n    {OP_inc,  0x470000, \"inc\", zDI, xx, zDI, xx, xx, i64, (fW6&(~fWC)), tex[12][0]},\n    {PREFIX,  0x470000, \"rex.rxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 8 */\n    {OP_dec,  0x480000, \"dec\", zAX, xx, zAX, xx, xx, i64, (fW6&(~fWC)), t64e[9][0]},\n    {PREFIX,  0x480000, \"rex.w\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W},\n  }, { /* x64_ext 9 */\n    {OP_dec,  0x490000, \"dec\", zCX, xx, zCX, xx, xx, i64, (fW6&(~fWC)), t64e[10][0]},\n    {PREFIX,  0x490000, \"rex.wb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_B},\n  }, { /* x64_ext 10 */\n    {OP_dec,  0x4a0000, \"dec\", zDX, xx, zDX, xx, xx, i64, (fW6&(~fWC)), t64e[11][0]},\n    {PREFIX,  0x4a0000, \"rex.wx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_X},\n  }, { /* x64_ext 11 */\n    {OP_dec,  0x4b0000, \"dec\", zBX, xx, zBX, xx, xx, i64, (fW6&(~fWC)), t64e[12][0]},\n    {PREFIX,  0x4b0000, \"rex.wxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 12 */\n    {OP_dec,  0x4c0000, \"dec\", zSP, xx, zSP, xx, xx, i64, (fW6&(~fWC)), t64e[13][0]},\n    {PREFIX,  0x4c0000, \"rex.wr\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R},\n  }, { /* x64_ext 13 */\n    {OP_dec,  0x4d0000, \"dec\", zBP, xx, zBP, xx, xx, i64, (fW6&(~fWC)), t64e[14][0]},\n    {PREFIX,  0x4d0000, \"rex.wrb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_B},\n  }, { /* x64_ext 14 */\n    {OP_dec,  0x4e0000, \"dec\", zSI, xx, zSI, xx, xx, i64, (fW6&(~fWC)), t64e[15][0]},\n    {PREFIX,  0x4e0000, \"rex.wrx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_X},\n  }, { /* x64_ext 15 */\n    {OP_dec,  0x4f0000, \"dec\", zDI, xx, zDI, xx, xx, i64, (fW6&(~fWC)), tex[12][1]},\n    {PREFIX,  0x4f0000, \"rex.wrxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 16 */\n    {OP_arpl,   0x630000, \"arpl\", Ew, xx, Gw, xx, xx, mrm|i64, fWZ, END_LIST},\n    {OP_movsxd, 0x630000, \"movsxd\", Gv, xx, Ed, xx, xx, mrm|o64, x, END_LIST},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on the first two bits of the 2nd byte,\n * or whether in x64 mode.\n */\nconst instr_info_t vex_prefix_extensions[][2] = {\n  {    /* vex_prefix_ext 0 */\n    {OP_les,  0xc40000, \"les\", Gz, es, Mp, xx, xx, mrm|i64, x, END_LIST},\n    {PREFIX,  0xc40000, \"vex+2b\", xx, xx, xx, xx, xx, no, x, PREFIX_VEX_3B},\n  }, { /* vex_prefix_ext 1 */\n    {OP_lds,  0xc50000, \"lds\", Gz, ds, Mp, xx, xx, mrm|i64, x, END_LIST},\n    {PREFIX,  0xc50000, \"vex+1b\", xx, xx, xx, xx, xx, no, x, PREFIX_VEX_2B},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on bits 4 and 5 of the 2nd byte.\n */\nconst instr_info_t xop_prefix_extensions[][2] = {\n  {    /* xop_prefix_ext 0 */\n    {EXTENSION, 0x8f0000, \"(group 1d)\", xx, xx, xx, xx, xx, mrm, x, 26},\n    {PREFIX,    0x8f0000, \"xop\", xx, xx, xx, xx, xx, no, x, PREFIX_XOP},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether vex-encoded and vex.L\n * Index 0 = no vex, 1 = vex and vex.L=0, 2 = vex and vex.L=1\n */\nconst instr_info_t vex_L_extensions[][3] = {\n  {    /* vex_L_ext 0 */\n    {OP_emms,       0x0f7710, \"emms\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vzeroupper, 0x0f7710, \"vzeroupper\", xx, xx, xx, xx, xx, vex, x, END_LIST},\n    {OP_vzeroall,   0x0f7790, \"vzeroall\", xx, xx, xx, xx, xx, vex, x, END_LIST},\n  },\n};\n\n/****************************************************************************\n* Instructions that differ depending on whether evex-encoded.\n* Index 0 = no evex, 1 = evex\n*/\n\nconst instr_info_t evex_prefix_extensions[][2] = {\n  {   /* evex_prefix_ext */\n    {OP_bound, 0x620000, \"bound\", xx, xx, Gv, Ma, xx, mrm|i64, x, END_LIST},\n    {PREFIX,   0x620000, \"(evex prefix)\", xx, xx, xx, xx, xx, no, x, PREFIX_EVEX},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether a rex prefix is present.\n */\n\n/* Instructions that differ depending on whether rex.b in is present.\n * The table is indexed by rex.b: index 0 is for no rex.b.\n */\nconst instr_info_t rex_b_extensions[][2] = {\n  { /* rex.b extension 0 */\n    {OP_nop,  0x900000, \"nop\", xx, xx, xx, xx, xx, no, x, tpe[103][2]},\n    /* For decoding we avoid needing new operand types by only getting\n     * here if rex.b is set.  For encode, we would need either to take\n     * REQUIRES_REX + OPCODE_SUFFIX or a new operand type for registers that\n     * must be extended (could also try to list r8 instead of eax but\n     * have to make sure all decode/encode routines can handle that as most\n     * assume the registers listed here are 32-bit base): that's too\n     * much effort for a corner case that we're not 100% certain works on\n     * all x64 processors, so we just don't list in the encoding chain.\n     */\n    {OP_xchg, 0x900000, \"xchg\", eAX_x, eAX, eAX_x, eAX, xx, o64, x, END_LIST},\n  },\n};\n\n/* Instructions that differ depending on whether rex.w in is present.\n * The table is indexed by rex.w: index 0 is for no rex.w.\n */\nconst instr_info_t rex_w_extensions[][2] = {\n  { /* rex.w extension 0 */\n    {OP_fxsave32, 0x0fae30, \"fxsave\",   Me, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_fxsave64, 0x0fae30, \"fxsave64\", Me, xx, xx, xx, xx, mrm|rex, x, END_LIST},\n  },\n  { /* rex.w extension 1 */\n    {OP_fxrstor32, 0x0fae31, \"fxrstor\",   xx, xx, Me, xx, xx, mrm, x, END_LIST},\n    {OP_fxrstor64, 0x0fae31, \"fxrstor64\", xx, xx, Me, xx, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 2 */\n    {OP_xsave32,   0x0fae34, \"xsave\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsave64,   0x0fae34, \"xsave64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 3 */\n    {OP_xrstor32, 0x0fae35, \"xrstor\",   xx, xx, Mxsave, edx, eax, mrm, x, END_LIST},\n    {OP_xrstor64, 0x0fae35, \"xrstor64\", xx, xx, Mxsave, edx, eax, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 4 */\n    {OP_xsaveopt32, 0x0fae36, \"xsaveopt\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsaveopt64, 0x0fae36, \"xsaveopt64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 5 */\n    {OP_xsavec32, 0x0fc734, \"xsavec\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsavec64, 0x0fc734, \"xsavec64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n};\n\n/****************************************************************************\n * 3-byte-opcode instructions: 0x0f 0x38 and 0x0f 0x3a.\n * SSSE3 and SSE4.\n *\n * XXX: if they add more 2nd byte possibilities, we could switch to one\n * large table here and one extension type with indices into which subtable.\n * For now we have two separate tables.\n *\n * N.B.: if any are added here that do not take modrm bytes, or whose\n * size can vary based on data16 or addr16, we need to modify our\n * decode_fast table assumptions!\n *\n * Many of these only come in Vdq,Wdq forms, yet still require the 0x66 prefix.\n * Rather than waste space in the prefix_extensions table for 4 entries 3 of which\n * are invalid, and need another layer of lookup, we use the new REQUIRES_PREFIX\n * flag (\"reqp\").\n *\n * Since large parts of the opcode space are empty, we save space by having a\n * table of 256 indices instead of 256 instr_info_t structs.\n */\nconst byte third_byte_38_index[256] = {\n  /* 0   1   2   3    4   5   6   7    8   9   A   B    C   D   E   F */\n     1,  2,  3,  4,   5,  6,  7,  8,   9, 10, 11, 12,  96, 97, 56, 57,  /* 0 */\n    16,  0,  0, 88,  17, 18,111, 19,  89, 90, 91,  0,  13, 14, 15,  0,  /* 1 */\n    20, 21, 22, 23,  24, 25,  0,  0,  26, 27, 28, 29,  92, 93, 94, 95,  /* 2 */\n    30, 31, 32, 33,  34, 35,112, 36,  37, 38, 39, 40,  41, 42, 43, 44,  /* 3 */\n    45, 46,  0,  0,   0,113,114,115,   0,  0,  0,  0,   0,  0,  0,  0,  /* 4 */\n     0,  0,  0,  0,   0,  0,  0,  0, 118,119,108,  0,   0,  0,  0,  0,  /* 5 */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* 6 */\n     0,  0,  0,  0,   0,  0,  0,  0, 116,117,  0,  0,   0,  0,  0,  0,  /* 7 */\n    49, 50,103,  0,   0,  0,  0,  0,   0,  0,  0,  0, 109,  0,110,  0,  /* 8 */\n   104,105,106,107,   0,  0, 58, 59,  60, 61, 62, 63,  64, 65, 66, 67,  /* 9 */\n     0,  0,  0,  0,   0,  0, 68, 69,  70, 71, 72, 73,  74, 75, 76, 77,  /* A */\n     0,  0,  0,  0,   0,  0, 78, 79,  80, 81, 82, 83,  84, 85, 86, 87,  /* B */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* C */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0, 51,  52, 53, 54, 55,  /* D */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* E */\n    47, 48,100, 99,   0,101,102, 98,   0,  0,  0,  0,   0,  0,  0,  0   /* F */\n};\n\nconst instr_info_t third_byte_38[] = {\n  {INVALID,     0x38ff18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},              /* 0*/\n  /**** SSSE3 ****/\n  {PREFIX_EXT,  0x380018,   \"(prefix ext 118)\", xx, xx, xx, xx, xx, mrm, x, 118},/* 1*/\n  {PREFIX_EXT,  0x380118,   \"(prefix ext 119)\", xx, xx, xx, xx, xx, mrm, x, 119},/* 2*/\n  {PREFIX_EXT,  0x380218,   \"(prefix ext 120)\", xx, xx, xx, xx, xx, mrm, x, 120},/* 3*/\n  {PREFIX_EXT,  0x380318,   \"(prefix ext 121)\", xx, xx, xx, xx, xx, mrm, x, 121},/* 4*/\n  {PREFIX_EXT,  0x380418,   \"(prefix ext 122)\", xx, xx, xx, xx, xx, mrm, x, 122},/* 5*/\n  {PREFIX_EXT,  0x380518,   \"(prefix ext 123)\", xx, xx, xx, xx, xx, mrm, x, 123},/* 6*/\n  {PREFIX_EXT,  0x380618,   \"(prefix ext 124)\", xx, xx, xx, xx, xx, mrm, x, 124},/* 7*/\n  {PREFIX_EXT,  0x380718,   \"(prefix ext 125)\", xx, xx, xx, xx, xx, mrm, x, 125},/* 8*/\n  {PREFIX_EXT,  0x380818,   \"(prefix ext 126)\", xx, xx, xx, xx, xx, mrm, x, 126},/* 9*/\n  {PREFIX_EXT,  0x380918,   \"(prefix ext 127)\", xx, xx, xx, xx, xx, mrm, x, 127},/*10*/\n  {PREFIX_EXT,  0x380a18,   \"(prefix ext 128)\", xx, xx, xx, xx, xx, mrm, x, 128},/*11*/\n  {PREFIX_EXT,  0x380b18,   \"(prefix ext 129)\", xx, xx, xx, xx, xx, mrm, x, 129},/*12*/\n  {PREFIX_EXT,  0x381c18,   \"(prefix ext 130)\", xx, xx, xx, xx, xx, mrm, x, 130},/*13*/\n  {PREFIX_EXT,  0x381d18,   \"(prefix ext 131)\", xx, xx, xx, xx, xx, mrm, x, 131},/*14*/\n  {PREFIX_EXT,  0x381e18,   \"(prefix ext 132)\", xx, xx, xx, xx, xx, mrm, x, 132},/*15*/\n  /**** SSE4 ****/\n  {OP_pblendvb, 0x66381018, \"pblendvb\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*16*/\n  {OP_blendvps, 0x66381418, \"blendvps\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*17*/\n  {OP_blendvpd, 0x66381518, \"blendvpd\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*18*/\n  {E_VEX_EXT,  0x66381718, \"(e_vex ext  3)\", xx, xx, xx, xx, xx, mrm, x,  3},/*19*/\n  /* 20 */\n  {E_VEX_EXT,  0x66382018, \"(e_vex ext  4)\", xx, xx, xx, xx, xx, mrm, x,  4},/*20*/\n  {E_VEX_EXT,  0x66382118, \"(e_vex ext  5)\", xx, xx, xx, xx, xx, mrm, x,  5},/*21*/\n  {E_VEX_EXT,  0x66382218, \"(e_vex ext  6)\", xx, xx, xx, xx, xx, mrm, x,  6},/*22*/\n  {E_VEX_EXT,  0x66382318, \"(e_vex ext  7)\", xx, xx, xx, xx, xx, mrm, x,  7},/*23*/\n  {E_VEX_EXT,  0x66382418, \"(e_vex ext  8)\", xx, xx, xx, xx, xx, mrm, x,  8},/*24*/\n  {E_VEX_EXT,  0x66382518, \"(e_vex ext  9)\", xx, xx, xx, xx, xx, mrm, x,  9},/*25*/\n  {E_VEX_EXT,  0x66382818, \"(e_vex ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},/*26*/\n  {E_VEX_EXT,  0x66382918, \"(e_vex ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},/*27*/\n  {E_VEX_EXT,  0x66382a18, \"(e_vex ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},/*28*/\n  {E_VEX_EXT,  0x66382b18, \"(e_vex ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},/*29*/\n  /* 30 */\n  {E_VEX_EXT,  0x66383018, \"(e_vex ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},/*30*/\n  {E_VEX_EXT,  0x66383118, \"(e_vex ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},/*31*/\n  {E_VEX_EXT,  0x66383218, \"(e_vex ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},/*32*/\n  {E_VEX_EXT,  0x66383318, \"(e_vex ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},/*33*/\n  {E_VEX_EXT,  0x66383418, \"(e_vex ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},/*34*/\n  {E_VEX_EXT,  0x66383518, \"(e_vex ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},/*35*/\n  {E_VEX_EXT,  0x66383718, \"(e_vex ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},/*36*/\n  {E_VEX_EXT,  0x66383818, \"(e_vex ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},/*37*/\n  {E_VEX_EXT,  0x66383918, \"(e_vex ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},/*38*/\n  {E_VEX_EXT,  0x66383a18, \"(e_vex ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},/*39*/\n  {E_VEX_EXT,  0x66383b18, \"(e_vex ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},/*40*/\n  {E_VEX_EXT,  0x66383c18, \"(e_vex ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},/*41*/\n  {E_VEX_EXT,  0x66383d18, \"(e_vex ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},/*42*/\n  {E_VEX_EXT,  0x66383e18, \"(e_vex ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},/*43*/\n  {E_VEX_EXT,  0x66383f18, \"(e_vex ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},/*44*/\n  /* 40 */\n  {E_VEX_EXT,  0x66384018, \"(e_vex ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},/*45*/\n  {E_VEX_EXT,  0x66384118, \"(e_vex ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},/*46*/\n  /* f0 */\n  {PREFIX_EXT,  0x38f018,   \"(prefix ext 138)\", xx, xx, xx, xx, xx, mrm, x, 138},/*47*/\n  {PREFIX_EXT,  0x38f118,   \"(prefix ext 139)\", xx, xx, xx, xx, xx, mrm, x, 139},/*48*/\n  /* 80 */\n  {OP_invept,   0x66388018, \"invept\",   xx, xx, Gr, Mdq, xx, mrm|reqp, x, END_LIST},/*49*/\n  {OP_invvpid,  0x66388118, \"invvpid\",  xx, xx, Gr, Mdq, xx, mrm|reqp, x, END_LIST},/*50*/\n  /* db-df */\n  {E_VEX_EXT,  0x6638db18, \"(e_vex ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},/*51*/\n  {E_VEX_EXT,  0x6638dc18, \"(e_vex ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},/*52*/\n  {E_VEX_EXT,  0x6638dd18, \"(e_vex ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},/*53*/\n  {E_VEX_EXT,  0x6638de18, \"(e_vex ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},/*54*/\n  {E_VEX_EXT,  0x6638df18, \"(e_vex ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},/*55*/\n  /* AVX */\n  {E_VEX_EXT,  0x66380e18, \"(e_vex ext 59)\", xx, xx, xx, xx, xx, mrm, x, 59},/*56*/\n  {E_VEX_EXT,  0x66380f18, \"(e_vex ext 60)\", xx, xx, xx, xx, xx, mrm, x, 60},/*57*/\n  /* FMA 96-9f */\n  {VEX_W_EXT, 0x66389618, \"(vex_W ext  6)\", xx, xx, xx, xx, xx, mrm, x,  6},/*58*/\n  {VEX_W_EXT, 0x66389718, \"(vex_W ext  9)\", xx, xx, xx, xx, xx, mrm, x,  9},/*59*/\n  {VEX_W_EXT, 0x66389818, \"(vex_W ext  0)\", xx, xx, xx, xx, xx, mrm, x,  0},/*60*/\n  {VEX_W_EXT, 0x66389918, \"(vex_W ext  3)\", xx, xx, xx, xx, xx, mrm, x,  3},/*61*/\n  {VEX_W_EXT, 0x66389a18, \"(vex_W ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},/*62*/\n  {VEX_W_EXT, 0x66389b18, \"(vex_W ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},/*63*/\n  {VEX_W_EXT, 0x66389c18, \"(vex_W ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},/*64*/\n  {VEX_W_EXT, 0x66389d18, \"(vex_W ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},/*65*/\n  {VEX_W_EXT, 0x66389e18, \"(vex_W ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},/*66*/\n  {VEX_W_EXT, 0x66389f18, \"(vex_W ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},/*67*/\n  /* FMA a6-af */\n  {VEX_W_EXT, 0x6638a618, \"(vex_W ext  7)\", xx, xx, xx, xx, xx, mrm, x,  7},/*68*/\n  {VEX_W_EXT, 0x6638a718, \"(vex_W ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},/*69*/\n  {VEX_W_EXT, 0x6638a818, \"(vex_W ext  1)\", xx, xx, xx, xx, xx, mrm, x,  1},/*70*/\n  {VEX_W_EXT, 0x6638a918, \"(vex_W ext  4)\", xx, xx, xx, xx, xx, mrm, x,  4},/*71*/\n  {VEX_W_EXT, 0x6638aa18, \"(vex_W ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},/*72*/\n  {VEX_W_EXT, 0x6638ab18, \"(vex_W ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},/*73*/\n  {VEX_W_EXT, 0x6638ac18, \"(vex_W ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},/*74*/\n  {VEX_W_EXT, 0x6638ad18, \"(vex_W ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},/*75*/\n  {VEX_W_EXT, 0x6638ae18, \"(vex_W ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},/*76*/\n  {VEX_W_EXT, 0x6638af18, \"(vex_W ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},/*77*/\n  /* FMA b6-bf */\n  {VEX_W_EXT, 0x6638b618, \"(vex_W ext  8)\", xx, xx, xx, xx, xx, mrm, x,  8},/*78*/\n  {VEX_W_EXT, 0x6638b718, \"(vex_W ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},/*79*/\n  {VEX_W_EXT, 0x6638b818, \"(vex_W ext  2)\", xx, xx, xx, xx, xx, mrm, x,  2},/*80*/\n  {VEX_W_EXT, 0x6638b918, \"(vex_W ext  5)\", xx, xx, xx, xx, xx, mrm, x,  5},/*81*/\n  {VEX_W_EXT, 0x6638ba18, \"(vex_W ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},/*82*/\n  {VEX_W_EXT, 0x6638bb18, \"(vex_W ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},/*83*/\n  {VEX_W_EXT, 0x6638bc18, \"(vex_W ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},/*84*/\n  {VEX_W_EXT, 0x6638bd18, \"(vex_W ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},/*85*/\n  {VEX_W_EXT, 0x6638be18, \"(vex_W ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},/*86*/\n  {VEX_W_EXT, 0x6638bf18, \"(vex_W ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},/*87*/\n  /* AVX overlooked in original pass */\n  {E_VEX_EXT, 0x66381318, \"(e_vex ext 63)\", xx, xx, xx, xx, xx, mrm, x, 63},/*88*/\n  {E_VEX_EXT, 0x66381818, \"(e_vex ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},/*89*/\n  {E_VEX_EXT, 0x66381918, \"(e_vex ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},/*90*/\n  {E_VEX_EXT, 0x66381a18, \"(e_vex ext 66)\", xx, xx, xx, xx, xx, mrm, x, 66},/*91*/\n  {E_VEX_EXT, 0x66382c18, \"(e_vex ext 67)\", xx, xx, xx, xx, xx, mrm, x, 67},/*92*/\n  {E_VEX_EXT, 0x66382d18, \"(e_vex ext 68)\", xx, xx, xx, xx, xx, mrm, x, 68},/*93*/\n  {E_VEX_EXT, 0x66382e18, \"(e_vex ext 69)\", xx, xx, xx, xx, xx, mrm, x, 69},/*94*/\n  {E_VEX_EXT, 0x66382f18, \"(e_vex ext 70)\", xx, xx, xx, xx, xx, mrm, x, 70},/*95*/\n  {E_VEX_EXT, 0x66380c18, \"(e_vex ext 77)\", xx, xx, xx, xx, xx, mrm, x, 77},/*96*/\n  {E_VEX_EXT, 0x66380d18, \"(e_vex ext 78)\", xx, xx, xx, xx, xx, mrm, x, 78},/*97*/\n  /* TBM */\n  {PREFIX_EXT, 0x38f718, \"(prefix ext 141)\", xx, xx, xx, xx, xx, mrm, x, 141},  /*98*/\n  /* BMI1 */\n  {EXTENSION, 0x38f318, \"(group 17)\", By, xx, Ey, xx, xx, mrm|vex, x, 31},      /*99*/\n  /* marked reqp b/c it should have no prefix (prefixes for future opcodes) */\n  {OP_andn, 0x38f218, \"andn\", Gy, xx, By, Ey, xx, mrm|vex|reqp, fW6, END_LIST},/*100*/\n  /* BMI2 */\n  {PREFIX_EXT, 0x38f518, \"(prefix ext 142)\", xx, xx, xx, xx, xx, mrm, x, 142}, /*101*/\n  {PREFIX_EXT, 0x38f618, \"(prefix ext 143)\", xx, xx, xx, xx, xx, mrm, x, 143}, /*102*/\n  {OP_invpcid, 0x66388218, \"invpcid\",  xx, xx, Gy, Mdq, xx, mrm|reqp, x, END_LIST},/*103*/\n  /* AVX2 */\n  {VEX_W_EXT, 0x66389018, \"(vex_W ext 66)\", xx, xx, xx, xx, xx, mrm|vex, x, 66},/*104*/\n  {VEX_W_EXT, 0x66389118, \"(vex_W ext 67)\", xx, xx, xx, xx, xx, mrm|vex, x, 67},/*105*/\n  {VEX_W_EXT, 0x66389218, \"(vex_W ext 68)\", xx, xx, xx, xx, xx, mrm|vex, x, 68},/*106*/\n  {VEX_W_EXT, 0x66389318, \"(vex_W ext 69)\", xx, xx, xx, xx, xx, mrm|vex, x, 69},/*107*/\n  {OP_vbroadcasti128,0x66385a18, \"vbroadcasti128\",Vqq,xx,Mdq,xx,xx,mrm|vex|reqp,x,END_LIST},/*108*/\n  {VEX_W_EXT, 0x66388c18, \"(vex_W ext 70)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 70},/*109*/\n  {VEX_W_EXT, 0x66388e18, \"(vex_W ext 71)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 71},/*110*/\n  /* Following Intel and not marking as packed float vs ints: just \"qq\". */\n  {OP_vpermps,0x66381618, \"vpermps\",Vqq,xx,Hqq,Wqq,xx, mrm|vex|reqp,x,END_LIST}, /*111*/\n  {OP_vpermd, 0x66383618, \"vpermd\", Vqq,xx,Hqq,Wqq,xx, mrm|vex|reqp,x,END_LIST}, /*112*/\n  {VEX_W_EXT, 0x66384518, \"(vex_W ext 72)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 72},/*113*/\n  {OP_vpsravd,0x66384618, \"vpsravd\", Vx,xx,Hx,Wx,xx, mrm|vex|reqp, x, END_LIST}, /*114*/\n  {VEX_W_EXT, 0x66384718, \"(vex_W ext 73)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 73},/*115*/\n  {OP_vpbroadcastb, 0x66387818, \"vpbroadcastb\", Vx, xx, Wb_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*116*/\n  {OP_vpbroadcastw, 0x66387918, \"vpbroadcastw\", Vx, xx, Ww_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*117*/\n  {OP_vpbroadcastd, 0x66385818, \"vpbroadcastd\", Vx, xx, Wd_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*118*/\n  {OP_vpbroadcastq, 0x66385918, \"vpbroadcastq\", Vx, xx, Wq_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*119*/\n};\n\n/* N.B.: every 0x3a instr so far has an immediate.  If a version w/o an immed\n * comes along we'll have to add a threebyte_3a_vex_extra[] table to decode_fast.c.\n */\nconst byte third_byte_3a_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n    59,60,61, 0, 28,29,30, 0,  6, 7, 8, 9, 10,11,12, 1,  /* 0 */\n     0, 0, 0, 0,  2, 3, 4, 5, 31,32, 0, 0,  0,33, 0, 0,  /* 1 */\n    13,14,15, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n    63,64,65,66,  0, 0, 0, 0, 57,58, 0, 0,  0, 0, 0, 0,  /* 3 */\n    16,17,18, 0, 23, 0,62, 0, 54,55,25,26, 27, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 34,35,36,37,  /* 5 */\n    19,20,21,22,  0, 0, 0, 0, 38,39,40,41, 42,43,44,45,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0, 46,47,48,49, 50,51,52,53,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0,24,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n    56, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst instr_info_t third_byte_3a[] = {\n  {INVALID,     0x3aff18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},                 /* 0*/\n  /**** SSSE3 ****/\n  {PREFIX_EXT,  0x3a0f18, \"(prefix ext 133)\", xx, xx, xx, xx, xx, mrm, x, 133},    /* 1*/\n  /**** SSE4 ****/\n  {E_VEX_EXT,  0x663a1418, \"(e_vex ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},/* 2*/\n  {E_VEX_EXT,  0x663a1518, \"(e_vex ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},/* 3*/\n  {E_VEX_EXT,  0x663a1618, \"(e_vex ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},/* 4*/\n  {E_VEX_EXT,  0x663a1718, \"(e_vex ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},/* 5*/\n  {E_VEX_EXT,  0x663a0818, \"(e_vex ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},/* 6*/\n  {E_VEX_EXT,  0x663a0918, \"(e_vex ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},/* 7*/\n  {E_VEX_EXT,  0x663a0a18, \"(e_vex ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},/* 8*/\n  {E_VEX_EXT,  0x663a0b18, \"(e_vex ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},/* 9*/\n  {E_VEX_EXT,  0x663a0c18, \"(e_vex ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},/*10*/\n  {E_VEX_EXT,  0x663a0d18, \"(e_vex ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},/*11*/\n  {E_VEX_EXT,  0x663a0e18, \"(e_vex ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},/*12*/\n  /* 20 */\n  {E_VEX_EXT,  0x663a2018, \"(e_vex ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},/*13*/\n  {E_VEX_EXT,  0x663a2118, \"(e_vex ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},/*14*/\n  {E_VEX_EXT,  0x663a2218, \"(e_vex ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},/*15*/\n  /* 40 */\n  {E_VEX_EXT,  0x663a4018, \"(e_vex ext 50)\", xx, xx, xx, xx, xx, mrm, x, 50},/*16*/\n  {E_VEX_EXT,  0x663a4118, \"(e_vex ext 51)\", xx, xx, xx, xx, xx, mrm, x, 51},/*17*/\n  {E_VEX_EXT,  0x663a4218, \"(e_vex ext 52)\", xx, xx, xx, xx, xx, mrm, x, 52},/*18*/\n  /* 60 */\n  {E_VEX_EXT,  0x663a6018, \"(e_vex ext 53)\", xx, xx, xx, xx, xx, mrm, x, 53},/*19*/\n  {E_VEX_EXT,  0x663a6118, \"(e_vex ext 54)\", xx, xx, xx, xx, xx, mrm, x, 54},/*20*/\n  {E_VEX_EXT,  0x663a6218, \"(e_vex ext 55)\", xx, xx, xx, xx, xx, mrm, x, 55},/*21*/\n  {E_VEX_EXT,  0x663a6318, \"(e_vex ext 56)\", xx, xx, xx, xx, xx, mrm, x, 56},/*22*/\n  {E_VEX_EXT,  0x663a4418, \"(e_vex ext 57)\", xx, xx, xx, xx, xx, mrm, x, 57},/*23*/\n  {E_VEX_EXT,  0x663adf18, \"(e_vex ext 58)\", xx, xx, xx, xx, xx, mrm, x, 58},/*24*/\n  /* AVX overlooked in original pass */\n  {E_VEX_EXT,  0x663a4a18, \"(e_vex ext  0)\", xx, xx, xx, xx, xx, mrm, x,  0},/*25*/\n  {E_VEX_EXT,  0x663a4b18, \"(e_vex ext  1)\", xx, xx, xx, xx, xx, mrm, x,  1},/*26*/\n  {E_VEX_EXT,  0x663a4c18, \"(e_vex ext  2)\", xx, xx, xx, xx, xx, mrm, x,  2},/*27*/\n  {E_VEX_EXT,  0x663a0418, \"(e_vex ext 71)\", xx, xx, xx, xx, xx, mrm, x, 71},/*28*/\n  {E_VEX_EXT,  0x663a0518, \"(e_vex ext 72)\", xx, xx, xx, xx, xx, mrm, x, 72},/*29*/\n  {E_VEX_EXT,  0x663a0618, \"(e_vex ext 73)\", xx, xx, xx, xx, xx, mrm, x, 73},/*30*/\n  {E_VEX_EXT,  0x663a1818, \"(e_vex ext 74)\", xx, xx, xx, xx, xx, mrm, x, 74},/*31*/\n  {E_VEX_EXT,  0x663a1918, \"(e_vex ext 75)\", xx, xx, xx, xx, xx, mrm, x, 75},/*32*/\n  {E_VEX_EXT,  0x663a1d18, \"(e_vex ext 76)\", xx, xx, xx, xx, xx, mrm, x, 76},/*33*/\n  /* FMA4 */\n  {VEX_W_EXT,0x663a5c18, \"(vex_W ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},/*34*/\n  {VEX_W_EXT,0x663a5d18, \"(vex_W ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},/*35*/\n  {VEX_W_EXT,0x663a5e18, \"(vex_W ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},/*36*/\n  {VEX_W_EXT,0x663a5f18, \"(vex_W ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},/*37*/\n  {VEX_W_EXT,0x663a6818, \"(vex_W ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},/*38*/\n  {VEX_W_EXT,0x663a6918, \"(vex_W ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},/*39*/\n  {VEX_W_EXT,0x663a6a18, \"(vex_W ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},/*40*/\n  {VEX_W_EXT,0x663a6b18, \"(vex_W ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},/*41*/\n  {VEX_W_EXT,0x663a6c18, \"(vex_W ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},/*42*/\n  {VEX_W_EXT,0x663a6d18, \"(vex_W ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},/*43*/\n  {VEX_W_EXT,0x663a6e18, \"(vex_W ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},/*44*/\n  {VEX_W_EXT,0x663a6f18, \"(vex_W ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},/*45*/\n  {VEX_W_EXT,0x663a7818, \"(vex_W ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},/*46*/\n  {VEX_W_EXT,0x663a7918, \"(vex_W ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},/*47*/\n  {VEX_W_EXT,0x663a7a18, \"(vex_W ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},/*48*/\n  {VEX_W_EXT,0x663a7b18, \"(vex_W ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},/*49*/\n  {VEX_W_EXT,0x663a7c18, \"(vex_W ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},/*50*/\n  {VEX_W_EXT,0x663a7d18, \"(vex_W ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},/*51*/\n  {VEX_W_EXT,0x663a7e18, \"(vex_W ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},/*52*/\n  {VEX_W_EXT,0x663a7f18, \"(vex_W ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},/*53*/\n  /* XOP */\n  {VEX_W_EXT,0x663a4818, \"(vex_W ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},/*54*/\n  {VEX_W_EXT,0x663a4918, \"(vex_W ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},/*55*/\n  /* BMI2 */\n  {OP_rorx,  0xf23af018, \"rorx\",  Gy, xx, Ey, Ib, xx, mrm|vex|reqp, x, END_LIST},/*56*/\n  /* AVX2 */\n  {OP_vinserti128,0x663a3818,\"vinserti128\",Vqq,xx,Hqq,Wdq,Ib,mrm|vex|reqp,x,END_LIST},/*57*/\n  {OP_vextracti128,0x663a3918,\"vextracti128\",Wdq,xx,Vqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*58*/\n  {OP_vpermq, 0x663a0058, \"vpermq\", Vqq,xx,Wqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*59*/\n  /* Following Intel and not marking as packed float vs ints: just \"qq\". */\n  {OP_vpermpd,0x663a0158, \"vpermpd\",Vqq,xx,Wqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*60*/\n  {OP_vpblendd,0x663a0218,\"vpblendd\",Vx,xx,Hx,Wx,Ib, mrm|vex|reqp,x,END_LIST},/*61*/\n  {OP_vperm2i128,0x663a4618,\"vperm2i128\",Vqq,xx,Hqq,Wqq,Ib, mrm|vex|reqp,x,END_LIST},/*62*/\n  /* AVX-512, VEX prefix. */\n  {VEX_W_EXT,0x660f3010, \"(vex_W ext 102)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 102},/*63*/\n  {VEX_W_EXT,0x660f3110, \"(vex_W ext 103)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 103},/*64*/\n  {VEX_W_EXT,0x660f3210, \"(vex_W ext 100)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 100},/*65*/\n  {VEX_W_EXT,0x660f3310, \"(vex_W ext 101)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 101},/*66*/\n};\n\n/****************************************************************************\n * Instructions that differ depending on vex.W\n * Index is vex.W value\n */\nconst instr_info_t vex_W_extensions[][2] = {\n  {    /* vex_W_ext 0 */\n    {OP_vfmadd132ps,0x66389818,\"vfmadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd132pd,0x66389858,\"vfmadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 1 */\n    {OP_vfmadd213ps,0x6638a818,\"vfmadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd213pd,0x6638a858,\"vfmadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 2 */\n    {OP_vfmadd231ps,0x6638b818,\"vfmadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd231pd,0x6638b858,\"vfmadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 3 */\n    {OP_vfmadd132ss,0x66389918,\"vfmadd132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd132sd,0x66389958,\"vfmadd132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 4 */\n    {OP_vfmadd213ss,0x6638a918,\"vfmadd213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd213sd,0x6638a958,\"vfmadd213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 5 */\n    {OP_vfmadd231ss,0x6638b918,\"vfmadd231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd231sd,0x6638b958,\"vfmadd231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 6 */\n    {OP_vfmaddsub132ps,0x66389618,\"vfmaddsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub132pd,0x66389658,\"vfmaddsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 7 */\n    {OP_vfmaddsub213ps,0x6638a618,\"vfmaddsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub213pd,0x6638a658,\"vfmaddsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 8 */\n    {OP_vfmaddsub231ps,0x6638b618,\"vfmaddsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub231pd,0x6638b658,\"vfmaddsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 9 */\n    {OP_vfmsubadd132ps,0x66389718,\"vfmsubadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd132pd,0x66389758,\"vfmsubadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 10 */\n    {OP_vfmsubadd213ps,0x6638a718,\"vfmsubadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd213pd,0x6638a758,\"vfmsubadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 11 */\n    {OP_vfmsubadd231ps,0x6638b718,\"vfmsubadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd231pd,0x6638b758,\"vfmsubadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 12 */\n    {OP_vfmsub132ps,0x66389a18,\"vfmsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub132pd,0x66389a58,\"vfmsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 13 */\n    {OP_vfmsub213ps,0x6638aa18,\"vfmsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub213pd,0x6638aa58,\"vfmsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 14 */\n    {OP_vfmsub231ps,0x6638ba18,\"vfmsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub231pd,0x6638ba58,\"vfmsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 15 */\n    {OP_vfmsub132ss,0x66389b18,\"vfmsub132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub132sd,0x66389b58,\"vfmsub132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 16 */\n    {OP_vfmsub213ss,0x6638ab18,\"vfmsub213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub213sd,0x6638ab58,\"vfmsub213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 17 */\n    {OP_vfmsub231ss,0x6638bb18,\"vfmsub231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub231sd,0x6638bb58,\"vfmsub231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 18 */\n    {OP_vfnmadd132ps,0x66389c18,\"vfnmadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd132pd,0x66389c58,\"vfnmadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 19 */\n    {OP_vfnmadd213ps,0x6638ac18,\"vfnmadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd213pd,0x6638ac58,\"vfnmadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 20 */\n    {OP_vfnmadd231ps,0x6638bc18,\"vfnmadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd231pd,0x6638bc58,\"vfnmadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 21 */\n    {OP_vfnmadd132ss,0x66389d18,\"vfnmadd132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd132sd,0x66389d58,\"vfnmadd132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 22 */\n    {OP_vfnmadd213ss,0x6638ad18,\"vfnmadd213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd213sd,0x6638ad58,\"vfnmadd213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 23 */\n    {OP_vfnmadd231ss,0x6638bd18,\"vfnmadd231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd231sd,0x6638bd58,\"vfnmadd231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 24 */\n    {OP_vfnmsub132ps,0x66389e18,\"vfnmsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub132pd,0x66389e58,\"vfnmsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 25 */\n    {OP_vfnmsub213ps,0x6638ae18,\"vfnmsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub213pd,0x6638ae58,\"vfnmsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 26 */\n    {OP_vfnmsub231ps,0x6638be18,\"vfnmsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub231pd,0x6638be58,\"vfnmsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 27 */\n    {OP_vfnmsub132ss,0x66389f18,\"vfnmsub132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub132sd,0x66389f58,\"vfnmsub132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 28 */\n    {OP_vfnmsub213ss,0x6638af18,\"vfnmsub213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub213sd,0x6638af58,\"vfnmsub213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 29 */\n    {OP_vfnmsub231ss,0x6638bf18,\"vfnmsub231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub231sd,0x6638bf58,\"vfnmsub231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 30 */\n    {OP_vfmaddsubps,0x663a5c18,\"vfmaddsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[30][1]},\n    {OP_vfmaddsubps,0x663a5c58,\"vfmaddsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 31 */\n    {OP_vfmaddsubpd,0x663a5d18,\"vfmaddsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[31][1]},\n    {OP_vfmaddsubpd,0x663a5d58,\"vfmaddsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 32 */\n    {OP_vfmsubaddps,0x663a5e18,\"vfmsubaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[32][1]},\n    {OP_vfmsubaddps,0x663a5e58,\"vfmsubaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 33 */\n    {OP_vfmsubaddpd,0x663a5f18,\"vfmsubaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[33][1]},\n    {OP_vfmsubaddpd,0x663a5f58,\"vfmsubaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 34 */\n    {OP_vfmaddps,0x663a6818,\"vfmaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[34][1]},\n    {OP_vfmaddps,0x663a6858,\"vfmaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 35 */\n    {OP_vfmaddpd,0x663a6918,\"vfmaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[35][1]},\n    {OP_vfmaddpd,0x663a6958,\"vfmaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 36 */\n    {OP_vfmaddss,0x663a6a18,\"vfmaddss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[36][1]},\n    {OP_vfmaddss,0x663a6a58,\"vfmaddss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 37 */\n    {OP_vfmaddsd,0x663a6b18,\"vfmaddsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[37][1]},\n    {OP_vfmaddsd,0x663a6b58,\"vfmaddsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 38 */\n    {OP_vfmsubps,0x663a6c18,\"vfmsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[38][1]},\n    {OP_vfmsubps,0x663a6c58,\"vfmsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 39 */\n    {OP_vfmsubpd,0x663a6d18,\"vfmsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[39][1]},\n    {OP_vfmsubpd,0x663a6d58,\"vfmsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 40 */\n    {OP_vfmsubss,0x663a6e18,\"vfmsubss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[40][1]},\n    {OP_vfmsubss,0x663a6e58,\"vfmsubss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 41 */\n    {OP_vfmsubsd,0x663a6f18,\"vfmsubsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[41][1]},\n    {OP_vfmsubsd,0x663a6f58,\"vfmsubsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 42 */\n    {OP_vfnmaddps,0x663a7818,\"vfnmaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[42][1]},\n    {OP_vfnmaddps,0x663a7858,\"vfnmaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 43 */\n    {OP_vfnmaddpd,0x663a7918,\"vfnmaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[43][1]},\n    {OP_vfnmaddpd,0x663a7958,\"vfnmaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 44 */\n    {OP_vfnmaddss,0x663a7a18,\"vfnmaddss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[44][1]},\n    {OP_vfnmaddss,0x663a7a58,\"vfnmaddss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 45 */\n    {OP_vfnmaddsd,0x663a7b18,\"vfnmaddsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[45][1]},\n    {OP_vfnmaddsd,0x663a7b58,\"vfnmaddsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 46 */\n    {OP_vfnmsubps,0x663a7c18,\"vfnmsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[46][1]},\n    {OP_vfnmsubps,0x663a7c58,\"vfnmsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 47 */\n    {OP_vfnmsubpd,0x663a7d18,\"vfnmsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[47][1]},\n    {OP_vfnmsubpd,0x663a7d58,\"vfnmsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 48 */\n    {OP_vfnmsubss,0x663a7e18,\"vfnmsubss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[48][1]},\n    {OP_vfnmsubss,0x663a7e58,\"vfnmsubss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 49 */\n    {OP_vfnmsubsd,0x663a7f18,\"vfnmsubsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[49][1]},\n    {OP_vfnmsubsd,0x663a7f58,\"vfnmsubsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 50 */\n    {OP_vpcmov,    0x08a218,\"vpcmov\",    Vvs,xx,Hvs,Wvs,Lvs,mrm|vex,x,tvexw[50][1]},\n    {OP_vpcmov,    0x08a258,\"vpcmov\",    Vvs,xx,Hvs,Lvs,Wvs,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 51 */\n    {OP_vpperm,    0x08a318,\"vpperm\",    Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,tvexw[51][1]},\n    {OP_vpperm,    0x08a358,\"vpperm\",    Vdq,xx,Hdq,Ldq,Wdq,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 52 */\n    {OP_vprotb,    0x099018,\"vprotb\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[52][1]},\n    {OP_vprotb,    0x099058,\"vprotb\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 53 */\n    {OP_vprotw,    0x099118,\"vprotw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[53][1]},\n    {OP_vprotw,    0x099158,\"vprotw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 54 */\n    {OP_vprotd,    0x099218,\"vprotd\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[54][1]},\n    {OP_vprotd,    0x099258,\"vprotd\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 55 */\n    {OP_vprotq,    0x099318,\"vprotq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[55][1]},\n    {OP_vprotq,    0x099358,\"vprotq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 56 */\n    {OP_vpshlb,    0x099418,\"vpshlb\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[56][1]},\n    {OP_vpshlb,    0x099458,\"vpshlb\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 57 */\n    {OP_vpshlw,    0x099518,\"vpshlw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[57][1]},\n    {OP_vpshlw,    0x099558,\"vpshlw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 58 */\n    {OP_vpshld,    0x099618,\"vpshld\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[58][1]},\n    {OP_vpshld,    0x099658,\"vpshld\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 59 */\n    {OP_vpshlq,    0x099718,\"vpshlq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[59][1]},\n    {OP_vpshlq,    0x099758,\"vpshlq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 60 */\n    {OP_vpshab,    0x099818,\"vpshab\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[60][1]},\n    {OP_vpshab,    0x099858,\"vpshab\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 61 */\n    {OP_vpshaw,    0x099918,\"vpshaw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[61][1]},\n    {OP_vpshaw,    0x099958,\"vpshaw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 62 */\n    {OP_vpshad,    0x099a18,\"vpshad\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[62][1]},\n    {OP_vpshad,    0x099a58,\"vpshad\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 63 */\n    {OP_vpshaq,    0x099b18,\"vpshaq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[63][1]},\n    {OP_vpshaq,    0x099b58,\"vpshaq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 64 */\n    {OP_vpermil2ps,0x663a4818,\"vpermil2ps\",Vvs,xx,Hvs,Wvs,Lvs,mrm|vex|reqp,x,tvexw[64][1]},\n    {OP_vpermil2ps,0x663a4858,\"vpermil2ps\",Vvs,xx,Hvs,Lvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 65 */\n    {OP_vpermil2pd,0x663a4918,\"vpermil2pd\",Vvs,xx,Hvs,Wvs,Lvs,mrm|vex|reqp,x,tvexw[65][1]},\n    {OP_vpermil2pd,0x663a4958,\"vpermil2pd\",Vvs,xx,Hvs,Lvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 66 */\n    /* XXX: OP_v*gather* raise #UD if any pair of the index, mask, or destination\n     * registers are identical.  We don't bother trying to detect that.\n     */\n    {OP_vpgatherdd,0x66389018,\"vpgatherdd\",Vx,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpgatherdq,0x66389058,\"vpgatherdq\",Vx,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 67 */\n    {OP_vpgatherqd,0x66389118,\"vpgatherqd\",Vx,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpgatherqq,0x66389158,\"vpgatherqq\",Vx,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 68 */\n    {OP_vgatherdps,0x66389218,\"vgatherdps\",Vvs,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vgatherdpd,0x66389258,\"vgatherdpd\",Vvd,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 69 */\n    {OP_vgatherqps,0x66389318,\"vgatherqps\",Vvs,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vgatherqpd,0x66389358,\"vgatherqpd\",Vvd,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 70 */\n    {OP_vpmaskmovd,0x66388c18,\"vpmaskmovd\",Vx,xx,Hx,Mx,xx, mrm|vex|reqp|predcx,x,tvexw[71][0]},\n    {OP_vpmaskmovq,0x66388c58,\"vpmaskmovq\",Vx,xx,Hx,Mx,xx, mrm|vex|reqp|predcx,x,tvexw[71][1]},\n  }, { /* vex_W_ext 71 */\n    /* Conditional store => predcx */\n    {OP_vpmaskmovd,0x66388e18,\"vpmaskmovd\",Mx,xx,Vx,Hx,xx, mrm|vex|reqp|predcx,x,END_LIST},\n    {OP_vpmaskmovq,0x66388e58,\"vpmaskmovq\",Mx,xx,Vx,Hx,xx, mrm|vex|reqp|predcx,x,END_LIST},\n  }, { /* vex_W_ext 72 */\n    {OP_vpsrlvd,0x66384518,\"vpsrlvd\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpsrlvq,0x66384558,\"vpsrlvq\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 73 */\n    {OP_vpsllvd,0x66384718,\"vpsllvd\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpsllvq,0x66384758,\"vpsllvq\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 74 */\n    {OP_kmovw,0x0f9010,\"kmovw\",KPw,xx,KQw,xx,xx, mrm|vex,x,tvexw[76][0]},\n    {OP_kmovq,0x0f9050,\"kmovq\",KPq,xx,KQq,xx,xx, mrm|vex,x,tvexw[76][1]},\n  }, { /* vex_W_ext 75 */\n    {OP_kmovb,0x660f9010,\"kmovb\",KPb,xx,KQb,xx,xx, mrm|vex,x,tvexw[77][0]},\n    {OP_kmovd,0x660f9050,\"kmovd\",KPd,xx,KQd,xx,xx, mrm|vex,x,tvexw[77][1]},\n  }, { /* vex_W_ext 76 */\n    {OP_kmovw,0x0f9110,\"kmovw\",KQw,xx,KPw,xx,xx, mrm|vex,x,tvexw[78][0]},\n    {OP_kmovq,0x0f9150,\"kmovq\",KQq,xx,KPq,xx,xx, mrm|vex,x,tvexw[106][1]},\n  }, { /* vex_W_ext 77 */\n    {OP_kmovb,0x660f9110,\"kmovb\",KQb,xx,KPb,xx,xx, mrm|vex,x,tvexw[79][0]},\n    {OP_kmovd,0x660f9150,\"kmovd\",KQd,xx,KPd,xx,xx, mrm|vex,x,tvexw[106][0]},\n  }, { /* vex_W_ext 78 */\n    {OP_kmovw,0x0f9210,\"kmovw\",KPw,xx,Ry,xx,xx, mrm|vex,x,tvexw[80][0]},\n    {INVALID, 0x0f9250,\"(bad)\", xx,xx,xx,xx,xx,      no,x,NA},\n  }, { /* vex_W_ext 79 */\n    {OP_kmovb,0x660f9210,\"kmovb\",KPb,xx,Ry,xx,xx, mrm|vex,x,tvexw[81][0]},\n    {INVALID, 0x660f9250,\"(bad)\", xx,xx,xx,xx,xx,      no,x,NA},\n  }, { /* vex_W_ext 80 */\n    {OP_kmovw,0x0f9310,\"kmovw\",  Gd,xx,KRw,xx,xx, mrm|vex,x,END_LIST},\n    {INVALID, 0x0f9450,\"(bad)\", xx,xx,xx,xx,xx,        no,x,NA},\n  }, { /* vex_W_ext 81 */\n    {OP_kmovb,0x660f9310,\"kmovb\",Gd,xx,KRb,xx,xx, mrm|vex,x,END_LIST},\n    {INVALID, 0x660f9350,\"(bad)\",xx,xx,xx,xx,xx,       no,x,NA},\n  }, { /* vex_W_ext 82 */\n    {OP_kandw,0x0f4110,\"kandw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kandq,0x0f4150,\"kandq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 83 */\n    {OP_kandb,0x660f4110,\"kandb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kandd,0x660f4150,\"kandd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 84 */\n    {OP_kandnw,0x0f4210,\"kandnw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kandnq,0x0f4250,\"kandnq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 85 */\n    {OP_kandnb,0x660f4210,\"kandnb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kandnd,0x660f4250,\"kandnd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 86 */\n    {OP_kunpckwd,0x0f4b10,\"kunpckwd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n    {OP_kunpckdq,0x0f4b50,\"kunpckdq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 87 */\n    {OP_kunpckbw,0x660f4b10,\"kunpckbw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {INVALID,    0x660f4b50,   \"(bad)\", xx,xx, xx,  xx,xx,     no,x,NA},\n  }, { /* vex_W_ext 88 */\n    {OP_knotw,0x0f4410,\"knotw\",KPw,xx,KRw,xx,xx, mrm|vex,x,END_LIST},\n    {OP_knotq,0x0f4450,\"knotq\",KPq,xx,KRq,xx,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 89 */\n    {OP_knotb,0x660f4410,\"knotb\",KPb,xx,KRb,xx,xx, mrm|vex,x,END_LIST},\n    {OP_knotd,0x660f4450,\"knotd\",KPd,xx,KRd,xx,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 90 */\n    {OP_korw,0x0f4510,\"korw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_korq,0x0f4550,\"korq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 91 */\n    {OP_korb,0x660f4510,\"korb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kord,0x660f4550,\"kord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 92 */\n    {OP_kxnorw,0x0f4610,\"kxnorw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kxnorq,0x0f4650,\"kxnorq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 93 */\n    {OP_kxnorb,0x660f4610,\"kxnorb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kxnord,0x660f4650,\"kxnord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 94 */\n    {OP_kxorw,0x0f4710,\"kxorw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kxorq,0x0f4750,\"kxorq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 95 */\n    {OP_kxorb,0x660f4710,\"kxorb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kxord,0x660f4750,\"kxord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 96 */\n    {OP_kaddw,0x0f4a10,\"kaddw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kaddq,0x0f4a50,\"kaddq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 97 */\n    {OP_kaddb,0x660f4a10,\"kaddb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kaddd,0x660f4a50,\"kaddd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 98 */\n    {OP_kortestw,0x0f9810,\"kortestw\",KPw,xx,KRw,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n    {OP_kortestq,0x0f9850,\"kortestq\",KPq,xx,KRq,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n  }, { /* vex_W_ext 99 */\n    {OP_kortestb,0x660f9810,\"kortestb\",KPb,xx,KRb,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n    {OP_kortestd,0x660f9850,\"kortestd\",KPd,xx,KRd,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n  }, { /* vex_W_ext 100 */\n    {OP_kshiftlb,0x663a3208,\"kshiftlb\",KPb,xx,KRb,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftlw,0x663a3248,\"kshiftlw\",KPw,xx,KRw,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 101 */\n    {OP_kshiftld,0x663a3308,\"kshiftld\",KPd,xx,KRd,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftlq,0x663a3348,\"kshiftlq\",KPq,xx,KRq,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 102 */\n    {OP_kshiftrb,0x663a3008,\"kshiftrb\",KPb,xx,KRb,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftrw,0x663a3048,\"kshiftrw\",KPw,xx,KRw,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 103 */\n    {OP_kshiftrd,0x663a3108,\"kshiftrd\",KPd,xx,KRd,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftrq,0x663a3148,\"kshiftrq\",KPq,xx,KRq,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 104 */\n    {OP_ktestw,0x0f9910,\"ktestw\",KPw,xx,KRw,xx,xx, mrm|vex,fW6,END_LIST},\n    {OP_ktestq,0x0f9950,\"ktestq\",KPq,xx,KRq,xx,xx, mrm|vex,fW6,END_LIST},\n  }, { /* vex_W_ext 105 */\n    {OP_ktestb,0x660f9910,\"ktestb\",KPb,xx,KRb,xx,xx, mrm|vex,fW6,END_LIST},\n    {OP_ktestd,0x660f9950,\"ktestd\",KPd,xx,KRd,xx,xx, mrm|vex,fW6,END_LIST},\n  }, { /* vex_W_ext 106 */\n    {OP_kmovd,0xf20f9210,\"kmovd\",KPd,xx,Ry,xx,xx, mrm|vex,x,tvexw[107][0]},\n    {OP_kmovq,0xf20f9250,\"kmovq\",KPq,xx,Ry,xx,xx, mrm|vex,x,tvexw[107][1]},\n  }, { /* vex_W_ext 107 */\n    {OP_kmovd,0xf20f9310,\"kmovd\",  Gd,xx,KRd,xx,xx, mrm|vex,x,END_LIST},\n    {OP_kmovq,0xf20f9350,\"kmovq\",Gd_q,xx,KRq,xx,xx, mrm|vex,x,END_LIST},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on evex.W.\n * Index is evex.W value\n */\nconst instr_info_t evex_W_extensions[][2] = {\n  {    /* evex_W_ext 0 */\n    {OP_vmovups, 0x0f1010,\"vmovups\", Ves,xx,KEd,Wes,xx,mrm|evex,x,tevexw[1][0]},\n    {INVALID, 0x0f1050,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 1 */\n    {OP_vmovups, 0x0f1110,\"vmovups\", Wes,xx,KEd,Ves,xx,mrm|evex,x,END_LIST},\n    {INVALID, 0x0f1150,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 2 */\n    {INVALID, 0x660f1010,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovupd, 0x660f1050,\"vmovupd\", Ved,xx,KEd,Wed,xx,mrm|evex,x,tevexw[3][1]},\n  },\n  {    /* evex_W_ext 3 */\n    {INVALID, 0x660f1110,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovupd, 0x660f1150,\"vmovupd\", Wed,xx,KEd,Ved,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 4 */\n    {OP_vmovaps, 0x0f2810,\"vmovaps\", Ves,xx,KEd,Wes,xx,mrm|evex,x,tevexw[5][0]},\n    {INVALID, 0x0f2850,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 5 */\n    {OP_vmovaps, 0x0f2910,\"vmovaps\", Wes,xx,KEd,Ves,xx,mrm|evex,x,END_LIST},\n    {INVALID, 0x0f2950,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 6 */\n    {INVALID, 0x660f2810,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovapd, 0x660f2850,\"vmovapd\", Ved,xx,KEd,Wed,xx,mrm|evex,x,tevexw[7][1]},\n  },\n  {    /* evex_W_ext 7 */\n    {INVALID, 0x660f2910,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovapd, 0x660f2950,\"vmovapd\", Wed,xx,KEd,Ved,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 8 */\n    {OP_vmovdqa32, 0x660f6f10,\"vmovdqa32\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[9][0]},\n    {OP_vmovdqa64, 0x660f6f50,\"vmovdqa64\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[9][1]},\n  },\n  {    /* evex_W_ext 9 */\n    {OP_vmovdqa32, 0x660f7f10,\"vmovdqa32\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n    {OP_vmovdqa64, 0x660f7f50,\"vmovdqa64\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 10 */\n    {OP_vmovdqu8, 0xf20f6f10,\"vmovdqu8\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[12][0]},\n    {OP_vmovdqu16, 0xf20f6f50,\"vmovdqu16\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[12][1]},\n  },\n  {    /* evex_W_ext 11 */\n    {OP_vmovdqu32, 0xf30f6f10,\"vmovdqu32\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[13][0]},\n    {OP_vmovdqu64, 0xf30f6f50,\"vmovdqu64\",Ve,xx,KEw,We,xx,mrm|evex,x,tevexw[13][1]},\n  },\n  {    /* evex_W_ext 12 */\n    {OP_vmovdqu8, 0xf20f7f10,\"vmovdqu8\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n    {OP_vmovdqu16, 0xf20f7f50,\"vmovdqu16\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 13 */\n    {OP_vmovdqu32, 0xf30f7f10,\"vmovdqu32\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n    {OP_vmovdqu64, 0xf30f7f50,\"vmovdqu64\",We,xx,KEw,Ve,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 14 */\n    {OP_vmovlps, 0x0f1210, \"vmovlps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|evex|reqL0|reqLL0, x, tevexw[15][0]}, /*\"vmovhlps\" if reg-reg */\n    {INVALID, 0x0f1250,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 15 */\n    {OP_vmovlps, 0x0f1310, \"vmovlps\", Mq, xx, Vq_dq, xx, xx, mrm|evex, x, END_LIST},\n    {INVALID, 0x0f1350,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 16 */\n    {INVALID, 0x660f1210,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovlpd, 0x660f1250, \"vmovlpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|evex|reqL0|reqLL0, x, tevexw[17][1]},\n  },\n  {    /* evex_W_ext 17 */\n    {INVALID, 0x660f1310,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovlpd, 0x660f1350, \"vmovlpd\", Mq, xx, Vq_dq, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 18 */\n    {OP_vmovsldup,0xf30f1210, \"vmovsldup\", Ves, xx, KEw, Wes, xx, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f1250,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 19 */\n    {INVALID, 0xf20f1210,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovddup, 0xf20f1250, \"vmovddup\", Ved, xx, KEb, Wh_e, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 20 */\n    {OP_vmovhps, 0x0f1610, \"vmovhps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|evex|reqL0|reqLL0, x, tevexw[21][0]}, /*\"vmovlhps\" if reg-reg */\n    {INVALID, 0x0f1650,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 21 */\n    {OP_vmovhps, 0x0f1710, \"vmovhps\", Mq, xx, Vq_dq, xx, xx, mrm|evex|reqL0|reqLL0, x, END_LIST},\n    {INVALID, 0x0f1750,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 22 */\n    {INVALID, 0x660f1610,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovhpd, 0x660f1650, \"vmovhpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|evex|reqL0|reqLL0, x, tevexw[23][1]},\n  },\n  {    /* evex_W_ext 23 */\n    {INVALID, 0x660f1710,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovhpd, 0x660f1750, \"vmovhpd\", Mq, xx, Vq_dq, xx, xx, mrm|evex|reqL0|reqLL0, x, END_LIST},\n  },\n  {    /* evex_W_ext 24 */\n    {OP_vmovshdup, 0xf30f1610, \"vmovshdup\", Ves, xx, KEw, Wes, xx, mrm|evex, x, END_LIST},\n    {INVALID, 0xf30f1650,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 25 */\n    {OP_vunpcklps, 0x0f1410, \"vunpcklps\", Ves, xx, KEw, Hh_e, Wh_e, mrm|evex, x, END_LIST},\n    {INVALID, 0x0f1450,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 26 */\n    {INVALID, 0x660f1410,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vunpcklpd, 0x660f1450, \"vunpcklpd\", Ved, xx, KEb, Hh_e, Wh_e, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 27 */\n    {OP_vunpckhps, 0x0f1510, \"vunpckhps\", Ves, xx, KEw, Hh_e, Wh_e, mrm|evex, x, END_LIST},\n    {INVALID, 0x0f1550,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 28 */\n    {INVALID, 0x660f1510,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vunpckhpd, 0x660f1550, \"vunpckhpd\", Ved, xx, KEb, Hh_e, Wh_e, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 29 */\n    {OP_vcvtss2si, 0xf30f2d10, \"vcvtss2si\", Gd_q, xx, Wss, xx, xx, mrm|evex, x, tevexw[29][1]},\n    {OP_vcvtss2si, 0xf30f2d50, \"vcvtss2si\", Gd_q, xx, Wss, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 30 */\n    {OP_vcvtsd2si, 0xf20f2d10, \"vcvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|evex, x, tevexw[30][1]},\n    {OP_vcvtsd2si, 0xf20f2d50, \"vcvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 31 */\n    {OP_vcvtsi2ss, 0xf30f2a10, \"vcvtsi2ss\", Vdq, xx, H12_dq, Ed_q, xx, mrm|evex, x, tevexw[31][1]},\n    {OP_vcvtsi2ss, 0xf30f2a50, \"vcvtsi2ss\", Vdq, xx, H12_dq, Ed_q, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 32 */\n    {OP_vcvtsi2sd, 0xf20f2a10, \"vcvtsi2sd\", Vdq, xx, Hsd, Ed_q, xx, mrm|evex, x, tevexw[32][1]},\n    {OP_vcvtsi2sd, 0xf20f2a50, \"vcvtsi2sd\", Vdq, xx, Hsd, Ed_q, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 33 */\n    {OP_vmovntps, 0x0f2b10, \"vmovntps\", Mes, xx, Ves, xx, xx, mrm|evex, x, END_LIST},\n    {INVALID, 0x0f2b50,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 34 */\n    {INVALID, 0x660f2b10,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovntpd, 0x660f2b50, \"vmovntpd\", Med, xx, Ved, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 35 */\n    {OP_vcvttss2si, 0xf30f2c10, \"vcvttss2si\", Gd_q, xx, Wss, xx, xx, mrm|evex, x, tevexw[35][1]},\n    {OP_vcvttss2si, 0xf30f2c50, \"vcvttss2si\", Gd_q, xx, Wss, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 36 */\n    {OP_vcvttsd2si, 0xf20f2c10, \"vcvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|evex, x, tevexw[36][1]},\n    {OP_vcvttsd2si, 0xf20f2c50, \"vcvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 37 */\n    {OP_vucomiss, 0x0f2e10, \"vucomiss\", xx, xx, Vss, Wss, xx, mrm|evex, fW6, END_LIST},\n    {INVALID, 0x0f2e50,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 38 */\n    {INVALID, 0x660f2e10,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vucomisd, 0x660f2e50, \"vucomisd\", xx, xx, Vsd, Wsd, xx, mrm|evex, fW6, END_LIST},\n  },\n  {    /* evex_W_ext 39 */\n    {OP_vcomiss,  0x0f2f10, \"vcomiss\",  xx, xx, Vss, Wss, xx, mrm|evex, fW6, END_LIST},\n    {INVALID, 0x0f2f50,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 40 */\n    {INVALID, 0x660f2e10,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vcomisd,  0x660f2f50, \"vcomisd\",  xx, xx, Vsd, Wsd, xx, mrm|evex, fW6, END_LIST},\n  },\n  {    /* evex_W_ext 41 */\n    {OP_vpandd, 0x660fdb10, \"vpandd\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {OP_vpandq, 0x660fdb50, \"vpandq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 42 */\n    {OP_vpandnd, 0x660fdf10, \"vpandnd\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {OP_vpandnq, 0x660fdf50, \"vpandnq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 43 */\n    {OP_vpord, 0x660feb10, \"vpord\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {OP_vporq, 0x660feb50, \"vporq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 44 */\n    {OP_vpxord, 0x660fef10, \"vpxord\", Ve, xx, KEw, He, We, mrm|evex, x, END_LIST},\n    {OP_vpxorq, 0x660fef50, \"vpxorq\", Ve, xx, KEb, He, We, mrm|evex, x, END_LIST},\n  },\n  {    /* evex_W_ext 45 */\n    {OP_vpmulld,  0x66384018, \"vpmulld\",   Ve, xx, KEw,He,We, mrm|evex|reqp, x, END_LIST},\n    {OP_vpmullq,  0x66384058, \"vpmullq\",   Ve, xx, KEb,He,We, mrm|evex|reqp, x, END_LIST},\n  },\n};\n\n/****************************************************************************\n * XOP instructions\n *\n * Since large parts of the opcode space are empty, we save space by having\n * tables of 256 indices instead of tables of 256 instr_info_t structs.\n */\n/* N.B.: all XOP 0x08 are assumed to have an immediate.  If this becomes\n * untrue we'll have to add an xop_8_extra[] table in decode_fast.c.\n */\nconst byte xop_8_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 1, 2, 3,  0, 0, 0, 0,  0, 0, 4, 5,  /* 8 */\n     0, 0, 0, 0,  0, 6, 7, 8,  0, 0, 0, 0,  0, 0, 9,10,  /* 9 */\n     0, 0,11,12,  0, 0,13, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0,14, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n    15,16,17,18,  0, 0, 0, 0,  0, 0, 0, 0, 19,20,21,22,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 23,24,25,26,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst byte xop_9_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0,58,59, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n     0, 0,61, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n    27,28,29,30,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n    31,32,33,34, 35,36,37,38, 39,40,41,42,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0,43,44,45,  0, 0,46,47,  0, 0, 0,48,  0, 0, 0, 0,  /* C */\n     0,49,50,51,  0, 0,52,53,  0, 0, 0,54,  0, 0, 0, 0,  /* D */\n     0,55,56,57,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\n/* N.B.: nothing here for initial XOP but upcoming TBM and LWP have opcodes here */\nconst byte xop_a_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n    60, 0,62, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\n\nconst instr_info_t xop_extensions[] = {\n  {INVALID,     0x000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},              /* 0*/\n  /* We are out of flags, and we want to share a lot of REQUIRES_VEX, so to\n   * distinguish XOP we just rely on the XOP.map_select being disjoint from\n   * the VEX.m-mmm field.\n   */\n  /* XOP.map_select = 0x08 */\n  {OP_vpmacssww, 0x088518,\"vpmacssww\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 1*/\n  {OP_vpmacsswd, 0x088618,\"vpmacsswd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 2*/\n  {OP_vpmacssdql,0x088718,\"vpmacssdql\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 3*/\n  {OP_vpmacssdd, 0x088e18,\"vpmacssdd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 4*/\n  {OP_vpmacssdqh,0x088f18,\"vpmacssdqh\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 5*/\n  {OP_vpmacsww,  0x089518,\"vpmacsww\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 6*/\n  {OP_vpmacswd,  0x089618,\"vpmacswd\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 7*/\n  {OP_vpmacsdql, 0x089718,\"vpmacsdql\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 8*/\n  {OP_vpmacsdd,  0x089e18,\"vpmacsdd\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 9*/\n  {OP_vpmacsdqh, 0x089f18,\"vpmacsdqh\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*10*/\n  {VEX_W_EXT,    0x08a218, \"(vex_W ext 50)\", xx,xx,xx,xx,xx, mrm|vex, x,  50},  /*11*/\n  {VEX_W_EXT,    0x08a318, \"(vex_W ext 51)\", xx,xx,xx,xx,xx, mrm|vex, x,  51},  /*12*/\n  {OP_vpmadcsswd,0x08a618,\"vpmadcsswd\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*13*/\n  {OP_vpmadcswd, 0x08b618,\"vpmadcswd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*14*/\n  {OP_vprotb,    0x08c018,\"vprotb\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[52][0]},/*15*/\n  {OP_vprotw,    0x08c118,\"vprotw\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[53][0]},/*16*/\n  {OP_vprotd,    0x08c218,\"vprotd\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[54][0]},/*17*/\n  {OP_vprotq,    0x08c318,\"vprotq\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[55][0]},/*18*/\n  {OP_vpcomb,    0x08cc18,\"vpcomb\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*19*/\n  {OP_vpcomw,    0x08cd18,\"vpcomw\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*20*/\n  {OP_vpcomd,    0x08ce18,\"vpcomd\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*21*/\n  {OP_vpcomq,    0x08cf18,\"vpcomq\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*22*/\n  {OP_vpcomub,   0x08ec18,\"vpcomub\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*23*/\n  {OP_vpcomuw,   0x08ed18,\"vpcomuw\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*24*/\n  {OP_vpcomud,   0x08ee18,\"vpcomud\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*25*/\n  {OP_vpcomuq,   0x08ef18,\"vpcomuq\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*26*/\n  /* XOP.map_select = 0x09 */\n  {OP_vfrczps,   0x098018,\"vfrczps\",   Vvs,xx,Wvs,xx,xx,mrm|vex,x,END_LIST},    /*27*/\n  {OP_vfrczpd,   0x098118,\"vfrczpd\",   Vvs,xx,Wvs,xx,xx,mrm|vex,x,END_LIST},    /*28*/\n  {OP_vfrczss,   0x098218,\"vfrczss\",   Vss,xx,Wss,xx,xx,mrm|vex,x,END_LIST},    /*29*/\n  {OP_vfrczsd,   0x098318,\"vfrczsd\",   Vsd,xx,Wsd,xx,xx,mrm|vex,x,END_LIST},    /*30*/\n  {VEX_W_EXT,    0x099018, \"(vex_W ext 52)\", xx,xx,xx,xx,xx, mrm|vex, x,  52},  /*31*/\n  {VEX_W_EXT,    0x099118, \"(vex_W ext 53)\", xx,xx,xx,xx,xx, mrm|vex, x,  53},  /*32*/\n  {VEX_W_EXT,    0x099218, \"(vex_W ext 54)\", xx,xx,xx,xx,xx, mrm|vex, x,  54},  /*33*/\n  {VEX_W_EXT,    0x099318, \"(vex_W ext 55)\", xx,xx,xx,xx,xx, mrm|vex, x,  55},  /*34*/\n  {VEX_W_EXT,    0x099418, \"(vex_W ext 56)\", xx,xx,xx,xx,xx, mrm|vex, x,  56},  /*35*/\n  {VEX_W_EXT,    0x099518, \"(vex_W ext 57)\", xx,xx,xx,xx,xx, mrm|vex, x,  57},  /*36*/\n  {VEX_W_EXT,    0x099618, \"(vex_W ext 58)\", xx,xx,xx,xx,xx, mrm|vex, x,  58},  /*37*/\n  {VEX_W_EXT,    0x099718, \"(vex_W ext 59)\", xx,xx,xx,xx,xx, mrm|vex, x,  59},  /*38*/\n  {VEX_W_EXT,    0x099818, \"(vex_W ext 60)\", xx,xx,xx,xx,xx, mrm|vex, x,  60},  /*39*/\n  {VEX_W_EXT,    0x099918, \"(vex_W ext 61)\", xx,xx,xx,xx,xx, mrm|vex, x,  61},  /*40*/\n  {VEX_W_EXT,    0x099a18, \"(vex_W ext 62)\", xx,xx,xx,xx,xx, mrm|vex, x,  62},  /*41*/\n  {VEX_W_EXT,    0x099b18, \"(vex_W ext 63)\", xx,xx,xx,xx,xx, mrm|vex, x,  63},  /*42*/\n  {OP_vphaddbw,  0x09c118,\"vphaddbw\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*43*/\n  {OP_vphaddbd,  0x09c218,\"vphaddbd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*44*/\n  {OP_vphaddbq,  0x09c318,\"vphaddbq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*45*/\n  {OP_vphaddwd,  0x09c618,\"vphaddwd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*46*/\n  {OP_vphaddwq,  0x09c718,\"vphaddwq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*47*/\n  {OP_vphadddq,  0x09cb18,\"vphadddq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*48*/\n  /* AMD decode table erroneously lists this as \"vphaddubwd\" */\n  {OP_vphaddubw, 0x09d118,\"vphaddubw\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*49*/\n  {OP_vphaddubd, 0x09d218,\"vphaddubd\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*50*/\n  {OP_vphaddubq, 0x09d318,\"vphaddubq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*51*/\n  {OP_vphadduwd, 0x09d618,\"vphadduwd\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*52*/\n  {OP_vphadduwq, 0x09d718,\"vphadduwq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*53*/\n  {OP_vphaddudq, 0x09db18,\"vphaddudq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*54*/\n  {OP_vphsubbw,  0x09e118,\"vphsubbw\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*55*/\n  {OP_vphsubwd,  0x09e218,\"vphsubwd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*56*/\n  {OP_vphsubdq,  0x09e318,\"vphsubdq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*57*/\n  {EXTENSION,    0x090118, \"(XOP group 1)\", xx,xx, xx,xx,xx, mrm|vex, x, 27},   /*58*/\n  {EXTENSION,    0x090218, \"(XOP group 2)\", xx,xx, xx,xx,xx, mrm|vex, x, 28},   /*59*/\n  /* XOP.map_select = 0x0a */\n  {OP_bextr,     0x0a1018, \"bextr\",  Gy,xx,Ey,Id,xx, mrm|vex, fW6, END_LIST},   /*60*/\n  /* Later-added instrs, from various tables */\n  {EXTENSION,    0x091218, \"(XOP group 3)\", xx,xx, xx,xx,xx, mrm|vex, x, 29},   /*61*/\n  {EXTENSION,    0x0a1218, \"(XOP group 4)\", xx,xx, xx,xx,xx, mrm|vex, x, 30},   /*62*/\n};\n\n/****************************************************************************\n * String instructions that differ depending on rep/repne prefix\n *\n * Note that Intel manuals prior to May 2011 claim that for x64 the count\n * register for ins and outs is rcx by default, but for all other rep* is ecx.\n * The AMD manual, and experimental evidence, contradicts this and has rcx\n * as the default count register for all rep*.\n * Furthermore, the Intel manual implies that w/o rex.w edi/esi are used\n * rather than rdi/rsi: which again the AMD manual and experimental\n * evidence contradict.\n */\nconst instr_info_t rep_extensions[][4] = {\n    /* FIXME: ins and outs access \"I/O ports\", are these memory addresses?\n     * if so, change Ib to Ob and change dx to i_dx (move to dest for outs)\n     */\n  { /* rep extension 0 */\n    {OP_ins,      0x6c0000, \"ins\",       Yb, axDI, dx, axDI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_ins,  0xf36c0000, \"rep ins\", Yb, axDI, dx, axDI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf36c0000, \"rep ins\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 1 */\n    {OP_ins,      0x6d0000, \"ins\",       Yz, axDI, dx, axDI, xx, no, fRD, tre[0][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_ins,  0xf36d0000, \"rep ins\", Yz, axDI, dx, axDI, axCX, xop_next, fRD, tre[0][2]},\n    {OP_CONTD,  0xf36d0000, \"rep ins\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 2 */\n    {OP_outs,      0x6e0000, \"outs\",       axSI, xx, Xb, dx, axSI, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_outs,  0xf36e0000, \"rep outs\", axSI, axCX, Xb, dx, axSI, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf36e0000, \"rep outs\", xx, xx, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 3 */\n    {OP_outs,      0x6f0000, \"outs\",       axSI, xx, Xz, dx, axSI, no, fRD, tre[2][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_outs,  0xf36f0000, \"rep outs\", axSI, axCX, Xz, dx, axSI, xop_next, fRD, tre[2][2]},\n    {OP_CONTD,  0xf36f0000, \"rep outs\", xx, xx, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 4 */\n    {OP_movs,      0xa40000, \"movs\",       Yb, axSI, Xb, axSI, axDI, xop_next, fRD, END_LIST},\n    {OP_CONTD,      0xa40000, \"movs\",       axDI, xx, xx, xx, xx, no, fRD, END_LIST},\n    {OP_rep_movs,  0xf3a40000, \"rep movs\", Yb, axSI, Xb, axSI, axDI, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3a40000, \"rep movs\", axDI, axCX, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 5 */\n    {OP_movs,      0xa50000, \"movs\",       Yv, axSI, Xv, axSI, axDI, xop_next, fRD, tre[4][0]},\n    {OP_CONTD,      0xa50000, \"movs\",       axDI, xx, xx, xx, xx, no, fRD, END_LIST},\n    {OP_rep_movs,  0xf3a50000, \"rep movs\", Yv, axSI, Xv, axSI, axDI, xop_next, fRD, tre[4][2]},\n    {OP_CONTD,  0xf3a50000, \"rep movs\", axDI, axCX, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 6 */\n    {OP_stos,      0xaa0000, \"stos\",       Yb, axDI, al, axDI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_stos,  0xf3aa0000, \"rep stos\", Yb, axDI, al, axDI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3aa0000, \"rep stos\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 7 */\n    {OP_stos,      0xab0000, \"stos\",       Yv, axDI, eAX, axDI, xx, no, fRD, tre[6][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_stos,  0xf3ab0000, \"rep stos\", Yv, axDI, eAX, axDI, axCX, xop_next, fRD, tre[6][2]},\n    {OP_CONTD,  0xf3ab0000, \"rep stos\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 8 */\n    {OP_lods,      0xac0000, \"lods\",       al, axSI, Xb, axSI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_lods,  0xf3ac0000, \"rep lods\", al, axSI, Xb, axSI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3ac0000, \"rep lods\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 9 */\n    {OP_lods,      0xad0000, \"lods\",       eAX, axSI, Xv, axSI, xx, no, fRD, tre[8][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_lods,  0xf3ad0000, \"rep lods\", eAX, axSI, Xv, axSI, axCX, xop_next, fRD, tre[8][2]},\n    {OP_CONTD,  0xf3ad0000, \"rep lods\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n};\n\nconst instr_info_t repne_extensions[][6] = {\n  { /* repne extension 0 */\n    {OP_cmps,       0xa60000, \"cmps\",         axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD), END_LIST},\n    {OP_CONTD,      0xa60000, \"cmps\",         xx, xx, axDI, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_rep_cmps,   0xf3a60000, \"rep cmps\",   axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf3a60000, \"rep cmps\",   axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_cmps, 0xf2a60000, \"repne cmps\", axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf2a60000, \"repne cmps\", axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 1 */\n    {OP_cmps,       0xa70000, \"cmps\",         axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD), tne[0][0]},\n    {OP_CONTD,      0xa70000, \"cmps\",         xx, xx, axDI, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_rep_cmps,   0xf3a70000, \"rep cmps\",   axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD|fRZ), tne[0][2]},\n    {OP_CONTD,      0xf3a70000, \"rep cmps\",   axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_cmps, 0xf2a70000, \"repne cmps\", axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD|fRZ), tne[0][4]},\n    {OP_CONTD,      0xf2a70000, \"repne cmps\", axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 2 */\n    {OP_scas,       0xae0000, \"scas\",         axDI, xx, Yb, al, axDI, no, (fW6|fRD), END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_scas,   0xf3ae0000, \"rep scas\",   axDI, axCX, Yb, al, axDI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf3ae0000, \"rep scas\",   xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_scas, 0xf2ae0000, \"repne scas\", axDI, axCX, Yb, al, axDI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf2ae0000, \"repne scas\", xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 3 */\n    {OP_scas,       0xaf0000, \"scas\",         axDI, xx, Yv, eAX, axDI, no, (fW6|fRD), tne[2][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_scas,   0xf3af0000, \"rep scas\",   axDI, axCX, Yv, eAX, axDI, xop_next, (fW6|fRD|fRZ), tne[2][2]},\n    {OP_CONTD,      0xf3af0000, \"rep scas\",   xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_scas, 0xf2af0000, \"repne scas\", axDI, axCX, Yv, eAX, axDI, xop_next, (fW6|fRD|fRZ), tne[2][4]},\n    {OP_CONTD,      0xf2af0000, \"repne scas\", xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n  }\n};\n\n/****************************************************************************\n * Float instructions with ModR/M from 0x00 to 0xbf\n * This is from Tables A-7, A-9, A-11, A-13, A-15, A-17, A-19, A-21\n * I've added my own symbol '+' to indicate a float, and:\n *   'x' to indicate extended real (80 bits)\n *   'y' to indicate 14/28 byte value in memory\n *   'z' to indicate 98/108 byte value in memory\n */\n/* FIXME: I ignore fp stack changes, should we model that? */\nconst instr_info_t float_low_modrm[] = {\n  /* d8 */\n  {OP_fadd,  0xd80020, \"fadd\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x20]}, /* 00 */\n  {OP_fmul,  0xd80021, \"fmul\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x21]},\n  {OP_fcom,  0xd80022, \"fcom\",  xx, xx, Fd, st0, xx, mrm, x, tfl[0x22]},\n  {OP_fcomp, 0xd80023, \"fcomp\", xx, xx, Fd, st0, xx, mrm, x, tfl[0x23]},\n  {OP_fsub,  0xd80024, \"fsub\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x24]},\n  {OP_fsubr, 0xd80025, \"fsubr\", st0, xx, Fd, st0, xx, mrm, x, tfl[0x25]},\n  {OP_fdiv,  0xd80026, \"fdiv\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x26]},\n  {OP_fdivr, 0xd80027, \"fdivr\", st0, xx, Fd, st0, xx, mrm, x, tfl[0x27]},\n  /*  d9 */\n  {OP_fld,    0xd90020, \"fld\",    st0, xx, Fd, xx, xx, mrm, x, tfl[0x1d]}, /* 08 */\n  {INVALID,   0xd90021, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fst,    0xd90022, \"fst\",    Fd, xx, st0, xx, xx, mrm, x, tfl[0x2a]},\n  {OP_fstp,   0xd90023, \"fstp\",   Fd, xx, st0, xx, xx, mrm, x, tfl[0x1f]},\n  {OP_fldenv, 0xd90024, \"fldenv\", xx, xx, Fy, xx, xx, mrm, x, END_LIST},\n  {OP_fldcw,  0xd90025, \"fldcw\",  xx, xx, Fw, xx, xx, mrm, x, END_LIST},\n  {OP_fnstenv, 0xd90026, \"fnstenv\", Fy, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, this is \"fstenv\"*/\n  {OP_fnstcw,  0xd90027, \"fnstcw\",  Fw, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, this is \"fstcw\"*/\n  /* da */\n  {OP_fiadd,  0xda0020, \"fiadd\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x30]}, /* 10 */\n  {OP_fimul,  0xda0021, \"fimul\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x31]},\n  {OP_ficom,  0xda0022, \"ficom\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x32]},\n  {OP_ficomp, 0xda0023, \"ficomp\", st0, xx, Md, st0, xx, mrm, x, tfl[0x33]},\n  {OP_fisub,  0xda0024, \"fisub\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x34]},\n  {OP_fisubr, 0xda0025, \"fisubr\", st0, xx, Md, st0, xx, mrm, x, tfl[0x35]},\n  {OP_fidiv,  0xda0026, \"fidiv\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x36]},\n  {OP_fidivr, 0xda0027, \"fidivr\", st0, xx, Md, st0, xx, mrm, x, tfl[0x37]},\n  /* db */\n  {OP_fild,  0xdb0020, \"fild\",  st0, xx, Md, xx, xx, mrm, x, tfl[0x38]}, /* 18 */\n  {OP_fisttp, 0xdb0021, \"fisttp\",  Md, xx, st0, xx, xx, no, x, tfl[0x39]},\n  {OP_fist,  0xdb0022, \"fist\",  Md, xx, st0, xx, xx, mrm, x, tfl[0x3a]},\n  {OP_fistp, 0xdb0023, \"fistp\", Md, xx, st0, xx, xx, mrm, x, tfl[0x3b]},\n  {INVALID,  0xdb0024, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fld,   0xdb0025, \"fld\",   st0, xx, Fx, xx, xx, mrm, x, tfl[0x28]},\n  {INVALID,  0xdb0026, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fstp,  0xdb0027, \"fstp\",  Fx, xx, st0, xx, xx, mrm, x, tfl[0x2b]},\n  /* dc */\n  {OP_fadd,  0xdc0020, \"fadd\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x00]}, /* 20 */\n  {OP_fmul,  0xdc0021, \"fmul\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x08]},\n  {OP_fcom,  0xdc0022, \"fcom\",  xx, xx, Fq, st0, xx, mrm, x, tfh[0][0x10]},\n  {OP_fcomp, 0xdc0023, \"fcomp\", xx, xx, Fq, st0, xx, mrm, x, tfh[0][0x18]},\n  {OP_fsub,  0xdc0024, \"fsub\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x20]},\n  {OP_fsubr, 0xdc0025, \"fsubr\", st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x28]},\n  {OP_fdiv,  0xdc0026, \"fdiv\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x30]},\n  {OP_fdivr, 0xdc0027, \"fdivr\", st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x38]},\n  /* dd */\n  {OP_fld,   0xdd0020, \"fld\",    st0, xx, Fq, xx, xx, mrm, x, tfh[1][0x00]}, /* 28 */\n  {OP_fisttp, 0xdd0021, \"fisttp\",  Mq, xx, st0, xx, xx, no, x, tfl[0x19]},\n  {OP_fst,   0xdd0022, \"fst\",    Fq, xx, st0, xx, xx, mrm, x, tfh[5][0x10]},\n  {OP_fstp,  0xdd0023, \"fstp\",   Fq, xx, st0, xx, xx, mrm, x, tfh[5][0x18]},\n  {OP_frstor,0xdd0024, \"frstor\", xx, xx, Fz, xx, xx, mrm, x, END_LIST},\n  {INVALID,  0xdd0025, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fnsave, 0xdd0026, \"fnsave\",  Fz, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME:w/ preceding fwait instr, this is \"fsave\"*/\n  {OP_fnstsw, 0xdd0027, \"fnstsw\",  Fw, xx, xx, xx, xx, mrm, x, tfh[7][0x20]},/*FIXME:w/ preceding fwait instr, this is \"fstsw\"*/\n  /* de */\n  {OP_fiadd,  0xde0020, \"fiadd\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST}, /* 30 */\n  {OP_fimul,  0xde0021, \"fimul\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_ficom,  0xde0022, \"ficom\",  xx, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_ficomp, 0xde0023, \"ficomp\", xx, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fisub,  0xde0024, \"fisub\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fisubr, 0xde0025, \"fisubr\", st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fidiv,  0xde0026, \"fidiv\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fidivr, 0xde0027, \"fidivr\", st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  /* df */\n  {OP_fild,   0xdf0020, \"fild\",    st0, xx, Fw, xx, xx, mrm, x, tfl[0x3d]}, /* 38 */\n  {OP_fisttp, 0xdf0021, \"fisttp\",  Mw, xx, st0, xx, xx, no, x, END_LIST},\n  {OP_fist,   0xdf0022, \"fist\",    Fw, xx, st0, xx, xx, mrm, x, END_LIST},\n  {OP_fistp,  0xdf0023, \"fistp\",   Fw, xx, st0, xx, xx, mrm, x, tfl[0x3f]},\n  {OP_fbld,   0xdf0024, \"fbld\",    st0, xx, Fx, xx, xx, mrm, x, END_LIST},\n  {OP_fild,   0xdf0025, \"fild\",    st0, xx, Fq, xx, xx, mrm, x, END_LIST},\n  {OP_fbstp,  0xdf0026, \"fbstp\",   Fx, xx, st0, xx, xx, mrm, x, END_LIST},\n  {OP_fistp,  0xdf0027, \"fistp\",   Fq, xx, st0, xx, xx, mrm, x, END_LIST},\n};\n\n/****************************************************************************\n * Float instructions with ModR/M above 0xbf\n * This is from Tables A-8, A-10, A-12, A-14, A-16, A-18, A-20, A-22\n */\nconst instr_info_t float_high_modrm[][64] = {\n    { /* d8 = [0] */\n        {OP_fadd, 0xd8c010, \"fadd\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x01]}, /* c0 = [0x00] */\n        {OP_fadd, 0xd8c110, \"fadd\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x02]},\n        {OP_fadd, 0xd8c210, \"fadd\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x03]},\n        {OP_fadd, 0xd8c310, \"fadd\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x04]},\n        {OP_fadd, 0xd8c410, \"fadd\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x05]},\n        {OP_fadd, 0xd8c510, \"fadd\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x06]},\n        {OP_fadd, 0xd8c610, \"fadd\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x07]},\n        {OP_fadd, 0xd8c710, \"fadd\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x00]},\n        {OP_fmul, 0xd8c810, \"fmul\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x09]}, /* c8 = [0x08] */\n        {OP_fmul, 0xd8c910, \"fmul\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x0a]},\n        {OP_fmul, 0xd8ca10, \"fmul\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x0b]},\n        {OP_fmul, 0xd8cb10, \"fmul\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x0c]},\n        {OP_fmul, 0xd8cc10, \"fmul\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x0d]},\n        {OP_fmul, 0xd8cd10, \"fmul\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x0e]},\n        {OP_fmul, 0xd8ce10, \"fmul\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x0f]},\n        {OP_fmul, 0xd8cf10, \"fmul\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x08]},\n        {OP_fcom, 0xd8d010, \"fcom\", xx, xx, st0, st0, xx, mrm, x, tfh[0][0x11]}, /* d0 = [0x10] */\n        {OP_fcom, 0xd8d110, \"fcom\", xx, xx, st0, st1, xx, mrm, x, tfh[0][0x12]},\n        {OP_fcom, 0xd8d210, \"fcom\", xx, xx, st0, st2, xx, mrm, x, tfh[0][0x13]},\n        {OP_fcom, 0xd8d310, \"fcom\", xx, xx, st0, st3, xx, mrm, x, tfh[0][0x14]},\n        {OP_fcom, 0xd8d410, \"fcom\", xx, xx, st0, st4, xx, mrm, x, tfh[0][0x15]},\n        {OP_fcom, 0xd8d510, \"fcom\", xx, xx, st0, st5, xx, mrm, x, tfh[0][0x16]},\n        {OP_fcom, 0xd8d610, \"fcom\", xx, xx, st0, st6, xx, mrm, x, tfh[0][0x17]},\n        {OP_fcom, 0xd8d710, \"fcom\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xd8d810, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, tfh[0][0x19]}, /* d8 = [0x18] */\n        {OP_fcomp, 0xd8d910, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, tfh[0][0x1a]},\n        {OP_fcomp, 0xd8da10, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, tfh[0][0x1b]},\n        {OP_fcomp, 0xd8db10, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, tfh[0][0x1c]},\n        {OP_fcomp, 0xd8dc10, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, tfh[0][0x1d]},\n        {OP_fcomp, 0xd8dd10, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, tfh[0][0x1e]},\n        {OP_fcomp, 0xd8de10, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, tfh[0][0x1f]},\n        {OP_fcomp, 0xd8df10, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsub, 0xd8e010, \"fsub\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x21]}, /* e0 = [0x20] */\n        {OP_fsub, 0xd8e110, \"fsub\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x22]},\n        {OP_fsub, 0xd8e210, \"fsub\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x23]},\n        {OP_fsub, 0xd8e310, \"fsub\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x24]},\n        {OP_fsub, 0xd8e410, \"fsub\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x25]},\n        {OP_fsub, 0xd8e510, \"fsub\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x26]},\n        {OP_fsub, 0xd8e610, \"fsub\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x27]},\n        {OP_fsub, 0xd8e710, \"fsub\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x28]},\n        {OP_fsubr, 0xd8e810, \"fsubr\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x29]}, /* e8 = [0x28] */\n        {OP_fsubr, 0xd8e910, \"fsubr\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x2a]},\n        {OP_fsubr, 0xd8ea10, \"fsubr\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x2b]},\n        {OP_fsubr, 0xd8eb10, \"fsubr\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x2c]},\n        {OP_fsubr, 0xd8ec10, \"fsubr\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x2d]},\n        {OP_fsubr, 0xd8ed10, \"fsubr\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x2e]},\n        {OP_fsubr, 0xd8ee10, \"fsubr\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x2f]},\n        {OP_fsubr, 0xd8ef10, \"fsubr\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x20]},\n        {OP_fdiv, 0xd8f010, \"fdiv\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x31]}, /* f0 = [0x30] */\n        {OP_fdiv, 0xd8f110, \"fdiv\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x32]},\n        {OP_fdiv, 0xd8f210, \"fdiv\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x33]},\n        {OP_fdiv, 0xd8f310, \"fdiv\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x34]},\n        {OP_fdiv, 0xd8f410, \"fdiv\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x35]},\n        {OP_fdiv, 0xd8f510, \"fdiv\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x36]},\n        {OP_fdiv, 0xd8f610, \"fdiv\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x37]},\n        {OP_fdiv, 0xd8f710, \"fdiv\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x38]},\n        {OP_fdivr, 0xd8f810, \"fdivr\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x39]}, /* f8 = [0x38] */\n        {OP_fdivr, 0xd8f910, \"fdivr\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x3a]},\n        {OP_fdivr, 0xd8fa10, \"fdivr\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x3b]},\n        {OP_fdivr, 0xd8fb10, \"fdivr\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x3c]},\n        {OP_fdivr, 0xd8fc10, \"fdivr\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x3d]},\n        {OP_fdivr, 0xd8fd10, \"fdivr\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x3e]},\n        {OP_fdivr, 0xd8fe10, \"fdivr\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x3f]},\n        {OP_fdivr, 0xd8ff10, \"fdivr\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x30]},\n   },\n    { /* d9 = [1] */\n        {OP_fld, 0xd9c010, \"fld\", st0, xx, st0, xx, xx, mrm, x, tfh[1][0x01]}, /* c0 = [0x00] */\n        {OP_fld, 0xd9c110, \"fld\", st0, xx, st1, xx, xx, mrm, x, tfh[1][0x02]},\n        {OP_fld, 0xd9c210, \"fld\", st0, xx, st2, xx, xx, mrm, x, tfh[1][0x03]},\n        {OP_fld, 0xd9c310, \"fld\", st0, xx, st3, xx, xx, mrm, x, tfh[1][0x04]},\n        {OP_fld, 0xd9c410, \"fld\", st0, xx, st4, xx, xx, mrm, x, tfh[1][0x05]},\n        {OP_fld, 0xd9c510, \"fld\", st0, xx, st5, xx, xx, mrm, x, tfh[1][0x06]},\n        {OP_fld, 0xd9c610, \"fld\", st0, xx, st6, xx, xx, mrm, x, tfh[1][0x07]},\n        {OP_fld, 0xd9c710, \"fld\", st0, xx, st7, xx, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xd9c810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, tfh[1][0x09]}, /* c8 = [0x08] */\n        {OP_fxch, 0xd9c910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, tfh[1][0x0a]},\n        {OP_fxch, 0xd9ca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, tfh[1][0x0b]},\n        {OP_fxch, 0xd9cb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, tfh[1][0x0c]},\n        {OP_fxch, 0xd9cc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, tfh[1][0x0d]},\n        {OP_fxch, 0xd9cd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, tfh[1][0x0e]},\n        {OP_fxch, 0xd9ce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, tfh[1][0x0f]},\n        {OP_fxch, 0xd9cf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fnop, 0xd9d010, \"fnop\", xx, xx, xx, xx, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {INVALID, 0xd9d110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        /* Undocumented.  On sandpile.org as \"fstp1\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xd9d810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fstp, 0xd9d910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9da10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9db10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9dc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9dd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9de10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9df10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fchs,   0xd9e010, \"fchs\",   st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* e0 = [0x20] */\n        {OP_fabs,   0xd9e110, \"fabs\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9e210, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID,   0xd9e310, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_ftst,   0xd9e410, \"ftst\",   st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fxam,   0xd9e510, \"fxam\",   xx, xx, st0, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9e610, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID,   0xd9e710, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fld1,   0xd9e810, \"fld1\",   st0, xx, cF, xx, xx, mrm, x, END_LIST}, /* e8 = [0x28] */\n        {OP_fldl2t, 0xd9e910, \"fldl2t\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldl2e, 0xd9ea10, \"fldl2e\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldpi,  0xd9eb10, \"fldpi\",  st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldlg2, 0xd9ec10, \"fldlg2\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldln2, 0xd9ed10, \"fldln2\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldz,   0xd9ee10, \"fldz\",   st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9ef10, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_f2xm1,  0xd9f010, \"f2xm1\",  st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* f0 = [0x30] */\n        {OP_fyl2x,  0xd9f110, \"fyl2x\",  st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fptan,  0xd9f210, \"fptan\",  st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fpatan, 0xd9f310, \"fpatan\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxtract,0xd9f410, \"fxtract\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fprem1, 0xd9f510, \"fprem1\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fdecstp,0xd9f610, \"fdecstp\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n        {OP_fincstp,0xd9f710, \"fincstp\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n        {OP_fprem,  0xd9f810, \"fprem\",  st0, st1, st0, st1, xx, mrm, x, END_LIST}, /* f8 = [0x38] */\n        {OP_fyl2xp1,0xd9f910, \"fyl2xp1\",st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fsqrt,  0xd9fa10, \"fsqrt\",  st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fsincos,0xd9fb10, \"fsincos\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_frndint,0xd9fc10, \"frndint\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fscale, 0xd9fd10, \"fscale\", st0, xx, st1, st0, xx, mrm, x, END_LIST},\n        {OP_fsin,   0xd9fe10, \"fsin\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fcos,   0xd9ff10, \"fcos\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n   },\n    { /* da = [2] */\n        {OP_fcmovb, 0xdac010, \"fcmovb\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x01]}, /* c0 = [0x00] */\n        {OP_fcmovb, 0xdac110, \"fcmovb\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x02]},\n        {OP_fcmovb, 0xdac210, \"fcmovb\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x03]},\n        {OP_fcmovb, 0xdac310, \"fcmovb\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x04]},\n        {OP_fcmovb, 0xdac410, \"fcmovb\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x05]},\n        {OP_fcmovb, 0xdac510, \"fcmovb\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x06]},\n        {OP_fcmovb, 0xdac610, \"fcmovb\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x07]},\n        {OP_fcmovb, 0xdac710, \"fcmovb\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmove, 0xdac810, \"fcmove\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x09]}, /* c8 = [0x08] */\n        {OP_fcmove, 0xdac910, \"fcmove\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0a]},\n        {OP_fcmove, 0xdaca10, \"fcmove\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0b]},\n        {OP_fcmove, 0xdacb10, \"fcmove\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0c]},\n        {OP_fcmove, 0xdacc10, \"fcmove\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0d]},\n        {OP_fcmove, 0xdacd10, \"fcmove\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0e]},\n        {OP_fcmove, 0xdace10, \"fcmove\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0f]},\n        {OP_fcmove, 0xdacf10, \"fcmove\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovbe, 0xdad010, \"fcmovbe\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x11]}, /* d0 = [0x10] */\n        {OP_fcmovbe, 0xdad110, \"fcmovbe\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x12]},\n        {OP_fcmovbe, 0xdad210, \"fcmovbe\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x13]},\n        {OP_fcmovbe, 0xdad310, \"fcmovbe\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x14]},\n        {OP_fcmovbe, 0xdad410, \"fcmovbe\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x15]},\n        {OP_fcmovbe, 0xdad510, \"fcmovbe\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x16]},\n        {OP_fcmovbe, 0xdad610, \"fcmovbe\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x17]},\n        {OP_fcmovbe, 0xdad710, \"fcmovbe\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovu, 0xdad810, \"fcmovu\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x19]}, /* d8 = [0x18] */\n        {OP_fcmovu, 0xdad910, \"fcmovu\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1a]},\n        {OP_fcmovu, 0xdada10, \"fcmovu\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1b]},\n        {OP_fcmovu, 0xdadb10, \"fcmovu\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1c]},\n        {OP_fcmovu, 0xdadc10, \"fcmovu\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1d]},\n        {OP_fcmovu, 0xdadd10, \"fcmovu\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1e]},\n        {OP_fcmovu, 0xdade10, \"fcmovu\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1f]},\n        {OP_fcmovu, 0xdadf10, \"fcmovu\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {INVALID, 0xdae010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e0 = [0x20] */\n        {INVALID, 0xdae110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e8 = [0x28] */\n        {OP_fucompp, 0xdae910, \"fucompp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {INVALID, 0xdaea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaeb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f0 = [0x30] */\n        {INVALID, 0xdaf110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdaf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* db = [3] */\n        {OP_fcmovnb, 0xdbc010, \"fcmovnb\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x01]}, /* c0 = [0x00] */\n        {OP_fcmovnb, 0xdbc110, \"fcmovnb\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x02]},\n        {OP_fcmovnb, 0xdbc210, \"fcmovnb\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x03]},\n        {OP_fcmovnb, 0xdbc310, \"fcmovnb\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x04]},\n        {OP_fcmovnb, 0xdbc410, \"fcmovnb\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x05]},\n        {OP_fcmovnb, 0xdbc510, \"fcmovnb\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x06]},\n        {OP_fcmovnb, 0xdbc610, \"fcmovnb\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x07]},\n        {OP_fcmovnb, 0xdbc710, \"fcmovnb\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovne, 0xdbc810, \"fcmovne\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x09]}, /* c8 = [0x08] */\n        {OP_fcmovne, 0xdbc910, \"fcmovne\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0a]},\n        {OP_fcmovne, 0xdbca10, \"fcmovne\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0b]},\n        {OP_fcmovne, 0xdbcb10, \"fcmovne\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0c]},\n        {OP_fcmovne, 0xdbcc10, \"fcmovne\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0d]},\n        {OP_fcmovne, 0xdbcd10, \"fcmovne\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0e]},\n        {OP_fcmovne, 0xdbce10, \"fcmovne\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0f]},\n        {OP_fcmovne, 0xdbcf10, \"fcmovne\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovnbe, 0xdbd010, \"fcmovnbe\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x12]}, /* d0 = [0x10] */\n        {OP_fcmovnbe, 0xdbd110, \"fcmovnbe\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x12]},\n        {OP_fcmovnbe, 0xdbd210, \"fcmovnbe\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x13]},\n        {OP_fcmovnbe, 0xdbd310, \"fcmovnbe\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x14]},\n        {OP_fcmovnbe, 0xdbd410, \"fcmovnbe\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x15]},\n        {OP_fcmovnbe, 0xdbd510, \"fcmovnbe\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x16]},\n        {OP_fcmovnbe, 0xdbd610, \"fcmovnbe\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x17]},\n        {OP_fcmovnbe, 0xdbd710, \"fcmovnbe\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovnu, 0xdbd810, \"fcmovnu\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x19]}, /* d8 = [0x18] */\n        {OP_fcmovnu, 0xdbd910, \"fcmovnu\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1a]},\n        {OP_fcmovnu, 0xdbda10, \"fcmovnu\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1b]},\n        {OP_fcmovnu, 0xdbdb10, \"fcmovnu\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1c]},\n        {OP_fcmovnu, 0xdbdc10, \"fcmovnu\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1d]},\n        {OP_fcmovnu, 0xdbdd10, \"fcmovnu\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1e]},\n        {OP_fcmovnu, 0xdbde10, \"fcmovnu\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1f]},\n        {OP_fcmovnu, 0xdbdf10, \"fcmovnu\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {INVALID, 0xdbe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e0 = [0x20] */\n        {INVALID, 0xdbe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fnclex, 0xdbe210, \"fnclex\", xx, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, called \"fclex\"*/\n        {OP_fninit, 0xdbe310, \"fninit\", xx, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, called \"finit\"*/\n        {INVALID, 0xdbe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fucomi, 0xdbe810, \"fucomi\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x29]}, /* e8 = [0x28] */\n        {OP_fucomi, 0xdbe910, \"fucomi\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2a]},\n        {OP_fucomi, 0xdbea10, \"fucomi\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2b]},\n        {OP_fucomi, 0xdbeb10, \"fucomi\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2c]},\n        {OP_fucomi, 0xdbec10, \"fucomi\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2d]},\n        {OP_fucomi, 0xdbed10, \"fucomi\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2e]},\n        {OP_fucomi, 0xdbee10, \"fucomi\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2f]},\n        {OP_fucomi, 0xdbef10, \"fucomi\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {OP_fcomi, 0xdbf010, \"fcomi\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x31]}, /* f0 = [0x30] */\n        {OP_fcomi, 0xdbf110, \"fcomi\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x32]},\n        {OP_fcomi, 0xdbf210, \"fcomi\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x33]},\n        {OP_fcomi, 0xdbf310, \"fcomi\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x34]},\n        {OP_fcomi, 0xdbf410, \"fcomi\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x35]},\n        {OP_fcomi, 0xdbf510, \"fcomi\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x36]},\n        {OP_fcomi, 0xdbf610, \"fcomi\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x37]},\n        {OP_fcomi, 0xdbf710, \"fcomi\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {INVALID, 0xdbf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdbf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* dc = [4] */\n        {OP_fadd, 0xdcc010, \"fadd\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x01]}, /* c0 = [0x00] */\n        {OP_fadd, 0xdcc110, \"fadd\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x02]},\n        {OP_fadd, 0xdcc210, \"fadd\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x03]},\n        {OP_fadd, 0xdcc310, \"fadd\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x04]},\n        {OP_fadd, 0xdcc410, \"fadd\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x05]},\n        {OP_fadd, 0xdcc510, \"fadd\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x06]},\n        {OP_fadd, 0xdcc610, \"fadd\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x07]},\n        {OP_fadd, 0xdcc710, \"fadd\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fmul, 0xdcc810, \"fmul\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x09]}, /* c8 = [0x08] */\n        {OP_fmul, 0xdcc910, \"fmul\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x0a]},\n        {OP_fmul, 0xdcca10, \"fmul\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x0b]},\n        {OP_fmul, 0xdccb10, \"fmul\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x0c]},\n        {OP_fmul, 0xdccc10, \"fmul\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x0d]},\n        {OP_fmul, 0xdccd10, \"fmul\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x0e]},\n        {OP_fmul, 0xdcce10, \"fmul\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x0f]},\n        {OP_fmul, 0xdccf10, \"fmul\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcom2\".  We assume an alias for fcom\n         * and do not include in the encode chain.\n         */\n        {OP_fcom, 0xdcd010, \"fcom\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fcom, 0xdcd110, \"fcom\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd210, \"fcom\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd310, \"fcom\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd410, \"fcom\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd510, \"fcom\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd610, \"fcom\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd710, \"fcom\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcomp3\".  We assume an alias for fcomp\n         * and do not include in the encode chain.\n         */\n        {OP_fcomp, 0xdcd810, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fcomp, 0xdcd910, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcda10, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdb10, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdc10, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdd10, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcde10, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdf10, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsubr, 0xdce010, \"fsubr\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x21]}, /* e0 = [0x20] */\n        {OP_fsubr, 0xdce110, \"fsubr\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x22]},\n        {OP_fsubr, 0xdce210, \"fsubr\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x23]},\n        {OP_fsubr, 0xdce310, \"fsubr\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x24]},\n        {OP_fsubr, 0xdce410, \"fsubr\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x25]},\n        {OP_fsubr, 0xdce510, \"fsubr\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x26]},\n        {OP_fsubr, 0xdce610, \"fsubr\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x27]},\n        {OP_fsubr, 0xdce710, \"fsubr\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsub, 0xdce810, \"fsub\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x29]}, /* e8 = [0x28] */\n        {OP_fsub, 0xdce910, \"fsub\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x2a]},\n        {OP_fsub, 0xdcea10, \"fsub\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x2b]},\n        {OP_fsub, 0xdceb10, \"fsub\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x2c]},\n        {OP_fsub, 0xdcec10, \"fsub\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x2d]},\n        {OP_fsub, 0xdced10, \"fsub\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x2e]},\n        {OP_fsub, 0xdcee10, \"fsub\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x2f]},\n        {OP_fsub, 0xdcef10, \"fsub\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivr, 0xdcf010, \"fdivr\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x31]}, /* f0 = [0x30] */\n        {OP_fdivr, 0xdcf110, \"fdivr\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x32]},\n        {OP_fdivr, 0xdcf210, \"fdivr\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x33]},\n        {OP_fdivr, 0xdcf310, \"fdivr\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x34]},\n        {OP_fdivr, 0xdcf410, \"fdivr\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x35]},\n        {OP_fdivr, 0xdcf510, \"fdivr\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x36]},\n        {OP_fdivr, 0xdcf610, \"fdivr\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x37]},\n        {OP_fdivr, 0xdcf710, \"fdivr\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdiv, 0xdcf810, \"fdiv\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x39]}, /* f8 = [0x38] */\n        {OP_fdiv, 0xdcf910, \"fdiv\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x3a]},\n        {OP_fdiv, 0xdcfa10, \"fdiv\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x3b]},\n        {OP_fdiv, 0xdcfb10, \"fdiv\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x3c]},\n        {OP_fdiv, 0xdcfc10, \"fdiv\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x3d]},\n        {OP_fdiv, 0xdcfd10, \"fdiv\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x3e]},\n        {OP_fdiv, 0xdcfe10, \"fdiv\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x3f]},\n        {OP_fdiv, 0xdcff10, \"fdiv\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n   },\n    { /* dd = [5] */\n        {OP_ffree, 0xddc010, \"ffree\", st0, xx, xx, xx, xx, mrm, x, tfh[5][0x01]}, /* c0 = [0x00] */\n        {OP_ffree, 0xddc110, \"ffree\", st1, xx, xx, xx, xx, mrm, x, tfh[5][0x02]},\n        {OP_ffree, 0xddc210, \"ffree\", st2, xx, xx, xx, xx, mrm, x, tfh[5][0x03]},\n        {OP_ffree, 0xddc310, \"ffree\", st3, xx, xx, xx, xx, mrm, x, tfh[5][0x04]},\n        {OP_ffree, 0xddc410, \"ffree\", st4, xx, xx, xx, xx, mrm, x, tfh[5][0x05]},\n        {OP_ffree, 0xddc510, \"ffree\", st5, xx, xx, xx, xx, mrm, x, tfh[5][0x06]},\n        {OP_ffree, 0xddc610, \"ffree\", st6, xx, xx, xx, xx, mrm, x, tfh[5][0x07]},\n        {OP_ffree, 0xddc710, \"ffree\", st7, xx, xx, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fxch4\".  We assume an alias for fxch\n         * and do not include in the encode chain.\n         */\n        {OP_fxch, 0xddc810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, END_LIST}, /* c8 = [0x08] */\n        {OP_fxch, 0xddc910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fst, 0xddd010, \"fst\", st0, xx, st0, xx, xx, mrm, x, tfh[5][0x11]}, /* d0 = [0x10] */\n        {OP_fst, 0xddd110, \"fst\", st1, xx, st0, xx, xx, mrm, x, tfh[5][0x12]},\n        {OP_fst, 0xddd210, \"fst\", st2, xx, st0, xx, xx, mrm, x, tfh[5][0x13]},\n        {OP_fst, 0xddd310, \"fst\", st3, xx, st0, xx, xx, mrm, x, tfh[5][0x14]},\n        {OP_fst, 0xddd410, \"fst\", st4, xx, st0, xx, xx, mrm, x, tfh[5][0x15]},\n        {OP_fst, 0xddd510, \"fst\", st5, xx, st0, xx, xx, mrm, x, tfh[5][0x16]},\n        {OP_fst, 0xddd610, \"fst\", st6, xx, st0, xx, xx, mrm, x, tfh[5][0x17]},\n        {OP_fst, 0xddd710, \"fst\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xddd810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, tfh[5][0x19]}, /* d8 = [0x18] */\n        {OP_fstp, 0xddd910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, tfh[5][0x1a]},\n        {OP_fstp, 0xddda10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, tfh[5][0x1b]},\n        {OP_fstp, 0xdddb10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, tfh[5][0x1c]},\n        {OP_fstp, 0xdddc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, tfh[5][0x1d]},\n        {OP_fstp, 0xdddd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, tfh[5][0x1e]},\n        {OP_fstp, 0xddde10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, tfh[5][0x1f]},\n        {OP_fstp, 0xdddf10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fucom, 0xdde010, \"fucom\", xx, xx, st0, st0, xx, mrm, x, tfh[5][0x21]}, /* e0 = [0x20] */\n        {OP_fucom, 0xdde110, \"fucom\", xx, xx, st1, st0, xx, mrm, x, tfh[5][0x22]},\n        {OP_fucom, 0xdde210, \"fucom\", xx, xx, st2, st0, xx, mrm, x, tfh[5][0x23]},\n        {OP_fucom, 0xdde310, \"fucom\", xx, xx, st3, st0, xx, mrm, x, tfh[5][0x24]},\n        {OP_fucom, 0xdde410, \"fucom\", xx, xx, st4, st0, xx, mrm, x, tfh[5][0x25]},\n        {OP_fucom, 0xdde510, \"fucom\", xx, xx, st5, st0, xx, mrm, x, tfh[5][0x26]},\n        {OP_fucom, 0xdde610, \"fucom\", xx, xx, st6, st0, xx, mrm, x, tfh[5][0x27]},\n        {OP_fucom, 0xdde710, \"fucom\", xx, xx, st7, st0, xx, mrm, x, END_LIST},\n        {OP_fucomp, 0xdde810, \"fucomp\", xx, xx, st0, st0, xx, mrm, x, tfh[5][0x29]}, /* e8 = [0x28] */\n        {OP_fucomp, 0xdde910, \"fucomp\", xx, xx, st1, st0, xx, mrm, x, tfh[5][0x2a]},\n        {OP_fucomp, 0xddea10, \"fucomp\", xx, xx, st2, st0, xx, mrm, x, tfh[5][0x2b]},\n        {OP_fucomp, 0xddeb10, \"fucomp\", xx, xx, st3, st0, xx, mrm, x, tfh[5][0x2c]},\n        {OP_fucomp, 0xddec10, \"fucomp\", xx, xx, st4, st0, xx, mrm, x, tfh[5][0x2d]},\n        {OP_fucomp, 0xdded10, \"fucomp\", xx, xx, st5, st0, xx, mrm, x, tfh[5][0x2e]},\n        {OP_fucomp, 0xddee10, \"fucomp\", xx, xx, st6, st0, xx, mrm, x, tfh[5][0x2f]},\n        {OP_fucomp, 0xddef10, \"fucomp\", xx, xx, st7, st0, xx, mrm, x, END_LIST},\n        {INVALID, 0xddf010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f0 = [0x30] */\n        {INVALID, 0xddf110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xddf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* de = [6]*/\n        {OP_faddp, 0xdec010, \"faddp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x01]}, /* c0 = [0x00] */\n        {OP_faddp, 0xdec110, \"faddp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x02]},\n        {OP_faddp, 0xdec210, \"faddp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x03]},\n        {OP_faddp, 0xdec310, \"faddp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x04]},\n        {OP_faddp, 0xdec410, \"faddp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x05]},\n        {OP_faddp, 0xdec510, \"faddp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x06]},\n        {OP_faddp, 0xdec610, \"faddp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x07]},\n        {OP_faddp, 0xdec710, \"faddp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fmulp, 0xdec810, \"fmulp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x09]}, /* c8 = [0x08] */\n        {OP_fmulp, 0xdec910, \"fmulp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x0a]},\n        {OP_fmulp, 0xdeca10, \"fmulp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x0b]},\n        {OP_fmulp, 0xdecb10, \"fmulp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x0c]},\n        {OP_fmulp, 0xdecc10, \"fmulp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x0d]},\n        {OP_fmulp, 0xdecd10, \"fmulp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x0e]},\n        {OP_fmulp, 0xdece10, \"fmulp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x0f]},\n        {OP_fmulp, 0xdecf10, \"fmulp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcomp5\".  We assume an alias for fcomp\n         * and do not include in the encode chain.\n         */\n        {OP_fcomp, 0xded010, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fcomp, 0xded110, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded210, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded310, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded410, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded510, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded610, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded710, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {INVALID, 0xded810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* d8 = [0x18] */\n        {OP_fcompp, 0xded910, \"fcompp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {INVALID, 0xdeda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdede10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fsubrp, 0xdee010, \"fsubrp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x21]}, /* e0 = [0x20] */\n        {OP_fsubrp, 0xdee110, \"fsubrp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x22]},\n        {OP_fsubrp, 0xdee210, \"fsubrp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x23]},\n        {OP_fsubrp, 0xdee310, \"fsubrp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x24]},\n        {OP_fsubrp, 0xdee410, \"fsubrp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x25]},\n        {OP_fsubrp, 0xdee510, \"fsubrp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x26]},\n        {OP_fsubrp, 0xdee610, \"fsubrp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x27]},\n        {OP_fsubrp, 0xdee710, \"fsubrp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsubp, 0xdee810, \"fsubp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x29]}, /* e8 = [0x28] */\n        {OP_fsubp, 0xdee910, \"fsubp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x2a]},\n        {OP_fsubp, 0xdeea10, \"fsubp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x2b]},\n        {OP_fsubp, 0xdeeb10, \"fsubp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x2c]},\n        {OP_fsubp, 0xdeec10, \"fsubp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x2d]},\n        {OP_fsubp, 0xdeed10, \"fsubp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x2e]},\n        {OP_fsubp, 0xdeee10, \"fsubp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x2f]},\n        {OP_fsubp, 0xdeef10, \"fsubp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivrp, 0xdef010, \"fdivrp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x31]}, /* f0 = [0x30] */\n        {OP_fdivrp, 0xdef110, \"fdivrp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x32]},\n        {OP_fdivrp, 0xdef210, \"fdivrp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x33]},\n        {OP_fdivrp, 0xdef310, \"fdivrp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x34]},\n        {OP_fdivrp, 0xdef410, \"fdivrp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x35]},\n        {OP_fdivrp, 0xdef510, \"fdivrp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x36]},\n        {OP_fdivrp, 0xdef610, \"fdivrp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x37]},\n        {OP_fdivrp, 0xdef710, \"fdivrp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivp, 0xdef810, \"fdivp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x39]}, /* f8 = [0x38] */\n        {OP_fdivp, 0xdef910, \"fdivp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x3a]},\n        {OP_fdivp, 0xdefa10, \"fdivp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x3b]},\n        {OP_fdivp, 0xdefb10, \"fdivp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x3c]},\n        {OP_fdivp, 0xdefc10, \"fdivp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x3d]},\n        {OP_fdivp, 0xdefd10, \"fdivp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x3e]},\n        {OP_fdivp, 0xdefe10, \"fdivp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x3f]},\n        {OP_fdivp, 0xdeff10, \"fdivp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n   },\n    { /* df = [7] */\n        /* Undocumented by Intel, but is on p152 of \"AMD Athlon\n         * Processor x86 Code Optimization Guide.\"\n         */\n        {OP_ffreep, 0xdfc010, \"ffreep\", st0, xx, xx, xx, xx, mrm, x, tfh[7][0x01]}, /* c0 = [0x00] */\n        {OP_ffreep, 0xdfc110, \"ffreep\", st1, xx, xx, xx, xx, mrm, x, tfh[7][0x02]},\n        {OP_ffreep, 0xdfc210, \"ffreep\", st2, xx, xx, xx, xx, mrm, x, tfh[7][0x03]},\n        {OP_ffreep, 0xdfc310, \"ffreep\", st3, xx, xx, xx, xx, mrm, x, tfh[7][0x04]},\n        {OP_ffreep, 0xdfc410, \"ffreep\", st4, xx, xx, xx, xx, mrm, x, tfh[7][0x05]},\n        {OP_ffreep, 0xdfc510, \"ffreep\", st5, xx, xx, xx, xx, mrm, x, tfh[7][0x06]},\n        {OP_ffreep, 0xdfc610, \"ffreep\", st6, xx, xx, xx, xx, mrm, x, tfh[7][0x07]},\n        {OP_ffreep, 0xdfc710, \"ffreep\", st7, xx, xx, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fxch7\".  We assume an alias for fxch\n         * and do not include in the encode chain.\n         */\n        {OP_fxch, 0xdfc810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, END_LIST}, /* c8 = [0x08] */\n        {OP_fxch, 0xdfc910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fstp8\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xdfd010, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fstp, 0xdfd110, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd210, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd310, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd410, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd510, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd610, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd710, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fstp9\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xdfd810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fstp, 0xdfd910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfda10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdb10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfde10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdf10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fnstsw, 0xdfe010, \"fnstsw\", ax, xx, xx, xx, xx, mrm, x, END_LIST}, /* e0 = [0x20] */ /*FIXME:w/ preceding fwait instr, this is \"fstsw\"*/\n        {INVALID, 0xdfe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fucomip, 0xdfe810, \"fucomip\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x29]}, /* e8 = [0x28] */\n        {OP_fucomip, 0xdfe910, \"fucomip\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2a]},\n        {OP_fucomip, 0xdfea10, \"fucomip\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2b]},\n        {OP_fucomip, 0xdfeb10, \"fucomip\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2c]},\n        {OP_fucomip, 0xdfec10, \"fucomip\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2d]},\n        {OP_fucomip, 0xdfed10, \"fucomip\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2e]},\n        {OP_fucomip, 0xdfee10, \"fucomip\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2f]},\n        {OP_fucomip, 0xdfef10, \"fucomip\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {OP_fcomip, 0xdff010, \"fcomip\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x31]}, /* f0 = [0x30] */\n        {OP_fcomip, 0xdff110, \"fcomip\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x32]},\n        {OP_fcomip, 0xdff210, \"fcomip\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x33]},\n        {OP_fcomip, 0xdff310, \"fcomip\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x34]},\n        {OP_fcomip, 0xdff410, \"fcomip\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x35]},\n        {OP_fcomip, 0xdff510, \"fcomip\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x36]},\n        {OP_fcomip, 0xdff610, \"fcomip\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x37]},\n        {OP_fcomip, 0xdff710, \"fcomip\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {INVALID, 0xdff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n};\n\n/****************************************************************************\n * Suffix extensions: 3DNow! and 3DNow!+\n * Since there are only 24 of them, we save space by having a\n * table of 256 indices instead of 256 instr_info_t structs.\n */\nconst byte suffix_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 20,18, 0, 0,  /* 0 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 21,19, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0,22, 0,  0, 0,23, 0,  /* 8 */\n     4, 0, 0, 0,  7, 0,10,13,  0, 0,16, 0,  0, 0, 2, 0,  /* 9 */\n     5, 0, 0, 0,  8, 0,11,14,  0, 0,17, 0,  0, 0, 3, 0,  /* A */\n     6, 0, 0, 0,  9, 0,12,15,  0, 0, 0,24,  0, 0, 0, 1,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst instr_info_t suffix_extensions[] = {\n    /* Rather than forging an exception let's anticipate future additions: we know\n     * (pretty sure anyway) that they'll have the same length and operand structure.\n     * Won't encode properly from Level 4 but that's ok.\n     */\n    {OP_unknown_3dnow, 0x000f0f90, \"unknown 3DNow\",\n                                          Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 0*/\n    {OP_pavgusb , 0xbf0f0f90, \"pavgusb\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 1*/\n    {OP_pfadd   , 0x9e0f0f90, \"pfadd\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 2*/\n    {OP_pfacc   , 0xae0f0f90, \"pfacc\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 3*/\n    {OP_pfcmpge , 0x900f0f90, \"pfcmpge\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 4*/\n    {OP_pfcmpgt , 0xa00f0f90, \"pfcmpgt\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 5*/\n    {OP_pfcmpeq , 0xb00f0f90, \"pfcmpeq\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 6*/\n    {OP_pfmin   , 0x940f0f90, \"pfmin\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 7*/\n    {OP_pfmax   , 0xa40f0f90, \"pfmax\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 8*/\n    {OP_pfmul   , 0xb40f0f90, \"pfmul\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 9*/\n    {OP_pfrcp   , 0x960f0f90, \"pfrcp\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*10*/\n    {OP_pfrcpit1, 0xa60f0f90, \"pfrcpit1\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*11*/\n    {OP_pfrcpit2, 0xb60f0f90, \"pfrcpit2\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*12*/\n    {OP_pfrsqrt , 0x970f0f90, \"pfrsqrt\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*13*/\n    {OP_pfrsqit1, 0xa70f0f90, \"pfrsqit1\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*14*/\n    {OP_pmulhrw , 0xb70f0f90, \"pmulhrw\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*15*/\n    {OP_pfsub   , 0x9a0f0f90, \"pfsub\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*16*/\n    {OP_pfsubr  , 0xaa0f0f90, \"pfsubr\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*17*/\n    {OP_pi2fd   , 0x0d0f0f90, \"pi2fd\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*18*/\n    {OP_pf2id   , 0x1d0f0f90, \"pf2id\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*19*/\n    {OP_pi2fw   , 0x0c0f0f90, \"pi2fw\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*20*/\n    {OP_pf2iw   , 0x1c0f0f90, \"pf2iw\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*21*/\n    {OP_pfnacc  , 0x8a0f0f90, \"pfnacc\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*22*/\n    {OP_pfpnacc , 0x8e0f0f90, \"pfpnacc\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*23*/\n    {OP_pswapd  , 0xbb0f0f90, \"pswapd\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*24*/\n};\n\n/****************************************************************************\n * To handle more than 2 dests or 3 sources we chain on extra instructions.\n * All cases where we have extra operands are single-encoding-only instructions,\n * so we use the list field to point to here.\n * N.B.: the size of this table is hardcoded in decode.c.\n * Also, only implicit operands are in these instruction extensions!!!\n */\nconst instr_info_t extra_operands[] =\n{\n    /* 0x00 */\n    {OP_CONTD, 0x000000, \"<pusha cont'd>\", xx, xx, eCX, eDX, eBP, xop, x, exop[0x01]},\n    {OP_CONTD, 0x000000, \"<pusha cont'd>\", xx, xx, eSI, eDI, xx, no, x, END_LIST},\n    /* 0x02 */\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eBX, eCX, xx, xx, xx, xop, x, exop[0x03]},\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eDX, eBP, xx, xx, xx, xop, x, exop[0x04]},\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eSI, eDI, xx, xx, xx, no, x, END_LIST},\n    /* 0x05 */\n    {OP_CONTD, 0x000000, \"<enter cont'd>\", xbp, xx, xbp, xx, xx, no, x, END_LIST},\n    /* 0x06 */\n    {OP_CONTD, 0x000000, \"<cpuid cont'd>\", ecx, edx, xx, xx, xx, no, x, END_LIST},\n    /* 0x07 */\n    {OP_CONTD, 0x000000, \"<cmpxchg8b cont'd>\", eDX, xx, eCX, eBX, xx, mrm, fWZ, END_LIST},\n    {OP_CONTD,0x663a6018, \"<pcmpestrm cont'd\", xx, xx, eax, edx, xx, mrm|reqp, fW6, END_LIST},\n    {OP_CONTD,0x663a6018, \"<pcmpestri cont'd\", xx, xx, eax, edx, xx, mrm|reqp, fW6, END_LIST},\n    /* 10 */\n    {OP_CONTD,0xf90f0177, \"<rdtscp cont'd>\", ecx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_CONTD,0x663a6018, \"<vpcmpestrm cont'd\", xx, xx, eax, edx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {OP_CONTD,0x663a6018, \"<vpcmpestri cont'd\", xx, xx, eax, edx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {OP_CONTD,0x0f3710, \"<getsec cont'd\", ecx, xx, xx, xx, xx, predcx, x, END_LIST},\n};\n\n/* clang-format on */\n", "idx": 10, "id": 17000, "msg": "", "proj": "DynamoRIO-dynamorio", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -160,7 +160,7 @@ class _InternalFrame(object):\n     |  3|  7| 11| 15| 19|\n     |  4|  8| 12| 16| 20|\n     +---+---+---+---+---+\n-    >>> internal.pandas_df\n+    >>> internal.to_pandas_frame\n        A  B   C   D   E\n     0  1  5   9  13  17\n     1  2  6  10  14  18", "y": 0, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nAn internal immutable DataFrame with some metadata to manage indexes.\n\"\"\"\nimport re\nfrom typing import Dict, List, Optional, Tuple, Union, TYPE_CHECKING\nfrom itertools import accumulate\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype, is_list_like\nfrom pyspark import sql as spark\nfrom pyspark._globals import _NoValue, _NoValueType\nfrom pyspark.sql import functions as F, Window\nfrom pyspark.sql.functions import PandasUDFType, pandas_udf\nfrom pyspark.sql.types import BooleanType, DataType, StructField, StructType, LongType\n\ntry:\n    from pyspark.sql.types import to_arrow_type\nexcept ImportError:\n    from pyspark.sql.pandas.types import to_arrow_type\n\nfrom databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.\n\nif TYPE_CHECKING:\n    # This is required in old Python 3.5 to prevent circular reference.\n    from databricks.koalas.series import Series\nfrom databricks.koalas.config import get_option\nfrom databricks.koalas.typedef import infer_pd_series_spark_type, spark_type_to_pandas_dtype\nfrom databricks.koalas.utils import (\n    column_labels_level,\n    default_session,\n    lazy_property,\n    name_like_string,\n    scol_for,\n    verify_temp_column_name,\n)\n\n\n# A function to turn given numbers to Spark columns that represent Koalas index.\nSPARK_INDEX_NAME_FORMAT = \"__index_level_{}__\".format\nSPARK_DEFAULT_INDEX_NAME = SPARK_INDEX_NAME_FORMAT(0)\n# A pattern to check if the name of a Spark column is a Koalas index name or not.\nSPARK_INDEX_NAME_PATTERN = re.compile(r\"__index_level_[0-9]+__\")\n\nNATURAL_ORDER_COLUMN_NAME = \"__natural_order__\"\n\nHIDDEN_COLUMNS = set([NATURAL_ORDER_COLUMN_NAME])\n\nIndexMap = Tuple[str, Optional[Tuple[str, ...]]]\n\n\nclass _InternalFrame(object):\n    \"\"\"\n    The internal immutable DataFrame which manages Spark DataFrame and column names and index\n    information.\n\n    .. note:: this is an internal class. It is not supposed to be exposed to users and users\n        should not directly access to it.\n\n    The internal immutable DataFrame represents the index information for a DataFrame it belongs to.\n    For instance, if we have a Koalas DataFrame as below, Pandas DataFrame does not store the index\n    as columns.\n\n    >>> kdf = ks.DataFrame({\n    ...     'A': [1, 2, 3, 4],\n    ...     'B': [5, 6, 7, 8],\n    ...     'C': [9, 10, 11, 12],\n    ...     'D': [13, 14, 15, 16],\n    ...     'E': [17, 18, 19, 20]}, columns = ['A', 'B', 'C', 'D', 'E'])\n    >>> kdf  # doctest: +NORMALIZE_WHITESPACE\n       A  B   C   D   E\n    0  1  5   9  13  17\n    1  2  6  10  14  18\n    2  3  7  11  15  19\n    3  4  8  12  16  20\n\n    However, all columns including index column are also stored in Spark DataFrame internally\n    as below.\n\n    >>> kdf._internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +-----------------+---+---+---+---+---+\n    |__index_level_0__|  A|  B|  C|  D|  E|\n    +-----------------+---+---+---+---+---+\n    |                0|  1|  5|  9| 13| 17|\n    |                1|  2|  6| 10| 14| 18|\n    |                2|  3|  7| 11| 15| 19|\n    |                3|  4|  8| 12| 16| 20|\n    +-----------------+---+---+---+---+---+\n\n    In order to fill this gap, the current metadata is used by mapping Spark's internal column\n    to Koalas' index. See the method below:\n\n    * `sdf` represents the internal Spark DataFrame\n\n    * `data_columns` represents non-indexing columns\n\n    * `index_columns` represents internal index columns\n\n    * `columns` represents all columns\n\n    * `index_names` represents the external index name\n\n    * `index_map` is zipped pairs of `index_columns` and `index_names`\n\n    * `spark_df` represents Spark DataFrame derived by the metadata\n\n    * `pandas_df` represents pandas DataFrame derived by the metadata\n\n    >>> internal = kdf._internal\n    >>> internal.sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    +-----------------+---+---+---+---+---+-----------------+\n    |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n    +-----------------+---+---+---+---+---+-----------------+\n    |                0|  1|  5|  9| 13| 17|...|\n    |                1|  2|  6| 10| 14| 18|...|\n    |                2|  3|  7| 11| 15| 19|...|\n    |                3|  4|  8| 12| 16| 20|...|\n    +-----------------+---+---+---+---+---+-----------------+\n    >>> internal.data_columns\n    ['A', 'B', 'C', 'D', 'E']\n    >>> internal.index_columns\n    ['__index_level_0__']\n    >>> internal.columns\n    ['__index_level_0__', 'A', 'B', 'C', 'D', 'E']\n    >>> internal.index_names\n    [None]\n    >>> internal.index_map\n    [('__index_level_0__', None)]\n    >>> internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +-----------------+---+---+---+---+---+\n    |__index_level_0__|  A|  B|  C|  D|  E|\n    +-----------------+---+---+---+---+---+\n    |                0|  1|  5|  9| 13| 17|\n    |                1|  2|  6| 10| 14| 18|\n    |                2|  3|  7| 11| 15| 19|\n    |                3|  4|  8| 12| 16| 20|\n    +-----------------+---+---+---+---+---+\n    >>> internal.spark_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+---+---+---+\n    |  A|  B|  C|  D|  E|\n    +---+---+---+---+---+\n    |  1|  5|  9| 13| 17|\n    |  2|  6| 10| 14| 18|\n    |  3|  7| 11| 15| 19|\n    |  4|  8| 12| 16| 20|\n    +---+---+---+---+---+\n    >>> internal.pandas_df\n       A  B   C   D   E\n    0  1  5   9  13  17\n    1  2  6  10  14  18\n    2  3  7  11  15  19\n    3  4  8  12  16  20\n\n    In case that index is set to one of the existing column as below:\n\n    >>> kdf1 = kdf.set_index(\"A\")\n    >>> kdf1  # doctest: +NORMALIZE_WHITESPACE\n       B   C   D   E\n    A\n    1  5   9  13  17\n    2  6  10  14  18\n    3  7  11  15  19\n    4  8  12  16  20\n\n    >>> kdf1._internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+---+---+---+\n    |  A|  B|  C|  D|  E|\n    +---+---+---+---+---+\n    |  1|  5|  9| 13| 17|\n    |  2|  6| 10| 14| 18|\n    |  3|  7| 11| 15| 19|\n    |  4|  8| 12| 16| 20|\n    +---+---+---+---+---+\n\n    >>> internal = kdf1._internal\n    >>> internal.sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    +-----------------+---+---+---+---+---+-----------------+\n    |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n    +-----------------+---+---+---+---+---+-----------------+\n    |                0|  1|  5|  9| 13| 17|...|\n    |                1|  2|  6| 10| 14| 18|...|\n    |                2|  3|  7| 11| 15| 19|...|\n    |                3|  4|  8| 12| 16| 20|...|\n    +-----------------+---+---+---+---+---+-----------------+\n    >>> internal.data_columns\n    ['B', 'C', 'D', 'E']\n    >>> internal.index_columns\n    ['A']\n    >>> internal.columns\n    ['A', 'B', 'C', 'D', 'E']\n    >>> internal.index_names\n    [('A',)]\n    >>> internal.index_map\n    [('A', ('A',))]\n    >>> internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+---+---+---+\n    |  A|  B|  C|  D|  E|\n    +---+---+---+---+---+\n    |  1|  5|  9| 13| 17|\n    |  2|  6| 10| 14| 18|\n    |  3|  7| 11| 15| 19|\n    |  4|  8| 12| 16| 20|\n    +---+---+---+---+---+\n    >>> internal.pandas_df  # doctest: +NORMALIZE_WHITESPACE\n       B   C   D   E\n    A\n    1  5   9  13  17\n    2  6  10  14  18\n    3  7  11  15  19\n    4  8  12  16  20\n\n    The `spark_df` will drop the index columns:\n\n    >>> internal.spark_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+---+---+\n    |  B|  C|  D|  E|\n    +---+---+---+---+\n    |  5|  9| 13| 17|\n    |  6| 10| 14| 18|\n    |  7| 11| 15| 19|\n    |  8| 12| 16| 20|\n    +---+---+---+---+\n\n    but if `drop=False`, the columns will still remain in `spark_df`:\n\n    >>> kdf.set_index(\"A\", drop=False)._internal.spark_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+---+---+---+\n    |  A|  B|  C|  D|  E|\n    +---+---+---+---+---+\n    |  1|  5|  9| 13| 17|\n    |  2|  6| 10| 14| 18|\n    |  3|  7| 11| 15| 19|\n    |  4|  8| 12| 16| 20|\n    +---+---+---+---+---+\n\n    In case that index becomes a multi index as below:\n\n    >>> kdf2 = kdf.set_index(\"A\", append=True)\n    >>> kdf2  # doctest: +NORMALIZE_WHITESPACE\n         B   C   D   E\n      A\n    0 1  5   9  13  17\n    1 2  6  10  14  18\n    2 3  7  11  15  19\n    3 4  8  12  16  20\n\n    >>> kdf2._internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +-----------------+---+---+---+---+---+\n    |__index_level_0__|  A|  B|  C|  D|  E|\n    +-----------------+---+---+---+---+---+\n    |                0|  1|  5|  9| 13| 17|\n    |                1|  2|  6| 10| 14| 18|\n    |                2|  3|  7| 11| 15| 19|\n    |                3|  4|  8| 12| 16| 20|\n    +-----------------+---+---+---+---+---+\n\n    >>> internal = kdf2._internal\n    >>> internal.sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    +-----------------+---+---+---+---+---+-----------------+\n    |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n    +-----------------+---+---+---+---+---+-----------------+\n    |                0|  1|  5|  9| 13| 17|...|\n    |                1|  2|  6| 10| 14| 18|...|\n    |                2|  3|  7| 11| 15| 19|...|\n    |                3|  4|  8| 12| 16| 20|...|\n    +-----------------+---+---+---+---+---+-----------------+\n    >>> internal.data_columns\n    ['B', 'C', 'D', 'E']\n    >>> internal.index_columns\n    ['__index_level_0__', 'A']\n    >>> internal.columns\n    ['__index_level_0__', 'A', 'B', 'C', 'D', 'E']\n    >>> internal.index_names\n    [None, ('A',)]\n    >>> internal.index_map\n    [('__index_level_0__', None), ('A', ('A',))]\n    >>> internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +-----------------+---+---+---+---+---+\n    |__index_level_0__|  A|  B|  C|  D|  E|\n    +-----------------+---+---+---+---+---+\n    |                0|  1|  5|  9| 13| 17|\n    |                1|  2|  6| 10| 14| 18|\n    |                2|  3|  7| 11| 15| 19|\n    |                3|  4|  8| 12| 16| 20|\n    +-----------------+---+---+---+---+---+\n    >>> internal.pandas_df  # doctest: +NORMALIZE_WHITESPACE\n         B   C   D   E\n      A\n    0 1  5   9  13  17\n    1 2  6  10  14  18\n    2 3  7  11  15  19\n    3 4  8  12  16  20\n\n    For multi-level columns, it also holds column_labels\n\n    >>> columns = pd.MultiIndex.from_tuples([('X', 'A'), ('X', 'B'),\n    ...                                      ('Y', 'C'), ('Y', 'D')])\n    >>> kdf3 = ks.DataFrame([\n    ...     [1, 2, 3, 4],\n    ...     [5, 6, 7, 8],\n    ...     [9, 10, 11, 12],\n    ...     [13, 14, 15, 16],\n    ...     [17, 18, 19, 20]], columns = columns)\n    >>> kdf3  # doctest: +NORMALIZE_WHITESPACE\n        X       Y\n        A   B   C   D\n    0   1   2   3   4\n    1   5   6   7   8\n    2   9  10  11  12\n    3  13  14  15  16\n    4  17  18  19  20\n\n    >>> internal = kdf3._internal\n    >>> internal.sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    +-----------------+------+------+------+------+-----------------+\n    |__index_level_0__|(X, A)|(X, B)|(Y, C)|(Y, D)|__natural_order__|\n    +-----------------+------+------+------+------+-----------------+\n    |                0|     1|     2|     3|     4|...|\n    |                1|     5|     6|     7|     8|...|\n    |                2|     9|    10|    11|    12|...|\n    |                3|    13|    14|    15|    16|...|\n    |                4|    17|    18|    19|    20|...|\n    +-----------------+------+------+------+------+-----------------+\n    >>> internal.data_columns\n    ['(X, A)', '(X, B)', '(Y, C)', '(Y, D)']\n    >>> internal.column_labels\n    [('X', 'A'), ('X', 'B'), ('Y', 'C'), ('Y', 'D')]\n\n    For series, it also holds scol to represent the column.\n\n    >>> kseries = kdf1.B\n    >>> kseries\n    A\n    1    5\n    2    6\n    3    7\n    4    8\n    Name: B, dtype: int64\n\n    >>> internal = kseries._internal\n    >>> internal.sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    +-----------------+---+---+---+---+---+-----------------+\n    |__index_level_0__|  A|  B|  C|  D|  E|__natural_order__|\n    +-----------------+---+---+---+---+---+-----------------+\n    |                0|  1|  5|  9| 13| 17|...|\n    |                1|  2|  6| 10| 14| 18|...|\n    |                2|  3|  7| 11| 15| 19|...|\n    |                3|  4|  8| 12| 16| 20|...|\n    +-----------------+---+---+---+---+---+-----------------+\n    >>> internal.scol\n    Column<b'B'>\n    >>> internal.data_columns\n    ['B']\n    >>> internal.index_columns\n    ['A']\n    >>> internal.columns\n    ['A', 'B']\n    >>> internal.index_names\n    [('A',)]\n    >>> internal.index_map\n    [('A', ('A',))]\n    >>> internal.spark_internal_df.show()  # doctest: +NORMALIZE_WHITESPACE\n    +---+---+\n    |  A|  B|\n    +---+---+\n    |  1|  5|\n    |  2|  6|\n    |  3|  7|\n    |  4|  8|\n    +---+---+\n    >>> internal.pandas_df  # doctest: +NORMALIZE_WHITESPACE\n       B\n    A\n    1  5\n    2  6\n    3  7\n    4  8\n    \"\"\"\n\n    def __init__(\n        self,\n        sdf: spark.DataFrame,\n        index_map: Optional[List[IndexMap]],\n        column_labels: Optional[List[Tuple[str, ...]]] = None,\n        column_scols: Optional[List[spark.Column]] = None,\n        column_label_names: Optional[List[str]] = None,\n        scol: Optional[spark.Column] = None,\n    ) -> None:\n        \"\"\"\n        Create a new internal immutable DataFrame to manage Spark DataFrame, column fields and\n        index fields and names.\n\n        :param sdf: Spark DataFrame to be managed.\n        :param index_map: list of string pair\n                           Each pair holds the index field name which exists in Spark fields,\n                           and the index name.\n        :param column_labels: list of tuples with the same length\n                              The multi-level values in the tuples.\n        :param column_scols: list of Spark Column\n                              Spark Columns to appear as columns. If scol is not None, this\n                              argument is ignored, otherwise if this is None, calculated from sdf.\n        :param column_label_names: Names for each of the index levels.\n        :param scol: Spark Column to be managed.\n\n        See the examples below to refer what each parameter means.\n\n        >>> column_labels = pd.MultiIndex.from_tuples(\n        ...     [('a', 'x'), ('a', 'y'), ('b', 'z')], names=[\"column_labels_a\", \"column_labels_b\"])\n        >>> row_index = pd.MultiIndex.from_tuples(\n        ...     [('foo', 'bar'), ('foo', 'bar'), ('zoo', 'bar')],\n        ...     names=[\"row_index_a\", \"row_index_b\"])\n        >>> kdf = ks.DataFrame(\n        ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9]], index=row_index, columns=column_labels)\n        >>> kdf.set_index(('a', 'x'), append=True, inplace=True)\n        >>> kdf  # doctest: +NORMALIZE_WHITESPACE\n        column_labels_a                  a  b\n        column_labels_b                  y  z\n        row_index_a row_index_b (a, x)\n        foo         bar         1       2  3\n                                4       5  6\n        zoo         bar         7       8  9\n\n        >>> internal = kdf[('a', 'y')]._internal\n\n        >>> internal._sdf.show()  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n        +-----------+-----------+------+------+------+...\n        |row_index_a|row_index_b|(a, x)|(a, y)|(b, z)|...\n        +-----------+-----------+------+------+------+...\n        |        foo|        bar|     1|     2|     3|...\n        |        foo|        bar|     4|     5|     6|...\n        |        zoo|        bar|     7|     8|     9|...\n        +-----------+-----------+------+------+------+...\n\n        >>> internal._index_map  # doctest: +NORMALIZE_WHITESPACE\n        [('row_index_a', ('row_index_a',)), ('row_index_b', ('row_index_b',)),\n         ('(a, x)', ('a', 'x'))]\n\n        >>> internal._column_labels\n        [('a', 'y')]\n\n        >>> internal._column_scols\n        [Column<b'(a, y)'>]\n\n        >>> list(internal._column_label_names)\n        ['column_labels_a', 'column_labels_b']\n\n        >>> internal._scol\n        Column<b'(a, y)'>\n        \"\"\"\n        assert isinstance(sdf, spark.DataFrame)\n        assert not sdf.isStreaming, \"Koalas does not support Structured Streaming.\"\n\n        if index_map is None:\n            assert not any(SPARK_INDEX_NAME_PATTERN.match(name) for name in sdf.columns), (\n                \"Index columns should not appear in columns of the Spark DataFrame. Avoid \"\n                \"index column names [%s].\" % SPARK_INDEX_NAME_PATTERN\n            )\n\n            # Create default index.\n            sdf = _InternalFrame.attach_default_index(sdf)\n            index_map = [(SPARK_DEFAULT_INDEX_NAME, None)]\n\n        if NATURAL_ORDER_COLUMN_NAME not in sdf.columns:\n            sdf = sdf.withColumn(NATURAL_ORDER_COLUMN_NAME, F.monotonically_increasing_id())\n\n        assert all(\n            isinstance(index_field, str)\n            and (\n                index_name is None\n                or (\n                    isinstance(index_name, tuple)\n                    and all(isinstance(name, str) for name in index_name)\n                )\n            )\n            for index_field, index_name in index_map\n        ), index_map\n        assert scol is None or isinstance(scol, spark.Column)\n        assert column_scols is None or all(isinstance(scol, spark.Column) for scol in column_scols)\n\n        self._sdf = sdf  # type: spark.DataFrame\n        self._index_map = index_map  # type: List[IndexMap]\n        self._scol = scol  # type: Optional[spark.Column]\n        if scol is not None:\n            self._column_scols = [scol]\n        elif column_scols is None:\n            index_columns = set(index_column for index_column, _ in self._index_map)\n            self._column_scols = [\n                scol_for(sdf, col)\n                for col in sdf.columns\n                if col not in index_columns and col not in HIDDEN_COLUMNS\n            ]\n        else:\n            self._column_scols = column_scols\n\n        if scol is not None:\n            assert column_labels is not None and len(column_labels) == 1, column_labels\n            assert all(\n                label is None or (isinstance(label, tuple) and len(label) > 0)\n                for label in column_labels\n            ), column_labels\n            self._column_labels = column_labels\n        elif column_labels is None:\n            self._column_labels = [(sdf.select(scol).columns[0],) for scol in self._column_scols]\n        else:\n            assert len(column_labels) == len(self._column_scols), (\n                len(column_labels),\n                len(self._column_scols),\n            )\n            assert all(isinstance(i, tuple) for i in column_labels), column_labels\n            assert len(set(len(i) for i in column_labels)) <= 1, column_labels\n            self._column_labels = column_labels\n\n        if column_label_names is not None and not is_list_like(column_label_names):\n            raise ValueError(\"Column_index_names should be list-like or None for a MultiIndex\")\n\n        if isinstance(column_label_names, list):\n            if all(name is None for name in column_label_names):\n                self._column_label_names = None\n            else:\n                self._column_label_names = column_label_names\n        else:\n            self._column_label_names = column_label_names\n\n    @staticmethod\n    def attach_default_index(sdf, default_index_type=None):\n        \"\"\"\n        This method attaches a default index to Spark DataFrame. Spark does not have the index\n        notion so corresponding column should be generated.\n        There are several types of default index can be configured by `compute.default_index_type`.\n\n        >>> sdf = ks.range(10).to_spark()\n        >>> sdf\n        DataFrame[id: bigint]\n\n        It adds the default index column '__index_level_0__'.\n\n        >>> sdf = _InternalFrame.attach_default_index(sdf)\n        >>> sdf\n        DataFrame[__index_level_0__: int, id: bigint]\n\n        It throws an exception if the given column name already exists.\n\n        >>> _InternalFrame.attach_default_index(sdf)\n        ... # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n          ...\n        AssertionError: '__index_level_0__' already exists...\n        \"\"\"\n        index_column = SPARK_DEFAULT_INDEX_NAME\n        assert (\n            index_column not in sdf.columns\n        ), \"'%s' already exists in the Spark column names '%s'\" % (index_column, sdf.columns)\n\n        if default_index_type is None:\n            default_index_type = get_option(\"compute.default_index_type\")\n\n        scols = [scol_for(sdf, column) for column in sdf.columns]\n        if default_index_type == \"sequence\":\n            sequential_index = (\n                F.row_number().over(Window.orderBy(F.monotonically_increasing_id())) - 1\n            )\n            return sdf.select(sequential_index.alias(index_column), *scols)\n        elif default_index_type == \"distributed-sequence\":\n            return _InternalFrame.attach_distributed_sequence_column(sdf, column_name=index_column)\n        elif default_index_type == \"distributed\":\n            return _InternalFrame.attach_distributed_column(sdf, column_name=index_column)\n        else:\n            raise ValueError(\n                \"'compute.default_index_type' should be one of 'sequence',\"\n                \" 'distributed-sequence' and 'distributed'\"\n            )\n\n    @staticmethod\n    def attach_distributed_column(sdf, column_name):\n        scols = [scol_for(sdf, column) for column in sdf.columns]\n        return sdf.select(F.monotonically_increasing_id().alias(column_name), *scols)\n\n    @staticmethod\n    def attach_distributed_sequence_column(sdf, column_name):\n        \"\"\"\n        This method attaches a Spark column that has a sequence in a distributed manner.\n        This is equivalent to the column assigned when default index type 'distributed-sequence'.\n\n        >>> sdf = ks.DataFrame(['a', 'b', 'c']).to_spark()\n        >>> sdf = _InternalFrame.attach_distributed_sequence_column(sdf, column_name=\"sequence\")\n        >>> sdf.sort(\"sequence\").show()  # doctest: +NORMALIZE_WHITESPACE\n        +--------+---+\n        |sequence|  0|\n        +--------+---+\n        |       0|  a|\n        |       1|  b|\n        |       2|  c|\n        +--------+---+\n        \"\"\"\n\n        scols = [scol_for(sdf, column) for column in sdf.columns]\n\n        spark_partition_column = verify_temp_column_name(sdf, \"__spark_partition_id__\")\n        offset_column = verify_temp_column_name(sdf, \"__offset__\")\n        row_number_column = verify_temp_column_name(sdf, \"__row_number__\")\n\n        # 1. Calculates counts per each partition ID. `counts` here is, for instance,\n        #     {\n        #         1: 83,\n        #         6: 83,\n        #         3: 83,\n        #         ...\n        #     }\n        sdf = sdf.withColumn(spark_partition_column, F.spark_partition_id())\n        counts = map(\n            lambda x: (x[\"key\"], x[\"count\"]),\n            sdf.groupby(sdf[spark_partition_column].alias(\"key\")).count().collect(),\n        )\n\n        # 2. Calculates cumulative sum in an order of partition id.\n        #     Note that it does not matter if partition id guarantees its order or not.\n        #     We just need a one-by-one sequential id.\n\n        # sort by partition key.\n        sorted_counts = sorted(counts, key=lambda x: x[0])\n        # get cumulative sum in an order of partition key.\n        cumulative_counts = [0] + list(accumulate(map(lambda count: count[1], sorted_counts)))\n        # zip it with partition key.\n        sums = dict(zip(map(lambda count: count[0], sorted_counts), cumulative_counts))\n\n        # 3. Attach offset for each partition.\n        @pandas_udf(LongType(), PandasUDFType.SCALAR)\n        def offset(id):\n            current_partition_offset = sums[id.iloc[0]]\n            return pd.Series(current_partition_offset).repeat(len(id))\n\n        sdf = sdf.withColumn(offset_column, offset(spark_partition_column))\n\n        # 4. Calculate row_number in each partition.\n        w = Window.partitionBy(spark_partition_column).orderBy(F.monotonically_increasing_id())\n        row_number = F.row_number().over(w)\n        sdf = sdf.withColumn(row_number_column, row_number)\n\n        # 5. Calculate the index.\n        return sdf.select(\n            (sdf[offset_column] + sdf[row_number_column] - 1).alias(column_name), *scols\n        )\n\n    @lazy_property\n    def _column_labels_to_name(self) -> Dict[Tuple[str, ...], str]:\n        return dict(zip(self.column_labels, self.data_columns))\n\n    def column_name_for(self, column_labels_or_index_column: Union[str, Tuple[str, ...]]) -> str:\n        \"\"\" Return the actual Spark column name for the given column name or index. \"\"\"\n        if column_labels_or_index_column in self._column_labels_to_name:\n            return self._column_labels_to_name[column_labels_or_index_column]\n        else:\n            if column_labels_or_index_column not in self.index_columns:\n                raise KeyError(name_like_string(column_labels_or_index_column))\n            return column_labels_or_index_column  # type: ignore\n\n    @lazy_property\n    def _column_labels_to_scol(self) -> Dict[Tuple[str, ...], spark.Column]:\n        return dict(zip(self.column_labels, self.column_scols))\n\n    def scol_for(self, column_labels_or_index_column: Union[str, Tuple[str, ...]]):\n        \"\"\" Return Spark Column for the given column name or index. \"\"\"\n        if column_labels_or_index_column in self._column_labels_to_scol:\n            return self._column_labels_to_scol[column_labels_or_index_column]\n        else:\n            if column_labels_or_index_column not in self.index_columns:\n                raise KeyError(name_like_string(column_labels_or_index_column))\n            return scol_for(self._sdf, self.column_name_for(column_labels_or_index_column))\n\n    def spark_type_for(\n        self, column_labels_or_index_column: Union[str, Tuple[str, ...]]\n    ) -> DataType:\n        \"\"\" Return DataType for the given column name or index. \"\"\"\n        return self._sdf.select(self.scol_for(column_labels_or_index_column)).schema[0].dataType\n\n    @property\n    def sdf(self) -> spark.DataFrame:\n        \"\"\" Return the managed Spark DataFrame. \"\"\"\n        return self._sdf\n\n    @lazy_property\n    def data_columns(self) -> List[str]:\n        \"\"\" Return the managed column field names. \"\"\"\n        return self.sdf.select(self.column_scols).columns\n\n    @property\n    def column_scols(self) -> List[spark.Column]:\n        \"\"\" Return Spark Columns for the managed data columns. \"\"\"\n        return self._column_scols\n\n    @lazy_property\n    def index_columns(self) -> List[str]:\n        \"\"\" Return the managed index field names. \"\"\"\n        return [index_column for index_column, _ in self._index_map]\n\n    @lazy_property\n    def index_scols(self) -> List[spark.Column]:\n        \"\"\" Return Spark Columns for the managed index columns. \"\"\"\n        return [self.scol_for(column) for column in self.index_columns]\n\n    @lazy_property\n    def columns(self) -> List[str]:\n        \"\"\" Return all the field names including index field names. \"\"\"\n        index_columns = set(self.index_columns)\n        return self.index_columns + [\n            column for column in self.data_columns if column not in index_columns\n        ]\n\n    @lazy_property\n    def scols(self) -> List[spark.Column]:\n        \"\"\" Return Spark Columns for the managed columns including index columns. \"\"\"\n        index_columns = set(self.index_columns)\n        return self.index_scols + [\n            self.scol_for(label)\n            for label in self.column_labels\n            if self.column_name_for(label) not in index_columns\n        ]\n\n    @property\n    def index_map(self) -> List[IndexMap]:\n        \"\"\" Return the managed index information. \"\"\"\n        assert len(self._index_map) > 0\n        return self._index_map\n\n    @lazy_property\n    def index_names(self) -> List[Optional[Tuple[str, ...]]]:\n        \"\"\" Return the managed index names. \"\"\"\n        return [index_name for _, index_name in self.index_map]\n\n    @property\n    def scol(self) -> Optional[spark.Column]:\n        \"\"\" Return the managed Spark Column. \"\"\"\n        return self._scol\n\n    @property\n    def column_labels(self) -> List[Tuple[str, ...]]:\n        \"\"\" Return the managed column index. \"\"\"\n        return self._column_labels\n\n    @lazy_property\n    def column_labels_level(self) -> int:\n        \"\"\" Return the level of the column index. \"\"\"\n        return column_labels_level(self._column_labels)\n\n    @property\n    def column_label_names(self) -> Optional[List[str]]:\n        \"\"\" Return names of the index levels. \"\"\"\n        return self._column_label_names\n\n    @lazy_property\n    def spark_internal_df(self) -> spark.DataFrame:\n        \"\"\"\n        Return as Spark DataFrame. This contains index columns as well\n        and should be only used for internal purposes.\n        \"\"\"\n        index_columns = set(self.index_columns)\n        data_columns = []\n        for i, (column, label) in enumerate(zip(self.data_columns, self.column_labels)):\n            if column not in index_columns:\n                scol = self.scol_for(label)\n                name = str(i) if label is None else name_like_string(label)\n                if column != name:\n                    scol = scol.alias(name)\n                data_columns.append(scol)\n        return self._sdf.select(self.index_scols + data_columns)\n\n    @lazy_property\n    def spark_df(self) -> spark.DataFrame:\n        \"\"\" Return as Spark DataFrame. \"\"\"\n        data_columns = []\n        for i, (column, label) in enumerate(zip(self.data_columns, self.column_labels)):\n            scol = self.scol_for(label)\n            name = str(i) if label is None else name_like_string(label)\n            if column != name:\n                scol = scol.alias(name)\n            data_columns.append(scol)\n        return self._sdf.select(data_columns)\n\n    @lazy_property\n    def pandas_df(self):\n        \"\"\" Return as pandas DataFrame. \"\"\"\n        sdf = self.spark_internal_df\n        pdf = sdf.toPandas()\n        if len(pdf) == 0 and len(sdf.schema) > 0:\n            pdf = pdf.astype(\n                {field.name: spark_type_to_pandas_dtype(field.dataType) for field in sdf.schema}\n            )\n\n        index_columns = self.index_columns\n        if len(index_columns) > 0:\n            append = False\n            for index_field in index_columns:\n                drop = index_field not in self.data_columns\n                pdf = pdf.set_index(index_field, drop=drop, append=append)\n                append = True\n            pdf = pdf[\n                [\n                    col\n                    if col in index_columns\n                    else str(i)\n                    if label is None\n                    else name_like_string(label)\n                    for i, (col, label) in enumerate(zip(self.data_columns, self.column_labels))\n                ]\n            ]\n\n        if self.column_labels_level > 1:\n            pdf.columns = pd.MultiIndex.from_tuples(self._column_labels)\n        else:\n            pdf.columns = [None if label is None else label[0] for label in self._column_labels]\n        if self._column_label_names is not None:\n            pdf.columns.names = self._column_label_names\n\n        index_names = self.index_names\n        if len(index_names) > 0:\n            pdf.index.names = [\n                name if name is None or len(name) > 1 else name[0] for name in index_names\n            ]\n        return pdf\n\n    def with_new_sdf(\n        self, sdf: spark.DataFrame, data_columns: Optional[List[str]] = None\n    ) -> \"_InternalFrame\":\n        \"\"\" Copy the immutable _InternalFrame with the updates by the specified Spark DataFrame.\n\n        :param sdf: the new Spark DataFrame\n        :param data_columns: the new column names.\n            If None, the original one is used.\n        :return: the copied _InternalFrame.\n        \"\"\"\n        if data_columns is None:\n            data_columns = self.data_columns\n        else:\n            assert len(data_columns) == len(self.column_labels), (\n                len(data_columns),\n                len(self.column_labels),\n            )\n        sdf = sdf.drop(NATURAL_ORDER_COLUMN_NAME)\n        return self.copy(sdf=sdf, column_scols=[scol_for(sdf, col) for col in data_columns])\n\n    def with_new_columns(\n        self,\n        scols_or_ksers: List[Union[spark.Column, \"Series\"]],\n        column_labels: Optional[List[Tuple[str, ...]]] = None,\n        keep_order: bool = True,\n    ) -> \"_InternalFrame\":\n        \"\"\"\n        Copy the immutable _InternalFrame with the updates by the specified Spark Columns or Series.\n\n        :param scols_or_ksers: the new Spark Columns or Series.\n        :param column_labels: the new column index.\n            If None, the its column_labels is used when the corresponding `scols_or_ksers` is\n            Series, otherwise the original one is used.\n        :return: the copied _InternalFrame.\n        \"\"\"\n        from databricks.koalas.series import Series\n\n        if column_labels is None:\n            if all(isinstance(scol_or_kser, Series) for scol_or_kser in scols_or_ksers):\n                column_labels = [kser._internal.column_labels[0] for kser in scols_or_ksers]\n            else:\n                assert len(scols_or_ksers) == len(self.column_labels), (\n                    len(scols_or_ksers),\n                    len(self.column_labels),\n                )\n                column_labels = []\n                for scol_or_kser, label in zip(scols_or_ksers, self.column_labels):\n                    if isinstance(scol_or_kser, Series):\n                        column_labels.append(scol_or_kser._internal.column_labels[0])\n                    else:\n                        column_labels.append(label)\n        else:\n            assert len(scols_or_ksers) == len(column_labels), (\n                len(scols_or_ksers),\n                len(column_labels),\n            )\n\n        column_scols = []\n        for scol_or_kser, label in zip(scols_or_ksers, column_labels):\n            if isinstance(scol_or_kser, Series):\n                scol = scol_or_kser._internal.scol\n            else:\n                scol = scol_or_kser\n            column_scols.append(scol)\n\n        hidden_columns = []\n        if keep_order:\n            hidden_columns.append(NATURAL_ORDER_COLUMN_NAME)\n\n        sdf = self._sdf.select(self.index_scols + column_scols + hidden_columns)\n\n        return self.copy(\n            sdf=sdf,\n            column_labels=column_labels,\n            column_scols=[scol_for(sdf, col) for col in self._sdf.select(column_scols).columns],\n            scol=None,\n        )\n\n    def with_filter(self, pred: Union[spark.Column, \"Series\"]):\n        \"\"\" Copy the immutable _InternalFrame with the updates by the predicate.\n\n        :param pred: the predicate to filter.\n        :return: the copied _InternalFrame.\n        \"\"\"\n        from databricks.koalas.series import Series\n\n        if isinstance(pred, Series):\n            assert isinstance(pred.spark_type, BooleanType), pred.spark_type\n            pred = pred._scol\n        else:\n            spark_type = self._sdf.select(pred).schema[0].dataType\n            assert isinstance(spark_type, BooleanType), spark_type\n\n        return self.copy(sdf=self._sdf.drop(NATURAL_ORDER_COLUMN_NAME).filter(pred))\n\n    def copy(\n        self,\n        sdf: Union[spark.DataFrame, _NoValueType] = _NoValue,\n        index_map: Union[List[IndexMap], _NoValueType] = _NoValue,\n        column_labels: Union[List[Tuple[str, ...]], _NoValueType] = _NoValue,\n        column_scols: Union[List[spark.Column], _NoValueType] = _NoValue,\n        column_label_names: Optional[Union[List[str], _NoValueType]] = _NoValue,\n        scol: Union[spark.Column, _NoValueType] = _NoValue,\n    ) -> \"_InternalFrame\":\n        \"\"\" Copy the immutable DataFrame.\n\n        :param sdf: the new Spark DataFrame. If None, then the original one is used.\n        :param index_map: the new index information. If None, then the original one is used.\n        :param column_labels: the new column index.\n        :param column_scols: the new Spark Columns. If None, then the original ones are used.\n        :param column_label_names: the new names of the index levels.\n        :param scol: the new Spark Column. If None, then the original one is used.\n        :return: the copied immutable DataFrame.\n        \"\"\"\n        if sdf is _NoValue:\n            sdf = self._sdf\n        if index_map is _NoValue:\n            index_map = self._index_map\n        if column_labels is _NoValue:\n            column_labels = self._column_labels\n        if column_scols is _NoValue:\n            column_scols = self._column_scols\n        if column_label_names is _NoValue:\n            column_label_names = self._column_label_names\n        if scol is _NoValue:\n            scol = self._scol\n        return _InternalFrame(\n            sdf,\n            index_map=index_map,\n            column_labels=column_labels,\n            column_scols=column_scols,\n            column_label_names=column_label_names,\n            scol=scol,\n        )\n\n    @staticmethod\n    def from_pandas(pdf: pd.DataFrame) -> \"_InternalFrame\":\n        \"\"\" Create an immutable DataFrame from pandas DataFrame.\n\n        :param pdf: :class:`pd.DataFrame`\n        :return: the created immutable DataFrame\n        \"\"\"\n        columns = pdf.columns\n        data_columns = [name_like_string(col) for col in columns]\n        if isinstance(columns, pd.MultiIndex):\n            column_labels = columns.tolist()\n        else:\n            column_labels = None\n        column_label_names = columns.names\n\n        index = pdf.index\n\n        index_map = []  # type: List[IndexMap]\n        if isinstance(index, pd.MultiIndex):\n            if index.names is None:\n                index_map = [(SPARK_INDEX_NAME_FORMAT(i), None) for i in range(len(index.levels))]\n            else:\n                index_map = [\n                    (\n                        SPARK_INDEX_NAME_FORMAT(i) if name is None else name_like_string(name),\n                        name if name is None or isinstance(name, tuple) else (name,),\n                    )\n                    for i, name in enumerate(index.names)\n                ]\n        else:\n            name = index.name\n            index_map = [\n                (\n                    name_like_string(name) if name is not None else SPARK_DEFAULT_INDEX_NAME,\n                    name if name is None or isinstance(name, tuple) else (name,),\n                )\n            ]\n\n        index_columns = [index_column for index_column, _ in index_map]\n\n        reset_index = pdf.reset_index()\n        reset_index.columns = index_columns + data_columns\n        schema = StructType(\n            [\n                StructField(\n                    name_like_string(name),\n                    infer_pd_series_spark_type(col),\n                    nullable=bool(col.isnull().any()),\n                )\n                for name, col in reset_index.iteritems()\n            ]\n        )\n        for name, col in reset_index.iteritems():\n            dt = col.dtype\n            if is_datetime64_dtype(dt) or is_datetime64tz_dtype(dt):\n                continue\n            reset_index[name] = col.replace({np.nan: None})\n        sdf = default_session().createDataFrame(reset_index, schema=schema)\n        return _InternalFrame(\n            sdf=sdf,\n            index_map=index_map,\n            column_labels=column_labels,\n            column_scols=[scol_for(sdf, col) for col in data_columns],\n            column_label_names=column_label_names,\n        )\n", "idx": 7, "id": 14504, "msg": "", "proj": "databricks-koalas", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -236,11 +236,11 @@ func (w *Waiter) receiptFromTipSet(ctx context.Context, msgCid cid.Cid, ts types\n \t\treturn nil, nil\n \t}\n \n-\tj, err := msgIndexOfTipSet(msgCid, ts, res.Failures)\n+\tj, err := w.msgIndexOfTipSet(ctx, msgCid, ts, res.Failures)\n \tif err != nil {\n \t\treturn nil, err\n \t}\n-\t// TODO: out of bounds receipt index should return an error.\n+\t// TODO #3194: out of bounds receipt index should return an error.\n \tif j < len(res.Results) {\n \t\trcpt = res.Results[j].Receipt\n \t}", "y": 0, "oldf": "package msg\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/cskr/pubsub\"\n\t\"github.com/ipfs/go-cid\"\n\t\"github.com/ipfs/go-hamt-ipld\"\n\tbstore \"github.com/ipfs/go-ipfs-blockstore\"\n\tlogging \"github.com/ipfs/go-log\"\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/filecoin-project/go-filecoin/chain\"\n\t\"github.com/filecoin-project/go-filecoin/consensus\"\n\t\"github.com/filecoin-project/go-filecoin/sampling\"\n\t\"github.com/filecoin-project/go-filecoin/state\"\n\t\"github.com/filecoin-project/go-filecoin/types\"\n\t\"github.com/filecoin-project/go-filecoin/vm\"\n)\n\nvar log = logging.Logger(\"messageimpl\")\n\n// Abstracts over a store of blockchain state.\ntype waiterChainReader interface {\n\tGetHead() types.TipSetKey\n\tGetTipSet(types.TipSetKey) (types.TipSet, error)\n\tGetTipSetState(context.Context, types.TipSetKey) (state.Tree, error)\n\tHeadEvents() *pubsub.PubSub\n}\n\n// Waiter waits for a message to appear on chain.\ntype Waiter struct {\n\tchainReader   waiterChainReader\n\tmessageReader chain.MessageReader // nolint: structcheck\n\tcst           *hamt.CborIpldStore\n\tbs            bstore.Blockstore\n}\n\n// ChainMessage is an on-chain message with its block and receipt.\ntype ChainMessage struct {\n\tMessage *types.SignedMessage\n\tBlock   *types.Block\n\tReceipt *types.MessageReceipt\n}\n\n// NewWaiter returns a new Waiter.\nfunc NewWaiter(chainStore waiterChainReader, messageReader chain.MessageReader, bs bstore.Blockstore, cst *hamt.CborIpldStore) *Waiter {\n\treturn &Waiter{\n\t\tchainReader: chainStore,\n\t\tcst:         cst,\n\t\tbs:          bs,\n\t}\n}\n\n// Find searches the blockchain history for a message (but doesn't wait).\nfunc (w *Waiter) Find(ctx context.Context, msgCid cid.Cid) (*ChainMessage, bool, error) {\n\theadTipSet, err := w.chainReader.GetTipSet(w.chainReader.GetHead())\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\treturn w.findMessage(ctx, headTipSet, msgCid)\n}\n\n// Wait invokes the callback when a message with the given cid appears on chain.\n// See api description.\n//\n// Note: this method does too much -- the callback should just receive the tipset\n// containing the message and the caller should pull the receipt out of the block\n// if in fact that's what it wants to do, using something like receiptFromTipset.\n// Something like receiptFromTipset is necessary because not every message in\n// a block will have a receipt in the tipset: it might be a duplicate message.\n//\n// TODO: This implementation will become prohibitively expensive since it\n// traverses the entire chain. We should use an index instead.\n// https://github.com/filecoin-project/go-filecoin/issues/1518\nfunc (w *Waiter) Wait(ctx context.Context, msgCid cid.Cid, cb func(*types.Block, *types.SignedMessage, *types.MessageReceipt) error) error {\n\tctx = log.Start(ctx, \"Waiter.Wait\")\n\tdefer log.Finish(ctx)\n\tlog.Infof(\"Calling Waiter.Wait CID: %s\", msgCid.String())\n\n\tch := w.chainReader.HeadEvents().Sub(chain.NewHeadTopic)\n\tdefer w.chainReader.HeadEvents().Unsub(ch, chain.NewHeadTopic)\n\n\tchainMsg, found, err := w.Find(ctx, msgCid)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif found {\n\t\treturn cb(chainMsg.Block, chainMsg.Message, chainMsg.Receipt)\n\t}\n\n\tchainMsg, found, err = w.waitForMessage(ctx, ch, msgCid)\n\tif found {\n\t\treturn cb(chainMsg.Block, chainMsg.Message, chainMsg.Receipt)\n\t}\n\treturn err\n}\n\n// findMessage looks for a message CID in the chain and returns the message,\n// block and receipt, when it is found. Returns the found message/block or nil\n// if now block with the given CID exists in the chain.\nfunc (w *Waiter) findMessage(ctx context.Context, ts types.TipSet, msgCid cid.Cid) (*ChainMessage, bool, error) {\n\tvar err error\n\tfor iterator := chain.IterAncestors(ctx, w.chainReader, ts); !iterator.Complete(); err = iterator.Next() {\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"Waiter.Wait: %s\", err)\n\t\t\treturn nil, false, err\n\t\t}\n\t\tfor i := 0; i < iterator.Value().Len(); i++ {\n\t\t\tblk := iterator.Value().At(i)\n\t\t\tfor _, msg := range blk.Messages {\n\t\t\t\tc, err := msg.Cid()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, false, err\n\t\t\t\t}\n\t\t\t\tif c.Equals(msgCid) {\n\t\t\t\t\trecpt, err := w.receiptFromTipSet(ctx, msgCid, iterator.Value())\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, false, errors.Wrap(err, \"error retrieving receipt from tipset\")\n\t\t\t\t\t}\n\t\t\t\t\treturn &ChainMessage{msg, blk, recpt}, true, nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil, false, nil\n}\n\n// waitForMessage looks for a message CID in a channel of tipsets and returns\n// the message, block and receipt, when it is found. Reads until the channel is\n// closed or the context done. Returns the found message/block (or nil if the\n// channel closed without finding it), whether it was found, or an error.\nfunc (w *Waiter) waitForMessage(ctx context.Context, ch <-chan interface{}, msgCid cid.Cid) (*ChainMessage, bool, error) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil, false, ctx.Err()\n\t\tcase raw, more := <-ch:\n\t\t\tif !more {\n\t\t\t\treturn nil, false, nil\n\t\t\t}\n\t\t\tswitch raw := raw.(type) {\n\t\t\tcase error:\n\t\t\t\te := raw.(error)\n\t\t\t\tlog.Errorf(\"Waiter.Wait: %s\", e)\n\t\t\t\treturn nil, false, e\n\t\t\tcase types.TipSet:\n\t\t\t\tfor i := 0; i < raw.Len(); i++ {\n\t\t\t\t\tblk := raw.At(i)\n\t\t\t\t\tfor _, msg := range blk.Messages {\n\t\t\t\t\t\tc, err := msg.Cid()\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn nil, false, err\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif c.Equals(msgCid) {\n\t\t\t\t\t\t\trecpt, err := w.receiptFromTipSet(ctx, msgCid, raw)\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn nil, false, errors.Wrap(err, \"error retrieving receipt from tipset\")\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn &ChainMessage{msg, blk, recpt}, true, nil\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn nil, false, fmt.Errorf(\"unexpected type in channel: %T\", raw)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// receiptFromTipSet finds the receipt for the message with msgCid in the\n// input tipset.  This can differ from the message's receipt as stored in its\n// parent block in the case that the message is in conflict with another\n// message of the tipset.\nfunc (w *Waiter) receiptFromTipSet(ctx context.Context, msgCid cid.Cid, ts types.TipSet) (*types.MessageReceipt, error) {\n\t// Receipts always match block if tipset has only 1 member.\n\tvar rcpt *types.MessageReceipt\n\tif ts.Len() == 1 {\n\t\tb := ts.At(0)\n\t\t// TODO: this should return an error if a receipt doesn't exist.\n\t\t// Right now doing so breaks tests because our test helpers\n\t\t// don't correctly apply messages when making test chains.\n\t\tj, err := msgIndexOfTipSet(msgCid, ts, make(map[cid.Cid]struct{}))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif j < len(b.MessageReceipts) {\n\t\t\trcpt = b.MessageReceipts[j]\n\t\t}\n\t\treturn rcpt, nil\n\t}\n\n\t// Apply all the tipset's messages to determine the correct receipts.\n\tids, err := ts.Parents()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tst, err := w.chainReader.GetTipSetState(ctx, ids)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ttsHeight, err := ts.Height()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttsBlockHeight := types.NewBlockHeight(tsHeight)\n\tancestorHeight := types.NewBlockHeight(consensus.AncestorRoundsNeeded)\n\tparentTs, err := w.chainReader.GetTipSet(ids)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tancestors, err := chain.GetRecentAncestors(ctx, parentTs, w.chainReader, tsBlockHeight, ancestorHeight, sampling.LookbackParameter)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar tsMessages [][]*types.SignedMessage\n\tfor i := 0; i < ts.Len(); i++ {\n\t\tblk := ts.At(i)\n\t\t// TODO #3103 this is a temporary way to force the consensus interface.\n\t\t// Once we separate messages out from blocks we'll need to read from\n\t\t// the message collection store.\n\t\ttsMessages = append(tsMessages, blk.Messages)\n\t}\n\n\tres, err := consensus.NewDefaultProcessor().ProcessTipSet(ctx, st, vm.NewStorageMap(w.bs), ts, tsMessages, ancestors)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If this is a failing conflict message there is no application receipt.\n\t_, failed := res.Failures[msgCid]\n\tif failed {\n\t\treturn nil, nil\n\t}\n\n\tj, err := msgIndexOfTipSet(msgCid, ts, res.Failures)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// TODO: out of bounds receipt index should return an error.\n\tif j < len(res.Results) {\n\t\trcpt = res.Results[j].Receipt\n\t}\n\treturn rcpt, nil\n}\n\n// msgIndexOfTipSet returns the order in which msgCid appears in the canonical\n// message ordering of the given tipset, or an error if it is not in the\n// tipset.\n// TODO: find a better home for this method\nfunc msgIndexOfTipSet(msgCid cid.Cid, ts types.TipSet, fails map[cid.Cid]struct{}) (int, error) {\n\tduplicates := make(map[cid.Cid]struct{})\n\tvar msgCnt int\n\tfor i := 0; i < ts.Len(); i++ {\n\t\tfor _, msg := range ts.At(i).Messages {\n\t\t\tc, err := msg.Cid()\n\t\t\tif err != nil {\n\t\t\t\treturn -1, err\n\t\t\t}\n\t\t\t_, failed := fails[c]\n\t\t\tif failed {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t_, isDup := duplicates[c]\n\t\t\tif isDup {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tduplicates[c] = struct{}{}\n\t\t\tif c.Equals(msgCid) {\n\t\t\t\treturn msgCnt, nil\n\t\t\t}\n\t\t\tmsgCnt++\n\t\t}\n\t}\n\n\treturn -1, fmt.Errorf(\"message cid %s not in tipset\", msgCid.String())\n}\n", "idx": 7, "id": 20711, "msg": "", "proj": "filecoin-project-venus", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -2,6 +2,9 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n+// Package debugapi exposes the debug API used to\n+// control and analyze low-level features and functionalities\n+// of Bee.\n package debugapi\n \n import (", "y": 1, "oldf": "// Copyright 2020 The Swarm Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage debugapi\n\nimport (\n\t\"crypto/ecdsa\"\n\t\"net/http\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethersphere/bee/pkg/accounting\"\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/p2p\"\n\t\"github.com/ethersphere/bee/pkg/pingpong\"\n\t\"github.com/ethersphere/bee/pkg/settlement\"\n\t\"github.com/ethersphere/bee/pkg/settlement/swap\"\n\t\"github.com/ethersphere/bee/pkg/settlement/swap/chequebook\"\n\t\"github.com/ethersphere/bee/pkg/storage\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n\t\"github.com/ethersphere/bee/pkg/tags\"\n\t\"github.com/ethersphere/bee/pkg/topology\"\n\t\"github.com/ethersphere/bee/pkg/tracing\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n)\n\ntype Service interface {\n\thttp.Handler\n\tMustRegisterMetrics(cs ...prometheus.Collector)\n}\n\ntype server struct {\n\tOverlay           swarm.Address\n\tPublicKey         ecdsa.PublicKey\n\tPSSPublicKey      ecdsa.PublicKey\n\tEthereumAddress   common.Address\n\tP2P               p2p.DebugService\n\tPingpong          pingpong.Interface\n\tTopologyDriver    topology.Driver\n\tStorer            storage.Storer\n\tLogger            logging.Logger\n\tTracer            *tracing.Tracer\n\tTags              *tags.Tags\n\tAccounting        accounting.Interface\n\tSettlement        settlement.Interface\n\tChequebookEnabled bool\n\tChequebook        chequebook.Service\n\tSwap              swap.ApiInterface\n\tmetricsRegistry   *prometheus.Registry\n\thttp.Handler\n}\n\nfunc New(overlay swarm.Address, publicKey, pssPublicKey ecdsa.PublicKey, ethereumAddress common.Address, p2p p2p.DebugService, pingpong pingpong.Interface, topologyDriver topology.Driver, storer storage.Storer, logger logging.Logger, tracer *tracing.Tracer, tags *tags.Tags, accounting accounting.Interface, settlement settlement.Interface, chequebookEnabled bool, swap swap.ApiInterface, chequebook chequebook.Service) Service {\n\ts := &server{\n\t\tOverlay:           overlay,\n\t\tPublicKey:         publicKey,\n\t\tPSSPublicKey:      pssPublicKey,\n\t\tEthereumAddress:   ethereumAddress,\n\t\tP2P:               p2p,\n\t\tPingpong:          pingpong,\n\t\tTopologyDriver:    topologyDriver,\n\t\tStorer:            storer,\n\t\tLogger:            logger,\n\t\tTracer:            tracer,\n\t\tTags:              tags,\n\t\tAccounting:        accounting,\n\t\tSettlement:        settlement,\n\t\tmetricsRegistry:   newMetricsRegistry(),\n\t\tChequebookEnabled: chequebookEnabled,\n\t\tChequebook:        chequebook,\n\t\tSwap:              swap,\n\t}\n\n\ts.setupRouting()\n\n\treturn s\n}\n", "idx": 1, "id": 13682, "msg": "Not just low-level, I would say the runtime state.", "proj": "ethersphere-bee", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -229,7 +229,10 @@ func (db *DB) putUpload(batch *leveldb.Batch, binIDs map[uint8]uint64, item shed\n \tif err != nil {\n \t\treturn false, 0, err\n \t}\n-\n+\terr = db.postage.putInBatch(batch, item)\n+\tif err != nil {\n+\t\treturn false, 0, err\n+\t}\n \treturn false, 0, nil\n }\n ", "y": 0, "oldf": "// Copyright 2018 The go-ethereum Authors\n// This file is part of the go-ethereum library.\n//\n// The go-ethereum library is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Lesser General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// The go-ethereum library is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Lesser General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public License\n// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.\n\npackage localstore\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"time\"\n\n\t\"github.com/ethersphere/bee/pkg/shed\"\n\t\"github.com/ethersphere/bee/pkg/storage\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n\t\"github.com/syndtr/goleveldb/leveldb\"\n)\n\n// Put stores Chunks to database and depending\n// on the Putter mode, it updates required indexes.\n// Put is required to implement storage.Store\n// interface.\nfunc (db *DB) Put(ctx context.Context, mode storage.ModePut, chs ...swarm.Chunk) (exist []bool, err error) {\n\n\tdb.metrics.ModePut.Inc()\n\tdefer totalTimeMetric(db.metrics.TotalTimePut, time.Now())\n\n\texist, err = db.put(mode, chs...)\n\tif err != nil {\n\t\tdb.metrics.ModePutFailure.Inc()\n\t}\n\n\treturn exist, err\n}\n\n// put stores Chunks to database and updates other indexes. It acquires lockAddr\n// to protect two calls of this function for the same address in parallel. Item\n// fields Address and Data must not be with their nil values. If chunks with the\n// same address are passed in arguments, only the first chunk will be stored,\n// and following ones will have exist set to true for their index in exist\n// slice. This is the same behaviour as if the same chunks are passed one by one\n// in multiple put method calls.\nfunc (db *DB) put(mode storage.ModePut, chs ...swarm.Chunk) (exist []bool, err error) {\n\t// protect parallel updates\n\tdb.batchMu.Lock()\n\tdefer db.batchMu.Unlock()\n\n\tbatch := new(leveldb.Batch)\n\n\t// variables that provide information for operations\n\t// to be done after write batch function successfully executes\n\tvar gcSizeChange int64                      // number to add or subtract from gcSize\n\tvar triggerPushFeed bool                    // signal push feed subscriptions to iterate\n\ttriggerPullFeed := make(map[uint8]struct{}) // signal pull feed subscriptions to iterate\n\n\texist = make([]bool, len(chs))\n\n\t// A lazy populated map of bin ids to properly set\n\t// BinID values for new chunks based on initial value from database\n\t// and incrementing them.\n\t// Values from this map are stored with the batch\n\tbinIDs := make(map[uint8]uint64)\n\n\tswitch mode {\n\tcase storage.ModePutRequest, storage.ModePutRequestPin:\n\t\tfor i, ch := range chs {\n\t\t\tif containsChunk(ch.Address(), chs[:i]...) {\n\t\t\t\texist[i] = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texists, c, err := db.putRequest(batch, binIDs, chunkToItem(ch))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\texist[i] = exists\n\t\t\tgcSizeChange += c\n\n\t\t\tif mode == storage.ModePutRequestPin {\n\t\t\t\terr = db.setPin(batch, ch.Address())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tcase storage.ModePutUpload, storage.ModePutUploadPin:\n\t\tfor i, ch := range chs {\n\t\t\tif containsChunk(ch.Address(), chs[:i]...) {\n\t\t\t\texist[i] = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texists, c, err := db.putUpload(batch, binIDs, chunkToItem(ch))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\texist[i] = exists\n\t\t\tif !exists {\n\t\t\t\t// chunk is new so, trigger subscription feeds\n\t\t\t\t// after the batch is successfully written\n\t\t\t\ttriggerPullFeed[db.po(ch.Address())] = struct{}{}\n\t\t\t\ttriggerPushFeed = true\n\t\t\t}\n\t\t\tgcSizeChange += c\n\t\t\tif mode == storage.ModePutUploadPin {\n\t\t\t\terr = db.setPin(batch, ch.Address())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\tcase storage.ModePutSync:\n\t\tfor i, ch := range chs {\n\t\t\tif containsChunk(ch.Address(), chs[:i]...) {\n\t\t\t\texist[i] = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texists, c, err := db.putSync(batch, binIDs, chunkToItem(ch))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\texist[i] = exists\n\t\t\tif !exists {\n\t\t\t\t// chunk is new so, trigger pull subscription feed\n\t\t\t\t// after the batch is successfully written\n\t\t\t\ttriggerPullFeed[db.po(ch.Address())] = struct{}{}\n\t\t\t}\n\t\t\tgcSizeChange += c\n\t\t}\n\n\tdefault:\n\t\treturn nil, ErrInvalidMode\n\t}\n\n\tfor po, id := range binIDs {\n\t\tdb.binIDs.PutInBatch(batch, uint64(po), id)\n\t}\n\n\terr = db.incGCSizeInBatch(batch, gcSizeChange)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = db.shed.WriteBatch(batch)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor po := range triggerPullFeed {\n\t\tdb.triggerPullSubscriptions(po)\n\t}\n\tif triggerPushFeed {\n\t\tdb.triggerPushSubscriptions()\n\t}\n\treturn exist, nil\n}\n\n// putRequest adds an Item to the batch by updating required indexes:\n//  - put to indexes: retrieve, gc\n//  - it does not enter the syncpool\n// The batch can be written to the database.\n// Provided batch and binID map are updated.\nfunc (db *DB) putRequest(batch *leveldb.Batch, binIDs map[uint8]uint64, item shed.Item) (exists bool, gcSizeChange int64, err error) {\n\thas, err := db.retrievalDataIndex.Has(item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\tif has {\n\t\treturn true, 0, nil\n\t}\n\n\titem.StoreTimestamp = now()\n\titem.BinID, err = db.incBinID(binIDs, db.po(swarm.NewAddress(item.Address)))\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\n\tgcSizeChange, err = db.setGC(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\n\terr = db.retrievalDataIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\n\treturn false, gcSizeChange, nil\n}\n\n// putUpload adds an Item to the batch by updating required indexes:\n//  - put to indexes: retrieve, push, pull\n// The batch can be written to the database.\n// Provided batch and binID map are updated.\nfunc (db *DB) putUpload(batch *leveldb.Batch, binIDs map[uint8]uint64, item shed.Item) (exists bool, gcSizeChange int64, err error) {\n\texists, err = db.retrievalDataIndex.Has(item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\tif exists {\n\t\treturn true, 0, nil\n\t}\n\n\titem.StoreTimestamp = now()\n\titem.BinID, err = db.incBinID(binIDs, db.po(swarm.NewAddress(item.Address)))\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\terr = db.retrievalDataIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\terr = db.pullIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\terr = db.pushIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\n\treturn false, 0, nil\n}\n\n// putSync adds an Item to the batch by updating required indexes:\n//  - put to indexes: retrieve, pull, gc\n// The batch can be written to the database.\n// Provided batch and binID map are updated.\nfunc (db *DB) putSync(batch *leveldb.Batch, binIDs map[uint8]uint64, item shed.Item) (exists bool, gcSizeChange int64, err error) {\n\texists, err = db.retrievalDataIndex.Has(item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\tif exists {\n\t\treturn true, 0, nil\n\t}\n\n\titem.StoreTimestamp = now()\n\titem.BinID, err = db.incBinID(binIDs, db.po(swarm.NewAddress(item.Address)))\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\terr = db.retrievalDataIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\terr = db.pullIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\tgcSizeChange, err = db.setGC(batch, item)\n\tif err != nil {\n\t\treturn false, 0, err\n\t}\n\n\treturn false, gcSizeChange, nil\n}\n\n// setGC is a helper function used to add chunks to the retrieval access\n// index and the gc index in the cases that the putToGCCheck condition\n// warrants a gc set. this is to mitigate index leakage in edge cases where\n// a chunk is added to a node's localstore and given that the chunk is\n// already within that node's NN (thus, it can be added to the gc index\n// safely)\nfunc (db *DB) setGC(batch *leveldb.Batch, item shed.Item) (gcSizeChange int64, err error) {\n\tif item.BinID == 0 {\n\t\ti, err := db.retrievalDataIndex.Get(item)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\titem.BinID = i.BinID\n\t}\n\ti, err := db.retrievalAccessIndex.Get(item)\n\tswitch {\n\tcase err == nil:\n\t\titem.AccessTimestamp = i.AccessTimestamp\n\t\terr = db.gcIndex.DeleteInBatch(batch, item)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tgcSizeChange--\n\tcase errors.Is(err, leveldb.ErrNotFound):\n\t\t// the chunk is not accessed before\n\tdefault:\n\t\treturn 0, err\n\t}\n\titem.AccessTimestamp = now()\n\terr = db.retrievalAccessIndex.PutInBatch(batch, item)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\t// add new entry to gc index ONLY if it is not present in pinIndex\n\tok, err := db.pinIndex.Has(item)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif !ok {\n\t\terr = db.gcIndex.PutInBatch(batch, item)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tgcSizeChange++\n\t}\n\n\treturn gcSizeChange, nil\n}\n\n// incBinID is a helper function for db.put* methods that increments bin id\n// based on the current value in the database. This function must be called under\n// a db.batchMu lock. Provided binID map is updated.\nfunc (db *DB) incBinID(binIDs map[uint8]uint64, po uint8) (id uint64, err error) {\n\tif _, ok := binIDs[po]; !ok {\n\t\tbinIDs[po], err = db.binIDs.Get(uint64(po))\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\tbinIDs[po]++\n\treturn binIDs[po], nil\n}\n\n// containsChunk returns true if the chunk with a specific address\n// is present in the provided chunk slice.\nfunc containsChunk(addr swarm.Address, chs ...swarm.Chunk) bool {\n\tfor _, c := range chs {\n\t\tif addr.Equal(c.Address()) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n", "idx": 8, "id": 14093, "msg": "", "proj": "ethersphere-bee", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -20,7 +20,7 @@ import (\n \t\"github.com/go-logr/logr\"\n \n \t\"github.com/pingcap/chaos-mesh/api/v1alpha1\"\n-\t\"github.com/pingcap/chaos-mesh/pkg/apiinterface\"\n+\t\"github.com/pingcap/chaos-mesh/controllers/reconciler\"\n \n \tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n \t\"k8s.io/client-go/util/retry\"", "y": 0, "oldf": "// Copyright 2019 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage common\n\nimport (\n\t\"context\"\n\t\"time\"\n\n\t\"github.com/go-logr/logr\"\n\n\t\"github.com/pingcap/chaos-mesh/api/v1alpha1\"\n\t\"github.com/pingcap/chaos-mesh/pkg/apiinterface\"\n\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/client-go/util/retry\"\n\n\tctrl \"sigs.k8s.io/controller-runtime\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\n// InnerCommonObject used in common chaos reconcile\ntype InnerCommonObject interface {\n\tIsDeleted() bool\n\tapiinterface.StatefulObject\n}\n\n// InnerCommonReconcile used in common chaos reconcile\ntype InnerCommonReconcile interface {\n\tApply(ctx context.Context, req ctrl.Request, chaos InnerCommonObject) error\n\n\tRecover(ctx context.Context, req ctrl.Request, chaos InnerCommonObject) error\n\n\tObject() InnerCommonObject\n}\n\n// Reconciler for common chaos\ntype Reconciler struct {\n\tInnerCommonReconcile\n\tclient.Client\n\tLog logr.Logger\n}\n\n// NewReconciler would create Reconciler for common chaos\nfunc NewReconciler(reconcile InnerCommonReconcile, c client.Client, log logr.Logger) *Reconciler {\n\treturn &Reconciler{\n\t\tInnerCommonReconcile: reconcile,\n\t\tClient:               c,\n\t\tLog:                  log,\n\t}\n}\n\n// Reconcile the common chaos\nfunc (r *Reconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {\n\tvar err error\n\n\tr.Log.Info(\"reconciling a common chaos\", \"name\", req.Name, \"namespace\", req.Namespace)\n\tctx := context.Background()\n\n\tchaos := r.Object()\n\tif err = r.Get(ctx, req.NamespacedName, chaos); err != nil {\n\t\tr.Log.Error(err, \"unable to get chaos\")\n\t\treturn ctrl.Result{}, nil\n\t}\n\tif chaos.IsDeleted() {\n\t\t// This chaos was deleted\n\t\tr.Log.Info(\"Removing self\")\n\t\terr = r.Recover(ctx, req, chaos)\n\t\tif err != nil {\n\t\t\tr.Log.Error(err, \"failed to recover chaos\")\n\t\t\treturn ctrl.Result{Requeue: true}, nil\n\t\t}\n\t}\n\n\t// Start failure action\n\tr.Log.Info(\"Performing Action\")\n\n\tstatus := chaos.GetStatus()\n\n\terr = r.Apply(ctx, req, chaos)\n\tif err != nil {\n\t\tr.Log.Error(err, \"failed to apply chaos action\")\n\n\t\tupdateError := retry.RetryOnConflict(retry.DefaultRetry, func() error {\n\t\t\treturn r.Update(ctx, chaos)\n\t\t})\n\t\tif updateError != nil {\n\t\t\tr.Log.Error(updateError, \"unable to update chaos finalizers\")\n\t\t}\n\n\t\treturn ctrl.Result{Requeue: true}, nil\n\t}\n\tstatus.Experiment.StartTime = &metav1.Time{\n\t\tTime: time.Now(),\n\t}\n\tstatus.Experiment.Phase = v1alpha1.ExperimentPhaseRunning\n\n\tif err := r.Update(ctx, chaos); err != nil {\n\t\tr.Log.Error(err, \"unable to update chaosctl status\")\n\t\treturn ctrl.Result{}, nil\n\t}\n\n\treturn ctrl.Result{}, nil\n}\n", "idx": 1, "id": 13116, "msg": "", "proj": "chaos-mesh-chaos-mesh", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -370,6 +370,7 @@ public class BCrypt {\n \t};\n \tstatic final int MIN_LOG_ROUNDS = 4;\n \tstatic final int MAX_LOG_ROUNDS = 31;\n+\tstatic final int DEFAULT_ROUNDS = 10;\n \n \t// Expanded Blowfish key\n \tprivate int P[];", "y": 1, "oldf": "// Copyright (c) 2006 Damien Miller <djm@mindrot.org>\n//\n// Permission to use, copy, modify, and distribute this software for any\n// purpose with or without fee is hereby granted, provided that the above\n// copyright notice and this permission notice appear in all copies.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n// WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n// MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n// ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n// WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n// ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n// OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\npackage org.springframework.security.crypto.bcrypt;\n\nimport java.nio.charset.StandardCharsets;\nimport java.security.MessageDigest;\nimport java.util.Arrays;\nimport java.security.SecureRandom;\n\n/**\n * BCrypt implements OpenBSD-style Blowfish password hashing using\n * the scheme described in \"A Future-Adaptable Password Scheme\" by\n * Niels Provos and David Mazieres.\n * <p>\n * This password hashing system tries to thwart off-line password\n * cracking using a computationally-intensive hashing algorithm,\n * based on Bruce Schneier's Blowfish cipher. The work factor of\n * the algorithm is parameterised, so it can be increased as\n * computers get faster.\n * <p>\n * Usage is really simple. To hash a password for the first time,\n * call the hashpw method with a random salt, like this:\n * <p>\n * <code>\n * String pw_hash = BCrypt.hashpw(plain_password, BCrypt.gensalt()); <br />\n * </code>\n * <p>\n * To check whether a plaintext password matches one that has been\n * hashed previously, use the checkpw method:\n * <p>\n * <code>\n * if (BCrypt.checkpw(candidate_password, stored_hash))<br />\n * &nbsp;&nbsp;&nbsp;&nbsp;System.out.println(\"It matches\");<br />\n * else<br />\n * &nbsp;&nbsp;&nbsp;&nbsp;System.out.println(\"It does not match\");<br />\n * </code>\n * <p>\n * The gensalt() method takes an optional parameter (log_rounds)\n * that determines the computational complexity of the hashing:\n * <p>\n * <code>\n * String strong_salt = BCrypt.gensalt(10)<br />\n * String stronger_salt = BCrypt.gensalt(12)<br />\n * </code>\n * <p>\n * The amount of work increases exponentially (2**log_rounds), so\n * each increment is twice as much work. The default log_rounds is\n * 10, and the valid range is 4 to 31.\n *\n * @author Damien Miller\n * @version 0.3\n */\npublic class BCrypt {\n\t// BCrypt parameters\n\tprivate static final int GENSALT_DEFAULT_LOG2_ROUNDS = 10;\n\tprivate static final int BCRYPT_SALT_LEN = 16;\n\n\t// Blowfish parameters\n\tprivate static final int BLOWFISH_NUM_ROUNDS = 16;\n\n\t// Initial contents of key schedule\n\tprivate static final int P_orig[] = {\n\t\t\t0x243f6a88, 0x85a308d3, 0x13198a2e, 0x03707344,\n\t\t\t0xa4093822, 0x299f31d0, 0x082efa98, 0xec4e6c89,\n\t\t\t0x452821e6, 0x38d01377, 0xbe5466cf, 0x34e90c6c,\n\t\t\t0xc0ac29b7, 0xc97c50dd, 0x3f84d5b5, 0xb5470917,\n\t\t\t0x9216d5d9, 0x8979fb1b\n\t};\n\tprivate static final int S_orig[] = {\n\t\t\t0xd1310ba6, 0x98dfb5ac, 0x2ffd72db, 0xd01adfb7,\n\t\t\t0xb8e1afed, 0x6a267e96, 0xba7c9045, 0xf12c7f99,\n\t\t\t0x24a19947, 0xb3916cf7, 0x0801f2e2, 0x858efc16,\n\t\t\t0x636920d8, 0x71574e69, 0xa458fea3, 0xf4933d7e,\n\t\t\t0x0d95748f, 0x728eb658, 0x718bcd58, 0x82154aee,\n\t\t\t0x7b54a41d, 0xc25a59b5, 0x9c30d539, 0x2af26013,\n\t\t\t0xc5d1b023, 0x286085f0, 0xca417918, 0xb8db38ef,\n\t\t\t0x8e79dcb0, 0x603a180e, 0x6c9e0e8b, 0xb01e8a3e,\n\t\t\t0xd71577c1, 0xbd314b27, 0x78af2fda, 0x55605c60,\n\t\t\t0xe65525f3, 0xaa55ab94, 0x57489862, 0x63e81440,\n\t\t\t0x55ca396a, 0x2aab10b6, 0xb4cc5c34, 0x1141e8ce,\n\t\t\t0xa15486af, 0x7c72e993, 0xb3ee1411, 0x636fbc2a,\n\t\t\t0x2ba9c55d, 0x741831f6, 0xce5c3e16, 0x9b87931e,\n\t\t\t0xafd6ba33, 0x6c24cf5c, 0x7a325381, 0x28958677,\n\t\t\t0x3b8f4898, 0x6b4bb9af, 0xc4bfe81b, 0x66282193,\n\t\t\t0x61d809cc, 0xfb21a991, 0x487cac60, 0x5dec8032,\n\t\t\t0xef845d5d, 0xe98575b1, 0xdc262302, 0xeb651b88,\n\t\t\t0x23893e81, 0xd396acc5, 0x0f6d6ff3, 0x83f44239,\n\t\t\t0x2e0b4482, 0xa4842004, 0x69c8f04a, 0x9e1f9b5e,\n\t\t\t0x21c66842, 0xf6e96c9a, 0x670c9c61, 0xabd388f0,\n\t\t\t0x6a51a0d2, 0xd8542f68, 0x960fa728, 0xab5133a3,\n\t\t\t0x6eef0b6c, 0x137a3be4, 0xba3bf050, 0x7efb2a98,\n\t\t\t0xa1f1651d, 0x39af0176, 0x66ca593e, 0x82430e88,\n\t\t\t0x8cee8619, 0x456f9fb4, 0x7d84a5c3, 0x3b8b5ebe,\n\t\t\t0xe06f75d8, 0x85c12073, 0x401a449f, 0x56c16aa6,\n\t\t\t0x4ed3aa62, 0x363f7706, 0x1bfedf72, 0x429b023d,\n\t\t\t0x37d0d724, 0xd00a1248, 0xdb0fead3, 0x49f1c09b,\n\t\t\t0x075372c9, 0x80991b7b, 0x25d479d8, 0xf6e8def7,\n\t\t\t0xe3fe501a, 0xb6794c3b, 0x976ce0bd, 0x04c006ba,\n\t\t\t0xc1a94fb6, 0x409f60c4, 0x5e5c9ec2, 0x196a2463,\n\t\t\t0x68fb6faf, 0x3e6c53b5, 0x1339b2eb, 0x3b52ec6f,\n\t\t\t0x6dfc511f, 0x9b30952c, 0xcc814544, 0xaf5ebd09,\n\t\t\t0xbee3d004, 0xde334afd, 0x660f2807, 0x192e4bb3,\n\t\t\t0xc0cba857, 0x45c8740f, 0xd20b5f39, 0xb9d3fbdb,\n\t\t\t0x5579c0bd, 0x1a60320a, 0xd6a100c6, 0x402c7279,\n\t\t\t0x679f25fe, 0xfb1fa3cc, 0x8ea5e9f8, 0xdb3222f8,\n\t\t\t0x3c7516df, 0xfd616b15, 0x2f501ec8, 0xad0552ab,\n\t\t\t0x323db5fa, 0xfd238760, 0x53317b48, 0x3e00df82,\n\t\t\t0x9e5c57bb, 0xca6f8ca0, 0x1a87562e, 0xdf1769db,\n\t\t\t0xd542a8f6, 0x287effc3, 0xac6732c6, 0x8c4f5573,\n\t\t\t0x695b27b0, 0xbbca58c8, 0xe1ffa35d, 0xb8f011a0,\n\t\t\t0x10fa3d98, 0xfd2183b8, 0x4afcb56c, 0x2dd1d35b,\n\t\t\t0x9a53e479, 0xb6f84565, 0xd28e49bc, 0x4bfb9790,\n\t\t\t0xe1ddf2da, 0xa4cb7e33, 0x62fb1341, 0xcee4c6e8,\n\t\t\t0xef20cada, 0x36774c01, 0xd07e9efe, 0x2bf11fb4,\n\t\t\t0x95dbda4d, 0xae909198, 0xeaad8e71, 0x6b93d5a0,\n\t\t\t0xd08ed1d0, 0xafc725e0, 0x8e3c5b2f, 0x8e7594b7,\n\t\t\t0x8ff6e2fb, 0xf2122b64, 0x8888b812, 0x900df01c,\n\t\t\t0x4fad5ea0, 0x688fc31c, 0xd1cff191, 0xb3a8c1ad,\n\t\t\t0x2f2f2218, 0xbe0e1777, 0xea752dfe, 0x8b021fa1,\n\t\t\t0xe5a0cc0f, 0xb56f74e8, 0x18acf3d6, 0xce89e299,\n\t\t\t0xb4a84fe0, 0xfd13e0b7, 0x7cc43b81, 0xd2ada8d9,\n\t\t\t0x165fa266, 0x80957705, 0x93cc7314, 0x211a1477,\n\t\t\t0xe6ad2065, 0x77b5fa86, 0xc75442f5, 0xfb9d35cf,\n\t\t\t0xebcdaf0c, 0x7b3e89a0, 0xd6411bd3, 0xae1e7e49,\n\t\t\t0x00250e2d, 0x2071b35e, 0x226800bb, 0x57b8e0af,\n\t\t\t0x2464369b, 0xf009b91e, 0x5563911d, 0x59dfa6aa,\n\t\t\t0x78c14389, 0xd95a537f, 0x207d5ba2, 0x02e5b9c5,\n\t\t\t0x83260376, 0x6295cfa9, 0x11c81968, 0x4e734a41,\n\t\t\t0xb3472dca, 0x7b14a94a, 0x1b510052, 0x9a532915,\n\t\t\t0xd60f573f, 0xbc9bc6e4, 0x2b60a476, 0x81e67400,\n\t\t\t0x08ba6fb5, 0x571be91f, 0xf296ec6b, 0x2a0dd915,\n\t\t\t0xb6636521, 0xe7b9f9b6, 0xff34052e, 0xc5855664,\n\t\t\t0x53b02d5d, 0xa99f8fa1, 0x08ba4799, 0x6e85076a,\n\t\t\t0x4b7a70e9, 0xb5b32944, 0xdb75092e, 0xc4192623,\n\t\t\t0xad6ea6b0, 0x49a7df7d, 0x9cee60b8, 0x8fedb266,\n\t\t\t0xecaa8c71, 0x699a17ff, 0x5664526c, 0xc2b19ee1,\n\t\t\t0x193602a5, 0x75094c29, 0xa0591340, 0xe4183a3e,\n\t\t\t0x3f54989a, 0x5b429d65, 0x6b8fe4d6, 0x99f73fd6,\n\t\t\t0xa1d29c07, 0xefe830f5, 0x4d2d38e6, 0xf0255dc1,\n\t\t\t0x4cdd2086, 0x8470eb26, 0x6382e9c6, 0x021ecc5e,\n\t\t\t0x09686b3f, 0x3ebaefc9, 0x3c971814, 0x6b6a70a1,\n\t\t\t0x687f3584, 0x52a0e286, 0xb79c5305, 0xaa500737,\n\t\t\t0x3e07841c, 0x7fdeae5c, 0x8e7d44ec, 0x5716f2b8,\n\t\t\t0xb03ada37, 0xf0500c0d, 0xf01c1f04, 0x0200b3ff,\n\t\t\t0xae0cf51a, 0x3cb574b2, 0x25837a58, 0xdc0921bd,\n\t\t\t0xd19113f9, 0x7ca92ff6, 0x94324773, 0x22f54701,\n\t\t\t0x3ae5e581, 0x37c2dadc, 0xc8b57634, 0x9af3dda7,\n\t\t\t0xa9446146, 0x0fd0030e, 0xecc8c73e, 0xa4751e41,\n\t\t\t0xe238cd99, 0x3bea0e2f, 0x3280bba1, 0x183eb331,\n\t\t\t0x4e548b38, 0x4f6db908, 0x6f420d03, 0xf60a04bf,\n\t\t\t0x2cb81290, 0x24977c79, 0x5679b072, 0xbcaf89af,\n\t\t\t0xde9a771f, 0xd9930810, 0xb38bae12, 0xdccf3f2e,\n\t\t\t0x5512721f, 0x2e6b7124, 0x501adde6, 0x9f84cd87,\n\t\t\t0x7a584718, 0x7408da17, 0xbc9f9abc, 0xe94b7d8c,\n\t\t\t0xec7aec3a, 0xdb851dfa, 0x63094366, 0xc464c3d2,\n\t\t\t0xef1c1847, 0x3215d908, 0xdd433b37, 0x24c2ba16,\n\t\t\t0x12a14d43, 0x2a65c451, 0x50940002, 0x133ae4dd,\n\t\t\t0x71dff89e, 0x10314e55, 0x81ac77d6, 0x5f11199b,\n\t\t\t0x043556f1, 0xd7a3c76b, 0x3c11183b, 0x5924a509,\n\t\t\t0xf28fe6ed, 0x97f1fbfa, 0x9ebabf2c, 0x1e153c6e,\n\t\t\t0x86e34570, 0xeae96fb1, 0x860e5e0a, 0x5a3e2ab3,\n\t\t\t0x771fe71c, 0x4e3d06fa, 0x2965dcb9, 0x99e71d0f,\n\t\t\t0x803e89d6, 0x5266c825, 0x2e4cc978, 0x9c10b36a,\n\t\t\t0xc6150eba, 0x94e2ea78, 0xa5fc3c53, 0x1e0a2df4,\n\t\t\t0xf2f74ea7, 0x361d2b3d, 0x1939260f, 0x19c27960,\n\t\t\t0x5223a708, 0xf71312b6, 0xebadfe6e, 0xeac31f66,\n\t\t\t0xe3bc4595, 0xa67bc883, 0xb17f37d1, 0x018cff28,\n\t\t\t0xc332ddef, 0xbe6c5aa5, 0x65582185, 0x68ab9802,\n\t\t\t0xeecea50f, 0xdb2f953b, 0x2aef7dad, 0x5b6e2f84,\n\t\t\t0x1521b628, 0x29076170, 0xecdd4775, 0x619f1510,\n\t\t\t0x13cca830, 0xeb61bd96, 0x0334fe1e, 0xaa0363cf,\n\t\t\t0xb5735c90, 0x4c70a239, 0xd59e9e0b, 0xcbaade14,\n\t\t\t0xeecc86bc, 0x60622ca7, 0x9cab5cab, 0xb2f3846e,\n\t\t\t0x648b1eaf, 0x19bdf0ca, 0xa02369b9, 0x655abb50,\n\t\t\t0x40685a32, 0x3c2ab4b3, 0x319ee9d5, 0xc021b8f7,\n\t\t\t0x9b540b19, 0x875fa099, 0x95f7997e, 0x623d7da8,\n\t\t\t0xf837889a, 0x97e32d77, 0x11ed935f, 0x16681281,\n\t\t\t0x0e358829, 0xc7e61fd6, 0x96dedfa1, 0x7858ba99,\n\t\t\t0x57f584a5, 0x1b227263, 0x9b83c3ff, 0x1ac24696,\n\t\t\t0xcdb30aeb, 0x532e3054, 0x8fd948e4, 0x6dbc3128,\n\t\t\t0x58ebf2ef, 0x34c6ffea, 0xfe28ed61, 0xee7c3c73,\n\t\t\t0x5d4a14d9, 0xe864b7e3, 0x42105d14, 0x203e13e0,\n\t\t\t0x45eee2b6, 0xa3aaabea, 0xdb6c4f15, 0xfacb4fd0,\n\t\t\t0xc742f442, 0xef6abbb5, 0x654f3b1d, 0x41cd2105,\n\t\t\t0xd81e799e, 0x86854dc7, 0xe44b476a, 0x3d816250,\n\t\t\t0xcf62a1f2, 0x5b8d2646, 0xfc8883a0, 0xc1c7b6a3,\n\t\t\t0x7f1524c3, 0x69cb7492, 0x47848a0b, 0x5692b285,\n\t\t\t0x095bbf00, 0xad19489d, 0x1462b174, 0x23820e00,\n\t\t\t0x58428d2a, 0x0c55f5ea, 0x1dadf43e, 0x233f7061,\n\t\t\t0x3372f092, 0x8d937e41, 0xd65fecf1, 0x6c223bdb,\n\t\t\t0x7cde3759, 0xcbee7460, 0x4085f2a7, 0xce77326e,\n\t\t\t0xa6078084, 0x19f8509e, 0xe8efd855, 0x61d99735,\n\t\t\t0xa969a7aa, 0xc50c06c2, 0x5a04abfc, 0x800bcadc,\n\t\t\t0x9e447a2e, 0xc3453484, 0xfdd56705, 0x0e1e9ec9,\n\t\t\t0xdb73dbd3, 0x105588cd, 0x675fda79, 0xe3674340,\n\t\t\t0xc5c43465, 0x713e38d8, 0x3d28f89e, 0xf16dff20,\n\t\t\t0x153e21e7, 0x8fb03d4a, 0xe6e39f2b, 0xdb83adf7,\n\t\t\t0xe93d5a68, 0x948140f7, 0xf64c261c, 0x94692934,\n\t\t\t0x411520f7, 0x7602d4f7, 0xbcf46b2e, 0xd4a20068,\n\t\t\t0xd4082471, 0x3320f46a, 0x43b7d4b7, 0x500061af,\n\t\t\t0x1e39f62e, 0x97244546, 0x14214f74, 0xbf8b8840,\n\t\t\t0x4d95fc1d, 0x96b591af, 0x70f4ddd3, 0x66a02f45,\n\t\t\t0xbfbc09ec, 0x03bd9785, 0x7fac6dd0, 0x31cb8504,\n\t\t\t0x96eb27b3, 0x55fd3941, 0xda2547e6, 0xabca0a9a,\n\t\t\t0x28507825, 0x530429f4, 0x0a2c86da, 0xe9b66dfb,\n\t\t\t0x68dc1462, 0xd7486900, 0x680ec0a4, 0x27a18dee,\n\t\t\t0x4f3ffea2, 0xe887ad8c, 0xb58ce006, 0x7af4d6b6,\n\t\t\t0xaace1e7c, 0xd3375fec, 0xce78a399, 0x406b2a42,\n\t\t\t0x20fe9e35, 0xd9f385b9, 0xee39d7ab, 0x3b124e8b,\n\t\t\t0x1dc9faf7, 0x4b6d1856, 0x26a36631, 0xeae397b2,\n\t\t\t0x3a6efa74, 0xdd5b4332, 0x6841e7f7, 0xca7820fb,\n\t\t\t0xfb0af54e, 0xd8feb397, 0x454056ac, 0xba489527,\n\t\t\t0x55533a3a, 0x20838d87, 0xfe6ba9b7, 0xd096954b,\n\t\t\t0x55a867bc, 0xa1159a58, 0xcca92963, 0x99e1db33,\n\t\t\t0xa62a4a56, 0x3f3125f9, 0x5ef47e1c, 0x9029317c,\n\t\t\t0xfdf8e802, 0x04272f70, 0x80bb155c, 0x05282ce3,\n\t\t\t0x95c11548, 0xe4c66d22, 0x48c1133f, 0xc70f86dc,\n\t\t\t0x07f9c9ee, 0x41041f0f, 0x404779a4, 0x5d886e17,\n\t\t\t0x325f51eb, 0xd59bc0d1, 0xf2bcc18f, 0x41113564,\n\t\t\t0x257b7834, 0x602a9c60, 0xdff8e8a3, 0x1f636c1b,\n\t\t\t0x0e12b4c2, 0x02e1329e, 0xaf664fd1, 0xcad18115,\n\t\t\t0x6b2395e0, 0x333e92e1, 0x3b240b62, 0xeebeb922,\n\t\t\t0x85b2a20e, 0xe6ba0d99, 0xde720c8c, 0x2da2f728,\n\t\t\t0xd0127845, 0x95b794fd, 0x647d0862, 0xe7ccf5f0,\n\t\t\t0x5449a36f, 0x877d48fa, 0xc39dfd27, 0xf33e8d1e,\n\t\t\t0x0a476341, 0x992eff74, 0x3a6f6eab, 0xf4f8fd37,\n\t\t\t0xa812dc60, 0xa1ebddf8, 0x991be14c, 0xdb6e6b0d,\n\t\t\t0xc67b5510, 0x6d672c37, 0x2765d43b, 0xdcd0e804,\n\t\t\t0xf1290dc7, 0xcc00ffa3, 0xb5390f92, 0x690fed0b,\n\t\t\t0x667b9ffb, 0xcedb7d9c, 0xa091cf0b, 0xd9155ea3,\n\t\t\t0xbb132f88, 0x515bad24, 0x7b9479bf, 0x763bd6eb,\n\t\t\t0x37392eb3, 0xcc115979, 0x8026e297, 0xf42e312d,\n\t\t\t0x6842ada7, 0xc66a2b3b, 0x12754ccc, 0x782ef11c,\n\t\t\t0x6a124237, 0xb79251e7, 0x06a1bbe6, 0x4bfb6350,\n\t\t\t0x1a6b1018, 0x11caedfa, 0x3d25bdd8, 0xe2e1c3c9,\n\t\t\t0x44421659, 0x0a121386, 0xd90cec6e, 0xd5abea2a,\n\t\t\t0x64af674e, 0xda86a85f, 0xbebfe988, 0x64e4c3fe,\n\t\t\t0x9dbc8057, 0xf0f7c086, 0x60787bf8, 0x6003604d,\n\t\t\t0xd1fd8346, 0xf6381fb0, 0x7745ae04, 0xd736fccc,\n\t\t\t0x83426b33, 0xf01eab71, 0xb0804187, 0x3c005e5f,\n\t\t\t0x77a057be, 0xbde8ae24, 0x55464299, 0xbf582e61,\n\t\t\t0x4e58f48f, 0xf2ddfda2, 0xf474ef38, 0x8789bdc2,\n\t\t\t0x5366f9c3, 0xc8b38e74, 0xb475f255, 0x46fcd9b9,\n\t\t\t0x7aeb2661, 0x8b1ddf84, 0x846a0e79, 0x915f95e2,\n\t\t\t0x466e598e, 0x20b45770, 0x8cd55591, 0xc902de4c,\n\t\t\t0xb90bace1, 0xbb8205d0, 0x11a86248, 0x7574a99e,\n\t\t\t0xb77f19b6, 0xe0a9dc09, 0x662d09a1, 0xc4324633,\n\t\t\t0xe85a1f02, 0x09f0be8c, 0x4a99a025, 0x1d6efe10,\n\t\t\t0x1ab93d1d, 0x0ba5a4df, 0xa186f20f, 0x2868f169,\n\t\t\t0xdcb7da83, 0x573906fe, 0xa1e2ce9b, 0x4fcd7f52,\n\t\t\t0x50115e01, 0xa70683fa, 0xa002b5c4, 0x0de6d027,\n\t\t\t0x9af88c27, 0x773f8641, 0xc3604c06, 0x61a806b5,\n\t\t\t0xf0177a28, 0xc0f586e0, 0x006058aa, 0x30dc7d62,\n\t\t\t0x11e69ed7, 0x2338ea63, 0x53c2dd94, 0xc2c21634,\n\t\t\t0xbbcbee56, 0x90bcb6de, 0xebfc7da1, 0xce591d76,\n\t\t\t0x6f05e409, 0x4b7c0188, 0x39720a3d, 0x7c927c24,\n\t\t\t0x86e3725f, 0x724d9db9, 0x1ac15bb4, 0xd39eb8fc,\n\t\t\t0xed545578, 0x08fca5b5, 0xd83d7cd3, 0x4dad0fc4,\n\t\t\t0x1e50ef5e, 0xb161e6f8, 0xa28514d9, 0x6c51133c,\n\t\t\t0x6fd5c7e7, 0x56e14ec4, 0x362abfce, 0xddc6c837,\n\t\t\t0xd79a3234, 0x92638212, 0x670efa8e, 0x406000e0,\n\t\t\t0x3a39ce37, 0xd3faf5cf, 0xabc27737, 0x5ac52d1b,\n\t\t\t0x5cb0679e, 0x4fa33742, 0xd3822740, 0x99bc9bbe,\n\t\t\t0xd5118e9d, 0xbf0f7315, 0xd62d1c7e, 0xc700c47b,\n\t\t\t0xb78c1b6b, 0x21a19045, 0xb26eb1be, 0x6a366eb4,\n\t\t\t0x5748ab2f, 0xbc946e79, 0xc6a376d2, 0x6549c2c8,\n\t\t\t0x530ff8ee, 0x468dde7d, 0xd5730a1d, 0x4cd04dc6,\n\t\t\t0x2939bbdb, 0xa9ba4650, 0xac9526e8, 0xbe5ee304,\n\t\t\t0xa1fad5f0, 0x6a2d519a, 0x63ef8ce2, 0x9a86ee22,\n\t\t\t0xc089c2b8, 0x43242ef6, 0xa51e03aa, 0x9cf2d0a4,\n\t\t\t0x83c061ba, 0x9be96a4d, 0x8fe51550, 0xba645bd6,\n\t\t\t0x2826a2f9, 0xa73a3ae1, 0x4ba99586, 0xef5562e9,\n\t\t\t0xc72fefd3, 0xf752f7da, 0x3f046f69, 0x77fa0a59,\n\t\t\t0x80e4a915, 0x87b08601, 0x9b09e6ad, 0x3b3ee593,\n\t\t\t0xe990fd5a, 0x9e34d797, 0x2cf0b7d9, 0x022b8b51,\n\t\t\t0x96d5ac3a, 0x017da67d, 0xd1cf3ed6, 0x7c7d2d28,\n\t\t\t0x1f9f25cf, 0xadf2b89b, 0x5ad6b472, 0x5a88f54c,\n\t\t\t0xe029ac71, 0xe019a5e6, 0x47b0acfd, 0xed93fa9b,\n\t\t\t0xe8d3c48d, 0x283b57cc, 0xf8d56629, 0x79132e28,\n\t\t\t0x785f0191, 0xed756055, 0xf7960e44, 0xe3d35e8c,\n\t\t\t0x15056dd4, 0x88f46dba, 0x03a16125, 0x0564f0bd,\n\t\t\t0xc3eb9e15, 0x3c9057a2, 0x97271aec, 0xa93a072a,\n\t\t\t0x1b3f6d9b, 0x1e6321f5, 0xf59c66fb, 0x26dcf319,\n\t\t\t0x7533d928, 0xb155fdf5, 0x03563482, 0x8aba3cbb,\n\t\t\t0x28517711, 0xc20ad9f8, 0xabcc5167, 0xccad925f,\n\t\t\t0x4de81751, 0x3830dc8e, 0x379d5862, 0x9320f991,\n\t\t\t0xea7a90c2, 0xfb3e7bce, 0x5121ce64, 0x774fbe32,\n\t\t\t0xa8b6e37e, 0xc3293d46, 0x48de5369, 0x6413e680,\n\t\t\t0xa2ae0810, 0xdd6db224, 0x69852dfd, 0x09072166,\n\t\t\t0xb39a460a, 0x6445c0dd, 0x586cdecf, 0x1c20c8ae,\n\t\t\t0x5bbef7dd, 0x1b588d40, 0xccd2017f, 0x6bb4e3bb,\n\t\t\t0xdda26a7e, 0x3a59ff45, 0x3e350a44, 0xbcb4cdd5,\n\t\t\t0x72eacea8, 0xfa6484bb, 0x8d6612ae, 0xbf3c6f47,\n\t\t\t0xd29be463, 0x542f5d9e, 0xaec2771b, 0xf64e6370,\n\t\t\t0x740e0d8d, 0xe75b1357, 0xf8721671, 0xaf537d5d,\n\t\t\t0x4040cb08, 0x4eb4e2cc, 0x34d2466a, 0x0115af84,\n\t\t\t0xe1b00428, 0x95983a1d, 0x06b89fb4, 0xce6ea048,\n\t\t\t0x6f3f3b82, 0x3520ab82, 0x011a1d4b, 0x277227f8,\n\t\t\t0x611560b1, 0xe7933fdc, 0xbb3a792b, 0x344525bd,\n\t\t\t0xa08839e1, 0x51ce794b, 0x2f32c9b7, 0xa01fbac9,\n\t\t\t0xe01cc87e, 0xbcc7d1f6, 0xcf0111c3, 0xa1e8aac7,\n\t\t\t0x1a908749, 0xd44fbd9a, 0xd0dadecb, 0xd50ada38,\n\t\t\t0x0339c32a, 0xc6913667, 0x8df9317c, 0xe0b12b4f,\n\t\t\t0xf79e59b7, 0x43f5bb3a, 0xf2d519ff, 0x27d9459c,\n\t\t\t0xbf97222c, 0x15e6fc2a, 0x0f91fc71, 0x9b941525,\n\t\t\t0xfae59361, 0xceb69ceb, 0xc2a86459, 0x12baa8d1,\n\t\t\t0xb6c1075e, 0xe3056a0c, 0x10d25065, 0xcb03a442,\n\t\t\t0xe0ec6e0e, 0x1698db3b, 0x4c98a0be, 0x3278e964,\n\t\t\t0x9f1f9532, 0xe0d392df, 0xd3a0342b, 0x8971f21e,\n\t\t\t0x1b0a7441, 0x4ba3348c, 0xc5be7120, 0xc37632d8,\n\t\t\t0xdf359f8d, 0x9b992f2e, 0xe60b6f47, 0x0fe3f11d,\n\t\t\t0xe54cda54, 0x1edad891, 0xce6279cf, 0xcd3e7e6f,\n\t\t\t0x1618b166, 0xfd2c1d05, 0x848fd2c5, 0xf6fb2299,\n\t\t\t0xf523f357, 0xa6327623, 0x93a83531, 0x56cccd02,\n\t\t\t0xacf08162, 0x5a75ebb5, 0x6e163697, 0x88d273cc,\n\t\t\t0xde966292, 0x81b949d0, 0x4c50901b, 0x71c65614,\n\t\t\t0xe6c6c7bd, 0x327a140a, 0x45e1d006, 0xc3f27b9a,\n\t\t\t0xc9aa53fd, 0x62a80f00, 0xbb25bfe2, 0x35bdd2f6,\n\t\t\t0x71126905, 0xb2040222, 0xb6cbcf7c, 0xcd769c2b,\n\t\t\t0x53113ec0, 0x1640e3d3, 0x38abbd60, 0x2547adf0,\n\t\t\t0xba38209c, 0xf746ce76, 0x77afa1c5, 0x20756060,\n\t\t\t0x85cbfe4e, 0x8ae88dd8, 0x7aaaf9b0, 0x4cf9aa7e,\n\t\t\t0x1948c25c, 0x02fb8a8c, 0x01c36ae4, 0xd6ebe1f9,\n\t\t\t0x90d4f869, 0xa65cdea0, 0x3f09252d, 0xc208e69f,\n\t\t\t0xb74e6132, 0xce77e25b, 0x578fdfe3, 0x3ac372e6\n\t};\n\n\t// bcrypt IV: \"OrpheanBeholderScryDoubt\"\n\tstatic private final int bf_crypt_ciphertext[] = {\n\t\t\t0x4f727068, 0x65616e42, 0x65686f6c,\n\t\t\t0x64657253, 0x63727944, 0x6f756274\n\t};\n\n\t// Table for Base64 encoding\n\tstatic private final char base64_code[] = {\n\t\t\t'.', '/', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n\t\t\t'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n\t\t\t'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n\t\t\t'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n\t\t\t'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5',\n\t\t\t'6', '7', '8', '9'\n\t};\n\n\t// Table for Base64 decoding\n\tstatic private final byte index_64[] = {\n\t\t\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t\t\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t\t\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t\t\t-1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n\t\t\t-1, -1, -1, -1, -1, -1, 0, 1, 54, 55,\n\t\t\t56, 57, 58, 59, 60, 61, 62, 63, -1, -1,\n\t\t\t-1, -1, -1, -1, -1, 2, 3, 4, 5, 6,\n\t\t\t7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n\t\t\t17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n\t\t\t-1, -1, -1, -1, -1, -1, 28, 29, 30,\n\t\t\t31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n\t\t\t41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n\t\t\t51, 52, 53, -1, -1, -1, -1, -1\n\t};\n\tstatic final int MIN_LOG_ROUNDS = 4;\n\tstatic final int MAX_LOG_ROUNDS = 31;\n\n\t// Expanded Blowfish key\n\tprivate int P[];\n\tprivate int S[];\n\n\t/**\n\t * Encode a byte array using bcrypt's slightly-modified base64 encoding scheme. Note\n\t * that this is <strong>not</strong> compatible with the standard MIME-base64\n\t * encoding.\n\t *\n\t * @param d the byte array to encode\n\t * @param len the number of bytes to encode\n\t * @param rs the destination buffer for the base64-encoded string\n\t * @exception IllegalArgumentException if the length is invalid\n\t */\n\tstatic void encode_base64(byte d[], int len, StringBuilder rs)\n\t\t\tthrows IllegalArgumentException {\n\t\tint off = 0;\n\t\tint c1, c2;\n\n\t\tif (len <= 0 || len > d.length) {\n\t\t\tthrow new IllegalArgumentException(\"Invalid len\");\n\t\t}\n\n\t\twhile (off < len) {\n\t\t\tc1 = d[off++] & 0xff;\n\t\t\trs.append(base64_code[(c1 >> 2) & 0x3f]);\n\t\t\tc1 = (c1 & 0x03) << 4;\n\t\t\tif (off >= len) {\n\t\t\t\trs.append(base64_code[c1 & 0x3f]);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tc2 = d[off++] & 0xff;\n\t\t\tc1 |= (c2 >> 4) & 0x0f;\n\t\t\trs.append(base64_code[c1 & 0x3f]);\n\t\t\tc1 = (c2 & 0x0f) << 2;\n\t\t\tif (off >= len) {\n\t\t\t\trs.append(base64_code[c1 & 0x3f]);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tc2 = d[off++] & 0xff;\n\t\t\tc1 |= (c2 >> 6) & 0x03;\n\t\t\trs.append(base64_code[c1 & 0x3f]);\n\t\t\trs.append(base64_code[c2 & 0x3f]);\n\t\t}\n\t}\n\n\t/**\n\t * Look up the 3 bits base64-encoded by the specified character,\n\t * range-checking againt conversion table\n\t * @param x\tthe base64-encoded value\n\t * @return\tthe decoded value of x\n\t */\n\tprivate static byte char64(char x) {\n\t\tif ((int) x < 0 || (int) x >= index_64.length)\n\t\t\treturn -1;\n\t\treturn index_64[(int) x];\n\t}\n\n\t/**\n\t * Decode a string encoded using bcrypt's base64 scheme to a\n\t * byte array. Note that this is *not* compatible with\n\t * the standard MIME-base64 encoding.\n\t * @param s\tthe string to decode\n\t * @param maxolen\tthe maximum number of bytes to decode\n\t * @return\tan array containing the decoded bytes\n\t * @throws IllegalArgumentException if maxolen is invalid\n\t */\n\tstatic byte[] decode_base64(String s, int maxolen)\n\t\t\tthrows IllegalArgumentException {\n\t\tStringBuilder rs = new StringBuilder();\n\t\tint off = 0, slen = s.length(), olen = 0;\n\t\tbyte ret[];\n\t\tbyte c1, c2, c3, c4, o;\n\n\t\tif (maxolen <= 0)\n\t\t\tthrow new IllegalArgumentException (\"Invalid maxolen\");\n\n\t\twhile (off < slen - 1 && olen < maxolen) {\n\t\t\tc1 = char64(s.charAt(off++));\n\t\t\tc2 = char64(s.charAt(off++));\n\t\t\tif (c1 == -1 || c2 == -1)\n\t\t\t\tbreak;\n\t\t\to = (byte) (c1 << 2);\n\t\t\to |= (c2 & 0x30) >> 4;\n\t\t\trs.append((char) o);\n\t\t\tif (++olen >= maxolen || off >= slen)\n\t\t\t\tbreak;\n\t\t\tc3 = char64(s.charAt(off++));\n\t\t\tif (c3 == -1)\n\t\t\t\tbreak;\n\t\t\to = (byte) ((c2 & 0x0f) << 4);\n\t\t\to |= (c3 & 0x3c) >> 2;\n\t\t\trs.append((char) o);\n\t\t\tif (++olen >= maxolen || off >= slen)\n\t\t\t\tbreak;\n\t\t\tc4 = char64(s.charAt(off++));\n\t\t\to = (byte) ((c3 & 0x03) << 6);\n\t\t\to |= c4;\n\t\t\trs.append((char) o);\n\t\t\t++olen;\n\t\t}\n\n\t\tret = new byte[olen];\n\t\tfor (off = 0; off < olen; off++)\n\t\t\tret[off] = (byte) rs.charAt(off);\n\t\treturn ret;\n\t}\n\n\t/**\n\t * Blowfish encipher a single 64-bit block encoded as\n\t * two 32-bit halves\n\t * @param lr\tan array containing the two 32-bit half blocks\n\t * @param off\tthe position in the array of the blocks\n\t */\n\tprivate void encipher(int lr[], int off) {\n\t\tint i, n, l = lr[off], r = lr[off + 1];\n\n\t\tl ^= P[0];\n\t\tfor (i = 0; i <= BLOWFISH_NUM_ROUNDS - 2;) {\n\t\t\t// Feistel substitution on left word\n\t\t\tn = S[(l >> 24) & 0xff];\n\t\t\tn += S[0x100 | ((l >> 16) & 0xff)];\n\t\t\tn ^= S[0x200 | ((l >> 8) & 0xff)];\n\t\t\tn += S[0x300 | (l & 0xff)];\n\t\t\tr ^= n ^ P[++i];\n\n\t\t\t// Feistel substitution on right word\n\t\t\tn = S[(r >> 24) & 0xff];\n\t\t\tn += S[0x100 | ((r >> 16) & 0xff)];\n\t\t\tn ^= S[0x200 | ((r >> 8) & 0xff)];\n\t\t\tn += S[0x300 | (r & 0xff)];\n\t\t\tl ^= n ^ P[++i];\n\t\t}\n\t\tlr[off] = r ^ P[BLOWFISH_NUM_ROUNDS + 1];\n\t\tlr[off + 1] = l;\n\t}\n\n\t/**\n\t * Cycically extract a word of key material\n\t * @param data\tthe string to extract the data from\n\t * @param offp\ta \"pointer\" (as a one-entry array) to the\n\t * current offset into data\n\t * @param signp\ta \"pointer\" (as a one-entry array) to the\n\t * cumulative flag for non-benign sign extension\n\t * @return\tcorrect and buggy next word of material from data as int[2]\n\t */\n\tprivate static int[] streamtowords(byte data[], int offp[], int signp[]) {\n\t\tint i;\n\t\tint words[] = { 0, 0 };\n\t\tint off = offp[0];\n\t\tint sign = signp[0];\n\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\twords[0] = (words[0] << 8) | (data[off] & 0xff);\n\t\t\twords[1] = (words[1] << 8) | (int) data[off]; // sign extension bug\n\t\t\tif (i > 0) sign |= words[1] & 0x80;\n\t\t\toff = (off + 1) % data.length;\n\t\t}\n\n\t\toffp[0] = off;\n\t\tsignp[0] = sign;\n\t\treturn words;\n\t}\n\n\t/**\n\t * Cycically extract a word of key material\n\t * @param data\tthe string to extract the data from\n\t * @param offp\ta \"pointer\" (as a one-entry array) to the\n\t * current offset into data\n\t * @return\tthe next word of material from data\n\t */\n\tprivate static int streamtoword(byte data[], int offp[]) {\n\t\tint signp[] = { 0 };\n\t\treturn streamtowords(data, offp, signp)[0];\n\t}\n\n\t/**\n\t * Cycically extract a word of key material, with sign-extension bug\n\t * @param data\tthe string to extract the data from\n\t * @param offp\ta \"pointer\" (as a one-entry array) to the\n\t * current offset into data\n\t * @return\tthe next word of material from data\n\t */\n\tprivate static int streamtoword_bug(byte data[], int offp[]) {\n\t\tint signp[] = { 0 };\n\t\treturn streamtowords(data, offp, signp)[1];\n\t}\n\n\t/**\n\t * Initialise the Blowfish key schedule\n\t */\n\tprivate void init_key() {\n\t\tP = P_orig.clone();\n\t\tS = S_orig.clone();\n\t}\n\n\t/**\n\t * Key the Blowfish cipher\n\t * @param key\tan array containing the key\n\t * @param sign_ext_bug\ttrue to implement the 2x bug\n\t * @param safety\t\tbit 16 is set when the safety measure is requested\n\t */\n\tprivate void key(byte key[], boolean sign_ext_bug, int safety) {\n\t\tint i;\n\t\tint koffp[] = { 0 };\n\t\tint lr[] = { 0, 0 };\n\t\tint plen = P.length, slen = S.length;\n\n\t\tfor (i = 0; i < plen; i++)\n\t\t\tif (!sign_ext_bug)\n\t\t\t\tP[i] = P[i] ^ streamtoword(key, koffp);\n\t\t\telse\n\t\t\t\tP[i] = P[i] ^ streamtoword_bug(key, koffp);\n\n\t\tfor (i = 0; i < plen; i += 2) {\n\t\t\tencipher(lr, 0);\n\t\t\tP[i] = lr[0];\n\t\t\tP[i + 1] = lr[1];\n\t\t}\n\n\t\tfor (i = 0; i < slen; i += 2) {\n\t\t\tencipher(lr, 0);\n\t\t\tS[i] = lr[0];\n\t\t\tS[i + 1] = lr[1];\n\t\t}\n\t}\n\n\t/**\n\t * Perform the \"enhanced key schedule\" step described by\n\t * Provos and Mazieres in \"A Future-Adaptable Password Scheme\"\n\t * https://www.openbsd.org/papers/bcrypt-paper.ps\n\t * @param data\tsalt information\n\t * @param key\tpassword information\n\t * @param sign_ext_bug\ttrue to implement the 2x bug\n\t * @param safety\t\tbit 16 is set when the safety measure is requested\n\t */\n\tprivate void ekskey(byte data[], byte key[],\n\t\t\t\t\t\tboolean sign_ext_bug, int safety) {\n\t\tint i;\n\t\tint koffp[] = { 0 }, doffp[] = { 0 };\n\t\tint lr[] = { 0, 0 };\n\t\tint plen = P.length, slen = S.length;\n\t\tint signp[] = { 0 }; // non-benign sign-extension flag\n\t\tint diff = 0;        // zero iff correct and buggy are same\n\n\t\tfor (i = 0; i < plen; i++) {\n\t\t\tint words[] = streamtowords(key, koffp, signp);\n\t\t\tdiff |= words[0] ^ words[1];\n\t\t\tP[i] = P[i] ^ words[sign_ext_bug ? 1 : 0];\n\t\t}\n\n\t\tint sign = signp[0];\n\n\t\t/*\n\t\t * At this point, \"diff\" is zero iff the correct and buggy algorithms produced\n\t\t * exactly the same result.  If so and if \"sign\" is non-zero, which indicates\n\t\t * that there was a non-benign sign extension, this means that we have a\n\t\t * collision between the correctly computed hash for this password and a set of\n\t\t * passwords that could be supplied to the buggy algorithm.  Our safety measure\n\t\t * is meant to protect from such many-buggy to one-correct collisions, by\n\t\t * deviating from the correct algorithm in such cases.  Let's check for this.\n\t\t */\n\t\tdiff |= diff >> 16; /* still zero iff exact match */\n\t\tdiff &= 0xffff; /* ditto */\n\t\tdiff += 0xffff; /* bit 16 set iff \"diff\" was non-zero (on non-match) */\n\t\tsign <<= 9; /* move the non-benign sign extension flag to bit 16 */\n\t\tsign &= ~diff & safety; /* action needed? */\n\n\t\t/*\n\t\t * If we have determined that we need to deviate from the correct algorithm,\n\t\t * flip bit 16 in initial expanded key.  (The choice of 16 is arbitrary, but\n\t\t * let's stick to it now.  It came out of the approach we used above, and it's\n\t\t * not any worse than any other choice we could make.)\n\t\t *\n\t\t * It is crucial that we don't do the same to the expanded key used in the main\n\t\t * Eksblowfish loop.  By doing it to only one of these two, we deviate from a\n\t\t * state that could be directly specified by a password to the buggy algorithm\n\t\t * (and to the fully correct one as well, but that's a side-effect).\n\t\t */\n\t\tP[0] ^= sign;\n\n\t\tfor (i = 0; i < plen; i += 2) {\n\t\t\tlr[0] ^= streamtoword(data, doffp);\n\t\t\tlr[1] ^= streamtoword(data, doffp);\n\t\t\tencipher(lr, 0);\n\t\t\tP[i] = lr[0];\n\t\t\tP[i + 1] = lr[1];\n\t\t}\n\n\t\tfor (i = 0; i < slen; i += 2) {\n\t\t\tlr[0] ^= streamtoword(data, doffp);\n\t\t\tlr[1] ^= streamtoword(data, doffp);\n\t\t\tencipher(lr, 0);\n\t\t\tS[i] = lr[0];\n\t\t\tS[i + 1] = lr[1];\n\t\t}\n\t}\n\n\tstatic long roundsForLogRounds(int log_rounds) {\n\t\tif (log_rounds < 4 || log_rounds > 31) {\n\t\t\tthrow new IllegalArgumentException(\"Bad number of rounds\");\n\t\t}\n\t\treturn 1L << log_rounds;\n\t}\n\n\t/**\n\t * Perform the central password hashing step in the\n\t * bcrypt scheme\n\t * @param password\tthe password to hash\n\t * @param salt\tthe binary salt to hash with the password\n\t * @param log_rounds\tthe binary logarithm of the number\n\t * of rounds of hashing to apply\n\t * @param sign_ext_bug\ttrue to implement the 2x bug\n\t * @param safety\t\tbit 16 is set when the safety measure is requested\n\t * @return\tan array containing the binary hashed password\n\t */\n\tprivate byte[] crypt_raw(byte password[], byte salt[], int log_rounds,\n\t\t\t\t\t\t\tboolean sign_ext_bug, int safety) {\n\t\tint rounds, i, j;\n\t\tint cdata[] =  bf_crypt_ciphertext.clone();\n\t\tint clen = cdata.length;\n\t\tbyte ret[];\n\n\t\tif (log_rounds < 4 || log_rounds > 31)\n\t\t\tthrow new IllegalArgumentException (\"Bad number of rounds\");\n\t\trounds = 1 << log_rounds;\n\t\tif (salt.length != BCRYPT_SALT_LEN)\n\t\t\tthrow new IllegalArgumentException (\"Bad salt length\");\n\n\t\tinit_key();\n\t\tekskey(salt, password, sign_ext_bug, safety);\n\t\tfor (i = 0; i < rounds; i++) {\n\t\t\tkey(password, sign_ext_bug, safety);\n\t\t\tkey(salt, false, safety);\n\t\t}\n\n\t\tfor (i = 0; i < 64; i++) {\n\t\t\tfor (j = 0; j < (clen >> 1); j++)\n\t\t\t\tencipher(cdata, j << 1);\n\t\t}\n\n\t\tret = new byte[clen * 4];\n\t\tfor (i = 0, j = 0; i < clen; i++) {\n\t\t\tret[j++] = (byte) ((cdata[i] >> 24) & 0xff);\n\t\t\tret[j++] = (byte) ((cdata[i] >> 16) & 0xff);\n\t\t\tret[j++] = (byte) ((cdata[i] >> 8) & 0xff);\n\t\t\tret[j++] = (byte) (cdata[i] & 0xff);\n\t\t}\n\t\treturn ret;\n\t}\n\n\t/**\n\t * Hash a password using the OpenBSD bcrypt scheme\n\t * @param password\tthe password to hash\n\t * @param salt\tthe salt to hash with (perhaps generated\n\t * using BCrypt.gensalt)\n\t * @return\tthe hashed password\n\t */\n\tpublic static String hashpw(String password, String salt) {\n\t\tbyte passwordb[];\n\n\t\tpasswordb = password.getBytes(StandardCharsets.UTF_8);\n\n\t\treturn hashpw(passwordb, salt);\n\t}\n\n\t/**\n\t * Hash a password using the OpenBSD bcrypt scheme\n\t * @param passwordb\tthe password to hash, as a byte array\n\t * @param salt\tthe salt to hash with (perhaps generated\n\t * using BCrypt.gensalt)\n\t * @return\tthe hashed password\n\t */\n\tpublic static String hashpw(byte passwordb[], String salt) {\n\t\tBCrypt B;\n\t\tString real_salt;\n\t\tbyte saltb[], hashed[];\n\t\tchar minor = (char) 0;\n\t\tint rounds, off;\n\t\tStringBuilder rs = new StringBuilder();\n\n\t\tif (salt == null) {\n\t\t\tthrow new IllegalArgumentException(\"salt cannot be null\");\n\t\t}\n\n\t\tint saltLength = salt.length();\n\n\t\tif (saltLength < 28) {\n\t\t\tthrow new IllegalArgumentException(\"Invalid salt\");\n\t\t}\n\n\t\tif (salt.charAt(0) != '$' || salt.charAt(1) != '2')\n\t\t\tthrow new IllegalArgumentException (\"Invalid salt version\");\n\t\tif (salt.charAt(2) == '$')\n\t\t\toff = 3;\n\t\telse {\n\t\t\tminor = salt.charAt(2);\n\t\t\tif ((minor != 'a' && minor != 'x' && minor != 'y' && minor != 'b')\n\t\t\t\t\t|| salt.charAt(3) != '$')\n\t\t\t\tthrow new IllegalArgumentException (\"Invalid salt revision\");\n\t\t\toff = 4;\n\t\t}\n\n\t\t// Extract number of rounds\n\t\tif (salt.charAt(off + 2) > '$')\n\t\t\tthrow new IllegalArgumentException (\"Missing salt rounds\");\n\n\t\tif (off == 4 && saltLength < 29) {\n\t\t\tthrow new IllegalArgumentException(\"Invalid salt\");\n\t\t}\n\t\trounds = Integer.parseInt(salt.substring(off, off + 2));\n\n\t\treal_salt = salt.substring(off + 3, off + 25);\n\t\tsaltb = decode_base64(real_salt, BCRYPT_SALT_LEN);\n\n\t\tif (minor >= 'a') // add null terminator\n\t\t\tpasswordb = Arrays.copyOf(passwordb, passwordb.length + 1);\n\n\t\tB = new BCrypt();\n\t\thashed = B.crypt_raw(passwordb, saltb, rounds, minor == 'x', minor == 'a' ? 0x10000 : 0);\n\n\t\trs.append(\"$2\");\n\t\tif (minor >= 'a')\n\t\t\trs.append(minor);\n\t\trs.append(\"$\");\n\t\tif (rounds < 10)\n\t\t\trs.append(\"0\");\n\t\trs.append(rounds);\n\t\trs.append(\"$\");\n\t\tencode_base64(saltb, saltb.length, rs);\n\t\tencode_base64(hashed, bf_crypt_ciphertext.length * 4 - 1, rs);\n\t\treturn rs.toString();\n\t}\n\n\t/**\n\t * Generate a salt for use with the BCrypt.hashpw() method\n\t * @param prefix\t\tthe prefix value (default $2a)\n\t * @param log_rounds\tthe log2 of the number of rounds of\n\t * hashing to apply - the work factor therefore increases as\n\t * 2**log_rounds.\n\t * @param random\t\tan instance of SecureRandom to use\n\t * @return\tan encoded salt value\n\t * @exception IllegalArgumentException if prefix or log_rounds is invalid\n\t */\n\tpublic static String gensalt(String prefix, int log_rounds, SecureRandom random)\n\t\t\tthrows IllegalArgumentException {\n\t\tStringBuilder rs = new StringBuilder();\n\t\tbyte rnd[] = new byte[BCRYPT_SALT_LEN];\n\n\t\tif (!prefix.startsWith(\"$2\") ||\n\t\t\t\t(prefix.charAt(2) != 'a' && prefix.charAt(2) != 'y' &&\n\t\t\t\t\t\tprefix.charAt(2) != 'b')) {\n\t\t\tthrow new IllegalArgumentException (\"Invalid prefix\");\n\t\t}\n\t\tif (log_rounds < 4 || log_rounds > 31) {\n\t\t\tthrow new IllegalArgumentException (\"Invalid log_rounds\");\n\t\t}\n\n\t\trandom.nextBytes(rnd);\n\n\t\trs.append(\"$2\");\n\t\trs.append(prefix.charAt(2));\n\t\trs.append(\"$\");\n\t\tif (log_rounds < 10)\n\t\t\trs.append(\"0\");\n\t\trs.append(log_rounds);\n\t\trs.append(\"$\");\n\t\tencode_base64(rnd, rnd.length, rs);\n\t\treturn rs.toString();\n\t}\n\n\t/**\n\t * Generate a salt for use with the BCrypt.hashpw() method\n\t * @param prefix\t\tthe prefix value (default $2a)\n\t * @param log_rounds\tthe log2 of the number of rounds of\n\t * hashing to apply - the work factor therefore increases as\n\t * 2**log_rounds.\n\t * @return\tan encoded salt value\n\t * @exception IllegalArgumentException if prefix or log_rounds is invalid\n\t */\n\tpublic static String gensalt(String prefix, int log_rounds)\n\t\t\tthrows IllegalArgumentException {\n\t\treturn gensalt(prefix, log_rounds, new SecureRandom());\n\t}\n\n\t/**\n\t * Generate a salt for use with the BCrypt.hashpw() method\n\t * @param log_rounds\tthe log2 of the number of rounds of\n\t * hashing to apply - the work factor therefore increases as\n\t * 2**log_rounds.\n\t * @param random\t\tan instance of SecureRandom to use\n\t * @return\tan encoded salt value\n\t * @exception IllegalArgumentException if log_rounds is invalid\n\t */\n\tpublic static String gensalt(int log_rounds, SecureRandom random)\n\t\t\tthrows IllegalArgumentException {\n\t\treturn gensalt(\"$2a\", log_rounds, random);\n\t}\n\n\t/**\n\t * Generate a salt for use with the BCrypt.hashpw() method\n\t * @param log_rounds\tthe log2 of the number of rounds of\n\t * hashing to apply - the work factor therefore increases as\n\t * 2**log_rounds.\n\t * @return\tan encoded salt value\n\t * @exception IllegalArgumentException if log_rounds is invalid\n\t */\n\tpublic static String gensalt(int log_rounds)\n\t\t\tthrows IllegalArgumentException {\n\t\treturn gensalt(log_rounds, new SecureRandom());\n\t}\n\n\tpublic static String gensalt(String prefix) {\n\t\treturn gensalt(prefix, GENSALT_DEFAULT_LOG2_ROUNDS);\n\t}\n\n\t/**\n\t * Generate a salt for use with the BCrypt.hashpw() method,\n\t * selecting a reasonable default for the number of hashing\n\t * rounds to apply\n\t * @return\tan encoded salt value\n\t */\n\tpublic static String gensalt() {\n\t\treturn gensalt(GENSALT_DEFAULT_LOG2_ROUNDS);\n\t}\n\n\t/**\n\t * Check that a plaintext password matches a previously hashed\n\t * one\n\t * @param plaintext\tthe plaintext password to verify\n\t * @param hashed\tthe previously-hashed password\n\t * @return\ttrue if the passwords match, false otherwise\n\t */\n\tpublic static boolean checkpw(String plaintext, String hashed) {\n\t\treturn equalsNoEarlyReturn(hashed, hashpw(plaintext, hashed));\n\t}\n\n\t/**\n\t * Check that a password (as a byte array) matches a previously hashed\n\t * one\n\t * @param passwordb\tthe password to verify, as a byte array\n\t * @param hashed\tthe previously-hashed password\n\t * @return\ttrue if the passwords match, false otherwise\n\t * @since 5.3\n\t */\n\tpublic static boolean checkpw(byte[] passwordb, String hashed) {\n\t\treturn equalsNoEarlyReturn(hashed, hashpw(passwordb, hashed));\n\t}\n\n\tstatic boolean equalsNoEarlyReturn(String a, String b) {\n\t\treturn MessageDigest.isEqual(a.getBytes(StandardCharsets.UTF_8), b.getBytes(StandardCharsets.UTF_8));\n\t}\n}\n", "idx": 1, "id": 16118, "msg": "I don't want to modify BCrypt unless there is an actual bug. The reason is this is a file we copied from another library and I don't want to deviate from it so that merging changes is easier. For that reason, I'd move this to the BCryptPasswordEncoder", "proj": "spring-projects-spring-security", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -472,6 +472,7 @@ class Realm {\n  * @typedef Realm~PropertyType\n  * @type {(\"bool\"|\"int\"|\"float\"|\"double\"|\"string\"|\"decimal128\"|\"objectId\"|\"date\"|\"data\"|\"list\"|\"linkingObjects\"|\"<ObjectType>\")}\n  *\n+ * @property {Mixed} \"mixed\" - Property value that allow any of the following types (`\"bool\",\"int\",\"float\",\"double\",\"string\",\"decimal128\",\"objectId\",\"date\",\"data\"`), this type is nullable by default.\n  * @property {boolean} \"bool\" - Property value may either be `true` or `false`.\n  * @property {number} \"int\" - Property may be assigned any number, but will be stored as a\n  *   round integer, meaning anything after the decimal will be truncated.", "y": 1, "oldf": "////////////////////////////////////////////////////////////////////////////\n//\n// Copyright 2016 Realm Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n////////////////////////////////////////////////////////////////////////////\n\n/* eslint getter-return: \"off\" */\n\n/**\n * A Realm instance represents a Realm database.\n *\n * ```js\n * const Realm = require('realm');\n * ```\n *\n */\nclass Realm {\n    /**\n     * Indicates if this Realm contains any objects.\n     * @type {boolean}\n     * @readonly\n     * @since 1.10.0\n     */\n    get empty() { }\n\n    /**\n     * The path to the file where this Realm is stored.\n     * @type {string}\n     * @readonly\n     * @since 0.12.0\n     */\n    get path() { }\n\n    /**\n     * Indicates if this Realm was opened as read-only.\n     * @type {boolean}\n     * @readonly\n     * @since 0.12.0\n     */\n    get readOnly() { }\n\n    /**\n     * A normalized representation of the schema provided in the\n     * {@link Realm~Configuration Configuration} when this Realm was constructed.\n     * @type {Realm~ObjectSchema[]}\n     * @readonly\n     * @since 0.12.0\n     */\n    get schema() { }\n\n    /**\n     * The current schema version of this Realm.\n     * @type {number}\n     * @readonly\n     * @since 0.12.0\n     */\n    get schemaVersion() { }\n\n    /**\n     * Indicates if this Realm is in a write transaction.\n     * @type {boolean}\n     * @readonly\n     * @since 1.10.3\n     */\n    get isInTransaction() { }\n\n    /**\n     * Indicates if this Realm has been closed.\n     * @type {boolean}\n     * @readonly\n     * @since 2.1.0\n     */\n    get isClosed() { }\n\n    /**\n     * Gets the sync session if this is a synced Realm\n     * @type {Session}\n     */\n    get syncSession() { }\n\n    /**\n     * Create a new `Realm` instance using the provided `config`. If a Realm does not yet exist\n     * at `config.path` (or {@link Realm.defaultPath} if not provided), then this constructor\n     * will create it with the provided `config.schema` (which is _required_ in this case).\n     * Otherwise, the instance will access the existing Realm from the file at that path.\n     * In this case, `config.schema` is _optional_ or not have changed, unless\n     * `config.schemaVersion` is incremented, in which case the Realm will be automatically\n     * migrated to use the new schema.\n     * In the case of query-based sync, `config.schema` is required. An exception will be\n     * thrown if `config.schema` is not defined.\n     * @param {Realm~Configuration} [config] - **Required** when first creating the Realm.\n     * @throws {Error} If anything in the provided `config` is invalid.\n     * @throws {IncompatibleSyncedRealmError} when an incompatible synced Realm is opened\n     */\n    constructor(config) { }\n\n    /**\n     * Open a Realm asynchronously with a promise. If the Realm is synced, it will be fully\n     * synchronized before it is available.\n     * In the case of query-based sync, `config.schema` is required. An exception will be\n     * thrown if `config.schema` is not defined.\n     * @param {Realm~Configuration} config - if no config is defined, it will open the default realm\n     * @returns {ProgressPromise} - a promise that will be resolved with the Realm instance when it's available.\n     * @throws {Error} If anything in the provided `config` is invalid.\n     */\n    static open(config) { }\n\n    /**\n     * Return a configuration for a default synced Realm. The server URL for the user will be used as base for\n     * the URL for the synced Realm. If no user is supplied, the current user will be used.\n     * @param {Realm.User} - an optional sync user\n     * @throws {Error} if zero or multiple users are logged in\n     * @returns {Realm~Configuration} - a configuration matching a default synced Realm.\n     * @since 2.3.0\n     * @deprecated use {@link Sync.User.createConfiguration()} instead.\n     */\n    static automaticSyncConfiguration(user) { }\n\n    /**\n     * Creates a template object for a Realm model class where all optional fields are `undefined` and all required\n     * fields have the default value for the given data type, either the value set by the `default` property in the\n     * schema or the default value for the datatype if the schema doesn't specify one, i.e. `0`, false and `\"\"`.\n     *\n     * @param {Realm~ObjectSchema} schema object describing the class\n     */\n    static createTemplateObject(objectSchema) { }\n\n    /**\n     * Closes this Realm so it may be re-opened with a newer schema version.\n     * All objects and collections from this Realm are no longer valid after calling this method.\n     * The method is idempotent.\n     */\n    close() { }\n\n    /**\n     * Create a new Realm object of the given type and with the specified properties.\n     * @param {Realm~ObjectType} type - The type of Realm object to create.\n     * @param {Object} properties - Property values for all required properties without a\n     *   default value.\n     * @param {boolean|string} [updateMode='never'] - Optional update mode. It can be one of the following values\n     *     - 'never': Objects are only created. If an existing object exists, an exception is thrown. This is the\n     *       default value.\n     *     - 'all': If an existing object is found, all properties provided will be updated, any other properties will\n     *       remain unchanged.\n     *     - 'modified': If an existing object exists, only properties where the value has actually changed will be\n     *       updated. This improves notifications and server side performance but also have implications for how changes\n     *       across devices are merged. For most use cases, the behaviour will match the intuitive behaviour of how\n     *       changes should be merged, but if updating an entire object is considered an atomic operation, this mode\n     *       should not be used.\n     * @returns {Realm.Object}\n     */\n    create(type, properties, updateMode) {}\n\n    /**\n     * Deletes the provided Realm object, or each one inside the provided collection.\n     * @param {Realm.Object|Realm.Object[]|Realm.List|Realm.Results} object\n     */\n    delete(object) { }\n\n    /**\n     * Deletes a Realm model, including all of its objects.\n     * If called outside a migration function, {@link Realm#schema schema} and {@link Realm#schemaVersion schemaVersion} are updated.\n     * @param {string} name - the model name\n     */\n    deleteModel(name) { }\n\n    /**\n     * **WARNING:** This will delete **all** objects in the Realm!\n     */\n    deleteAll() { }\n\n    /**\n     * Returns all objects of the given `type` in the Realm.\n     * @param {Realm~ObjectType} type - The type of Realm objects to retrieve.\n     * @throws {Error} If type passed into this method is invalid or if the type is marked embedded.\n     * @returns {Realm.Results} that will live-update as objects are created and destroyed.\n     */\n    objects(type) { }\n\n    /**\n     * Searches for a Realm object by its primary key.\n     * @param {Realm~ObjectType} type - The type of Realm object to search for.\n     * @param {number|string} key - The primary key value of the object to search for.\n     * @throws {Error} If type passed into this method is invalid or if the object type did\n     *   not have a `primaryKey` specified in its {@link Realm~ObjectSchema ObjectSchema}.\n     * @returns {Realm.Object|undefined} if no object is found.\n     * @since 0.14.0\n     */\n    objectForPrimaryKey(type, key) { }\n\n    /**\n     * Add a listener `callback` for the specified event `name`.\n     * @param {string} name - The name of event that should cause the callback to be called.\n     *   _Currently, only the \"change\" and \"schema\" events are supported_.\n     * @param {callback(Realm, string)|callback(Realm, string, Schema)} callback - Function to be called when a change event occurs.\n     *   Each callback will only be called once per event, regardless of the number of times\n     *   it was added.\n     * @throws {Error} If an invalid event `name` is supplied, or if `callback` is not a function.\n     */\n    addListener(name, callback) { }\n\n    /**\n     * Remove the listener `callback` for the specfied event `name`.\n     * @param {string} name - The event name.\n     *   _Currently, only the \"change\" and \"schema\" events are supported_.\n     * @param {callback(Realm, string)|callback(Realm, string, Schema)} callback - Function that was previously added as a\n     *   listener for this event through the {@link Realm#addListener addListener} method.\n     * @throws {Error} If an invalid event `name` is supplied, or if `callback` is not a function.\n     */\n    removeListener(name, callback) { }\n\n    /**\n     * Remove all event listeners (restricted to the event `name`, if provided).\n     * @param {string} [name] - The name of the event whose listeners should be removed.\n     *   _Currently, only the \"change\" and \"schema\" events are supported_.\n     * @throws {Error} When invalid event `name` is supplied\n     */\n    removeAllListeners(name) { }\n\n    /**\n     * Synchronously call the provided `callback` inside a write transaction. If an exception happens inside a transaction,\n     * you\u2019ll lose the changes in that transaction, but the Realm itself won\u2019t be affected (or corrupted).\n     * More precisely, {@link Realm#beginTransaction beginTransaction()} and {@link Realm#commitTransaction commitTransaction()} will be called\n     * automatically. If any exception is thrown during the transaction {@link Realm#cancelTransaction cancelTransaction()} will\n     * be called instead of {@link Realm#commitTransaction commitTransaction()} and the exception will be re-thrown to the caller of `write()`.\n     *\n     * Nested transactions (calling `write()` within `write()`) is not possible.\n     * @param {function()} callback\n     */\n    write(callback) { }\n\n    /**\n     * Initiate a write transaction.\n     *\n     * When doing a transaction, it is highly recommended to do error handling.\n     * If you don't handle errors, your data might become inconsistent. Error handling\n     * will often involve canceling the transaction.\n     *\n     * @example\n     * realm.beginTransaction();\n     * try {\n     *   realm.create('Person', { name: 'Arthur Dent',  origin: 'Earth' });\n     *   realm.create('Person', { name: 'Ford Prefect', origin: 'Betelgeuse Five' });\n     *   realm.commitTransaction();\n     * } catch (e) {\n     *   realm.cancelTransaction();\n     *   throw e;\n     * }\n     * @throws {Error} When already in write transaction\n     * @see {@link Realm#cancelTransaction cancelTransaction()}\n     * @see {@link Realm#commitTransaction commitTransaction()}\n     */\n    beginTransaction() { }\n\n    /**\n     * Commit a write transaction.\n     *\n     * @see {@link Realm#beginTransaction beginTransaction()}\n     */\n    commitTransaction() { }\n\n    /**\n     * Cancel a write transaction.\n     *\n     * @see {@link Realm#beginTransaction beginTransaction()}\n     */\n    cancelTransaction() { }\n\n    /**\n     * Replaces all string columns in this Realm with a string enumeration column and compacts the\n     * database file.\n     *\n     * Cannot be called from a write transaction.\n     *\n     * Compaction will not occur if other `Realm` instances exist.\n     *\n     * While compaction is in progress, attempts by other threads or processes to open the database will\n     * wait.\n     *\n     * Be warned that resource requirements for compaction is proportional to the amount of live data in\n     * the database. Compaction works by writing the database contents to a temporary database file and\n     * then replacing the database with the temporary one.\n     * @returns {true} if compaction succeeds.\n     */\n    compact() { }\n\n    /**\n     * Writes a compacted copy of the Realm to the given path.\n     *\n     * The destination file cannot already exist.\n     *\n     * Note that if this method is called from within a write transaction, the current data is written,\n     * not the data from the point when the previous write transaction was committed.\n     * @param {string} path path to save the Realm to\n     * @param {ArrayBuffer|ArrayBufferView} [encryptionKey] - Optional 64-byte encryption key to encrypt the new file with.\n     */\n    writeCopyTo(path, encryptionKey) { }\n\n    /**\n     * Get the current schema version of the Realm at the given path.\n     * @param {string} path - The path to the file where the\n     *   Realm database is stored.\n     * @param {ArrayBuffer|ArrayBufferView} [encryptionKey] - Required only when\n     *   accessing encrypted Realms.\n     * @throws {Error} When passing an invalid or non-matching encryption key.\n     * @returns {number} version of the schema, or `-1` if no Realm exists at `path`.\n     */\n    static schemaVersion(path, encryptionKey) { }\n\n    /**\n     * Delete the Realm file for the given configuration.\n     * @param {Realm~Configuration} config\n     * @throws {Error} If anything in the provided `config` is invalid.\n     */\n    static deleteFile(config) { }\n\n    /**\n     * Checks if the Realm already exists on disk.\n     * @param {Realm~Configuration} config The configuration for the Realm.\n     * @throws {Error} if anything in the provided `config` is invalid.\n     * @returns {boolean} returns `true` if the Realm exists on the device, `false` if not.\n     */\n    static exists(config) { }\n\n    /**\n     * Copy all bundled Realm files to app's default file folder.\n     * This is only implemented for React Native.\n     * @throws {Error} If an I/O error occured or method is not implemented.\n     */\n    static copyBundledRealmFiles() { }\n}\n/**\n * This describes the different options used to create a {@link Realm} instance.\n * @typedef Realm~Configuration\n * @type {Object}\n * @property {ArrayBuffer|ArrayBufferView} [encryptionKey] - The 512-bit (64-byte) encryption\n *   key used to encrypt and decrypt all data in the Realm.\n * @property {callback(Realm, Realm)} [migration] - The function to run if a migration is needed.\n *   This function should provide all the logic for converting data models from previous schemas\n *   to the new schema.\n *   This function takes two arguments:\n *   - `oldRealm` - The Realm before migration is performed.\n *   - `newRealm` - The Realm that uses the latest `schema`, which should be modified as necessary.\n * @property {boolean} [deleteRealmIfMigrationNeeded=false] - Specifies if this Realm should be deleted\n *   if a migration is needed.\n *   This option is not available on synced realms.\n * @property {callback(number, number)} [shouldCompactOnLaunch] - The function called when opening\n *   a Realm for the first time during the life of a process to determine if it should be compacted\n *   before being returned to the user. The function takes two arguments:\n *     - `totalSize` - The total file size (data + free space)\n *     - `usedSize` - The total bytes used by data in the file.\n *   It returns `true` to indicate that an attempt to compact the file should be made. The compaction\n *   will be skipped if another process is accessing it.\n * @property {string} [path={@link Realm.defaultPath}] - The path to the file where the\n *   Realm database should be stored.\n * @property {string} [fifoFilesFallbackPath] - Opening a Realm creates a number of FIFO special files in order to\n * coordinate access to the Realm across threads and processes. If the Realm file is stored in a location\n * that does not allow the creation of FIFO special files (e.g. FAT32 filesystems), then the Realm cannot be opened.\n * In that case Realm needs a different location to store these files and this property defines that location.\n * The FIFO special files are very lightweight and the main Realm file will still be stored in the location defined\n * by the `path` property. This property is ignored if the directory defined by `path` allow FIFO special files.\n * @property {boolean} [inMemory=false] - Specifies if this Realm should be opened in-memory. This\n *    still requires a path (can be the default path) to identify the Realm so other processes can\n *    open the same Realm. The file will also be used as swap space if the Realm becomes bigger than\n *    what fits in memory, but it is not persistent and will be removed when the last instance\n *    is closed.\n * @property {boolean} [readOnly=false] - Specifies if this Realm should be opened as read-only.\n * @property {boolean} [disableFormatUpgrade=false] - Specifies if this Realm's file format should\n *    be automatically upgraded if it was created with an older version of the Realm library.\n *    If set to `true` and a file format upgrade is required, an error will be thrown instead.\n * @property {Array<Realm~ObjectClass|Realm~ObjectSchema>} [schema] - Specifies all the\n *   object types in this Realm. **Required** when first creating a Realm at this `path`.\n *   If omitted, the schema will be read from the existing Realm file.\n * @property {number} [schemaVersion] - **Required** (and must be incremented) after\n *   changing the `schema`.\n * @property {Realm.App.Sync~SyncConfiguration} [sync] - Sync configuration parameters.\n */\n\n/**\n * Realm objects will inherit methods, getters, and setters from the `prototype` of this\n * constructor. It is **highly recommended** that this constructor inherit from\n * {@link Realm.Object}.\n * @typedef Realm~ObjectClass\n * @type {Class}\n * @property {Realm~ObjectSchema} schema - Static property specifying object schema information.\n */\n\n/**\n * @typedef Realm~ObjectSchema\n * @type {Object}\n * @property {string} name - Represents the object type.\n * @property {string} [primaryKey] - The name of a `\"string\"` or `\"int\"` property\n *   that must be unique across all objects of this type within the same Realm.\n * @property {boolean} [embedded] - True if the object type is embedded. An embedded object\n *   can be linked to by at most one parent object. Default value: false.\n * @property {Object<string, (Realm~PropertyType|Realm~ObjectSchemaProperty|Realm~ObjectSchema)>} properties -\n *   An object where the keys are property names and the values represent the property type.\n *\n * @example\n * let MyClassSchema = {\n *     name: 'MyClass',\n *     primaryKey: 'pk',\n *     properties: {\n *         pk: 'int',\n *         optionalFloatValue: 'float?' // or {type: 'float', optional: true}\n *         listOfStrings: 'string[]',\n *         listOfOptionalDates: 'date?[]',\n *         indexedInt: {type: 'int', indexed: true}\n *\n *         linkToObject: 'MyClass',\n *         listOfObjects: 'MyClass[]', // or {type: 'list', objectType: 'MyClass'}\n *         objectsLinkingToThisObject: {type: 'linkingObjects', objectType: 'MyClass', property: 'linkToObject'}\n *     }\n * };\n */\n\n/**\n * @typedef Realm~ObjectSchemaProperty\n * @type {Object}\n * @property {Realm~PropertyType} type - The type of this property.\n * @property {Realm~PropertyType} [objectType] - **Required**  when `type` is `\"list\"` or `\"linkingObjects\"`,\n *   and must match the type of an object in the same schema, or, for `\"list\"`\n *   only, any other type which may be stored as a Realm property.\n * @property {string} [property] - **Required** when `type` is `\"linkingObjects\"`, and must match\n *   the name of a property on the type specified in `objectType` that links to the type this property belongs to.\n * @property {any} [default] - The default value for this property on creation when not\n *   otherwise specified.\n * @property {boolean} [optional] - Signals if this property may be assigned `null` or `undefined`.\n *   For `\"list\"` properties of non-object types, this instead signals whether the values inside the list may be assigned `null` or `undefined`.\n *   This is not supported for `\"list\"` properties of object types and `\"linkingObjects\"` properties.\n * @property {boolean} [indexed] - Signals if this property should be indexed. Only supported for\n *   `\"string\"`, `\"int\"`, and `\"bool\"` properties.\n * @property {string} [mapTo] - Set this to the name of the underlying property in the Realm file if the Javascript property\n *   name is different than the name used in the Realm file. This can e.g. be used to have different naming convention in\n *   Javascript than what is being used in the Realm file. Reading and writing properties must be done using the public\n *   name. Queries can be done using both the public and the underlying property name.\n */\n\n/**\n * The type of an object may either be specified as a string equal to the `name` in a\n * {@link Realm~ObjectSchema ObjectSchema} definition, **or** a constructor that was specified\n * in the {@link Realm~Configuration configuration} `schema`.\n * @typedef Realm~ObjectType\n * @type {string|Realm~ObjectClass}\n */\n\n/**\n * A property type may be specified as one of the standard builtin types, or as\n * an object type inside the same schema.\n *\n * When specifying property types in an {@linkplain Realm~ObjectSchema object schema}, you\n * may append `?` to any of the property types to indicate that it is optional\n * (i.e. it can be `null` in addition to the normal values) and `[]` to\n * indicate that it is instead a list of that type. For example,\n * `optionalIntList: 'int?[]'` would declare a property which is a list of\n * nullable integers. The property types reported by {@linkplain Realm.Collection\n * collections} and in a Realm's schema will never\n * use these forms.\n *\n * @typedef Realm~PropertyType\n * @type {(\"bool\"|\"int\"|\"float\"|\"double\"|\"string\"|\"decimal128\"|\"objectId\"|\"date\"|\"data\"|\"list\"|\"linkingObjects\"|\"<ObjectType>\")}\n *\n * @property {boolean} \"bool\" - Property value may either be `true` or `false`.\n * @property {number} \"int\" - Property may be assigned any number, but will be stored as a\n *   round integer, meaning anything after the decimal will be truncated.\n * @property {number} \"float\" - Property may be assigned any number, but will be stored as a\n *   `float`, which may result in a loss of precision.\n * @property {number} \"double\" - Property may be assigned any number, and will have no loss\n *   of precision.\n * @property {string} \"string\" - Property value may be any arbitrary string.\n * @property {Decimal128} \"decimal128\" - Property value may be a `Decimal128` object (see `bson` for details).\n * @property {ObjectId} \"objectId\" - Property valye may be an `ObjectId` object (see `bson` for details).\n * @property {Date} \"date\" - Property may be assigned any `Date` instance.\n * @property {ArrayBuffer} \"data\" - Property may either be assigned an `ArrayBuffer`\n *   or `ArrayBufferView` (e.g. `DataView`, `Int8Array`, `Float32Array`, etc.) instance,\n *   but will always be returned as an `ArrayBuffer`.\n * @property {Realm.List} \"list\" - Property may be assigned any ordered collection\n *   (e.g. `Array`, {@link Realm.List}, {@link Realm.Results}) of objects all matching the\n *   `objectType` specified in the {@link Realm~ObjectSchemaProperty ObjectSchemaProperty}.\n * @property {Realm.Results} \"linkingObjects\" - Property is read-only and always returns a {@link Realm.Results}\n *   of all the objects matching the `objectType` that are linking to the current object\n *   through the `property` relationship specified in {@link Realm~ObjectSchemaProperty ObjectSchemaProperty}.\n * @property {Realm.Object} \"<ObjectType>\" - A string that matches the `name` of an object in the\n *   same schema (see {@link Realm~ObjectSchema ObjectSchema}) \u2013 this property may be assigned\n *   any object of this type from inside the same Realm, and will always be _optional_\n *   (meaning it may also be assigned `null` or `undefined`).\n */\n", "idx": 1, "id": 20001, "msg": "Mixed can also hold objects, not only primitives.", "proj": "realm-realm-js", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -17,8 +17,47 @@\n \n package org.openqa.grid.common;\n \n+import static org.openqa.grid.common.RegistrationRequest.PATH;\n+import static org.openqa.grid.common.RegistrationRequest.SELENIUM_PROTOCOL;\n+\n+import org.openqa.grid.common.exception.GridException;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+\n public enum SeleniumProtocol {\n-  Selenium, WebDriver;\n+  Selenium(\"/selenium-server/driver\"),\n+  WebDriver(\"/wd/hub\");\n+  private String path;\n+\n+  SeleniumProtocol(String path) {\n+    this.path = path;\n+  }\n+\n+  public static SeleniumProtocol fromCapabilitiesMap(Map<String, ?> capabilities) {\n+    String type = (String) capabilities.get(SELENIUM_PROTOCOL);\n+    if (type == null || type.trim().isEmpty()) {\n+      return WebDriver;\n+    }\n+    try {\n+      return SeleniumProtocol.valueOf(type);\n+    } catch (IllegalArgumentException e) {\n+      throw new GridException(type + \" isn't a valid protocol type for grid. Valid values :[\" +\n+                              Arrays.toString(values()) + \"]\", e);\n+    }\n+  }\n+\n+  public String getPathConsideringCapabilitiesMap(Map<String, ?> capabilities) {\n+    String localPath = (String) capabilities.get(PATH);\n+    if (localPath != null) {\n+      return localPath;\n+    }\n+    return path;\n+  }\n+\n+  public String getPath() {\n+    return path;\n+  }\n \n   public boolean isSelenium() {\n     return Selenium.equals(this);", "y": 1, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.grid.common;\n\npublic enum SeleniumProtocol {\n  Selenium, WebDriver;\n\n  public boolean isSelenium() {\n    return Selenium.equals(this);\n  }\n}\n", "idx": 1, "id": 14093, "msg": "But it's not considering \"desired\" capabilities anymore. It's just `capabilities` so the method name needs to better reflect this. Same with `fromDesiredCapablities` above.", "proj": "SeleniumHQ-selenium", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -18,6 +18,7 @@ require 'travis/build/script/crystal'\n require 'travis/build/script/csharp'\n require 'travis/build/script/d'\n require 'travis/build/script/dart'\n+require 'travis/build/script/elm'\n require 'travis/build/script/erlang'\n require 'travis/build/script/elixir'\n require 'travis/build/script/go'", "y": 1, "oldf": "require 'core_ext/hash/deep_merge'\nrequire 'core_ext/hash/deep_symbolize_keys'\nrequire 'core_ext/object/false'\nrequire 'erb'\nrequire 'rbconfig'\n\nrequire 'travis/build/addons'\nrequire 'travis/build/appliances'\nrequire 'travis/build/git'\nrequire 'travis/build/helpers'\nrequire 'travis/build/stages'\n\nrequire 'travis/build/script/android'\nrequire 'travis/build/script/c'\nrequire 'travis/build/script/clojure'\nrequire 'travis/build/script/cpp'\nrequire 'travis/build/script/crystal'\nrequire 'travis/build/script/csharp'\nrequire 'travis/build/script/d'\nrequire 'travis/build/script/dart'\nrequire 'travis/build/script/erlang'\nrequire 'travis/build/script/elixir'\nrequire 'travis/build/script/go'\nrequire 'travis/build/script/groovy'\nrequire 'travis/build/script/generic'\nrequire 'travis/build/script/haskell'\nrequire 'travis/build/script/haxe'\nrequire 'travis/build/script/julia'\nrequire 'travis/build/script/nix'\nrequire 'travis/build/script/node_js'\nrequire 'travis/build/script/objective_c'\nrequire 'travis/build/script/perl'\nrequire 'travis/build/script/perl6'\nrequire 'travis/build/script/php'\nrequire 'travis/build/script/pure_java'\nrequire 'travis/build/script/python'\nrequire 'travis/build/script/r'\nrequire 'travis/build/script/ruby'\nrequire 'travis/build/script/rust'\nrequire 'travis/build/script/scala'\nrequire 'travis/build/script/smalltalk'\nrequire 'travis/build/script/shared/directory_cache'\n\nmodule Travis\n  module Build\n    class Script\n      TEMPLATES_PATH = File.expand_path('../templates', __FILE__)\n      DEFAULTS = {}\n\n      class << self\n        def defaults\n          Git::DEFAULTS.merge(self::DEFAULTS)\n        end\n      end\n\n      include Module.new { Stages::STAGES.each_slice(2).map(&:last).flatten.each { |stage| define_method(stage) {} } }\n      include Appliances, DirectoryCache, Deprecation, Template\n\n      attr_reader :sh, :data, :options, :validator, :addons, :stages\n      attr_accessor :setup_cache_has_run_for\n\n      def initialize(data)\n        @data = Data.new({ config: self.class.defaults }.deep_merge(data.deep_symbolize_keys))\n        @options = {}\n\n        @sh = Shell::Builder.new\n        @addons = Addons.new(self, sh, self.data, config)\n        @stages = Stages.new(self, sh, config)\n        @setup_cache_has_run_for = {}\n      end\n\n      def compile(ignore_taint = false)\n        Shell.generate(sexp, ignore_taint)\n      end\n\n      def sexp\n        run\n        sh.to_sexp\n      end\n\n      def cache_slug_keys\n        plain_env_vars = Array((config[:env] || []).dup).delete_if {|env| env.start_with? 'SECURE '}\n\n        [\n          'cache',\n          config[:os],\n          config[:dist],\n          config[:osx_image],\n          OpenSSL::Digest::SHA256.hexdigest(plain_env_vars.sort.join('='))\n        ]\n      end\n\n      def cache_slug\n        cache_slug_keys.compact.join('-')\n      end\n\n      def archive_url_for(bucket, version, lang = self.class.name.split('::').last.downcase, ext = 'bz2')\n        sh.if \"$(uname) = 'Linux'\" do\n          sh.raw \"travis_host_os=$(lsb_release -is | tr 'A-Z' 'a-z')\"\n          sh.raw \"travis_rel_version=$(lsb_release -rs)\"\n        end\n        sh.elif \"$(uname) = 'Darwin'\" do\n          sh.raw \"travis_host_os=osx\"\n          sh.raw \"travis_rel=$(sw_vers -productVersion)\"\n          sh.raw \"travis_rel_version=${travis_rel%*.*}\"\n        end\n        \"archive_url=https://s3.amazonaws.com/#{bucket}/binaries/${travis_host_os}/${travis_rel_version}/$(uname -m)/#{lang}-#{version}.tar.#{ext}\"\n      end\n\n      def debug_build_via_api?\n        ! data.debug_options.empty?\n      end\n\n      private\n\n        def config\n          data.config\n        end\n\n        def debug\n          if debug_build_via_api?\n            sh.echo \"Debug build initiated by #{data.debug_options[:created_by]}\", ansi: :yellow\n            if debug_quiet?\n              sh.raw \"travis_debug --quiet\"\n            else\n              sh.raw \"travis_debug\"\n            end\n\n            sh.echo\n            sh.echo \"All remaining steps, including caching and deploy, will be skipped.\", ansi: :yellow\n          end\n        end\n\n        def run\n          stages.run if apply :validate\n          sh.raw template('footer.sh')\n          # apply :deprecations\n        end\n\n        def header\n          sh.raw(\n            template(\n              'header.sh',\n              build_dir: BUILD_DIR,\n              internal_ruby_regex: Travis::Build.config.internal_ruby_regex.untaint,\n              root: '/',\n              home: HOME_DIR\n            ), pos: 0\n          )\n        end\n\n        def configure\n          apply :show_system_info\n          apply :update_glibc\n          apply :clean_up_path\n          apply :fix_resolv_conf\n          apply :fix_etc_hosts\n          apply :no_ipv6_localhost\n          apply :fix_etc_mavenrc\n          apply :etc_hosts_pinning\n          apply :fix_wwdr_certificate\n          apply :put_localhost_first\n          apply :home_paths\n          apply :disable_initramfs\n          apply :disable_ssh_roaming\n          apply :debug_tools\n          apply :npm_registry\n          apply :rvm_use\n          apply :rm_oraclejdk8_symlink\n        end\n\n        def checkout\n          apply :checkout\n        end\n\n        def export\n          apply :env\n        end\n\n        def prepare\n          apply :services\n          apply :fix_ps4 # TODO if this is to fix an rvm issue (as the specs say) then should this go to Rvm instead?\n        end\n\n        def disable_sudo\n          apply :disable_sudo\n        end\n\n        def reset_state\n          if debug_build_via_api?\n            raise \"Debug payload does not contain 'previous_state' value.\" unless previous_state = data.debug_options[:previous_state]\n\n            sh.echo\n            sh.echo \"This is a debug build. The build result is reset to its previous value, \\\\\\\"#{previous_state}\\\\\\\".\", ansi: :yellow\n\n            case previous_state\n            when \"passed\"\n              sh.export 'TRAVIS_TEST_RESULT', '0', echo: false\n            when \"failed\"\n              sh.export 'TRAVIS_TEST_RESULT', '1', echo: false\n            when \"errored\"\n              sh.raw 'travis_terminate 2'\n            end\n          end\n        end\n\n        def config_env_vars\n          @config_env_vars ||= Build::Env::Config.new(data, config)\n          Array(@config_env_vars.data[:env])\n        end\n\n        def host_os\n          case RbConfig::CONFIG[\"host_os\"]\n          when /^(?i:linux)/\n            '$(lsb_release -is | tr \"A-Z\" \"a-z\")'\n          when /^(?i:darwin)/\n            'osx'\n          end\n        end\n\n        def rel_version\n          case RbConfig::CONFIG[\"host_os\"]\n          when /^(?i:linux)/\n            '$(lsb_release -rs)'\n          when /^(?i:darwin)/\n            '${$(sw_vers -productVersion)%*.*}'\n          end\n        end\n\n        def debug_quiet?\n          debug_build_via_api? && data.debug_options[:quiet]\n        end\n\n        def debug_enabled?\n          Travis::Build.config.enable_debug_tools == '1'\n        end\n    end\n  end\nend\n", "idx": 1, "id": 14785, "msg": "This should appear after `node_js` is `require`d.", "proj": "travis-ci-travis-build", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -1,8 +1,10 @@\n package hibernation\n \n import (\n+\t\"context\"\n \t\"fmt\"\n \t\"testing\"\n+\t\"time\"\n \n \t\"github.com/aws/aws-sdk-go/aws\"\n \t\"github.com/aws/aws-sdk-go/service/ec2\"", "y": 0, "oldf": "package hibernation\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/aws/aws-sdk-go/service/ec2\"\n\t\"github.com/golang/mock/gomock\"\n\tlog \"github.com/sirupsen/logrus\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"k8s.io/apimachinery/pkg/util/sets\"\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\n\thivev1 \"github.com/openshift/hive/apis/hive/v1\"\n\thivev1aws \"github.com/openshift/hive/apis/hive/v1/aws\"\n\t\"github.com/openshift/hive/pkg/awsclient\"\n\tmockawsclient \"github.com/openshift/hive/pkg/awsclient/mock\"\n\ttestcd \"github.com/openshift/hive/pkg/test/clusterdeployment\"\n)\n\nfunc TestCanHandle(t *testing.T) {\n\tcd := testcd.BasicBuilder().Options(func(cd *hivev1.ClusterDeployment) {\n\t\tcd.Spec.Platform.AWS = &hivev1aws.Platform{}\n\t}).Build()\n\tactuator := awsActuator{}\n\tassert.True(t, actuator.CanHandle(cd))\n\n\tcd = testcd.BasicBuilder().Build()\n\tassert.False(t, actuator.CanHandle(cd))\n}\n\nfunc TestStopAndStartMachines(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\ttestFunc    string\n\t\tinstances   map[string]int\n\t\tsetupClient func(*testing.T, *mockawsclient.MockClient)\n\t}{\n\t\t{\n\t\t\tname:      \"stop no running instances\",\n\t\t\ttestFunc:  \"StopMachines\",\n\t\t\tinstances: map[string]int{\"terminated\": 2, \"stopping\": 2, \"stopped\": 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"stop running instances\",\n\t\t\ttestFunc:  \"StopMachines\",\n\t\t\tinstances: map[string]int{\"terminated\": 2, \"running\": 2},\n\t\t\tsetupClient: func(t *testing.T, c *mockawsclient.MockClient) {\n\t\t\t\tc.EXPECT().StopInstances(gomock.Any()).Do(\n\t\t\t\t\tfunc(input *ec2.StopInstancesInput) {\n\t\t\t\t\t\tmatchInstanceIDs(t, input.InstanceIds, map[string]int{\"running\": 2})\n\t\t\t\t\t}).Return(nil, nil)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"stop pending and running instances\",\n\t\t\ttestFunc:  \"StopMachines\",\n\t\t\tinstances: map[string]int{\"terminated\": 5, \"shutting-down\": 3, \"stopped\": 4, \"pending\": 1, \"running\": 3},\n\t\t\tsetupClient: func(t *testing.T, c *mockawsclient.MockClient) {\n\t\t\t\tc.EXPECT().StopInstances(gomock.Any()).Do(\n\t\t\t\t\tfunc(input *ec2.StopInstancesInput) {\n\t\t\t\t\t\tmatchInstanceIDs(t, input.InstanceIds, map[string]int{\"pending\": 1, \"running\": 3})\n\t\t\t\t\t}).Return(nil, nil)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"start no stopped instances\",\n\t\t\ttestFunc:  \"StartMachines\",\n\t\t\tinstances: map[string]int{\"terminated\": 3, \"pending\": 4, \"running\": 3},\n\t\t},\n\t\t{\n\t\t\tname:      \"start stopped instances\",\n\t\t\ttestFunc:  \"StartMachines\",\n\t\t\tinstances: map[string]int{\"stopped\": 3, \"terminated\": 2, \"running\": 3},\n\t\t\tsetupClient: func(t *testing.T, c *mockawsclient.MockClient) {\n\t\t\t\tc.EXPECT().StartInstances(gomock.Any()).Do(\n\t\t\t\t\tfunc(input *ec2.StartInstancesInput) {\n\t\t\t\t\t\tmatchInstanceIDs(t, input.InstanceIds, map[string]int{\"stopped\": 3})\n\t\t\t\t\t}).Return(nil, nil)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:      \"start stopped and stopping instances\",\n\t\t\ttestFunc:  \"StartMachines\",\n\t\t\tinstances: map[string]int{\"stopped\": 3, \"stopping\": 1, \"terminated\": 3},\n\t\t\tsetupClient: func(t *testing.T, c *mockawsclient.MockClient) {\n\t\t\t\tc.EXPECT().StartInstances(gomock.Any()).Do(\n\t\t\t\t\tfunc(input *ec2.StartInstancesInput) {\n\t\t\t\t\t\tmatchInstanceIDs(t, input.InstanceIds, map[string]int{\"stopped\": 3, \"stopping\": 1})\n\t\t\t\t\t}).Return(nil, nil)\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tctrl := gomock.NewController(t)\n\t\t\tawsClient := mockawsclient.NewMockClient(ctrl)\n\t\t\tsetupClientInstances(awsClient, test.instances)\n\t\t\tif test.setupClient != nil {\n\t\t\t\ttest.setupClient(t, awsClient)\n\t\t\t}\n\t\t\tactuator := testAWSActuator(awsClient)\n\t\t\tvar err error\n\t\t\tswitch test.testFunc {\n\t\t\tcase \"StopMachines\":\n\t\t\t\terr = actuator.StopMachines(testClusterDeployment(), nil, log.New())\n\t\t\tcase \"StartMachines\":\n\t\t\t\terr = actuator.StartMachines(testClusterDeployment(), nil, log.New())\n\t\t\tdefault:\n\t\t\t\tt.Fatal(\"Invalid function to test\")\n\t\t\t}\n\t\t\tassert.Nil(t, err)\n\t\t})\n\t}\n}\n\nfunc TestMachinesStoppedAndRunning(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\ttestFunc    string\n\t\texpected    bool\n\t\tinstances   map[string]int\n\t\tsetupClient func(*testing.T, *mockawsclient.MockClient)\n\t}{\n\t\t{\n\t\t\tname:      \"Stopped - All machines stopped or terminated\",\n\t\t\ttestFunc:  \"MachinesStopped\",\n\t\t\texpected:  true,\n\t\t\tinstances: map[string]int{\"terminated\": 3, \"stopped\": 2},\n\t\t},\n\t\t{\n\t\t\tname:      \"Stopped - Some machines pending\",\n\t\t\ttestFunc:  \"MachinesStopped\",\n\t\t\texpected:  false,\n\t\t\tinstances: map[string]int{\"terminated\": 3, \"stopped\": 2, \"pending\": 2},\n\t\t},\n\t\t{\n\t\t\tname:      \"Stopped - machines running\",\n\t\t\ttestFunc:  \"MachinesStopped\",\n\t\t\texpected:  false,\n\t\t\tinstances: map[string]int{\"running\": 3, \"terminated\": 2},\n\t\t},\n\t\t{\n\t\t\tname:      \"Running - All machines running or terminated\",\n\t\t\ttestFunc:  \"MachinesRunning\",\n\t\t\texpected:  true,\n\t\t\tinstances: map[string]int{\"running\": 3, \"terminated\": 2},\n\t\t},\n\t\t{\n\t\t\tname:      \"Running - Some machines pending\",\n\t\t\ttestFunc:  \"MachinesRunning\",\n\t\t\texpected:  false,\n\t\t\tinstances: map[string]int{\"running\": 3, \"terminated\": 2, \"pending\": 1},\n\t\t},\n\t\t{\n\t\t\tname:      \"Running - Some machines stopped or shutting-down\",\n\t\t\ttestFunc:  \"MachinesRunning\",\n\t\t\texpected:  false,\n\t\t\tinstances: map[string]int{\"running\": 3, \"terminated\": 2, \"stopped\": 1, \"shutting-down\": 3},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tctrl := gomock.NewController(t)\n\t\t\tawsClient := mockawsclient.NewMockClient(ctrl)\n\t\t\tsetupClientInstances(awsClient, test.instances)\n\t\t\tif test.setupClient != nil {\n\t\t\t\ttest.setupClient(t, awsClient)\n\t\t\t}\n\t\t\tactuator := testAWSActuator(awsClient)\n\t\t\tvar err error\n\t\t\tvar result bool\n\t\t\tswitch test.testFunc {\n\t\t\tcase \"MachinesStopped\":\n\t\t\t\tresult, _, err = actuator.MachinesStopped(testClusterDeployment(), nil, log.New())\n\t\t\tcase \"MachinesRunning\":\n\t\t\t\tresult, _, err = actuator.MachinesRunning(testClusterDeployment(), nil, log.New())\n\t\t\tdefault:\n\t\t\t\tt.Fatal(\"Invalid function to test\")\n\t\t\t}\n\t\t\trequire.Nil(t, err)\n\t\t\tassert.Equal(t, test.expected, result)\n\t\t})\n\t}\n}\n\nfunc matchInstanceIDs(t *testing.T, actual []*string, states map[string]int) {\n\texpected := sets.NewString()\n\tfor state, count := range states {\n\t\tfor i := 0; i < count; i++ {\n\t\t\texpected.Insert(fmt.Sprintf(\"%s-%d\", state, i))\n\t\t}\n\t}\n\tactualSet := sets.NewString()\n\tfor _, a := range actual {\n\t\tactualSet.Insert(aws.StringValue(a))\n\t}\n\tassert.True(t, expected.Equal(actualSet), \"Unexpected set of instance IDs: %v\", actualSet.List())\n}\n\nfunc testAWSActuator(awsClient awsclient.Client) *awsActuator {\n\treturn &awsActuator{\n\t\tawsClientFn: func(*hivev1.ClusterDeployment, client.Client, log.FieldLogger) (awsclient.Client, error) {\n\t\t\treturn awsClient, nil\n\t\t},\n\t}\n}\n\nfunc testClusterDeployment() *hivev1.ClusterDeployment {\n\treturn testcd.BasicBuilder().Options(func(cd *hivev1.ClusterDeployment) {\n\t\tcd.Spec.ClusterMetadata = &hivev1.ClusterMetadata{\n\t\t\tInfraID: \"abcd1234\",\n\t\t}\n\t}).Build()\n}\n\nfunc setupClientInstances(awsClient *mockawsclient.MockClient, states map[string]int) {\n\tinstances := []*ec2.Instance{}\n\tfor state, count := range states {\n\t\tfor i := 0; i < count; i++ {\n\t\t\tinstances = append(instances, &ec2.Instance{\n\t\t\t\tInstanceId: aws.String(fmt.Sprintf(\"%s-%d\", state, i)),\n\t\t\t\tState: &ec2.InstanceState{\n\t\t\t\t\tName: aws.String(state),\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\treservation := &ec2.Reservation{Instances: instances}\n\treservations := []*ec2.Reservation{reservation}\n\tawsClient.EXPECT().DescribeInstances(gomock.Any()).Times(1).Return(\n\t\t&ec2.DescribeInstancesOutput{\n\t\t\tReservations: reservations,\n\t\t},\n\t\tnil)\n}\n", "idx": 1, "id": 18515, "msg": "", "proj": "openshift-hive", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -26,8 +26,6 @@ import org.slf4j.LoggerFactory;\n /**\n  * Class implements ContainerMetrics and emit metrics for containerized executions\n  */\n-//Todo haqin: setup timeToDispatch, flowSubmitToExecutor, flowSubmitToContainer, implement\n-//corresponding methods\n public class ContainerizationMetricsImpl implements ContainerizationMetrics {\n \n   private static final Logger logger = LoggerFactory.getLogger(ContainerizationMetricsImpl.class);", "y": 0, "oldf": "/*\n * Copyright 2021 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n */\n\npackage azkaban.metrics;\n\nimport azkaban.utils.Props;\nimport com.codahale.metrics.Histogram;\nimport com.codahale.metrics.Meter;\nimport com.google.inject.Inject;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Class implements ContainerMetrics and emit metrics for containerized executions\n */\n//Todo haqin: setup timeToDispatch, flowSubmitToExecutor, flowSubmitToContainer, implement\n//corresponding methods\npublic class ContainerizationMetricsImpl implements ContainerizationMetrics {\n\n  private static final Logger logger = LoggerFactory.getLogger(ContainerizationMetricsImpl.class);\n  private final MetricsManager metricsManager;\n  private Meter podCompleted, podRequested, podScheduled, initContainerRunning,\n      appContainerStarting, podReady, podInitFailure, podAppFailure;\n  private Meter flowSubmitToExecutor, flowSubmitToContainer;\n  private Histogram timeToDispatch;\n\n  @Inject\n  public ContainerizationMetricsImpl(MetricsManager metricsManager) {\n    this.metricsManager = metricsManager;\n  }\n\n  @Override\n  public void setUp() {\n    logger.info(String.format(\"Setting up container metrics.\"));\n    this.podCompleted = this.metricsManager.addMeter(\"Pod-Completed-Meter\");\n    this.podRequested = this.metricsManager.addMeter(\"Pod-Requested-Meter\");\n    this.podScheduled = this.metricsManager.addMeter(\"Pod-Scheduled-Meter\");\n    this.initContainerRunning = this.metricsManager.addMeter(\"Init-Container-Running-Meter\");\n    this.appContainerStarting = this.metricsManager.addMeter(\"App-Container-Starting-Meter\");\n    this.podReady = this.metricsManager.addMeter(\"Pod-Ready-Meter\");\n    this.podInitFailure = this.metricsManager.addMeter(\"Pod-Init-Failure-Meter\");\n    this.podAppFailure = this.metricsManager.addMeter(\"Pod-App-Failure-Meter\");\n  }\n\n  @Override\n  public void startReporting(Props props) {\n    logger.info(String.format(\"Start reporting container metrics\"));\n    this.metricsManager.startReporting(\"AZ-WEB\", props);\n  }\n\n  /**\n   * Mark the occurrence of various pod statuses, defined by {@link azkaban.executor.container.watch.AzPodStatus}\n   */\n\n  @Override\n  public void markPodCompleted() {\n    this.podCompleted.mark();\n  }\n\n  @Override\n  public void markPodRequested() {\n    this.podRequested.mark();\n  }\n\n  @Override\n  public void markPodScheduled() {\n    this.podScheduled.mark();\n  }\n\n  @Override\n  public void markInitContainerRunning() {\n    this.initContainerRunning.mark();\n  }\n\n  @Override\n  public void markAppContainerStarting() {\n    this.appContainerStarting.mark();\n  }\n\n  @Override\n  public void markPodReady() { this.podReady.mark(); }\n\n  @Override\n  public void markPodInitFailure() {\n    this.podInitFailure.mark();\n  }\n\n  @Override\n  public void markPodAppFailure() {\n    this.podAppFailure.mark();\n  }\n\n\n  @Override\n  public void addTimeToDispatch(final long time) { timeToDispatch.update(time); }\n\n  @Override\n  public void markFlowSubmitToExecutor() { flowSubmitToExecutor.mark(); }\n\n  @Override\n  public void markFlowSubmitToContainer() { flowSubmitToContainer.mark(); }\n}\n", "idx": 1, "id": 22142, "msg": "", "proj": "azkaban-azkaban", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -142,6 +142,10 @@ const (\n \tdefaultCgroupCPUPeriod = 100 * time.Millisecond\n \tmaximumCgroupCPUPeriod = 100 * time.Millisecond\n \tminimumCgroupCPUPeriod = 8 * time.Millisecond\n+\n+\t// DefaultContainerMetricsPublishInterval is the default interval that we publish\n+\t// metrics to the ECS telemetry backend (TACS)\n+\tDefaultContainerMetricsPublishInterval = 20 * time.Second\n )\n \n const (", "y": 1, "oldf": "// Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n// not use this file except in compliance with the License. A copy of the\n// License is located at\n//\n//\thttp://aws.amazon.com/apache2.0/\n//\n// or in the \"license\" file accompanying this file. This file is distributed\n// on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n// express or implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage config\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\tapierrors \"github.com/aws/amazon-ecs-agent/agent/api/errors\"\n\t\"github.com/aws/amazon-ecs-agent/agent/dockerclient\"\n\t\"github.com/aws/amazon-ecs-agent/agent/ec2\"\n\t\"github.com/aws/amazon-ecs-agent/agent/utils\"\n\t\"github.com/cihub/seelog\"\n)\n\nconst (\n\t// http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=docker\n\tDockerReservedPort    = 2375\n\tDockerReservedSSLPort = 2376\n\t// DockerTagSeparator is the charactor used to separate names and tag in docker\n\tDockerTagSeparator = \":\"\n\t// DefaultDockerTag is the default tag used by docker\n\tDefaultDockerTag = \"latest\"\n\n\tSSHPort = 22\n\n\t// AgentIntrospectionPort is used to serve the metadata about the agent and to query the tasks being managed by the agent.\n\tAgentIntrospectionPort = 51678\n\n\t// AgentCredentialsPort is used to serve the credentials for tasks.\n\tAgentCredentialsPort = 51679\n\n\t// AgentPrometheusExpositionPort is used to expose Prometheus metrics that can be scraped by a Prometheus server\n\tAgentPrometheusExpositionPort = 51680\n\n\t// defaultConfigFileName is the default (json-formatted) config file\n\tdefaultConfigFileName = \"/etc/ecs_container_agent/config.json\"\n\n\t// DefaultClusterName is the name of the default cluster.\n\tDefaultClusterName = \"default\"\n\n\t// DefaultTaskCleanupWaitDuration specifies the default value for task cleanup duration. It is used to\n\t// clean up task's containers.\n\tDefaultTaskCleanupWaitDuration = 3 * time.Hour\n\n\t// DefaultPollingMetricsWaitDuration specifies the default value for polling metrics wait duration\n\t// This is only used when PollMetrics is set to true\n\tDefaultPollingMetricsWaitDuration = 15 * time.Second\n\n\t// defaultDockerStopTimeout specifies the value for container stop timeout duration\n\tdefaultDockerStopTimeout = 30 * time.Second\n\n\t// DefaultImageCleanupTimeInterval specifies the default value for image cleanup duration. It is used to\n\t// remove the images pulled by agent.\n\tDefaultImageCleanupTimeInterval = 30 * time.Minute\n\n\t// DefaultNumImagesToDeletePerCycle specifies the default number of images to delete when agent performs\n\t// image cleanup.\n\tDefaultNumImagesToDeletePerCycle = 5\n\n\t// DefaultNumNonECSContainersToDeletePerCycle specifies the default number of nonecs containers to delete when agent performs\n\t// nonecs containers cleanup.\n\tDefaultNumNonECSContainersToDeletePerCycle = 5\n\n\t// DefaultImageDeletionAge specifies the default value for minimum amount of elapsed time after an image\n\t// has been pulled before it can be deleted.\n\tDefaultImageDeletionAge = 1 * time.Hour\n\n\t// DefaultNonECSImageDeletionAge specifies the default value for minimum amount of elapsed time after an image\n\t// has been created before it can be deleted\n\tDefaultNonECSImageDeletionAge = 1 * time.Hour\n\n\t// minimumTaskCleanupWaitDuration specifies the minimum duration to wait before cleaning up\n\t// a task's container. This is used to enforce sane values for the config.TaskCleanupWaitDuration field.\n\tminimumTaskCleanupWaitDuration = 1 * time.Minute\n\n\t// minimumImagePullInactivityTimeout specifies the minimum amount of time for that an image can be\n\t// 'stuck' in the pull / unpack step. Very small values are unsafe and lead to high failure rate.\n\tminimumImagePullInactivityTimeout = 1 * time.Minute\n\n\t// minimumPollingMetricsWaitDuration specifies the minimum duration to wait before polling for new stats\n\t// from docker. This is only used when PollMetrics is set to true\n\tminimumPollingMetricsWaitDuration = 1 * time.Second\n\n\t// maximumPollingMetricsWaitDuration specifies the maximum duration to wait before polling for new stats\n\t// from docker. This is only used when PollMetrics is set to true\n\tmaximumPollingMetricsWaitDuration = 20 * time.Second\n\n\t// minimumDockerStopTimeout specifies the minimum value for docker StopContainer API\n\tminimumDockerStopTimeout = 1 * time.Second\n\n\t// minimumImageCleanupInterval specifies the minimum time for agent to wait before performing\n\t// image cleanup.\n\tminimumImageCleanupInterval = 10 * time.Minute\n\n\t// minimumNumImagesToDeletePerCycle specifies the minimum number of images that to be deleted when\n\t// performing image cleanup.\n\tminimumNumImagesToDeletePerCycle = 1\n\n\t// defaultCNIPluginsPath is the default path where cni binaries are located\n\tdefaultCNIPluginsPath = \"/amazon-ecs-cni-plugins\"\n\n\t// DefaultMinSupportedCNIVersion denotes the minimum version of cni spec required\n\tDefaultMinSupportedCNIVersion = \"0.3.0\"\n\n\t// pauseContainerTarball is the path to the pause container tarball\n\tpauseContainerTarballPath = \"/images/amazon-ecs-pause.tar\"\n\n\t// DefaultTaskMetadataSteadyStateRate is set as 40. This is arrived from our benchmarking\n\t// results where task endpoint can handle 4000 rps effectively. Here, 100 containers\n\t// will be able to send out 40 rps.\n\tDefaultTaskMetadataSteadyStateRate = 40\n\n\t// DefaultTaskMetadataBurstRate is set to handle 60 burst requests at once\n\tDefaultTaskMetadataBurstRate = 60\n\n\t//Known cached image names\n\tCachedImageNamePauseContainer = \"amazon/amazon-ecs-pause:0.1.0\"\n\tCachedImageNameAgentContainer = \"amazon/amazon-ecs-agent:latest\"\n\n\t// DefaultNvidiaRuntime is the name of the runtime to pass Nvidia GPUs to containers\n\tDefaultNvidiaRuntime = \"nvidia\"\n\n\t// defaultCgroupCPUPeriod is set to 100 ms to set isCFS period and quota for task limits\n\tdefaultCgroupCPUPeriod = 100 * time.Millisecond\n\tmaximumCgroupCPUPeriod = 100 * time.Millisecond\n\tminimumCgroupCPUPeriod = 8 * time.Millisecond\n)\n\nconst (\n\t// ImagePullDefaultBehavior specifies the behavior that if an image pull API call fails,\n\t// agent tries to start from the Docker image cache anyway, assuming that the image has not changed.\n\tImagePullDefaultBehavior ImagePullBehaviorType = iota\n\n\t// ImagePullAlwaysBehavior specifies the behavior that if an image pull API call fails,\n\t// the task fails instead of using cached image.\n\tImagePullAlwaysBehavior\n\n\t// ImagePullOnceBehavior specifies the behavior that agent will only attempt to pull\n\t// the same image once, once an image is pulled, local image cache will be used\n\t// for all the containers.\n\tImagePullOnceBehavior\n\n\t// ImagePullPreferCachedBehavior specifies the behavior that agent will only attempt to pull\n\t// the image if there is no cached image.\n\tImagePullPreferCachedBehavior\n)\n\nconst (\n\t// When ContainerInstancePropagateTagsFromNoneType is specified, no DescribeTags\n\t// API call will be made.\n\tContainerInstancePropagateTagsFromNoneType ContainerInstancePropagateTagsFromType = iota\n\n\t// When ContainerInstancePropagateTagsFromEC2InstanceType is specified, agent will\n\t// make DescribeTags API call to get tags remotely.\n\tContainerInstancePropagateTagsFromEC2InstanceType\n)\n\nvar (\n\t// DefaultPauseContainerImageName is the name of the pause container image. The linker's\n\t// load flags are used to populate this value from the Makefile\n\tDefaultPauseContainerImageName = \"\"\n\n\t// DefaultPauseContainerTag is the tag for the pause container image. The linker's load\n\t// flags are used to populate this value from the Makefile\n\tDefaultPauseContainerTag = \"\"\n)\n\n// Merge merges two config files, preferring the ones on the left. Any nil or\n// zero values present in the left that are not present in the right will be\n// overridden\nfunc (cfg *Config) Merge(rhs Config) *Config {\n\tleft := reflect.ValueOf(cfg).Elem()\n\tright := reflect.ValueOf(&rhs).Elem()\n\n\tfor i := 0; i < left.NumField(); i++ {\n\t\tleftField := left.Field(i)\n\t\tif utils.ZeroOrNil(leftField.Interface()) {\n\t\t\tleftField.Set(reflect.ValueOf(right.Field(i).Interface()))\n\t\t}\n\t}\n\n\treturn cfg //make it chainable\n}\n\n// NewConfig returns a config struct created by merging environment variables,\n// a config file, and EC2 Metadata info.\n// The 'config' struct it returns can be used, even if an error is returned. An\n// error is returned, however, if the config is incomplete in some way that is\n// considered fatal.\nfunc NewConfig(ec2client ec2.EC2MetadataClient) (*Config, error) {\n\tvar errs []error\n\tenvConfig, err := environmentConfig() //Environment overrides all else\n\tif err != nil {\n\t\terrs = append(errs, err)\n\t}\n\tconfig := &envConfig\n\n\tif config.complete() {\n\t\t// No need to do file / network IO\n\t\treturn config, nil\n\t}\n\n\tfcfg, err := fileConfig()\n\tif err != nil {\n\t\terrs = append(errs, err)\n\t}\n\tconfig.Merge(fcfg)\n\n\tconfig.Merge(userDataConfig(ec2client))\n\n\tif config.AWSRegion == \"\" {\n\t\tif config.NoIID {\n\t\t\t// get it from AWS SDK if we don't have instance identity document\n\t\t\tawsRegion, err := ec2client.Region()\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t\tconfig.AWSRegion = awsRegion\n\t\t} else {\n\t\t\t// Get it from metadata only if we need to (network io)\n\t\t\tconfig.Merge(ec2MetadataConfig(ec2client))\n\t\t}\n\t}\n\n\treturn config, config.mergeDefaultConfig(errs)\n}\n\nfunc (config *Config) mergeDefaultConfig(errs []error) error {\n\tconfig.trimWhitespace()\n\tconfig.Merge(DefaultConfig())\n\terr := config.validateAndOverrideBounds()\n\tif err != nil {\n\t\terrs = append(errs, err)\n\t}\n\tif len(errs) != 0 {\n\t\treturn apierrors.NewMultiError(errs...)\n\t}\n\treturn nil\n}\n\n// trimWhitespace trims whitespace from all string cfg values with the\n// `trim` tag\nfunc (cfg *Config) trimWhitespace() {\n\tcfgElem := reflect.ValueOf(cfg).Elem()\n\tcfgStructField := reflect.Indirect(reflect.ValueOf(cfg)).Type()\n\n\tfor i := 0; i < cfgElem.NumField(); i++ {\n\t\tcfgField := cfgElem.Field(i)\n\t\tif !cfgField.CanInterface() {\n\t\t\tcontinue\n\t\t}\n\t\ttrimTag := cfgStructField.Field(i).Tag.Get(\"trim\")\n\t\tif len(trimTag) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif cfgField.Kind() != reflect.String {\n\t\t\tseelog.Warnf(\"Cannot trim non-string field type %v index %v\", cfgField.Kind().String(), i)\n\t\t\tcontinue\n\t\t}\n\t\tstr := cfgField.Interface().(string)\n\t\tcfgField.SetString(strings.TrimSpace(str))\n\t}\n}\n\n// validateAndOverrideBounds performs validation over members of the Config struct\n// and check the value against the minimum required value.\nfunc (cfg *Config) validateAndOverrideBounds() error {\n\terr := cfg.checkMissingAndDepreciated()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif cfg.DockerStopTimeout < minimumDockerStopTimeout {\n\t\treturn fmt.Errorf(\"config: invalid value for docker container stop timeout: %v\", cfg.DockerStopTimeout.String())\n\t}\n\n\tif cfg.ContainerStartTimeout < minimumContainerStartTimeout {\n\t\treturn fmt.Errorf(\"config: invalid value for docker container start timeout: %v\", cfg.ContainerStartTimeout.String())\n\t}\n\tvar badDrivers []string\n\tfor _, driver := range cfg.AvailableLoggingDrivers {\n\t\t_, ok := dockerclient.LoggingDriverMinimumVersion[driver]\n\t\tif !ok {\n\t\t\tbadDrivers = append(badDrivers, string(driver))\n\t\t}\n\t}\n\tif len(badDrivers) > 0 {\n\t\treturn errors.New(\"Invalid logging drivers: \" + strings.Join(badDrivers, \", \"))\n\t}\n\n\t// If a value has been set for taskCleanupWaitDuration and the value is less than the minimum allowed cleanup duration,\n\t// print a warning and override it\n\tif cfg.TaskCleanupWaitDuration < minimumTaskCleanupWaitDuration {\n\t\tseelog.Warnf(\"Invalid value for ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION, will be overridden with the default value: %s. Parsed value: %v, minimum value: %v.\", DefaultTaskCleanupWaitDuration.String(), cfg.TaskCleanupWaitDuration, minimumTaskCleanupWaitDuration)\n\t\tcfg.TaskCleanupWaitDuration = DefaultTaskCleanupWaitDuration\n\t}\n\n\tif cfg.ImagePullInactivityTimeout < minimumImagePullInactivityTimeout {\n\t\tseelog.Warnf(\"Invalid value for image pull inactivity timeout duration, will be overridden with the default value: %s. Parsed value: %v, minimum value: %v.\", defaultImagePullInactivityTimeout.String(), cfg.ImagePullInactivityTimeout, minimumImagePullInactivityTimeout)\n\t\tcfg.ImagePullInactivityTimeout = defaultImagePullInactivityTimeout\n\t}\n\n\tif cfg.ImageCleanupInterval < minimumImageCleanupInterval {\n\t\tseelog.Warnf(\"Invalid value for ECS_IMAGE_CLEANUP_INTERVAL, will be overridden with the default value: %s. Parsed value: %v, minimum value: %v.\", DefaultImageCleanupTimeInterval.String(), cfg.ImageCleanupInterval, minimumImageCleanupInterval)\n\t\tcfg.ImageCleanupInterval = DefaultImageCleanupTimeInterval\n\t}\n\n\tif cfg.NumImagesToDeletePerCycle < minimumNumImagesToDeletePerCycle {\n\t\tseelog.Warnf(\"Invalid value for number of images to delete for image cleanup, will be overridden with the default value: %d. Parsed value: %d, minimum value: %d.\", DefaultImageDeletionAge, cfg.NumImagesToDeletePerCycle, minimumNumImagesToDeletePerCycle)\n\t\tcfg.NumImagesToDeletePerCycle = DefaultNumImagesToDeletePerCycle\n\t}\n\n\tif cfg.TaskMetadataSteadyStateRate <= 0 || cfg.TaskMetadataBurstRate <= 0 {\n\t\tseelog.Warnf(\"Invalid values for rate limits, will be overridden with default values: %d,%d.\", DefaultTaskMetadataSteadyStateRate, DefaultTaskMetadataBurstRate)\n\t\tcfg.TaskMetadataSteadyStateRate = DefaultTaskMetadataSteadyStateRate\n\t\tcfg.TaskMetadataBurstRate = DefaultTaskMetadataBurstRate\n\t}\n\n\t// check the PollMetrics specific configurations\n\tcfg.pollMetricsOverrides()\n\n\tcfg.platformOverrides()\n\n\treturn nil\n}\n\nfunc (cfg *Config) pollMetricsOverrides() {\n\tif cfg.PollMetrics {\n\t\tif cfg.PollingMetricsWaitDuration < minimumPollingMetricsWaitDuration {\n\t\t\tseelog.Warnf(\"Invalid value for polling metrics wait duration, will be overridden with the default value: %s. Parsed value: %v, minimum value: %v.\", DefaultPollingMetricsWaitDuration.String(), cfg.PollingMetricsWaitDuration, minimumPollingMetricsWaitDuration)\n\t\t\tcfg.PollingMetricsWaitDuration = DefaultPollingMetricsWaitDuration\n\t\t}\n\n\t\tif cfg.PollingMetricsWaitDuration > maximumPollingMetricsWaitDuration {\n\t\t\tseelog.Warnf(\"Invalid value for polling metrics wait duration, will be overridden with the default value: %s. Parsed value: %v, maximum value: %v.\", DefaultPollingMetricsWaitDuration.String(), cfg.PollingMetricsWaitDuration, maximumPollingMetricsWaitDuration)\n\t\t\tcfg.PollingMetricsWaitDuration = DefaultPollingMetricsWaitDuration\n\t\t}\n\t}\n}\n\n// checkMissingAndDeprecated checks all zero-valued fields for tags of the form\n// missing:STRING and acts based on that string. Current options are: fatal,\n// warn. Fatal will result in an error being returned, warn will result in a\n// warning that the field is missing being logged.\nfunc (cfg *Config) checkMissingAndDepreciated() error {\n\tcfgElem := reflect.ValueOf(cfg).Elem()\n\tcfgStructField := reflect.Indirect(reflect.ValueOf(cfg)).Type()\n\n\tfatalFields := []string{}\n\tfor i := 0; i < cfgElem.NumField(); i++ {\n\t\tcfgField := cfgElem.Field(i)\n\t\tif utils.ZeroOrNil(cfgField.Interface()) {\n\t\t\tmissingTag := cfgStructField.Field(i).Tag.Get(\"missing\")\n\t\t\tif len(missingTag) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch missingTag {\n\t\t\tcase \"warn\":\n\t\t\t\tseelog.Warnf(\"Configuration key not set, key: %v\", cfgStructField.Field(i).Name)\n\t\t\tcase \"fatal\":\n\t\t\t\tseelog.Criticalf(\"Configuration key not set, key: %v\", cfgStructField.Field(i).Name)\n\t\t\t\tfatalFields = append(fatalFields, cfgStructField.Field(i).Name)\n\t\t\tdefault:\n\t\t\t\tseelog.Warnf(\"Unexpected `missing` tag value, tag %v\", missingTag)\n\t\t\t}\n\t\t} else {\n\t\t\t// present\n\t\t\tdeprecatedTag := cfgStructField.Field(i).Tag.Get(\"deprecated\")\n\t\t\tif len(deprecatedTag) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tseelog.Warnf(\"Use of deprecated configuration key, key: %v message: %v\", cfgStructField.Field(i).Name, deprecatedTag)\n\t\t}\n\t}\n\tif len(fatalFields) > 0 {\n\t\treturn errors.New(\"Missing required fields: \" + strings.Join(fatalFields, \", \"))\n\t}\n\treturn nil\n}\n\n// complete returns true if all fields of the config are populated / nonzero\nfunc (cfg *Config) complete() bool {\n\tcfgElem := reflect.ValueOf(cfg).Elem()\n\n\tfor i := 0; i < cfgElem.NumField(); i++ {\n\t\tif utils.ZeroOrNil(cfgElem.Field(i).Interface()) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc fileConfig() (Config, error) {\n\tfileName := utils.DefaultIfBlank(os.Getenv(\"ECS_AGENT_CONFIG_FILE_PATH\"), defaultConfigFileName)\n\tcfg := Config{}\n\n\tfile, err := os.Open(fileName)\n\tif err != nil {\n\t\treturn cfg, nil\n\t}\n\tdata, err := ioutil.ReadAll(file)\n\tif err != nil {\n\t\tseelog.Errorf(\"Unable to read cfg file, err %v\", err)\n\t\treturn cfg, err\n\t}\n\tif strings.TrimSpace(string(data)) == \"\" {\n\t\t// empty file, not an error\n\t\treturn cfg, nil\n\t}\n\n\terr = json.Unmarshal(data, &cfg)\n\tif err != nil {\n\t\tseelog.Criticalf(\"Error reading cfg json data, err %v\", err)\n\t\treturn cfg, err\n\t}\n\n\t// Handle any deprecated keys correctly here\n\tif utils.ZeroOrNil(cfg.Cluster) && !utils.ZeroOrNil(cfg.ClusterArn) {\n\t\tcfg.Cluster = cfg.ClusterArn\n\t}\n\treturn cfg, nil\n}\n\n// userDataConfig reads configuration JSON from instance's userdata. It doesn't\n// return any error as it's entirely optional to configure the ECS agent using\n// this method.\n// Example:\n// {\"ECSAgentConfiguration\":{\"Cluster\":\"default\"}}\nfunc userDataConfig(ec2Client ec2.EC2MetadataClient) Config {\n\ttype userDataParser struct {\n\t\tConfig Config `json:\"ECSAgentConfiguration\"`\n\t}\n\n\tparsedUserData := userDataParser{\n\t\tConfig: Config{},\n\t}\n\n\tuserData, err := ec2Client.GetUserData()\n\tif err != nil {\n\t\tseelog.Warnf(\"Unable to fetch user data: %v\", err)\n\t\t// Unable to read userdata from instance metadata. Just\n\t\t// return early\n\t\treturn parsedUserData.Config\n\t}\n\t// In the future, if we want to support base64 encoded config,\n\t// we'd need to add logic to decode the string here.\n\terr = json.Unmarshal([]byte(userData), &parsedUserData)\n\tif err != nil {\n\t\tseelog.Debugf(\"Non-json user data, skip merging into agent config: %v\", err)\n\t\t// Unable to parse userdata as a valid JSON. Return the\n\t\t// empty config\n\t\treturn Config{}\n\t}\n\n\treturn parsedUserData.Config\n}\n\n// environmentConfig reads the given configs from the environment and attempts\n// to convert them to the given type\nfunc environmentConfig() (Config, error) {\n\tdataDir := os.Getenv(\"ECS_DATADIR\")\n\n\tsteadyStateRate, burstRate := parseTaskMetadataThrottles()\n\n\tvar errs []error\n\tinstanceAttributes, errs := parseInstanceAttributes(errs)\n\n\tcontainerInstanceTags, errs := parseContainerInstanceTags(errs)\n\n\tadditionalLocalRoutes, errs := parseAdditionalLocalRoutes(errs)\n\n\tvar err error\n\tif len(errs) > 0 {\n\t\terr = apierrors.NewMultiError(errs...)\n\t}\n\treturn Config{\n\t\tCluster:                             os.Getenv(\"ECS_CLUSTER\"),\n\t\tAPIEndpoint:                         os.Getenv(\"ECS_BACKEND_HOST\"),\n\t\tAWSRegion:                           os.Getenv(\"AWS_DEFAULT_REGION\"),\n\t\tDockerEndpoint:                      os.Getenv(\"DOCKER_HOST\"),\n\t\tReservedPorts:                       parseReservedPorts(\"ECS_RESERVED_PORTS\"),\n\t\tReservedPortsUDP:                    parseReservedPorts(\"ECS_RESERVED_PORTS_UDP\"),\n\t\tDataDir:                             dataDir,\n\t\tCheckpoint:                          parseCheckpoint(dataDir),\n\t\tEngineAuthType:                      os.Getenv(\"ECS_ENGINE_AUTH_TYPE\"),\n\t\tEngineAuthData:                      NewSensitiveRawMessage([]byte(os.Getenv(\"ECS_ENGINE_AUTH_DATA\"))),\n\t\tUpdatesEnabled:                      utils.ParseBool(os.Getenv(\"ECS_UPDATES_ENABLED\"), false),\n\t\tUpdateDownloadDir:                   os.Getenv(\"ECS_UPDATE_DOWNLOAD_DIR\"),\n\t\tDisableMetrics:                      utils.ParseBool(os.Getenv(\"ECS_DISABLE_METRICS\"), false),\n\t\tReservedMemory:                      parseEnvVariableUint16(\"ECS_RESERVED_MEMORY\"),\n\t\tAvailableLoggingDrivers:             parseAvailableLoggingDrivers(),\n\t\tPrivilegedDisabled:                  utils.ParseBool(os.Getenv(\"ECS_DISABLE_PRIVILEGED\"), false),\n\t\tSELinuxCapable:                      utils.ParseBool(os.Getenv(\"ECS_SELINUX_CAPABLE\"), false),\n\t\tAppArmorCapable:                     utils.ParseBool(os.Getenv(\"ECS_APPARMOR_CAPABLE\"), false),\n\t\tTaskCleanupWaitDuration:             parseEnvVariableDuration(\"ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION\"),\n\t\tTaskENIEnabled:                      utils.ParseBool(os.Getenv(\"ECS_ENABLE_TASK_ENI\"), false),\n\t\tTaskIAMRoleEnabled:                  utils.ParseBool(os.Getenv(\"ECS_ENABLE_TASK_IAM_ROLE\"), false),\n\t\tDeleteNonECSImagesEnabled:           utils.ParseBool(os.Getenv(\"ECS_ENABLE_UNTRACKED_IMAGE_CLEANUP\"), false),\n\t\tTaskCPUMemLimit:                     parseTaskCPUMemLimitEnabled(),\n\t\tDockerStopTimeout:                   parseDockerStopTimeout(),\n\t\tContainerStartTimeout:               parseContainerStartTimeout(),\n\t\tImagePullInactivityTimeout:          parseImagePullInactivityTimeout(),\n\t\tCredentialsAuditLogFile:             os.Getenv(\"ECS_AUDIT_LOGFILE\"),\n\t\tCredentialsAuditLogDisabled:         utils.ParseBool(os.Getenv(\"ECS_AUDIT_LOGFILE_DISABLED\"), false),\n\t\tTaskIAMRoleEnabledForNetworkHost:    utils.ParseBool(os.Getenv(\"ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST\"), false),\n\t\tImageCleanupDisabled:                utils.ParseBool(os.Getenv(\"ECS_DISABLE_IMAGE_CLEANUP\"), false),\n\t\tMinimumImageDeletionAge:             parseEnvVariableDuration(\"ECS_IMAGE_MINIMUM_CLEANUP_AGE\"),\n\t\tNonECSMinimumImageDeletionAge:       parseEnvVariableDuration(\"NON_ECS_IMAGE_MINIMUM_CLEANUP_AGE\"),\n\t\tImageCleanupInterval:                parseEnvVariableDuration(\"ECS_IMAGE_CLEANUP_INTERVAL\"),\n\t\tNumImagesToDeletePerCycle:           parseNumImagesToDeletePerCycle(),\n\t\tNumNonECSContainersToDeletePerCycle: parseNumNonECSContainersToDeletePerCycle(),\n\t\tImagePullBehavior:                   parseImagePullBehavior(),\n\t\tImageCleanupExclusionList:           parseImageCleanupExclusionList(\"ECS_EXCLUDE_UNTRACKED_IMAGE\"),\n\t\tInstanceAttributes:                  instanceAttributes,\n\t\tCNIPluginsPath:                      os.Getenv(\"ECS_CNI_PLUGINS_PATH\"),\n\t\tAWSVPCBlockInstanceMetdata:          utils.ParseBool(os.Getenv(\"ECS_AWSVPC_BLOCK_IMDS\"), false),\n\t\tAWSVPCAdditionalLocalRoutes:         additionalLocalRoutes,\n\t\tContainerMetadataEnabled:            utils.ParseBool(os.Getenv(\"ECS_ENABLE_CONTAINER_METADATA\"), false),\n\t\tDataDirOnHost:                       os.Getenv(\"ECS_HOST_DATA_DIR\"),\n\t\tOverrideAWSLogsExecutionRole:        utils.ParseBool(os.Getenv(\"ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE\"), false),\n\t\tCgroupPath:                          os.Getenv(\"ECS_CGROUP_PATH\"),\n\t\tTaskMetadataSteadyStateRate:         steadyStateRate,\n\t\tTaskMetadataBurstRate:               burstRate,\n\t\tSharedVolumeMatchFullConfig:         utils.ParseBool(os.Getenv(\"ECS_SHARED_VOLUME_MATCH_FULL_CONFIG\"), false),\n\t\tContainerInstanceTags:               containerInstanceTags,\n\t\tContainerInstancePropagateTagsFrom:  parseContainerInstancePropagateTagsFrom(),\n\t\tPollMetrics:                         utils.ParseBool(os.Getenv(\"ECS_POLL_METRICS\"), false),\n\t\tPollingMetricsWaitDuration:          parseEnvVariableDuration(\"ECS_POLLING_METRICS_WAIT_DURATION\"),\n\t\tDisableDockerHealthCheck:            utils.ParseBool(os.Getenv(\"ECS_DISABLE_DOCKER_HEALTH_CHECK\"), false),\n\t\tGPUSupportEnabled:                   utils.ParseBool(os.Getenv(\"ECS_ENABLE_GPU_SUPPORT\"), false),\n\t\tNvidiaRuntime:                       os.Getenv(\"ECS_NVIDIA_RUNTIME\"),\n\t\tTaskMetadataAZDisabled:              utils.ParseBool(os.Getenv(\"ECS_DISABLE_TASK_METADATA_AZ\"), false),\n\t\tCgroupCPUPeriod:                     parseCgroupCPUPeriod(),\n\t\tSpotInstanceDrainingEnabled:         utils.ParseBool(os.Getenv(\"ECS_ENABLE_SPOT_INSTANCE_DRAINING\"), false),\n\t\tGMSACapable:                         parseGMSACapability(),\n\t\tVolumePluginCapabilities:            parseVolumePluginCapabilities(),\n\t}, err\n}\n\nfunc ec2MetadataConfig(ec2client ec2.EC2MetadataClient) Config {\n\tiid, err := ec2client.InstanceIdentityDocument()\n\tif err != nil {\n\t\tseelog.Criticalf(\"Unable to communicate with EC2 Metadata service to infer region: %v\", err.Error())\n\t\treturn Config{}\n\t}\n\treturn Config{AWSRegion: iid.Region}\n}\n\n// String returns a lossy string representation of the config suitable for human readable display.\n// Consequently, it *should not* return any sensitive information.\nfunc (cfg *Config) String() string {\n\treturn fmt.Sprintf(\n\t\t\"Cluster: %v, \"+\n\t\t\t\" Region: %v, \"+\n\t\t\t\" DataDir: %v,\"+\n\t\t\t\" Checkpoint: %v, \"+\n\t\t\t\"AuthType: %v, \"+\n\t\t\t\"UpdatesEnabled: %v, \"+\n\t\t\t\"DisableMetrics: %v, \"+\n\t\t\t\"PollMetrics: %v, \"+\n\t\t\t\"PollingMetricsWaitDuration: %v, \"+\n\t\t\t\"ReservedMem: %v, \"+\n\t\t\t\"TaskCleanupWaitDuration: %v, \"+\n\t\t\t\"DockerStopTimeout: %v, \"+\n\t\t\t\"ContainerStartTimeout: %v, \"+\n\t\t\t\"TaskCPUMemLimit: %v, \"+\n\t\t\t\"%s\",\n\t\tcfg.Cluster,\n\t\tcfg.AWSRegion,\n\t\tcfg.DataDir,\n\t\tcfg.Checkpoint,\n\t\tcfg.EngineAuthType,\n\t\tcfg.UpdatesEnabled,\n\t\tcfg.DisableMetrics,\n\t\tcfg.PollMetrics,\n\t\tcfg.PollingMetricsWaitDuration,\n\t\tcfg.ReservedMemory,\n\t\tcfg.TaskCleanupWaitDuration,\n\t\tcfg.DockerStopTimeout,\n\t\tcfg.ContainerStartTimeout,\n\t\tcfg.TaskCPUMemLimit,\n\t\tcfg.platformString(),\n\t)\n}\n", "idx": 1, "id": 24401, "msg": "why is this present in config package? do we foresee customers setting this value?", "proj": "aws-amazon-ecs-agent", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -1200,7 +1200,7 @@ func selectorPodWatchHandler(a handler.MapObject) []reconcile.Request {\n }\n \n func (r *ReconcileClusterDeployment) calcInstallPodRestarts(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (int, error) {\n-\tinstallerPodLabels := map[string]string{install.ClusterDeploymentNameLabel: cd.Name, install.InstallJobLabel: \"true\"}\n+\tinstallerPodLabels := map[string]string{constants.ClusterDeploymentNameLabel: cd.Name, constants.InstallJobLabel: \"true\"}\n \tpods := &corev1.PodList{}\n \terr := r.Client.List(context.Background(), pods, client.InNamespace(cd.Namespace), client.MatchingLabels(installerPodLabels))\n \tif err != nil {", "y": 0, "oldf": "package clusterdeployment\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\tlog \"github.com/sirupsen/logrus\"\n\n\troutev1 \"github.com/openshift/api/route/v1\"\n\tbatchv1 \"k8s.io/api/batch/v1\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\tapierrors \"k8s.io/apimachinery/pkg/api/errors\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\t\"k8s.io/client-go/tools/clientcmd\"\n\n\t\"sigs.k8s.io/controller-runtime/pkg/client\"\n\t\"sigs.k8s.io/controller-runtime/pkg/controller\"\n\t\"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\"\n\t\"sigs.k8s.io/controller-runtime/pkg/handler\"\n\t\"sigs.k8s.io/controller-runtime/pkg/manager\"\n\t\"sigs.k8s.io/controller-runtime/pkg/metrics\"\n\t\"sigs.k8s.io/controller-runtime/pkg/reconcile\"\n\t\"sigs.k8s.io/controller-runtime/pkg/source\"\n\n\tapihelpers \"github.com/openshift/hive/pkg/apis/helpers\"\n\thivev1 \"github.com/openshift/hive/pkg/apis/hive/v1alpha1\"\n\t\"github.com/openshift/hive/pkg/constants\"\n\t\"github.com/openshift/hive/pkg/controller/images\"\n\thivemetrics \"github.com/openshift/hive/pkg/controller/metrics\"\n\tcontrollerutils \"github.com/openshift/hive/pkg/controller/utils\"\n\t\"github.com/openshift/hive/pkg/imageset\"\n\t\"github.com/openshift/hive/pkg/install\"\n)\n\nconst (\n\tcontrollerName     = \"clusterDeployment\"\n\tserviceAccountName = \"cluster-installer\" // service account that can run the installer and upload artifacts to the cluster's namespace.\n\tdefaultRequeueTime = 10 * time.Second\n\n\tadminSSHKeySecretKey  = \"ssh-publickey\"\n\tadminKubeconfigKey    = \"kubeconfig\"\n\trawAdminKubeconfigKey = \"raw-kubeconfig\"\n\n\tclusterImageSetNotFoundReason = \"ClusterImageSetNotFound\"\n\tclusterImageSetFoundReason    = \"ClusterImageSetFound\"\n\n\tdnsNotReadyReason  = \"DNSNotReady\"\n\tdnsReadyReason     = \"DNSReady\"\n\tdnsReadyAnnotation = \"hive.openshift.io/dnsready\"\n\n\tclusterDeploymentGenerationAnnotation = \"hive.openshift.io/cluster-deployment-generation\"\n\tjobHashAnnotation                     = \"hive.openshift.io/jobhash\"\n\tfirstTimeInstallAnnotation            = \"hive.openshift.io/first-time-install\"\n\tdeleteAfterAnnotation                 = \"hive.openshift.io/delete-after\" // contains a duration after which the cluster should be cleaned up.\n)\n\nvar (\n\tmetricCompletedInstallJobRestarts = prometheus.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"hive_cluster_deployment_completed_install_restart\",\n\t\t\tHelp:    \"Distribution of the number of restarts for all completed cluster installations.\",\n\t\t\tBuckets: []float64{0, 2, 10, 20, 50},\n\t\t},\n\t\t[]string{\"cluster_type\"},\n\t)\n\tmetricInstallJobDuration = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"hive_cluster_deployment_install_job_duration_seconds\",\n\t\t\tHelp:    \"Distribution of the runtime of completed install jobs.\",\n\t\t\tBuckets: []float64{60, 300, 600, 1200, 1800, 2400, 3000, 3600},\n\t\t},\n\t)\n\tmetricInstallDelaySeconds = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"hive_cluster_deployment_install_job_delay_seconds\",\n\t\t\tHelp:    \"Time between cluster deployment creation and creation of the job to install/provision the cluster.\",\n\t\t\tBuckets: []float64{30, 60, 120, 300, 600, 1200, 1800},\n\t\t},\n\t)\n\tmetricImageSetDelaySeconds = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"hive_cluster_deployment_imageset_job_delay_seconds\",\n\t\t\tHelp:    \"Time between cluster deployment creation and creation of the job which resolves the installer image to use for a ClusterImageSet.\",\n\t\t\tBuckets: []float64{10, 30, 60, 300, 600, 1200, 1800},\n\t\t},\n\t)\n\tmetricClustersCreated = prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"hive_cluster_deployments_created_total\",\n\t\tHelp: \"Counter incremented every time we observe a new cluster.\",\n\t},\n\t\t[]string{\"cluster_type\"},\n\t)\n\tmetricClustersInstalled = prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"hive_cluster_deployments_installed_total\",\n\t\tHelp: \"Counter incremented every time we observe a successful installation.\",\n\t},\n\t\t[]string{\"cluster_type\"},\n\t)\n\tmetricClustersDeleted = prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\tName: \"hive_cluster_deployments_deleted_total\",\n\t\tHelp: \"Counter incremented every time we observe a deleted cluster.\",\n\t},\n\t\t[]string{\"cluster_type\"},\n\t)\n\tmetricDNSDelaySeconds = prometheus.NewHistogram(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"hive_cluster_deployment_dns_delay_seconds\",\n\t\t\tHelp:    \"Time between cluster deployment with spec.manageDNS creation and the DNSZone becoming ready.\",\n\t\t\tBuckets: []float64{10, 30, 60, 300, 600, 1200, 1800},\n\t\t},\n\t)\n\n\t// regex to find/replace wildcard ingress entries\n\t// case-insensitive leading literal '*' followed by a literal '.'\n\twildcardDomain = regexp.MustCompile(`(?i)^\\*\\.`)\n)\n\nfunc init() {\n\tmetrics.Registry.MustRegister(metricInstallJobDuration)\n\tmetrics.Registry.MustRegister(metricCompletedInstallJobRestarts)\n\tmetrics.Registry.MustRegister(metricInstallDelaySeconds)\n\tmetrics.Registry.MustRegister(metricImageSetDelaySeconds)\n\tmetrics.Registry.MustRegister(metricClustersCreated)\n\tmetrics.Registry.MustRegister(metricClustersInstalled)\n\tmetrics.Registry.MustRegister(metricClustersDeleted)\n\tmetrics.Registry.MustRegister(metricDNSDelaySeconds)\n}\n\n// Add creates a new ClusterDeployment controller and adds it to the manager with default RBAC.\nfunc Add(mgr manager.Manager) error {\n\treturn AddToManager(mgr, NewReconciler(mgr))\n}\n\n// NewReconciler returns a new reconcile.Reconciler\nfunc NewReconciler(mgr manager.Manager) reconcile.Reconciler {\n\treturn &ReconcileClusterDeployment{\n\t\tClient:                        controllerutils.NewClientWithMetricsOrDie(mgr, controllerName),\n\t\tscheme:                        mgr.GetScheme(),\n\t\tlogger:                        log.WithField(\"controller\", controllerName),\n\t\tremoteClusterAPIClientBuilder: controllerutils.BuildClusterAPIClientFromKubeconfig,\n\t}\n}\n\n// AddToManager adds a new Controller to mgr with r as the reconcile.Reconciler\nfunc AddToManager(mgr manager.Manager, r reconcile.Reconciler) error {\n\tc, err := controller.New(\"clusterdeployment-controller\", mgr, controller.Options{Reconciler: r, MaxConcurrentReconciles: controllerutils.GetConcurrentReconciles()})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error getting new cluster deployment\")\n\t\treturn err\n\t}\n\n\t// Watch for changes to ClusterDeployment\n\terr = c.Watch(&source.Kind{Type: &hivev1.ClusterDeployment{}}, &handler.EnqueueRequestForObject{})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error watching cluster deployment\")\n\t\treturn err\n\t}\n\n\t// Watch for jobs created by a ClusterDeployment:\n\terr = c.Watch(&source.Kind{Type: &batchv1.Job{}}, &handler.EnqueueRequestForOwner{\n\t\tIsController: true,\n\t\tOwnerType:    &hivev1.ClusterDeployment{},\n\t})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error watching cluster deployment job\")\n\t\treturn err\n\t}\n\n\t// Watch for pods created by an install job\n\terr = c.Watch(&source.Kind{Type: &corev1.Pod{}}, &handler.EnqueueRequestsFromMapFunc{\n\t\tToRequests: handler.ToRequestsFunc(selectorPodWatchHandler),\n\t})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error watching cluster deployment pods\")\n\t\treturn err\n\t}\n\n\t// Watch for deprovision requests created by a ClusterDeployment\n\terr = c.Watch(&source.Kind{Type: &hivev1.ClusterDeprovisionRequest{}}, &handler.EnqueueRequestForOwner{\n\t\tIsController: true,\n\t\tOwnerType:    &hivev1.ClusterDeployment{},\n\t})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error watching deprovision request created by cluster deployment\")\n\t\treturn err\n\t}\n\n\t// Watch for dnszones created by a ClusterDeployment\n\terr = c.Watch(&source.Kind{Type: &hivev1.DNSZone{}}, &handler.EnqueueRequestForOwner{\n\t\tIsController: true,\n\t\tOwnerType:    &hivev1.ClusterDeployment{},\n\t})\n\tif err != nil {\n\t\tlog.WithField(\"controller\", controllerName).WithError(err).Error(\"Error watching cluster deployment dnszones\")\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nvar _ reconcile.Reconciler = &ReconcileClusterDeployment{}\n\n// ReconcileClusterDeployment reconciles a ClusterDeployment object\ntype ReconcileClusterDeployment struct {\n\tclient.Client\n\tscheme *runtime.Scheme\n\tlogger log.FieldLogger\n\n\t// remoteClusterAPIClientBuilder is a function pointer to the function that builds a client for the\n\t// remote cluster's cluster-api\n\tremoteClusterAPIClientBuilder func(string, string) (client.Client, error)\n}\n\n// Reconcile reads that state of the cluster for a ClusterDeployment object and makes changes based on the state read\n// and what is in the ClusterDeployment.Spec\n//\n// Automatically generate RBAC rules to allow the Controller to read and write Deployments\n//\n// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=core,resources=serviceaccounts;secrets;configmaps;events,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=core,resources=pods;namespaces,verbs=get;list;watch\n// +kubebuilder:rbac:groups=rbac.authorization.k8s.io,resources=roles;rolebindings,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=hive.openshift.io,resources=clusterdeployments;clusterdeployments/status;clusterdeployments/finalizers,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=hive.openshift.io,resources=clusterimagesets,verbs=get;list;watch;create;update;patch;delete\n// +kubebuilder:rbac:groups=hive.openshift.io,resources=clusterimagesets/status,verbs=get;update;patch\nfunc (r *ReconcileClusterDeployment) Reconcile(request reconcile.Request) (reconcile.Result, error) {\n\tstart := time.Now()\n\tcdLog := r.logger.WithFields(log.Fields{\n\t\t\"controller\":        controllerName,\n\t\t\"clusterDeployment\": request.Name,\n\t\t\"namespace\":         request.Namespace,\n\t})\n\n\t// For logging, we need to see when the reconciliation loop starts and ends.\n\tcdLog.Info(\"reconciling cluster deployment\")\n\tdefer func() {\n\t\tdur := time.Since(start)\n\t\thivemetrics.MetricControllerReconcileTime.WithLabelValues(controllerName).Observe(dur.Seconds())\n\t\tcdLog.WithField(\"elapsed\", dur).Info(\"reconcile complete\")\n\t}()\n\n\t// Fetch the ClusterDeployment instance\n\tcd := &hivev1.ClusterDeployment{}\n\terr := r.Get(context.TODO(), request.NamespacedName, cd)\n\tif err != nil {\n\t\tif apierrors.IsNotFound(err) {\n\t\t\t// Object not found, return.  Created objects are automatically garbage collected.\n\t\t\t// For additional cleanup logic use finalizers.\n\t\t\tcdLog.Info(\"cluster deployment Not Found\")\n\t\t\treturn reconcile.Result{}, nil\n\t\t}\n\t\t// Error reading the object - requeue the request.\n\t\tcdLog.WithError(err).Error(\"Error getting cluster deployment\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\treturn r.reconcile(request, cd, cdLog)\n}\n\nfunc (r *ReconcileClusterDeployment) reconcile(request reconcile.Request, cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (reconcile.Result, error) {\n\torigCD := cd\n\tcd = cd.DeepCopy()\n\n\t// TODO: We may want to remove this fix in future.\n\t// Handle pre-existing clusters with older status version structs that did not have the new\n\t// cluster version mandatory fields defined.\n\t// NOTE: removing this is causing the imageset job to fail. Please leave it in until\n\t// we can determine what needs to be fixed.\n\tcontrollerutils.FixupEmptyClusterVersionFields(&cd.Status.ClusterVersionStatus)\n\tif !reflect.DeepEqual(origCD.Status, cd.Status) {\n\t\tcdLog.Info(\"correcting empty cluster version fields\")\n\t\terr := r.Status().Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error updating cluster deployment status\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\treturn reconcile.Result{\n\t\t\tRequeue:      true,\n\t\t\tRequeueAfter: defaultRequeueTime,\n\t\t}, nil\n\t}\n\n\t// We previously allowed clusterdeployment.spec.ingress[] entries to have ingress domains with a leading '*'.\n\t// Migrate the clusterdeployment to the new format if we find a wildcard ingress domain.\n\t// TODO: we can one day remove this once all clusterdeployment are known to have non-wildcard data\n\tif migrateWildcardIngress(cd) {\n\t\tcdLog.Info(\"migrating wildcard ingress entries\")\n\t\terr := r.Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"failed to update cluster deployment\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\treturn reconcile.Result{}, nil\n\t}\n\n\t// TODO: remove this once clusterdeployments have been migrated. We are no longer storing syncset status\n\t// on clusterdeployments, remove it.\n\tif len(cd.Status.SyncSetStatus) > 0 || len(cd.Status.SelectorSyncSetStatus) > 0 {\n\t\tcd.Status.SyncSetStatus = nil\n\t\tcd.Status.SelectorSyncSetStatus = nil\n\t\terr := r.Status().Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"failed to migrate cluster deployment status\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\treturn reconcile.Result{}, nil\n\t}\n\n\timageSet, modified, err := r.getClusterImageSet(cd, cdLog)\n\tif modified || err != nil {\n\t\treturn reconcile.Result{}, err\n\t}\n\n\thiveImage := r.getHiveImage(cd, imageSet, cdLog)\n\treleaseImage := r.getReleaseImage(cd, imageSet, cdLog)\n\n\tif cd.DeletionTimestamp != nil {\n\t\tif !controllerutils.HasFinalizer(cd, hivev1.FinalizerDeprovision) {\n\t\t\tclearUnderwaySecondsMetrics(cd)\n\t\t\treturn reconcile.Result{}, nil\n\t\t}\n\n\t\t// Deprovision still underway, report metric for this cluster.\n\t\thivemetrics.MetricClusterDeploymentDeprovisioningUnderwaySeconds.WithLabelValues(\n\t\t\tcd.Name,\n\t\t\tcd.Namespace,\n\t\t\thivemetrics.GetClusterDeploymentType(cd)).Set(\n\t\t\ttime.Since(cd.DeletionTimestamp.Time).Seconds())\n\n\t\t// If the cluster never made it to installed, make sure we clear the provisioning\n\t\t// underway metric.\n\t\tif !cd.Status.Installed {\n\t\t\thivemetrics.MetricClusterDeploymentProvisionUnderwaySeconds.WithLabelValues(\n\t\t\t\tcd.Name,\n\t\t\t\tcd.Namespace,\n\t\t\t\thivemetrics.GetClusterDeploymentType(cd)).Set(0.0)\n\t\t}\n\n\t\treturn r.syncDeletedClusterDeployment(cd, hiveImage, cdLog)\n\t}\n\n\t// requeueAfter will be used to determine if cluster should be requeued after\n\t// reconcile has completed\n\tvar requeueAfter time.Duration\n\t// Check for the delete-after annotation, and if the cluster has expired, delete it\n\tdeleteAfter, ok := cd.Annotations[deleteAfterAnnotation]\n\tif ok {\n\t\tcdLog.Debugf(\"found delete after annotation: %s\", deleteAfter)\n\t\tdur, err := time.ParseDuration(deleteAfter)\n\t\tif err != nil {\n\t\t\treturn reconcile.Result{}, fmt.Errorf(\"error parsing %s as a duration: %v\", deleteAfterAnnotation, err)\n\t\t}\n\t\tif !cd.CreationTimestamp.IsZero() {\n\t\t\texpiry := cd.CreationTimestamp.Add(dur)\n\t\t\tcdLog.Debugf(\"cluster expires at: %s\", expiry)\n\t\t\tif time.Now().After(expiry) {\n\t\t\t\tcdLog.WithField(\"expiry\", expiry).Info(\"cluster has expired, issuing delete\")\n\t\t\t\terr := r.Delete(context.TODO(), cd)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcdLog.WithError(err).Error(\"error deleting expired cluster\")\n\t\t\t\t}\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\n\t\t\t// We have an expiry time but we're not expired yet. Set requeueAfter for just after expiry time\n\t\t\t// so that we requeue cluster for deletion once reconcile has completed\n\t\t\trequeueAfter = expiry.Sub(time.Now()) + 60*time.Second\n\t\t}\n\t}\n\n\tif !controllerutils.HasFinalizer(cd, hivev1.FinalizerDeprovision) {\n\t\tcdLog.Debugf(\"adding clusterdeployment finalizer\")\n\t\tif err := r.addClusterDeploymentFinalizer(cd); err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error adding finalizer\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\tmetricClustersCreated.WithLabelValues(hivemetrics.GetClusterDeploymentType(cd)).Inc()\n\t\treturn reconcile.Result{}, nil\n\t}\n\n\tcdLog.Debug(\"loading SSH key secret\")\n\tif cd.Spec.SSHKey == nil {\n\t\tcdLog.Error(\"cluster has no ssh key set, unable to launch install\")\n\t\treturn reconcile.Result{}, fmt.Errorf(\"cluster has no ssh key set, unable to launch install\")\n\t}\n\tsshKey, err := controllerutils.LoadSecretData(r.Client, cd.Spec.SSHKey.Name,\n\t\tcd.Namespace, adminSSHKeySecretKey)\n\tif err != nil {\n\t\tcdLog.WithError(err).Error(\"unable to load ssh key from secret\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\tcdLog.Debug(\"loading pull secrets\")\n\tpullSecret, err := r.mergePullSecrets(cd, cdLog)\n\tif err != nil {\n\t\tcdLog.WithError(err).Error(\"Error merging pull secrets\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// Update the pull secret object if required\n\tmodifiedCD, err := r.updatePullSecretInfo(pullSecret, cd, cdLog)\n\tif err != nil || modifiedCD {\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"Error updating the merged pull secret\")\n\t\t}\n\t\treturn reconcile.Result{}, err\n\t}\n\n\tif cd.Status.InstallerImage == nil {\n\t\treturn r.resolveInstallerImage(cd, imageSet, releaseImage, hiveImage, cdLog)\n\t}\n\n\tif cd.Spec.ManageDNS {\n\t\tmanagedDNSZoneAvailable, dnsZone, err := r.ensureManagedDNSZone(cd, cdLog)\n\t\tif err != nil {\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\n\t\tmodified, err := r.setDNSNotReadyCondition(cd, managedDNSZoneAvailable, cdLog)\n\t\tif modified || err != nil {\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\n\t\tif !managedDNSZoneAvailable {\n\t\t\t// The clusterdeployment will be queued when the owned DNSZone's status\n\t\t\t// is updated to available.\n\t\t\tcdLog.Debug(\"DNSZone is not yet available. Waiting for zone to become available.\")\n\t\t\treturn reconcile.Result{}, nil\n\t\t}\n\t\tupdated, err := r.setDNSDelayMetric(cd, dnsZone, cdLog)\n\t\tif updated || err != nil {\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t}\n\n\t// firstInstalledObserve is the flag that is used for reporting the provision job duration metric\n\tfirstInstalledObserve := false\n\tcontainerRestarts := 0\n\t// Check if an install job already exists:\n\texistingJob := &batchv1.Job{}\n\tinstallJobName := install.GetInstallJobName(cd)\n\terr = r.Get(context.TODO(), types.NamespacedName{Name: installJobName, Namespace: cd.Namespace}, existingJob)\n\tif err != nil {\n\t\tif apierrors.IsNotFound(err) {\n\t\t\tcdLog.Debug(\"no install job exists\")\n\t\t\texistingJob = nil\n\t\t} else {\n\t\t\tcdLog.WithError(err).Error(\"error looking for install job\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t} else {\n\t\tif !existingJob.DeletionTimestamp.IsZero() {\n\t\t\tcdLog.WithError(err).Error(\"install job is being deleted, requeueing to wait for deletion\")\n\t\t\treturn reconcile.Result{RequeueAfter: defaultRequeueTime}, nil\n\t\t}\n\t\t// setting the flag so that we can report the metric after cd is installed\n\t\tif existingJob.Status.Succeeded > 0 && !cd.Status.Installed {\n\t\t\tfirstInstalledObserve = true\n\t\t}\n\t}\n\n\tif cd.Status.Installed {\n\t\tcdLog.Debug(\"cluster is already installed, no processing of install job needed\")\n\t} else {\n\t\t// Indicate that the cluster is still installing:\n\t\thivemetrics.MetricClusterDeploymentProvisionUnderwaySeconds.WithLabelValues(\n\t\t\tcd.Name,\n\t\t\tcd.Namespace,\n\t\t\thivemetrics.GetClusterDeploymentType(cd)).Set(\n\t\t\ttime.Since(cd.CreationTimestamp.Time).Seconds())\n\n\t\tjob, err := install.GenerateInstallerJob(\n\t\t\tcd,\n\t\t\thiveImage,\n\t\t\treleaseImage,\n\t\t\tserviceAccountName,\n\t\t\tsshKey)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error generating install job\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\n\t\tjobHash, err := controllerutils.CalculateJobSpecHash(job)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"failed to calculate hash for generated install job\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\tif job.Annotations == nil {\n\t\t\tjob.Annotations = map[string]string{}\n\t\t}\n\t\tjob.Annotations[jobHashAnnotation] = jobHash\n\n\t\tif err = controllerutil.SetControllerReference(cd, job, r.scheme); err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error setting controller reference on job\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\n\t\tcdLog = cdLog.WithField(\"job\", job.Name)\n\n\t\tif existingJob == nil {\n\t\t\tcdLog.Infof(\"creating install job\")\n\t\t\t_, err = controllerutils.SetupClusterInstallServiceAccount(r, cd.Namespace, cdLog)\n\t\t\tif err != nil {\n\t\t\t\tcdLog.WithError(err).Error(\"error setting up service account and role\")\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\n\t\t\terr = r.Create(context.TODO(), job)\n\t\t\tif err != nil {\n\t\t\t\tcdLog.Errorf(\"error creating job: %v\", err)\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\t\t\tif _, ok := cd.Annotations[firstTimeInstallAnnotation]; !ok {\n\t\t\t\tinitializeAnnotations(cd)\n\n\t\t\t\t// Add the annotation for first time install\n\t\t\t\tcd.Annotations[firstTimeInstallAnnotation] = \"true\"\n\t\t\t\tif err := r.Client.Update(context.TODO(), cd); err != nil {\n\t\t\t\t\tcdLog.WithError(err).Error(\"failed to save annotation for firstTimeInstall\")\n\t\t\t\t}\n\t\t\t\tkickstartDuration := time.Since(cd.CreationTimestamp.Time)\n\t\t\t\tcdLog.WithField(\"elapsed\", kickstartDuration.Seconds()).Info(\"calculated time to install job seconds\")\n\t\t\t\tmetricInstallDelaySeconds.Observe(float64(kickstartDuration.Seconds()))\n\t\t\t}\n\t\t} else {\n\t\t\tcdLog.Debug(\"provision job exists\")\n\t\t\tcontainerRestarts, err = r.calcInstallPodRestarts(cd, cdLog)\n\t\t\tif err != nil {\n\t\t\t\t// Metrics calculation should not shut down reconciliation, logging and moving on.\n\t\t\t\tcdLog.WithError(err).Warn(\"error listing pods, unable to calculate pod restarts but continuing\")\n\t\t\t} else {\n\t\t\t\tif containerRestarts > 0 {\n\t\t\t\t\tcdLog.WithFields(log.Fields{\n\t\t\t\t\t\t\"restarts\": containerRestarts,\n\t\t\t\t\t}).Warn(\"install pod has restarted\")\n\t\t\t\t}\n\n\t\t\t\t// Store the restart count on the cluster deployment status.\n\t\t\t\tcd.Status.InstallRestarts = containerRestarts\n\t\t\t}\n\n\t\t\tif existingJob.Annotations != nil {\n\t\t\t\tdidGenerationChange, err := r.updateOutdatedConfigurations(cd.Generation, existingJob, cdLog)\n\t\t\t\tif didGenerationChange || err != nil {\n\t\t\t\t\treturn reconcile.Result{}, err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tjobDeleted, err := r.deleteJobOnHashChange(existingJob, job, cdLog)\n\t\t\tif jobDeleted || err != nil {\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\t\t}\n\t}\n\n\terr = r.updateClusterDeploymentStatus(cd, origCD, existingJob, cdLog)\n\tif err != nil {\n\t\tcdLog.WithError(err).Errorf(\"error updating cluster deployment status\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// firstInstalledObserve will be true if this is the first time we've noticed the install job completed.\n\t// If true, we know we can report the metrics associated with a completed job.\n\tif firstInstalledObserve {\n\t\t// jobDuration calculates the time elapsed since the install job started\n\t\tjobDuration := existingJob.Status.CompletionTime.Time.Sub(existingJob.Status.StartTime.Time)\n\t\tcdLog.WithField(\"duration\", jobDuration.Seconds()).Debug(\"install job completed\")\n\t\tmetricInstallJobDuration.Observe(float64(jobDuration.Seconds()))\n\n\t\t// Report a metric for the total number of container restarts:\n\t\tmetricCompletedInstallJobRestarts.WithLabelValues(hivemetrics.GetClusterDeploymentType(cd)).\n\t\t\tObserve(float64(containerRestarts))\n\n\t\t// Clear the install underway seconds metric. After this no-one should be reporting\n\t\t// this metric for this cluster.\n\t\thivemetrics.MetricClusterDeploymentProvisionUnderwaySeconds.WithLabelValues(\n\t\t\tcd.Name,\n\t\t\tcd.Namespace,\n\t\t\thivemetrics.GetClusterDeploymentType(cd)).Set(0.0)\n\n\t\tmetricClustersInstalled.WithLabelValues(hivemetrics.GetClusterDeploymentType(cd)).Inc()\n\t}\n\n\t// Check for requeueAfter duration\n\tif requeueAfter != 0 {\n\t\tcdLog.Debugf(\"cluster will re-sync due to expiry time in: %v\", requeueAfter)\n\t\treturn reconcile.Result{RequeueAfter: requeueAfter}, nil\n\t}\n\treturn reconcile.Result{}, nil\n}\n\n// getHiveImage looks for a Hive image to use in clusterdeployment jobs in the following order:\n// 1 - specified in the cluster deployment spec.images.hiveImage\n// 2 - referenced in the cluster deployment spec.imageSet\n// 3 - specified via environment variable to the hive controller\n// 4 - fallback default hardcoded image reference\nfunc (r *ReconcileClusterDeployment) getHiveImage(cd *hivev1.ClusterDeployment, imageSet *hivev1.ClusterImageSet, cdLog log.FieldLogger) string {\n\tif cd.Spec.Images.HiveImage != \"\" {\n\t\treturn cd.Spec.Images.HiveImage\n\t}\n\tif imageSet != nil && imageSet.Spec.HiveImage != nil {\n\t\treturn *imageSet.Spec.HiveImage\n\t}\n\treturn images.GetHiveImage(cdLog)\n}\n\n// getReleaseImage looks for a a release image in clusterdeployment or its corresponding imageset in the following order:\n// 1 - specified in the cluster deployment spec.images.releaseImage\n// 2 - referenced in the cluster deployment spec.imageSet\nfunc (r *ReconcileClusterDeployment) getReleaseImage(cd *hivev1.ClusterDeployment, imageSet *hivev1.ClusterImageSet, cdLog log.FieldLogger) string {\n\tif cd.Spec.Images.ReleaseImage != \"\" {\n\t\treturn cd.Spec.Images.ReleaseImage\n\t}\n\tif imageSet != nil && imageSet.Spec.ReleaseImage != nil {\n\t\treturn *imageSet.Spec.ReleaseImage\n\t}\n\treturn \"\"\n}\n\nfunc (r *ReconcileClusterDeployment) getClusterImageSet(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (*hivev1.ClusterImageSet, bool, error) {\n\tif cd.Spec.ImageSet == nil || len(cd.Spec.ImageSet.Name) == 0 {\n\t\treturn nil, false, nil\n\t}\n\timageSet := &hivev1.ClusterImageSet{}\n\terr := r.Get(context.TODO(), types.NamespacedName{Name: cd.Spec.ImageSet.Name}, imageSet)\n\tswitch {\n\tcase apierrors.IsNotFound(err):\n\t\tcdLog.WithField(\"clusterimageset\", cd.Spec.ImageSet.Name).Warning(\"clusterdeployment references non-existent clusterimageset\")\n\t\tmodified, err := r.setImageSetNotFoundCondition(cd, false, cdLog)\n\t\treturn nil, modified, err\n\tcase err != nil:\n\t\tcdLog.WithError(err).WithField(\"clusterimageset\", cd.Spec.ImageSet.Name).Error(\"unexpected error retrieving clusterimageset\")\n\t\treturn nil, false, err\n\tdefault:\n\t\treturn imageSet, false, nil\n\t}\n}\n\nfunc (r *ReconcileClusterDeployment) statusUpdate(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) error {\n\terr := r.Status().Update(context.TODO(), cd)\n\tif err != nil {\n\t\tcdLog.WithError(err).Error(\"cannot update clusterdeployment status\")\n\t}\n\treturn err\n}\n\nfunc (r *ReconcileClusterDeployment) resolveInstallerImage(cd *hivev1.ClusterDeployment, imageSet *hivev1.ClusterImageSet, releaseImage, hiveImage string, cdLog log.FieldLogger) (reconcile.Result, error) {\n\tif len(cd.Spec.Images.InstallerImage) > 0 {\n\t\tcdLog.WithField(\"image\", cd.Spec.Images.InstallerImage).\n\t\t\tDebug(\"setting status.InstallerImage to the value in spec.images.installerImage\")\n\t\tcd.Status.InstallerImage = &cd.Spec.Images.InstallerImage\n\t\treturn reconcile.Result{}, r.statusUpdate(cd, cdLog)\n\t}\n\tif imageSet != nil && imageSet.Spec.InstallerImage != nil {\n\t\tcd.Status.InstallerImage = imageSet.Spec.InstallerImage\n\t\tcdLog.WithField(\"imageset\", imageSet.Name).Debug(\"setting status.InstallerImage using imageSet.Spec.InstallerImage\")\n\t\treturn reconcile.Result{}, r.statusUpdate(cd, cdLog)\n\t}\n\tcliImage := images.GetCLIImage(cdLog)\n\tjob := imageset.GenerateImageSetJob(cd, releaseImage, serviceAccountName, imageset.AlwaysPullImage(cliImage), imageset.AlwaysPullImage(hiveImage))\n\tif err := controllerutil.SetControllerReference(cd, job, r.scheme); err != nil {\n\t\tcdLog.WithError(err).Error(\"error setting controller reference on job\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\tjobName := types.NamespacedName{Name: job.Name, Namespace: job.Namespace}\n\tjobLog := cdLog.WithField(\"job\", jobName)\n\n\texistingJob := &batchv1.Job{}\n\terr := r.Get(context.TODO(), jobName, existingJob)\n\tswitch {\n\t// If the job exists but is in the process of getting deleted, requeue and wait for the delete\n\t// to complete.\n\tcase err == nil && !job.DeletionTimestamp.IsZero():\n\t\tjobLog.Debug(\"imageset job is being deleted. Will recreate once deleted\")\n\t\treturn reconcile.Result{RequeueAfter: defaultRequeueTime}, err\n\t// If job exists and is finished, delete so we can recreate it\n\tcase err == nil && controllerutils.IsFinished(existingJob):\n\t\tjobLog.WithField(\"successful\", controllerutils.IsSuccessful(existingJob)).\n\t\t\tWarning(\"Finished job found, but installer image is not yet resolved. Deleting.\")\n\t\terr := r.Delete(context.Background(), existingJob,\n\t\t\tclient.PropagationPolicy(metav1.DeletePropagationForeground))\n\t\tif err != nil {\n\t\t\tjobLog.WithError(err).Error(\"cannot delete imageset job\")\n\t\t}\n\t\treturn reconcile.Result{}, err\n\tcase apierrors.IsNotFound(err):\n\t\tjobLog.WithField(\"releaseImage\", releaseImage).Info(\"creating imageset job\")\n\t\t_, err = controllerutils.SetupClusterInstallServiceAccount(r, cd.Namespace, cdLog)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error setting up service account and role\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\n\t\terr = r.Create(context.TODO(), job)\n\t\tif err != nil {\n\t\t\tjobLog.WithError(err).Error(\"error creating job\")\n\t\t} else {\n\t\t\t// kickstartDuration calculates the delay between creation of cd and start of imageset job\n\t\t\tkickstartDuration := time.Since(cd.CreationTimestamp.Time)\n\t\t\tcdLog.WithField(\"elapsed\", kickstartDuration.Seconds()).Info(\"calculated time to imageset job seconds\")\n\t\t\tmetricImageSetDelaySeconds.Observe(float64(kickstartDuration.Seconds()))\n\t\t}\n\t\treturn reconcile.Result{}, err\n\tcase err != nil:\n\t\tjobLog.WithError(err).Error(\"cannot get job\")\n\t\treturn reconcile.Result{}, err\n\tdefault:\n\t\tjobLog.Debug(\"job exists and is in progress\")\n\t}\n\treturn reconcile.Result{}, nil\n}\n\nfunc (r *ReconcileClusterDeployment) setDNSNotReadyCondition(cd *hivev1.ClusterDeployment, isReady bool, cdLog log.FieldLogger) (modified bool, err error) {\n\toriginal := cd.DeepCopy()\n\tstatus := corev1.ConditionFalse\n\treason := dnsReadyReason\n\tmessage := \"DNS Zone available\"\n\tif !isReady {\n\t\tstatus = corev1.ConditionTrue\n\t\treason = dnsNotReadyReason\n\t\tmessage = \"DNS Zone not yet available\"\n\t}\n\tcd.Status.Conditions = controllerutils.SetClusterDeploymentCondition(\n\t\tcd.Status.Conditions,\n\t\thivev1.DNSNotReadyCondition,\n\t\tstatus,\n\t\treason,\n\t\tmessage,\n\t\tcontrollerutils.UpdateConditionNever)\n\tif !reflect.DeepEqual(original.Status.Conditions, cd.Status.Conditions) {\n\t\tcdLog.Debugf(\"setting DNSNotReadyCondition to %v\", status)\n\t\terr := r.Status().Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"cannot update status conditions\")\n\t\t}\n\t\treturn true, err\n\t}\n\treturn false, nil\n}\n\nfunc (r *ReconcileClusterDeployment) setImageSetNotFoundCondition(cd *hivev1.ClusterDeployment, isNotFound bool, cdLog log.FieldLogger) (modified bool, err error) {\n\toriginal := cd.DeepCopy()\n\tstatus := corev1.ConditionFalse\n\treason := clusterImageSetFoundReason\n\tmessage := fmt.Sprintf(\"ClusterImageSet %s is available\", cd.Spec.ImageSet.Name)\n\tif isNotFound {\n\t\tstatus = corev1.ConditionTrue\n\t\treason = clusterImageSetNotFoundReason\n\t\tmessage = fmt.Sprintf(\"ClusterImageSet %s is not available\", cd.Spec.ImageSet.Name)\n\t}\n\tcd.Status.Conditions = controllerutils.SetClusterDeploymentCondition(\n\t\tcd.Status.Conditions,\n\t\thivev1.ClusterImageSetNotFoundCondition,\n\t\tstatus,\n\t\treason,\n\t\tmessage,\n\t\tcontrollerutils.UpdateConditionNever)\n\tif !reflect.DeepEqual(original.Status.Conditions, cd.Status.Conditions) {\n\t\tcdLog.Info(\"setting ClusterImageSetNotFoundCondition to %v\", status)\n\t\terr := r.Status().Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"cannot update status conditions\")\n\t\t}\n\t\treturn true, err\n\t}\n\treturn false, nil\n}\n\n// Deletes the job if it exists and its generation does not match the cluster deployment's\n// genetation. Updates the config map if it is outdated too\nfunc (r *ReconcileClusterDeployment) updateOutdatedConfigurations(cdGeneration int64, existingJob *batchv1.Job, cdLog log.FieldLogger) (bool, error) {\n\tvar err error\n\tvar didGenerationChange bool\n\tif jobGeneration, ok := existingJob.Annotations[clusterDeploymentGenerationAnnotation]; ok {\n\t\tconvertedJobGeneration, _ := strconv.ParseInt(jobGeneration, 10, 64)\n\t\tif convertedJobGeneration < cdGeneration {\n\t\t\tdidGenerationChange = true\n\t\t\tcdLog.Info(\"deleting outdated install job due to cluster deployment generation change\")\n\t\t\terr = r.Delete(context.TODO(), existingJob, client.PropagationPolicy(metav1.DeletePropagationForeground))\n\t\t\tif err != nil {\n\t\t\t\tcdLog.WithError(err).Errorf(\"error deleting outdated install job\")\n\t\t\t\treturn didGenerationChange, err\n\t\t\t}\n\t\t}\n\t}\n\treturn didGenerationChange, err\n}\n\nfunc (r *ReconcileClusterDeployment) updateClusterDeploymentStatus(cd *hivev1.ClusterDeployment, origCD *hivev1.ClusterDeployment, job *batchv1.Job, cdLog log.FieldLogger) error {\n\tcdLog.Debug(\"updating cluster deployment status\")\n\tif job != nil && job.Name != \"\" && job.Namespace != \"\" {\n\t\t// Job exists, check it's status:\n\t\tcd.Status.Installed = controllerutils.IsSuccessful(job)\n\t}\n\n\t// The install manager sets this secret name, but we don't consider it a critical failure and\n\t// will attempt to heal it here, as the value is predictable.\n\tif cd.Status.Installed && cd.Status.AdminKubeconfigSecret.Name == \"\" {\n\t\tcd.Status.AdminKubeconfigSecret = corev1.LocalObjectReference{Name: apihelpers.GetResourceName(cd.Name, \"admin-kubeconfig\")}\n\t}\n\n\tif cd.Status.AdminKubeconfigSecret.Name != \"\" {\n\t\tadminKubeconfigSecret := &corev1.Secret{}\n\t\terr := r.Get(context.Background(), types.NamespacedName{Namespace: cd.Namespace, Name: cd.Status.AdminKubeconfigSecret.Name}, adminKubeconfigSecret)\n\t\tif err != nil {\n\t\t\tif apierrors.IsNotFound(err) {\n\t\t\t\tlog.Warn(\"admin kubeconfig does not yet exist\")\n\t\t\t} else {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\terr = r.fixupAdminKubeconfigSecret(adminKubeconfigSecret, cdLog)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = r.setAdminKubeconfigStatus(cd, adminKubeconfigSecret, cdLog)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Update cluster deployment status if changed:\n\tif !reflect.DeepEqual(cd.Status, origCD.Status) {\n\t\tcdLog.Infof(\"status has changed, updating cluster deployment\")\n\t\tcdLog.Debugf(\"orig: %v\", origCD)\n\t\tcdLog.Debugf(\"new : %v\", cd.Status)\n\t\terr := r.Status().Update(context.TODO(), cd)\n\t\tif err != nil {\n\t\t\tcdLog.Errorf(\"error updating cluster deployment: %v\", err)\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tcdLog.Debug(\"cluster deployment status unchanged\")\n\t}\n\treturn nil\n}\n\nfunc (r *ReconcileClusterDeployment) fixupAdminKubeconfigSecret(secret *corev1.Secret, cdLog log.FieldLogger) error {\n\toriginalSecret := secret.DeepCopy()\n\n\trawData, hasRawData := secret.Data[rawAdminKubeconfigKey]\n\tif !hasRawData {\n\t\tsecret.Data[rawAdminKubeconfigKey] = secret.Data[adminKubeconfigKey]\n\t\trawData = secret.Data[adminKubeconfigKey]\n\t}\n\n\tvar err error\n\tsecret.Data[adminKubeconfigKey], err = controllerutils.FixupKubeconfig(rawData)\n\tif err != nil {\n\t\tcdLog.WithError(err).Errorf(\"cannot fixup kubeconfig to generate new one\")\n\t\treturn err\n\t}\n\n\tif reflect.DeepEqual(originalSecret.Data, secret.Data) {\n\t\tcdLog.Debug(\"secret data has not changed, no need to update\")\n\t\treturn nil\n\t}\n\n\terr = r.Update(context.TODO(), secret)\n\tif err != nil {\n\t\tcdLog.WithError(err).Error(\"error updated admin kubeconfig secret\")\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// setAdminKubeconfigStatus sets all cluster status fields that depend on the admin kubeconfig.\nfunc (r *ReconcileClusterDeployment) setAdminKubeconfigStatus(cd *hivev1.ClusterDeployment, adminKubeconfigSecret *corev1.Secret, cdLog log.FieldLogger) error {\n\tif cd.Status.WebConsoleURL == \"\" || cd.Status.APIURL == \"\" {\n\t\tremoteClusterAPIClient, err := r.remoteClusterAPIClientBuilder(string(adminKubeconfigSecret.Data[adminKubeconfigKey]), controllerName)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error building remote cluster-api client connection\")\n\t\t\treturn err\n\t\t}\n\n\t\t// Parse the admin kubeconfig for the server URL:\n\t\tconfig, err := clientcmd.Load(adminKubeconfigSecret.Data[\"kubeconfig\"])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcluster, ok := config.Clusters[cd.Spec.ClusterName]\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"error parsing admin kubeconfig secret data\")\n\t\t}\n\n\t\t// We should be able to assume only one cluster in here:\n\t\tserver := cluster.Server\n\t\tcdLog.Debugf(\"found cluster API URL in kubeconfig: %s\", server)\n\t\tcd.Status.APIURL = server\n\t\trouteObject := &routev1.Route{}\n\t\terr = remoteClusterAPIClient.Get(context.Background(),\n\t\t\ttypes.NamespacedName{Namespace: \"openshift-console\", Name: \"console\"}, routeObject)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error fetching remote route object\")\n\t\t\treturn err\n\t\t}\n\t\tcdLog.Debugf(\"read remote route object: %s\", routeObject)\n\t\tcd.Status.WebConsoleURL = \"https://\" + routeObject.Spec.Host\n\t}\n\treturn nil\n}\n\n// ensureManagedDNSZoneDeleted is a safety check to ensure that the child managed DNSZone\n// linked to the parent cluster deployment gets a deletionTimestamp when the parent is deleted.\n// Normally we expect Kube garbage collection to do this for us, but in rare cases we've seen it\n// not working as intended.\nfunc (r *ReconcileClusterDeployment) ensureManagedDNSZoneDeleted(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (*reconcile.Result, error) {\n\tif !cd.Spec.ManageDNS {\n\t\treturn nil, nil\n\t}\n\tdnsZone := &hivev1.DNSZone{}\n\tdnsZoneNamespacedName := types.NamespacedName{Namespace: cd.Namespace, Name: dnsZoneName(cd.Name)}\n\terr := r.Get(context.TODO(), dnsZoneNamespacedName, dnsZone)\n\tif err != nil && !apierrors.IsNotFound(err) {\n\t\tcdLog.WithError(err).Error(\"error looking up managed dnszone\")\n\t\treturn &reconcile.Result{}, err\n\t}\n\tif apierrors.IsNotFound(err) || !dnsZone.DeletionTimestamp.IsZero() {\n\t\tcdLog.Debug(\"dnszone has been deleted or is getting deleted\")\n\t\treturn nil, nil\n\t}\n\tcdLog.Warn(\"managed dnszone did not get a deletionTimestamp when parent cluster deployment was deleted, deleting manually\")\n\terr = r.Delete(context.TODO(), dnsZone,\n\t\tclient.PropagationPolicy(metav1.DeletePropagationForeground))\n\tif err != nil {\n\t\tcdLog.WithError(err).Error(\"error deleting managed dnszone\")\n\t}\n\treturn &reconcile.Result{}, err\n}\n\nfunc (r *ReconcileClusterDeployment) syncDeletedClusterDeployment(cd *hivev1.ClusterDeployment, hiveImage string, cdLog log.FieldLogger) (reconcile.Result, error) {\n\n\tresult, err := r.ensureManagedDNSZoneDeleted(cd, cdLog)\n\tif result != nil {\n\t\treturn *result, err\n\t}\n\tif err != nil {\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// Delete the install job in case it's still running:\n\tinstallJob := &batchv1.Job{}\n\terr = r.Get(context.Background(),\n\t\ttypes.NamespacedName{\n\t\t\tName:      install.GetInstallJobName(cd),\n\t\t\tNamespace: cd.Namespace,\n\t\t},\n\t\tinstallJob)\n\tif err != nil && apierrors.IsNotFound(err) {\n\t\tcdLog.Debug(\"install job no longer exists, nothing to cleanup\")\n\t} else if err != nil {\n\t\tcdLog.WithError(err).Errorf(\"error getting existing install job for deleted cluster deployment\")\n\t\treturn reconcile.Result{}, err\n\t} else if !installJob.DeletionTimestamp.IsZero() {\n\t\tcdLog.WithField(\"finalizers\", installJob.Finalizers).Info(\"install job is being deleted, requeueing to wait for deletion\")\n\t\treturn reconcile.Result{RequeueAfter: defaultRequeueTime}, nil\n\t} else {\n\t\terr = r.Delete(context.Background(), installJob,\n\t\t\tclient.PropagationPolicy(metav1.DeletePropagationForeground))\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Errorf(\"error deleting existing install job for deleted cluster deployment\")\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\tcdLog.WithField(\"jobName\", installJob.Name).Info(\"install job deleted\")\n\t\treturn reconcile.Result{}, nil\n\t}\n\n\t// Skips creation of deprovision request if PreserveOnDelete is true and cluster is installed\n\tif cd.Spec.PreserveOnDelete {\n\t\tif cd.Status.Installed {\n\t\t\tcdLog.Warn(\"skipping creation of deprovisioning request for installed cluster due to PreserveOnDelete=true\")\n\t\t\tif controllerutils.HasFinalizer(cd, hivev1.FinalizerDeprovision) {\n\t\t\t\terr = r.removeClusterDeploymentFinalizer(cd)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcdLog.WithError(err).Error(\"error removing finalizer\")\n\t\t\t\t}\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\t\t\treturn reconcile.Result{}, nil\n\t\t}\n\t\t// Overriding PreserveOnDelete because we might have deleted the cluster deployment before it finished\n\t\t// installing, which can cause AWS resources to leak\n\t\tcdLog.Infof(\"PreserveOnDelete=true but creating deprovisioning request as cluster was never successfully provisioned\")\n\t}\n\n\tif cd.Status.InfraID == \"\" {\n\t\tcdLog.Warn(\"skipping uninstall for cluster that never had clusterID set\")\n\t\terr = r.removeClusterDeploymentFinalizer(cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error removing finalizer\")\n\t\t}\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// Generate a deprovision request\n\trequest := generateDeprovisionRequest(cd)\n\terr = controllerutil.SetControllerReference(cd, request, r.scheme)\n\tif err != nil {\n\t\tcdLog.Errorf(\"error setting controller reference on deprovision request: %v\", err)\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// Check if deprovision request already exists:\n\texistingRequest := &hivev1.ClusterDeprovisionRequest{}\n\terr = r.Get(context.TODO(), types.NamespacedName{Name: cd.Name, Namespace: cd.Namespace}, existingRequest)\n\tif err != nil && apierrors.IsNotFound(err) {\n\t\tcdLog.Infof(\"creating deprovision request for cluster deployment\")\n\t\terr = r.Create(context.TODO(), request)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Errorf(\"error creating deprovision request\")\n\t\t\t// Check if namespace is terminated, if so we can give up, remove the finalizer, and let\n\t\t\t// the cluster go away.\n\t\t\tns := &corev1.Namespace{}\n\t\t\terr = r.Get(context.TODO(), types.NamespacedName{Name: cd.Namespace}, ns)\n\t\t\tif err != nil {\n\t\t\t\tcdLog.WithError(err).Error(\"error checking for deletionTimestamp on namespace\")\n\t\t\t\treturn reconcile.Result{}, err\n\t\t\t}\n\t\t\tif ns.DeletionTimestamp != nil {\n\t\t\t\tcdLog.Warn(\"detected a namespace deleted before deprovision request could be created, giving up on deprovision and removing finalizer\")\n\t\t\t\terr = r.removeClusterDeploymentFinalizer(cd)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcdLog.WithError(err).Error(\"error removing finalizer\")\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn reconcile.Result{}, err\n\t\t}\n\t\treturn reconcile.Result{}, nil\n\t} else if err != nil {\n\t\tcdLog.WithError(err).Errorf(\"error getting deprovision request\")\n\t\treturn reconcile.Result{}, err\n\t}\n\n\t// Deprovision request exists, check whether it has completed\n\tif existingRequest.Status.Completed {\n\t\tcdLog.Infof(\"deprovision request completed, removing finalizer\")\n\t\terr = r.removeClusterDeploymentFinalizer(cd)\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Error(\"error removing finalizer\")\n\t\t}\n\t\treturn reconcile.Result{}, err\n\t}\n\n\tcdLog.Debug(\"deprovision request not yet completed\")\n\n\treturn reconcile.Result{}, nil\n}\n\nfunc (r *ReconcileClusterDeployment) addClusterDeploymentFinalizer(cd *hivev1.ClusterDeployment) error {\n\tcd = cd.DeepCopy()\n\tcontrollerutils.AddFinalizer(cd, hivev1.FinalizerDeprovision)\n\treturn r.Update(context.TODO(), cd)\n}\n\nfunc (r *ReconcileClusterDeployment) removeClusterDeploymentFinalizer(cd *hivev1.ClusterDeployment) error {\n\n\tcd = cd.DeepCopy()\n\tcontrollerutils.DeleteFinalizer(cd, hivev1.FinalizerDeprovision)\n\terr := r.Update(context.TODO(), cd)\n\n\tif err == nil {\n\t\tclearUnderwaySecondsMetrics(cd)\n\n\t\t// Increment the clusters deleted counter:\n\t\tmetricClustersDeleted.WithLabelValues(hivemetrics.GetClusterDeploymentType(cd)).Inc()\n\t}\n\n\treturn err\n}\n\n// setDNSDelayMetric will calculate the amount of time elapsed from clusterdeployment creation\n// to when the dnszone became ready, and set a metric to report the delay.\n// Will return a bool indicating whether the clusterdeployment has been modified, and whether any error was encountered.\nfunc (r *ReconcileClusterDeployment) setDNSDelayMetric(cd *hivev1.ClusterDeployment, dnsZone *hivev1.DNSZone, cdLog log.FieldLogger) (bool, error) {\n\tmodified := false\n\tinitializeAnnotations(cd)\n\n\tif _, ok := cd.Annotations[dnsReadyAnnotation]; ok {\n\t\t// already have recorded the dnsdelay metric\n\t\treturn modified, nil\n\t}\n\n\treadyTimestamp := dnsReadyTransitionTime(dnsZone)\n\tif readyTimestamp == nil {\n\t\tmsg := \"did not find timestamp for when dnszone became ready\"\n\t\tcdLog.WithField(\"dnszone\", dnsZone.Name).Error(msg)\n\t\treturn modified, fmt.Errorf(msg)\n\t}\n\n\tdnsDelayDuration := readyTimestamp.Sub(cd.CreationTimestamp.Time)\n\tcdLog.WithField(\"duration\", dnsDelayDuration.Seconds()).Info(\"DNS ready\")\n\tcd.Annotations[dnsReadyAnnotation] = dnsDelayDuration.String()\n\tif err := r.Client.Update(context.TODO(), cd); err != nil {\n\t\tcdLog.WithError(err).Error(\"failed to save annotation marking DNS becoming ready\")\n\t\treturn modified, err\n\t}\n\tmodified = true\n\n\tmetricDNSDelaySeconds.Observe(float64(dnsDelayDuration.Seconds()))\n\n\treturn modified, nil\n}\n\nfunc (r *ReconcileClusterDeployment) ensureManagedDNSZone(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (bool, *hivev1.DNSZone, error) {\n\t// for now we only support AWS\n\tif cd.Spec.AWS == nil || cd.Spec.PlatformSecrets.AWS == nil {\n\t\tcdLog.Error(\"cluster deployment platform is not AWS, cannot manage DNS zone\")\n\t\treturn false, nil, fmt.Errorf(\"only AWS managed DNS is supported\")\n\t}\n\tdnsZone := &hivev1.DNSZone{}\n\tdnsZoneNamespacedName := types.NamespacedName{Namespace: cd.Namespace, Name: dnsZoneName(cd.Name)}\n\tlogger := cdLog.WithField(\"zone\", dnsZoneNamespacedName.String())\n\n\terr := r.Get(context.TODO(), dnsZoneNamespacedName, dnsZone)\n\tif err == nil {\n\t\tavailableCondition := controllerutils.FindDNSZoneCondition(dnsZone.Status.Conditions, hivev1.ZoneAvailableDNSZoneCondition)\n\t\treturn availableCondition != nil && availableCondition.Status == corev1.ConditionTrue, dnsZone, nil\n\t}\n\tif apierrors.IsNotFound(err) {\n\t\tlogger.Info(\"creating new DNSZone for cluster deployment\")\n\t\treturn false, nil, r.createManagedDNSZone(cd, logger)\n\t}\n\tlogger.WithError(err).Error(\"failed to fetch DNS zone\")\n\treturn false, nil, err\n}\n\nfunc (r *ReconcileClusterDeployment) createManagedDNSZone(cd *hivev1.ClusterDeployment, logger log.FieldLogger) error {\n\tdnsZone := &hivev1.DNSZone{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      dnsZoneName(cd.Name),\n\t\t\tNamespace: cd.Namespace,\n\t\t},\n\t\tSpec: hivev1.DNSZoneSpec{\n\t\t\tZone:               cd.Spec.BaseDomain,\n\t\t\tLinkToParentDomain: true,\n\t\t\tAWS: &hivev1.AWSDNSZoneSpec{\n\t\t\t\tAccountSecret: cd.Spec.PlatformSecrets.AWS.Credentials,\n\t\t\t\tRegion:        cd.Spec.AWS.Region,\n\t\t\t},\n\t\t},\n\t}\n\n\tfor k, v := range cd.Spec.AWS.UserTags {\n\t\tdnsZone.Spec.AWS.AdditionalTags = append(dnsZone.Spec.AWS.AdditionalTags, hivev1.AWSResourceTag{Key: k, Value: v})\n\t}\n\n\tif err := controllerutil.SetControllerReference(cd, dnsZone, r.scheme); err != nil {\n\t\tlogger.WithError(err).Error(\"error setting controller reference on dnszone\")\n\t\treturn err\n\t}\n\n\terr := r.Create(context.TODO(), dnsZone)\n\tif err != nil {\n\t\tlogger.WithError(err).Error(\"cannot create DNS zone\")\n\t\treturn err\n\t}\n\tlogger.Info(\"dns zone created\")\n\treturn nil\n}\n\nfunc dnsZoneName(cdName string) string {\n\treturn apihelpers.GetResourceName(cdName, \"zone\")\n}\n\nfunc selectorPodWatchHandler(a handler.MapObject) []reconcile.Request {\n\tretval := []reconcile.Request{}\n\n\tpod := a.Object.(*corev1.Pod)\n\tif pod == nil {\n\t\t// Wasn't a Pod, bail out. This should not happen.\n\t\tlog.Errorf(\"Error converting MapObject.Object to Pod. Value: %+v\", a.Object)\n\t\treturn retval\n\t}\n\tif pod.Labels == nil {\n\t\treturn retval\n\t}\n\tcdName, ok := pod.Labels[install.ClusterDeploymentNameLabel]\n\tif !ok {\n\t\treturn retval\n\t}\n\tretval = append(retval, reconcile.Request{NamespacedName: types.NamespacedName{\n\t\tName:      cdName,\n\t\tNamespace: pod.Namespace,\n\t}})\n\treturn retval\n}\n\nfunc (r *ReconcileClusterDeployment) calcInstallPodRestarts(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (int, error) {\n\tinstallerPodLabels := map[string]string{install.ClusterDeploymentNameLabel: cd.Name, install.InstallJobLabel: \"true\"}\n\tpods := &corev1.PodList{}\n\terr := r.Client.List(context.Background(), pods, client.InNamespace(cd.Namespace), client.MatchingLabels(installerPodLabels))\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif len(pods.Items) > 1 {\n\t\tlog.Warnf(\"found %d install pods for cluster\", len(pods.Items))\n\t}\n\n\t// Calculate restarts across all containers in the pod:\n\tcontainerRestarts := 0\n\tfor _, pod := range pods.Items {\n\t\tfor _, cs := range pod.Status.ContainerStatuses {\n\t\t\tcontainerRestarts += int(cs.RestartCount)\n\t\t}\n\t}\n\treturn containerRestarts, nil\n}\n\nfunc (r *ReconcileClusterDeployment) deleteJobOnHashChange(existingJob, generatedJob *batchv1.Job, cdLog log.FieldLogger) (bool, error) {\n\tnewJobNeeded := false\n\tif _, ok := existingJob.Annotations[jobHashAnnotation]; !ok {\n\t\t// this job predates tracking the job hash, so assume we need a new job\n\t\tnewJobNeeded = true\n\t}\n\n\tif existingJob.Annotations[jobHashAnnotation] != generatedJob.Annotations[jobHashAnnotation] {\n\t\t// delete the job so we get a fresh one with the new job spec\n\t\tnewJobNeeded = true\n\t}\n\n\tif newJobNeeded {\n\t\t// delete the existing job\n\t\tcdLog.Info(\"deleting existing install job due to updated/missing hash detected\")\n\t\terr := r.Delete(context.TODO(), existingJob, client.PropagationPolicy(metav1.DeletePropagationForeground))\n\t\tif err != nil {\n\t\t\tcdLog.WithError(err).Errorf(\"error deleting outdated install job\")\n\t\t\treturn newJobNeeded, err\n\t\t}\n\t}\n\n\treturn newJobNeeded, nil\n}\n\nfunc generateDeprovisionRequest(cd *hivev1.ClusterDeployment) *hivev1.ClusterDeprovisionRequest {\n\treq := &hivev1.ClusterDeprovisionRequest{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      cd.Name,\n\t\t\tNamespace: cd.Namespace,\n\t\t},\n\t\tSpec: hivev1.ClusterDeprovisionRequestSpec{\n\t\t\tInfraID:   cd.Status.InfraID,\n\t\t\tClusterID: cd.Status.ClusterID,\n\t\t\tPlatform: hivev1.ClusterDeprovisionRequestPlatform{\n\t\t\t\tAWS: &hivev1.AWSClusterDeprovisionRequest{},\n\t\t\t},\n\t\t},\n\t}\n\n\tif cd.Spec.Platform.AWS != nil {\n\t\treq.Spec.Platform.AWS.Region = cd.Spec.Platform.AWS.Region\n\t}\n\n\tif cd.Spec.PlatformSecrets.AWS != nil {\n\t\treq.Spec.Platform.AWS.Credentials = &cd.Spec.PlatformSecrets.AWS.Credentials\n\t}\n\n\treturn req\n}\n\nfunc generatePullSecretObj(pullSecret string, pullSecretName string, cd *hivev1.ClusterDeployment) *corev1.Secret {\n\treturn &corev1.Secret{\n\t\tTypeMeta: metav1.TypeMeta{\n\t\t\tKind:       \"Secret\",\n\t\t\tAPIVersion: corev1.SchemeGroupVersion.String(),\n\t\t},\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      pullSecretName,\n\t\t\tNamespace: cd.Namespace,\n\t\t},\n\t\tType: corev1.SecretTypeDockerConfigJson,\n\t\tStringData: map[string]string{\n\t\t\tcorev1.DockerConfigJsonKey: pullSecret,\n\t\t},\n\t}\n}\n\nfunc migrateWildcardIngress(cd *hivev1.ClusterDeployment) bool {\n\tmigrated := false\n\tfor i, ingress := range cd.Spec.Ingress {\n\t\tnewIngress := wildcardDomain.ReplaceAllString(ingress.Domain, \"\")\n\t\tif newIngress != ingress.Domain {\n\t\t\tcd.Spec.Ingress[i].Domain = newIngress\n\t\t\tmigrated = true\n\t\t}\n\t}\n\treturn migrated\n}\n\nfunc dnsReadyTransitionTime(dnsZone *hivev1.DNSZone) *time.Time {\n\treadyCondition := controllerutils.FindDNSZoneCondition(dnsZone.Status.Conditions, hivev1.ZoneAvailableDNSZoneCondition)\n\n\tif readyCondition != nil && readyCondition.Status == corev1.ConditionTrue {\n\t\treturn &readyCondition.LastTransitionTime.Time\n\t}\n\n\treturn nil\n}\n\nfunc strPtr(s string) *string {\n\treturn &s\n}\n\nfunc clearUnderwaySecondsMetrics(cd *hivev1.ClusterDeployment) {\n\t// If we've successfully cleared the deprovision finalizer we know this is a good time to\n\t// reset the underway metric to 0, after which it will no longer be reported.\n\thivemetrics.MetricClusterDeploymentDeprovisioningUnderwaySeconds.WithLabelValues(\n\t\tcd.Name,\n\t\tcd.Namespace,\n\t\thivemetrics.GetClusterDeploymentType(cd)).Set(0.0)\n\n\t// Clear the install underway seconds metric if this cluster was still installing.\n\tif !cd.Status.Installed {\n\t\thivemetrics.MetricClusterDeploymentProvisionUnderwaySeconds.WithLabelValues(\n\t\t\tcd.Name,\n\t\t\tcd.Namespace,\n\t\t\thivemetrics.GetClusterDeploymentType(cd)).Set(0.0)\n\t}\n}\n\n// initializeAnnotations() initializes the annotations if it is not already\nfunc initializeAnnotations(cd *hivev1.ClusterDeployment) {\n\tif cd.Annotations == nil {\n\t\tcd.Annotations = map[string]string{}\n\t}\n}\n\n// mergePullSecrets merges the global pull secret JSON (if defined) with the cluster's pull secret JSON (if defined)\n// An error will be returned if neither is defined\nfunc (r *ReconcileClusterDeployment) mergePullSecrets(cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (string, error) {\n\tvar localPullSecret string\n\tvar err error\n\n\t// For code readability let's call the pull secret in cluster deployment config as local pull secret\n\tif cd.Spec.PullSecret != nil {\n\t\tlocalPullSecret, err = controllerutils.LoadSecretData(r.Client, cd.Spec.PullSecret.Name, cd.Namespace, corev1.DockerConfigJsonKey)\n\t\tif err != nil {\n\t\t\tif !apierrors.IsNotFound(err) {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check if global pull secret from env as it comes from hive config\n\tglobalPullSecret := os.Getenv(\"GLOBAL_PULL_SECRET\")\n\n\tswitch {\n\tcase globalPullSecret != \"\" && localPullSecret != \"\":\n\t\t// Merge local pullSecret and globalPullSecret. If both pull secrets have same registry name\n\t\t// then the merged pull secret will have registry secret from local pull secret\n\t\tpullSecret, err := controllerutils.MergeJsons(globalPullSecret, localPullSecret, cdLog)\n\t\tif err != nil {\n\t\t\terrMsg := \"unable to merge global pull secret with local pull secret\"\n\t\t\tcdLog.WithError(err).Error(errMsg)\n\t\t\treturn \"\", errors.Wrap(err, errMsg)\n\t\t}\n\t\treturn pullSecret, nil\n\tcase globalPullSecret != \"\":\n\t\treturn globalPullSecret, nil\n\tcase localPullSecret != \"\":\n\t\treturn localPullSecret, nil\n\tdefault:\n\t\terrMsg := \"clusterdeployment must specify pull secret since hiveconfig does not specify a global pull secret\"\n\t\tcdLog.Error(errMsg)\n\t\treturn \"\", errors.New(errMsg)\n\t}\n}\n\n// updatePullSecretInfo adds pull secret information in cluster deployment and cluster deployment status.\n// It returns true when cluster deployment status has been updated.\nfunc (r *ReconcileClusterDeployment) updatePullSecretInfo(pullSecret string, cd *hivev1.ClusterDeployment, cdLog log.FieldLogger) (bool, error) {\n\tvar err error\n\tpullSecretObjExists := true\n\texistingPullSecretObj := &corev1.Secret{}\n\tmergedSecretName := constants.GetMergedPullSecretName(cd)\n\terr = r.Get(context.TODO(), types.NamespacedName{Name: mergedSecretName, Namespace: cd.Namespace}, existingPullSecretObj)\n\tif err != nil {\n\t\tif apierrors.IsNotFound(err) {\n\t\t\tcdLog.Info(\"Existing pull secret object not found\")\n\t\t\tpullSecretObjExists = false\n\t\t} else {\n\t\t\treturn false, errors.Wrap(err, \"Error getting pull secret from cluster deployment\")\n\t\t}\n\t}\n\n\tif pullSecretObjExists {\n\t\texistingPullSecret, ok := existingPullSecretObj.Data[corev1.DockerConfigJsonKey]\n\t\tif !ok {\n\t\t\treturn false, errors.New(fmt.Sprintf(\"Pull secret %s did not contain key %s\", mergedSecretName, corev1.DockerConfigJsonKey))\n\t\t}\n\t\tif controllerutils.GetHashOfPullSecret(string(existingPullSecret)) == controllerutils.GetHashOfPullSecret(pullSecret) {\n\t\t\tcdLog.Debug(\"Existing and the new merged pull secret are same\")\n\t\t\treturn false, nil\n\t\t}\n\t\tcdLog.Info(\"Existing merged pull secret hash did not match with latest merged pull secret\")\n\t\texistingPullSecretObj.Data[corev1.DockerConfigJsonKey] = []byte(pullSecret)\n\t\terr = r.Update(context.TODO(), existingPullSecretObj)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"error updating merged pull secret object\")\n\t\t}\n\t\tcdLog.WithField(\"secretName\", mergedSecretName).Info(\"Updated the merged pull secret object successfully\")\n\t} else {\n\n\t\t// create a new pull secret object\n\t\tnewPullSecretObj := generatePullSecretObj(\n\t\t\tpullSecret,\n\t\t\tmergedSecretName,\n\t\t\tcd,\n\t\t)\n\t\terr = controllerutil.SetControllerReference(cd, newPullSecretObj, r.scheme)\n\t\tif err != nil {\n\t\t\tcdLog.Errorf(\"error setting controller reference on new merged pull secret: %v\", err)\n\t\t\treturn false, err\n\t\t}\n\t\terr = r.Create(context.TODO(), newPullSecretObj)\n\t\tif err != nil {\n\t\t\treturn false, errors.Wrap(err, \"error creating new pull secret object\")\n\t\t}\n\t\tcdLog.WithField(\"secretName\", mergedSecretName).Info(\"Created the merged pull secret object successfully\")\n\t}\n\treturn true, nil\n}\n", "idx": 8, "id": 7142, "msg": "", "proj": "openshift-hive", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -30,13 +30,14 @@ import socket\n import sys\n \n from .firefox_binary import FirefoxBinary\n+from .options import Options\n from .remote_connection import FirefoxRemoteConnection\n from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n from selenium.webdriver.firefox.extension_connection import ExtensionConnection\n from selenium.webdriver.firefox.firefox_profile import FirefoxProfile\n from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver\n from .service import Service\n-from .options import Options\n+from .webelement import FirefoxWebElement\n \n \n class WebDriver(RemoteWebDriver):", "y": 1, "oldf": "# Licensed to the Software Freedom Conservancy (SFC) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The SFC licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\ntry:\n    import http.client as http_client\nexcept ImportError:\n    import httplib as http_client\n\ntry:\n    basestring\nexcept NameError:  # Python 3.x\n    basestring = str\n\nimport shutil\nimport socket\nimport sys\n\nfrom .firefox_binary import FirefoxBinary\nfrom .remote_connection import FirefoxRemoteConnection\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\nfrom selenium.webdriver.firefox.extension_connection import ExtensionConnection\nfrom selenium.webdriver.firefox.firefox_profile import FirefoxProfile\nfrom selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver\nfrom .service import Service\nfrom .options import Options\n\n\nclass WebDriver(RemoteWebDriver):\n\n    # There is no native event support on Mac\n    NATIVE_EVENTS_ALLOWED = sys.platform != \"darwin\"\n\n    def __init__(self, firefox_profile=None, firefox_binary=None, timeout=30,\n                 capabilities=None, proxy=None, executable_path=\"geckodriver\", firefox_options=None):\n        capabilities = capabilities or DesiredCapabilities.FIREFOX.copy()\n\n        self.profile = firefox_profile or FirefoxProfile()\n        self.profile.native_events_enabled = (\n            self.NATIVE_EVENTS_ALLOWED and self.profile.native_events_enabled)\n\n        self.binary = firefox_binary or capabilities.get(\"binary\", FirefoxBinary())\n\n        self.options = firefox_options or Options()\n        self.options.binary_location = self.binary if isinstance(self.binary, basestring) else self.binary._start_cmd\n        self.options.profile = self.profile\n        capabilities.update(self.options.to_capabilities())\n\n        # marionette\n        if capabilities.get(\"marionette\"):\n            self.service = Service(executable_path, firefox_binary=self.options.binary_location)\n            self.service.start()\n\n            executor = FirefoxRemoteConnection(\n                remote_server_addr=self.service.service_url)\n            RemoteWebDriver.__init__(\n                self,\n                command_executor=executor,\n                desired_capabilities=capabilities,\n                keep_alive=True)\n        else:\n            # Oh well... sometimes the old way is the best way.\n            if proxy is not None:\n                proxy.add_to_capabilities(capabilities)\n\n            executor = ExtensionConnection(\"127.0.0.1\", self.profile,\n                                           self.binary, timeout)\n            RemoteWebDriver.__init__(\n                self,\n                command_executor=executor,\n                desired_capabilities=capabilities,\n                keep_alive=True)\n\n        self._is_remote = False\n\n    def quit(self):\n        \"\"\"Quits the driver and close every associated window.\"\"\"\n        try:\n            RemoteWebDriver.quit(self)\n        except (http_client.BadStatusLine, socket.error):\n            # Happens if Firefox shutsdown before we've read the response from\n            # the socket.\n            pass\n        if \"specificationLevel\" in self.capabilities:\n            self.service.stop()\n        else:\n            self.binary.kill()\n        try:\n            shutil.rmtree(self.profile.path)\n            if self.profile.tempfolder is not None:\n                shutil.rmtree(self.profile.tempfolder)\n        except Exception as e:\n            print(str(e))\n\n    @property\n    def firefox_profile(self):\n        return self.profile\n\n    def set_context(self, context):\n        self.execute(\"SET_CONTEXT\", {\"context\": context})\n", "idx": 1, "id": 13574, "msg": "i think this should get put into its own file. This could start to grow :)", "proj": "SeleniumHQ-selenium", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -107,6 +107,12 @@ type KBFSOps interface {\n \t// isn't favorited.\n \tDeleteFavorite(ctx context.Context, fav Favorite) error\n \n+\t// GetOrInitializeNewMDMaster gets the existing MD or initialize a new MD for\n+\t// master branch\n+\tGetOrInitializeNewMDMaster(ctx context.Context, mdops MDOps, h *TlfHandle) (\n+\t\tinitialized bool, md ImmutableRootMetadata,\n+\t\tid TlfID, err error)\n+\n \t// GetOrCreateRootNode returns the root node and root entry\n \t// info associated with the given TLF handle and branch, if\n \t// the logged-in user has read permissions to the top-level", "y": 1, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"reflect\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/libkb\"\n\t\"github.com/keybase/client/go/logger\"\n\tkeybase1 \"github.com/keybase/client/go/protocol\"\n\tmetrics \"github.com/rcrowley/go-metrics\"\n\t\"golang.org/x/net/context\"\n)\n\n// AuthTokenRefreshHandler defines a callback to be called when an auth token refresh\n// is needed.\ntype AuthTokenRefreshHandler interface {\n\tRefreshAuthToken(context.Context)\n}\n\n// Block just needs to be (de)serialized using msgpack\ntype Block interface {\n\t// GetEncodedSize returns the encoded size of this block, but only\n\t// if it has been previously set; otherwise it returns 0.\n\tGetEncodedSize() uint32\n\t// SetEncodedSize sets the encoded size of this block, locally\n\t// caching it.  The encoded size is not serialized.\n\tSetEncodedSize(size uint32)\n\t// DataVersion returns the data version for this block\n\tDataVersion() DataVer\n}\n\n// NodeID is a unique but transient ID for a Node. That is, two Node\n// objects in memory at the same time represent the same file or\n// directory if and only if their NodeIDs are equal (by pointer).\ntype NodeID interface {\n\t// ParentID returns the NodeID of the directory containing the\n\t// pointed-to file or directory, or nil if none exists.\n\tParentID() NodeID\n}\n\n// Node represents a direct pointer to a file or directory in KBFS.\n// It is somewhat like an inode in a regular file system.  Users of\n// KBFS can use Node as a handle when accessing files or directories\n// they have previously looked up.\ntype Node interface {\n\t// GetID returns the ID of this Node. This should be used as a\n\t// map key instead of the Node itself.\n\tGetID() NodeID\n\t// GetFolderBranch returns the folder ID and branch for this Node.\n\tGetFolderBranch() FolderBranch\n\t// GetBasename returns the current basename of the node, or \"\"\n\t// if the node has been unlinked.\n\tGetBasename() string\n}\n\n// KBFSOps handles all file system operations.  Expands all indirect\n// pointers.  Operations that modify the server data change all the\n// block IDs along the path, and so must return a path with the new\n// BlockIds so the caller can update their references.\n//\n// KBFSOps implementations must guarantee goroutine-safety of calls on\n// a per-top-level-folder basis.\n//\n// There are two types of operations that could block:\n//   * remote-sync operations, that need to synchronously update the\n//     MD for the corresponding top-level folder.  When these\n//     operations return successfully, they will have guaranteed to\n//     have successfully written the modification to the KBFS servers.\n//   * remote-access operations, that don't sync any modifications to KBFS\n//     servers, but may block on reading data from the servers.\n//\n// KBFSOps implementations are supposed to give git-like consistency\n// semantics for modification operations; they will be visible to\n// other clients immediately after the remote-sync operations succeed,\n// if and only if there was no other intervening modification to the\n// same folder.  If not, the change will be sync'd to the server in a\n// special per-device \"unmerged\" area before the operation succeeds.\n// In this case, the modification will not be visible to other clients\n// until the KBFS code on this device performs automatic conflict\n// resolution in the background.\n//\n// All methods take a Context (see https://blog.golang.org/context),\n// and if that context is cancelled during the operation, KBFSOps will\n// abort any blocking calls and return ctx.Err(). Any notifications\n// resulting from an operation will also include this ctx (or a\n// Context derived from it), allowing the caller to determine whether\n// the notification is a result of their own action or an external\n// action.\ntype KBFSOps interface {\n\t// GetFavorites returns the logged-in user's list of favorite\n\t// top-level folders.  This is a remote-access operation.\n\tGetFavorites(ctx context.Context) ([]Favorite, error)\n\t// RefreshCachedFavorites tells the instances to forget any cached\n\t// favorites list and fetch a new list from the server.  The\n\t// effects are asychronous; if there's an error refreshing the\n\t// favorites, the cached favorites will become empty.\n\tRefreshCachedFavorites(ctx context.Context)\n\t// AddFavorite adds the favorite to both the server and\n\t// the local cache.\n\tAddFavorite(ctx context.Context, fav Favorite) error\n\t// DeleteFavorite deletes the favorite from both the server and\n\t// the local cache.  Idempotent, so it succeeds even if the folder\n\t// isn't favorited.\n\tDeleteFavorite(ctx context.Context, fav Favorite) error\n\n\t// GetOrCreateRootNode returns the root node and root entry\n\t// info associated with the given TLF handle and branch, if\n\t// the logged-in user has read permissions to the top-level\n\t// folder. It creates the folder if one doesn't exist yet (and\n\t// branch == MasterBranch), and the logged-in user has write\n\t// permissions to the top-level folder.  This is a\n\t// remote-access operation.\n\tGetOrCreateRootNode(\n\t\tctx context.Context, h *TlfHandle, branch BranchName) (\n\t\tnode Node, ei EntryInfo, err error)\n\t// GetDirChildren returns a map of children in the directory,\n\t// mapped to their EntryInfo, if the logged-in user has read\n\t// permission for the top-level folder.  This is a remote-access\n\t// operation.\n\tGetDirChildren(ctx context.Context, dir Node) (map[string]EntryInfo, error)\n\t// Lookup returns the Node and entry info associated with a\n\t// given name in a directory, if the logged-in user has read\n\t// permissions to the top-level folder.  The returned Node is nil\n\t// if the name is a symlink.  This is a remote-access operation.\n\tLookup(ctx context.Context, dir Node, name string) (Node, EntryInfo, error)\n\t// Stat returns the entry info associated with a\n\t// given Node, if the logged-in user has read permissions to the\n\t// top-level folder.  This is a remote-access operation.\n\tStat(ctx context.Context, node Node) (EntryInfo, error)\n\t// CreateDir creates a new subdirectory under the given node, if\n\t// the logged-in user has write permission to the top-level\n\t// folder.  Returns the new Node for the created subdirectory, and\n\t// its new entry info.  This is a remote-sync operation.\n\tCreateDir(ctx context.Context, dir Node, name string) (\n\t\tNode, EntryInfo, error)\n\t// CreateFile creates a new file under the given node, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// Returns the new Node for the created file, and its new\n\t// entry info. excl (when implemented) specifies whether this is an exclusive\n\t// create.  Semantically setting excl to WithExcl is like O_CREAT|O_EXCL in a\n\t// Unix open() call.\n\t//\n\t// This is a remote-sync operation.\n\tCreateFile(ctx context.Context, dir Node, name string, isExec bool, excl Excl) (\n\t\tNode, EntryInfo, error)\n\t// CreateLink creates a new symlink under the given node, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// Returns the new entry info for the created symlink.  This\n\t// is a remote-sync operation.\n\tCreateLink(ctx context.Context, dir Node, fromName string, toPath string) (\n\t\tEntryInfo, error)\n\t// RemoveDir removes the subdirectory represented by the given\n\t// node, if the logged-in user has write permission to the\n\t// top-level folder.  Will return an error if the subdirectory is\n\t// not empty.  This is a remote-sync operation.\n\tRemoveDir(ctx context.Context, dir Node, dirName string) error\n\t// RemoveEntry removes the directory entry represented by the\n\t// given node, if the logged-in user has write permission to the\n\t// top-level folder.  This is a remote-sync operation.\n\tRemoveEntry(ctx context.Context, dir Node, name string) error\n\t// Rename performs an atomic rename operation with a given\n\t// top-level folder if the logged-in user has write permission to\n\t// that folder, and will return an error if nodes from different\n\t// folders are passed in.  Also returns an error if the new name\n\t// already has an entry corresponding to an existing directory\n\t// (only non-dir types may be renamed over).  This is a\n\t// remote-sync operation.\n\tRename(ctx context.Context, oldParent Node, oldName string, newParent Node,\n\t\tnewName string) error\n\t// Read fills in the given buffer with data from the file at the\n\t// given node starting at the given offset, if the logged-in user\n\t// has read permission to the top-level folder.  The read data\n\t// reflects any outstanding writes and truncates to that file that\n\t// have been written through this KBFSOps object, even if those\n\t// writes have not yet been sync'd.  There is no guarantee that\n\t// Read returns all of the requested data; it will return the\n\t// number of bytes that it wrote to the dest buffer.  Reads on an\n\t// unlinked file may or may not succeed, depending on whether or\n\t// not the data has been cached locally.  If (0, nil) is returned,\n\t// that means EOF has been reached. This is a remote-access\n\t// operation.\n\tRead(ctx context.Context, file Node, dest []byte, off int64) (int64, error)\n\t// Write modifies the file at the given node, by writing the given\n\t// buffer at the given offset within the file, if the logged-in\n\t// user has write permission to the top-level folder.  It\n\t// overwrites any data already there, and extends the file size as\n\t// necessary to accomodate the new data.  It guarantees to write\n\t// the entire buffer in one operation.  Writes on an unlinked file\n\t// may or may not succeed as no-ops, depending on whether or not\n\t// the necessary blocks have been locally cached.  This is a\n\t// remote-access operation.\n\tWrite(ctx context.Context, file Node, data []byte, off int64) error\n\t// Truncate modifies the file at the given node, by either\n\t// shrinking or extending its size to match the given size, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// If extending the file, it pads the new data with 0s.  Truncates\n\t// on an unlinked file may or may not succeed as no-ops, depending\n\t// on whether or not the necessary blocks have been locally\n\t// cached.  This is a remote-access operation.\n\tTruncate(ctx context.Context, file Node, size uint64) error\n\t// SetEx turns on or off the executable bit on the file\n\t// represented by a given node, if the logged-in user has write\n\t// permissions to the top-level folder.  This is a remote-sync\n\t// operation.\n\tSetEx(ctx context.Context, file Node, ex bool) error\n\t// SetMtime sets the modification time on the file represented by\n\t// a given node, if the logged-in user has write permissions to\n\t// the top-level folder.  If mtime is nil, it is a noop.  This is\n\t// a remote-sync operation.\n\tSetMtime(ctx context.Context, file Node, mtime *time.Time) error\n\t// Sync flushes all outstanding writes and truncates for the given\n\t// file to the KBFS servers, if the logged-in user has write\n\t// permissions to the top-level folder.  If done through a file\n\t// system interface, this may include modifications done via\n\t// multiple file handles.  This is a remote-sync operation.\n\tSync(ctx context.Context, file Node) error\n\t// FolderStatus returns the status of a particular folder/branch, along\n\t// with a channel that will be closed when the status has been\n\t// updated (to eliminate the need for polling this method).\n\tFolderStatus(ctx context.Context, folderBranch FolderBranch) (\n\t\tFolderBranchStatus, <-chan StatusUpdate, error)\n\t// Status returns the status of KBFS, along with a channel that will be\n\t// closed when the status has been updated (to eliminate the need for\n\t// polling this method). KBFSStatus can be non-empty even if there is an\n\t// error.\n\tStatus(ctx context.Context) (\n\t\tKBFSStatus, <-chan StatusUpdate, error)\n\t// UnstageForTesting clears out this device's staged state, if\n\t// any, and fast-forwards to the current head of this\n\t// folder-branch.\n\tUnstageForTesting(ctx context.Context, folderBranch FolderBranch) error\n\t// Rekey rekeys this folder.\n\tRekey(ctx context.Context, id TlfID) error\n\t// SyncFromServerForTesting blocks until the local client has\n\t// contacted the server and guaranteed that all known updates\n\t// for the given top-level folder have been applied locally\n\t// (and notifications sent out to any observers).  It returns\n\t// an error if this folder-branch is currently unmerged or\n\t// dirty locally.\n\tSyncFromServerForTesting(ctx context.Context, folderBranch FolderBranch) error\n\t// GetUpdateHistory returns a complete history of all the merged\n\t// updates of the given folder, in a data structure that's\n\t// suitable for encoding directly into JSON.  This is an expensive\n\t// operation, and should only be used for ocassional debugging.\n\t// Note that the history does not include any unmerged changes or\n\t// outstanding writes from the local device.\n\tGetUpdateHistory(ctx context.Context, folderBranch FolderBranch) (\n\t\thistory TLFUpdateHistory, err error)\n\t// GetEditHistory returns a clustered list of the most recent file\n\t// edits by each of the valid writers of the given folder.  users\n\t// looking to get updates to this list can register as an observer\n\t// for the folder.\n\tGetEditHistory(ctx context.Context, folderBranch FolderBranch) (\n\t\tedits TlfWriterEdits, err error)\n\n\t// Shutdown is called to clean up any resources associated with\n\t// this KBFSOps instance.\n\tShutdown() error\n\t// PushConnectionStatusChange updates the status of a service for\n\t// human readable connection status tracking.\n\tPushConnectionStatusChange(service string, newStatus error)\n}\n\n// KeybaseService is an interface for communicating with the keybase\n// service.\ntype KeybaseService interface {\n\t// Resolve, given an assertion, resolves it to a username/UID\n\t// pair. The username <-> UID mapping is trusted and\n\t// immutable, so it can be cached. If the assertion is just\n\t// the username or a UID assertion, then the resolution can\n\t// also be trusted. If the returned pair is equal to that of\n\t// the current session, then it can also be\n\t// trusted. Otherwise, Identify() needs to be called on the\n\t// assertion before the assertion -> (username, UID) mapping\n\t// can be trusted.\n\tResolve(ctx context.Context, assertion string) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n\n\t// Identify, given an assertion, returns a UserInfo struct\n\t// with the user that matches that assertion, or an error\n\t// otherwise. The reason string is displayed on any tracker\n\t// popups spawned.\n\tIdentify(ctx context.Context, assertion, reason string) (UserInfo, error)\n\n\t// LoadUserPlusKeys returns a UserInfo struct for a\n\t// user with the specified UID.\n\t// If you have the UID for a user and don't require Identify to\n\t// validate an assertion or the identity of a user, use this to\n\t// get UserInfo structs as it is much cheaper than Identify.\n\tLoadUserPlusKeys(ctx context.Context, uid keybase1.UID) (UserInfo, error)\n\n\t// LoadUnverifiedKeys returns a list of unverified public keys.  They are the union\n\t// of all known public keys associated with the account and the currently verified\n\t// keys currently part of the user's sigchain.\n\tLoadUnverifiedKeys(ctx context.Context, uid keybase1.UID) (\n\t\t[]keybase1.PublicKey, error)\n\n\t// CurrentSession returns a SessionInfo struct with all the\n\t// information for the current session, or an error otherwise.\n\tCurrentSession(ctx context.Context, sessionID int) (SessionInfo, error)\n\n\t// FavoriteAdd adds the given folder to the list of favorites.\n\tFavoriteAdd(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteAdd removes the given folder from the list of\n\t// favorites.\n\tFavoriteDelete(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteList returns the current list of favorites.\n\tFavoriteList(ctx context.Context, sessionID int) ([]keybase1.Folder, error)\n\n\t// Notify sends a filesystem notification.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification) error\n\n\t// FlushUserFromLocalCache instructs this layer to clear any\n\t// KBFS-side, locally-cached information about the given user.\n\t// This does NOT involve communication with the daemon, this is\n\t// just to force future calls loading this user to fall through to\n\t// the daemon itself, rather than being served from the cache.\n\tFlushUserFromLocalCache(ctx context.Context, uid keybase1.UID)\n\n\t// FlushUserUnverifiedKeysFromLocalCache instructs this layer to clear any\n\t// KBFS-side, locally-cached unverified keys for the given user.\n\tFlushUserUnverifiedKeysFromLocalCache(ctx context.Context, uid keybase1.UID)\n\n\t// TODO: Add CryptoClient methods, too.\n\n\t// Shutdown frees any resources associated with this\n\t// instance. No other methods may be called after this is\n\t// called.\n\tShutdown()\n}\n\n// KeybaseServiceCn defines methods needed to construct KeybaseService\n// and Crypto implementations.\ntype KeybaseServiceCn interface {\n\tNewKeybaseService(config Config, params InitParams, ctx Context, log logger.Logger) (KeybaseService, error)\n\tNewCrypto(config Config, params InitParams, ctx Context, log logger.Logger) (Crypto, error)\n}\n\ntype resolver interface {\n\t// Resolve, given an assertion, resolves it to a username/UID\n\t// pair. The username <-> UID mapping is trusted and\n\t// immutable, so it can be cached. If the assertion is just\n\t// the username or a UID assertion, then the resolution can\n\t// also be trusted. If the returned pair is equal to that of\n\t// the current session, then it can also be\n\t// trusted. Otherwise, Identify() needs to be called on the\n\t// assertion before the assertion -> (username, UID) mapping\n\t// can be trusted.\n\tResolve(ctx context.Context, assertion string) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n}\n\ntype identifier interface {\n\t// Identify resolves an assertion (which could also be a\n\t// username) to a UserInfo struct, spawning tracker popups if\n\t// necessary.  The reason string is displayed on any tracker\n\t// popups spawned.\n\tIdentify(ctx context.Context, assertion, reason string) (UserInfo, error)\n}\n\ntype normalizedUsernameGetter interface {\n\t// GetNormalizedUsername returns the normalized username\n\t// corresponding to the given UID.\n\tGetNormalizedUsername(ctx context.Context, uid keybase1.UID) (libkb.NormalizedUsername, error)\n}\n\n// KBPKI interacts with the Keybase daemon to fetch user info.\ntype KBPKI interface {\n\t// GetCurrentToken gets the current keybase session token.\n\tGetCurrentToken(ctx context.Context) (string, error)\n\t// GetCurrentUserInfo gets the name and UID of the current\n\t// logged-in user.\n\tGetCurrentUserInfo(ctx context.Context) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n\t// GetCurrentCryptPublicKey gets the crypt public key for the\n\t// currently-active device.\n\tGetCurrentCryptPublicKey(ctx context.Context) (CryptPublicKey, error)\n\t// GetCurrentVerifyingKey gets the public key used for signing for the\n\t// currently-active device.\n\tGetCurrentVerifyingKey(ctx context.Context) (VerifyingKey, error)\n\n\tresolver\n\tidentifier\n\tnormalizedUsernameGetter\n\n\t// HasVerifyingKey returns nil if the given user has the given\n\t// VerifyingKey, and an error otherwise.\n\tHasVerifyingKey(ctx context.Context, uid keybase1.UID,\n\t\tverifyingKey VerifyingKey, atServerTime time.Time) error\n\n\t// HasUnverifiedVerifyingKey returns nil if the given user has the given\n\t// unverified VerifyingKey, and an error otherwise.  Note that any match\n\t// is with a key not verified to be currently connected to the user via\n\t// their sigchain.  This is currently only used to verify finalized or\n\t// reset TLFs.  Further note that unverified keys is a super set of\n\t// verified keys.\n\tHasUnverifiedVerifyingKey(ctx context.Context, uid keybase1.UID,\n\t\tverifyingKey VerifyingKey) error\n\n\t// GetCryptPublicKeys gets all of a user's crypt public keys (including\n\t// paper keys).\n\tGetCryptPublicKeys(ctx context.Context, uid keybase1.UID) (\n\t\t[]CryptPublicKey, error)\n\n\t// TODO: Split the methods below off into a separate\n\t// FavoriteOps interface.\n\n\t// FavoriteAdd adds folder to the list of the logged in user's\n\t// favorite folders.  It is idempotent.\n\tFavoriteAdd(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteDelete deletes folder from the list of the logged in user's\n\t// favorite folders.  It is idempotent.\n\tFavoriteDelete(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteList returns the list of all favorite folders for\n\t// the logged in user.\n\tFavoriteList(ctx context.Context) ([]keybase1.Folder, error)\n\n\t// Notify sends a filesystem notification.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification) error\n}\n\n// KeyMetadata is an interface for something that holds key\n// information. This is usually implemented by RootMetadata.\ntype KeyMetadata interface {\n\t// TlfID returns the ID of the TLF for which this object holds\n\t// key info.\n\tTlfID() TlfID\n\n\t// LatestKeyGeneration returns the most recent key generation\n\t// with key data in this object, or PublicKeyGen if this TLF\n\t// is public.\n\tLatestKeyGeneration() KeyGen\n\n\t// GetTlfHandle returns the handle for the TLF. It must not\n\t// return nil.\n\t//\n\t// TODO: Remove the need for this function in this interface,\n\t// so that BareRootMetadata can implement this interface\n\t// fully.\n\tGetTlfHandle() *TlfHandle\n\n\t// HasKeyForUser returns whether or not the given user has\n\t// keys for at least one device at the given key\n\t// generation. Returns false if the TLF is public, or if the\n\t// given key generation is invalid.\n\tHasKeyForUser(keyGen KeyGen, user keybase1.UID) bool\n\n\t// GetTLFCryptKeyParams returns all the necessary info to\n\t// construct the TLF crypt key for the given key generation,\n\t// user, and device (identified by its crypt public key), or\n\t// false if not found. This returns an error if the TLF is\n\t// public.\n\tGetTLFCryptKeyParams(\n\t\tkeyGen KeyGen, user keybase1.UID, key CryptPublicKey) (\n\t\tTLFEphemeralPublicKey, EncryptedTLFCryptKeyClientHalf,\n\t\tTLFCryptKeyServerHalfID, bool, error)\n}\n\ntype encryptionKeyGetter interface {\n\t// GetTLFCryptKeyForEncryption gets the crypt key to use for\n\t// encryption (i.e., with the latest key generation) for the\n\t// TLF with the given metadata.\n\tGetTLFCryptKeyForEncryption(ctx context.Context, kmd KeyMetadata) (\n\t\tTLFCryptKey, error)\n}\n\n// KeyManager fetches and constructs the keys needed for KBFS file\n// operations.\ntype KeyManager interface {\n\tencryptionKeyGetter\n\n\t// GetTLFCryptKeyForMDDecryption gets the crypt key to use for the\n\t// TLF with the given metadata to decrypt the private portion of\n\t// the metadata.  It finds the appropriate key from mdWithKeys\n\t// (which in most cases is the same as mdToDecrypt) if it's not\n\t// already cached.\n\tGetTLFCryptKeyForMDDecryption(ctx context.Context,\n\t\tkmdToDecrypt, kmdWithKeys KeyMetadata) (TLFCryptKey, error)\n\n\t// GetTLFCryptKeyForBlockDecryption gets the crypt key to use\n\t// for the TLF with the given metadata to decrypt the block\n\t// pointed to by the given pointer.\n\tGetTLFCryptKeyForBlockDecryption(ctx context.Context, kmd KeyMetadata,\n\t\tblockPtr BlockPointer) (TLFCryptKey, error)\n\n\t// Rekey checks the given MD object, if it is a private TLF,\n\t// against the current set of device keys for all valid\n\t// readers and writers.  If there are any new devices, it\n\t// updates all existing key generations to include the new\n\t// devices.  If there are devices that have been removed, it\n\t// creates a new epoch of keys for the TLF.  If no devices\n\t// have changed, or if there was an error, it returns false.\n\t// Otherwise, it returns true. If a new key generation is\n\t// added the second return value points to this new key. This\n\t// is to allow for caching of the TLF crypt key only after a\n\t// successful merged write of the metadata. Otherwise we could\n\t// prematurely pollute the key cache.\n\t//\n\t// If the given MD object is a public TLF, it simply updates\n\t// the TLF's handle with any newly-resolved writers.\n\t//\n\t// If promptPaper is set, prompts for any unlocked paper keys.\n\t// promptPaper shouldn't be set if md is for a public TLF.\n\tRekey(ctx context.Context, md *RootMetadata, promptPaper bool) (bool, *TLFCryptKey, error)\n}\n\n// Reporter exports events (asynchronously) to any number of sinks\ntype Reporter interface {\n\t// ReportErr records that a given error happened.\n\tReportErr(ctx context.Context, tlfName CanonicalTlfName, public bool,\n\t\tmode ErrorModeType, err error)\n\t// AllKnownErrors returns all errors known to this Reporter.\n\tAllKnownErrors() []ReportedError\n\t// Notify sends the given notification to any sink.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification)\n\t// Shutdown frees any resources allocated by a Reporter.\n\tShutdown()\n}\n\n// MDCache gets and puts plaintext top-level metadata into the cache.\ntype MDCache interface {\n\t// Get gets the metadata object associated with the given TlfID,\n\t// revision number, and branch ID (NullBranchID for merged MD).\n\tGet(tlf TlfID, rev MetadataRevision, bid BranchID) (ImmutableRootMetadata, error)\n\t// Put stores the metadata object.\n\tPut(md ImmutableRootMetadata) error\n}\n\n// KeyCache handles caching for both TLFCryptKeys and BlockCryptKeys.\ntype KeyCache interface {\n\t// GetTLFCryptKey gets the crypt key for the given TLF.\n\tGetTLFCryptKey(TlfID, KeyGen) (TLFCryptKey, error)\n\t// PutTLFCryptKey stores the crypt key for the given TLF.\n\tPutTLFCryptKey(TlfID, KeyGen, TLFCryptKey) error\n}\n\n// BlockCacheLifetime denotes the lifetime of an entry in BlockCache.\ntype BlockCacheLifetime int\n\nconst (\n\t// TransientEntry means that the cache entry may be evicted at\n\t// any time.\n\tTransientEntry BlockCacheLifetime = iota\n\t// PermanentEntry means that the cache entry must remain until\n\t// explicitly removed from the cache.\n\tPermanentEntry\n)\n\n// BlockCache gets and puts plaintext dir blocks and file blocks into\n// a cache.  These blocks are immutable and identified by their\n// content hash.\ntype BlockCache interface {\n\t// Get gets the block associated with the given block ID.\n\tGet(ptr BlockPointer) (Block, error)\n\t// CheckForKnownPtr sees whether this cache has a transient\n\t// entry for the given file block, which must be a direct file\n\t// block containing data).  Returns the full BlockPointer\n\t// associated with that ID, including key and data versions.\n\t// If no ID is known, return an uninitialized BlockPointer and\n\t// a nil error.\n\tCheckForKnownPtr(tlf TlfID, block *FileBlock) (BlockPointer, error)\n\t// Put stores the final (content-addressable) block associated\n\t// with the given block ID. If lifetime is TransientEntry,\n\t// then it is assumed that the block exists on the server and\n\t// the entry may be evicted from the cache at any time. If\n\t// lifetime is PermanentEntry, then it is assumed that the\n\t// block doesn't exist on the server and must remain in the\n\t// cache until explicitly removed. As an intermediary state,\n\t// as when a block is being sent to the server, the block may\n\t// be put into the cache both with TransientEntry and\n\t// PermanentEntry -- these are two separate entries. This is\n\t// fine, since the block should be the same.\n\tPut(ptr BlockPointer, tlf TlfID, block Block,\n\t\tlifetime BlockCacheLifetime) error\n\t// DeleteTransient removes the transient entry for the given\n\t// pointer from the cache, as well as any cached IDs so the block\n\t// won't be reused.\n\tDeleteTransient(ptr BlockPointer, tlf TlfID) error\n\t// Delete removes the permanent entry for the non-dirty block\n\t// associated with the given block ID from the cache.  No\n\t// error is returned if no block exists for the given ID.\n\tDeletePermanent(id BlockID) error\n\t// DeleteKnownPtr removes the cached ID for the given file\n\t// block. It does not remove the block itself.\n\tDeleteKnownPtr(tlf TlfID, block *FileBlock) error\n}\n\n// DirtyPermChan is a channel that gets closed when the holder has\n// permission to write.  We are forced to define it as a type due to a\n// bug in mockgen that can't handle return values with a chan\n// struct{}.\ntype DirtyPermChan <-chan struct{}\n\n// DirtyBlockCache gets and puts plaintext dir blocks and file blocks\n// into a cache, which have been modified by the application and not\n// yet committed on the KBFS servers.  They are identified by a\n// (potentially random) ID that may not have any relationship with\n// their context, along with a Branch in case the same TLF is being\n// modified via multiple branches.  Dirty blocks are never evicted,\n// they must be deleted explicitly.\ntype DirtyBlockCache interface {\n\t// Get gets the block associated with the given block ID.  Returns\n\t// the dirty block for the given ID, if one exists.\n\tGet(ptr BlockPointer, branch BranchName) (Block, error)\n\t// Put stores a dirty block currently identified by the\n\t// given block pointer and branch name.\n\tPut(ptr BlockPointer, branch BranchName, block Block) error\n\t// Delete removes the dirty block associated with the given block\n\t// pointer and branch from the cache.  No error is returned if no\n\t// block exists for the given ID.\n\tDelete(ptr BlockPointer, branch BranchName) error\n\t// IsDirty states whether or not the block associated with the\n\t// given block pointer and branch name is dirty in this cache.\n\tIsDirty(ptr BlockPointer, branch BranchName) bool\n\t// RequestPermissionToDirty is called whenever a user wants to\n\t// write data to a file.  The caller provides an estimated number\n\t// of bytes that will become dirty -- this is difficult to know\n\t// exactly without pre-fetching all the blocks involved, but in\n\t// practice we can just use the number of bytes sent in via the\n\t// Write. It returns a channel that blocks until the cache is\n\t// ready to receive more dirty data, at which point the channel is\n\t// closed.  The user must call\n\t// `UpdateUnsyncedBytes(-estimatedDirtyBytes)` once it has\n\t// completed its write and called `UpdateUnsyncedBytes` for all\n\t// the exact dirty block sizes.\n\tRequestPermissionToDirty(ctx context.Context,\n\t\testimatedDirtyBytes int64) (DirtyPermChan, error)\n\t// UpdateUnsyncedBytes is called by a user, who has already been\n\t// granted permission to write, with the delta in block sizes that\n\t// were dirtied as part of the write.  So for example, if a\n\t// newly-dirtied block of 20 bytes was extended by 5 bytes, they\n\t// should send 25.  If on the next write (before any syncs), bytes\n\t// 10-15 of that same block were overwritten, they should send 0\n\t// over the channel because there were no new bytes.  If an\n\t// already-dirtied block is truncated, or if previously requested\n\t// bytes have now been updated more accurately in previous\n\t// requests, newUnsyncedBytes may be negative.  wasSyncing should\n\t// be true if `BlockSyncStarted` has already been called for this\n\t// block.\n\tUpdateUnsyncedBytes(newUnsyncedBytes int64, wasSyncing bool)\n\t// UpdateSyncingBytes is called when a particular block has\n\t// started syncing, or with a negative number when a block is no\n\t// longer syncing due to an error (and BlockSyncFinished will\n\t// never be called).\n\tUpdateSyncingBytes(size int64)\n\t// BlockSyncFinished is called when a particular block has\n\t// finished syncing, though the overall sync might not yet be\n\t// complete.  This lets the cache know it might be able to grant\n\t// more permission to writers.\n\tBlockSyncFinished(size int64)\n\t// SyncFinished is called when a complete sync has completed and\n\t// its dirty blocks have been removed from the cache.  This lets\n\t// the cache know it might be able to grant more permission to\n\t// writers.\n\tSyncFinished(size int64)\n\t// ShouldForceSync returns true if the sync buffer is full enough\n\t// to force all callers to sync their data immediately.\n\tShouldForceSync() bool\n\n\t// Shutdown frees any resources associated with this instance.  It\n\t// returns an error if there are any unsynced blocks.\n\tShutdown() error\n}\n\n// cryptoPure contains all methods of Crypto that don't depend on\n// implicit state, i.e. they're pure functions of the input.\ntype cryptoPure interface {\n\t// MakeRandomTlfID generates a dir ID using a CSPRNG.\n\tMakeRandomTlfID(isPublic bool) (TlfID, error)\n\n\t// MakeRandomBranchID generates a per-device branch ID using a CSPRNG.\n\tMakeRandomBranchID() (BranchID, error)\n\n\t// MakeMdID computes the MD ID of a RootMetadata object.\n\tMakeMdID(md *BareRootMetadata) (MdID, error)\n\n\t// MakeMerkleHash computes the hash of a RootMetadataSigned object\n\t// for inclusion into the KBFS Merkle tree.\n\tMakeMerkleHash(md *RootMetadataSigned) (MerkleHash, error)\n\n\t// MakeTemporaryBlockID generates a temporary block ID using a\n\t// CSPRNG. This is used for indirect blocks before they're\n\t// committed to the server.\n\tMakeTemporaryBlockID() (BlockID, error)\n\n\t// MakePermanentBlockID computes the permanent ID of a block\n\t// given its encoded and encrypted contents.\n\tMakePermanentBlockID(encodedEncryptedData []byte) (BlockID, error)\n\n\t// VerifyBlockID verifies that the given block ID is the\n\t// permanent block ID for the given encoded and encrypted\n\t// data.\n\tVerifyBlockID(encodedEncryptedData []byte, id BlockID) error\n\n\t// MakeRefNonce generates a block reference nonce using a\n\t// CSPRNG. This is used for distinguishing different references to\n\t// the same BlockID.\n\tMakeBlockRefNonce() (BlockRefNonce, error)\n\n\t// MakeRandomTLFKeys generates top-level folder keys using a CSPRNG.\n\tMakeRandomTLFKeys() (TLFPublicKey, TLFPrivateKey, TLFEphemeralPublicKey,\n\t\tTLFEphemeralPrivateKey, TLFCryptKey, error)\n\t// MakeRandomTLFCryptKeyServerHalf generates the server-side of a\n\t// top-level folder crypt key.\n\tMakeRandomTLFCryptKeyServerHalf() (TLFCryptKeyServerHalf, error)\n\t// MakeRandomBlockCryptKeyServerHalf generates the server-side of\n\t// a block crypt key.\n\tMakeRandomBlockCryptKeyServerHalf() (BlockCryptKeyServerHalf, error)\n\n\t// MaskTLFCryptKey returns the client-side of a top-level folder crypt key.\n\tMaskTLFCryptKey(serverHalf TLFCryptKeyServerHalf, key TLFCryptKey) (\n\t\tTLFCryptKeyClientHalf, error)\n\t// UnmaskTLFCryptKey returns the top-level folder crypt key.\n\tUnmaskTLFCryptKey(serverHalf TLFCryptKeyServerHalf,\n\t\tclientHalf TLFCryptKeyClientHalf) (TLFCryptKey, error)\n\t// UnmaskBlockCryptKey returns the block crypt key.\n\tUnmaskBlockCryptKey(serverHalf BlockCryptKeyServerHalf,\n\t\ttlfCryptKey TLFCryptKey) (BlockCryptKey, error)\n\n\t// Verify verifies that sig matches msg being signed with the\n\t// private key that corresponds to verifyingKey.\n\tVerify(msg []byte, sigInfo SignatureInfo) error\n\n\t// EncryptTLFCryptKeyClientHalf encrypts a TLFCryptKeyClientHalf\n\t// using both a TLF's ephemeral private key and a device pubkey.\n\tEncryptTLFCryptKeyClientHalf(privateKey TLFEphemeralPrivateKey,\n\t\tpublicKey CryptPublicKey, clientHalf TLFCryptKeyClientHalf) (\n\t\tEncryptedTLFCryptKeyClientHalf, error)\n\n\t// EncryptPrivateMetadata encrypts a PrivateMetadata object.\n\tEncryptPrivateMetadata(pmd *PrivateMetadata, key TLFCryptKey) (EncryptedPrivateMetadata, error)\n\t// DecryptPrivateMetadata decrypts a PrivateMetadata object.\n\tDecryptPrivateMetadata(encryptedPMD EncryptedPrivateMetadata, key TLFCryptKey) (*PrivateMetadata, error)\n\n\t// EncryptBlocks encrypts a block. plainSize is the size of the encoded\n\t// block; EncryptBlock() must guarantee that plainSize <=\n\t// len(encryptedBlock).\n\tEncryptBlock(block Block, key BlockCryptKey) (\n\t\tplainSize int, encryptedBlock EncryptedBlock, err error)\n\n\t// DecryptBlock decrypts a block. Similar to EncryptBlock(),\n\t// DecryptBlock() must guarantee that (size of the decrypted\n\t// block) <= len(encryptedBlock).\n\tDecryptBlock(encryptedBlock EncryptedBlock, key BlockCryptKey, block Block) error\n\n\t// GetTLFCryptKeyServerHalfID creates a unique ID for this particular\n\t// TLFCryptKeyServerHalf.\n\tGetTLFCryptKeyServerHalfID(\n\t\tuser keybase1.UID, deviceKID keybase1.KID,\n\t\tserverHalf TLFCryptKeyServerHalf) (TLFCryptKeyServerHalfID, error)\n\n\t// VerifyTLFCryptKeyServerHalfID verifies the ID is the proper HMAC result.\n\tVerifyTLFCryptKeyServerHalfID(serverHalfID TLFCryptKeyServerHalfID, user keybase1.UID,\n\t\tdeviceKID keybase1.KID, serverHalf TLFCryptKeyServerHalf) error\n\n\t// EncryptMerkleLeaf encrypts a Merkle leaf node with the TLFPublicKey.\n\tEncryptMerkleLeaf(leaf MerkleLeaf, pubKey TLFPublicKey, nonce *[24]byte,\n\t\tePrivKey TLFEphemeralPrivateKey) (EncryptedMerkleLeaf, error)\n\n\t// DecryptMerkleLeaf decrypts a Merkle leaf node with the TLFPrivateKey.\n\tDecryptMerkleLeaf(encryptedLeaf EncryptedMerkleLeaf, privKey TLFPrivateKey,\n\t\tnonce *[24]byte, ePubKey TLFEphemeralPublicKey) (*MerkleLeaf, error)\n}\n\ntype cryptoSigner interface {\n\t// Sign signs the msg with the current device's private key.\n\tSign(ctx context.Context, msg []byte) (sigInfo SignatureInfo, err error)\n\t// Sign signs the msg with the current device's private key and output\n\t// the full serialized NaclSigInfo.\n\tSignToString(ctx context.Context, msg []byte) (signature string, err error)\n}\n\n// Crypto signs, verifies, encrypts, and decrypts stuff.\ntype Crypto interface {\n\tcryptoPure\n\tcryptoSigner\n\n\t// DecryptTLFCryptKeyClientHalf decrypts a TLFCryptKeyClientHalf\n\t// using the current device's private key and the TLF's ephemeral\n\t// public key.\n\tDecryptTLFCryptKeyClientHalf(ctx context.Context,\n\t\tpublicKey TLFEphemeralPublicKey,\n\t\tencryptedClientHalf EncryptedTLFCryptKeyClientHalf) (\n\t\tTLFCryptKeyClientHalf, error)\n\n\t// DecryptTLFCryptKeyClientHalfAny decrypts one of the\n\t// TLFCryptKeyClientHalf using the available private keys and the\n\t// ephemeral public key.  If promptPaper is true, the service will\n\t// prompt the user for any unlocked paper keys.\n\tDecryptTLFCryptKeyClientHalfAny(ctx context.Context,\n\t\tkeys []EncryptedTLFCryptKeyClientAndEphemeral, promptPaper bool) (\n\t\tTLFCryptKeyClientHalf, int, error)\n\n\t// Shutdown frees any resources associated with this instance.\n\tShutdown()\n}\n\n// Codec encodes and decodes arbitrary data\ntype Codec interface {\n\t// Decode unmarshals the given buffer into the given object, if possible.\n\tDecode(buf []byte, obj interface{}) error\n\t// Encode marshals the given object into a returned buffer.\n\tEncode(obj interface{}) ([]byte, error)\n\t// RegisterType should be called for all types that are stored\n\t// under ambiguous types (like interface{} or nil interface) in a\n\t// struct that will be encoded/decoded by the codec.  Each must\n\t// have a unique extCode.  Types that include other extension\n\t// types are not supported.\n\tRegisterType(rt reflect.Type, code extCode)\n\t// RegisterIfaceSliceType should be called for all encoded slices\n\t// that contain ambiguous interface types.  Each must have a\n\t// unique extCode.  Slice element types that include other\n\t// extension types are not supported.\n\t//\n\t// If non-nil, typer is used to do a type assertion during\n\t// decoding, to convert the encoded value into the value expected\n\t// by the rest of the code.  This is needed, for example, when the\n\t// codec cannot decode interface types to their desired pointer\n\t// form.\n\tRegisterIfaceSliceType(rt reflect.Type, code extCode,\n\t\ttyper func(interface{}) reflect.Value)\n}\n\n// MDOps gets and puts root metadata to an MDServer.  On a get, it\n// verifies the metadata is signed by the metadata's signing key.\ntype MDOps interface {\n\t// GetForHandle returns the current metadata object\n\t// corresponding to the given top-level folder's handle and\n\t// merge status, if the logged-in user has read permission on\n\t// the folder.  It creates the folder if one doesn't exist\n\t// yet, and the logged-in user has permission to do so.\n\tGetForHandle(\n\t\tctx context.Context, handle *TlfHandle, mStatus MergeStatus) (\n\t\tTlfID, ImmutableRootMetadata, error)\n\n\t// GetForTLF returns the current metadata object\n\t// corresponding to the given top-level folder, if the logged-in\n\t// user has read permission on the folder.\n\tGetForTLF(ctx context.Context, id TlfID) (ImmutableRootMetadata, error)\n\n\t// GetUnmergedForTLF is the same as the above but for unmerged\n\t// metadata.\n\tGetUnmergedForTLF(ctx context.Context, id TlfID, bid BranchID) (\n\t\tImmutableRootMetadata, error)\n\n\t// GetRange returns a range of metadata objects corresponding to\n\t// the passed revision numbers (inclusive).\n\tGetRange(ctx context.Context, id TlfID, start, stop MetadataRevision) (\n\t\t[]ImmutableRootMetadata, error)\n\n\t// GetUnmergedRange is the same as the above but for unmerged\n\t// metadata history (inclusive).\n\tGetUnmergedRange(ctx context.Context, id TlfID, bid BranchID,\n\t\tstart, stop MetadataRevision) ([]ImmutableRootMetadata, error)\n\n\t// Put stores the metadata object for the given\n\t// top-level folder.\n\tPut(ctx context.Context, rmd *RootMetadata) (MdID, error)\n\n\t// PutUnmerged is the same as the above but for unmerged\n\t// metadata history.\n\tPutUnmerged(ctx context.Context, rmd *RootMetadata) (MdID, error)\n\n\t// PruneBranch prunes all unmerged history for the given TLF\n\t// branch.\n\tPruneBranch(ctx context.Context, id TlfID, bid BranchID) error\n\n\t// GetLatestHandleForTLF returns the server's idea of the latest handle for the TLF,\n\t// which may not yet be reflected in the MD if the TLF hasn't been rekeyed since it\n\t// entered into a conflicting state.\n\tGetLatestHandleForTLF(ctx context.Context, id TlfID) (\n\t\tBareTlfHandle, error)\n}\n\n// KeyOps fetches server-side key halves from the key server.\ntype KeyOps interface {\n\t// GetTLFCryptKeyServerHalf gets a server-side key half for a\n\t// device given the key half ID.\n\tGetTLFCryptKeyServerHalf(ctx context.Context,\n\t\tserverHalfID TLFCryptKeyServerHalfID,\n\t\tcryptPublicKey CryptPublicKey) (TLFCryptKeyServerHalf, error)\n\n\t// PutTLFCryptKeyServerHalves stores a server-side key halves for a\n\t// set of users and devices.\n\tPutTLFCryptKeyServerHalves(ctx context.Context,\n\t\tserverKeyHalves map[keybase1.UID]map[keybase1.KID]TLFCryptKeyServerHalf) error\n\n\t// DeleteTLFCryptKeyServerHalf deletes a server-side key half for a\n\t// device given the key half ID.\n\tDeleteTLFCryptKeyServerHalf(ctx context.Context,\n\t\tuid keybase1.UID, kid keybase1.KID,\n\t\tserverHalfID TLFCryptKeyServerHalfID) error\n}\n\n// BlockOps gets and puts data blocks to a BlockServer. It performs\n// the necessary crypto operations on each block.\ntype BlockOps interface {\n\t// Get gets the block associated with the given block pointer\n\t// (which belongs to the TLF with the given key metadata),\n\t// decrypts it if necessary, and fills in the provided block\n\t// object with its contents, if the logged-in user has read\n\t// permission for that block.\n\tGet(ctx context.Context, kmd KeyMetadata, blockPtr BlockPointer,\n\t\tblock Block) error\n\n\t// Ready turns the given block (which belongs to the TLF with\n\t// the given key metadata) into encoded (and encrypted) data,\n\t// and calculates its ID and size, so that we can do a bunch\n\t// of block puts in parallel for every write. Ready() must\n\t// guarantee that plainSize <= readyBlockData.QuotaSize().\n\tReady(ctx context.Context, kmd KeyMetadata, block Block) (\n\t\tid BlockID, plainSize int, readyBlockData ReadyBlockData, err error)\n\n\t// Put stores the readied block data under the given block\n\t// pointer (which belongs to the TLF with the given ID) on the\n\t// server.\n\tPut(ctx context.Context, tlfID TlfID, blockPtr BlockPointer,\n\t\treadyBlockData ReadyBlockData) error\n\n\t// Delete instructs the server to delete the given block references.\n\t// It returns the number of not-yet deleted references to\n\t// each block reference\n\tDelete(ctx context.Context, tlfID TlfID, ptrs []BlockPointer) (\n\t\tliveCounts map[BlockID]int, err error)\n\n\t// Archive instructs the server to mark the given block references\n\t// as \"archived\"; that is, they are not being used in the current\n\t// view of the folder, and shouldn't be served to anyone other\n\t// than folder writers.\n\tArchive(ctx context.Context, tlfID TlfID, ptrs []BlockPointer) error\n}\n\n// MDServer gets and puts metadata for each top-level directory.  The\n// instantiation should be able to fetch session/user details via KBPKI.  On a\n// put, the server is responsible for 1) ensuring the user has appropriate\n// permissions for whatever modifications were made; 2) ensuring that\n// LastModifyingWriter and LastModifyingUser are updated appropriately; and 3)\n// detecting conflicting writes based on the previous root block ID (i.e., when\n// it supports strict consistency).  On a get, it verifies the logged-in user\n// has read permissions.\n//\n// TODO: Add interface for searching by time\ntype MDServer interface {\n\tAuthTokenRefreshHandler\n\n\t// GetForHandle returns the current (signed/encrypted) metadata\n\t// object corresponding to the given top-level folder's handle, if\n\t// the logged-in user has read permission on the folder.  It\n\t// creates the folder if one doesn't exist yet, and the logged-in\n\t// user has permission to do so.\n\tGetForHandle(ctx context.Context, handle BareTlfHandle,\n\t\tmStatus MergeStatus) (TlfID, *RootMetadataSigned, error)\n\n\t// GetForTLF returns the current (signed/encrypted) metadata object\n\t// corresponding to the given top-level folder, if the logged-in\n\t// user has read permission on the folder.\n\tGetForTLF(ctx context.Context, id TlfID, bid BranchID, mStatus MergeStatus) (\n\t\t*RootMetadataSigned, error)\n\n\t// GetRange returns a range of (signed/encrypted) metadata objects\n\t// corresponding to the passed revision numbers (inclusive).\n\tGetRange(ctx context.Context, id TlfID, bid BranchID, mStatus MergeStatus,\n\t\tstart, stop MetadataRevision) ([]*RootMetadataSigned, error)\n\n\t// Put stores the (signed/encrypted) metadata object for the given\n\t// top-level folder. Note: If the unmerged bit is set in the metadata\n\t// block's flags bitmask it will be appended to the unmerged per-device\n\t// history.\n\tPut(ctx context.Context, rmds *RootMetadataSigned) error\n\n\t// PruneBranch prunes all unmerged history for the given TLF branch.\n\tPruneBranch(ctx context.Context, id TlfID, bid BranchID) error\n\n\t// RegisterForUpdate tells the MD server to inform the caller when\n\t// there is a merged update with a revision number greater than\n\t// currHead, which did NOT originate from this same MD server\n\t// session.  This method returns a chan which can receive only a\n\t// single error before it's closed.  If the received err is nil,\n\t// then there is updated MD ready to fetch which didn't originate\n\t// locally; if it is non-nil, then the previous registration\n\t// cannot send the next notification (e.g., the connection to the\n\t// MD server may have failed). In either case, the caller must\n\t// re-register to get a new chan that can receive future update\n\t// notifications.\n\tRegisterForUpdate(ctx context.Context, id TlfID,\n\t\tcurrHead MetadataRevision) (<-chan error, error)\n\n\t// CheckForRekeys initiates the rekey checking process on the\n\t// server.  The server is allowed to delay this request, and so it\n\t// returns a channel for returning the error. Actual rekey\n\t// requests are expected to come in asynchronously.\n\tCheckForRekeys(ctx context.Context) <-chan error\n\n\t// TruncateLock attempts to take the history truncation lock for\n\t// this folder, for a TTL defined by the server.  Returns true if\n\t// the lock was successfully taken.\n\tTruncateLock(ctx context.Context, id TlfID) (bool, error)\n\t// TruncateUnlock attempts to release the history truncation lock\n\t// for this folder.  Returns true if the lock was successfully\n\t// released.\n\tTruncateUnlock(ctx context.Context, id TlfID) (bool, error)\n\n\t// DisableRekeyUpdatesForTesting disables processing rekey updates\n\t// received from the mdserver while testing.\n\tDisableRekeyUpdatesForTesting()\n\n\t// Shutdown is called to shutdown an MDServer connection.\n\tShutdown()\n\n\t// IsConnected returns whether the MDServer is connected.\n\tIsConnected() bool\n\n\t// GetLatestHandleForTLF returns the server's idea of the latest handle for the TLF,\n\t// which may not yet be reflected in the MD if the TLF hasn't been rekeyed since it\n\t// entered into a conflicting state.  For the highest level of confidence, the caller\n\t// should verify the mapping with a Merkle tree lookup.\n\tGetLatestHandleForTLF(ctx context.Context, id TlfID) (\n\t\tBareTlfHandle, error)\n\n\t// OffsetFromServerTime is the current estimate for how off our\n\t// local clock is from the mdserver clock.  Add this to any\n\t// mdserver-provided timestamps to get the \"local\" time of the\n\t// corresponding event.  If the returned bool is false, then we\n\t// don't have a current estimate for the offset.\n\tOffsetFromServerTime() (time.Duration, bool)\n}\n\ntype mdServerLocal interface {\n\tMDServer\n\taddNewAssertionForTest(\n\t\tuid keybase1.UID, newAssertion keybase1.SocialAssertion) error\n\tgetCurrentMergedHeadRevision(ctx context.Context, id TlfID) (\n\t\trev MetadataRevision, err error)\n\tisShutdown() bool\n\tcopy(config Config) mdServerLocal\n}\n\n// BlockServer gets and puts opaque data blocks.  The instantiation\n// should be able to fetch session/user details via KBPKI.  On a\n// put/delete, the server is reponsible for: 1) checking that the ID\n// matches the hash of the buffer; and 2) enforcing writer quotas.\ntype BlockServer interface {\n\tAuthTokenRefreshHandler\n\n\t// Get gets the (encrypted) block data associated with the given\n\t// block ID and context, uses the provided block key to decrypt\n\t// the block, and fills in the provided block object with its\n\t// contents, if the logged-in user has read permission for that\n\t// block.\n\tGet(ctx context.Context, tlfID TlfID, id BlockID, context BlockContext) (\n\t\t[]byte, BlockCryptKeyServerHalf, error)\n\t// Put stores the (encrypted) block data under the given ID and\n\t// context on the server, along with the server half of the block\n\t// key.  context should contain a BlockRefNonce of zero.  There\n\t// will be an initial reference for this block for the given\n\t// context.\n\t//\n\t// Put should be idempotent, although it should also return an\n\t// error if, for a given ID, any of the other arguments differ\n\t// from previous Put calls with the same ID.\n\t//\n\t// If this returns a BServerErrorOverQuota, with Throttled=false,\n\t// the caller can treat it as informational and otherwise ignore\n\t// the error.\n\tPut(ctx context.Context, tlfID TlfID, id BlockID, context BlockContext,\n\t\tbuf []byte, serverHalf BlockCryptKeyServerHalf) error\n\n\t// AddBlockReference adds a new reference to the given block,\n\t// defined by the given context (which should contain a non-zero\n\t// BlockRefNonce).  (Contexts with a BlockRefNonce of zero should\n\t// be used when putting the block for the first time via Put().)\n\t// Returns a BServerErrorBlockNonExistent if id is unknown within\n\t// this folder.\n\t//\n\t// AddBlockReference should be idempotent, although it should\n\t// also return an error if, for a given ID and refnonce, any\n\t// of the other fields of context differ from previous\n\t// AddBlockReference calls with the same ID and refnonce.\n\t//\n\t// If this returns a BServerErrorOverQuota, with Throttled=false,\n\t// the caller can treat it as informational and otherwise ignore\n\t// the error.\n\tAddBlockReference(ctx context.Context, tlfID TlfID, id BlockID,\n\t\tcontext BlockContext) error\n\t// RemoveBlockReferences removes the references to the given block\n\t// ID defined by the given contexts.  If no references to the block\n\t// remain after this call, the server is allowed to delete the\n\t// corresponding block permanently.  If the reference defined by\n\t// the count has already been removed, the call is a no-op.\n\t// It returns the number of remaining not-yet-deleted references after this\n\t// reference has been removed\n\tRemoveBlockReferences(ctx context.Context, tlfID TlfID,\n\t\tcontexts map[BlockID][]BlockContext) (liveCounts map[BlockID]int, err error)\n\n\t// ArchiveBlockReferences marks the given block references as\n\t// \"archived\"; that is, they are not being used in the current\n\t// view of the folder, and shouldn't be served to anyone other\n\t// than folder writers.\n\t//\n\t// For a given ID/refnonce pair, ArchiveBlockReferences should\n\t// be idempotent, although it should also return an error if\n\t// any of the other fields of the context differ from previous\n\t// calls with the same ID/refnonce pair.\n\tArchiveBlockReferences(ctx context.Context, tlfID TlfID,\n\t\tcontexts map[BlockID][]BlockContext) error\n\n\t// Shutdown is called to shutdown a BlockServer connection.\n\tShutdown()\n\n\t// GetUserQuotaInfo returns the quota for the user.\n\tGetUserQuotaInfo(ctx context.Context) (info *UserQuotaInfo, err error)\n}\n\ntype blockRefLocalStatus int\n\nconst (\n\tliveBlockRef     blockRefLocalStatus = 1\n\tarchivedBlockRef                     = 2\n)\n\n// blockServerLocal is the interface for BlockServer implementations\n// that store data locally.\ntype blockServerLocal interface {\n\tBlockServer\n\t// getAll returns all the known block references, and should only be\n\t// used during testing.\n\tgetAll(ctx context.Context, tlfID TlfID) (\n\t\tmap[BlockID]map[BlockRefNonce]blockRefLocalStatus, error)\n}\n\n// BlockSplitter decides when a file or directory block needs to be split\ntype BlockSplitter interface {\n\t// CopyUntilSplit copies data into the block until we reach the\n\t// point where we should split, but only if writing to the end of\n\t// the last block.  If this is writing into the middle of a file,\n\t// just copy everything that will fit into the block, and assume\n\t// that block boundaries will be fixed later. Return how much was\n\t// copied.\n\tCopyUntilSplit(\n\t\tblock *FileBlock, lastBlock bool, data []byte, off int64) int64\n\n\t// CheckSplit, given a block, figures out whether it ends at the\n\t// right place.  If so, return 0.  If not, return either the\n\t// offset in the block where it should be split, or -1 if more\n\t// bytes from the next block should be appended.\n\tCheckSplit(block *FileBlock) int64\n\n\t// ShouldEmbedBlockChanges decides whether we should keep the\n\t// block changes embedded in the MD or not.\n\tShouldEmbedBlockChanges(bc *BlockChanges) bool\n}\n\n// KeyServer fetches/writes server-side key halves from/to the key server.\ntype KeyServer interface {\n\t// GetTLFCryptKeyServerHalf gets a server-side key half for a\n\t// device given the key half ID.\n\tGetTLFCryptKeyServerHalf(ctx context.Context,\n\t\tserverHalfID TLFCryptKeyServerHalfID,\n\t\tcryptPublicKey CryptPublicKey) (TLFCryptKeyServerHalf, error)\n\n\t// PutTLFCryptKeyServerHalves stores a server-side key halves for a\n\t// set of users and devices.\n\tPutTLFCryptKeyServerHalves(ctx context.Context,\n\t\tserverKeyHalves map[keybase1.UID]map[keybase1.KID]TLFCryptKeyServerHalf) error\n\n\t// DeleteTLFCryptKeyServerHalf deletes a server-side key half for a\n\t// device given the key half ID.\n\tDeleteTLFCryptKeyServerHalf(ctx context.Context,\n\t\tuid keybase1.UID, kid keybase1.KID,\n\t\tserverHalfID TLFCryptKeyServerHalfID) error\n\n\t// Shutdown is called to free any KeyServer resources.\n\tShutdown()\n}\n\n// NodeChange represents a change made to a node as part of an atomic\n// file system operation.\ntype NodeChange struct {\n\tNode Node\n\t// Basenames of entries added/removed.\n\tDirUpdated  []string\n\tFileUpdated []WriteRange\n}\n\n// Observer can be notified that there is an available update for a\n// given directory.  The notification callbacks should not block, or\n// make any calls to the Notifier interface.  Nodes passed to the\n// observer should not be held past the end of the notification\n// callback.\ntype Observer interface {\n\t// LocalChange announces that the file at this Node has been\n\t// updated locally, but not yet saved at the server.\n\tLocalChange(ctx context.Context, node Node, write WriteRange)\n\t// BatchChanges announces that the nodes have all been updated\n\t// together atomically.  Each NodeChange in changes affects the\n\t// same top-level folder and branch.\n\tBatchChanges(ctx context.Context, changes []NodeChange)\n\t// TlfHandleChange announces that the handle of the corresponding\n\t// folder branch has changed, likely due to previously-unresolved\n\t// assertions becoming resolved.  This indicates that the listener\n\t// should switch over any cached paths for this folder-branch to\n\t// the new name.  Nodes that were acquired under the old name will\n\t// still continue to work, but new lookups on the old name may\n\t// either encounter alias errors or entirely new TLFs (in the case\n\t// of conflicts).\n\tTlfHandleChange(ctx context.Context, newHandle *TlfHandle)\n}\n\n// Notifier notifies registrants of directory changes\ntype Notifier interface {\n\t// RegisterForChanges declares that the given Observer wants to\n\t// subscribe to updates for the given top-level folders.\n\tRegisterForChanges(folderBranches []FolderBranch, obs Observer) error\n\t// UnregisterFromChanges declares that the given Observer no\n\t// longer wants to subscribe to updates for the given top-level\n\t// folders.\n\tUnregisterFromChanges(folderBranches []FolderBranch, obs Observer) error\n}\n\n// Clock is an interface for getting the current time\ntype Clock interface {\n\t// Now returns the current time.\n\tNow() time.Time\n}\n\n// ConflictRenamer deals with names for conflicting directory entries.\ntype ConflictRenamer interface {\n\t// ConflictRename returns the appropriately modified filename.\n\tConflictRename(op op, original string) string\n}\n\n// Config collects all the singleton instance instantiations needed to\n// run KBFS in one place.  The methods below are self-explanatory and\n// do not require comments.\ntype Config interface {\n\tKBFSOps() KBFSOps\n\tSetKBFSOps(KBFSOps)\n\tKBPKI() KBPKI\n\tSetKBPKI(KBPKI)\n\tKeyManager() KeyManager\n\tSetKeyManager(KeyManager)\n\tReporter() Reporter\n\tSetReporter(Reporter)\n\tMDCache() MDCache\n\tSetMDCache(MDCache)\n\tKeyCache() KeyCache\n\tSetKeyCache(KeyCache)\n\tBlockCache() BlockCache\n\tSetBlockCache(BlockCache)\n\tDirtyBlockCache() DirtyBlockCache\n\tSetDirtyBlockCache(DirtyBlockCache)\n\tCrypto() Crypto\n\tSetCrypto(Crypto)\n\tCodec() Codec\n\tSetCodec(Codec)\n\tMDOps() MDOps\n\tSetMDOps(MDOps)\n\tKeyOps() KeyOps\n\tSetKeyOps(KeyOps)\n\tBlockOps() BlockOps\n\tSetBlockOps(BlockOps)\n\tMDServer() MDServer\n\tSetMDServer(MDServer)\n\tBlockServer() BlockServer\n\tSetBlockServer(BlockServer)\n\tKeyServer() KeyServer\n\tSetKeyServer(KeyServer)\n\tKeybaseService() KeybaseService\n\tSetKeybaseService(KeybaseService)\n\tBlockSplitter() BlockSplitter\n\tSetBlockSplitter(BlockSplitter)\n\tNotifier() Notifier\n\tSetNotifier(Notifier)\n\tClock() Clock\n\tSetClock(Clock)\n\tConflictRenamer() ConflictRenamer\n\tSetConflictRenamer(ConflictRenamer)\n\tMetadataVersion() MetadataVer\n\tDataVersion() DataVer\n\tRekeyQueue() RekeyQueue\n\tSetRekeyQueue(RekeyQueue)\n\t// ReqsBufSize indicates the number of read or write operations\n\t// that can be buffered per folder\n\tReqsBufSize() int\n\t// MaxFileBytes indicates the maximum supported plaintext size of\n\t// a file in bytes.\n\tMaxFileBytes() uint64\n\t// MaxNameBytes indicates the maximum supported size of a\n\t// directory entry name in bytes.\n\tMaxNameBytes() uint32\n\t// MaxDirBytes indicates the maximum supported plaintext size of a\n\t// directory in bytes.\n\tMaxDirBytes() uint64\n\t// DoBackgroundFlushes says whether we should periodically try to\n\t// flush dirty files, even without a sync from the user.  Should\n\t// be true except for during some testing.\n\tDoBackgroundFlushes() bool\n\tSetDoBackgroundFlushes(bool)\n\t// RekeyWithPromptWaitTime indicates how long to wait, after\n\t// setting the rekey bit, before prompting for a paper key.\n\tRekeyWithPromptWaitTime() time.Duration\n\n\t// QuotaReclamationPeriod indicates how often should each TLF\n\t// should check for quota to reclaim.  If the Duration.Seconds()\n\t// == 0, quota reclamation should not run automatically.\n\tQuotaReclamationPeriod() time.Duration\n\t// QuotaReclamationMinUnrefAge indicates the minimum time a block\n\t// must have been unreferenced before it can be reclaimed.\n\tQuotaReclamationMinUnrefAge() time.Duration\n\n\t// ResetCaches clears and re-initializes all data and key caches.\n\tResetCaches()\n\n\tMakeLogger(module string) logger.Logger\n\tSetLoggerMaker(func(module string) logger.Logger)\n\t// MetricsRegistry may be nil, which should be interpreted as\n\t// not using metrics at all. (i.e., as if UseNilMetrics were\n\t// set). This differs from how go-metrics treats nil Registry\n\t// objects, which is to use the default registry.\n\tMetricsRegistry() metrics.Registry\n\tSetMetricsRegistry(metrics.Registry)\n\t// TLFValidDuration is the time TLFs are valid before identification needs to be redone.\n\tTLFValidDuration() time.Duration\n\t// SetTLFValidDuration sets TLFValidDuration.\n\tSetTLFValidDuration(time.Duration)\n\t// Shutdown is called to free config resources.\n\tShutdown() error\n\t// CheckStateOnShutdown tells the caller whether or not it is safe\n\t// to check the state of the system on shutdown.\n\tCheckStateOnShutdown() bool\n}\n\n// NodeCache holds Nodes, and allows libkbfs to update them when\n// things change about the underlying KBFS blocks.  It is probably\n// most useful to instantiate this on a per-folder-branch basis, so\n// that it can create a Path with the correct DirId and Branch name.\ntype NodeCache interface {\n\t// GetOrCreate either makes a new Node for the given\n\t// BlockPointer, or returns an existing one. TODO: If we ever\n\t// support hard links, we will have to revisit the \"name\" and\n\t// \"parent\" parameters here.  name must not be empty. Returns\n\t// an error if parent cannot be found.\n\tGetOrCreate(ptr BlockPointer, name string, parent Node) (Node, error)\n\t// Get returns the Node associated with the given ptr if one\n\t// already exists.  Otherwise, it returns nil.\n\tGet(ref blockRef) Node\n\t// UpdatePointer updates the BlockPointer for the corresponding\n\t// Node.  NodeCache ignores this call when oldRef is not cached in\n\t// any Node.\n\tUpdatePointer(oldRef blockRef, newPtr BlockPointer)\n\t// Move swaps the parent node for the corresponding Node, and\n\t// updates the node's name.  NodeCache ignores the call when ptr\n\t// is not cached.  Returns an error if newParent cannot be found.\n\t// If newParent is nil, it treats the ptr's corresponding node as\n\t// being unlinked from the old parent completely.\n\tMove(ref blockRef, newParent Node, newName string) error\n\t// Unlink set the corresponding node's parent to nil and caches\n\t// the provided path in case the node is still open. NodeCache\n\t// ignores the call when ptr is not cached.  The path is required\n\t// because the caller may have made changes to the parent nodes\n\t// already that shouldn't be reflected in the cached path.\n\tUnlink(ref blockRef, oldPath path)\n\t// PathFromNode creates the path up to a given Node.\n\tPathFromNode(node Node) path\n}\n\n// fileBlockDeepCopier fetches a file block, makes a deep copy of it\n// (duplicating pointer for any indirect blocks) and generates a new\n// random temporary block ID for it.  It returns the new BlockPointer,\n// and internally saves the block for future uses.\ntype fileBlockDeepCopier func(context.Context, string, BlockPointer) (\n\tBlockPointer, error)\n\n// crAction represents a specific action to take as part of the\n// conflict resolution process.\ntype crAction interface {\n\t// swapUnmergedBlock should be called before do(), and if it\n\t// returns true, the caller must use the merged block\n\t// corresponding to the returned BlockPointer instead of\n\t// unmergedBlock when calling do().  If BlockPointer{} is zeroPtr\n\t// (and true is returned), just swap in the regular mergedBlock.\n\tswapUnmergedBlock(unmergedChains *crChains, mergedChains *crChains,\n\t\tunmergedBlock *DirBlock) (bool, BlockPointer, error)\n\t// do modifies the given merged block in place to resolve the\n\t// conflict, and potential uses the provided blockCopyFetchers to\n\t// obtain copies of other blocks (along with new BlockPointers)\n\t// when requiring a block copy.\n\tdo(ctx context.Context, unmergedCopier fileBlockDeepCopier,\n\t\tmergedCopier fileBlockDeepCopier, unmergedBlock *DirBlock,\n\t\tmergedBlock *DirBlock) error\n\t// updateOps potentially modifies, in place, the slices of\n\t// unmerged and merged operations stored in the corresponding\n\t// crChains for the given unmerged and merged most recent\n\t// pointers.  Eventually, the \"unmerged\" ops will be pushed as\n\t// part of a MD update, and so should contain any necessarily\n\t// operations to fully merge the unmerged data, including any\n\t// conflict resolution.  The \"merged\" ops will be played through\n\t// locally, to notify any caches about the newly-obtained merged\n\t// data (and any changes to local data that were required as part\n\t// of conflict resolution, such as renames).  A few things to note:\n\t// * A particular action's updateOps method may be called more than\n\t//   once for different sets of chains, however it should only add\n\t//   new directory operations (like create/rm/rename) into directory\n\t//   chains.\n\t// * updateOps doesn't necessarily result in correct BlockPointers within\n\t//   each of those ops; that must happen in a later phase.\n\t// * mergedBlock can be nil if the chain is for a file.\n\tupdateOps(unmergedMostRecent BlockPointer, mergedMostRecent BlockPointer,\n\t\tunmergedBlock *DirBlock, mergedBlock *DirBlock,\n\t\tunmergedChains *crChains, mergedChains *crChains) error\n\t// String returns a string representation for this crAction, used\n\t// for debugging.\n\tString() string\n}\n\n// RekeyQueue is a managed queue of folders needing some rekey action taken upon them\n// by the current client.\ntype RekeyQueue interface {\n\t// Enqueue enqueues a folder for rekey action.\n\tEnqueue(TlfID) <-chan error\n\t// IsRekeyPending returns true if the given folder is in the rekey queue.\n\tIsRekeyPending(TlfID) bool\n\t// GetRekeyChannel will return any rekey completion channel (if pending.)\n\tGetRekeyChannel(id TlfID) <-chan error\n\t// Clear cancels all pending rekey actions and clears the queue.\n\tClear()\n\t// Waits for all queued rekeys to finish\n\tWait(ctx context.Context) error\n}\n", "idx": 1, "id": 12639, "msg": "Previously we worked to remove all references from metadata from the `KBFSOps` interface -- we don't really want outside libraries to know about that concept. So ideally neither `ImmutableRootMetadata` or the term \"MD\" would appear in this interface. I can think of two possible alternatives: - Define some new interface containing just this method, make it available on `Config`, and set it to the same `*KBFSOpsStandard` instance whenever a `Config` is made. - When `KeybaseDaemonRPC` is constructed, pass it a function that calls that method on `KBFSOpsStandard`. (And maybe don't capitalize the method name.) What do you think? Other options? I slightly prefer the second option.", "proj": "keybase-kbfs", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,39 @@\n+package stringutils\n+\n+// https://play.golang.org/p/xSEX1CAcQE\n+\n+import (\n+\t\"fmt\"\n+\t\"regexp\"\n+\t\"strings\"\n+)\n+\n+var rex = regexp.MustCompile(\"\\\\s*(\\\\w+)=(.+)\")\n+\n+// ParseKeyValueList parses a comma-separated key=value list to a map.\n+// Keys must contain only word characters (leading spaces ignored).\n+// A valid map is always returned even when the error is != nil.\n+// Spaces in the value are preserved.\n+func ParseKeyValueList(param string) (*map[string]string, error) {\n+\tres := make(map[string]string)\n+\tif len(strings.TrimSpace(param)) == 0 {\n+\t\treturn &res, nil\n+\t}\n+\tvar invalidItems []string\n+\tfor _, item := range strings.Split(param, \",\") {\n+\t\tif item == \"\" {\n+\t\t\t// Accept empty items (e.g tailing \",\")\n+\t\t\tcontinue\n+\t\t}\n+\t\tkv := rex.FindStringSubmatch(item)\n+\t\tif kv == nil {\n+\t\t\tinvalidItems = append(invalidItems, item)\n+\t\t\tcontinue\n+\t\t}\n+\t\tres[kv[1]] = kv[2]\n+\t}\n+\tif len(invalidItems) > 0 {\n+\t\treturn &res, fmt.Errorf(\"Invalid items %v\", invalidItems)\n+\t}\n+\treturn &res, nil\n+}", "y": 1, "oldf": "", "idx": 1, "id": 17757, "msg": "Go idiom is not to return a value and an error from the same call.", "proj": "projectcalico-felix", "lang": "c", "sampling_weight": 0.060228073380130996}
{"patch": "@@ -14,7 +14,7 @@\n  */\n package org.hyperledger.besu.enclave;\n \n-public class EnclaveClientException extends IllegalArgumentException {\n+public class EnclaveClientException extends EnclaveException {\n   private int statusCode;\n \n   public EnclaveClientException(final int statusCode, final String message) {", "y": 1, "oldf": "/*\n * Copyright ConsenSys AG.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n * the License. You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n * specific language governing permissions and limitations under the License.\n *\n * SPDX-License-Identifier: Apache-2.0\n */\npackage org.hyperledger.besu.enclave;\n\npublic class EnclaveClientException extends IllegalArgumentException {\n  private int statusCode;\n\n  public EnclaveClientException(final int statusCode, final String message) {\n    super(message);\n    this.statusCode = statusCode;\n  }\n\n  public EnclaveClientException(final String message, final Throwable cause) {\n    super(message, cause);\n  }\n\n  public int getStatusCode() {\n    return statusCode;\n  }\n}\n", "idx": 1, "id": 21113, "msg": "What is the difference between a `EnclaveClientException` and a `EnclaveException`, is there value in actually having two, would just one exception suffice?", "proj": "hyperledger-besu", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -20,6 +20,7 @@ import (\n \n \tosconfigpb \"github.com/GoogleCloudPlatform/compute-image-tools/cli_tools/osconfig_agent/_internal/gapi-cloud-osconfig-go/google.golang.org/genproto/googleapis/cloud/osconfig/v1alpha1\"\n \t\"github.com/golang/protobuf/ptypes/duration\"\n+\tdate \"google.golang.org/genproto/googleapis/type/date\"\n \t\"google.golang.org/genproto/googleapis/type/dayofweek\"\n \t\"google.golang.org/genproto/googleapis/type/timeofday\"\n )", "y": 1, "oldf": "//  Copyright 2018 Google Inc. All Rights Reserved.\n//\n//  Licensed under the Apache License, Version 2.0 (the \"License\");\n//  you may not use this file except in compliance with the License.\n//  You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n//  Unless required by applicable law or agreed to in writing, software\n//  distributed under the License is distributed on an \"AS IS\" BASIS,\n//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n//  See the License for the specific language governing permissions and\n//  limitations under the License.\n\npackage main\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\tosconfigpb \"github.com/GoogleCloudPlatform/compute-image-tools/cli_tools/osconfig_agent/_internal/gapi-cloud-osconfig-go/google.golang.org/genproto/googleapis/cloud/osconfig/v1alpha1\"\n\t\"github.com/golang/protobuf/ptypes/duration\"\n\t\"google.golang.org/genproto/googleapis/type/dayofweek\"\n\t\"google.golang.org/genproto/googleapis/type/timeofday\"\n)\n\nfunc TestNextWindow(t *testing.T) {\n\tnow := time.Date(2018, 7, 1, 5, 0, 0, 0, time.UTC) // July 1st 2018 is a Sunday\n\tvar tests = []struct {\n\t\tdesc      string\n\t\tpw        *osconfigpb.PatchWindow\n\t\tnow       time.Time\n\t\twantStart time.Time\n\t\twantEnd   time.Time\n\t}{\n\t\t{\n\t\t\t\"daily (today before patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Daily_{Daily: &osconfigpb.PatchWindow_Daily{}},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Daily at 5\n\t\t\tnow.Add(-2 * time.Hour), // We should be before the patch window\n\t\t\tnow,\n\t\t\tnow.Add(3600 * time.Second), // Todays patch window\n\t\t},\n\t\t{\n\t\t\t\"daily (today inside patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Daily_{Daily: &osconfigpb.PatchWindow_Daily{}},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Daily at 5\n\t\t\tnow, // We should be inside the patch window\n\t\t\tnow,\n\t\t\tnow.Add(3600 * time.Second), // Todays patch window\n\t\t},\n\t\t{\n\t\t\t\"daily (today after patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Daily_{Daily: &osconfigpb.PatchWindow_Daily{}},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Daily at 5\n\t\t\tnow.Add(2 * time.Hour), // Now is after todays patch window\n\t\t\tnow.AddDate(0, 0, 1),\n\t\t\tnow.Add(3600*time.Second).AddDate(0, 0, 1), // Tomorrows patch window\n\t\t},\n\t\t{\n\t\t\t\"weekly (before this weeks patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Weekly_{\n\t\t\t\t\tWeekly: &osconfigpb.PatchWindow_Weekly{Day: dayofweek.DayOfWeek_FRIDAY},\n\t\t\t\t}, StartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration: &duration.Duration{Seconds: 3600},\n\t\t\t}, // Weekly on Friday at 5\n\t\t\tnow, // We should be before the patch window\n\t\t\ttime.Date(2018, 7, 6, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 6, 5, 0, 3600, 0, time.UTC), // This week, 6th July\n\t\t},\n\t\t{\n\t\t\t\"weekly (during this weeks patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Weekly_{\n\t\t\t\t\tWeekly: &osconfigpb.PatchWindow_Weekly{Day: dayofweek.DayOfWeek_FRIDAY},\n\t\t\t\t}, StartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration: &duration.Duration{Seconds: 3600},\n\t\t\t}, // Weekly on Friday at 5\n\t\t\tnow.AddDate(0, 0, 5), // Sunday + 5 = Friday\n\t\t\ttime.Date(2018, 7, 6, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 6, 5, 0, 3600, 0, time.UTC), // This week, 6th July\n\t\t},\n\t\t{\n\t\t\t\"weekly (after this weeks patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Weekly_{\n\t\t\t\t\tWeekly: &osconfigpb.PatchWindow_Weekly{Day: dayofweek.DayOfWeek_FRIDAY},\n\t\t\t\t}, StartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration: &duration.Duration{Seconds: 3600},\n\t\t\t}, // Weekly on Friday at 5.\n\t\t\tnow.AddDate(0, 0, 6), // Sunday + 6 = Saturday\n\t\t\ttime.Date(2018, 7, 13, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 13, 5, 0, 3600, 0, time.UTC), // Next week, 13th July\n\t\t},\n\t\t{\n\t\t\t\"monthly 5th day of the month (before this months patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{Day: &osconfigpb.PatchWindow_Monthly_DayOfMonth{DayOfMonth: 5}},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the 5th at 5.\n\t\t\tnow, // We should be before the patch window\n\t\t\ttime.Date(2018, 7, 5, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 5, 5, 0, 3600, 0, time.UTC), // This month, 5th July\n\t\t},\n\t\t{\n\t\t\t\"monthly 5th day of the month (during this months patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{Day: &osconfigpb.PatchWindow_Monthly_DayOfMonth{DayOfMonth: 5}},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the 5th at 5.\n\t\t\tnow.AddDate(0, 0, 4), // 1st + 4 = 5th\n\t\t\ttime.Date(2018, 7, 5, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 5, 5, 0, 3600, 0, time.UTC), // This month, 5th July\n\t\t},\n\t\t{\n\t\t\t\"monthly 5th day of the month (after this months patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{Day: &osconfigpb.PatchWindow_Monthly_DayOfMonth{DayOfMonth: 5}},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the 5th at 5.\n\t\t\tnow.AddDate(0, 0, 6), // 1st + 6 = 7th\n\t\t\ttime.Date(2018, 8, 5, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 8, 5, 5, 0, 3600, 0, time.UTC), // Next month, 5th Aug\n\t\t},\n\t\t{\n\t\t\t\"monthly last day of the month (before this months patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{Day: &osconfigpb.PatchWindow_Monthly_DayOfMonth{DayOfMonth: -1}},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the last day at 5.\n\t\t\tnow, // We should be before the patch window\n\t\t\ttime.Date(2018, 7, 31, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 31, 5, 0, 3600, 0, time.UTC), // This month, 31st of July\n\t\t},\n\t\t{\n\t\t\t\"monthly last day of the month (after this months patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{Day: &osconfigpb.PatchWindow_Monthly_DayOfMonth{DayOfMonth: -1}},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the last day at 5.\n\t\t\tnow.Add(5*time.Hour).AddDate(0, 0, 30),\n\t\t\ttime.Date(2018, 8, 31, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 8, 31, 5, 0, 3600, 0, time.UTC), // Next month, 31st of Aug\n\t\t},\n\t\t{\n\t\t\t\"monthly on the second Tuesday (before this weeks patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{\n\t\t\t\t\t\tDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay_{\n\t\t\t\t\t\t\tOccurrenceOfDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay{\n\t\t\t\t\t\t\t\tDay:        dayofweek.DayOfWeek_TUESDAY,\n\t\t\t\t\t\t\t\tOccurrence: 2,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the second Tuesday at 5\n\t\t\tnow, // We should be before the patch window\n\t\t\ttime.Date(2018, 7, 10, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 7, 10, 5, 0, 3600, 0, time.UTC), // This month, 10th of July\n\t\t},\n\t\t{\n\t\t\t\"monthly on the second Tuesday (after this weeks patch window)\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{\n\t\t\t\t\t\tDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay_{\n\t\t\t\t\t\t\tOccurrenceOfDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay{\n\t\t\t\t\t\t\t\tDay:        dayofweek.DayOfWeek_TUESDAY,\n\t\t\t\t\t\t\t\tOccurrence: 2,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t}, // Monthly on the second Tuesday at 5\n\t\t\tnow.AddDate(0, 0, 15), // 15 days is after this months window\n\t\t\ttime.Date(2018, 8, 14, 5, 0, 0, 0, time.UTC),\n\t\t\ttime.Date(2018, 8, 14, 5, 0, 3600, 0, time.UTC), // Next month, 14th of Aug\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tgotStart, gotEnd, err := nextWindow(tt.now, tt.pw, 0)\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%s: %v\", tt.desc, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tif tt.wantStart != gotStart {\n\t\t\tt.Errorf(\"%s start: want(%q) != got(%q)\", tt.desc, tt.wantStart, gotStart)\n\t\t}\n\t\tif tt.wantEnd != gotEnd {\n\t\t\tt.Errorf(\"%s end: want(%q) != got(%q)\", tt.desc, tt.wantEnd, gotEnd)\n\t\t}\n\t}\n}\n\nfunc TestNextWindowErrors(t *testing.T) {\n\tvar tests = []struct {\n\t\tdesc string\n\t\tpw   *osconfigpb.PatchWindow\n\t}{\n\t\t{\n\t\t\t\"no window\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"bad duration\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Daily_{},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"weekly invalid day\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Weekly_{\n\t\t\t\t\tWeekly: &osconfigpb.PatchWindow_Weekly{Day: dayofweek.DayOfWeek_DAY_OF_WEEK_UNSPECIFIED},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"monthly invalid day\",\n\t\t\t&osconfigpb.PatchWindow{\n\t\t\t\tFrequency: &osconfigpb.PatchWindow_Monthly_{\n\t\t\t\t\tMonthly: &osconfigpb.PatchWindow_Monthly{\n\t\t\t\t\t\tDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay_{\n\t\t\t\t\t\t\tOccurrenceOfDay: &osconfigpb.PatchWindow_Monthly_OccurrenceOfDay{\n\t\t\t\t\t\t\t\tDay:        dayofweek.DayOfWeek_DAY_OF_WEEK_UNSPECIFIED,\n\t\t\t\t\t\t\t\tOccurrence: 2,\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tStartTime: &timeofday.TimeOfDay{Hours: 5},\n\t\t\t\tDuration:  &duration.Duration{Seconds: 3600},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tif _, _, err := nextWindow(time.Now(), tt.pw, 0); err == nil {\n\t\t\tt.Errorf(\"%s: expected error\", tt.desc)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n", "idx": 1, "id": 7943, "msg": "no need for the named import, just \"google.golang.org/genproto/googleapis/type/date\" I'll have to go back through and rename all the pb imports to NAMEpb as is standard", "proj": "GoogleCloudPlatform-compute-image-tools", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,35 @@\n+# Copyright 2016 Google Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Module containing base class for logging transport.\"\"\"\n+\n+\n+class Transport(object):\n+    \"\"\"Base class for Google Cloud Logging handler transports.\n+\n+    Subclasses of :class:`Transport` must have constructors that accept a\n+    client and name object, and must override :meth:`send`.\n+    \"\"\"\n+\n+    def send(self, record, message):\n+        \"\"\"Transport send to be implemented by subclasses.\n+\n+        :type record: :class:`logging.LogRecord`\n+        :param record: Python log record that the handler was called with.\n+\n+        :type message: str\n+        :param message: The message from the ``LogRecord`` after being\n+                        formatted by the associated log formatters.\n+        \"\"\"\n+        raise NotImplementedError", "y": 1, "oldf": "", "idx": 1, "id": 25168, "msg": "Why don't we just pip install google-cloud-logging and use it as a dependency?", "proj": "forseti-security-forseti-security", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -574,7 +574,7 @@ void GBDT::LoadModelFromString(const std::string& model_str) {\n     Common::Atoi(Common::Split(line.c_str(), '=')[1].c_str(), &label_idx_);\n   } else {\n     Log::Fatal(\"Model file doesn't specify the label index\");\n-    return;\n+    return false;\n   }\n   // get max_feature_idx first\n   line = Common::FindFromLines(lines, \"max_feature_idx=\");", "y": 0, "oldf": "#include \"gbdt.h\"\n\n#include <LightGBM/utils/openmp_wrapper.h>\n\n#include <LightGBM/utils/common.h>\n\n#include <LightGBM/feature.h>\n#include <LightGBM/objective_function.h>\n#include <LightGBM/metric.h>\n\n#include <ctime>\n\n#include <sstream>\n#include <chrono>\n#include <string>\n#include <vector>\n#include <utility>\n\nnamespace LightGBM {\n\nGBDT::GBDT()\n  :iter_(0),\n  train_data_(nullptr),\n  object_function_(nullptr),\n  early_stopping_round_(0),\n  max_feature_idx_(0),\n  num_class_(1),\n  sigmoid_(1.0f),\n  num_iteration_for_pred_(0),\n  shrinkage_rate_(0.1f),\n  num_init_iteration_(0) {\n#pragma omp parallel\n#pragma omp master\n    {\n      num_threads_ = omp_get_num_threads();\n    }\n}\n\nGBDT::~GBDT() {\n\n}\n\nvoid GBDT::Init(const BoostingConfig* config, const Dataset* train_data, const ObjectiveFunction* object_function,\n     const std::vector<const Metric*>& training_metrics) {\n  iter_ = 0;\n  num_iteration_for_pred_ = 0;\n  max_feature_idx_ = 0;\n  num_class_ = config->num_class;\n  for (int i = 0; i < num_threads_; ++i) {\n    random_.emplace_back(config->bagging_seed + i);\n  }\n  train_data_ = nullptr;\n  gbdt_config_ = nullptr;\n  tree_learner_ = nullptr;\n  ResetTrainingData(config, train_data, object_function, training_metrics);\n}\n\nvoid GBDT::ResetTrainingData(const BoostingConfig* config, const Dataset* train_data, const ObjectiveFunction* object_function,\n  const std::vector<const Metric*>& training_metrics) {\n  auto new_config = std::unique_ptr<BoostingConfig>(new BoostingConfig(*config));\n  if (train_data_ != nullptr && !train_data_->CheckAlign(*train_data)) {\n    Log::Fatal(\"cannot reset training data, since new training data has different bin mappers\");\n  }\n  early_stopping_round_ = new_config->early_stopping_round;\n  shrinkage_rate_ = new_config->learning_rate;\n\n  object_function_ = object_function;\n\n  sigmoid_ = -1.0f;\n  if (object_function_ != nullptr\n    && std::string(object_function_->GetName()) == std::string(\"binary\")) {\n    // only binary classification need sigmoid transform\n    sigmoid_ = new_config->sigmoid;\n  }\n\n  if (train_data_ != train_data && train_data != nullptr) {\n    if (tree_learner_ == nullptr) {\n      tree_learner_ = std::unique_ptr<TreeLearner>(TreeLearner::CreateTreeLearner(new_config->tree_learner_type, &new_config->tree_config));\n    }\n    // init tree learner\n    tree_learner_->Init(train_data);\n\n    // push training metrics\n    training_metrics_.clear();\n    for (const auto& metric : training_metrics) {\n      training_metrics_.push_back(metric);\n    }\n    training_metrics_.shrink_to_fit();\n    // not same training data, need reset score and others\n    // create score tracker\n    train_score_updater_.reset(new ScoreUpdater(train_data, num_class_));\n    // update score\n    for (int i = 0; i < iter_; ++i) {\n      for (int curr_class = 0; curr_class < num_class_; ++curr_class) {\n        auto curr_tree = (i + num_init_iteration_) * num_class_ + curr_class;\n        train_score_updater_->AddScore(models_[curr_tree].get(), curr_class);\n      }\n    }\n    num_data_ = train_data->num_data();\n    // create buffer for gradients and hessians\n    if (object_function_ != nullptr) {\n      size_t total_size = static_cast<size_t>(num_data_) * num_class_;\n      gradients_.resize(total_size);\n      hessians_.resize(total_size);\n    }\n    // get max feature index\n    max_feature_idx_ = train_data->num_total_features() - 1;\n    // get label index\n    label_idx_ = train_data->label_idx();\n    // get feature names\n    feature_names_ = train_data->feature_names();\n    // get feature infos\n    feature_infos_.clear();\n    for (int i = 0; i < max_feature_idx_ + 1; ++i) {\n      int feature_idx = train_data->GetInnerFeatureIndex(i);\n      if (feature_idx < 0) { \n        feature_infos_.push_back(\"trival feature\"); \n      } else {\n        feature_infos_.push_back(train_data->FeatureAt(feature_idx)->bin_mapper()->bin_info());\n      }\n    }\n  }\n\n  if ((train_data_ != train_data && train_data != nullptr)\n    || (gbdt_config_ != nullptr && gbdt_config_->bagging_fraction != new_config->bagging_fraction)) {\n    // if need bagging, create buffer\n    if (new_config->bagging_fraction < 1.0 && new_config->bagging_freq > 0) {\n      bag_data_cnt_ =\n        static_cast<data_size_t>(new_config->bagging_fraction * num_data_);\n      bag_data_indices_.resize(num_data_);\n      tmp_indices_.resize(num_data_);\n      offsets_buf_.resize(num_threads_);\n      left_cnts_buf_.resize(num_threads_);\n      right_cnts_buf_.resize(num_threads_);\n      left_write_pos_buf_.resize(num_threads_);\n      right_write_pos_buf_.resize(num_threads_);\n    } else {\n      bag_data_cnt_ = num_data_;\n      bag_data_indices_.clear();\n      tmp_indices_.clear();\n    }\n  }\n  train_data_ = train_data;\n  if (train_data_ != nullptr) {\n    // reset config for tree learner\n    tree_learner_->ResetConfig(&new_config->tree_config);\n  }\n  gbdt_config_.reset(new_config.release());\n}\n\nvoid GBDT::AddValidDataset(const Dataset* valid_data,\n  const std::vector<const Metric*>& valid_metrics) {\n  if (!train_data_->CheckAlign(*valid_data)) {\n    Log::Fatal(\"cannot add validation data, since it has different bin mappers with training data\");\n  }\n  // for a validation dataset, we need its score and metric\n  auto new_score_updater = std::unique_ptr<ScoreUpdater>(new ScoreUpdater(valid_data, num_class_));\n  // update score\n  for (int i = 0; i < iter_; ++i) {\n    for (int curr_class = 0; curr_class < num_class_; ++curr_class) {\n      auto curr_tree = (i + num_init_iteration_) * num_class_ + curr_class;\n      new_score_updater->AddScore(models_[curr_tree].get(), curr_class);\n    }\n  }\n  valid_score_updater_.push_back(std::move(new_score_updater));\n  valid_metrics_.emplace_back();\n  if (early_stopping_round_ > 0) {\n    best_iter_.emplace_back();\n    best_score_.emplace_back();\n    best_msg_.emplace_back();\n  }\n  for (const auto& metric : valid_metrics) {\n    valid_metrics_.back().push_back(metric);\n    if (early_stopping_round_ > 0) {\n      best_iter_.back().push_back(0);\n      best_score_.back().push_back(kMinScore);\n      best_msg_.back().emplace_back();\n    }\n  }\n  valid_metrics_.back().shrink_to_fit();\n}\n\ndata_size_t GBDT::BaggingHelper(data_size_t start, data_size_t cnt, data_size_t* buffer){\n  const int tid = omp_get_thread_num();\n  data_size_t bag_data_cnt =\n    static_cast<data_size_t>(gbdt_config_->bagging_fraction * cnt);\n  data_size_t cur_left_cnt = 0;\n  data_size_t cur_right_cnt = 0;\n  // random bagging, minimal unit is one record\n  for (data_size_t i = 0; i < cnt; ++i) {\n    double prob =\n      (bag_data_cnt - cur_left_cnt) / static_cast<double>(cnt - i);\n    if (random_[tid].NextDouble() < prob) {\n      buffer[cur_left_cnt++] = start + i;\n    } else {\n      buffer[bag_data_cnt + cur_right_cnt++] = start + i;\n    }\n  }\n  CHECK(cur_left_cnt == bag_data_cnt);\n  return cur_left_cnt;\n}\n\nvoid GBDT::Bagging(int iter) {\n  // if need bagging\n  if (bag_data_cnt_ < num_data_ && iter % gbdt_config_->bagging_freq == 0) {\n    const data_size_t min_inner_size = 10000;\n    data_size_t inner_size = (num_data_ + num_threads_ - 1) / num_threads_;\n    if (inner_size < min_inner_size) { inner_size = min_inner_size; }\n\n#pragma omp parallel for schedule(static, 1)\n    for (int i = 0; i < num_threads_; ++i) {\n      left_cnts_buf_[i] = 0;\n      right_cnts_buf_[i] = 0;\n      data_size_t cur_start = i * inner_size;\n      if (cur_start > num_data_) { continue; }\n      data_size_t cur_cnt = inner_size;\n      if (cur_start + cur_cnt > num_data_) { cur_cnt = num_data_ - cur_start; }\n      data_size_t cur_left_count = BaggingHelper(cur_start, cur_cnt, tmp_indices_.data() + cur_start);\n      offsets_buf_[i] = cur_start;\n      left_cnts_buf_[i] = cur_left_count;\n      right_cnts_buf_[i] = cur_cnt - cur_left_count;\n    }\n    data_size_t left_cnt = 0;\n    left_write_pos_buf_[0] = 0;\n    right_write_pos_buf_[0] = 0;\n    for (int i = 1; i < num_threads_; ++i) {\n      left_write_pos_buf_[i] = left_write_pos_buf_[i - 1] + left_cnts_buf_[i - 1];\n      right_write_pos_buf_[i] = right_write_pos_buf_[i - 1] + right_cnts_buf_[i - 1];\n    }\n    left_cnt = left_write_pos_buf_[num_threads_ - 1] + left_cnts_buf_[num_threads_ - 1];\n\n#pragma omp parallel for schedule(static, 1)\n    for (int i = 0; i < num_threads_; ++i) {\n      if (left_cnts_buf_[i] > 0) {\n        std::memcpy(bag_data_indices_.data() + left_write_pos_buf_[i],\n          tmp_indices_.data() + offsets_buf_[i], left_cnts_buf_[i] * sizeof(data_size_t));\n      }\n      if (right_cnts_buf_[i] > 0) {\n        std::memcpy(bag_data_indices_.data() + left_cnt + right_write_pos_buf_[i],\n          tmp_indices_.data() + offsets_buf_[i] + left_cnts_buf_[i], right_cnts_buf_[i] * sizeof(data_size_t));\n      }\n    }\n    Log::Debug(\"Re-bagging, using %d data to train\", bag_data_cnt_);\n    // set bagging data to tree learner\n    tree_learner_->SetBaggingData(bag_data_indices_.data(), bag_data_cnt_);\n  }\n}\n\nvoid GBDT::UpdateScoreOutOfBag(const Tree* tree, const int curr_class) {\n  // we need to predict out-of-bag socres of data for boosting\n  if (num_data_ - bag_data_cnt_ > 0) {\n    train_score_updater_->AddScore(tree, bag_data_indices_.data() + bag_data_cnt_, num_data_ - bag_data_cnt_, curr_class);\n  }\n}\n\nbool GBDT::TrainOneIter(const score_t* gradient, const score_t* hessian, bool is_eval) {\n  // boosting first\n  if (gradient == nullptr || hessian == nullptr) {\n    Boosting();\n    gradient = gradients_.data();\n    hessian = hessians_.data();\n  }\n  // bagging logic\n  Bagging(iter_);\n  for (int curr_class = 0; curr_class < num_class_; ++curr_class) {\n\n    // train a new tree\n    std::unique_ptr<Tree> new_tree(tree_learner_->Train(gradient + curr_class * num_data_, hessian + curr_class * num_data_));\n    // if cannot learn a new tree, then stop\n    if (new_tree->num_leaves() <= 1) {\n      Log::Info(\"Stopped training because there are no more leafs that meet the split requirements.\");\n      return true;\n    }\n\n    // shrinkage by learning rate\n    new_tree->Shrinkage(shrinkage_rate_);\n    // update score\n    UpdateScore(new_tree.get(), curr_class);\n    UpdateScoreOutOfBag(new_tree.get(), curr_class);\n\n    // add model\n    models_.push_back(std::move(new_tree));\n  }\n  ++iter_;\n  if (is_eval) {\n    return EvalAndCheckEarlyStopping();\n  } else {\n    return false;\n  }\n\n}\n\nvoid GBDT::RollbackOneIter() {\n  if (iter_ <= 0) { return; }\n  int cur_iter = iter_ + num_init_iteration_ - 1;\n  // reset score\n  for (int curr_class = 0; curr_class < num_class_; ++curr_class) {\n    auto curr_tree = cur_iter * num_class_ + curr_class;\n    models_[curr_tree]->Shrinkage(-1.0);\n    train_score_updater_->AddScore(models_[curr_tree].get(), curr_class);\n    for (auto& score_updater : valid_score_updater_) {\n      score_updater->AddScore(models_[curr_tree].get(), curr_class);\n    }\n  }\n  // remove model\n  for (int curr_class = 0; curr_class < num_class_; ++curr_class) {\n    models_.pop_back();\n  }\n  --iter_;\n}\n\nbool GBDT::EvalAndCheckEarlyStopping() {\n  bool is_met_early_stopping = false;\n  // print message for metric\n  auto best_msg = OutputMetric(iter_);\n  is_met_early_stopping = !best_msg.empty();\n  if (is_met_early_stopping) {\n    Log::Info(\"Early stopping at iteration %d, the best iteration round is %d\",\n      iter_, iter_ - early_stopping_round_);\n    Log::Info(\"Output of best iteration round:\\n%s\", best_msg.c_str());\n    // pop last early_stopping_round_ models\n    for (int i = 0; i < early_stopping_round_ * num_class_; ++i) {\n      models_.pop_back();\n    }\n  }\n  return is_met_early_stopping;\n}\n\nvoid GBDT::UpdateScore(const Tree* tree, const int curr_class) {\n  // update training score\n  train_score_updater_->AddScore(tree_learner_.get(), curr_class);\n  // update validation score\n  for (auto& score_updater : valid_score_updater_) {\n    score_updater->AddScore(tree, curr_class);\n  }\n}\n\nstd::string GBDT::OutputMetric(int iter) {\n  bool need_output = (iter % gbdt_config_->output_freq) == 0;\n  std::string ret = \"\";\n  std::stringstream msg_buf;\n  std::vector<std::pair<size_t, size_t>> meet_early_stopping_pairs;\n  // print training metric\n  if (need_output) {\n    for (auto& sub_metric : training_metrics_) {\n      auto name = sub_metric->GetName();\n      auto scores = sub_metric->Eval(train_score_updater_->score());\n      for (size_t k = 0; k < name.size(); ++k) {\n        std::stringstream tmp_buf;\n        tmp_buf << \"Iteration:\" << iter\n          << \", training \" << name[k]\n          << \" : \" << scores[k];\n        Log::Info(tmp_buf.str().c_str());\n        if (early_stopping_round_ > 0) {\n          msg_buf << tmp_buf.str() << std::endl;\n        }\n      }\n    }\n  }\n  // print validation metric\n  if (need_output || early_stopping_round_ > 0) {\n    for (size_t i = 0; i < valid_metrics_.size(); ++i) {\n      for (size_t j = 0; j < valid_metrics_[i].size(); ++j) {\n        auto test_scores = valid_metrics_[i][j]->Eval(valid_score_updater_[i]->score());\n        auto name = valid_metrics_[i][j]->GetName();\n        for (size_t k = 0; k < name.size(); ++k) {\n          std::stringstream tmp_buf;\n          tmp_buf << \"Iteration:\" << iter\n            << \", valid_\" << i + 1 << \" \" << name[k]\n            << \" : \" << test_scores[k];\n          if (need_output) {\n            Log::Info(tmp_buf.str().c_str());\n          }\n          if (early_stopping_round_ > 0) {\n            msg_buf << tmp_buf.str() << std::endl;\n          }\n        }\n        if (ret.empty() && early_stopping_round_ > 0) {\n          auto cur_score = valid_metrics_[i][j]->factor_to_bigger_better() * test_scores.back();\n          if (cur_score > best_score_[i][j]) {\n            best_score_[i][j] = cur_score;\n            best_iter_[i][j] = iter;\n            meet_early_stopping_pairs.emplace_back(i, j);\n          } else {\n            if (iter - best_iter_[i][j] >= early_stopping_round_) { ret = best_msg_[i][j]; }\n          }\n        }\n      }\n    }\n  }\n  for (auto& pair : meet_early_stopping_pairs) {\n    best_msg_[pair.first][pair.second] = msg_buf.str();\n  }\n  return ret;\n}\n\n/*! \\brief Get eval result */\nstd::vector<double> GBDT::GetEvalAt(int data_idx) const {\n  CHECK(data_idx >= 0 && data_idx <= static_cast<int>(valid_score_updater_.size()));\n  std::vector<double> ret;\n  if (data_idx == 0) {\n    for (auto& sub_metric : training_metrics_) {\n      auto scores = sub_metric->Eval(train_score_updater_->score());\n      for (auto score : scores) {\n        ret.push_back(score);\n      }\n    }\n  }\n  else {\n    auto used_idx = data_idx - 1;\n    for (size_t j = 0; j < valid_metrics_[used_idx].size(); ++j) {\n      auto test_scores = valid_metrics_[used_idx][j]->Eval(valid_score_updater_[used_idx]->score());\n      for (auto score : test_scores) {\n        ret.push_back(score);\n      }\n    }\n  }\n  return ret;\n}\n\n/*! \\brief Get training scores result */\nconst double* GBDT::GetTrainingScore(int64_t* out_len) {\n  *out_len = static_cast<int64_t>(train_score_updater_->num_data()) * num_class_;\n  return train_score_updater_->score();\n}\n\nvoid GBDT::GetPredictAt(int data_idx, double* out_result, int64_t* out_len) {\n  CHECK(data_idx >= 0 && data_idx <= static_cast<int>(valid_score_updater_.size()));\n\n  const double* raw_scores = nullptr;\n  data_size_t num_data = 0;\n  if (data_idx == 0) {\n    raw_scores = GetTrainingScore(out_len);\n    num_data = train_score_updater_->num_data();\n  } else {\n    auto used_idx = data_idx - 1;\n    raw_scores = valid_score_updater_[used_idx]->score();\n    num_data = valid_score_updater_[used_idx]->num_data();\n    *out_len = static_cast<int64_t>(num_data) * num_class_;\n  }\n  if (num_class_ > 1) {\n#pragma omp parallel for schedule(static)\n    for (data_size_t i = 0; i < num_data; ++i) {\n      std::vector<double> tmp_result(num_class_);\n      for (int j = 0; j < num_class_; ++j) {\n        tmp_result[j] = raw_scores[j * num_data + i];\n      }\n      Common::Softmax(&tmp_result);\n      for (int j = 0; j < num_class_; ++j) {\n        out_result[j * num_data + i] = static_cast<double>(tmp_result[j]);\n      }\n    }\n  } else if(sigmoid_ > 0.0f){\n#pragma omp parallel for schedule(static)\n    for (data_size_t i = 0; i < num_data; ++i) {\n      out_result[i] = static_cast<double>(1.0f / (1.0f + std::exp(-2.0f * sigmoid_ * raw_scores[i])));\n    }\n  } else {\n#pragma omp parallel for schedule(static)\n    for (data_size_t i = 0; i < num_data; ++i) {\n      out_result[i] = static_cast<double>(raw_scores[i]);\n    }\n  }\n\n}\n\nvoid GBDT::Boosting() {\n  if (object_function_ == nullptr) {\n    Log::Fatal(\"No object function provided\");\n  }\n  // objective function will calculate gradients and hessians\n  int64_t num_score = 0;\n  object_function_->\n    GetGradients(GetTrainingScore(&num_score), gradients_.data(), hessians_.data());\n}\n\nstd::string GBDT::DumpModel(int num_iteration) const {\n  std::stringstream str_buf;\n\n  str_buf << \"{\";\n  str_buf << \"\\\"name\\\":\\\"\" << SubModelName() << \"\\\",\" << std::endl;\n  str_buf << \"\\\"num_class\\\":\" << num_class_ << \",\" << std::endl;\n  str_buf << \"\\\"label_index\\\":\" << label_idx_ << \",\" << std::endl;\n  str_buf << \"\\\"max_feature_idx\\\":\" << max_feature_idx_ << \",\" << std::endl;\n  str_buf << \"\\\"sigmoid\\\":\" << sigmoid_ << \",\" << std::endl;\n\n  str_buf << \"\\\"feature_names\\\":[\\\"\" \n     << Common::Join(feature_names_, \"\\\",\\\"\") << \"\\\"],\" \n     << std::endl;\n\n  str_buf << \"\\\"tree_info\\\":[\";\n  int num_used_model = static_cast<int>(models_.size());\n  if (num_iteration > 0) {\n    num_used_model = std::min(num_iteration * num_class_, num_used_model);\n  } \n  for (int i = 0; i < num_used_model; ++i) {\n    if (i > 0) {\n      str_buf << \",\";\n    }\n    str_buf << \"{\";\n    str_buf << \"\\\"tree_index\\\":\" << i << \",\";\n    str_buf << models_[i]->ToJSON();\n    str_buf << \"}\";\n  }\n  str_buf << \"]\" << std::endl;\n\n  str_buf << \"}\" << std::endl;\n\n  return str_buf.str();\n}\n\nvoid GBDT::SaveModelToFile(int num_iteration, const char* filename) const {\n  /*! \\brief File to write models */\n  std::ofstream output_file;\n  output_file.open(filename);\n  // output model type\n  output_file << SubModelName() << std::endl;\n  // output number of class\n  output_file << \"num_class=\" << num_class_ << std::endl;\n  // output label index\n  output_file << \"label_index=\" << label_idx_ << std::endl;\n  // output max_feature_idx\n  output_file << \"max_feature_idx=\" << max_feature_idx_ << std::endl;\n  // output objective name\n  if (object_function_ != nullptr) {\n    output_file << \"objective=\" << object_function_->GetName() << std::endl;\n  }\n  // output sigmoid parameter\n  output_file << \"sigmoid=\" << sigmoid_ << std::endl;\n\n  output_file << \"feature_names=\" << Common::Join(feature_names_, \" \") << std::endl;\n\n  output_file << std::endl;\n  int num_used_model = static_cast<int>(models_.size());\n  if (num_iteration > 0) {\n    num_used_model = std::min(num_iteration * num_class_, num_used_model);\n  }\n  // output tree models\n  for (int i = 0; i < num_used_model; ++i) {\n    output_file << \"Tree=\" << i << std::endl;\n    output_file << models_[i]->ToString() << std::endl;\n  }\n\n  std::vector<std::pair<size_t, std::string>> pairs = FeatureImportance();\n  output_file << std::endl << \"feature importances:\" << std::endl;\n  for (size_t i = 0; i < pairs.size(); ++i) {\n    output_file << pairs[i].second << \"=\" << std::to_string(pairs[i].first) << std::endl;\n  }\n\n  output_file << std::endl << \"feature information:\" << std::endl;\n  for (size_t i = 0; i < max_feature_idx_ + 1; ++i) {\n    output_file << feature_names_[i] << \"=\" << feature_infos_[i] << std::endl;\n  }\n\n  output_file.close();\n}\n\nvoid GBDT::LoadModelFromString(const std::string& model_str) {\n  // use serialized string to restore this object\n  models_.clear();\n  std::vector<std::string> lines = Common::Split(model_str.c_str(), '\\n');\n\n  // get number of classes\n  auto line = Common::FindFromLines(lines, \"num_class=\");\n  if (line.size() > 0) {\n    Common::Atoi(Common::Split(line.c_str(), '=')[1].c_str(), &num_class_);\n  } else {\n    Log::Fatal(\"Model file doesn't specify the number of classes\");\n    return;\n  }\n  // get index of label\n  line = Common::FindFromLines(lines, \"label_index=\");\n  if (line.size() > 0) {\n    Common::Atoi(Common::Split(line.c_str(), '=')[1].c_str(), &label_idx_);\n  } else {\n    Log::Fatal(\"Model file doesn't specify the label index\");\n    return;\n  }\n  // get max_feature_idx first\n  line = Common::FindFromLines(lines, \"max_feature_idx=\");\n  if (line.size() > 0) {\n    Common::Atoi(Common::Split(line.c_str(), '=')[1].c_str(), &max_feature_idx_);\n  } else {\n    Log::Fatal(\"Model file doesn't specify max_feature_idx\");\n    return;\n  }\n  // get sigmoid parameter\n  line = Common::FindFromLines(lines, \"sigmoid=\");\n  if (line.size() > 0) {\n    Common::Atof(Common::Split(line.c_str(), '=')[1].c_str(), &sigmoid_);\n  } else {\n    sigmoid_ = -1.0f;\n  }\n  // get feature names\n  line = Common::FindFromLines(lines, \"feature_names=\");\n  if (line.size() > 0) {\n    feature_names_ = Common::Split(line.substr(std::strlen(\"feature_names=\")).c_str(), \" \");\n    if (feature_names_.size() != static_cast<size_t>(max_feature_idx_ + 1)) {\n      Log::Fatal(\"Wrong size of feature_names\");\n      return;\n    }\n  } else {\n    Log::Fatal(\"Model file doesn't contain feature names\");\n    return;\n  }\n\n  // get tree models\n  size_t i = 0;\n  while (i < lines.size()) {\n    size_t find_pos = lines[i].find(\"Tree=\");\n    if (find_pos != std::string::npos) {\n      ++i;\n      int start = static_cast<int>(i);\n      while (i < lines.size() && lines[i].find(\"Tree=\") == std::string::npos) { ++i; }\n      int end = static_cast<int>(i);\n      std::string tree_str = Common::Join<std::string>(lines, start, end, \"\\n\");\n      auto new_tree = std::unique_ptr<Tree>(new Tree(tree_str));\n      models_.push_back(std::move(new_tree));\n    } else {\n      ++i;\n    }\n  }\n  Log::Info(\"Finished loading %d models\", models_.size());\n  num_iteration_for_pred_ = static_cast<int>(models_.size()) / num_class_;\n  num_init_iteration_ = num_iteration_for_pred_;\n  iter_ = 0;\n}\n\nstd::vector<std::pair<size_t, std::string>> GBDT::FeatureImportance() const {\n\n  std::vector<size_t> feature_importances(max_feature_idx_ + 1, 0);\n    for (size_t iter = 0; iter < models_.size(); ++iter) {\n        for (int split_idx = 0; split_idx < models_[iter]->num_leaves() - 1; ++split_idx) {\n            ++feature_importances[models_[iter]->split_feature_real(split_idx)];\n        }\n    }\n    // store the importance first\n    std::vector<std::pair<size_t, std::string>> pairs;\n    for (size_t i = 0; i < feature_importances.size(); ++i) {\n      if (feature_importances[i] > 0) {\n        pairs.emplace_back(feature_importances[i], feature_names_[i]);\n      }\n    }\n    // sort the importance\n    std::sort(pairs.begin(), pairs.end(),\n      [](const std::pair<size_t, std::string>& lhs,\n        const std::pair<size_t, std::string>& rhs) {\n      return lhs.first > rhs.first;\n    });\n    return pairs;\n}\n\nstd::vector<double> GBDT::PredictRaw(const double* value) const {\n  std::vector<double> ret(num_class_, 0.0f);\n  for (int i = 0; i < num_iteration_for_pred_; ++i) {\n    for (int j = 0; j < num_class_; ++j) {\n      ret[j] += models_[i * num_class_ + j]->Predict(value);\n    }\n  }\n  return ret;\n}\n\nstd::vector<double> GBDT::Predict(const double* value) const {\n  std::vector<double> ret(num_class_, 0.0f);\n  for (int i = 0; i < num_iteration_for_pred_; ++i) {\n    for (int j = 0; j < num_class_; ++j) {\n      ret[j] += models_[i * num_class_ + j]->Predict(value);\n    }\n  }\n  // if need sigmoid transform\n  if (sigmoid_ > 0 && num_class_ == 1) {\n    ret[0] = 1.0f / (1.0f + std::exp(- 2.0f * sigmoid_ * ret[0]));\n  } else if (num_class_ > 1) {\n    Common::Softmax(&ret);\n  }\n  return ret;\n}\n\nstd::vector<int> GBDT::PredictLeafIndex(const double* value) const {\n  std::vector<int> ret;\n  for (int i = 0; i < num_iteration_for_pred_; ++i) {\n    for (int j = 0; j < num_class_; ++j) {\n      ret.push_back(models_[i * num_class_ + j]->PredictLeafIndex(value));\n    }\n  }\n  return ret;\n}\n\n}  // namespace LightGBM\n", "idx": 4, "id": 16091, "msg": "", "proj": "microsoft-LightGBM", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -72,7 +72,8 @@ func Example_withTLS() {\n \t\tlog.Fatalf(\"failed to create gRPC client TLS credentials: %v\", err)\n \t}\n \n-\texp, err := otlp.NewExporter(otlp.WithTLSCredentials(creds))\n+\texp, err := otlp.NewExporter(otlp.EmptyConfiguration,\n+\t\totlp.NewConnectionConfig(otlp.WithTLSCredentials(creds)))\n \tif err != nil {\n \t\tlog.Fatalf(\"failed to create the collector exporter: %v\", err)\n \t}", "y": 0, "oldf": "// Copyright The OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage otlp_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"time\"\n\n\t\"google.golang.org/grpc/credentials\"\n\n\t\"go.opentelemetry.io/otel/api/global\"\n\t\"go.opentelemetry.io/otel/exporters/otlp\"\n\tsdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n)\n\nfunc Example_insecure() {\n\texp, err := otlp.NewExporter(otlp.WithInsecure())\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to create the collector exporter: %v\", err)\n\t}\n\tdefer func() {\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tif err := exp.Shutdown(ctx); err != nil {\n\t\t\tglobal.Handle(err)\n\t\t}\n\t}()\n\n\ttp := sdktrace.NewProvider(\n\t\tsdktrace.WithConfig(sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}),\n\t\tsdktrace.WithBatcher(\n\t\t\texp,\n\t\t\t// add following two options to ensure flush\n\t\t\tsdktrace.WithBatchTimeout(5),\n\t\t\tsdktrace.WithMaxExportBatchSize(10),\n\t\t),\n\t)\n\tglobal.SetTracerProvider(tp)\n\n\ttracer := global.Tracer(\"test-tracer\")\n\n\t// Then use the OpenTelemetry tracing library, like we normally would.\n\tctx, span := tracer.Start(context.Background(), \"CollectorExporter-Example\")\n\tdefer span.End()\n\n\tfor i := 0; i < 10; i++ {\n\t\t_, iSpan := tracer.Start(ctx, fmt.Sprintf(\"Sample-%d\", i))\n\t\t<-time.After(6 * time.Millisecond)\n\t\tiSpan.End()\n\t}\n}\n\nfunc Example_withTLS() {\n\t// Please take at look at https://pkg.go.dev/google.golang.org/grpc/credentials#TransportCredentials\n\t// for ways on how to initialize gRPC TransportCredentials.\n\tcreds, err := credentials.NewClientTLSFromFile(\"my-cert.pem\", \"\")\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create gRPC client TLS credentials: %v\", err)\n\t}\n\n\texp, err := otlp.NewExporter(otlp.WithTLSCredentials(creds))\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create the collector exporter: %v\", err)\n\t}\n\tdefer func() {\n\t\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\n\t\tdefer cancel()\n\t\tif err := exp.Shutdown(ctx); err != nil {\n\t\t\tglobal.Handle(err)\n\t\t}\n\t}()\n\n\ttp := sdktrace.NewProvider(\n\t\tsdktrace.WithConfig(sdktrace.Config{DefaultSampler: sdktrace.AlwaysSample()}),\n\t\tsdktrace.WithBatcher(\n\t\t\texp,\n\t\t\t// add following two options to ensure flush\n\t\t\tsdktrace.WithBatchTimeout(5),\n\t\t\tsdktrace.WithMaxExportBatchSize(10),\n\t\t),\n\t)\n\tglobal.SetTracerProvider(tp)\n\n\ttracer := global.Tracer(\"test-tracer\")\n\n\t// Then use the OpenTelemetry tracing library, like we normally would.\n\tctx, span := tracer.Start(context.Background(), \"Securely-Talking-To-Collector-Span\")\n\tdefer span.End()\n\n\tfor i := 0; i < 10; i++ {\n\t\t_, iSpan := tracer.Start(ctx, fmt.Sprintf(\"Sample-%d\", i))\n\t\t<-time.After(6 * time.Millisecond)\n\t\tiSpan.End()\n\t}\n}\n", "idx": 2, "id": 13371, "msg": "", "proj": "open-telemetry-opentelemetry-go", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -21,6 +21,7 @@ import android.provider.MediaStore;\n import android.support.annotation.NonNull;\n import android.support.design.widget.AppBarLayout;\n import android.support.design.widget.BottomNavigationView;\n+import android.support.design.widget.CoordinatorLayout;\n import android.support.v4.content.ContextCompat;\n import android.support.v4.view.GravityCompat;\n import android.support.v4.widget.DrawerLayout;", "y": 0, "oldf": "package org.fossasia.phimpme.gallery.activities;\n\nimport android.annotation.TargetApi;\nimport android.content.ContentResolver;\nimport android.content.ContentUris;\nimport android.content.DialogInterface;\nimport android.content.Intent;\nimport android.content.res.Configuration;\nimport android.database.Cursor;\nimport android.graphics.Bitmap;\nimport android.graphics.BitmapFactory;\nimport android.graphics.PorterDuff;\nimport android.graphics.PorterDuffColorFilter;\nimport android.graphics.drawable.Drawable;\nimport android.net.Uri;\nimport android.os.AsyncTask;\nimport android.os.Build;\nimport android.os.Bundle;\nimport android.os.Handler;\nimport android.provider.MediaStore;\nimport android.support.annotation.NonNull;\nimport android.support.design.widget.AppBarLayout;\nimport android.support.design.widget.BottomNavigationView;\nimport android.support.v4.content.ContextCompat;\nimport android.support.v4.view.GravityCompat;\nimport android.support.v4.widget.DrawerLayout;\nimport android.support.v4.widget.SwipeRefreshLayout;\nimport android.support.v7.app.ActionBarDrawerToggle;\nimport android.support.v7.app.AlertDialog;\nimport android.support.v7.widget.CardView;\nimport android.support.v7.widget.DefaultItemAnimator;\nimport android.support.v7.widget.GridLayoutManager;\nimport android.support.v7.widget.RecyclerView;\nimport android.support.v7.widget.SwitchCompat;\nimport android.support.v7.widget.Toolbar;\nimport android.text.Html;\nimport android.util.Log;\nimport android.view.Menu;\nimport android.view.MenuItem;\nimport android.view.View;\nimport android.view.WindowManager;\nimport android.webkit.MimeTypeMap;\nimport android.widget.CompoundButton;\nimport android.widget.EditText;\nimport android.widget.RadioButton;\nimport android.widget.RadioGroup;\nimport android.widget.ScrollView;\nimport android.widget.SeekBar;\nimport android.widget.Spinner;\nimport android.widget.TextView;\nimport android.widget.Toast;\n\nimport com.mikepenz.google_material_typeface_library.GoogleMaterial;\nimport com.mikepenz.iconics.view.IconicsImageView;\n\nimport org.fossasia.phimpme.R;\nimport org.fossasia.phimpme.base.SharedMediaActivity;\nimport org.fossasia.phimpme.gallery.SelectAlbumBottomSheet;\nimport org.fossasia.phimpme.gallery.adapters.AlbumsAdapter;\nimport org.fossasia.phimpme.gallery.adapters.MediaAdapter;\nimport org.fossasia.phimpme.gallery.data.Album;\nimport org.fossasia.phimpme.gallery.data.CustomAlbumsHelper;\nimport org.fossasia.phimpme.gallery.data.HandlingAlbums;\nimport org.fossasia.phimpme.gallery.data.Media;\nimport org.fossasia.phimpme.gallery.data.base.MediaComparators;\nimport org.fossasia.phimpme.gallery.data.base.SortingOrder;\nimport org.fossasia.phimpme.gallery.data.providers.StorageProvider;\nimport org.fossasia.phimpme.gallery.util.Affix;\nimport org.fossasia.phimpme.gallery.util.AlertDialogsHelper;\nimport org.fossasia.phimpme.gallery.util.ContentHelper;\nimport org.fossasia.phimpme.gallery.util.Measure;\nimport org.fossasia.phimpme.gallery.util.PreferenceUtil;\nimport org.fossasia.phimpme.gallery.util.SecurityHelper;\nimport org.fossasia.phimpme.gallery.util.StringUtils;\nimport org.fossasia.phimpme.gallery.views.GridSpacingItemDecoration;\nimport org.fossasia.phimpme.uploadhistory.UploadHistory;\nimport org.fossasia.phimpme.utilities.ActivitySwitchHelper;\nimport org.fossasia.phimpme.utilities.SnackBarHandler;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.Locale;\n\nimport static org.fossasia.phimpme.gallery.data.base.SortingMode.DATE;\nimport static org.fossasia.phimpme.gallery.data.base.SortingMode.NAME;\nimport static org.fossasia.phimpme.gallery.data.base.SortingMode.NUMERIC;\nimport static org.fossasia.phimpme.gallery.data.base.SortingMode.SIZE;\n\n\npublic class LFMainActivity extends SharedMediaActivity {\n\n    private static String TAG = \"AlbumsAct\";\n    private int REQUEST_CODE_SD_CARD_PERMISSIONS = 42;\n    private boolean about=false,settings=false,uploadHistory=false;\n    private CustomAlbumsHelper customAlbumsHelper = CustomAlbumsHelper.getInstance(LFMainActivity.this);\n    private PreferenceUtil SP;\n    private SecurityHelper securityObj;\n\n    private RecyclerView rvAlbums;\n    private AlbumsAdapter albumsAdapter;\n    private GridSpacingItemDecoration rvAlbumsDecoration;\n\n    private RecyclerView rvMedia;\n    private MediaAdapter mediaAdapter;\n    private GridSpacingItemDecoration rvMediaDecoration;\n\n    private DrawerLayout mDrawerLayout;\n    private Toolbar toolbar;\n    private SelectAlbumBottomSheet bottomSheetDialogFragment;\n    private SwipeRefreshLayout swipeRefreshLayout;\n    private boolean hidden = false, pickMode = false, editMode = false, albumsMode = true, firstLaunch = true,localFolder=true;\n\n    //To handle all photos/Album conditions\n    public boolean all_photos = false;\n    final String REVIEW_ACTION = \"com.android.camera.action.REVIEW\";\n    public static ArrayList<Media> listAll;\n    public int size;\n    public int pos;\n    private ArrayList<Media> media;\n    private ArrayList<Media> selectedMedias = new ArrayList<>();\n    public boolean visible;\n\n    // To handle back pressed\n    boolean doubleBackToExitPressedOnce = false;\n\n    /*\n    editMode-  When true, user can select items by clicking on them one by one\n     */\n\n    /**\n     * Handles long clicks on photos.\n     * If first long click on photo (editMode = false), go into selection mode and set editMode = true.\n     * If not first long click, means that already in selection mode- s0 select all photos upto chosen one.\n     */\n    private View.OnLongClickListener photosOnLongClickListener = new View.OnLongClickListener() {\n        @Override\n        public boolean onLongClick(View v) {\n            Media m = (Media) v.findViewById(R.id.photo_path).getTag();\n            //If first long press, turn on selection mode\n            if (!all_photos) {\n                appBarOverlay();\n                if (!editMode) {\n                    mediaAdapter.notifyItemChanged(getAlbum().toggleSelectPhoto(m));\n                    editMode = true;\n                } else getAlbum().selectAllPhotosUpTo(getAlbum().getIndex(m), mediaAdapter);\n\n                invalidateOptionsMenu();\n            } else if (!editMode) {\n                mediaAdapter.notifyItemChanged(toggleSelectPhoto(m));\n                editMode = true;\n            } else selectAllPhotosUpTo(getImagePosition(m.getPath()), mediaAdapter);\n            return true;\n        }\n    };\n\n\n    private int toggleSelectPhoto(Media m) {\n        if (m != null) {\n            m.setSelected(!m.isSelected());\n            if (m.isSelected())\n                selectedMedias.add(m);\n            else\n                selectedMedias.remove(m);\n        }\n        if (selectedMedias.size() == 0) {\n            editMode = false;\n            toolbar.setTitle(getString(R.string.all));\n        } else {\n            toolbar.setTitle(selectedMedias.size() + \"/\" + size);\n        }\n        invalidateOptionsMenu();\n        return getImagePosition(m.getPath());\n    }\n\n    public void clearSelectedPhotos() {\n        for (Media m : selectedMedias)\n            m.setSelected(false);\n        if (selectedMedias != null)\n            selectedMedias.clear();\n        if(localFolder) toolbar.setTitle(getString(R.string.local_folder));\n        else toolbar.setTitle(getString(R.string.hidden_folder));\n    }\n\n\n    public void selectAllPhotos() {\n        for (Media m : listAll) {\n            m.setSelected(true);\n            selectedMedias.add(m);\n        }\n        toolbar.setTitle(selectedMedias.size() + \"/\" + size);\n    }\n\n\n    public void selectAllPhotosUpTo(int targetIndex, MediaAdapter adapter) {\n        int indexRightBeforeOrAfter = -1;\n        int indexNow;\n        for (Media sm : selectedMedias) {\n            indexNow = getImagePosition(sm.getPath());\n            if (indexRightBeforeOrAfter == -1) indexRightBeforeOrAfter = indexNow;\n\n            if (indexNow > targetIndex) break;\n            indexRightBeforeOrAfter = indexNow;\n        }\n\n        if (indexRightBeforeOrAfter != -1) {\n            for (int index = Math.min(targetIndex, indexRightBeforeOrAfter); index <= Math.max(targetIndex, indexRightBeforeOrAfter); index++) {\n                if (listAll.get(index) != null && !listAll.get(index).isSelected()) {\n                    listAll.get(index).setSelected(true);\n                    selectedMedias.add(listAll.get(index));\n                    adapter.notifyItemChanged(index);\n                }\n            }\n        }\n        toolbar.setTitle(selectedMedias.size() + \"/\" + size);\n    }\n\n    /**\n     * Handles short clicks on photos.\n     * If in selection mode (editMode = true) , select the photo if it is unselected and unselect it if it's selected.\n     * This mechanism makes it possible to select photos one by one by short-clicking on them.\n     * If not in selection mode (editMode = false) , get current photo from album and open it in singleActivity\n     */\n    private View.OnClickListener photosOnClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            Media m = (Media) v.findViewById(R.id.photo_path).getTag();\n            if (all_photos) {\n                pos = getImagePosition(m.getPath());\n            }\n            if (!all_photos) {\n                if (!pickMode) {\n                    //if in selection mode, toggle the selected/unselect state of photo\n                    if (editMode) {\n                        appBarOverlay();\n                        mediaAdapter.notifyItemChanged(getAlbum().toggleSelectPhoto(m));\n                        invalidateOptionsMenu();\n                    } else {\n                        getAlbum().setCurrentPhotoIndex(m);\n                        Intent intent = new Intent(LFMainActivity.this, SingleMediaActivity.class);\n                        intent.putExtra(\"path\", Uri.fromFile(new File(m.getPath())).toString());\n                        intent.setAction(SingleMediaActivity.ACTION_OPEN_ALBUM);\n                        startActivity(intent);\n                    }\n                } else {\n                    setResult(RESULT_OK, new Intent().setData(m.getUri()));\n                    finish();\n                }\n            } else {\n                if (!editMode) {\n                    Intent intent = new Intent(REVIEW_ACTION, Uri.fromFile(new File(m.getPath())));\n                    intent.putExtra(getString(R.string.all_photo_mode), true);\n                    intent.putExtra(getString(R.string.position), pos);\n                    intent.putExtra(getString(R.string.allMediaSize), size);\n                    intent.setClass(getApplicationContext(), SingleMediaActivity.class);\n                    startActivity(intent);\n                } else {\n                    mediaAdapter.notifyItemChanged(toggleSelectPhoto(m));\n                }\n\n            }\n        }\n    };\n\n    private View.OnLongClickListener albumOnLongCLickListener = new View.OnLongClickListener() {\n        @Override\n        public boolean onLongClick(View v) {\n            albumsAdapter.notifyItemChanged(getAlbums().toggleSelectAlbum(((Album) v.findViewById(R.id.album_name).getTag())));\n            editMode = true;\n            invalidateOptionsMenu();\n            return true;\n        }\n    };\n\n    private View.OnClickListener albumOnClickListener = new View.OnClickListener() {\n        @Override\n        public void onClick(View v) {\n            Album album = (Album) v.findViewById(R.id.album_name).getTag();\n            //int index = Integer.parseInt(v.findViewById(R.id.album_name).getTag().toString());\n            if (editMode) {\n                albumsAdapter.notifyItemChanged(getAlbums().toggleSelectAlbum(album));\n                invalidateOptionsMenu();\n            } else {\n                getAlbums().setCurrentAlbum(album);\n                displayCurrentAlbumMedia(true);\n                setRecentApp(getAlbums().getCurrentAlbum().getName());\n            }\n        }\n    };\n\n    /**\n     *  Method for clearing the scroll flags.\n     */\n    private void appBarOverlay(){\n        Toolbar toolbar = (Toolbar)findViewById(R.id.toolbar);\n        AppBarLayout.LayoutParams params = (AppBarLayout.LayoutParams) toolbar.getLayoutParams();\n        params.setScrollFlags(AppBarLayout.LayoutParams.SCROLL_FLAG_EXIT_UNTIL_COLLAPSED);  // clear all scroll flags\n    }\n\n    /**\n     * Method for adding the scroll flags.\n     */\n    private void clearOverlay(){\n        Toolbar toolbar = (Toolbar)findViewById(R.id.toolbar);\n        AppBarLayout.LayoutParams params = (AppBarLayout.LayoutParams) toolbar.getLayoutParams();\n        params.setScrollFlags(AppBarLayout.LayoutParams.SCROLL_FLAG_SCROLL\n                | AppBarLayout.LayoutParams.SCROLL_FLAG_ENTER_ALWAYS);\n    }\n\n    public int getImagePosition(String path) {\n        int pos = 0;\n        for (int i = 0; i < listAll.size(); i++) {\n            if (listAll.get(i).getPath().equals(path)) {\n                pos = i;\n                break;\n            }\n        }\n        return pos;\n    }\n\n    @Override\n    public void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        Log.e(\"TAG\", \"lfmain\");\n\n\n        BottomNavigationView navigationView = (BottomNavigationView)findViewById(R.id.bottombar);\n\n        SP = PreferenceUtil.getInstance(getApplicationContext());\n        albumsMode = true;\n        editMode = false;\n        securityObj = new SecurityHelper(LFMainActivity.this);\n        if (getIntent().getExtras() != null)\n            pickMode = getIntent().getExtras().getBoolean(SplashScreen.PICK_MODE);\n        SP.putBoolean(getString(R.string.preference_use_alternative_provider), false);\n        initUI();\n        new initAllPhotos().execute();\n        displayData(getIntent().getExtras());\n        checkNothing();\n\n        navigationView.setOnNavigationItemSelectedListener(new BottomNavigationView.OnNavigationItemSelectedListener() {\n            @Override\n            public boolean onNavigationItemSelected(@NonNull MenuItem item) {\n                int itemID = item.getItemId();\n                if(itemID==R.id.navigation_home){\n                    if(!localFolder){\n                        hidden = false;\n                        localFolder = true;\n                    }\n                    displayAlbums();\n                    return true;\n                }\n                return LFMainActivity.super.onNavigationItemSelected(item);\n            }\n        });\n\n    }\n\n    @Override\n    public void onResume() {\n        super.onResume();\n        ActivitySwitchHelper.setContext(this);\n        securityObj.updateSecuritySetting();\n        setupUI();\n        getAlbums().clearSelectedAlbums();\n        getAlbum().clearSelectedPhotos();\n        if (all_photos)\n            mediaAdapter.swapDataSet(listAll);\n        if (!all_photos) {\n            if (SP.getBoolean(\"auto_update_media\", false)) {\n                if (albumsMode) {\n                    if (!firstLaunch) new PrepareAlbumTask().execute();\n                } else new PreparePhotosTask().execute();\n            } else {\n                albumsAdapter.notifyDataSetChanged();\n                mediaAdapter.notifyDataSetChanged();\n            }\n        }\n        invalidateOptionsMenu();\n        firstLaunch = false;\n    }\n\n    private void displayCurrentAlbumMedia(boolean reload) {\n        toolbar.setTitle(getAlbum().getName());\n        toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_arrow_back));\n        mDrawerLayout.setDrawerLockMode(DrawerLayout.LOCK_MODE_LOCKED_CLOSED);\n        mediaAdapter.swapDataSet(getAlbum().getMedia());\n        if (reload) new PreparePhotosTask().execute();\n        toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                displayAlbums();\n            }\n        });\n        albumsMode = editMode = false;\n        invalidateOptionsMenu();\n    }\n\n    private void displayAllMedia(boolean reload) {\n        clearSelectedPhotos();\n        toolbar.setTitle(getString(R.string.all_media));\n        toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_arrow_back));\n        mDrawerLayout.setDrawerLockMode(DrawerLayout.LOCK_MODE_LOCKED_CLOSED);\n        mediaAdapter.swapDataSet(listAll);\n        if (reload) new PrepareAllPhotos().execute();\n        toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                displayAlbums();\n            }\n        });\n        albumsMode = editMode = false;\n        invalidateOptionsMenu();\n    }\n\n    private void displayAlbums() {\n        all_photos = false;\n        displayAlbums(true);\n    }\n\n    private void displayAlbums(boolean reload) {\n        if(localFolder) {\n            toolbar.setTitle(getString(R.string.local_folder));\n        }\n        else{\n            toolbar.setTitle(getString(R.string.hidden_folder));\n        }\n        toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_menu));\n        mDrawerLayout.setDrawerLockMode(DrawerLayout.LOCK_MODE_UNLOCKED);\n        albumsAdapter.swapDataSet(getAlbums().dispAlbums);\n        if (reload) new PrepareAlbumTask().execute();\n        toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                mDrawerLayout.openDrawer(GravityCompat.START);\n            }\n        });\n\n        albumsMode = true;\n        editMode = false;\n        invalidateOptionsMenu();\n        mediaAdapter.swapDataSet(new ArrayList<Media>());\n        rvMedia.scrollToPosition(0);\n    }\n\n\n    @Override\n    public void onConfigurationChanged(Configuration newConfig) {\n        super.onConfigurationChanged(newConfig);\n    }\n\n    private boolean displayData(Bundle data) {\n        if (data != null) {\n            switch (data.getInt(SplashScreen.CONTENT)) {\n                case SplashScreen.ALBUMS_PREFETCHED:\n                    displayAlbums(false);\n                    // we pass the albumMode here . If true, show rvAlbum recyclerView. If false, show rvMedia recyclerView\n                    toggleRecyclersVisibility(true);\n                    return true;\n\n                case SplashScreen.ALBUMS_BACKUP:\n                    displayAlbums(true);\n                    // we pass the albumMode here . If true, show rvAlbum recyclerView. If false, show rvMedia recyclerView\n                    toggleRecyclersVisibility(true);\n                    return true;\n\n                case SplashScreen.PHOTOS_PREFETCHED:\n                    //TODO ask password if hidden\n                    new Thread(new Runnable() {\n                        @Override\n                        public void run() {\n                            getAlbums().loadAlbums(getApplicationContext(), getAlbum().isHidden());\n                        }\n                    }).start();\n                    displayCurrentAlbumMedia(false);\n\n                    // we pass the albumMode here . If true, show rvAlbum recyclerView. If false, show rvMedia recyclerView\n                    toggleRecyclersVisibility(false);\n                    return true;\n            }\n        }\n\n        displayAlbums(true);\n        return false;\n    }\n\n    private class initAllPhotos extends AsyncTask<Void, Void, Void> {\n        @Override\n        protected Void doInBackground(Void... arg0) {\n            listAll = StorageProvider.getAllShownImages(LFMainActivity.this);\n            size = listAll.size();\n            media = listAll;\n            Collections.sort(listAll, MediaComparators.getComparator(getAlbum().settings.getSortingMode(), getAlbum().settings.getSortingOrder()));\n            return null;\n        }\n    }\n\n    private void initUI() {\n        clearOverlay();\n        /**** TOOLBAR ****/\n        toolbar = (Toolbar) findViewById(R.id.toolbar);\n        setSupportActionBar(toolbar);\n\n        /**** RECYCLER VIEW ****/\n        rvAlbums = (RecyclerView) findViewById(R.id.grid_albums);\n        rvMedia = ((RecyclerView) findViewById(R.id.grid_photos));\n        rvAlbums.setHasFixedSize(true);\n        rvAlbums.setItemAnimator(new DefaultItemAnimator());\n        rvMedia.setHasFixedSize(true);\n        rvMedia.setItemAnimator(new DefaultItemAnimator());\n\n\n        albumsAdapter = new AlbumsAdapter(getAlbums().dispAlbums, LFMainActivity.this);\n\n        albumsAdapter.setOnClickListener(albumOnClickListener);\n        albumsAdapter.setOnLongClickListener(albumOnLongCLickListener);\n        rvAlbums.setAdapter(albumsAdapter);\n\n        mediaAdapter = new MediaAdapter(getAlbum().getMedia(), LFMainActivity.this);\n\n        mediaAdapter.setOnClickListener(photosOnClickListener);\n        mediaAdapter.setOnLongClickListener(photosOnLongClickListener);\n        rvMedia.setAdapter(mediaAdapter);\n\n        int spanCount = SP.getInt(\"n_columns_folders\", 2);\n        rvAlbumsDecoration = new GridSpacingItemDecoration(spanCount, Measure.pxToDp(3, getApplicationContext()), true);\n        rvAlbums.addItemDecoration(rvAlbumsDecoration);\n        rvAlbums.setLayoutManager(new GridLayoutManager(this, spanCount));\n\n        spanCount = SP.getInt(\"n_columns_media\", 3);\n        rvMediaDecoration = new GridSpacingItemDecoration(spanCount, Measure.pxToDp(3, getApplicationContext()), true);\n        rvMedia.setLayoutManager(new GridLayoutManager(getApplicationContext(), spanCount));\n        rvMedia.addItemDecoration(rvMediaDecoration);\n\n\n        /**** SWIPE TO REFRESH ****/\n        swipeRefreshLayout = (SwipeRefreshLayout) findViewById(R.id.swipeRefreshLayout);\n        swipeRefreshLayout.setColorSchemeColors(getAccentColor());\n        swipeRefreshLayout.setProgressBackgroundColorSchemeColor(getBackgroundColor());\n        swipeRefreshLayout.setOnRefreshListener(new SwipeRefreshLayout.OnRefreshListener() {\n            @Override\n            public void onRefresh() {\n                if (albumsMode) {\n                    getAlbums().clearSelectedAlbums();\n                    new PrepareAlbumTask().execute();\n                } else {\n                    if (!all_photos) {\n                        getAlbum().clearSelectedPhotos();\n                        new PreparePhotosTask().execute();\n                    } else {\n                        new PrepareAllPhotos().execute();\n                    }\n                }\n            }\n        });\n\n        /**** DRAWER ****/\n        mDrawerLayout = (DrawerLayout) findViewById(R.id.drawer_layout);\n        mDrawerLayout.addDrawerListener(new ActionBarDrawerToggle(this,\n                mDrawerLayout, toolbar, R.string.drawer_open, R.string.drawer_close) {\n            public void onDrawerClosed(View view) {\n                //Put your code here\n                // materialMenu.animateIconState(MaterialMenuDrawable.IconState.BURGER);\n                Intent intent=null;\n                if(settings){\n                    intent = new Intent(LFMainActivity.this, SettingsActivity.class);\n                    startActivity(intent);\n                    settings=false;\n                } else if(about){\n                    intent = new Intent(LFMainActivity.this, AboutActivity.class);\n                    startActivity(intent);\n                    about=false;\n                } else if(uploadHistory){\n                    intent = new Intent(LFMainActivity.this, UploadHistory.class);\n                    startActivity(intent);\n                    uploadHistory=false;\n                }\n\n            }\n\n            public void onDrawerOpened(View drawerView) {\n                //Put your code here\n                //materialMenu.animateIconState(MaterialMenuDrawable.IconState.ARROW);\n            }\n        });\n\n        setRecentApp(getString(R.string.app_name));\n        setupUI();\n        if (pickMode) {\n            hideNavigationBar();\n            swipeRefreshLayout.setPadding(0, 0, 0, 0);\n        }\n    }\n\n    private void updateColumnsRvs() {\n        updateColumnsRvAlbums();\n        updateColumnsRvMedia();\n    }\n\n    private void updateColumnsRvAlbums() {\n        int spanCount = SP.getInt(\"n_columns_folders\", 2);\n        if (spanCount != ((GridLayoutManager) rvAlbums.getLayoutManager()).getSpanCount()) {\n            rvAlbums.removeItemDecoration(rvAlbumsDecoration);\n            rvAlbumsDecoration = new GridSpacingItemDecoration(spanCount, Measure.pxToDp(3, getApplicationContext()), true);\n            rvAlbums.addItemDecoration(rvAlbumsDecoration);\n            rvAlbums.setLayoutManager(new GridLayoutManager(this, spanCount));\n        }\n    }\n\n    private void updateColumnsRvMedia() {\n        int spanCount = SP.getInt(\"n_columns_media\", 3);\n        if (spanCount != ((GridLayoutManager) rvMedia.getLayoutManager()).getSpanCount()) {\n            ((GridLayoutManager) rvMedia.getLayoutManager()).getSpanCount();\n            rvMedia.removeItemDecoration(rvMediaDecoration);\n            rvMediaDecoration = new GridSpacingItemDecoration(spanCount, Measure.pxToDp(3, getApplicationContext()), true);\n            rvMedia.setLayoutManager(new GridLayoutManager(getApplicationContext(), spanCount));\n            rvMedia.addItemDecoration(rvMediaDecoration);\n        }\n    }\n\n    //region TESTING\n\n    @TargetApi(Build.VERSION_CODES.LOLLIPOP)\n    @Override\n    public final void onActivityResult(final int requestCode, final int resultCode, final Intent resultData) {\n        if (resultCode == RESULT_OK) {\n            if (requestCode == REQUEST_CODE_SD_CARD_PERMISSIONS) {\n                Uri treeUri = resultData.getData();\n                // Persist URI in shared preference so that you can use it later.\n                ContentHelper.saveSdCardInfo(getApplicationContext(), treeUri);\n                getContentResolver().takePersistableUriPermission(treeUri, Intent.FLAG_GRANT_WRITE_URI_PERMISSION);\n                SnackBarHandler.show(mDrawerLayout, R.string.got_permission_wr_sdcard);\n            }\n        }\n    }\n    //endregion\n\n    private void requestSdCardPermissions() {\n        final AlertDialog.Builder dialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n\n        AlertDialogsHelper.getTextDialog(LFMainActivity.this, dialogBuilder,\n                R.string.sd_card_write_permission_title, R.string.sd_card_permissions_message, null);\n\n        dialogBuilder.setPositiveButton(getString(R.string.ok_action).toUpperCase(), new DialogInterface.OnClickListener() {\n            @Override\n            public void onClick(DialogInterface dialogInterface, int i) {\n                if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP)\n                    startActivityForResult(new Intent(Intent.ACTION_OPEN_DOCUMENT_TREE), REQUEST_CODE_SD_CARD_PERMISSIONS);\n            }\n        });\n        dialogBuilder.show();\n    }\n\n\n    //region UI/GRAPHIC\n    private void setupUI() {\n        updateColumnsRvs();\n        //TODO: MUST BE FIXED\n        toolbar.setPopupTheme(getPopupToolbarStyle());\n        toolbar.setBackgroundColor(getPrimaryColor());\n        if(localFolder) {\n            toolbar.setTitle(getString(R.string.local_folder));\n        }\n        else{\n            toolbar.setTitle(getString(R.string.hidden_folder));\n        }\n\n        /**** SWIPE TO REFRESH ****/\n        swipeRefreshLayout.setColorSchemeColors(getAccentColor());\n        swipeRefreshLayout.setProgressBackgroundColorSchemeColor(getBackgroundColor());\n\n        setStatusBarColor();\n        setNavBarColor();\n\n        setDrawerTheme();\n        rvAlbums.setBackgroundColor(getBackgroundColor());\n        rvMedia.setBackgroundColor(getBackgroundColor());\n        mediaAdapter.updatePlaceholder(getApplicationContext());\n        albumsAdapter.updateTheme();\n        /**** DRAWER ****/\n        setScrollViewColor((ScrollView) findViewById(R.id.drawer_scrollbar));\n\n        /**** recyclers drawable *****/\n        Drawable drawableScrollBar = ContextCompat.getDrawable(getApplicationContext(), R.drawable.ic_scrollbar);\n        drawableScrollBar.setColorFilter(new PorterDuffColorFilter(getPrimaryColor(), PorterDuff.Mode.SRC_ATOP));\n    }\n\n    private void setDrawerTheme() {\n\n        findViewById(R.id.Drawer_Header).setBackgroundColor(getPrimaryColor());\n        findViewById(R.id.Drawer_Body).setBackgroundColor(getDrawerBackground());\n        findViewById(R.id.drawer_scrollbar).setBackgroundColor(getDrawerBackground());\n        findViewById(R.id.Drawer_Body_Divider).setBackgroundColor(getIconColor());\n\n        /** TEXT VIEWS **/\n        int color = getTextColor();\n        ((TextView) findViewById(R.id.Drawer_Default_Item)).setTextColor(color);\n        ((TextView) findViewById(R.id.Drawer_Setting_Item)).setTextColor(color);\n\n        ((TextView) findViewById(R.id.Drawer_About_Item)).setTextColor(color);\n        ((TextView) findViewById(R.id.Drawer_hidden_Item)).setTextColor(color);\n        ((TextView) findViewById(R.id.Drawer_share_Item)).setTextColor(color);\n        ((TextView) findViewById(R.id.Drawer_rate_Item)).setTextColor(color);\n        ((TextView) findViewById(R.id.Drawer_Upload_Item)).setTextColor(color);\n\n        /** ICONS **/\n        color = getIconColor();\n        ((IconicsImageView) findViewById(R.id.Drawer_Default_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_Setting_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_About_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_hidden_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_share_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_rate_Icon)).setColor(color);\n        ((IconicsImageView) findViewById(R.id.Drawer_Upload_Icon)).setColor(color);\n\n\n        findViewById(R.id.ll_drawer_Setting).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                settings=true;\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n            }\n        });\n\n        findViewById(R.id.ll_drawer_About).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                about=true;\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n\n            }\n        });\n\n        findViewById(R.id.ll_drawer_uploadhistory).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                uploadHistory=true;\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n            }\n        });\n\n        findViewById(R.id.ll_drawer_Default).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                localFolder=true;\n                toolbar.setTitle(getString(R.string.local_folder));\n                hidden = false;\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n                new PrepareAlbumTask().execute();\n            }\n        });\n        findViewById(R.id.ll_drawer_hidden).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                localFolder=false;\n                toolbar.setTitle(getString(R.string.hidden_folder));\n                if (securityObj.isActiveSecurity() && securityObj.isPasswordOnHidden()) {\n                    AlertDialog.Builder passwordDialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n                    final EditText editTextPassword = securityObj.getInsertPasswordDialog(LFMainActivity.this, passwordDialogBuilder);\n                    passwordDialogBuilder.setPositiveButton(getString(R.string.ok_action).toUpperCase(), new DialogInterface.OnClickListener() {\n                        public void onClick(DialogInterface dialog, int which) {\n                        }\n                    });\n\n                    passwordDialogBuilder.setNegativeButton(getString(R.string.cancel).toUpperCase(), null);\n\n                    final AlertDialog passwordDialog = passwordDialogBuilder.create();\n                    passwordDialog.show();\n\n                    passwordDialog.getButton(AlertDialog.BUTTON_POSITIVE).setOnClickListener(new View\n                            .OnClickListener() {\n                        @Override\n                        public void onClick(View v) {\n                            if (securityObj.checkPassword(editTextPassword.getText().toString())) {\n                                hidden = true;\n                                mDrawerLayout.closeDrawer(GravityCompat.START);\n                                new PrepareAlbumTask().execute();\n                                passwordDialog.dismiss();\n                            } else {\n                                SnackBarHandler.show(mDrawerLayout, R.string.wrong_password);\n                                editTextPassword.getText().clear();\n                                editTextPassword.requestFocus();\n                            }\n                        }\n                    });\n                } else {\n                    hidden = true;\n                    mDrawerLayout.closeDrawer(GravityCompat.START);\n                    new PrepareAlbumTask().execute();\n                }\n            }\n        });\n\n        findViewById(R.id.ll_share_phimpme).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                onInviteClicked();\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n            }\n        });\n        findViewById(R.id.ll_rate_phimpme).setOnClickListener(new View.OnClickListener() {\n            @Override\n            public void onClick(View v) {\n                final String appPackageName = getPackageName();\n                try {\n                    startActivity(new Intent(Intent.ACTION_VIEW, Uri.parse(\"market://details?id=\" + appPackageName)));\n                } catch (android.content.ActivityNotFoundException anfe) {\n                    startActivity(new Intent(Intent.ACTION_VIEW, Uri.parse(\"https://play.google.com/store/apps/details?id=\" + appPackageName)));\n                }\n                mDrawerLayout.closeDrawer(GravityCompat.START);\n            }\n        });\n    }\n\n    private void onInviteClicked() {\n        Intent sendIntent = new Intent();\n        sendIntent.setAction(Intent.ACTION_SEND);\n        sendIntent.putExtra(Intent.EXTRA_TEXT, getString(R.string.install_phimpme) + \"\\n \" + getString(R.string.invitation_deep_link));\n        sendIntent.setType(\"text/plain\");\n        startActivity(sendIntent);\n    }\n    //endregion\n\n    private void updateSelectedStuff() {\n        if (albumsMode) {\n            if(getAlbums().getSelectedCount()==0)\n                clearOverlay();\n            else\n                appBarOverlay();\n            if (editMode)\n                toolbar.setTitle(getAlbums().getSelectedCount() + \"/\" + getAlbums().dispAlbums.size());\n            else {\n                if(hidden)\n                    toolbar.setTitle(getString(R.string.hidden_folder));\n                else toolbar.setTitle(getString(R.string.local_folder));\n                toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_menu));\n                toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n                    @Override\n                    public void onClick(View v) {\n                        mDrawerLayout.openDrawer(GravityCompat.START);\n                    }\n                });\n            }\n        } else {\n            if(getAlbum().getSelectedCount()==0)\n                clearOverlay();\n            else\n                appBarOverlay();\n            if (editMode)\n                if (!all_photos)\n                    toolbar.setTitle(getAlbum().getSelectedCount() + \"/\" + getAlbum().getMedia().size());\n                else {\n                    toolbar.setTitle(selectedMedias.size() + \"/\" + size);}\n            else {\n                if (!all_photos)\n                    toolbar.setTitle(getAlbum().getName());\n                else toolbar.setTitle(getString(R.string.all_media));\n                toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_arrow_back));\n                toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n                    @Override\n                    public void onClick(View v) {\n                        displayAlbums();\n                    }\n                });\n            }\n        }\n\n        if (editMode) {\n            toolbar.setNavigationIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_clear));\n            toolbar.setNavigationOnClickListener(new View.OnClickListener() {\n                @Override\n                public void onClick(View v) {\n                    finishEditMode();\n                    clearSelectedPhotos();\n                }\n            });\n        }\n    }\n\n    //called from onBackPressed()\n    private void finishEditMode() {\n        editMode = false;\n        if (albumsMode) {\n            getAlbums().clearSelectedAlbums();\n            albumsAdapter.notifyDataSetChanged();\n        } else {\n            getAlbum().clearSelectedPhotos();\n            mediaAdapter.notifyDataSetChanged();\n        }\n        invalidateOptionsMenu();\n    }\n\n    private void checkNothing() {\n        TextView a = (TextView) findViewById(R.id.nothing_to_show);\n        a.setTextColor(getTextColor());\n        a.setVisibility((albumsMode && getAlbums().dispAlbums.size() == 0) || (!albumsMode && getAlbum().getMedia().size() == 0) ? View.VISIBLE : View.GONE);\n    }\n\n    //region MENU\n    @Override\n    public boolean onCreateOptionsMenu(Menu menu) {\n\n        // Inflate the menu; this adds items to the action bar if it is present.\n        getMenuInflater().inflate(R.menu.menu_albums, menu);\n\n        if (albumsMode) {\n            menu.findItem(R.id.select_all).setTitle(\n                    getString(getAlbums().getSelectedCount() == albumsAdapter.getItemCount()\n                            ? R.string.clear_selected\n                            : R.string.select_all));\n            menu.findItem(R.id.ascending_sort_action).setChecked(getAlbums().getSortingOrder() == SortingOrder.ASCENDING);\n            switch (getAlbums().getSortingMode()) {\n                case NAME:\n                    menu.findItem(R.id.name_sort_action).setChecked(true);\n                    break;\n                case SIZE:\n                    menu.findItem(R.id.size_sort_action).setChecked(true);\n                    break;\n                case DATE:\n                default:\n                    menu.findItem(R.id.date_taken_sort_action).setChecked(true);\n                    break;\n                case NUMERIC:\n                    menu.findItem(R.id.numeric_sort_action).setChecked(true);\n                    break;\n            }\n\n        } else {\n            menu.findItem(R.id.select_all).setTitle(getString(\n                    getAlbum().getSelectedCount() == mediaAdapter.getItemCount()\n                            || selectedMedias.size() == size\n                            ? R.string.clear_selected\n                            : R.string.select_all));\n            menu.findItem(R.id.ascending_sort_action).setChecked(getAlbum().settings.getSortingOrder() == SortingOrder.ASCENDING);\n            switch (getAlbum().settings.getSortingMode()) {\n                case NAME:\n                    menu.findItem(R.id.name_sort_action).setChecked(true);\n                    break;\n                case SIZE:\n                    menu.findItem(R.id.size_sort_action).setChecked(true);\n                    break;\n                case DATE:\n                default:\n                    menu.findItem(R.id.date_taken_sort_action).setChecked(true);\n                    break;\n                case NUMERIC:\n                    menu.findItem(R.id.numeric_sort_action).setChecked(true);\n                    break;\n            }\n        }\n\n        menu.findItem(R.id.hideAlbumButton).setTitle(hidden ? getString(R.string.unhide) : getString(R.string.hide));\n        menu.findItem(R.id.delete_action).setIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_delete));\n        menu.findItem(R.id.sort_action).setIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_sort));\n        menu.findItem(R.id.sharePhotos).setIcon(getToolbarIcon(GoogleMaterial.Icon.gmd_share));\n\n        return true;\n    }\n\n    @Override\n    public boolean onPrepareOptionsMenu(final Menu menu) {\n        if (albumsMode) {\n            editMode = getAlbums().getSelectedCount() != 0;\n            menu.setGroupVisible(R.id.album_options_menu, editMode);\n            menu.setGroupVisible(R.id.photos_option_men, false);\n            menu.findItem(R.id.all_photos).setVisible(!editMode);\n        } else {\n            if (!all_photos) {\n                editMode = getAlbum().areMediaSelected();\n                menu.setGroupVisible(R.id.photos_option_men, editMode);\n                menu.setGroupVisible(R.id.album_options_menu, !editMode);\n                menu.findItem(R.id.all_photos).setVisible(false);\n            } else {\n                editMode = selectedMedias.size() != 0;\n                menu.setGroupVisible(R.id.photos_option_men, editMode);\n                menu.setGroupVisible(R.id.album_options_menu, !editMode);\n                menu.findItem(R.id.all_photos).setVisible(false);\n            }\n        }\n\n        togglePrimaryToolbarOptions(menu);\n        updateSelectedStuff();\n        visible = getAlbum().getSelectedCount() > 0;\n        menu.findItem(R.id.action_copy).setVisible(visible);\n        menu.findItem(R.id.action_move).setVisible(visible || editMode);\n        menu.findItem(R.id.excludeAlbumButton).setVisible(editMode && !all_photos && albumsMode);\n        menu.findItem(R.id.select_all).setVisible(editMode);\n        menu.findItem(R.id.delete_action).setVisible((!albumsMode || editMode) && (!all_photos || editMode));\n        menu.findItem(R.id.hideAlbumButton).setVisible(!all_photos && getAlbums().getSelectedCount() > 0);\n\n        menu.findItem(R.id.clear_album_preview).setVisible(!albumsMode && getAlbum().hasCustomCover());\n        menu.findItem(R.id.renameAlbum).setVisible(((albumsMode && getAlbums().getSelectedCount() == 1) || (!albumsMode && !editMode)) && !all_photos);\n        if (getAlbums().getSelectedCount() == 1)\n            menu.findItem(R.id.set_pin_album).setTitle(getAlbums().getSelectedAlbum(0).isPinned() ? getString(R.string.un_pin) : getString(R.string.pin));\n        menu.findItem(R.id.set_pin_album).setVisible(albumsMode && getAlbums().getSelectedCount() == 1);\n        menu.findItem(R.id.setAsAlbumPreview).setVisible(!albumsMode && !all_photos && getAlbum().getSelectedCount() == 1);\n        menu.findItem(R.id.affixPhoto).setVisible(!albumsMode && (getAlbum().getSelectedCount() > 1) || selectedMedias.size() > 1);\n        return super.onPrepareOptionsMenu(menu);\n    }\n\n    private void togglePrimaryToolbarOptions(final Menu menu) {\n        menu.setGroupVisible(R.id.general_action, !editMode);\n\n    }\n\n    //endregion\n\n    @Override\n    public boolean onOptionsItemSelected(MenuItem item) {\n\n        switch (item.getItemId()) {\n\n            case R.id.all_photos:\n                if (!all_photos) {\n                    all_photos = true;\n                    displayAllMedia(true);\n                } else {\n                    displayAlbums();\n                }\n                return true;\n\n            case R.id.select_all:\n                if (albumsMode) {\n                    //if all albums are already selected, unselect all of them\n                    if (getAlbums().getSelectedCount() == albumsAdapter.getItemCount()) {\n                        editMode = false;\n                        getAlbums().clearSelectedAlbums();\n                    }\n                    // else, select all albums\n                    else getAlbums().selectAllAlbums();\n                    albumsAdapter.notifyDataSetChanged();\n                } else {\n                    if (!all_photos) {\n                        //if all photos are already selected, unselect all of them\n                        if (getAlbum().getSelectedCount() == mediaAdapter.getItemCount()) {\n                            editMode = false;\n                            getAlbum().clearSelectedPhotos();\n                        }\n                        // else, select all photos\n                        else getAlbum().selectAllPhotos();\n                        mediaAdapter.notifyDataSetChanged();\n                    } else {\n\n                        if (selectedMedias.size() == size) {\n                            editMode = false;\n                            clearSelectedPhotos();\n                        }\n                        // else, select all photos\n                        else {\n                            clearSelectedPhotos();\n                            selectAllPhotos();\n                        }\n                        mediaAdapter.notifyDataSetChanged();\n                    }\n                }\n                invalidateOptionsMenu();\n                return true;\n\n            case R.id.set_pin_album:\n                getAlbums().getSelectedAlbum(0).settings.togglePin(getApplicationContext());\n                getAlbums().sortAlbums(getApplicationContext());\n                getAlbums().clearSelectedAlbums();\n                albumsAdapter.swapDataSet(getAlbums().dispAlbums);\n                invalidateOptionsMenu();\n                return true;\n\n            case R.id.settings:\n                startActivity(new Intent(LFMainActivity.this, SettingsActivity.class));\n                return true;\n\n            case R.id.hideAlbumButton:\n                final AlertDialog.Builder hideDialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n\n                AlertDialogsHelper.getTextDialog(LFMainActivity.this, hideDialogBuilder,\n                        hidden ? R.string.unhide : R.string.hide,\n                        hidden ? R.string.unhide_album_message : R.string.hide_album_message, null);\n\n                hideDialogBuilder.setPositiveButton(getString(hidden ? R.string.unhide : R.string.hide).toUpperCase(), new DialogInterface.OnClickListener() {\n                    public void onClick(DialogInterface dialog, int id) {\n                        if (albumsMode) {\n                            if (hidden) getAlbums().unHideSelectedAlbums(getApplicationContext());\n                            else getAlbums().hideSelectedAlbums(getApplicationContext());\n                            albumsAdapter.notifyDataSetChanged();\n                            invalidateOptionsMenu();\n                        } else {\n                            if (hidden)\n                                getAlbums().unHideAlbum(getAlbum().getPath(), getApplicationContext());\n                            else\n                                getAlbums().hideAlbum(getAlbum().getPath(), getApplicationContext());\n                            displayAlbums(true);\n                        }\n                    }\n                });\n                if (!hidden) {\n                    hideDialogBuilder.setNeutralButton(this.getString(R.string.exclude).toUpperCase(), new DialogInterface.OnClickListener() {\n                        @Override\n                        public void onClick(DialogInterface dialog, int which) {\n                            if (albumsMode) {\n                                getAlbums().excludeSelectedAlbums(getApplicationContext());\n                                albumsAdapter.notifyDataSetChanged();\n                                invalidateOptionsMenu();\n                            } else {\n                                customAlbumsHelper.excludeAlbum(getAlbum().getPath());\n                                displayAlbums(true);\n                            }\n                        }\n                    });\n                }\n                hideDialogBuilder.setNegativeButton(this.getString(R.string.cancel).toUpperCase(), null);\n                hideDialogBuilder.show();\n                return true;\n\n            case R.id.delete_action:\n                class DeletePhotos extends AsyncTask<String, Integer, Boolean> {\n                    @Override\n                    protected void onPreExecute() {\n                        swipeRefreshLayout.setRefreshing(true);\n                        super.onPreExecute();\n                    }\n\n\n                    @Override\n                    protected Boolean doInBackground(String... arg0) {\n                        //if in album mode, delete selected albums\n                        if (albumsMode)\n                            return getAlbums().deleteSelectedAlbums(LFMainActivity.this);\n                        else {\n                            // if in selection mode, delete selected media\n                            if (editMode && !all_photos)\n                                return getAlbum().deleteSelectedMedia(getApplicationContext());\n                            else if (all_photos) {\n                                Boolean succ = false;\n                                for (Media media : selectedMedias) {\n                                    String[] projection = {MediaStore.Images.Media._ID};\n\n                                    // Match on the file path\n                                    String selection = MediaStore.Images.Media.DATA + \" = ?\";\n                                    String[] selectionArgs = new String[]{media.getPath()};\n\n                                    // Query for the ID of the media matching the file path\n                                    Uri queryUri = MediaStore.Images.Media.EXTERNAL_CONTENT_URI;\n                                    ContentResolver contentResolver = getContentResolver();\n                                    Cursor c = contentResolver.query(queryUri, projection, selection, selectionArgs, null);\n                                    if (c.moveToFirst()) {\n                                        // We found the ID. Deleting the item via the content provider will also remove the file\n                                        long id = c.getLong(c.getColumnIndexOrThrow(MediaStore.Images.Media._ID));\n                                        Uri deleteUri = ContentUris.withAppendedId(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, id);\n                                        contentResolver.delete(deleteUri, null, null);\n                                        succ = true;\n                                    } else {\n                                        succ = false;\n                                        // File not found in media store DB\n                                    }\n                                    c.close();\n                                }\n                                return succ;\n                            }\n                            // if not in selection mode, delete current album entirely\n                            else {\n                                boolean succ = getAlbums().deleteAlbum(getAlbum(), getApplicationContext());\n                                getAlbum().getMedia().clear();\n                                return succ;\n                            }\n                        }\n                    }\n\n                    @Override\n                    protected void onPostExecute(Boolean result) {\n                        if (result) {\n                            // in albumsMode, the selected albums have been deleted.\n                            if (albumsMode) {\n                                getAlbums().clearSelectedAlbums();\n                                albumsAdapter.notifyDataSetChanged();\n                            } else {\n                                if (!all_photos) {\n                                    //if all media in current album have been deleted, delete current album too.\n                                    if (getAlbum().getMedia().size() == 0) {\n                                        getAlbums().removeCurrentAlbum();\n                                        albumsAdapter.notifyDataSetChanged();\n                                        displayAlbums();\n                                        swipeRefreshLayout.setRefreshing(true);\n                                    } else\n                                        mediaAdapter.swapDataSet(getAlbum().getMedia());\n                                } else {\n                                    clearSelectedPhotos();\n                                    listAll = StorageProvider.getAllShownImages(LFMainActivity.this);\n                                    media = listAll;\n                                    size = listAll.size();\n                                    mediaAdapter.swapDataSet(listAll);\n                                }\n                            }\n                        } else requestSdCardPermissions();\n\n                        invalidateOptionsMenu();\n                        checkNothing();\n                        swipeRefreshLayout.setRefreshing(false);\n                    }\n                }\n\n                AlertDialog.Builder deleteDialog = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n                AlertDialogsHelper.getTextDialog(this, deleteDialog, R.string.delete, albumsMode || !editMode ? R.string.delete_album_message : R.string.delete_photos_message, null);\n\n                deleteDialog.setNegativeButton(this.getString(R.string.cancel).toUpperCase(), null);\n                deleteDialog.setPositiveButton(this.getString(R.string.delete).toUpperCase(), new DialogInterface.OnClickListener() {\n                    public void onClick(DialogInterface dialog, int id) {\n                        if (securityObj.isActiveSecurity() && securityObj.isPasswordOnDelete()) {\n                            AlertDialog.Builder passwordDialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n                            final EditText editTextPassword = securityObj.getInsertPasswordDialog(LFMainActivity.this, passwordDialogBuilder);\n                            passwordDialogBuilder.setNegativeButton(getString(R.string.cancel).toUpperCase(), null);\n\n                            passwordDialogBuilder.setPositiveButton(getString(R.string.ok_action).toUpperCase(), new DialogInterface.OnClickListener() {\n                                @Override\n                                public void onClick(DialogInterface dialog, int which) {\n                                    //This should br empty it will be overwrite later\n                                    //to avoid dismiss of the dialog on wrong password\n                                }\n                            });\n\n                            final AlertDialog passwordDialog = passwordDialogBuilder.create();\n                            passwordDialog.getWindow().setSoftInputMode(WindowManager.LayoutParams.SOFT_INPUT_STATE_VISIBLE);\n                            passwordDialog.show();\n\n                            passwordDialog.getButton(AlertDialog.BUTTON_POSITIVE).setOnClickListener(new View.OnClickListener() {\n                                @Override\n                                public void onClick(View v) {\n                                    // if password is correct, call DeletePhotos and perform deletion\n                                    if (securityObj.checkPassword(editTextPassword.getText().toString())) {\n                                        passwordDialog.dismiss();\n                                        new DeletePhotos().execute();\n                                    }\n                                    // if password is incorrect, don't delete and notify user of incorrect password\n                                    else {\n                                        SnackBarHandler.show(mDrawerLayout, R.string.wrong_password);\n                                        editTextPassword.getText().clear();\n                                        editTextPassword.requestFocus();\n                                    }\n                                }\n                            });\n                        } else new DeletePhotos().execute();\n                    }\n                });\n                deleteDialog.show();\n\n                return true;\n            case R.id.excludeAlbumButton:\n                final AlertDialog.Builder excludeDialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n\n                final View excludeDialogLayout = getLayoutInflater().inflate(R.layout.dialog_exclude, null);\n                TextView textViewExcludeTitle = (TextView) excludeDialogLayout.findViewById(R.id.text_dialog_title);\n                TextView textViewExcludeMessage = (TextView) excludeDialogLayout.findViewById(R.id.text_dialog_message);\n                final Spinner spinnerParents = (Spinner) excludeDialogLayout.findViewById(R.id.parents_folder);\n\n                spinnerParents.getBackground().setColorFilter(getIconColor(), PorterDuff.Mode.SRC_ATOP);\n\n                ((CardView) excludeDialogLayout.findViewById(R.id.message_card)).setCardBackgroundColor(getCardBackgroundColor());\n                textViewExcludeTitle.setBackgroundColor(getPrimaryColor());\n                textViewExcludeTitle.setText(getString(R.string.exclude));\n\n                if ((albumsMode && getAlbums().getSelectedCount() > 1)) {\n                    textViewExcludeMessage.setText(R.string.exclude_albums_message);\n                    spinnerParents.setVisibility(View.GONE);\n                } else {\n                    textViewExcludeMessage.setText(R.string.exclude_album_message);\n                    spinnerParents.setAdapter(getSpinnerAdapter(albumsMode ? getAlbums().getSelectedAlbum(0).getParentsFolders() : getAlbum().getParentsFolders()));\n                }\n\n                textViewExcludeMessage.setTextColor(getTextColor());\n                excludeDialogBuilder.setView(excludeDialogLayout);\n\n                excludeDialogBuilder.setPositiveButton(this.getString(R.string.exclude).toUpperCase(), new DialogInterface.OnClickListener() {\n                    public void onClick(DialogInterface dialog, int id) {\n\n                        if ((albumsMode && getAlbums().getSelectedCount() > 1)) {\n                            getAlbums().excludeSelectedAlbums(getApplicationContext());\n                            albumsAdapter.notifyDataSetChanged();\n                            invalidateOptionsMenu();\n                        } else {\n                            customAlbumsHelper.excludeAlbum(spinnerParents.getSelectedItem().toString());\n                            finishEditMode();\n                            displayAlbums(true);\n                        }\n                    }\n                });\n                excludeDialogBuilder.setNegativeButton(this.getString(R.string.cancel).toUpperCase(), null);\n                excludeDialogBuilder.show();\n                return true;\n\n            case R.id.sharePhotos:\n                Intent intent = new Intent();\n                intent.setAction(Intent.ACTION_SEND_MULTIPLE);\n                intent.putExtra(Intent.EXTRA_SUBJECT, getString(R.string.sent_to_action));\n\n                // list of all selected media in current album\n                ArrayList<Uri> files = new ArrayList<Uri>();\n                if (!all_photos) {\n                    for (Media f : getAlbum().getSelectedMedia())\n                        files.add(f.getUri());\n                } else {\n                    for (Media f : selectedMedias)\n                        files.add(f.getUri());\n                }\n                String extension = files.get(0).getPath().substring(files.get(0).getPath().lastIndexOf('.') + 1);\n                String mimeType = MimeTypeMap.getSingleton().getMimeTypeFromExtension(extension);\n\n                intent.putParcelableArrayListExtra(Intent.EXTRA_STREAM, files);\n                if (!all_photos)\n                    intent.setType(StringUtils.getGenericMIME(getAlbum().getSelectedMedia(0).getMimeType()));\n                else intent.setType(mimeType);\n                finishEditMode();\n                startActivity(Intent.createChooser(intent, getResources().getText(R.string.send_to)));\n                return true;\n\n\n            case R.id.name_sort_action:\n                if (albumsMode) {\n                    getAlbums().setDefaultSortingMode(NAME);\n                    new SortingUtilsAlbums().execute();\n                } else {\n                    getAlbum().setDefaultSortingMode(getApplicationContext(), NAME);\n                    new SortingUtilsPhtots().execute();\n                    if (all_photos) {\n                        new SortingUtilsListAll().execute();\n                    }\n                }\n                item.setChecked(true);\n                return true;\n\n            case R.id.date_taken_sort_action:\n                if (albumsMode) {\n                    getAlbums().setDefaultSortingMode(DATE);\n                    new SortingUtilsAlbums().execute();\n                } else {\n                    getAlbum().setDefaultSortingMode(getApplicationContext(), DATE);\n                    new SortingUtilsPhtots().execute();\n                    if (all_photos) {\n                        new SortingUtilsListAll().execute();\n                    }\n                }\n                item.setChecked(true);\n                return true;\n\n            case R.id.size_sort_action:\n                if (albumsMode) {\n                    getAlbums().setDefaultSortingMode(SIZE);\n                    new SortingUtilsAlbums().execute();\n                } else {\n                    getAlbum().setDefaultSortingMode(getApplicationContext(), SIZE);\n                    new SortingUtilsPhtots().execute();\n                    if (all_photos) {\n                        new SortingUtilsListAll().execute();\n                    }\n                }\n                item.setChecked(true);\n                return true;\n\n            case R.id.numeric_sort_action:\n                if (albumsMode) {\n                    getAlbums().setDefaultSortingMode(NUMERIC);\n                    new SortingUtilsAlbums().execute();\n                } else {\n                    getAlbum().setDefaultSortingMode(getApplicationContext(), NUMERIC);\n                    new SortingUtilsPhtots().execute();\n                    if (all_photos) {\n                        new SortingUtilsListAll().execute();\n                    }\n                }\n                item.setChecked(true);\n                return true;\n\n            case R.id.ascending_sort_action:\n                if (albumsMode) {\n                    getAlbums().setDefaultSortingAscending(item.isChecked() ? SortingOrder.DESCENDING : SortingOrder.ASCENDING);\n                    new SortingUtilsAlbums().execute();\n                } else {\n                    getAlbum().setDefaultSortingAscending(getApplicationContext(), item.isChecked() ? SortingOrder.DESCENDING : SortingOrder.ASCENDING);\n                    new SortingUtilsPhtots().execute();\n                    if (all_photos) {\n                        new SortingUtilsListAll().execute();\n                    }\n                }\n                item.setChecked(!item.isChecked());\n                return true;\n\n            //region Affix\n            case R.id.affixPhoto:\n\n                //region Async MediaAffix\n                class affixMedia extends AsyncTask<Affix.Options, Integer, Void> {\n                    private AlertDialog dialog;\n\n                    @Override\n                    protected void onPreExecute() {\n                        AlertDialog.Builder progressDialog = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n\n                        dialog = AlertDialogsHelper.getProgressDialog(LFMainActivity.this, progressDialog,\n                                getString(R.string.affix), getString(R.string.affix_text));\n                        dialog.show();\n                        super.onPreExecute();\n                    }\n\n                    @Override\n                    protected Void doInBackground(Affix.Options... arg0) {\n                        ArrayList<Bitmap> bitmapArray = new ArrayList<Bitmap>();\n                        if (!all_photos) {\n                            for (int i = 0; i < getAlbum().getSelectedCount(); i++) {\n                                bitmapArray.add(getBitmap(getAlbum().getSelectedMedia(i).getPath()));\n                            }\n                        } else {\n                            for (int i = 0; i < selectedMedias.size(); i++) {\n                                bitmapArray.add(getBitmap(selectedMedias.get(i).getPath()));\n                            }\n                        }\n\n                        if (bitmapArray.size() > 1)\n                            Affix.AffixBitmapList(getApplicationContext(), bitmapArray, arg0[0]);\n                        else runOnUiThread(new Runnable() {\n                            @Override\n                            public void run() {\n                                SnackBarHandler.show(mDrawerLayout, R.string.affix_error);\n                            }\n                        });\n                        return null;\n                    }\n\n                    @Override\n                    protected void onPostExecute(Void result) {\n                        editMode = false;\n                        if (!all_photos)\n                            getAlbum().clearSelectedPhotos();\n                        else clearSelectedPhotos();\n                        dialog.dismiss();\n                        invalidateOptionsMenu();\n                        mediaAdapter.notifyDataSetChanged();\n                        if (!all_photos)\n                            new PreparePhotosTask().execute();\n                        else clearSelectedPhotos();\n\n                    }\n                }\n                //endregion\n\n                final AlertDialog.Builder builder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n                final View dialogLayout = getLayoutInflater().inflate(R.layout.dialog_affix, null);\n\n                dialogLayout.findViewById(R.id.affix_title).setBackgroundColor(getPrimaryColor());\n                ((CardView) dialogLayout.findViewById(R.id.affix_card)).setCardBackgroundColor(getCardBackgroundColor());\n\n                //ITEMS\n                final SwitchCompat swVertical = (SwitchCompat) dialogLayout.findViewById(R.id.affix_vertical_switch);\n                final SwitchCompat swSaveHere = (SwitchCompat) dialogLayout.findViewById(R.id.save_here_switch);\n\n                final RadioGroup radioFormatGroup = (RadioGroup) dialogLayout.findViewById(R.id.radio_format);\n\n                final TextView txtQuality = (TextView) dialogLayout.findViewById(R.id.affix_quality_title);\n                final SeekBar seekQuality = (SeekBar) dialogLayout.findViewById(R.id.seek_bar_quality);\n\n                //region THEME STUFF\n                setScrollViewColor((ScrollView) dialogLayout.findViewById(R.id.affix_scrollView));\n\n                /** TextViews **/\n                int color = getTextColor();\n                ((TextView) dialogLayout.findViewById(R.id.affix_vertical_title)).setTextColor(color);\n                ((TextView) dialogLayout.findViewById(R.id.compression_settings_title)).setTextColor(color);\n                ((TextView) dialogLayout.findViewById(R.id.save_here_title)).setTextColor(color);\n\n                /** Sub TextViews **/\n                color = getTextColor();\n                ((TextView) dialogLayout.findViewById(R.id.save_here_sub)).setTextColor(color);\n                ((TextView) dialogLayout.findViewById(R.id.affix_vertical_sub)).setTextColor(color);\n                ((TextView) dialogLayout.findViewById(R.id.affix_format_sub)).setTextColor(color);\n                txtQuality.setTextColor(color);\n\n                /** Icons **/\n                color = getIconColor();\n                ((IconicsImageView) dialogLayout.findViewById(R.id.affix_quality_icon)).setColor(color);\n                ((IconicsImageView) dialogLayout.findViewById(R.id.affix_format_icon)).setColor(color);\n                ((IconicsImageView) dialogLayout.findViewById(R.id.affix_vertical_icon)).setColor(color);\n                ((IconicsImageView) dialogLayout.findViewById(R.id.save_here_icon)).setColor(color);\n\n                seekQuality.getProgressDrawable().setColorFilter(new PorterDuffColorFilter(getAccentColor(), PorterDuff.Mode.SRC_IN));\n                seekQuality.getThumb().setColorFilter(new PorterDuffColorFilter(getAccentColor(), PorterDuff.Mode.SRC_IN));\n\n                updateRadioButtonColor((RadioButton) dialogLayout.findViewById(R.id.radio_jpeg));\n                updateRadioButtonColor((RadioButton) dialogLayout.findViewById(R.id.radio_png));\n                updateRadioButtonColor((RadioButton) dialogLayout.findViewById(R.id.radio_webp));\n\n                updateSwitchColor(swVertical, getAccentColor());\n                updateSwitchColor(swSaveHere, getAccentColor());\n                //endregion\n\n                seekQuality.setOnSeekBarChangeListener(new SeekBar.OnSeekBarChangeListener() {\n                    @Override\n                    public void onProgressChanged(SeekBar seekBar, int progress, boolean fromUser) {\n                        txtQuality.setText(Html.fromHtml(\n                                String.format(Locale.getDefault(), \"%s <b>%d</b>\", getString(R.string.quality), progress)));\n                    }\n\n                    @Override\n                    public void onStartTrackingTouch(SeekBar seekBar) {\n\n                    }\n\n                    @Override\n                    public void onStopTrackingTouch(SeekBar seekBar) {\n\n                    }\n                });\n                seekQuality.setProgress(90); //DEFAULT\n\n                swVertical.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {\n                    @Override\n                    public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {\n                        updateSwitchColor(swVertical, getAccentColor());\n                    }\n                });\n\n                swSaveHere.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {\n                    @Override\n                    public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {\n                        updateSwitchColor(swSaveHere, getAccentColor());\n                    }\n                });\n                builder.setView(dialogLayout);\n                builder.setPositiveButton(this.getString(R.string.ok_action).toUpperCase(), new DialogInterface.OnClickListener() {\n                    public void onClick(DialogInterface dialog, int id) {\n                        Bitmap.CompressFormat compressFormat;\n                        switch (radioFormatGroup.getCheckedRadioButtonId()) {\n                            case R.id.radio_jpeg:\n                            default:\n                                compressFormat = Bitmap.CompressFormat.JPEG;\n                                break;\n                            case R.id.radio_png:\n                                compressFormat = Bitmap.CompressFormat.PNG;\n                                break;\n                            case R.id.radio_webp:\n                                compressFormat = Bitmap.CompressFormat.WEBP;\n                                break;\n                        }\n\n                        Affix.Options options = new Affix.Options(\n                                swSaveHere.isChecked() ? getAlbum().getPath() : Affix.getDefaultDirectoryPath(),\n                                compressFormat,\n                                seekQuality.getProgress(),\n                                swVertical.isChecked());\n                        new affixMedia().execute(options);\n                    }\n                });\n                builder.setNegativeButton(this.getString(R.string.cancel).toUpperCase(), null);\n                builder.show();\n\n\n                return true;\n            //endregion\n\n            case R.id.action_move:\n\n                bottomSheetDialogFragment = new SelectAlbumBottomSheet();\n                bottomSheetDialogFragment.setTitle(getString(R.string.move_to));\n                bottomSheetDialogFragment.setSelectAlbumInterface(new SelectAlbumBottomSheet.SelectAlbumInterface() {\n                    @Override\n                    public void folderSelected(String path) {\n                        swipeRefreshLayout.setRefreshing(true);\n                        if (getAlbum().moveSelectedMedia(getApplicationContext(), path) > 0) {\n                            if (getAlbum().getMedia().size() == 0) {\n                                getAlbums().removeCurrentAlbum();\n                                albumsAdapter.notifyDataSetChanged();\n                                displayAlbums();\n                            }\n                            mediaAdapter.swapDataSet(getAlbum().getMedia());\n                            finishEditMode();\n                            invalidateOptionsMenu();\n                        } else requestSdCardPermissions();\n\n                        swipeRefreshLayout.setRefreshing(false);\n                        bottomSheetDialogFragment.dismiss();\n                    }\n                });\n                bottomSheetDialogFragment.show(getSupportFragmentManager(), bottomSheetDialogFragment.getTag());\n                return true;\n\n            case R.id.action_copy:\n                bottomSheetDialogFragment = new SelectAlbumBottomSheet();\n                bottomSheetDialogFragment.setTitle(getString(R.string.copy_to));\n                bottomSheetDialogFragment.setSelectAlbumInterface(new SelectAlbumBottomSheet.SelectAlbumInterface() {\n                    @Override\n                    public void folderSelected(String path) {\n                        boolean success = getAlbum().copySelectedPhotos(getApplicationContext(), path);\n                        finishEditMode();\n                        bottomSheetDialogFragment.dismiss();\n                        if (!success)\n                            requestSdCardPermissions();\n                    }\n                });\n                bottomSheetDialogFragment.show(getSupportFragmentManager(), bottomSheetDialogFragment.getTag());\n                return true;\n\n            case R.id.renameAlbum:\n                AlertDialog.Builder renameDialogBuilder = new AlertDialog.Builder(LFMainActivity.this, getDialogStyle());\n                final EditText editTextNewName = new EditText(getApplicationContext());\n                editTextNewName.setText(albumsMode ? getAlbums().getSelectedAlbum(0).getName() : getAlbum().getName());\n\n                AlertDialogsHelper.getInsertTextDialog(LFMainActivity.this, renameDialogBuilder,\n                        editTextNewName, R.string.rename_album, null);\n\n                renameDialogBuilder.setNegativeButton(getString(R.string.cancel).toUpperCase(), null);\n\n                renameDialogBuilder.setPositiveButton(getString(R.string.ok_action).toUpperCase(), new DialogInterface.OnClickListener() {\n                    @Override\n                    public void onClick(DialogInterface dialog, int which) {\n                        //This should br empty it will be overwrite later\n                        //to avoid dismiss of the dialog\n                    }\n                });\n                final AlertDialog renameDialog = renameDialogBuilder.create();\n                renameDialog.getWindow().setSoftInputMode(WindowManager.LayoutParams.SOFT_INPUT_IS_FORWARD_NAVIGATION);\n                editTextNewName.setSelection(editTextNewName.getText().toString().length());\n                renameDialog.show();\n\n                renameDialog.getButton(DialogInterface.BUTTON_POSITIVE).setOnClickListener(new View.OnClickListener() {\n                    @Override\n                    public void onClick(View dialog) {\n                        if (editTextNewName.length() != 0) {\n                            swipeRefreshLayout.setRefreshing(true);\n                            boolean success = false;\n                            if (albumsMode) {\n                                int index = getAlbums().dispAlbums.indexOf(getAlbums().getSelectedAlbum(0));\n                                getAlbums().getAlbum(index).updatePhotos(getApplicationContext());\n                                success = getAlbums().getAlbum(index).renameAlbum(getApplicationContext(),\n                                        editTextNewName.getText().toString());\n                                albumsAdapter.notifyItemChanged(index);\n                            } else {\n                                success = getAlbum().renameAlbum(getApplicationContext(), editTextNewName.getText().toString());\n                                toolbar.setTitle(getAlbum().getName());\n                                mediaAdapter.notifyDataSetChanged();\n                            }\n                            renameDialog.dismiss();\n                            if (success) {\n                                SnackBarHandler.show(getWindow().getDecorView().getRootView(), getString(R.string.rename_succes));\n                                getAlbums().clearSelectedAlbums();\n                                invalidateOptionsMenu();\n                            } else {\n                                SnackBarHandler.show(getWindow().getDecorView().getRootView(), getString(R.string.rename_error));\n                                requestSdCardPermissions();\n                            }\n                            swipeRefreshLayout.setRefreshing(false);\n                        } else {\n                            SnackBarHandler.show(mDrawerLayout, R.string.insert_something);\n                            editTextNewName.requestFocus();\n                        }\n                    }\n                });\n                return true;\n\n            case R.id.clear_album_preview:\n                if (!albumsMode) {\n                    getAlbum().removeCoverAlbum(getApplicationContext());\n                }\n                return true;\n\n            case R.id.setAsAlbumPreview:\n                if (!albumsMode) {\n                    getAlbum().setSelectedPhotoAsPreview(getApplicationContext());\n                    finishEditMode();\n                }\n                return true;\n\n            default:\n                // If we got here, the user's action was not recognized.\n                // Invoke the superclass to handle it.\n                return super.onOptionsItemSelected(item);\n        }\n    }\n\n    private Bitmap getBitmap(String path) {\n\n        Uri uri = Uri.fromFile(new File(path));\n        InputStream in = null;\n        try {\n            final int IMAGE_MAX_SIZE = 1200000; // 1.2MP\n            in = getContentResolver().openInputStream(uri);\n\n            // Decode image size\n            BitmapFactory.Options o = new BitmapFactory.Options();\n            o.inJustDecodeBounds = true;\n            BitmapFactory.decodeStream(in, null, o);\n            in.close();\n\n            int scale = 1;\n            while ((o.outWidth * o.outHeight) * (1 / Math.pow(scale, 2)) >\n                    IMAGE_MAX_SIZE) {\n                scale++;\n            }\n\n            Bitmap bitmap = null;\n            in = getContentResolver().openInputStream(uri);\n            if (scale > 1) {\n                scale--;\n                // scale to max possible inSampleSize that still yields an image\n                // larger than target\n                o = new BitmapFactory.Options();\n                o.inSampleSize = scale;\n                bitmap = BitmapFactory.decodeStream(in, null, o);\n\n                // resize to desired dimensions\n                int height = bitmap.getHeight();\n                int width = bitmap.getWidth();\n\n                double y = Math.sqrt(IMAGE_MAX_SIZE\n                        / (((double) width) / height));\n                double x = (y / height) * width;\n\n                Bitmap scaledBitmap = Bitmap.createScaledBitmap(bitmap, (int) x,\n                        (int) y, true);\n                bitmap.recycle();\n                bitmap = scaledBitmap;\n\n                System.gc();\n            } else {\n                bitmap = BitmapFactory.decodeStream(in);\n            }\n            in.close();\n\n            Log.d(TAG, \"bitmap size - width: \" + bitmap.getWidth() + \", height: \" +\n                    bitmap.getHeight());\n            return bitmap;\n        } catch (IOException e) {\n            Log.e(TAG, e.getMessage(), e);\n            return null;\n        }\n    }\n\n    /**\n     * If we are in albumsMode, make the albums recyclerView visible. If we are not, make media recyclerView visible.\n     *\n     * @param albumsMode it indicates whether we are in album selection mode or not\n     */\n    private void toggleRecyclersVisibility(boolean albumsMode) {\n        rvAlbums.setVisibility(albumsMode ? View.VISIBLE : View.GONE);\n        rvMedia.setVisibility(albumsMode ? View.GONE : View.VISIBLE);\n        //touchScrollBar.setScrollBarHidden(albumsMode);\n\n    }\n\n    /**\n     * handles back presses.\n     * If we are currently in selection mode, back press will take us out of selection mode.\n     * If we are not in selection mode but in albumsMode and the drawer is open, back press will close it.\n     * If we are not in selection mode but in albumsMode and the drawer is closed, finish the activity.\n     * If we are neither in selection mode nor in albumsMode, display the albums again.\n     */\n    @Override\n    public void onBackPressed() {\n        if(editMode && all_photos)\n            clearSelectedPhotos();\n        if (editMode) finishEditMode();\n        else {\n            if (albumsMode) {\n                if (mDrawerLayout.isDrawerOpen(GravityCompat.START))\n                    mDrawerLayout.closeDrawer(GravityCompat.START);\n                else\n                {\n                    if(doubleBackToExitPressedOnce && isTaskRoot())\n                        finish();\n                    else if(isTaskRoot())\n                    {\n                        doubleBackToExitPressedOnce = true;\n                        Toast.makeText(this, R.string.press_back_again_to_exit, Toast.LENGTH_SHORT).show();\n\n                        new Handler().postDelayed(new Runnable() {\n                            @Override\n                            public void run() {\n                                doubleBackToExitPressedOnce = false;\n                            }\n                        }, 2000);\n                    }\n                    else\n                        super.onBackPressed();\n                }\n            } else {\n                displayAlbums();\n                setRecentApp(getString(R.string.app_name));\n            }\n        }\n    }\n\n\n    private class PrepareAlbumTask extends AsyncTask<Void, Integer, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            swipeRefreshLayout.setRefreshing(true);\n            toggleRecyclersVisibility(true);\n            super.onPreExecute();\n        }\n\n        @Override\n        protected Void doInBackground(Void... arg0) {\n            getAlbums().loadAlbums(getApplicationContext(), hidden);\n            return null;\n        }\n\n        @Override\n        protected void onPostExecute(Void result) {\n            albumsAdapter.swapDataSet(getAlbums().dispAlbums);\n            checkNothing();\n            swipeRefreshLayout.setRefreshing(false);\n            getAlbums().saveBackup(getApplicationContext());\n            invalidateOptionsMenu();\n            finishEditMode();\n        }\n    }\n\n    private class PreparePhotosTask extends AsyncTask<Void, Void, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            swipeRefreshLayout.setRefreshing(true);\n            toggleRecyclersVisibility(false);\n            super.onPreExecute();\n        }\n\n        @Override\n        protected Void doInBackground(Void... arg0) {\n            getAlbum().updatePhotos(getApplicationContext());\n            return null;\n        }\n\n        @Override\n        protected void onPostExecute(Void result) {\n            mediaAdapter.swapDataSet(getAlbum().getMedia());\n            if (!hidden)\n                HandlingAlbums.addAlbumToBackup(getApplicationContext(), getAlbum());\n            checkNothing();\n            swipeRefreshLayout.setRefreshing(false);\n            invalidateOptionsMenu();\n            finishEditMode();\n        }\n    }\n\n    private class PrepareAllPhotos extends AsyncTask<Void, Void, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            swipeRefreshLayout.setRefreshing(true);\n            toggleRecyclersVisibility(false);\n            super.onPreExecute();\n        }\n\n        @Override\n        protected Void doInBackground(Void... arg0) {\n            getAlbum().updatePhotos(getApplicationContext());\n            return null;\n        }\n\n        @Override\n        protected void onPostExecute(Void result) {\n            listAll = StorageProvider.getAllShownImages(LFMainActivity.this);\n            Collections.sort(listAll, MediaComparators.getComparator(getAlbum().settings.getSortingMode(), getAlbum().settings.getSortingOrder()));\n            mediaAdapter.swapDataSet(listAll);\n            if (!hidden)\n                HandlingAlbums.addAlbumToBackup(getApplicationContext(), getAlbum());\n            checkNothing();\n            swipeRefreshLayout.setRefreshing(false);\n            invalidateOptionsMenu();\n            finishEditMode();\n            toolbar.setTitle(getString(R.string.all_media));\n            clearSelectedPhotos();\n        }\n    }\n\n    /*\n    Async Class for Sorting Photos - NOT listAll\n     */\n    private class SortingUtilsPhtots extends AsyncTask<Void, Void, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            super.onPreExecute();\n            swipeRefreshLayout.setRefreshing(true);\n        }\n\n        @Override\n        protected Void doInBackground(Void... aVoid) {\n            getAlbum().sortPhotos();\n            return null;\n        }\n\n        protected void onPostExecute(Void aVoid) {\n            super.onPostExecute(aVoid);\n            swipeRefreshLayout.setRefreshing(false);\n            mediaAdapter.swapDataSet(getAlbum().getMedia());\n        }\n    }\n\n    /*\n    Async Class for Sorting Photos - listAll\n     */\n    private class SortingUtilsListAll extends AsyncTask<Void, Void, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            super.onPreExecute();\n            swipeRefreshLayout.setRefreshing(true);\n        }\n\n        @Override\n        protected Void doInBackground(Void... aVoid) {\n            Collections.sort(listAll, MediaComparators.getComparator(getAlbum().settings.getSortingMode(), getAlbum().settings.getSortingOrder()));\n            return null;\n        }\n\n        @Override\n        protected void onPostExecute(Void aVoid) {\n            super.onPostExecute(aVoid);\n            swipeRefreshLayout.setRefreshing(false);\n            mediaAdapter.swapDataSet(listAll);\n        }\n    }\n\n    /*\n    Async Class for Sorting Albums\n     */\n    private class SortingUtilsAlbums extends AsyncTask<Void, Void, Void> {\n\n        @Override\n        protected void onPreExecute() {\n            super.onPreExecute();\n            swipeRefreshLayout.setRefreshing(true);\n        }\n\n        @Override\n        protected Void doInBackground(Void... aVoid) {\n            getAlbums().sortAlbums(getApplicationContext());\n            return null;\n        }\n\n        @Override\n        protected void onPostExecute(Void aVoid) {\n            super.onPostExecute(aVoid);\n            swipeRefreshLayout.setRefreshing(false);\n            albumsAdapter.swapDataSet(getAlbums().dispAlbums);\n            new PrepareAlbumTask().execute();\n        }\n    }\n}\n", "idx": 1, "id": 11575, "msg": "", "proj": "fossasia-phimpme-android", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -1363,5 +1363,6 @@ func createMetricsSnapshotView(ss *im.Snapshot) *topology.MetricSnapshotView {\n \t\tConnectionTotalDuration:    ss.ConnectionTotalDuration.Truncate(time.Second).Seconds(),\n \t\tSessionConnectionDuration:  ss.SessionConnectionDuration.Truncate(time.Second).Seconds(),\n \t\tSessionConnectionDirection: string(ss.SessionConnectionDirection),\n+\t\tLatencyEWMA:                ss.LatencyEWMA.Milliseconds(),\n \t}\n }", "y": 0, "oldf": "// Copyright 2020 The Swarm Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage kademlia\n\nimport (\n\t\"context\"\n\trandom \"crypto/rand\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"math/big\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ethersphere/bee/pkg/addressbook\"\n\t\"github.com/ethersphere/bee/pkg/discovery\"\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/p2p\"\n\t\"github.com/ethersphere/bee/pkg/shed\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n\t\"github.com/ethersphere/bee/pkg/topology\"\n\tim \"github.com/ethersphere/bee/pkg/topology/kademlia/internal/metrics\"\n\t\"github.com/ethersphere/bee/pkg/topology/kademlia/internal/waitnext\"\n\t\"github.com/ethersphere/bee/pkg/topology/pslice\"\n\tma \"github.com/multiformats/go-multiaddr\"\n)\n\nconst (\n\tnnLowWatermark         = 2 // the number of peers in consecutive deepest bins that constitute as nearest neighbours\n\tmaxConnAttempts        = 1 // when there is maxConnAttempts failed connect calls for a given peer it is considered non-connectable\n\tmaxBootNodeAttempts    = 3 // how many attempts to dial to boot-nodes before giving up\n\tdefaultBitSuffixLength = 3 // the number of bits used to create pseudo addresses for balancing\n\n\taddPeerBatchSize = 500\n\n\tpeerConnectionAttemptTimeout = 5 * time.Second // Timeout for establishing a new connection with peer.\n)\n\nvar (\n\tquickSaturationPeers        = 4\n\tsaturationPeers             = 8\n\toverSaturationPeers         = 20\n\tbootNodeOverSaturationPeers = 20\n\tshortRetry                  = 30 * time.Second\n\ttimeToRetry                 = 2 * shortRetry\n\tbroadcastBinSize            = 4\n)\n\nvar (\n\terrOverlayMismatch   = errors.New(\"overlay mismatch\")\n\terrPruneEntry        = errors.New(\"prune entry\")\n\terrEmptyBin          = errors.New(\"empty bin\")\n\terrAnnounceLightNode = errors.New(\"announcing light node\")\n)\n\ntype (\n\tbinSaturationFunc  func(bin uint8, peers, connected *pslice.PSlice) (saturated bool, oversaturated bool)\n\tsanctionedPeerFunc func(peer swarm.Address) bool\n\tpruneFunc          func(depth uint8)\n)\n\nvar noopSanctionedPeerFn = func(_ swarm.Address) bool { return false }\n\n// Options for injecting services to Kademlia.\ntype Options struct {\n\tSaturationFunc  binSaturationFunc\n\tBootnodes       []ma.Multiaddr\n\tBootnodeMode    bool\n\tBitSuffixLength int\n\tPruneFunc       pruneFunc\n}\n\n// Kad is the Swarm forwarding kademlia implementation.\ntype Kad struct {\n\tbase              swarm.Address         // this node's overlay address\n\tdiscovery         discovery.Driver      // the discovery driver\n\taddressBook       addressbook.Interface // address book to get underlays\n\tp2p               p2p.Service           // p2p service to connect to nodes with\n\tsaturationFunc    binSaturationFunc     // pluggable saturation function\n\tbitSuffixLength   int                   // additional depth of common prefix for bin\n\tcommonBinPrefixes [][]swarm.Address     // list of address prefixes for each bin\n\tconnectedPeers    *pslice.PSlice        // a slice of peers sorted and indexed by po, indexes kept in `bins`\n\tknownPeers        *pslice.PSlice        // both are po aware slice of addresses\n\tbootnodes         []ma.Multiaddr\n\tdepth             uint8         // current neighborhood depth\n\tradius            uint8         // storage area of responsibility\n\tdepthMu           sync.RWMutex  // protect depth changes\n\tmanageC           chan struct{} // trigger the manage forever loop to connect to new peers\n\tpeerSig           []chan struct{}\n\tpeerSigMtx        sync.Mutex\n\tlogger            logging.Logger // logger\n\tbootnode          bool           // indicates whether the node is working in bootnode mode\n\tcollector         *im.Collector\n\tquit              chan struct{} // quit channel\n\thalt              chan struct{} // halt channel\n\tdone              chan struct{} // signal that `manage` has quit\n\twg                sync.WaitGroup\n\twaitNext          *waitnext.WaitNext\n\tmetrics           metrics\n\tpruneFunc         pruneFunc // pluggable prune function\n}\n\n// New returns a new Kademlia.\nfunc New(\n\tbase swarm.Address,\n\taddressbook addressbook.Interface,\n\tdiscovery discovery.Driver,\n\tp2p p2p.Service,\n\tmetricsDB *shed.DB,\n\tlogger logging.Logger,\n\to Options,\n) (*Kad, error) {\n\tif o.SaturationFunc == nil {\n\t\tos := overSaturationPeers\n\t\tif o.BootnodeMode {\n\t\t\tos = bootNodeOverSaturationPeers\n\t\t}\n\t\to.SaturationFunc = binSaturated(os)\n\t}\n\tif o.BitSuffixLength == 0 {\n\t\to.BitSuffixLength = defaultBitSuffixLength\n\t}\n\n\tstart := time.Now()\n\timc, err := im.NewCollector(metricsDB)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlogger.Debugf(\"kademlia: NewCollector(...) took %v\", time.Since(start))\n\n\tk := &Kad{\n\t\tbase:              base,\n\t\tdiscovery:         discovery,\n\t\taddressBook:       addressbook,\n\t\tp2p:               p2p,\n\t\tsaturationFunc:    o.SaturationFunc,\n\t\tbitSuffixLength:   o.BitSuffixLength,\n\t\tcommonBinPrefixes: make([][]swarm.Address, int(swarm.MaxBins)),\n\t\tconnectedPeers:    pslice.New(int(swarm.MaxBins), base),\n\t\tknownPeers:        pslice.New(int(swarm.MaxBins), base),\n\t\tbootnodes:         o.Bootnodes,\n\t\tmanageC:           make(chan struct{}, 1),\n\t\twaitNext:          waitnext.New(),\n\t\tlogger:            logger,\n\t\tbootnode:          o.BootnodeMode,\n\t\tcollector:         imc,\n\t\tquit:              make(chan struct{}),\n\t\thalt:              make(chan struct{}),\n\t\tdone:              make(chan struct{}),\n\t\twg:                sync.WaitGroup{},\n\t\tmetrics:           newMetrics(),\n\t\tpruneFunc:         o.PruneFunc,\n\t}\n\n\tif k.pruneFunc == nil {\n\t\tk.pruneFunc = k.pruneOversaturatedBins\n\t}\n\n\tif k.bitSuffixLength > 0 {\n\t\tk.commonBinPrefixes = generateCommonBinPrefixes(k.base, k.bitSuffixLength)\n\t}\n\n\treturn k, nil\n}\n\ntype peerConnInfo struct {\n\tpo   uint8\n\taddr swarm.Address\n}\n\n// connectBalanced attempts to connect to the balanced peers first.\nfunc (k *Kad) connectBalanced(wg *sync.WaitGroup, peerConnChan chan<- *peerConnInfo) {\n\tskipPeers := func(peer swarm.Address) bool {\n\t\tif k.waitNext.Waiting(peer) {\n\t\t\tk.metrics.TotalBeforeExpireWaits.Inc()\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t}\n\n\tdepth := k.NeighborhoodDepth()\n\n\tfor i := range k.commonBinPrefixes {\n\n\t\tbinPeersLength := k.knownPeers.BinSize(uint8(i))\n\n\t\t// balancer should skip on bins where neighborhood connector would connect to peers anyway\n\t\t// and there are not enough peers in known addresses to properly balance the bin\n\t\tif i >= int(depth) && binPeersLength < len(k.commonBinPrefixes[i]) {\n\t\t\tcontinue\n\t\t}\n\n\t\tbinPeers := k.knownPeers.BinPeers(uint8(i))\n\n\t\tfor j := range k.commonBinPrefixes[i] {\n\t\t\tpseudoAddr := k.commonBinPrefixes[i][j]\n\n\t\t\tclosestConnectedPeer, err := closestPeer(k.connectedPeers, pseudoAddr, noopSanctionedPeerFn)\n\t\t\tif err != nil {\n\t\t\t\tif errors.Is(err, topology.ErrNotFound) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tk.logger.Errorf(\"closest connected peer: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tclosestConnectedPO := swarm.ExtendedProximity(closestConnectedPeer.Bytes(), pseudoAddr.Bytes())\n\t\t\tif int(closestConnectedPO) >= i+k.bitSuffixLength+1 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Connect to closest known peer which we haven't tried connecting to recently.\n\t\t\tclosestKnownPeer, err := closestPeerInSlice(binPeers, pseudoAddr, skipPeers)\n\t\t\tif err != nil {\n\t\t\t\tif errors.Is(err, topology.ErrNotFound) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tk.logger.Errorf(\"closest known peer: %v\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif k.connectedPeers.Exists(closestKnownPeer) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tclosestKnownPeerPO := swarm.ExtendedProximity(closestKnownPeer.Bytes(), pseudoAddr.Bytes())\n\t\t\tif int(closestKnownPeerPO) < i+k.bitSuffixLength+1 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-k.quit:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\twg.Add(1)\n\t\t\t\tpeerConnChan <- &peerConnInfo{\n\t\t\t\t\tpo:   swarm.Proximity(k.base.Bytes(), closestKnownPeer.Bytes()),\n\t\t\t\t\taddr: closestKnownPeer,\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// connectNeighbours attempts to connect to the neighbours\n// which were not considered by the connectBalanced method.\nfunc (k *Kad) connectNeighbours(wg *sync.WaitGroup, peerConnChan chan<- *peerConnInfo) {\n\n\tsent := 0\n\tvar currentPo uint8 = 0\n\n\t_ = k.knownPeers.EachBinRev(func(addr swarm.Address, po uint8) (bool, bool, error) {\n\t\tdepth := k.NeighborhoodDepth()\n\n\t\t// out of depth, skip bin\n\t\tif po < depth {\n\t\t\treturn false, true, nil\n\t\t}\n\n\t\tif po != currentPo {\n\t\t\tcurrentPo = po\n\t\t\tsent = 0\n\t\t}\n\n\t\tif k.connectedPeers.Exists(addr) {\n\t\t\treturn false, false, nil\n\t\t}\n\n\t\tif k.waitNext.Waiting(addr) {\n\t\t\tk.metrics.TotalBeforeExpireWaits.Inc()\n\t\t\treturn false, false, nil\n\t\t}\n\n\t\tselect {\n\t\tcase <-k.quit:\n\t\t\treturn true, false, nil\n\t\tdefault:\n\t\t\twg.Add(1)\n\t\t\tpeerConnChan <- &peerConnInfo{\n\t\t\t\tpo:   po,\n\t\t\t\taddr: addr,\n\t\t\t}\n\t\t\tsent++\n\t\t}\n\n\t\t// We want 'sent' equal to 'saturationPeers'\n\t\t// in order to skip to the next bin and speed up the topology build.\n\t\treturn false, sent == saturationPeers, nil\n\t})\n}\n\n// connectionAttemptsHandler handles the connection attempts\n// to peers sent by the producers to the peerConnChan.\nfunc (k *Kad) connectionAttemptsHandler(ctx context.Context, wg *sync.WaitGroup, neighbourhoodChan, balanceChan <-chan *peerConnInfo) {\n\tconnect := func(peer *peerConnInfo) {\n\t\tbzzAddr, err := k.addressBook.Get(peer.addr)\n\t\tswitch {\n\t\tcase errors.Is(err, addressbook.ErrNotFound):\n\t\t\tk.logger.Debugf(\"kademlia: empty address book entry for peer %q\", peer.addr)\n\t\t\tk.knownPeers.Remove(peer.addr)\n\t\t\treturn\n\t\tcase err != nil:\n\t\t\tk.logger.Debugf(\"kademlia: failed to get address book entry for peer %q: %v\", peer.addr, err)\n\t\t\treturn\n\t\t}\n\n\t\tremove := func(peer *peerConnInfo) {\n\t\t\tk.waitNext.Remove(peer.addr)\n\t\t\tk.knownPeers.Remove(peer.addr)\n\t\t\tif err := k.addressBook.Remove(peer.addr); err != nil {\n\t\t\t\tk.logger.Debugf(\"kademlia: could not remove peer %q from addressbook\", peer.addr)\n\t\t\t}\n\t\t}\n\n\t\tswitch err = k.connect(ctx, peer.addr, bzzAddr.Underlay); {\n\t\tcase errors.Is(err, errPruneEntry):\n\t\t\tk.logger.Debugf(\"kademlia: dial to light node with overlay %q and underlay %q\", peer.addr, bzzAddr.Underlay)\n\t\t\tremove(peer)\n\t\t\treturn\n\t\tcase errors.Is(err, errOverlayMismatch):\n\t\t\tk.logger.Debugf(\"kademlia: overlay mismatch has occurred to an overlay %q with underlay %q\", peer.addr, bzzAddr.Underlay)\n\t\t\tremove(peer)\n\t\t\treturn\n\t\tcase err != nil:\n\t\t\tk.logger.Debugf(\"kademlia: peer not reachable from kademlia %q: %v\", bzzAddr, err)\n\t\t\tk.logger.Warningf(\"peer not reachable when attempting to connect\")\n\t\t\treturn\n\t\t}\n\n\t\tk.waitNext.Set(peer.addr, time.Now().Add(shortRetry), 0)\n\n\t\tk.connectedPeers.Add(peer.addr)\n\n\t\tk.metrics.TotalOutboundConnections.Inc()\n\t\tk.collector.Record(peer.addr, im.PeerLogIn(time.Now(), im.PeerConnectionDirectionOutbound))\n\n\t\tk.depthMu.Lock()\n\t\tk.depth = recalcDepth(k.connectedPeers, k.radius)\n\t\tk.depthMu.Unlock()\n\n\t\tk.logger.Debugf(\"kademlia: connected to peer: %q in bin: %d\", peer.addr, peer.po)\n\t\tk.notifyManageLoop()\n\t\tk.notifyPeerSig()\n\t}\n\n\tvar (\n\t\t// The inProgress helps to avoid making a connection\n\t\t// to a peer who has the connection already in progress.\n\t\tinProgress   = make(map[string]bool)\n\t\tinProgressMu sync.Mutex\n\t)\n\tconnAttempt := func(peerConnChan <-chan *peerConnInfo) {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-k.quit:\n\t\t\t\treturn\n\t\t\tcase peer := <-peerConnChan:\n\t\t\t\taddr := peer.addr.String()\n\n\t\t\t\tif k.waitNext.Waiting(peer.addr) {\n\t\t\t\t\tk.metrics.TotalBeforeExpireWaits.Inc()\n\t\t\t\t\twg.Done()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tinProgressMu.Lock()\n\t\t\t\tif !inProgress[addr] {\n\t\t\t\t\tinProgress[addr] = true\n\t\t\t\t\tinProgressMu.Unlock()\n\t\t\t\t\tconnect(peer)\n\t\t\t\t\tinProgressMu.Lock()\n\t\t\t\t\tdelete(inProgress, addr)\n\t\t\t\t}\n\t\t\t\tinProgressMu.Unlock()\n\t\t\t\twg.Done()\n\t\t\t}\n\t\t}\n\t}\n\tfor i := 0; i < 16; i++ {\n\t\tgo connAttempt(balanceChan)\n\t}\n\tfor i := 0; i < 32; i++ {\n\t\tgo connAttempt(neighbourhoodChan)\n\t}\n}\n\n// notifyManageLoop notifies kademlia manage loop.\nfunc (k *Kad) notifyManageLoop() {\n\tselect {\n\tcase k.manageC <- struct{}{}:\n\tdefault:\n\t}\n}\n\n// manage is a forever loop that manages the connection to new peers\n// once they get added or once others leave.\nfunc (k *Kad) manage() {\n\tdefer k.wg.Done()\n\tdefer close(k.done)\n\tdefer k.logger.Debugf(\"kademlia manage loop exited\")\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tgo func() {\n\t\t<-k.quit\n\t\tcancel()\n\t}()\n\n\t// The wg makes sure that we wait for all the connection attempts,\n\t// spun up by goroutines, to finish before we try the boot-nodes.\n\tvar wg sync.WaitGroup\n\tneighbourhoodChan := make(chan *peerConnInfo)\n\tbalanceChan := make(chan *peerConnInfo)\n\tgo k.connectionAttemptsHandler(ctx, &wg, neighbourhoodChan, balanceChan)\n\n\tk.wg.Add(1)\n\tgo func() {\n\t\tdefer k.wg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-k.halt:\n\t\t\t\treturn\n\t\t\tcase <-k.quit:\n\t\t\t\treturn\n\t\t\tcase <-time.After(5 * time.Minute):\n\t\t\t\tstart := time.Now()\n\t\t\t\tk.logger.Tracef(\"kademlia: starting to flush metrics at %s\", start)\n\t\t\t\tif err := k.collector.Flush(); err != nil {\n\t\t\t\t\tk.metrics.InternalMetricsFlushTotalErrors.Inc()\n\t\t\t\t\tk.logger.Debugf(\"kademlia: unable to flush metrics counters to the persistent store: %v\", err)\n\t\t\t\t} else {\n\t\t\t\t\tk.metrics.InternalMetricsFlushTime.Observe(time.Since(start).Seconds())\n\t\t\t\t\tk.logger.Tracef(\"kademlia: took %s to flush\", time.Since(start))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor {\n\t\tselect {\n\t\tcase <-k.quit:\n\t\t\treturn\n\t\tcase <-time.After(15 * time.Second):\n\t\t\tk.notifyManageLoop()\n\t\tcase <-k.manageC:\n\t\t\tstart := time.Now()\n\n\t\t\tselect {\n\t\t\tcase <-k.halt:\n\t\t\t\t// halt stops dial-outs while shutting down\n\t\t\t\treturn\n\t\t\tcase <-k.quit:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tif k.bootnode {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\toldDepth := k.NeighborhoodDepth()\n\t\t\tk.connectBalanced(&wg, balanceChan)\n\t\t\tk.connectNeighbours(&wg, neighbourhoodChan)\n\t\t\twg.Wait()\n\n\t\t\tk.depthMu.Lock()\n\t\t\tdepth := k.depth\n\t\t\tradius := k.radius\n\t\t\tk.depthMu.Unlock()\n\n\t\t\tk.pruneFunc(depth)\n\n\t\t\tk.logger.Tracef(\n\t\t\t\t\"kademlia: connector took %s to finish: old depth %d; new depth %d\",\n\t\t\t\ttime.Since(start),\n\t\t\t\toldDepth,\n\t\t\t\tdepth,\n\t\t\t)\n\n\t\t\tk.metrics.CurrentDepth.Set(float64(depth))\n\t\t\tk.metrics.CurrentRadius.Set(float64(radius))\n\t\t\tk.metrics.CurrentlyKnownPeers.Set(float64(k.knownPeers.Length()))\n\t\t\tk.metrics.CurrentlyConnectedPeers.Set(float64(k.connectedPeers.Length()))\n\n\t\t\tif k.connectedPeers.Length() == 0 {\n\t\t\t\tselect {\n\t\t\t\tcase <-k.halt:\n\t\t\t\t\tcontinue\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tk.logger.Debug(\"kademlia: no connected peers, trying bootnodes\")\n\t\t\t\tk.connectBootNodes(ctx)\n\t\t\t}\n\t\t}\n\t}\n}\n\n// PruneOversaturatedBins disconnects out of depth peers from oversaturated bins\n// while maintaining the balance of the bin and favoring peers with longers connections\nfunc (k *Kad) pruneOversaturatedBins(depth uint8) {\n\n\tfor i := range k.commonBinPrefixes {\n\n\t\tif i >= int(depth) {\n\t\t\treturn\n\t\t}\n\n\t\tbinPeersCount := k.connectedPeers.BinSize(uint8(i))\n\t\tif binPeersCount < overSaturationPeers {\n\t\t\tcontinue\n\t\t}\n\n\t\tbinPeers := k.connectedPeers.BinPeers(uint8(i))\n\n\t\tpeersToRemove := binPeersCount - overSaturationPeers\n\n\t\tfor j := 0; peersToRemove > 0 && j < len(k.commonBinPrefixes[i]); j++ {\n\n\t\t\tpseudoAddr := k.commonBinPrefixes[i][j]\n\t\t\tpeers := k.balancedSlotPeers(pseudoAddr, binPeers, i)\n\n\t\t\tif len(peers) <= 1 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar smallestDuration time.Duration\n\t\t\tvar newestPeer swarm.Address\n\t\t\tfor _, peer := range peers {\n\t\t\t\tduration := k.collector.Inspect(peer).SessionConnectionDuration\n\t\t\t\tif smallestDuration == 0 || duration < smallestDuration {\n\t\t\t\t\tsmallestDuration = duration\n\t\t\t\t\tnewestPeer = peer\n\t\t\t\t}\n\t\t\t}\n\t\t\terr := k.p2p.Disconnect(newestPeer, \"pruned from oversaturated bin\")\n\t\t\tif err != nil {\n\t\t\t\tk.logger.Debugf(\"prune disconnect fail %v\", err)\n\t\t\t}\n\t\t\tpeersToRemove--\n\t\t}\n\t}\n}\n\nfunc (k *Kad) balancedSlotPeers(pseudoAddr swarm.Address, peers []swarm.Address, po int) []swarm.Address {\n\n\tvar ret []swarm.Address\n\n\tfor _, peer := range peers {\n\t\tpeerPo := swarm.ExtendedProximity(peer.Bytes(), pseudoAddr.Bytes())\n\t\tif int(peerPo) >= po+k.bitSuffixLength+1 {\n\t\t\tret = append(ret, peer)\n\t\t}\n\t}\n\n\treturn ret\n}\n\nfunc (k *Kad) Start(_ context.Context) error {\n\tk.wg.Add(1)\n\tgo k.manage()\n\n\tgo func() {\n\t\tselect {\n\t\tcase <-k.halt:\n\t\t\treturn\n\t\tcase <-k.quit:\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\t\tvar (\n\t\t\tstart     = time.Now()\n\t\t\taddresses []swarm.Address\n\t\t)\n\n\t\terr := k.addressBook.IterateOverlays(func(addr swarm.Address) (stop bool, err error) {\n\t\t\taddresses = append(addresses, addr)\n\t\t\tif len(addresses) == addPeerBatchSize {\n\t\t\t\tk.AddPeers(addresses...)\n\t\t\t\taddresses = nil\n\t\t\t}\n\t\t\treturn false, nil\n\t\t})\n\t\tif err != nil {\n\t\t\tk.logger.Errorf(\"addressbook overlays: %w\", err)\n\t\t\treturn\n\t\t}\n\t\tk.AddPeers(addresses...)\n\t\tk.metrics.StartAddAddressBookOverlaysTime.Observe(time.Since(start).Seconds())\n\t}()\n\n\t// trigger the first manage loop immediately so that\n\t// we can start connecting to the bootnode quickly\n\tk.notifyManageLoop()\n\n\treturn nil\n}\n\nfunc (k *Kad) connectBootNodes(ctx context.Context) {\n\tvar attempts, connected int\n\ttotalAttempts := maxBootNodeAttempts * len(k.bootnodes)\n\n\tctx, cancel := context.WithTimeout(ctx, 15*time.Second)\n\tdefer cancel()\n\n\tfor _, addr := range k.bootnodes {\n\t\tif attempts >= totalAttempts || connected >= 3 {\n\t\t\treturn\n\t\t}\n\n\t\tif _, err := p2p.Discover(ctx, addr, func(addr ma.Multiaddr) (stop bool, err error) {\n\t\t\tk.logger.Tracef(\"connecting to bootnode %s\", addr)\n\t\t\tif attempts >= maxBootNodeAttempts {\n\t\t\t\treturn true, nil\n\t\t\t}\n\t\t\tbzzAddress, err := k.p2p.Connect(ctx, addr)\n\n\t\t\tattempts++\n\t\t\tk.metrics.TotalBootNodesConnectionAttempts.Inc()\n\n\t\t\tif err != nil {\n\t\t\t\tif !errors.Is(err, p2p.ErrAlreadyConnected) {\n\t\t\t\t\tk.logger.Debugf(\"connect fail %s: %v\", addr, err)\n\t\t\t\t\tk.logger.Warningf(\"connect to bootnode %s\", addr)\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\t\t\t\tk.logger.Debugf(\"connect to bootnode fail: %v\", err)\n\t\t\t\treturn false, nil\n\t\t\t}\n\n\t\t\tif err := k.onConnected(ctx, bzzAddress.Overlay); err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tk.logger.Tracef(\"connected to bootnode %s\", addr)\n\t\t\tconnected++\n\t\t\t// connect to max 3 bootnodes\n\t\t\treturn connected >= 3, nil\n\t\t}); err != nil && !errors.Is(err, context.Canceled) {\n\t\t\tk.logger.Debugf(\"discover fail %s: %v\", addr, err)\n\t\t\tk.logger.Warningf(\"discover to bootnode %s\", addr)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// binSaturated indicates whether a certain bin is saturated or not.\n// when a bin is not saturated it means we would like to proactively\n// initiate connections to other peers in the bin.\nfunc binSaturated(oversaturationAmount int) binSaturationFunc {\n\treturn func(bin uint8, peers, connected *pslice.PSlice) (bool, bool) {\n\t\tpotentialDepth := recalcDepth(peers, swarm.MaxPO)\n\n\t\t// short circuit for bins which are >= depth\n\t\tif bin >= potentialDepth {\n\t\t\treturn false, false\n\t\t}\n\n\t\t// lets assume for now that the minimum number of peers in a bin\n\t\t// would be 2, under which we would always want to connect to new peers\n\t\t// obviously this should be replaced with a better optimization\n\t\t// the iterator is used here since when we check if a bin is saturated,\n\t\t// the plain number of size of bin might not suffice (for example for squared\n\t\t// gaps measurement)\n\n\t\tsize := 0\n\t\t_ = connected.EachBin(func(_ swarm.Address, po uint8) (bool, bool, error) {\n\t\t\tif po == bin {\n\t\t\t\tsize++\n\t\t\t}\n\t\t\treturn false, false, nil\n\t\t})\n\n\t\treturn size >= saturationPeers, size >= oversaturationAmount\n\t}\n}\n\n// recalcDepth calculates and returns the kademlia depth.\nfunc recalcDepth(peers *pslice.PSlice, radius uint8) uint8 {\n\t// handle edge case separately\n\tif peers.Length() <= nnLowWatermark {\n\t\treturn 0\n\t}\n\tvar (\n\t\tpeersCtr                     = uint(0)\n\t\tcandidate                    = uint8(0)\n\t\tshallowestEmpty, noEmptyBins = peers.ShallowestEmpty()\n\t)\n\n\tshallowestUnsaturated := uint8(0)\n\tbinCount := 0\n\t_ = peers.EachBinRev(func(_ swarm.Address, bin uint8) (bool, bool, error) {\n\t\tif bin == shallowestUnsaturated {\n\t\t\tbinCount++\n\t\t\treturn false, false, nil\n\t\t}\n\t\tif bin > shallowestUnsaturated && binCount < quickSaturationPeers {\n\t\t\t// this means we have less than quickSaturationPeers in the previous bin\n\t\t\t// therefore we can return assuming that bin is the unsaturated one.\n\t\t\treturn true, false, nil\n\t\t}\n\t\tshallowestUnsaturated = bin\n\t\tbinCount = 1\n\n\t\treturn false, false, nil\n\t})\n\n\t// if there are some empty bins and the shallowestEmpty is\n\t// smaller than the shallowestUnsaturated then set shallowest\n\t// unsaturated to the empty bin.\n\tif !noEmptyBins && shallowestEmpty < shallowestUnsaturated {\n\t\tshallowestUnsaturated = shallowestEmpty\n\t}\n\n\t_ = peers.EachBin(func(_ swarm.Address, po uint8) (bool, bool, error) {\n\t\tpeersCtr++\n\t\tif peersCtr >= nnLowWatermark {\n\t\t\tcandidate = po\n\t\t\treturn true, false, nil\n\t\t}\n\t\treturn false, false, nil\n\t})\n\tif shallowestUnsaturated > candidate {\n\t\tif radius < candidate {\n\t\t\treturn radius\n\t\t}\n\t\treturn candidate\n\t}\n\n\tif radius < shallowestUnsaturated {\n\t\treturn radius\n\t}\n\treturn shallowestUnsaturated\n}\n\n// connect connects to a peer and gossips its address to our connected peers,\n// as well as sends the peers we are connected to to the newly connected peer\nfunc (k *Kad) connect(ctx context.Context, peer swarm.Address, ma ma.Multiaddr) error {\n\tk.logger.Infof(\"attempting to connect to peer %q\", peer)\n\n\tctx, cancel := context.WithTimeout(ctx, peerConnectionAttemptTimeout)\n\tdefer cancel()\n\n\tk.metrics.TotalOutboundConnectionAttempts.Inc()\n\n\tswitch i, err := k.p2p.Connect(ctx, ma); {\n\tcase errors.Is(err, p2p.ErrDialLightNode):\n\t\treturn errPruneEntry\n\tcase errors.Is(err, p2p.ErrAlreadyConnected):\n\t\tif !i.Overlay.Equal(peer) {\n\t\t\treturn errOverlayMismatch\n\t\t}\n\t\treturn nil\n\tcase errors.Is(err, context.Canceled):\n\t\treturn err\n\tcase err != nil:\n\t\tk.logger.Debugf(\"could not connect to peer %q: %v\", peer, err)\n\n\t\tretryTime := time.Now().Add(timeToRetry)\n\t\tvar e *p2p.ConnectionBackoffError\n\t\tfailedAttempts := 0\n\t\tif errors.As(err, &e) {\n\t\t\tretryTime = e.TryAfter()\n\t\t} else {\n\t\t\tfailedAttempts = k.waitNext.Attempts(peer)\n\t\t\tfailedAttempts++\n\t\t}\n\n\t\tk.metrics.TotalOutboundConnectionFailedAttempts.Inc()\n\t\tk.collector.Record(peer, im.IncSessionConnectionRetry())\n\n\t\tss := k.collector.Inspect(peer)\n\t\tquickPrune := ss == nil || ss.HasAtMaxOneConnectionAttempt()\n\t\tif (k.connectedPeers.Length() > 0 && quickPrune) || failedAttempts >= maxConnAttempts {\n\t\t\tk.waitNext.Remove(peer)\n\t\t\tk.knownPeers.Remove(peer)\n\t\t\tif err := k.addressBook.Remove(peer); err != nil {\n\t\t\t\tk.logger.Debugf(\"could not remove peer from addressbook: %q\", peer)\n\t\t\t}\n\t\t\tk.logger.Debugf(\"kademlia pruned peer from address book %q\", peer)\n\t\t} else {\n\t\t\tk.waitNext.Set(peer, retryTime, failedAttempts)\n\t\t}\n\n\t\treturn err\n\tcase !i.Overlay.Equal(peer):\n\t\t_ = k.p2p.Disconnect(peer, errOverlayMismatch.Error())\n\t\t_ = k.p2p.Disconnect(i.Overlay, errOverlayMismatch.Error())\n\t\treturn errOverlayMismatch\n\t}\n\n\treturn k.Announce(ctx, peer, true)\n}\n\n// Announce a newly connected peer to our connected peers, but also\n// notify the peer about our already connected peers\nfunc (k *Kad) Announce(ctx context.Context, peer swarm.Address, fullnode bool) error {\n\tvar addrs []swarm.Address\n\n\tfor bin := uint8(0); bin < swarm.MaxBins; bin++ {\n\n\t\tconnectedPeers, err := randomSubset(k.connectedPeers.BinPeers(bin), broadcastBinSize)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tfor _, connectedPeer := range connectedPeers {\n\t\t\tif connectedPeer.Equal(peer) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\taddrs = append(addrs, connectedPeer)\n\n\t\t\tif !fullnode {\n\t\t\t\t// we continue here so we dont gossip\n\t\t\t\t// about lightnodes to others.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tgo func(connectedPeer swarm.Address) {\n\t\t\t\tif err := k.discovery.BroadcastPeers(ctx, connectedPeer, peer); err != nil {\n\t\t\t\t\tk.logger.Debugf(\"could not gossip peer %s to peer %s: %v\", peer, connectedPeer, err)\n\t\t\t\t}\n\t\t\t}(connectedPeer)\n\t\t}\n\t}\n\n\tif len(addrs) == 0 {\n\t\treturn nil\n\t}\n\n\terr := k.discovery.BroadcastPeers(ctx, peer, addrs...)\n\tif err != nil {\n\t\tk.logger.Errorf(\"kademlia: could not broadcast to peer %s\", peer)\n\t\t_ = k.p2p.Disconnect(peer, \"failed broadcasting to peer\")\n\t}\n\n\treturn err\n}\n\n// AnnounceTo announces a selected peer to another.\nfunc (k *Kad) AnnounceTo(ctx context.Context, addressee, peer swarm.Address, fullnode bool) error {\n\tif !fullnode {\n\t\treturn errAnnounceLightNode\n\t}\n\n\treturn k.discovery.BroadcastPeers(ctx, addressee, peer)\n}\n\n// AddPeers adds peers to the knownPeers list.\n// This does not guarantee that a connection will immediately\n// be made to the peer.\nfunc (k *Kad) AddPeers(addrs ...swarm.Address) {\n\tk.knownPeers.Add(addrs...)\n\tk.notifyManageLoop()\n}\n\nfunc (k *Kad) Pick(peer p2p.Peer) bool {\n\tk.metrics.PickCalls.Inc()\n\tif k.bootnode {\n\t\t// shortcircuit for bootnode mode - always accept connections,\n\t\t// at least until we find a better solution.\n\t\treturn true\n\t}\n\tpo := swarm.Proximity(k.base.Bytes(), peer.Address.Bytes())\n\t_, oversaturated := k.saturationFunc(po, k.knownPeers, k.connectedPeers)\n\t// pick the peer if we are not oversaturated\n\tif !oversaturated {\n\t\treturn true\n\t}\n\tk.metrics.PickCallsFalse.Inc()\n\treturn false\n}\n\n// Connected is called when a peer has dialed in.\n// If forceConnection is true `overSaturated` is ignored for non-bootnodes.\nfunc (k *Kad) Connected(ctx context.Context, peer p2p.Peer, forceConnection bool) error {\n\taddress := peer.Address\n\tpo := swarm.Proximity(k.base.Bytes(), address.Bytes())\n\n\tif _, overSaturated := k.saturationFunc(po, k.knownPeers, k.connectedPeers); overSaturated {\n\t\tif k.bootnode {\n\t\t\trandPeer, err := k.randomPeer(po)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t_ = k.p2p.Disconnect(randPeer, \"kicking out random peer to accommodate node\")\n\t\t\treturn k.onConnected(ctx, address)\n\t\t}\n\t\tif !forceConnection {\n\t\t\treturn topology.ErrOversaturated\n\t\t}\n\t}\n\n\treturn k.onConnected(ctx, address)\n}\n\nfunc (k *Kad) onConnected(ctx context.Context, addr swarm.Address) error {\n\tif err := k.Announce(ctx, addr, true); err != nil {\n\t\treturn err\n\t}\n\n\tk.knownPeers.Add(addr)\n\tk.connectedPeers.Add(addr)\n\n\tk.metrics.TotalInboundConnections.Inc()\n\tk.collector.Record(addr, im.PeerLogIn(time.Now(), im.PeerConnectionDirectionInbound))\n\n\tk.waitNext.Remove(addr)\n\n\tk.depthMu.Lock()\n\tk.depth = recalcDepth(k.connectedPeers, k.radius)\n\tk.depthMu.Unlock()\n\n\tk.notifyManageLoop()\n\tk.notifyPeerSig()\n\treturn nil\n}\n\n// Disconnected is called when peer disconnects.\nfunc (k *Kad) Disconnected(peer p2p.Peer) {\n\tk.logger.Debugf(\"kademlia: disconnected peer %s\", peer.Address)\n\n\tk.connectedPeers.Remove(peer.Address)\n\n\tk.waitNext.SetTryAfter(peer.Address, time.Now().Add(timeToRetry))\n\n\tk.metrics.TotalInboundDisconnections.Inc()\n\tk.collector.Record(peer.Address, im.PeerLogOut(time.Now()))\n\n\tk.depthMu.Lock()\n\tk.depth = recalcDepth(k.connectedPeers, k.radius)\n\tk.depthMu.Unlock()\n\n\tk.notifyManageLoop()\n\tk.notifyPeerSig()\n}\n\nfunc (k *Kad) notifyPeerSig() {\n\tk.peerSigMtx.Lock()\n\tdefer k.peerSigMtx.Unlock()\n\n\tfor _, c := range k.peerSig {\n\t\t// Every peerSig channel has a buffer capacity of 1,\n\t\t// so every receiver will get the signal even if the\n\t\t// select statement has the default case to avoid blocking.\n\t\tselect {\n\t\tcase c <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n}\n\nfunc closestPeer(peers *pslice.PSlice, addr swarm.Address, spf sanctionedPeerFunc) (swarm.Address, error) {\n\tclosest := swarm.ZeroAddress\n\terr := peers.EachBinRev(closestPeerFunc(&closest, addr, spf))\n\tif err != nil {\n\t\treturn closest, err\n\t}\n\n\t// check if found\n\tif closest.IsZero() {\n\t\treturn closest, topology.ErrNotFound\n\t}\n\n\treturn closest, nil\n}\n\nfunc closestPeerInSlice(peers []swarm.Address, addr swarm.Address, spf sanctionedPeerFunc) (swarm.Address, error) {\n\tclosest := swarm.ZeroAddress\n\tclosestFunc := closestPeerFunc(&closest, addr, spf)\n\n\tfor _, peer := range peers {\n\t\t_, _, err := closestFunc(peer, 0)\n\t\tif err != nil {\n\t\t\treturn closest, err\n\t\t}\n\t}\n\n\t// check if found\n\tif closest.IsZero() {\n\t\treturn closest, topology.ErrNotFound\n\t}\n\n\treturn closest, nil\n}\n\nfunc closestPeerFunc(closest *swarm.Address, addr swarm.Address, spf sanctionedPeerFunc) func(peer swarm.Address, po uint8) (bool, bool, error) {\n\treturn func(peer swarm.Address, po uint8) (bool, bool, error) {\n\t\t// check whether peer is sanctioned\n\t\tif spf(peer) {\n\t\t\treturn false, false, nil\n\t\t}\n\t\tif closest.IsZero() {\n\t\t\t*closest = peer\n\t\t\treturn false, false, nil\n\t\t}\n\t\tdcmp, err := swarm.DistanceCmp(addr.Bytes(), closest.Bytes(), peer.Bytes())\n\t\tif err != nil {\n\t\t\treturn false, false, err\n\t\t}\n\t\tswitch dcmp {\n\t\tcase 0:\n\t\t\t// do nothing\n\t\tcase -1:\n\t\t\t// current peer is closer\n\t\t\t*closest = peer\n\t\tcase 1:\n\t\t\t// closest is already closer to chunk\n\t\t\t// do nothing\n\t\t}\n\t\treturn false, false, nil\n\t}\n}\n\n// ClosestPeer returns the closest peer to a given address.\nfunc (k *Kad) ClosestPeer(addr swarm.Address, includeSelf bool, skipPeers ...swarm.Address) (swarm.Address, error) {\n\tif k.connectedPeers.Length() == 0 {\n\t\treturn swarm.Address{}, topology.ErrNotFound\n\t}\n\n\tclosest := swarm.ZeroAddress\n\n\tif includeSelf {\n\t\tclosest = k.base\n\t}\n\n\terr := k.connectedPeers.EachBinRev(func(peer swarm.Address, po uint8) (bool, bool, error) {\n\n\t\tfor _, a := range skipPeers {\n\t\t\tif a.Equal(peer) {\n\t\t\t\treturn false, false, nil\n\t\t\t}\n\t\t}\n\n\t\tif closest.IsZero() {\n\t\t\tclosest = peer\n\t\t}\n\n\t\tdcmp, err := swarm.DistanceCmp(addr.Bytes(), closest.Bytes(), peer.Bytes())\n\t\tif err != nil {\n\t\t\treturn false, false, err\n\t\t}\n\t\tswitch dcmp {\n\t\tcase 0:\n\t\t\t// do nothing\n\t\tcase -1:\n\t\t\t// current peer is closer\n\t\t\tclosest = peer\n\t\tcase 1:\n\t\t\t// closest is already closer to chunk\n\t\t\t// do nothing\n\t\t}\n\t\treturn false, false, nil\n\t})\n\tif err != nil {\n\t\treturn swarm.Address{}, err\n\t}\n\n\tif closest.IsZero() { // no peers\n\t\treturn swarm.Address{}, topology.ErrNotFound // only for light nodes\n\t}\n\n\t// check if self\n\tif closest.Equal(k.base) {\n\t\treturn swarm.Address{}, topology.ErrWantSelf\n\t}\n\n\treturn closest, nil\n}\n\n// IsWithinDepth returns if an address is within the neighborhood depth of a node.\nfunc (k *Kad) IsWithinDepth(addr swarm.Address) bool {\n\treturn swarm.Proximity(k.base.Bytes(), addr.Bytes()) >= k.NeighborhoodDepth()\n}\n\n// EachNeighbor iterates from closest bin to farthest of the neighborhood peers.\nfunc (k *Kad) EachNeighbor(f topology.EachPeerFunc) error {\n\tdepth := k.NeighborhoodDepth()\n\tfn := func(a swarm.Address, po uint8) (bool, bool, error) {\n\t\tif po < depth {\n\t\t\treturn true, false, nil\n\t\t}\n\t\treturn f(a, po)\n\t}\n\treturn k.connectedPeers.EachBin(fn)\n}\n\n// EachNeighborRev iterates from farthest bin to closest of the neighborhood peers.\nfunc (k *Kad) EachNeighborRev(f topology.EachPeerFunc) error {\n\tdepth := k.NeighborhoodDepth()\n\tfn := func(a swarm.Address, po uint8) (bool, bool, error) {\n\t\tif po < depth {\n\t\t\treturn false, true, nil\n\t\t}\n\t\treturn f(a, po)\n\t}\n\treturn k.connectedPeers.EachBinRev(fn)\n}\n\n// EachPeer iterates from closest bin to farthest.\nfunc (k *Kad) EachPeer(f topology.EachPeerFunc) error {\n\treturn k.connectedPeers.EachBin(f)\n}\n\n// EachPeerRev iterates from farthest bin to closest.\nfunc (k *Kad) EachPeerRev(f topology.EachPeerFunc) error {\n\treturn k.connectedPeers.EachBinRev(f)\n}\n\n// SubscribePeersChange returns the channel that signals when the connected peers\n// set changes. Returned function is safe to be called multiple times.\nfunc (k *Kad) SubscribePeersChange() (c <-chan struct{}, unsubscribe func()) {\n\tchannel := make(chan struct{}, 1)\n\tvar closeOnce sync.Once\n\n\tk.peerSigMtx.Lock()\n\tdefer k.peerSigMtx.Unlock()\n\n\tk.peerSig = append(k.peerSig, channel)\n\n\tunsubscribe = func() {\n\t\tk.peerSigMtx.Lock()\n\t\tdefer k.peerSigMtx.Unlock()\n\n\t\tfor i, c := range k.peerSig {\n\t\t\tif c == channel {\n\t\t\t\tk.peerSig = append(k.peerSig[:i], k.peerSig[i+1:]...)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tcloseOnce.Do(func() { close(channel) })\n\t}\n\n\treturn channel, unsubscribe\n}\n\n// NeighborhoodDepth returns the current Kademlia depth.\nfunc (k *Kad) NeighborhoodDepth() uint8 {\n\tk.depthMu.RLock()\n\tdefer k.depthMu.RUnlock()\n\n\treturn k.depth\n}\n\n// IsBalanced returns if Kademlia is balanced to bin.\nfunc (k *Kad) IsBalanced(bin uint8) bool {\n\tk.depthMu.RLock()\n\tdefer k.depthMu.RUnlock()\n\n\tif int(bin) > len(k.commonBinPrefixes) {\n\t\treturn false\n\t}\n\n\t// for each pseudo address\n\tfor i := range k.commonBinPrefixes[bin] {\n\t\tpseudoAddr := k.commonBinPrefixes[bin][i]\n\t\tclosestConnectedPeer, err := closestPeer(k.connectedPeers, pseudoAddr, noopSanctionedPeerFn)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\n\t\tclosestConnectedPO := swarm.ExtendedProximity(closestConnectedPeer.Bytes(), pseudoAddr.Bytes())\n\t\tif int(closestConnectedPO) < int(bin)+k.bitSuffixLength+1 {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\nfunc (k *Kad) SetRadius(r uint8) {\n\tk.depthMu.Lock()\n\tdefer k.depthMu.Unlock()\n\tif k.radius == r {\n\t\treturn\n\t}\n\tk.radius = r\n\toldD := k.depth\n\tk.depth = recalcDepth(k.connectedPeers, k.radius)\n\tif k.depth != oldD {\n\t\tk.notifyManageLoop()\n\t}\n}\n\nfunc (k *Kad) Snapshot() *topology.KadParams {\n\tvar infos []topology.BinInfo\n\tfor i := int(swarm.MaxPO); i >= 0; i-- {\n\t\tinfos = append(infos, topology.BinInfo{})\n\t}\n\n\tss := k.collector.Snapshot(time.Now())\n\n\t_ = k.connectedPeers.EachBin(func(addr swarm.Address, po uint8) (bool, bool, error) {\n\t\tinfos[po].BinConnected++\n\t\tinfos[po].ConnectedPeers = append(\n\t\t\tinfos[po].ConnectedPeers,\n\t\t\t&topology.PeerInfo{\n\t\t\t\tAddress: addr,\n\t\t\t\tMetrics: createMetricsSnapshotView(ss[addr.ByteString()]),\n\t\t\t},\n\t\t)\n\t\treturn false, false, nil\n\t})\n\n\t// output (k.knownPeers \u00ac k.connectedPeers) here to not repeat the peers we already have in the connected peers list\n\t_ = k.knownPeers.EachBin(func(addr swarm.Address, po uint8) (bool, bool, error) {\n\t\tinfos[po].BinPopulation++\n\n\t\tfor _, v := range infos[po].ConnectedPeers {\n\t\t\t// peer already connected, don't show in the known peers list\n\t\t\tif v.Address.Equal(addr) {\n\t\t\t\treturn false, false, nil\n\t\t\t}\n\t\t}\n\n\t\tinfos[po].DisconnectedPeers = append(\n\t\t\tinfos[po].DisconnectedPeers,\n\t\t\t&topology.PeerInfo{\n\t\t\t\tAddress: addr,\n\t\t\t\tMetrics: createMetricsSnapshotView(ss[addr.ByteString()]),\n\t\t\t},\n\t\t)\n\t\treturn false, false, nil\n\t})\n\n\treturn &topology.KadParams{\n\t\tBase:           k.base.String(),\n\t\tPopulation:     k.knownPeers.Length(),\n\t\tConnected:      k.connectedPeers.Length(),\n\t\tTimestamp:      time.Now(),\n\t\tNNLowWatermark: nnLowWatermark,\n\t\tDepth:          k.NeighborhoodDepth(),\n\t\tBins: topology.KadBins{\n\t\t\tBin0:  infos[0],\n\t\t\tBin1:  infos[1],\n\t\t\tBin2:  infos[2],\n\t\t\tBin3:  infos[3],\n\t\t\tBin4:  infos[4],\n\t\t\tBin5:  infos[5],\n\t\t\tBin6:  infos[6],\n\t\t\tBin7:  infos[7],\n\t\t\tBin8:  infos[8],\n\t\t\tBin9:  infos[9],\n\t\t\tBin10: infos[10],\n\t\t\tBin11: infos[11],\n\t\t\tBin12: infos[12],\n\t\t\tBin13: infos[13],\n\t\t\tBin14: infos[14],\n\t\t\tBin15: infos[15],\n\t\t\tBin16: infos[16],\n\t\t\tBin17: infos[17],\n\t\t\tBin18: infos[18],\n\t\t\tBin19: infos[19],\n\t\t\tBin20: infos[20],\n\t\t\tBin21: infos[21],\n\t\t\tBin22: infos[22],\n\t\t\tBin23: infos[23],\n\t\t\tBin24: infos[24],\n\t\t\tBin25: infos[25],\n\t\t\tBin26: infos[26],\n\t\t\tBin27: infos[27],\n\t\t\tBin28: infos[28],\n\t\t\tBin29: infos[29],\n\t\t\tBin30: infos[30],\n\t\t\tBin31: infos[31],\n\t\t},\n\t}\n}\n\n// String returns a string represenstation of Kademlia.\nfunc (k *Kad) String() string {\n\tj := k.Snapshot()\n\tb, err := json.MarshalIndent(j, \"\", \"  \")\n\tif err != nil {\n\t\tk.logger.Errorf(\"could not marshal kademlia into json: %v\", err)\n\t\treturn \"\"\n\t}\n\treturn string(b)\n}\n\n// Halt stops outgoing connections from happening.\n// This is needed while we shut down, so that further topology\n// changes do not happen while we shut down.\nfunc (k *Kad) Halt() {\n\tclose(k.halt)\n}\n\n// Close shuts down kademlia.\nfunc (k *Kad) Close() error {\n\tk.logger.Info(\"kademlia shutting down\")\n\tclose(k.quit)\n\tcc := make(chan struct{})\n\n\tgo func() {\n\t\tk.wg.Wait()\n\t\tclose(cc)\n\t}()\n\n\tselect {\n\tcase <-cc:\n\tcase <-time.After(peerConnectionAttemptTimeout):\n\t\tk.logger.Warning(\"kademlia shutting down with announce goroutines\")\n\t}\n\n\tselect {\n\tcase <-k.done:\n\tcase <-time.After(5 * time.Second):\n\t\tk.logger.Warning(\"kademlia manage loop did not shut down properly\")\n\t}\n\n\tk.logger.Info(\"kademlia persisting peer metrics\")\n\tstart := time.Now()\n\tif err := k.collector.Finalize(start, false); err != nil {\n\t\tk.logger.Debugf(\"kademlia: unable to finalize open sessions: %v\", err)\n\t}\n\tk.logger.Debugf(\"kademlia: Finalize(...) took %v\", time.Since(start))\n\n\treturn nil\n}\n\nfunc randomSubset(addrs []swarm.Address, count int) ([]swarm.Address, error) {\n\tif count >= len(addrs) {\n\t\treturn addrs, nil\n\t}\n\n\tfor i := 0; i < len(addrs); i++ {\n\t\tb, err := random.Int(random.Reader, big.NewInt(int64(len(addrs))))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tj := int(b.Int64())\n\t\taddrs[i], addrs[j] = addrs[j], addrs[i]\n\t}\n\n\treturn addrs[:count], nil\n}\n\nfunc (k *Kad) randomPeer(bin uint8) (swarm.Address, error) {\n\tpeers := k.connectedPeers.BinPeers(bin)\n\n\tif len(peers) == 0 {\n\t\treturn swarm.ZeroAddress, errEmptyBin\n\t}\n\n\trndIndx, err := random.Int(random.Reader, big.NewInt(int64(len(peers))))\n\tif err != nil {\n\t\treturn swarm.ZeroAddress, err\n\t}\n\n\treturn peers[rndIndx.Int64()], nil\n}\n\n// createMetricsSnapshotView creates new topology.MetricSnapshotView from the\n// given metrics.Snapshot and rounds all the timestamps and durations to its\n// nearest second.\nfunc createMetricsSnapshotView(ss *im.Snapshot) *topology.MetricSnapshotView {\n\tif ss == nil {\n\t\treturn nil\n\t}\n\treturn &topology.MetricSnapshotView{\n\t\tLastSeenTimestamp:          time.Unix(0, ss.LastSeenTimestamp).Unix(),\n\t\tSessionConnectionRetry:     ss.SessionConnectionRetry,\n\t\tConnectionTotalDuration:    ss.ConnectionTotalDuration.Truncate(time.Second).Seconds(),\n\t\tSessionConnectionDuration:  ss.SessionConnectionDuration.Truncate(time.Second).Seconds(),\n\t\tSessionConnectionDirection: string(ss.SessionConnectionDirection),\n\t}\n}\n", "idx": 8, "id": 15563, "msg": "", "proj": "ethersphere-bee", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -10,6 +10,19 @@\n namespace nebula {\n namespace graph {\n \n+ExecutionContext::~ExecutionContext() {\n+    if (nullptr != sm_) {\n+        sm_ = nullptr;\n+    }\n+\n+    if (nullptr != storage_) {\n+        storage_ = nullptr;\n+    }\n+\n+    if (nullptr != metaClient_) {\n+        metaClient_ = nullptr;\n+    }\n+}\n \n }   // namespace graph\n }   // namespace nebula", "y": 1, "oldf": "/* Copyright (c) 2018 - present, VE Software Inc. All rights reserved\n *\n * This source code is licensed under Apache 2.0 License\n *  (found in the LICENSE.Apache file in the root directory)\n */\n\n#include \"base/Base.h\"\n#include \"graph/ExecutionContext.h\"\n\nnamespace nebula {\nnamespace graph {\n\n\n}   // namespace graph\n}   // namespace nebula\n", "idx": 1, "id": 16783, "msg": "With no need for it. std::unique_ptr has done it inside.", "proj": "vesoft-inc-nebula", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -199,25 +199,26 @@ def Array(dtype, size=None, ref=False):\n   suffix = 'ArrayRef' if ref else 'Array'\n   arrayFactory = getattr(engine, dtype + suffix)\n   arrayFactory.getType = getArrayType\n-  \n+\n   if size:\n     a = arrayFactory(size)\n   else:\n     a = arrayFactory()\n-    \n+\n   a._dtype = basicTypes[index]\n   return a\n \n+\n def ArrayRef(dtype):\n   return Array(dtype, None, True)\n \n \n-\n class CollectionIterator(object):\n+\n   def __init__(self, collection):\n     self.collection = collection\n     self.index = 0\n-    \n+\n   def next(self):\n     index = self.index\n     if index == self.collection.getCount():", "y": 0, "oldf": "#!/usr/bin/env python\n\n# ----------------------------------------------------------------------\n# Numenta Platform for Intelligent Computing (NuPIC)\n# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement\n# with Numenta, Inc., for a separate license for this software code, the\n# following terms and conditions apply:\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero Public License version 3 as\n# published by the Free Software Foundation.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# See the GNU Affero Public License for more details.\n#\n# You should have received a copy of the GNU Affero Public License\n# along with this program.  If not, see http://www.gnu.org/licenses.\n#\n# http://numenta.org/licenses/\n# ----------------------------------------------------------------------\n\nimport os\nimport sys\nimport nupic.bindings.engine_internal as engine\nfrom nupic.support.lockattributes import LockAttributesMixin\nimport functools\n\nbasicTypes = ['Byte', 'Int16', 'UInt16', 'Int32', 'UInt32', 'Int64', 'UInt64', 'Real32', 'Real64', 'Handle']\n\npyRegions = ((\"nupic.regions.AnomalyRegion\", \"AnomalyRegion\"),\n             (\"nupic.regions.CLAClassifierRegion\", \"CLAClassifierRegion\"),\n             (\"nupic.regions.ImageSensor\", \"ImageSensor\"),\n             (\"nupic.regions.KNNAnomalyClassifierRegion\", \"KNNAnomalyClassifierRegion\"),\n             (\"nupic.regions.KNNClassifierRegion\", \"KNNClassifierRegion\"),\n             (\"nupic.regions.PyRegion\", \"PyRegion\"),\n             (\"nupic.regions.RecordSensor\", \"RecordSensor\"),\n             (\"nupic.regions.SPRegion\", \"SPRegion\"),\n             (\"nupic.regions.SVMClassifierNode\", \"SVMClassifierNode\"),\n             (\"nupic.regions.TPRegion\", \"TPRegion\"),\n             (\"nupic.regions.TestNode\", \"TestNode\"),\n             (\"nupic.regions.TestRegion\", \"TestRegion\"),\n             (\"nupic.regions.UnimportableNode\", \"UnimportableNode\"),\n             (\"nupic.regions.extra.GaborNode2\", \"GaborNode2\"))\n\nregisteredRegions = False\n\ndef registerBuiltInRegions():\n  global registeredRegions\n\n  # Initialize nupic regions\n  if not registeredRegions:\n    for module, className in pyRegions:\n      engine.Network.registerPyRegion(module, className)\n  registeredRegions = True\nregisterBuiltInRegions()\n\n# Import all the array types from engine (there is no HandleArray)\narrayTypes = [t + 'Array' for t in basicTypes[:-1]]\nfor a in arrayTypes:\n  exec('from %s import %s as %s' % (engine.__name__, a, a))\n\n# Intercept the default exception handling for the purposes of stripping\n# parts of the stack trace that can confuse users. If you want the original\n# stack trace define this environment variable\nif not 'NTA_STANDARD_PYTHON_UNHANDLED_EXCEPTIONS' in os.environ:\n  import traceback\n  import cStringIO\n  \n  def customExceptionHandler(type, value, tb):\n    \"\"\"Catch unhandled Python exception\n    \n    The handler prints the original exception info including into a buffer.\n    It then extracts the original error message (when the exception is raised\n    inside a Py node additional stacktrace info will be appended in the end)\n    and saves the original exception to a file called error.txt. It prints\n    just the error message to the screen and tells the user about the error.txt\n    file.\n    \"\"\"\n    # Print the exception info to a string IO buffer for manipulation\n    buff = cStringIO.StringIO()\n    traceback.print_exception(type, value, tb, file=buff)\n\n    text = buff.getvalue()\n      \n    # get the lines skip the first one: \"Traceback (most recent call last)\"\n    lines = text.split('\\n')[1:]\n    #\n    # Extract the error message\n    begin = 0\n    end = len(lines)\n    for i, line in enumerate(lines):\n      if line.startswith('RuntimeError:'):\n        begin = i\n    #  \n    #  elif line.startswith('Traceback (most recent call last):'):\n    #    end = i\n    #    break\n    #\n    message = '\\n'.join(lines[begin:end])\n    message = message[len('Runtime Error:'):]\n    #stacktrace = lines[end:]\n    \n    # Get the stack trace if available (default to empty string)\n    stacktrace = getattr(value, 'stackTrace', '')    \n\n        \n\n    # Remove engine from stack trace\n    lines = [x for x in lines if 'engine' not in x]\n    \n    failMessage = 'The program failed with the following error message:'\n    dashes = '-' * len(failMessage)\n    print\n    print dashes\n    print 'Traceback (most recent call last):'\n    print '\\n'.join(lines[:begin-2])\n    if stacktrace:\n      print stacktrace\n    print dashes\n    print 'The program failed with the following error message:'\n    print dashes\n    print message\n    print\n\n  #sys.excepthook = customExceptionHandler\n\n\n\n# Expose the timer class directly\n# Do it this way instead of bringing engine.Timer \n# into the namespace to avoid engine\n# in the class name\nclass Timer(engine.Timer):\n  pass\n\n\n\n# Expose the os class directly\n# The only wrapped method is getProcessMemoryUsage()\nclass OS(engine.OS):\n  pass\n\n\n\nclass Dimensions(engine.Dimensions):\n  \"\"\"Represent the topology of an N-dimensional region\n  \n  Basically, it is a list of integers such as: [4, 8, 6]\n  In this example the topology is a 3 dimensional region with\n  4 x 8 x 6 nodes.\n  \n  You can initialize it with a list of dimensions or with no arguments\n  and then append dimensions.\n  \n  \"\"\"\n  def __init__(self, *args):\n    \"\"\"Construct a Dimensions object\n    \n    The constructor can be called with no arguments or with a list\n    of integers\n    \"\"\"\n    # Init the base class\n    engine.Dimensions.__init__(self, *args)\n    \n  def __str__(self):\n    return self.toString()\n\n\n\ndef Array(dtype, size=None, ref=False):\n  \"\"\"Factory function that creates typed Array or ArrayRef objects\n  \n  dtype - the data type of the array (as string).\n    Supported types are: Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64, Real32, Real64\n  \n  size - the size of the array. Must be positive integer.\n  \"\"\"\n  \n  def getArrayType(self):\n    \"\"\"A little function to replace the getType() method of arrays\n    \n    It returns a string representation of the array element type instead of the\n    integer value (NTA_BasicType enum) returned by the origianl array\n    \"\"\"\n    return self._dtype\n      \n    \n  # ArrayRef can't be allocated\n  if ref:\n    assert size is None\n    \n  index = basicTypes.index(dtype)\n  if index == -1:\n    raise Exception('Invalid data type: ' + dtype)\n  if size and size <= 0:\n    raise Exception('Array size must be positive')\n  suffix = 'ArrayRef' if ref else 'Array'\n  arrayFactory = getattr(engine, dtype + suffix)\n  arrayFactory.getType = getArrayType\n  \n  if size:\n    a = arrayFactory(size)\n  else:\n    a = arrayFactory()\n    \n  a._dtype = basicTypes[index]\n  return a\n\ndef ArrayRef(dtype):\n  return Array(dtype, None, True)\n\n\n\nclass CollectionIterator(object):\n  def __init__(self, collection):\n    self.collection = collection\n    self.index = 0\n    \n  def next(self):\n    index = self.index\n    if index == self.collection.getCount():\n      raise StopIteration\n    self.index += 1\n    return self.collection.getByIndex(index)[0]\n\nclass CollectionWrapper(object):\n  \"\"\"Wrap an nupic::Collection with a dict-like interface\n  \n  The optional valueWrapper is used to wrap values for adaptation purposes.\n  Maintains the original documentation\n  \n  collection - the original collection\n  valueWrapper - an optional callable object used to wrap values.\n  \"\"\"\n  def IdentityWrapper(o):\n    return o\n  \n  \n  def __init__(self, collection, valueWrapper=IdentityWrapper):\n    self.collection = collection\n    self.valueWrapper = valueWrapper\n    self.__class__.__doc__ == collection.__class__.__doc__    \n\n  def __iter__(self):\n    return CollectionIterator(self.collection)\n  \n  def __str__(self):\n    return str(self.collection)\n\n  def __repr__(self):\n    return repr(self.collection)\n  \n  def __len__(self):\n    return self.collection.getCount()\n    \n  def __getitem__(self, key):\n    if not self.collection.contains(key):\n      raise KeyError('Key ' + key + ' not found')\n\n    value =  self.collection.getByName(key)\n    value = self.valueWrapper(key, value)\n    \n    return value\n  \n  def get(self, key, default=None):\n    try:\n      return self.__getitem__(key)\n    except KeyError:\n      return default\n        \n  def __contains__(self, key):\n    return self.collection.contains(key)\n    \n  def keys(self):\n    keys = set()\n    for i in range(self.collection.getCount()):\n      keys.add(self.collection.getByIndex(i)[0])\n    return keys\n  \n  def values(self):\n    values = set()\n    \n    for i in range(self.collection.getCount()):\n      p = self.collection.getByIndex(i)\n      values.add(self.valueWrapper(p[0], p[1]))\n    return values\n  \n  def items(self):\n    items = set()\n    for i in range(self.collection.getCount()):\n      p = self.collection.getByIndex(i)\n      items.add((p[0], self.valueWrapper(p[0], p[1])))\n    return items\n  \n  def __cmp__(self, other):\n    return self.collection == other.collection\n  \n  def __hash__(self):\n    return hash(self.collection)\n\n\n\nclass SpecItem(object):\n  \"\"\"Wrapper that translates the data type and access code to a string\n  \n  The original values are an enumerated type in C++ that become\n  just integers in Python. This class wraps the original ParameterSpec\n  and translates the integer values to meaningful strings: that correspond to the C++ enum labels.\n  \n  It is used to wrap ParameterSpec, InputSpec and OutputSpec\n  \"\"\"\n  accessModes = ['Create', 'ReadOnly', 'ReadWrite']\n\n  def __init__(self, name, item):\n    self.name = name\n    self.item = item\n    self.__class__.__doc__ == item.__class__.__doc__\n    # Translate data type to string representation\n    self.dataType = basicTypes[item.dataType]\n    # Translate access mode to string representation\n    if hasattr(item, 'accessMode'): # ParameterSpec only\n      self.accessMode = SpecItem.accessModes[item.accessMode]\n            \n  def __getattr__(self, name):\n    return getattr(self.item, name)\n    \n  def __str__(self):\n    d = dict(name=self.name,\n             description=self.description,\n             dataType=self.dataType,\n             count=self.count)\n    if hasattr(self.item, 'accessMode'): # ParameterSpec only\n      self.accessMode = SpecItem.accessModes[self.item.accessMode]\n    if hasattr(self.item, 'accessMode'): # ParameterSpec only\n      d['accessMode'] = self.accessMode\n    if hasattr(self.item, 'constraints'): # ParameterSpec only\n      d['constraints'] = self.constraints\n    if hasattr(self.item, 'defaultValue'): # ParameterSpec only\n      d['defaultValue'] = self.defaultValue\n    \n    return str(d)\n\n\n\nclass Spec(object):\n  def __init__(self, spec):\n    self.spec = spec\n    self.__class__.__doc__ == spec.__class__.__doc__\n    self.description = spec.description\n    self.singleNodeOnly = spec.singleNodeOnly\n    self.inputs = CollectionWrapper(spec.inputs, SpecItem)\n    self.outputs = CollectionWrapper(spec.outputs, SpecItem)\n    self.parameters = CollectionWrapper(spec.parameters, SpecItem)\n    self.commands = CollectionWrapper(spec.commands)\n          \n  def __str__(self):\n    return self.spec.toString()\n\n  def __repr__(self):\n    return self.spec.toString()\n  \nclass _ArrayParameterHelper:\n  \"\"\"This class is used by Region._getParameterMethods\"\"\"\n  def __init__(self, region, datatype):\n    self._region = region\n    self.datatype = basicTypes[datatype]\n\n  def getParameterArray(self, paramName):\n    # return a PyArray instead of a plain array. \n    # PyArray constructor/class for type X is called XArray()\n    #factoryName = self.datatype + 'Array'\n    #if factoryName not in globals():\n    #  import exceptions\n    #  raise exceptions.Exception(\"Internal error -- did not find %s constructor in engine\" % factoryName)\n    #\n    #arrayFactory = globals()[factoryName]\n    #a = arrayFactory();\n    a = Array(self.datatype)\n    self._region.getParameterArray(paramName, a)\n    return a\n\n\n\nclass Region(LockAttributesMixin):\n  \"\"\"\n  @doc:place_holder(Region.description)\n  \"\"\"\n\n  #Wrapper for a network region\n  #- Maintains original documentation\n  #- Implement syntactic sugar properties:  \n      #name = property(getName)\n      #type = property(getType)\n      #spec = property(getSpec)\n      #dimensions = property(getDimensions, setDimensions)\n      #network = property(getNetwork)\n  #- Makes sure that returned objects are high-level wrapper objects\n  #- Forwards everything else to internal region\n  \n  def __init__(self, region, network):\n    \"\"\"Store the wraped region and hosting network\n    \n    The network is the high-level Network and not the internal\n    Network. This is important in case the user requests the network\n    from the region (never leak a engine object, remember)\n    \"\"\"\n    self._network = network\n    self._region = region\n    self.__class__.__doc__ == region.__class__.__doc__\n    \n    # A cache for typed get/setPArameter() calls\n    self._paramTypeCache = {}\n    \n  def __getattr__(self, name):\n    if not '_region' in self.__dict__:\n      raise AttributeError\n    return getattr(self._region, name)\n\n  def __setattr__(self, name, value):\n    if name in ('_region', '__class__', '_network'):\n      self.__dict__[name] = value\n    elif name == 'dimensions':\n      self.setDimensions(value)\n    else:\n      setattr(self._region, name, value)\n        \n  @staticmethod\n  def getSpecFromType(nodeType):\n    \"\"\"\n    @doc:place_holder(Region.getSpecFromType)\n    \"\"\"\n    return Spec(engine.Region.getSpecFromType(nodeType))\n    \n  def compute(self):\n    \"\"\"\n    @doc:place_holder(Region.compute)\n    \n    ** This line comes from the original docstring (not generated by Documentor) \n    \n    \"\"\"\n    return self._region.compute()\n    \n  def getInputData(self, inputName):\n    \"\"\"\n    @doc:place_holder(Region.getInputData)\n    \"\"\"\n    return self._region.getInputArray(inputName)\n\n  def getOutputData(self, outputName):\n    \"\"\"\n    @doc:place_holder(Region.getOutputData)\n    \"\"\"\n    return self._region.getOutputArray(outputName)\n    \n  \n  def executeCommand(self, args):\n    \"\"\"\n    @doc:place_holder(Region.executeCommand)\n    \"\"\"    \n    return self._region.executeCommand(args)\n    \n    \n  def _getSpec(self):\n    \"\"\"Spec of the region\"\"\"\n    return Spec(self._region.getSpec())\n\n  def _getDimensions(self):\n    \"\"\"Dimensions of the region\"\"\"\n    return Dimensions(tuple(self._region.getDimensions()))\n      \n  def _getNetwork(self):\n    \"\"\"Network for the region\"\"\"\n    return self._network\n            \n  def __hash__(self):\n    \"\"\"Hash a region\"\"\"\n    return self._region.__hash__()\n    \n  def __cmp__(self, other):\n    \"\"\"Compare regions\"\"\"\n    return self._region == other._region\n \n\n  def _getParameterMethods(self, paramName):\n    \"\"\"Returns functions to set/get the parameter. These are \n    the strongly typed functions get/setParameterUInt32, etc.\n    The return value is a pair:\n        setfunc, getfunc\n    If the parameter is not available on this region, setfunc/getfunc\n    are None. \"\"\"\n    if paramName in self._paramTypeCache:\n      return self._paramTypeCache[paramName]\n    try:\n      # Catch the error here. We will re-throw in getParameter or \n      # setParameter with a better error message than we could generate here\n      paramSpec = self.getSpec().parameters.getByName(paramName)\n    except:\n      return (None, None)\n    dataType = paramSpec.dataType\n    dataTypeName = basicTypes[dataType]\n    count = paramSpec.count\n    if count == 1:\n      # Dynamically generate the proper typed get/setParameter<dataType>\n      x = 'etParameter' + dataTypeName\n      try:\n        g = getattr(self, 'g' + x) # get the typed getParameter method\n        s = getattr(self, 's' + x) # get the typed setParameter method\n      except AttributeError:\n        raise Exception(\"Internal error: unknown parameter type %s\" % dataTypeName)\n      info = (s, g)      \n    else:\n      if dataTypeName == \"Byte\":\n        info = (self.setParameterString, self.getParameterString)\n      else:\n        helper = _ArrayParameterHelper(self, dataType)\n        info = (self.setParameterArray, helper.getParameterArray)\n\n    self._paramTypeCache[paramName] = info\n    return info\n\n  def getParameter(self, paramName):\n    \"\"\"Get parameter value\"\"\"\n    (setter, getter) = self._getParameterMethods(paramName)\n    if getter is None:\n      import exceptions\n      raise exceptions.Exception(\"getParameter -- parameter name '%s' does not exist in region %s of type %s\" %\n                       (paramName, self.name, self.type))\n    return getter(paramName)\n\n  def setParameter(self, paramName, value):\n    \"\"\"Set parameter value\"\"\"\n    (setter, getter) = self._getParameterMethods(paramName)\n    if setter is None:\n      import exceptions\n      raise exceptions.Exception(\"setParameter -- parameter name '%s' does not exist in region %s of type %s\" %\n                       (paramName, self.name, self.type))\n    setter(paramName, value)\n\n\n  def _get(self, method):\n    \"\"\"Auto forwarding of properties to get methods of internal region\"\"\"\n    return getattr(self._region, method)()\n\n  network = property(_getNetwork,\n                     doc='@property:place_holder(Region.getNetwork)')\n\n  name = property(functools.partial(_get, method='getName'),\n                  doc=\"@property:place_holder(Region.getName)\")\n\n  type = property(functools.partial(_get, method='getType'),\n                      doc='@property:place_holder(Region.getType)')\n\n  spec = property(_getSpec,\n                      doc='@property:place_holder(Region.getSpec)')\n      \n  dimensions = property(_getDimensions,\n                        engine.Region.setDimensions,\n                        doc='@property:place_holder(Region.getDimensions)')\n  \n  computeTimer = property(functools.partial(_get, method='getComputeTimer'),\n                          doc='@property:place_holder(Region.getComputeTimer)')\n\n  executeTimer = property(functools.partial(_get, method='getExecuteTimer'),\n                          doc='@property:place_holder(Region.getExecuteTimer)')\n\n\n\nclass Network(engine.Network):\n  \"\"\"\n  @doc:place_holder(Network.description)\n  \"\"\"\n\n  def __init__(self, *args):\n    \"\"\"Constructor\n    \n    - Initialize the internal engine.Network class generated by Swig\n    - Attach docstrings to selected methods\n    \"\"\"\n    # Init engine.Network class\n    engine.Network.__init__(self, *args)\n    \n    # Prepare documentation table.\n    # Each item is pair of method/property, docstring\n    # The docstring is attached later to the method or property.\n    # The key for method items is the method object of the engine.Network class.\n    # The key for properties is the property name\n\n    docTable = (\n        (engine.Network.getRegions, 'Get the collection of regions in a network'),\n    )\n    \n    # Attach documentation to methods and properties\n    for obj, docString in docTable:\n      if isinstance(obj, str):\n        prop = getattr(Network, obj)\n        assert isinstance(prop, property)\n        setattr(Network, obj, property(prop.fget, prop.fset, prop.fdel, docString))\n      else:\n        obj.im_func.__doc__ = docString\n    \n  def _getRegions(self):\n    \"\"\"Get the collection of regions in a network\n    \n    This is a tricky one. The collection of regions returned from\n    from the internal network is a collection of internal regions.\n    The desired collection is a collelcion of net.Region objects\n    that also points to this network (net.network) and not to\n    the internal network. To achieve that a CollectionWrapper\n    class is used with a custom makeRegion() function (see bellow)\n    as a value wrapper. The CollectionWrapper class wraps each value in the\n    original collection with the result of the valueWrapper.\n    \"\"\"\n\n    def makeRegion(name, r):\n      \"\"\"Wrap a engine region with a nupic.engine.Region\n      \n      Also passes the containing nupic.engine.Network network in _network. This\n      function is passed a value wrapper to the CollectionWrapper      \n      \"\"\"\n      r = Region(r, self)\n      #r._network = self\n      return r\n    \n    regions = CollectionWrapper(engine.Network.getRegions(self), makeRegion)    \n    return regions\n    \n  def addRegion(self, name, nodeType, nodeParams):    \n    \"\"\"\n    @doc:place_holder(Network.addRegion)\n    \"\"\"\n    engine.Network.addRegion(self, name, nodeType, nodeParams)\n    return self._getRegions()[name]\n    \n\n  def addRegionFromBundle(self, name, nodeType, dimensions, bundlePath, label):\n    \"\"\"\n    @doc:place_holder(Network.addRegionFromBundle)\n    \"\"\"\n\n    engine.Network.addRegionFromBundle(self,\n                                   name,\n                                   nodeType,\n                                   dimensions,\n                                   bundlePath,\n                                   label)\n    return self._getRegions()[name]\n\n  def setPhases(self, name, phases):\n    \"\"\"\n    @doc:place_holder(Network.setPhases)\n    \"\"\"\n    phases = engine.UInt32Set(phases)\n    engine.Network.setPhases(self, name, phases)\n\n  def run(self, n):\n    \"\"\"\n    @doc:place_holder(Network.run)\n    \"\"\"\n\n    #Just forward to the internal network\n    #This is needed for inspectors to work properly because they wrap some key\n    #methods such as 'run'.\n\n    engine.Network.run(self, n)\n\n  def disableProfiling(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.disableProfiling)\n    \"\"\"\n    engine.Network.disableProfiling(self, *args, **kwargs)\n\n  def enableProfiling(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.enableProfiling)\n    \"\"\"\n    engine.Network.enableProfiling(self, *args, **kwargs)\n\n  def getCallbacks(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.getCallbacks)\n    \"\"\"\n    engine.Network.getCallbacks(self, *args, **kwargs)\n\n\n  def initialize(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.initialize)\n    \"\"\"\n    engine.Network.initialize(self, *args, **kwargs)\n\n  def link(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.link)\n    \"\"\"\n    engine.Network.link(self, *args, **kwargs)\n\n  def removeLink(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.removeLink)\n    \"\"\"\n    engine.Network.removeLink(self, *args, **kwargs)\n\n  def removeRegion(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.removeRegion)\n    \"\"\"\n    engine.Network.removeRegion(self, *args, **kwargs)\n\n  def resetProfiling(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.resetProfiling)\n    \"\"\"\n    engine.Network.resetProfiling(self, *args, **kwargs)\n\n  def save(self, *args, **kwargs):\n    \"\"\"\n    @doc:place_holder(Network.save)\n    \"\"\"\n    engine.Network.save(self, *args, **kwargs)\n\n\n  def inspect(self):\n    \"\"\"Launch a GUI inpector to inspect the network\"\"\"\n    from nupic.analysis import inspect    \n    inspect(self)\n\n  @staticmethod\n  def registerRegion(regionClass):\n    \"\"\"\n    Adds the module and class name for the region to the list of classes the network can use\n    regionClass: a pointer to a subclass of PyRegion\n    \"\"\"\n    engine.Network.registerPyRegion(regionClass.__module__, regionClass.__name__)\n\n  @staticmethod\n  def unregisterRegion(regionName):\n    \"\"\"\n    Unregisters a region from the internal list of regions\n\n    :param str regionName: The name of the region to unregister\n        (ex: regionName=regionClass.__name__)\n    \"\"\"\n    engine.Network.unregisterPyRegion(regionName)\n\n  # Syntactic sugar properties\n  regions = property(_getRegions, doc='@property:place_holder(Network.getRegions)')\n  minPhase = property(engine.Network.getMinPhase, doc='@property:place_holder(Network.getMinPhase)')\n  maxPhase = property(engine.Network.getMaxPhase, doc='@property:place_holder(Network.getMaxPhase)')\n  minEnabledPhase = property(engine.Network.getMinEnabledPhase, engine.Network.setMinEnabledPhase, doc='@property:place_holder(Network.getMinEnabledPhase)')\n  maxEnabledPhase = property(engine.Network.getMaxEnabledPhase, engine.Network.setMaxEnabledPhase, doc='@property:place_holder(Network.getMaxEnabledPhase)')\n    \nif __name__=='__main__':\n  n = Network()\n  print n.regions\n  print len(n.regions)\n  print Network.regions.__doc__\n  \n  d = Dimensions([3, 4, 5])\n  print len(d)\n  print d\n  \n  a = Array('Byte', 5)\n  print len(a)\n  for i in range(len(a)):\n    a[i] = ord('A') + i\n    \n  for i in range(len(a)):\n    print a[i]\n    \n  r = n.addRegion('r', 'TestNode', '')\n  print 'name:', r.name\n  print 'node type:', r.type\n  print 'node spec:', r.spec\n\n", "idx": 11, "id": 19821, "msg": "", "proj": "numenta-nupic", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,18 @@\n+\"\"\"Checks of Dosctrings 'bad-docstring-quotes'\"\"\"\n+# pylint: disable=docstring-first-line-empty,missing-class-docstring\n+\n+\n+class FFFF:\n+    def method1(self):  # [bad-docstring-quotes]\n+        '''\n+        Test Triple Single Quotes docstring\n+        '''\n+\n+    def method2(self):  # [bad-docstring-quotes]\n+        \"bad docstring 1\"\n+\n+    def method3(self):  # [bad-docstring-quotes]\n+        'bad docstring 2'\n+\n+    def method4(self):  # [bad-docstring-quotes]\n+        ' \"\"\"bad docstring 3 '", "y": 1, "oldf": "", "idx": 1, "id": 19111, "msg": "Could you merge the `bad` and `ok` files? That makes it a bit easier to see what this checker is checking specifically.", "proj": "PyCQA-pylint", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -231,7 +231,7 @@ func (h *serviceImpl) Start() {\n }\n \n // Stop closes the associated transport\n-func (h *serviceImpl) Stop() {\n+func (h *oneboxServiceImpl) Stop() {\n \tif !atomic.CompareAndSwapInt32(&h.status, common.DaemonStatusStarted, common.DaemonStatusStopped) {\n \t\treturn\n \t}", "y": 0, "oldf": "// Copyright (c) 2017 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage service\n\nimport (\n\t\"math/rand\"\n\t\"net\"\n\t\"os\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/uber-go/tally\"\n\t\"go.temporal.io/temporal-proto/workflowservice\"\n\t\"go.uber.org/yarpc\"\n\n\t\"github.com/temporalio/temporal/client\"\n\t\"github.com/temporalio/temporal/common\"\n\t\"github.com/temporalio/temporal/common/archiver\"\n\t\"github.com/temporalio/temporal/common/archiver/provider\"\n\t\"github.com/temporalio/temporal/common/authorization\"\n\t\"github.com/temporalio/temporal/common/clock\"\n\t\"github.com/temporalio/temporal/common/cluster\"\n\tes \"github.com/temporalio/temporal/common/elasticsearch\"\n\t\"github.com/temporalio/temporal/common/log\"\n\t\"github.com/temporalio/temporal/common/log/tag\"\n\t\"github.com/temporalio/temporal/common/membership\"\n\t\"github.com/temporalio/temporal/common/messaging\"\n\t\"github.com/temporalio/temporal/common/metrics\"\n\t\"github.com/temporalio/temporal/common/persistence\"\n\t\"github.com/temporalio/temporal/common/service/config\"\n\t\"github.com/temporalio/temporal/common/service/dynamicconfig\"\n)\n\ntype (\n\t// BootstrapParams holds the set of parameters\n\t// needed to bootstrap a service\n\tBootstrapParams struct {\n\t\tName            string\n\t\tInstanceID      string\n\t\tLogger          log.Logger\n\t\tThrottledLogger log.Logger\n\n\t\tMetricScope         tally.Scope\n\t\tMembershipFactory   MembershipMonitorFactory\n\t\tRPCFactory          common.RPCFactory\n\t\tPProfInitializer    common.PProfInitializer\n\t\tPersistenceConfig   config.Persistence\n\t\tClusterMetadata     cluster.Metadata\n\t\tReplicatorConfig    config.Replicator\n\t\tMetricsClient       metrics.Client\n\t\tMessagingClient     messaging.Client\n\t\tESClient            es.Client\n\t\tESConfig            *es.Config\n\t\tDynamicConfig       dynamicconfig.Client\n\t\tDispatcherProvider  client.DispatcherProvider\n\t\tDCRedirectionPolicy config.DCRedirectionPolicy\n\t\tPublicClient        workflowservice.WorkflowServiceClient\n\t\tArchivalMetadata    archiver.ArchivalMetadata\n\t\tArchiverProvider    provider.ArchiverProvider\n\t\tAuthorizer          authorization.Authorizer\n\t}\n\n\t// MembershipMonitorFactory provides a bootstrapped membership monitor\n\tMembershipMonitorFactory interface {\n\t\t// GetMembershipMonitor return a membership monitor\n\t\tGetMembershipMonitor() (membership.Monitor, error)\n\t}\n\n\t// Service contains the objects specific to this service\n\tserviceImpl struct {\n\t\tstatus                int32\n\t\tsName                 string\n\t\thostName              string\n\t\thostInfo              *membership.HostInfo\n\t\ttchannelDispatcher    *yarpc.Dispatcher\n\t\tgrpcListener          net.Listener\n\t\tringpopDispatcher     *yarpc.Dispatcher\n\t\tmembershipFactory     MembershipMonitorFactory\n\t\tmembershipMonitor     membership.Monitor\n\t\trpcFactory            common.RPCFactory\n\t\tpprofInitializer      common.PProfInitializer\n\t\tclientBean            client.Bean\n\t\ttimeSource            clock.TimeSource\n\t\tnumberOfHistoryShards int\n\n\t\tlogger          log.Logger\n\t\tthrottledLogger log.Logger\n\n\t\tmetricsScope           tally.Scope\n\t\truntimeMetricsReporter *metrics.RuntimeMetricsReporter\n\t\tmetricsClient          metrics.Client\n\t\tclusterMetadata        cluster.Metadata\n\t\tmessagingClient        messaging.Client\n\t\tdynamicCollection      *dynamicconfig.Collection\n\t\tdispatcherProvider     client.DispatcherProvider\n\t\tarchivalMetadata       archiver.ArchivalMetadata\n\t\tarchiverProvider       provider.ArchiverProvider\n\t\tserializer             persistence.PayloadSerializer\n\t}\n)\n\nvar _ Service = (*serviceImpl)(nil)\n\n// New instantiates a Service Instance\n// TODO: have a better name for Service.\nfunc New(params *BootstrapParams) Service {\n\tsVice := &serviceImpl{\n\t\tstatus:                common.DaemonStatusInitialized,\n\t\tsName:                 params.Name,\n\t\tlogger:                params.Logger,\n\t\tthrottledLogger:       params.ThrottledLogger,\n\t\trpcFactory:            params.RPCFactory,\n\t\tmembershipFactory:     params.MembershipFactory,\n\t\tpprofInitializer:      params.PProfInitializer,\n\t\ttimeSource:            clock.NewRealTimeSource(),\n\t\tmetricsScope:          params.MetricScope,\n\t\tnumberOfHistoryShards: params.PersistenceConfig.NumHistoryShards,\n\t\tclusterMetadata:       params.ClusterMetadata,\n\t\tmetricsClient:         params.MetricsClient,\n\t\tmessagingClient:       params.MessagingClient,\n\t\tdispatcherProvider:    params.DispatcherProvider,\n\t\tdynamicCollection:     dynamicconfig.NewCollection(params.DynamicConfig, params.Logger),\n\t\tarchivalMetadata:      params.ArchivalMetadata,\n\t\tarchiverProvider:      params.ArchiverProvider,\n\t\tserializer:            persistence.NewPayloadSerializer(),\n\t}\n\n\tsVice.runtimeMetricsReporter = metrics.NewRuntimeMetricsReporter(params.MetricScope, time.Minute, sVice.GetLogger(), params.InstanceID)\n\tsVice.tchannelDispatcher = sVice.rpcFactory.GetTChannelDispatcher()\n\tif sVice.tchannelDispatcher == nil {\n\t\tsVice.logger.Fatal(\"Unable to create yarpc TChannel dispatcher\")\n\t}\n\n\tsVice.grpcListener = sVice.rpcFactory.GetGRPCListener()\n\tif sVice.grpcListener == nil {\n\t\tsVice.logger.Fatal(\"Unable to create gRPC listener\")\n\t}\n\n\tsVice.ringpopDispatcher = sVice.rpcFactory.GetRingpopDispatcher()\n\tif sVice.ringpopDispatcher == nil {\n\t\tsVice.logger.Fatal(\"Unable to create yarpc dispatcher for ringpop\")\n\t}\n\n\t// Get the host name and set it on the service.  This is used for emitting metric with a tag for hostname\n\tif hostName, err := os.Hostname(); err != nil {\n\t\tsVice.logger.WithTags(tag.Error(err)).Fatal(\"Error getting hostname\")\n\t} else {\n\t\tsVice.hostName = hostName\n\t}\n\treturn sVice\n}\n\n// UpdateLoggerWithServiceName tag logging with service name from the top level\nfunc (params *BootstrapParams) UpdateLoggerWithServiceName(name string) {\n\tparams.Logger = params.Logger.WithTags(tag.Service(name))\n\tparams.ThrottledLogger = params.ThrottledLogger.WithTags(tag.Service(name))\n}\n\n// GetHostName returns the name of host running the service\nfunc (h *serviceImpl) GetHostName() string {\n\treturn h.hostName\n}\n\n// Start starts a yarpc service\nfunc (h *serviceImpl) Start() {\n\tif !atomic.CompareAndSwapInt32(&h.status, common.DaemonStatusInitialized, common.DaemonStatusStarted) {\n\t\treturn\n\t}\n\n\tvar err error\n\n\th.metricsScope.Counter(metrics.RestartCount).Inc(1)\n\th.runtimeMetricsReporter.Start()\n\n\tif err := h.pprofInitializer.Start(); err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"Failed to start pprof\")\n\t}\n\n\tif err := h.tchannelDispatcher.Start(); err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"Failed to start yarpc TChannel dispatcher\")\n\t}\n\n\tif err := h.ringpopDispatcher.Start(); err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"Failed to start yarpc dispatcher for ringpop\")\n\t}\n\n\th.membershipMonitor, err = h.membershipFactory.GetMembershipMonitor()\n\tif err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"Membership monitor creation failed\")\n\t}\n\n\th.membershipMonitor.Start()\n\n\thostInfo, err := h.membershipMonitor.WhoAmI()\n\tif err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"failed to get host info from membership monitor\")\n\t}\n\th.hostInfo = hostInfo\n\n\th.clientBean, err = client.NewClientBean(\n\t\tclient.NewRPCClientFactory(h.rpcFactory, h.membershipMonitor, h.metricsClient, h.dynamicCollection, h.numberOfHistoryShards, h.logger),\n\t\th.dispatcherProvider,\n\t\th.clusterMetadata,\n\t)\n\tif err != nil {\n\t\th.logger.WithTags(tag.Error(err)).Fatal(\"fail to initialize client bean\")\n\t}\n\n\t// The service is now started up\n\th.logger.Info(\"Service started\")\n\t// seed the random generator once for this service\n\trand.Seed(time.Now().UTC().UnixNano())\n}\n\n// Stop closes the associated transport\nfunc (h *serviceImpl) Stop() {\n\tif !atomic.CompareAndSwapInt32(&h.status, common.DaemonStatusStarted, common.DaemonStatusStopped) {\n\t\treturn\n\t}\n\n\tif h.membershipMonitor != nil {\n\t\th.membershipMonitor.Stop()\n\t}\n\n\tif h.ringpopDispatcher != nil {\n\t\t_ = h.ringpopDispatcher.Stop()\n\t}\n\n\tif h.tchannelDispatcher != nil {\n\t\t_ = h.tchannelDispatcher.Stop()\n\t}\n\n\th.runtimeMetricsReporter.Stop()\n}\n\nfunc (h *serviceImpl) GetLogger() log.Logger {\n\treturn h.logger\n}\n\nfunc (h *serviceImpl) GetThrottledLogger() log.Logger {\n\treturn h.throttledLogger\n}\n\nfunc (h *serviceImpl) GetMetricsClient() metrics.Client {\n\treturn h.metricsClient\n}\n\nfunc (h *serviceImpl) GetClientBean() client.Bean {\n\treturn h.clientBean\n}\n\nfunc (h *serviceImpl) GetTimeSource() clock.TimeSource {\n\treturn h.timeSource\n}\n\nfunc (h *serviceImpl) GetMembershipMonitor() membership.Monitor {\n\treturn h.membershipMonitor\n}\n\nfunc (h *serviceImpl) GetHostInfo() *membership.HostInfo {\n\treturn h.hostInfo\n}\n\nfunc (h *serviceImpl) GetDispatcher() *yarpc.Dispatcher {\n\treturn h.tchannelDispatcher\n}\n\nfunc (h *serviceImpl) GetGRPCListener() net.Listener {\n\treturn h.grpcListener\n}\n\n// GetClusterMetadata returns the service cluster metadata\nfunc (h *serviceImpl) GetClusterMetadata() cluster.Metadata {\n\treturn h.clusterMetadata\n}\n\n// GetMessagingClient returns the messaging client against Kafka\nfunc (h *serviceImpl) GetMessagingClient() messaging.Client {\n\treturn h.messagingClient\n}\n\nfunc (h *serviceImpl) GetArchivalMetadata() archiver.ArchivalMetadata {\n\treturn h.archivalMetadata\n}\n\nfunc (h *serviceImpl) GetArchiverProvider() provider.ArchiverProvider {\n\treturn h.archiverProvider\n}\n\nfunc (h *serviceImpl) GetPayloadSerializer() persistence.PayloadSerializer {\n\treturn h.serializer\n}\n\n// GetMetricsServiceIdx returns the metrics name\nfunc GetMetricsServiceIdx(serviceName string, logger log.Logger) metrics.ServiceIdx {\n\tswitch serviceName {\n\tcase common.FrontendServiceName:\n\t\treturn metrics.Frontend\n\tcase common.HistoryServiceName:\n\t\treturn metrics.History\n\tcase common.MatchingServiceName:\n\t\treturn metrics.Matching\n\tcase common.WorkerServiceName:\n\t\treturn metrics.Worker\n\tdefault:\n\t\tlogger.Fatal(\"Unknown service name '%v' for metrics!\", tag.Service(serviceName))\n\t}\n\n\t// this should never happen!\n\treturn metrics.NumServices\n}\n", "idx": 8, "id": 9260, "msg": "", "proj": "temporalio-temporal", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,35 @@\n+/*\n+ * Copyright ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ *\n+ */\n+package org.hyperledger.besu.evmtool;\n+\n+import static picocli.CommandLine.defaultExceptionHandler;\n+\n+import picocli.CommandLine;\n+\n+public final class EvmTool {\n+  private static final int SUCCESS_EXIT_CODE = 0;\n+  private static final int ERROR_EXIT_CODE = 1;\n+\n+  public static void main(final String... args) {\n+\n+    final EvmToolCommand evmToolCommand = new EvmToolCommand();\n+\n+    evmToolCommand.parse(\n+        new CommandLine.RunLast().andExit(SUCCESS_EXIT_CODE),\n+        defaultExceptionHandler().andExit(ERROR_EXIT_CODE),\n+        args);\n+  }\n+}", "y": 1, "oldf": "", "idx": 1, "id": 22415, "msg": "Why do we need a separate main class ? I thought it was a classical subcommand.", "proj": "hyperledger-besu", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -2359,6 +2359,13 @@ bool CompleteAliasType::operator==(const CompleteAliasType& other) const\n             m_body == other.m_body;\n }\n \n+//bool CompleteAliasType::consistent(const CompleteAliasType &x,\n+//        const TypeConsistencyEnforcementQosPolicy& consistency) const\n+//{\n+//    return m_header.consistent(x.m_header, consistency)\n+//        && m_body.consistent(x.m_body, consistency);\n+//}\n+\n MinimalAliasType::MinimalAliasType()\n {\n }", "y": 0, "oldf": "// Copyright 2018 Proyectos y Sistemas de Mantenimiento SL (eProsima).\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n#include <fastrtps/types/TypeObject.h>\n#include <fastcdr/exceptions/BadParamException.h>\n#include <fastcdr/Cdr.h>\n\n// The types in this file shall be serialized with XCDR encoding version 2\nnamespace eprosima{\nnamespace fastrtps{\n\nusing namespace rtps;\nusing namespace eprosima::fastcdr::exception;\n\nnamespace types{\n\nCommonStructMember::CommonStructMember()\n{\n}\n\nCommonStructMember::~CommonStructMember()\n{\n}\n\nCommonStructMember::CommonStructMember(const CommonStructMember &x)\n{\n    m_member_id = x.m_member_id;\n    m_member_flags = x.m_member_flags;\n    m_member_type_id = x.m_member_type_id;\n}\n\nCommonStructMember::CommonStructMember(CommonStructMember &&x)\n{\n    m_member_id = std::move(x.m_member_id);\n    m_member_flags = std::move(x.m_member_flags);\n    m_member_type_id = std::move(x.m_member_type_id);\n}\n\nCommonStructMember& CommonStructMember::operator=(const CommonStructMember &x)\n{\n    m_member_id = x.m_member_id;\n    m_member_flags = x.m_member_flags;\n    m_member_type_id = x.m_member_type_id;\n\n    return *this;\n}\n\nCommonStructMember& CommonStructMember::operator=(CommonStructMember &&x)\n{\n    m_member_id = std::move(x.m_member_id);\n    m_member_flags = std::move(x.m_member_flags);\n    m_member_type_id = std::move(x.m_member_type_id);\n\n    return *this;\n}\n\nsize_t CommonStructMember::getCdrSerializedSize(const CommonStructMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    current_alignment += StructMemberFlag::getCdrSerializedSize(data.member_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.member_type_id(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonStructMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_member_id;\n    scdr << m_member_flags;\n    scdr << m_member_type_id;\n}\n\nvoid CommonStructMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_member_id;\n    dcdr >> m_member_flags;\n    dcdr >> m_member_type_id;\n}\n\nbool CommonStructMember::operator==(const CommonStructMember& other) const\n{\n    return m_member_id == other.m_member_id &&\n            m_member_flags == other.m_member_flags &&\n            m_member_type_id == other.m_member_type_id;\n}\n\nCompleteMemberDetail::CompleteMemberDetail()\n{\n}\n\nCompleteMemberDetail::~CompleteMemberDetail()\n{\n}\n\nCompleteMemberDetail::CompleteMemberDetail(const CompleteMemberDetail &x)\n{\n    m_name = x.m_name;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n}\n\nCompleteMemberDetail::CompleteMemberDetail(CompleteMemberDetail &&x)\n{\n    m_name = std::move(x.m_name);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n}\n\nCompleteMemberDetail& CompleteMemberDetail::operator=(const CompleteMemberDetail &x)\n{\n    m_name = x.m_name;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n\n    return *this;\n}\n\nCompleteMemberDetail& CompleteMemberDetail::operator=(CompleteMemberDetail &&x)\n{\n    m_name = std::move(x.m_name);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n\n    return *this;\n}\n\nsize_t CompleteMemberDetail::getCdrSerializedSize(const CompleteMemberDetail& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.name().size()  + 1;\n    current_alignment += AppliedBuiltinMemberAnnotations::getCdrSerializedSize(data.ann_builtin(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.ann_custom().size(); ++a)\n    {\n        current_alignment += AppliedAnnotation::getCdrSerializedSize(data.ann_custom().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteMemberDetail::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_name;\n    scdr << m_ann_builtin;\n    scdr << m_ann_custom;\n}\n\nvoid CompleteMemberDetail::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_name;\n    dcdr >> m_ann_builtin;\n    dcdr >> m_ann_custom;\n}\n\nbool CompleteMemberDetail::operator==(const CompleteMemberDetail& other) const\n{\n    if(m_name == other.m_name &&\n        m_ann_builtin == other.m_ann_builtin)\n    {\n        return compareSequence(m_ann_custom, other.m_ann_custom);\n    }\n    return false;\n}\n\nMinimalMemberDetail::MinimalMemberDetail()\n{\n}\n\nMinimalMemberDetail::~MinimalMemberDetail()\n{\n}\n\nMinimalMemberDetail::MinimalMemberDetail(const MinimalMemberDetail &x)\n{\n    m_name_hash = x.m_name_hash;\n}\n\nMinimalMemberDetail::MinimalMemberDetail(MinimalMemberDetail &&x)\n{\n    m_name_hash = std::move(x.m_name_hash);\n}\n\nMinimalMemberDetail& MinimalMemberDetail::operator=(const MinimalMemberDetail &x)\n{\n    m_name_hash = x.m_name_hash;\n\n    return *this;\n}\n\nMinimalMemberDetail& MinimalMemberDetail::operator=(MinimalMemberDetail &&x)\n{\n    m_name_hash = std::move(x.m_name_hash);\n\n    return *this;\n}\n\nsize_t MinimalMemberDetail::getCdrSerializedSize(const MinimalMemberDetail&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += ((4) * 1) + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalMemberDetail::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_name_hash;\n}\n\nvoid MinimalMemberDetail::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_name_hash;\n}\n\nbool MinimalMemberDetail::operator==(const MinimalMemberDetail& other) const\n{\n    return m_name_hash == other.m_name_hash;\n}\n\nCompleteStructMember::CompleteStructMember()\n{\n}\n\nCompleteStructMember::~CompleteStructMember()\n{\n}\n\nCompleteStructMember::CompleteStructMember(const CompleteStructMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteStructMember::CompleteStructMember(CompleteStructMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteStructMember& CompleteStructMember::operator=(const CompleteStructMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteStructMember& CompleteStructMember::operator=(CompleteStructMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteStructMember::getCdrSerializedSize(const CompleteStructMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonStructMember::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteStructMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteStructMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteStructMember::operator==(const CompleteStructMember& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalStructMember::MinimalStructMember()\n{\n}\n\nMinimalStructMember::~MinimalStructMember()\n{\n}\n\nMinimalStructMember::MinimalStructMember(const MinimalStructMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nMinimalStructMember::MinimalStructMember(MinimalStructMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalStructMember& MinimalStructMember::operator=(const MinimalStructMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalStructMember& MinimalStructMember::operator=(MinimalStructMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalStructMember::getCdrSerializedSize(const MinimalStructMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonStructMember::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += MinimalMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalStructMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid MinimalStructMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool MinimalStructMember::operator==(const MinimalStructMember& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nAppliedBuiltinTypeAnnotations::AppliedBuiltinTypeAnnotations()\n{\n}\n\nAppliedBuiltinTypeAnnotations::~AppliedBuiltinTypeAnnotations()\n{\n}\n\nAppliedBuiltinTypeAnnotations::AppliedBuiltinTypeAnnotations(const AppliedBuiltinTypeAnnotations &x)\n{\n    m_verbatim = x.m_verbatim;\n}\n\nAppliedBuiltinTypeAnnotations::AppliedBuiltinTypeAnnotations(AppliedBuiltinTypeAnnotations &&x)\n{\n    m_verbatim = std::move(x.m_verbatim);\n}\n\nAppliedBuiltinTypeAnnotations& AppliedBuiltinTypeAnnotations::operator=(const AppliedBuiltinTypeAnnotations &x)\n{\n    m_verbatim = x.m_verbatim;\n\n    return *this;\n}\n\nAppliedBuiltinTypeAnnotations& AppliedBuiltinTypeAnnotations::operator=(AppliedBuiltinTypeAnnotations &&x)\n{\n    m_verbatim = std::move(x.m_verbatim);\n\n    return *this;\n}\n\nsize_t AppliedBuiltinTypeAnnotations::getCdrSerializedSize(const AppliedBuiltinTypeAnnotations& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AppliedVerbatimAnnotation::getCdrSerializedSize(data.verbatim(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid AppliedBuiltinTypeAnnotations::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_verbatim;\n}\n\nvoid AppliedBuiltinTypeAnnotations::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_verbatim;\n}\n\nbool AppliedBuiltinTypeAnnotations::operator==(const AppliedBuiltinTypeAnnotations& other) const\n{\n    return m_verbatim == other.m_verbatim;\n}\n\nMinimalTypeDetail::MinimalTypeDetail()\n{\n}\n\nMinimalTypeDetail::~MinimalTypeDetail()\n{\n}\n\nMinimalTypeDetail::MinimalTypeDetail(const MinimalTypeDetail &)\n{\n}\n\nMinimalTypeDetail::MinimalTypeDetail(MinimalTypeDetail &&)\n{\n}\n\nMinimalTypeDetail& MinimalTypeDetail::operator=(const MinimalTypeDetail &)\n{\n    return *this;\n}\n\nMinimalTypeDetail& MinimalTypeDetail::operator=(MinimalTypeDetail &&)\n{\n    return *this;\n}\n\nsize_t MinimalTypeDetail::getCdrSerializedSize(const MinimalTypeDetail&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalTypeDetail::serialize(eprosima::fastcdr::Cdr &) const\n{\n}\n\nvoid MinimalTypeDetail::deserialize(eprosima::fastcdr::Cdr &)\n{\n}\n\nCompleteTypeDetail::CompleteTypeDetail()\n{\n}\n\nCompleteTypeDetail::~CompleteTypeDetail()\n{\n}\n\nCompleteTypeDetail::CompleteTypeDetail(const CompleteTypeDetail &x)\n{\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n    m_type_name = x.m_type_name;\n}\n\nCompleteTypeDetail::CompleteTypeDetail(CompleteTypeDetail &&x)\n{\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n    m_type_name = std::move(x.m_type_name);\n}\n\nCompleteTypeDetail& CompleteTypeDetail::operator=(const CompleteTypeDetail &x)\n{\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n    m_type_name = x.m_type_name;\n\n    return *this;\n}\n\nCompleteTypeDetail& CompleteTypeDetail::operator=(CompleteTypeDetail &&x)\n{\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n    m_type_name = std::move(x.m_type_name);\n\n    return *this;\n}\n\nsize_t CompleteTypeDetail::getCdrSerializedSize(const CompleteTypeDetail& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AppliedBuiltinTypeAnnotations::getCdrSerializedSize(data.ann_builtin(), current_alignment);\n\tcurrent_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.ann_custom().size(); ++a)\n    {\n        current_alignment += AppliedAnnotation::getCdrSerializedSize(data.ann_custom().at(a), current_alignment);\n    }\n\tcurrent_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.type_name().size() + 1;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteTypeDetail::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_ann_builtin;\n    scdr << m_ann_custom;\n    scdr << m_type_name;\n}\n\nvoid CompleteTypeDetail::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_ann_builtin;\n    dcdr >> m_ann_custom;\n    dcdr >> m_type_name;\n}\n\nbool CompleteTypeDetail::operator==(const CompleteTypeDetail& other) const\n{\n    if (m_ann_builtin == other.m_ann_builtin)\n    {\n        return compareSequence(m_ann_custom, other.m_ann_custom);\n    }\n    return false;\n}\n\nCompleteStructHeader::CompleteStructHeader()\n{\n}\n\nCompleteStructHeader::~CompleteStructHeader()\n{\n}\n\nCompleteStructHeader::CompleteStructHeader(const CompleteStructHeader &x)\n{\n    m_base_type = x.m_base_type;\n    m_detail = x.m_detail;\n}\n\nCompleteStructHeader::CompleteStructHeader(CompleteStructHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteStructHeader& CompleteStructHeader::operator=(const CompleteStructHeader &x)\n{\n    m_base_type = x.m_base_type;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteStructHeader& CompleteStructHeader::operator=(CompleteStructHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteStructHeader::getCdrSerializedSize(const CompleteStructHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.base_type(), current_alignment);\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteStructHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_base_type;\n    scdr << m_detail;\n}\n\nvoid CompleteStructHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_base_type;\n    dcdr >> m_detail;\n}\n\nbool CompleteStructHeader::operator==(const CompleteStructHeader& other) const\n{\n    return m_base_type == other.m_base_type &&\n            m_detail == other.m_detail;\n}\n\nMinimalStructHeader::MinimalStructHeader()\n{\n}\n\nMinimalStructHeader::~MinimalStructHeader()\n{\n}\n\nMinimalStructHeader::MinimalStructHeader(const MinimalStructHeader &x)\n{\n    m_base_type = x.m_base_type;\n    m_detail = x.m_detail;\n}\n\nMinimalStructHeader::MinimalStructHeader(MinimalStructHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalStructHeader& MinimalStructHeader::operator=(const MinimalStructHeader &x)\n{\n    m_base_type = x.m_base_type;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalStructHeader& MinimalStructHeader::operator=(MinimalStructHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalStructHeader::getCdrSerializedSize(const MinimalStructHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.base_type(), current_alignment);\n    current_alignment += MinimalTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalStructHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_base_type;\n    scdr << m_detail;\n}\n\nvoid MinimalStructHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_base_type;\n    dcdr >> m_detail;\n}\n\nbool MinimalStructHeader::operator==(const MinimalStructHeader& other) const\n{\n    return m_base_type == other.m_base_type &&\n            m_detail == other.m_detail;\n}\n\nCompleteStructType::CompleteStructType()\n{\n}\n\nCompleteStructType::~CompleteStructType()\n{\n}\n\nCompleteStructType::CompleteStructType(const CompleteStructType &x)\n{\n    m_struct_flags = x.m_struct_flags;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n}\n\nCompleteStructType::CompleteStructType(CompleteStructType &&x)\n{\n    m_struct_flags = std::move(x.m_struct_flags);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nCompleteStructType& CompleteStructType::operator=(const CompleteStructType &x)\n{\n    m_struct_flags = x.m_struct_flags;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nCompleteStructType& CompleteStructType::operator=(CompleteStructType &&x)\n{\n    m_struct_flags = std::move(x.m_struct_flags);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t CompleteStructType::getCdrSerializedSize(const CompleteStructType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += StructTypeFlag::getCdrSerializedSize(data.struct_flags(), current_alignment);\n    current_alignment += CompleteStructHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += CompleteStructMember::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteStructType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_struct_flags;\n    scdr << m_header;\n    scdr << m_member_seq;\n}\n\nvoid CompleteStructType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_struct_flags;\n    dcdr >> m_header;\n    dcdr >> m_member_seq;\n}\n\nbool CompleteStructType::operator==(const CompleteStructType& other) const\n{\n    if(m_struct_flags == other.m_struct_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nMinimalStructType::MinimalStructType()\n{\n}\n\nMinimalStructType::~MinimalStructType()\n{\n}\n\nMinimalStructType::MinimalStructType(const MinimalStructType &x)\n{\n    m_struct_flags = x.m_struct_flags;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n}\n\nMinimalStructType::MinimalStructType(MinimalStructType &&x)\n{\n    m_struct_flags = std::move(x.m_struct_flags);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nMinimalStructType& MinimalStructType::operator=(const MinimalStructType &x)\n{\n    m_struct_flags = x.m_struct_flags;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nMinimalStructType& MinimalStructType::operator=(MinimalStructType &&x)\n{\n    m_struct_flags = std::move(x.m_struct_flags);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t MinimalStructType::getCdrSerializedSize(const MinimalStructType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += StructTypeFlag::getCdrSerializedSize(data.struct_flags(), current_alignment);\n    current_alignment += MinimalStructHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += MinimalStructMember::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalStructType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_struct_flags;\n    scdr << m_header;\n    scdr << m_member_seq;\n}\n\nvoid MinimalStructType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_struct_flags;\n    dcdr >> m_header;\n    dcdr >> m_member_seq;\n}\n\nbool MinimalStructType::operator==(const MinimalStructType& other) const\n{\n    if(m_struct_flags == other.m_struct_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nCommonUnionMember::CommonUnionMember()\n{\n}\n\nCommonUnionMember::~CommonUnionMember()\n{\n}\n\nCommonUnionMember::CommonUnionMember(const CommonUnionMember &x)\n{\n\tm_member_id = x.m_member_id;\n    m_member_flags = x.m_member_flags;\n    m_type_id = x.m_type_id;\n    m_label_seq = x.m_label_seq;\n}\n\nCommonUnionMember::CommonUnionMember(CommonUnionMember &&x)\n{\n\tm_member_id = std::move(x.m_member_id);\n    m_member_flags = std::move(x.m_member_flags);\n    m_type_id = std::move(x.m_type_id);\n    m_label_seq = std::move(x.m_label_seq);\n}\n\nCommonUnionMember& CommonUnionMember::operator=(const CommonUnionMember &x)\n{\n\tm_member_id = x.m_member_id;\n    m_member_flags = x.m_member_flags;\n    m_type_id = x.m_type_id;\n    m_label_seq = x.m_label_seq;\n\n    return *this;\n}\n\nCommonUnionMember& CommonUnionMember::operator=(CommonUnionMember &&x)\n{\n\tm_member_id = std::move(x.m_member_id);\n    m_member_flags = std::move(x.m_member_flags);\n    m_type_id = std::move(x.m_type_id);\n    m_label_seq = std::move(x.m_label_seq);\n\n    return *this;\n}\n\nsize_t CommonUnionMember::getCdrSerializedSize(const CommonUnionMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    current_alignment += UnionMemberFlag::getCdrSerializedSize(data.member_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_id(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.label_seq().size(); ++a)\n    {\n        current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonUnionMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_member_id;\n    scdr << m_member_flags;\n    scdr << m_type_id;\n    scdr << m_label_seq;\n}\n\nvoid CommonUnionMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_member_id;\n    dcdr >> m_member_flags;\n    dcdr >> m_type_id;\n    dcdr >> m_label_seq;\n}\n\nbool CommonUnionMember::operator==(const CommonUnionMember& other) const\n{\n    if(m_member_id == other.m_member_id &&\n        m_member_flags == other.m_member_flags &&\n        m_type_id == other.m_type_id)\n    {\n        return compareSequence(m_label_seq, other.m_label_seq);\n    }\n    return false;\n}\n\nCompleteUnionMember::CompleteUnionMember()\n{\n}\n\nCompleteUnionMember::~CompleteUnionMember()\n{\n}\n\nCompleteUnionMember::CompleteUnionMember(const CompleteUnionMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteUnionMember::CompleteUnionMember(CompleteUnionMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteUnionMember& CompleteUnionMember::operator=(const CompleteUnionMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteUnionMember& CompleteUnionMember::operator=(CompleteUnionMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteUnionMember::getCdrSerializedSize(const CompleteUnionMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonUnionMember::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteUnionMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteUnionMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteUnionMember::operator==(const CompleteUnionMember& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalUnionMember::MinimalUnionMember()\n{\n}\n\nMinimalUnionMember::~MinimalUnionMember()\n{\n}\n\nMinimalUnionMember::MinimalUnionMember(const MinimalUnionMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nMinimalUnionMember::MinimalUnionMember(MinimalUnionMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalUnionMember& MinimalUnionMember::operator=(const MinimalUnionMember &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalUnionMember& MinimalUnionMember::operator=(MinimalUnionMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalUnionMember::getCdrSerializedSize(const MinimalUnionMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonUnionMember::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += MinimalMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalUnionMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid MinimalUnionMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool MinimalUnionMember::operator==(const MinimalUnionMember& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nCommonDiscriminatorMember::CommonDiscriminatorMember()\n{\n}\n\nCommonDiscriminatorMember::~CommonDiscriminatorMember()\n{\n}\n\nCommonDiscriminatorMember::CommonDiscriminatorMember(const CommonDiscriminatorMember &x)\n{\n    m_member_flags = x.m_member_flags;\n    m_type_id = x.m_type_id;\n}\n\nCommonDiscriminatorMember::CommonDiscriminatorMember(CommonDiscriminatorMember &&x)\n{\n    m_member_flags = std::move(x.m_member_flags);\n    m_type_id = std::move(x.m_type_id);\n}\n\nCommonDiscriminatorMember& CommonDiscriminatorMember::operator=(const CommonDiscriminatorMember &x)\n{\n    m_member_flags = x.m_member_flags;\n    m_type_id = x.m_type_id;\n\n    return *this;\n}\n\nCommonDiscriminatorMember& CommonDiscriminatorMember::operator=(CommonDiscriminatorMember &&x)\n{\n    m_member_flags = std::move(x.m_member_flags);\n    m_type_id = std::move(x.m_type_id);\n\n    return *this;\n}\n\nsize_t CommonDiscriminatorMember::getCdrSerializedSize(const CommonDiscriminatorMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += UnionDiscriminatorFlag::getCdrSerializedSize(data.member_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_id(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonDiscriminatorMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_member_flags;\n    scdr << m_type_id;\n}\n\nvoid CommonDiscriminatorMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_member_flags;\n    dcdr >> m_type_id;\n}\n\nbool CommonDiscriminatorMember::operator==(const CommonDiscriminatorMember& other) const\n{\n    return m_member_flags == other.m_member_flags &&\n            m_type_id == other.m_type_id;\n}\n\nCompleteDiscriminatorMember::CompleteDiscriminatorMember()\n{\n}\n\nCompleteDiscriminatorMember::~CompleteDiscriminatorMember()\n{\n}\n\nCompleteDiscriminatorMember::CompleteDiscriminatorMember(const CompleteDiscriminatorMember &x)\n{\n    m_common = x.m_common;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n}\n\nCompleteDiscriminatorMember::CompleteDiscriminatorMember(CompleteDiscriminatorMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n}\n\nCompleteDiscriminatorMember& CompleteDiscriminatorMember::operator=(const CompleteDiscriminatorMember &x)\n{\n    m_common = x.m_common;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n\n    return *this;\n}\n\nCompleteDiscriminatorMember& CompleteDiscriminatorMember::operator=(CompleteDiscriminatorMember &&x)\n{\n    m_common = std::move(x.m_common);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n\n    return *this;\n}\n\nsize_t CompleteDiscriminatorMember::getCdrSerializedSize(const CompleteDiscriminatorMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonDiscriminatorMember::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += AppliedBuiltinTypeAnnotations::getCdrSerializedSize(data.ann_builtin(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.ann_custom().size(); ++a)\n    {\n        current_alignment += AppliedAnnotation::getCdrSerializedSize(data.ann_custom().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteDiscriminatorMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_ann_builtin;\n    scdr << m_ann_custom;\n}\n\nvoid CompleteDiscriminatorMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_ann_builtin;\n    dcdr >> m_ann_custom;\n}\n\nbool CompleteDiscriminatorMember::operator==(const CompleteDiscriminatorMember& other) const\n{\n    if(m_common == other.m_common &&\n        m_ann_builtin == other.m_ann_builtin)\n    {\n        return compareSequence(m_ann_custom, other.m_ann_custom);\n    }\n    return false;\n}\n\nMinimalDiscriminatorMember::MinimalDiscriminatorMember()\n{\n}\n\nMinimalDiscriminatorMember::~MinimalDiscriminatorMember()\n{\n}\n\nMinimalDiscriminatorMember::MinimalDiscriminatorMember(const MinimalDiscriminatorMember &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalDiscriminatorMember::MinimalDiscriminatorMember(MinimalDiscriminatorMember &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalDiscriminatorMember& MinimalDiscriminatorMember::operator=(const MinimalDiscriminatorMember &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalDiscriminatorMember& MinimalDiscriminatorMember::operator=(MinimalDiscriminatorMember &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalDiscriminatorMember::getCdrSerializedSize(const MinimalDiscriminatorMember& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonDiscriminatorMember::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalDiscriminatorMember::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalDiscriminatorMember::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalDiscriminatorMember::operator==(const MinimalDiscriminatorMember& other) const\n{\n    return m_common == other.m_common;\n}\n\nCompleteUnionHeader::CompleteUnionHeader()\n{\n}\n\nCompleteUnionHeader::~CompleteUnionHeader()\n{\n}\n\nCompleteUnionHeader::CompleteUnionHeader(const CompleteUnionHeader &x)\n{\n    m_detail = x.m_detail;\n}\n\nCompleteUnionHeader::CompleteUnionHeader(CompleteUnionHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteUnionHeader& CompleteUnionHeader::operator=(const CompleteUnionHeader &x)\n{\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteUnionHeader& CompleteUnionHeader::operator=(CompleteUnionHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteUnionHeader::getCdrSerializedSize(const CompleteUnionHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteUnionHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_detail;\n}\n\nvoid CompleteUnionHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_detail;\n}\n\nbool CompleteUnionHeader::operator==(const CompleteUnionHeader& other) const\n{\n    return m_detail == other.m_detail;\n}\n\nMinimalUnionHeader::MinimalUnionHeader()\n{\n}\n\nMinimalUnionHeader::~MinimalUnionHeader()\n{\n}\n\nMinimalUnionHeader::MinimalUnionHeader(const MinimalUnionHeader &x)\n{\n    m_detail = x.m_detail;\n}\n\nMinimalUnionHeader::MinimalUnionHeader(MinimalUnionHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalUnionHeader& MinimalUnionHeader::operator=(const MinimalUnionHeader &x)\n{\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalUnionHeader& MinimalUnionHeader::operator=(MinimalUnionHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalUnionHeader::getCdrSerializedSize(const MinimalUnionHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += MinimalTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalUnionHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_detail;\n}\n\nvoid MinimalUnionHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_detail;\n}\n\nbool MinimalUnionHeader::operator==(const MinimalUnionHeader& other) const\n{\n    return m_detail == other.m_detail;\n}\n\nCompleteUnionType::CompleteUnionType()\n{\n}\n\nCompleteUnionType::~CompleteUnionType()\n{\n}\n\nCompleteUnionType::CompleteUnionType(const CompleteUnionType &x)\n{\n    m_union_flags = x.m_union_flags;\n    m_header = x.m_header;\n    m_discriminator = x.m_discriminator;\n    m_member_seq = x.m_member_seq;\n}\n\nCompleteUnionType::CompleteUnionType(CompleteUnionType &&x)\n{\n    m_union_flags = std::move(x.m_union_flags);\n    m_header = std::move(x.m_header);\n    m_discriminator = std::move(x.m_discriminator);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nCompleteUnionType& CompleteUnionType::operator=(const CompleteUnionType &x)\n{\n    m_union_flags = x.m_union_flags;\n    m_header = x.m_header;\n    m_discriminator = x.m_discriminator;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nCompleteUnionType& CompleteUnionType::operator=(CompleteUnionType &&x)\n{\n    m_union_flags = std::move(x.m_union_flags);\n    m_header = std::move(x.m_header);\n    m_discriminator = std::move(x.m_discriminator);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t CompleteUnionType::getCdrSerializedSize(const CompleteUnionType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += UnionTypeFlag::getCdrSerializedSize(data.union_flags(), current_alignment);\n    current_alignment += CompleteUnionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += CompleteDiscriminatorMember::getCdrSerializedSize(data.discriminator(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += CompleteUnionMember::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteUnionType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_union_flags;\n    scdr << m_header;\n    scdr << m_discriminator;\n    scdr << m_member_seq;\n}\n\nvoid CompleteUnionType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_union_flags;\n    dcdr >> m_header;\n    dcdr >> m_discriminator;\n    dcdr >> m_member_seq;\n}\n\nbool CompleteUnionType::operator==(const CompleteUnionType& other) const\n{\n    if(m_union_flags == other.m_union_flags &&\n        m_header == other.m_header &&\n        m_discriminator == other.m_discriminator)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nMinimalUnionType::MinimalUnionType()\n{\n}\n\nMinimalUnionType::~MinimalUnionType()\n{\n}\n\nMinimalUnionType::MinimalUnionType(const MinimalUnionType &x)\n{\n    m_union_flags = x.m_union_flags;\n    m_header = x.m_header;\n    m_discriminator = x.m_discriminator;\n    m_member_seq = x.m_member_seq;\n}\n\nMinimalUnionType::MinimalUnionType(MinimalUnionType &&x)\n{\n    m_union_flags = std::move(x.m_union_flags);\n    m_header = std::move(x.m_header);\n    m_discriminator = std::move(x.m_discriminator);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nMinimalUnionType& MinimalUnionType::operator=(const MinimalUnionType &x)\n{\n    m_union_flags = x.m_union_flags;\n    m_header = x.m_header;\n    m_discriminator = x.m_discriminator;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nMinimalUnionType& MinimalUnionType::operator=(MinimalUnionType &&x)\n{\n    m_union_flags = std::move(x.m_union_flags);\n    m_header = std::move(x.m_header);\n    m_discriminator = std::move(x.m_discriminator);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t MinimalUnionType::getCdrSerializedSize(const MinimalUnionType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += UnionTypeFlag::getCdrSerializedSize(data.union_flags(), current_alignment);\n    current_alignment += MinimalUnionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += MinimalDiscriminatorMember::getCdrSerializedSize(data.discriminator(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += MinimalUnionMember::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalUnionType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_union_flags;\n    scdr << m_header;\n    scdr << m_discriminator;\n    scdr << m_member_seq;\n}\n\nvoid MinimalUnionType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_union_flags;\n    dcdr >> m_header;\n    dcdr >> m_discriminator;\n    dcdr >> m_member_seq;\n}\n\nbool MinimalUnionType::operator==(const MinimalUnionType& other) const\n{\n    if(m_union_flags == other.m_union_flags &&\n        m_header == other.m_header &&\n        m_discriminator == other.m_discriminator)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nCommonAnnotationParameter::CommonAnnotationParameter()\n{\n}\n\nCommonAnnotationParameter::~CommonAnnotationParameter()\n{\n}\n\nCommonAnnotationParameter::CommonAnnotationParameter(const CommonAnnotationParameter &x)\n{\n    m_member_flags = x.m_member_flags;\n    m_member_type_id = x.m_member_type_id;\n}\n\nCommonAnnotationParameter::CommonAnnotationParameter(CommonAnnotationParameter &&x)\n{\n    m_member_flags = std::move(x.m_member_flags);\n    m_member_type_id = std::move(x.m_member_type_id);\n}\n\nCommonAnnotationParameter& CommonAnnotationParameter::operator=(const CommonAnnotationParameter &x)\n{\n    m_member_flags = x.m_member_flags;\n    m_member_type_id = x.m_member_type_id;\n\n    return *this;\n}\n\nCommonAnnotationParameter& CommonAnnotationParameter::operator=(CommonAnnotationParameter &&x)\n{\n    m_member_flags = std::move(x.m_member_flags);\n    m_member_type_id = std::move(x.m_member_type_id);\n\n    return *this;\n}\n\nsize_t CommonAnnotationParameter::getCdrSerializedSize(const CommonAnnotationParameter& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AnnotationParameterFlag::getCdrSerializedSize(data.member_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.member_type_id(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonAnnotationParameter::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_member_flags;\n    scdr << m_member_type_id;\n}\n\nvoid CommonAnnotationParameter::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_member_flags;\n    dcdr >> m_member_type_id;\n}\n\nbool CommonAnnotationParameter::operator==(const CommonAnnotationParameter& other) const\n{\n    return m_member_flags == other.m_member_flags &&\n            m_member_type_id == other.m_member_type_id;\n}\n\nCompleteAnnotationParameter::CompleteAnnotationParameter()\n{\n}\n\nCompleteAnnotationParameter::~CompleteAnnotationParameter()\n{\n}\n\nCompleteAnnotationParameter::CompleteAnnotationParameter(const CompleteAnnotationParameter &x)\n{\n    m_common = x.m_common;\n    m_name = x.m_name;\n    m_default_value = x.m_default_value;\n}\n\nCompleteAnnotationParameter::CompleteAnnotationParameter(CompleteAnnotationParameter &&x)\n{\n    m_common = std::move(x.m_common);\n    m_name = std::move(x.m_name);\n    m_default_value = std::move(x.m_default_value);\n}\n\nCompleteAnnotationParameter& CompleteAnnotationParameter::operator=(const CompleteAnnotationParameter &x)\n{\n    m_common = x.m_common;\n    m_name = x.m_name;\n    m_default_value = x.m_default_value;\n\n    return *this;\n}\n\nCompleteAnnotationParameter& CompleteAnnotationParameter::operator=(CompleteAnnotationParameter &&x)\n{\n    m_common = std::move(x.m_common);\n    m_name = std::move(x.m_name);\n    m_default_value = std::move(x.m_default_value);\n\n    return *this;\n}\n\nsize_t CompleteAnnotationParameter::getCdrSerializedSize(const CompleteAnnotationParameter& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonAnnotationParameter::getCdrSerializedSize(data.common(), current_alignment);\n\tcurrent_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.name().size() + 1;\n    current_alignment += AnnotationParameterValue::getCdrSerializedSize(data.default_value(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAnnotationParameter::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_name;\n    scdr << m_default_value;\n}\n\nvoid CompleteAnnotationParameter::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_name;\n    dcdr >> m_default_value;\n}\n\nbool CompleteAnnotationParameter::operator==(const CompleteAnnotationParameter& other) const\n{\n    return m_common == other.m_common &&\n            m_name == other.m_name &&\n            m_default_value == other.m_default_value;\n}\n\nMinimalAnnotationParameter::MinimalAnnotationParameter()\n{\n}\n\nMinimalAnnotationParameter::~MinimalAnnotationParameter()\n{\n}\n\nMinimalAnnotationParameter::MinimalAnnotationParameter(const MinimalAnnotationParameter &x)\n{\n    m_common = x.m_common;\n    m_name = x.m_name;\n    m_default_value = x.m_default_value;\n}\n\nMinimalAnnotationParameter::MinimalAnnotationParameter(MinimalAnnotationParameter &&x)\n{\n    m_common = std::move(x.m_common);\n    m_name = std::move(x.m_name);\n    m_default_value = std::move(x.m_default_value);\n}\n\nMinimalAnnotationParameter& MinimalAnnotationParameter::operator=(const MinimalAnnotationParameter &x)\n{\n    m_common = x.m_common;\n    m_name = x.m_name;\n    m_default_value = x.m_default_value;\n\n    return *this;\n}\n\nMinimalAnnotationParameter& MinimalAnnotationParameter::operator=(MinimalAnnotationParameter &&x)\n{\n    m_common = std::move(x.m_common);\n    m_name = std::move(x.m_name);\n    m_default_value = std::move(x.m_default_value);\n\n    return *this;\n}\n\nsize_t MinimalAnnotationParameter::getCdrSerializedSize(const MinimalAnnotationParameter& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonAnnotationParameter::getCdrSerializedSize(data.common(), current_alignment);\n\tcurrent_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.name().size() + 1;\n    current_alignment += AnnotationParameterValue::getCdrSerializedSize(data.default_value(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAnnotationParameter::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_name;\n    scdr << m_default_value;\n}\n\nvoid MinimalAnnotationParameter::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_name;\n    dcdr >> m_default_value;\n}\n\nbool MinimalAnnotationParameter::operator==(const MinimalAnnotationParameter& other) const\n{\n    return m_common == other.m_common &&\n            m_name == other.m_name &&\n            m_default_value == other.m_default_value;\n}\n\nCompleteAnnotationHeader::CompleteAnnotationHeader()\n{\n}\n\nCompleteAnnotationHeader::~CompleteAnnotationHeader()\n{\n}\n\nCompleteAnnotationHeader::CompleteAnnotationHeader(const CompleteAnnotationHeader &x)\n{\n    m_annotation_name = x.m_annotation_name;\n}\n\nCompleteAnnotationHeader::CompleteAnnotationHeader(CompleteAnnotationHeader &&x)\n{\n    m_annotation_name = std::move(x.m_annotation_name);\n}\n\nCompleteAnnotationHeader& CompleteAnnotationHeader::operator=(const CompleteAnnotationHeader &x)\n{\n    m_annotation_name = x.m_annotation_name;\n\n    return *this;\n}\n\nCompleteAnnotationHeader& CompleteAnnotationHeader::operator=(CompleteAnnotationHeader &&x)\n{\n    m_annotation_name = std::move(x.m_annotation_name);\n\n    return *this;\n}\n\nsize_t CompleteAnnotationHeader::getCdrSerializedSize(const CompleteAnnotationHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.annotation_name().size() + 1;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAnnotationHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_annotation_name;\n}\n\nvoid CompleteAnnotationHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_annotation_name;\n}\n\nbool CompleteAnnotationHeader::operator==(const CompleteAnnotationHeader& other) const\n{\n    return m_annotation_name == other.m_annotation_name;\n}\n\nMinimalAnnotationHeader::MinimalAnnotationHeader()\n{\n}\n\nMinimalAnnotationHeader::~MinimalAnnotationHeader()\n{\n}\n\nMinimalAnnotationHeader::MinimalAnnotationHeader(const MinimalAnnotationHeader &)\n{\n}\n\nMinimalAnnotationHeader::MinimalAnnotationHeader(MinimalAnnotationHeader &&)\n{\n}\n\nMinimalAnnotationHeader& MinimalAnnotationHeader::operator=(const MinimalAnnotationHeader &)\n{\n    return *this;\n}\n\nMinimalAnnotationHeader& MinimalAnnotationHeader::operator=(MinimalAnnotationHeader &&)\n{\n    return *this;\n}\n\nsize_t MinimalAnnotationHeader::getCdrSerializedSize(const MinimalAnnotationHeader&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAnnotationHeader::serialize(eprosima::fastcdr::Cdr &) const\n{\n}\n\nvoid MinimalAnnotationHeader::deserialize(eprosima::fastcdr::Cdr &)\n{\n}\n\nCompleteAnnotationType::CompleteAnnotationType()\n{\n}\n\nCompleteAnnotationType::~CompleteAnnotationType()\n{\n}\n\nCompleteAnnotationType::CompleteAnnotationType(const CompleteAnnotationType &x)\n{\n    m_annotation_flag = x.m_annotation_flag;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n}\n\nCompleteAnnotationType::CompleteAnnotationType(CompleteAnnotationType &&x)\n{\n    m_annotation_flag = std::move(x.m_annotation_flag);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nCompleteAnnotationType& CompleteAnnotationType::operator=(const CompleteAnnotationType &x)\n{\n    m_annotation_flag = x.m_annotation_flag;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nCompleteAnnotationType& CompleteAnnotationType::operator=(CompleteAnnotationType &&x)\n{\n    m_annotation_flag = std::move(x.m_annotation_flag);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t CompleteAnnotationType::getCdrSerializedSize(const CompleteAnnotationType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AnnotationTypeFlag::getCdrSerializedSize(data.annotation_flag(), current_alignment);\n    current_alignment += CompleteAnnotationHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += CompleteAnnotationParameter::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAnnotationType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_annotation_flag;\n    scdr << m_header;\n    scdr << m_member_seq;\n}\n\nvoid CompleteAnnotationType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_annotation_flag;\n    dcdr >> m_header;\n    dcdr >> m_member_seq;\n}\n\nbool CompleteAnnotationType::operator==(const CompleteAnnotationType& other) const\n{\n    if(m_annotation_flag == other.m_annotation_flag &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nMinimalAnnotationType::MinimalAnnotationType()\n{\n}\n\nMinimalAnnotationType::~MinimalAnnotationType()\n{\n}\n\nMinimalAnnotationType::MinimalAnnotationType(const MinimalAnnotationType &x)\n{\n    m_annotation_flag = x.m_annotation_flag;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n}\n\nMinimalAnnotationType::MinimalAnnotationType(MinimalAnnotationType &&x)\n{\n    m_annotation_flag = std::move(x.m_annotation_flag);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n}\n\nMinimalAnnotationType& MinimalAnnotationType::operator=(const MinimalAnnotationType &x)\n{\n    m_annotation_flag = x.m_annotation_flag;\n    m_header = x.m_header;\n    m_member_seq = x.m_member_seq;\n\n    return *this;\n}\n\nMinimalAnnotationType& MinimalAnnotationType::operator=(MinimalAnnotationType &&x)\n{\n    m_annotation_flag = std::move(x.m_annotation_flag);\n    m_header = std::move(x.m_header);\n    m_member_seq = std::move(x.m_member_seq);\n\n    return *this;\n}\n\nsize_t MinimalAnnotationType::getCdrSerializedSize(const MinimalAnnotationType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AnnotationTypeFlag::getCdrSerializedSize(data.annotation_flag(), current_alignment);\n    current_alignment += MinimalAnnotationHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.member_seq().size(); ++a)\n    {\n        current_alignment += MinimalAnnotationParameter::getCdrSerializedSize(data.member_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAnnotationType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_annotation_flag;\n    scdr << m_header;\n    scdr << m_member_seq;\n}\n\nvoid MinimalAnnotationType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_annotation_flag;\n    dcdr >> m_header;\n    dcdr >> m_member_seq;\n}\n\nbool MinimalAnnotationType::operator==(const MinimalAnnotationType& other) const\n{\n    if(m_annotation_flag == other.m_annotation_flag &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_member_seq, other.m_member_seq);\n    }\n    return false;\n}\n\nCommonAliasBody::CommonAliasBody()\n{\n}\n\nCommonAliasBody::~CommonAliasBody()\n{\n}\n\nCommonAliasBody::CommonAliasBody(const CommonAliasBody &x)\n{\n    m_related_flags = x.m_related_flags;\n    m_related_type = x.m_related_type;\n}\n\nCommonAliasBody::CommonAliasBody(CommonAliasBody &&x)\n{\n    m_related_flags = std::move(x.m_related_flags);\n    m_related_type = std::move(x.m_related_type);\n}\n\nCommonAliasBody& CommonAliasBody::operator=(const CommonAliasBody &x)\n{\n    m_related_flags = x.m_related_flags;\n    m_related_type = x.m_related_type;\n\n    return *this;\n}\n\nCommonAliasBody& CommonAliasBody::operator=(CommonAliasBody &&x)\n{\n    m_related_flags = std::move(x.m_related_flags);\n    m_related_type = std::move(x.m_related_type);\n\n    return *this;\n}\n\nsize_t CommonAliasBody::getCdrSerializedSize(const CommonAliasBody& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AliasMemberFlag::getCdrSerializedSize(data.related_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.related_type(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonAliasBody::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_related_flags;\n    scdr << m_related_type;\n}\n\nvoid CommonAliasBody::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_related_flags;\n    dcdr >> m_related_type;\n}\n\nbool CommonAliasBody::operator==(const CommonAliasBody& other) const\n{\n    return m_related_flags == other.m_related_flags &&\n        m_related_type == other.m_related_type;\n}\n\nCompleteAliasBody::CompleteAliasBody()\n{\n}\n\nCompleteAliasBody::~CompleteAliasBody()\n{\n}\n\nCompleteAliasBody::CompleteAliasBody(const CompleteAliasBody &x)\n{\n    m_common = x.m_common;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n}\n\nCompleteAliasBody::CompleteAliasBody(CompleteAliasBody &&x)\n{\n    m_common = std::move(x.m_common);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n}\n\nCompleteAliasBody& CompleteAliasBody::operator=(const CompleteAliasBody &x)\n{\n    m_common = x.m_common;\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n\n    return *this;\n}\n\nCompleteAliasBody& CompleteAliasBody::operator=(CompleteAliasBody &&x)\n{\n    m_common = std::move(x.m_common);\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n\n    return *this;\n}\n\nsize_t CompleteAliasBody::getCdrSerializedSize(const CompleteAliasBody& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonAliasBody::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += AppliedBuiltinMemberAnnotations::getCdrSerializedSize(data.ann_builtin(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.ann_custom().size(); ++a)\n    {\n        current_alignment += AppliedAnnotation::getCdrSerializedSize(data.ann_custom().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAliasBody::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_ann_builtin;\n    scdr << m_ann_custom;\n}\n\nvoid CompleteAliasBody::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_ann_builtin;\n    dcdr >> m_ann_custom;\n}\n\nbool CompleteAliasBody::operator==(const CompleteAliasBody& other) const\n{\n    if(m_common == other.m_common &&\n        m_ann_builtin == other.m_ann_builtin)\n    {\n        return compareSequence(m_ann_custom, other.m_ann_custom);\n    }\n    return false;\n}\n\nMinimalAliasBody::MinimalAliasBody()\n{\n}\n\nMinimalAliasBody::~MinimalAliasBody()\n{\n}\n\nMinimalAliasBody::MinimalAliasBody(const MinimalAliasBody &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalAliasBody::MinimalAliasBody(MinimalAliasBody &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalAliasBody& MinimalAliasBody::operator=(const MinimalAliasBody &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalAliasBody& MinimalAliasBody::operator=(MinimalAliasBody &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalAliasBody::getCdrSerializedSize(const MinimalAliasBody& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonAliasBody::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAliasBody::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalAliasBody::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalAliasBody::operator==(const MinimalAliasBody& other) const\n{\n    return m_common == other.m_common;\n}\n\nCompleteAliasHeader::CompleteAliasHeader()\n{\n}\n\nCompleteAliasHeader::~CompleteAliasHeader()\n{\n}\n\nCompleteAliasHeader::CompleteAliasHeader(const CompleteAliasHeader &x)\n{\n    m_detail = x.m_detail;\n}\n\nCompleteAliasHeader::CompleteAliasHeader(CompleteAliasHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteAliasHeader& CompleteAliasHeader::operator=(const CompleteAliasHeader &x)\n{\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteAliasHeader& CompleteAliasHeader::operator=(CompleteAliasHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteAliasHeader::getCdrSerializedSize(const CompleteAliasHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAliasHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_detail;\n}\n\nvoid CompleteAliasHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_detail;\n}\n\nbool CompleteAliasHeader::operator==(const CompleteAliasHeader& other) const\n{\n    return m_detail == other.m_detail;\n}\n\nMinimalAliasHeader::MinimalAliasHeader()\n{\n}\n\nMinimalAliasHeader::~MinimalAliasHeader()\n{\n}\n\nMinimalAliasHeader::MinimalAliasHeader(const MinimalAliasHeader &)\n{\n}\n\nMinimalAliasHeader::MinimalAliasHeader(MinimalAliasHeader &&)\n{\n}\n\nMinimalAliasHeader& MinimalAliasHeader::operator=(const MinimalAliasHeader &)\n{\n    return *this;\n}\n\nMinimalAliasHeader& MinimalAliasHeader::operator=(MinimalAliasHeader &&)\n{\n    return *this;\n}\n\nsize_t MinimalAliasHeader::getCdrSerializedSize(const MinimalAliasHeader&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAliasHeader::serialize(eprosima::fastcdr::Cdr &) const\n{\n}\n\nvoid MinimalAliasHeader::deserialize(eprosima::fastcdr::Cdr &)\n{\n}\n\nCompleteAliasType::CompleteAliasType()\n{\n}\n\nCompleteAliasType::~CompleteAliasType()\n{\n}\n\nCompleteAliasType::CompleteAliasType(const CompleteAliasType &x)\n{\n    m_alias_flags = x.m_alias_flags;\n    m_header = x.m_header;\n    m_body = x.m_body;\n}\n\nCompleteAliasType::CompleteAliasType(CompleteAliasType &&x)\n{\n    m_alias_flags = std::move(x.m_alias_flags);\n    m_header = std::move(x.m_header);\n    m_body = std::move(x.m_body);\n}\n\nCompleteAliasType& CompleteAliasType::operator=(const CompleteAliasType &x)\n{\n    m_alias_flags = x.m_alias_flags;\n    m_header = x.m_header;\n    m_body = x.m_body;\n\n    return *this;\n}\n\nCompleteAliasType& CompleteAliasType::operator=(CompleteAliasType &&x)\n{\n    m_alias_flags = std::move(x.m_alias_flags);\n    m_header = std::move(x.m_header);\n    m_body = std::move(x.m_body);\n\n    return *this;\n}\n\nsize_t CompleteAliasType::getCdrSerializedSize(const CompleteAliasType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AliasTypeFlag::getCdrSerializedSize(data.alias_flags(), current_alignment);\n    current_alignment += CompleteAliasHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += CompleteAliasBody::getCdrSerializedSize(data.body(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteAliasType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_alias_flags;\n    scdr << m_header;\n    scdr << m_body;\n}\n\nvoid CompleteAliasType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_alias_flags;\n    dcdr >> m_header;\n    dcdr >> m_body;\n}\n\nbool CompleteAliasType::operator==(const CompleteAliasType& other) const\n{\n    return m_alias_flags == other.m_alias_flags &&\n            m_header == other.m_header &&\n            m_body == other.m_body;\n}\n\nMinimalAliasType::MinimalAliasType()\n{\n}\n\nMinimalAliasType::~MinimalAliasType()\n{\n}\n\nMinimalAliasType::MinimalAliasType(const MinimalAliasType &x)\n{\n    m_alias_flags = x.m_alias_flags;\n    m_header = x.m_header;\n    m_body = x.m_body;\n}\n\nMinimalAliasType::MinimalAliasType(MinimalAliasType &&x)\n{\n    m_alias_flags = std::move(x.m_alias_flags);\n    m_header = std::move(x.m_header);\n    m_body = std::move(x.m_body);\n}\n\nMinimalAliasType& MinimalAliasType::operator=(const MinimalAliasType &x)\n{\n    m_alias_flags = x.m_alias_flags;\n    m_header = x.m_header;\n    m_body = x.m_body;\n\n    return *this;\n}\n\nMinimalAliasType& MinimalAliasType::operator=(MinimalAliasType &&x)\n{\n    m_alias_flags = std::move(x.m_alias_flags);\n    m_header = std::move(x.m_header);\n    m_body = std::move(x.m_body);\n\n    return *this;\n}\n\nsize_t MinimalAliasType::getCdrSerializedSize(const MinimalAliasType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AliasTypeFlag::getCdrSerializedSize(data.alias_flags(), current_alignment);\n    current_alignment += MinimalAliasHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += MinimalAliasBody::getCdrSerializedSize(data.body(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalAliasType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_alias_flags;\n    scdr << m_header;\n    scdr << m_body;\n}\n\nvoid MinimalAliasType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_alias_flags;\n    dcdr >> m_header;\n    dcdr >> m_body;\n}\n\nbool MinimalAliasType::operator==(const MinimalAliasType& other) const\n{\n    return m_alias_flags == other.m_alias_flags &&\n            m_header == other.m_header &&\n            m_body == other.m_body;\n}\n\nCompleteElementDetail::CompleteElementDetail()\n{\n}\n\nCompleteElementDetail::~CompleteElementDetail()\n{\n}\n\nCompleteElementDetail::CompleteElementDetail(const CompleteElementDetail &x)\n{\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n}\n\nCompleteElementDetail::CompleteElementDetail(CompleteElementDetail &&x)\n{\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n}\n\nCompleteElementDetail& CompleteElementDetail::operator=(const CompleteElementDetail &x)\n{\n    m_ann_builtin = x.m_ann_builtin;\n    m_ann_custom = x.m_ann_custom;\n\n    return *this;\n}\n\nCompleteElementDetail& CompleteElementDetail::operator=(CompleteElementDetail &&x)\n{\n    m_ann_builtin = std::move(x.m_ann_builtin);\n    m_ann_custom = std::move(x.m_ann_custom);\n\n    return *this;\n}\n\nsize_t CompleteElementDetail::getCdrSerializedSize(const CompleteElementDetail& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += AppliedBuiltinMemberAnnotations::getCdrSerializedSize(data.ann_builtin(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.ann_custom().size(); ++a)\n    {\n        current_alignment += AppliedAnnotation::getCdrSerializedSize(data.ann_custom().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteElementDetail::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_ann_builtin;\n    scdr << m_ann_custom;\n}\n\nvoid CompleteElementDetail::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_ann_builtin;\n    dcdr >> m_ann_custom;\n}\n\nbool CompleteElementDetail::operator==(const CompleteElementDetail& other) const\n{\n    if(m_ann_builtin == other.m_ann_builtin)\n    {\n        return compareSequence(m_ann_custom, other.m_ann_custom);\n    }\n    return false;\n}\n\nCommonCollectionElement::CommonCollectionElement()\n{\n}\n\nCommonCollectionElement::~CommonCollectionElement()\n{\n}\n\nCommonCollectionElement::CommonCollectionElement(const CommonCollectionElement &x)\n{\n    m_element_flags = x.m_element_flags;\n    m_type = x.m_type;\n}\n\nCommonCollectionElement::CommonCollectionElement(CommonCollectionElement &&x)\n{\n    m_element_flags = std::move(x.m_element_flags);\n    m_type = std::move(x.m_type);\n}\n\nCommonCollectionElement& CommonCollectionElement::operator=(const CommonCollectionElement &x)\n{\n    m_element_flags = x.m_element_flags;\n    m_type = x.m_type;\n\n    return *this;\n}\n\nCommonCollectionElement& CommonCollectionElement::operator=(CommonCollectionElement &&x)\n{\n    m_element_flags = std::move(x.m_element_flags);\n    m_type = std::move(x.m_type);\n\n    return *this;\n}\n\nsize_t CommonCollectionElement::getCdrSerializedSize(const CommonCollectionElement& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CollectionElementFlag::getCdrSerializedSize(data.element_flags(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonCollectionElement::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_element_flags;\n    scdr << m_type;\n}\n\nvoid CommonCollectionElement::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_element_flags;\n    dcdr >> m_type;\n}\n\nbool CommonCollectionElement::operator==(const CommonCollectionElement& other) const\n{\n    return m_element_flags == other.m_element_flags &&\n            m_type == other.m_type;\n}\n\nCompleteCollectionElement::CompleteCollectionElement()\n{\n}\n\nCompleteCollectionElement::~CompleteCollectionElement()\n{\n}\n\nCompleteCollectionElement::CompleteCollectionElement(const CompleteCollectionElement &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteCollectionElement::CompleteCollectionElement(CompleteCollectionElement &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteCollectionElement& CompleteCollectionElement::operator=(const CompleteCollectionElement &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteCollectionElement& CompleteCollectionElement::operator=(CompleteCollectionElement &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteCollectionElement::getCdrSerializedSize(const CompleteCollectionElement& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonCollectionElement::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteElementDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteCollectionElement::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteCollectionElement::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteCollectionElement::operator==(const CompleteCollectionElement& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalCollectionElement::MinimalCollectionElement()\n{\n}\n\nMinimalCollectionElement::~MinimalCollectionElement()\n{\n}\n\nMinimalCollectionElement::MinimalCollectionElement(const MinimalCollectionElement &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalCollectionElement::MinimalCollectionElement(MinimalCollectionElement &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalCollectionElement& MinimalCollectionElement::operator=(const MinimalCollectionElement &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalCollectionElement& MinimalCollectionElement::operator=(MinimalCollectionElement &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalCollectionElement::getCdrSerializedSize(const MinimalCollectionElement& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonCollectionElement::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalCollectionElement::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalCollectionElement::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalCollectionElement::operator==(const MinimalCollectionElement& other) const\n{\n    return m_common == other.m_common;\n}\n\nCommonCollectionHeader::CommonCollectionHeader()\n{\n}\n\nCommonCollectionHeader::~CommonCollectionHeader()\n{\n}\n\nCommonCollectionHeader::CommonCollectionHeader(const CommonCollectionHeader &x)\n{\n    m_bound = x.m_bound;\n}\n\nCommonCollectionHeader::CommonCollectionHeader(CommonCollectionHeader &&x)\n{\n    m_bound = std::move(x.m_bound);\n}\n\nCommonCollectionHeader& CommonCollectionHeader::operator=(const CommonCollectionHeader &x)\n{\n    m_bound = x.m_bound;\n\n    return *this;\n}\n\nCommonCollectionHeader& CommonCollectionHeader::operator=(CommonCollectionHeader &&x)\n{\n    m_bound = std::move(x.m_bound);\n\n    return *this;\n}\n\nsize_t CommonCollectionHeader::getCdrSerializedSize(const CommonCollectionHeader&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + 255  + 1;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonCollectionHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bound;\n}\n\nvoid CommonCollectionHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bound;\n}\n\nbool CommonCollectionHeader::operator==(const CommonCollectionHeader& other) const\n{\n    return m_bound == other.m_bound;\n}\n\nCompleteCollectionHeader::CompleteCollectionHeader()\n{\n}\n\nCompleteCollectionHeader::~CompleteCollectionHeader()\n{\n}\n\nCompleteCollectionHeader::CompleteCollectionHeader(const CompleteCollectionHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteCollectionHeader::CompleteCollectionHeader(CompleteCollectionHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteCollectionHeader& CompleteCollectionHeader::operator=(const CompleteCollectionHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteCollectionHeader& CompleteCollectionHeader::operator=(CompleteCollectionHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteCollectionHeader::getCdrSerializedSize(const CompleteCollectionHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonCollectionHeader::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteCollectionHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteCollectionHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteCollectionHeader::operator==(const CompleteCollectionHeader& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalCollectionHeader::MinimalCollectionHeader()\n{\n}\n\nMinimalCollectionHeader::~MinimalCollectionHeader()\n{\n}\n\nMinimalCollectionHeader::MinimalCollectionHeader(const MinimalCollectionHeader &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalCollectionHeader::MinimalCollectionHeader(MinimalCollectionHeader &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalCollectionHeader& MinimalCollectionHeader::operator=(const MinimalCollectionHeader &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalCollectionHeader& MinimalCollectionHeader::operator=(MinimalCollectionHeader &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalCollectionHeader::getCdrSerializedSize(const MinimalCollectionHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonCollectionHeader::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalCollectionHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalCollectionHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalCollectionHeader::operator==(const MinimalCollectionHeader& other) const\n{\n    return m_common == other.m_common;\n}\n\nCompleteSequenceType::CompleteSequenceType()\n{\n}\n\nCompleteSequenceType::~CompleteSequenceType()\n{\n}\n\nCompleteSequenceType::CompleteSequenceType(const CompleteSequenceType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n}\n\nCompleteSequenceType::CompleteSequenceType(CompleteSequenceType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n}\n\nCompleteSequenceType& CompleteSequenceType::operator=(const CompleteSequenceType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nCompleteSequenceType& CompleteSequenceType::operator=(CompleteSequenceType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t CompleteSequenceType::getCdrSerializedSize(const CompleteSequenceType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    // FIXED_SIXE current_alignment += ((4) * 1) + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += CompleteCollectionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += CompleteCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    // STRING current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.str().size() + 1;\n    // SEQUENCE\n    /*\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.param_seq().size(); ++a)\n    {\n        current_alignment += AppliedAnnotationParameter::getCdrSerializedSize(data.param_seq().at(a), current_alignment);\n    }\n    */\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteSequenceType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_element;\n}\n\nvoid CompleteSequenceType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_element;\n}\n\nbool CompleteSequenceType::operator==(const CompleteSequenceType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_element == other.m_element;\n}\n\nMinimalSequenceType::MinimalSequenceType()\n{\n}\n\nMinimalSequenceType::~MinimalSequenceType()\n{\n}\n\nMinimalSequenceType::MinimalSequenceType(const MinimalSequenceType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n}\n\nMinimalSequenceType::MinimalSequenceType(MinimalSequenceType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n}\n\nMinimalSequenceType& MinimalSequenceType::operator=(const MinimalSequenceType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nMinimalSequenceType& MinimalSequenceType::operator=(MinimalSequenceType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t MinimalSequenceType::getCdrSerializedSize(const MinimalSequenceType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    // FIXED_SIXE current_alignment += ((4) * 1) + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += MinimalCollectionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += MinimalCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    // STRING current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4) + data.str().size() + 1;\n    // SEQUENCE\n    /*\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.param_seq().size(); ++a)\n    {\n        current_alignment += AppliedAnnotationParameter::getCdrSerializedSize(data.param_seq().at(a), current_alignment);\n    }\n    */\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalSequenceType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_element;\n}\n\nvoid MinimalSequenceType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_element;\n}\n\nbool MinimalSequenceType::operator==(const MinimalSequenceType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_element == other.m_element;\n}\n\nCommonArrayHeader::CommonArrayHeader()\n{\n}\n\nCommonArrayHeader::~CommonArrayHeader()\n{\n}\n\nCommonArrayHeader::CommonArrayHeader(const CommonArrayHeader &x)\n{\n    m_bound_seq = x.m_bound_seq;\n}\n\nCommonArrayHeader::CommonArrayHeader(CommonArrayHeader &&x)\n{\n    m_bound_seq = std::move(x.m_bound_seq);\n}\n\nCommonArrayHeader& CommonArrayHeader::operator=(const CommonArrayHeader &x)\n{\n    m_bound_seq = x.m_bound_seq;\n\n    return *this;\n}\n\nCommonArrayHeader& CommonArrayHeader::operator=(CommonArrayHeader &&x)\n{\n    m_bound_seq = std::move(x.m_bound_seq);\n\n    return *this;\n}\n\nsize_t CommonArrayHeader::getCdrSerializedSize(const CommonArrayHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.bound_seq().size(); ++a)\n    {\n        current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonArrayHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bound_seq;\n}\n\nvoid CommonArrayHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bound_seq;\n}\n\nbool CommonArrayHeader::operator==(const CommonArrayHeader& other) const\n{\n    return compareSequence(m_bound_seq, other.m_bound_seq);\n}\n\nCompleteArrayHeader::CompleteArrayHeader()\n{\n}\n\nCompleteArrayHeader::~CompleteArrayHeader()\n{\n}\n\nCompleteArrayHeader::CompleteArrayHeader(const CompleteArrayHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteArrayHeader::CompleteArrayHeader(CompleteArrayHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteArrayHeader& CompleteArrayHeader::operator=(const CompleteArrayHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteArrayHeader& CompleteArrayHeader::operator=(CompleteArrayHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteArrayHeader::getCdrSerializedSize(const CompleteArrayHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonArrayHeader::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteArrayHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteArrayHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteArrayHeader::operator==(const CompleteArrayHeader& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalArrayHeader::MinimalArrayHeader()\n{\n}\n\nMinimalArrayHeader::~MinimalArrayHeader()\n{\n}\n\nMinimalArrayHeader::MinimalArrayHeader(const MinimalArrayHeader &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalArrayHeader::MinimalArrayHeader(MinimalArrayHeader &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalArrayHeader& MinimalArrayHeader::operator=(const MinimalArrayHeader &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalArrayHeader& MinimalArrayHeader::operator=(MinimalArrayHeader &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalArrayHeader::getCdrSerializedSize(const MinimalArrayHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonArrayHeader::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalArrayHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalArrayHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalArrayHeader::operator==(const MinimalArrayHeader& other) const\n{\n    return m_common == other.m_common;\n}\n\nCompleteArrayType::CompleteArrayType()\n{\n}\n\nCompleteArrayType::~CompleteArrayType()\n{\n}\n\nCompleteArrayType::CompleteArrayType(const CompleteArrayType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n}\n\nCompleteArrayType::CompleteArrayType(CompleteArrayType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n}\n\nCompleteArrayType& CompleteArrayType::operator=(const CompleteArrayType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nCompleteArrayType& CompleteArrayType::operator=(CompleteArrayType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t CompleteArrayType::getCdrSerializedSize(const CompleteArrayType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += CompleteArrayHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += CompleteCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteArrayType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_element;\n}\n\nvoid CompleteArrayType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_element;\n}\n\nbool CompleteArrayType::operator==(const CompleteArrayType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_element == other.m_element;\n}\n\nMinimalArrayType::MinimalArrayType()\n{\n}\n\nMinimalArrayType::~MinimalArrayType()\n{\n}\n\nMinimalArrayType::MinimalArrayType(const MinimalArrayType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n}\n\nMinimalArrayType::MinimalArrayType(MinimalArrayType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n}\n\nMinimalArrayType& MinimalArrayType::operator=(const MinimalArrayType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nMinimalArrayType& MinimalArrayType::operator=(MinimalArrayType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t MinimalArrayType::getCdrSerializedSize(const MinimalArrayType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += MinimalArrayHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += MinimalCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalArrayType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_element;\n}\n\nvoid MinimalArrayType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_element;\n}\n\nbool MinimalArrayType::operator==(const MinimalArrayType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_element == other.m_element;\n}\n\nCompleteMapType::CompleteMapType()\n{\n}\n\nCompleteMapType::~CompleteMapType()\n{\n}\n\nCompleteMapType::CompleteMapType(const CompleteMapType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_key = x.m_key;\n    m_element = x.m_element;\n}\n\nCompleteMapType::CompleteMapType(CompleteMapType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_key = std::move(x.m_key);\n    m_element = std::move(x.m_element);\n}\n\nCompleteMapType& CompleteMapType::operator=(const CompleteMapType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_key = x.m_key;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nCompleteMapType& CompleteMapType::operator=(CompleteMapType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_key = std::move(x.m_key);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t CompleteMapType::getCdrSerializedSize(const CompleteMapType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += CompleteCollectionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += CompleteCollectionElement::getCdrSerializedSize(data.key(), current_alignment);\n    current_alignment += CompleteCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteMapType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_key;\n    scdr << m_element;\n}\n\nvoid CompleteMapType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_key;\n    dcdr >> m_element;\n}\n\nbool CompleteMapType::operator==(const CompleteMapType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_key == other.m_key &&\n            m_element == other.m_element;\n}\n\nMinimalMapType::MinimalMapType()\n{\n}\n\nMinimalMapType::~MinimalMapType()\n{\n}\n\nMinimalMapType::MinimalMapType(const MinimalMapType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_key = x.m_key;\n    m_element = x.m_element;\n}\n\nMinimalMapType::MinimalMapType(MinimalMapType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_key = std::move(x.m_key);\n    m_element = std::move(x.m_element);\n}\n\nMinimalMapType& MinimalMapType::operator=(const MinimalMapType &x)\n{\n    m_collection_flag = x.m_collection_flag;\n    m_header = x.m_header;\n    m_key = x.m_key;\n    m_element = x.m_element;\n\n    return *this;\n}\n\nMinimalMapType& MinimalMapType::operator=(MinimalMapType &&x)\n{\n    m_collection_flag = std::move(x.m_collection_flag);\n    m_header = std::move(x.m_header);\n    m_key = std::move(x.m_key);\n    m_element = std::move(x.m_element);\n\n    return *this;\n}\n\nsize_t MinimalMapType::getCdrSerializedSize(const MinimalMapType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CollectionTypeFlag::getCdrSerializedSize(data.collection_flag(), current_alignment);\n    current_alignment += MinimalCollectionHeader::getCdrSerializedSize(data.header(), current_alignment);\n    current_alignment += MinimalCollectionElement::getCdrSerializedSize(data.key(), current_alignment);\n    current_alignment += MinimalCollectionElement::getCdrSerializedSize(data.element(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalMapType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_collection_flag;\n    scdr << m_header;\n    scdr << m_key;\n    scdr << m_element;\n}\n\nvoid MinimalMapType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_collection_flag;\n    dcdr >> m_header;\n    dcdr >> m_key;\n    dcdr >> m_element;\n}\n\nbool MinimalMapType::operator==(const MinimalMapType& other) const\n{\n    return m_collection_flag == other.m_collection_flag &&\n            m_header == other.m_header &&\n            m_key == other.m_key &&\n            m_element == other.m_element;\n}\n\nCommonEnumeratedLiteral::CommonEnumeratedLiteral()\n{\n}\n\nCommonEnumeratedLiteral::~CommonEnumeratedLiteral()\n{\n}\n\nCommonEnumeratedLiteral::CommonEnumeratedLiteral(const CommonEnumeratedLiteral &x)\n{\n    m_value = x.m_value;\n    m_flags = x.m_flags;\n}\n\nCommonEnumeratedLiteral::CommonEnumeratedLiteral(CommonEnumeratedLiteral &&x)\n{\n    m_value = std::move(x.m_value);\n    m_flags = std::move(x.m_flags);\n}\n\nCommonEnumeratedLiteral& CommonEnumeratedLiteral::operator=(const CommonEnumeratedLiteral &x)\n{\n    m_value = x.m_value;\n    m_flags = x.m_flags;\n\n    return *this;\n}\n\nCommonEnumeratedLiteral& CommonEnumeratedLiteral::operator=(CommonEnumeratedLiteral &&x)\n{\n    m_value = std::move(x.m_value);\n    m_flags = std::move(x.m_flags);\n\n    return *this;\n}\n\nsize_t CommonEnumeratedLiteral::getCdrSerializedSize(const CommonEnumeratedLiteral& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    current_alignment += EnumeratedLiteralFlag::getCdrSerializedSize(data.flags(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonEnumeratedLiteral::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_value;\n    scdr << m_flags;\n}\n\nvoid CommonEnumeratedLiteral::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_value;\n    dcdr >> m_flags;\n}\n\nbool CommonEnumeratedLiteral::operator==(const CommonEnumeratedLiteral& other) const\n{\n    return m_value == other.m_value &&\n            m_flags == other.m_flags;\n}\n\nCompleteEnumeratedLiteral::CompleteEnumeratedLiteral()\n{\n}\n\nCompleteEnumeratedLiteral::~CompleteEnumeratedLiteral()\n{\n}\n\nCompleteEnumeratedLiteral::CompleteEnumeratedLiteral(const CompleteEnumeratedLiteral &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteEnumeratedLiteral::CompleteEnumeratedLiteral(CompleteEnumeratedLiteral &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteEnumeratedLiteral& CompleteEnumeratedLiteral::operator=(const CompleteEnumeratedLiteral &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteEnumeratedLiteral& CompleteEnumeratedLiteral::operator=(CompleteEnumeratedLiteral &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteEnumeratedLiteral::getCdrSerializedSize(const CompleteEnumeratedLiteral& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonEnumeratedLiteral::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteEnumeratedLiteral::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteEnumeratedLiteral::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteEnumeratedLiteral::operator==(const CompleteEnumeratedLiteral& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalEnumeratedLiteral::MinimalEnumeratedLiteral()\n{\n}\n\nMinimalEnumeratedLiteral::~MinimalEnumeratedLiteral()\n{\n}\n\nMinimalEnumeratedLiteral::MinimalEnumeratedLiteral(const MinimalEnumeratedLiteral &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nMinimalEnumeratedLiteral::MinimalEnumeratedLiteral(MinimalEnumeratedLiteral &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalEnumeratedLiteral& MinimalEnumeratedLiteral::operator=(const MinimalEnumeratedLiteral &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalEnumeratedLiteral& MinimalEnumeratedLiteral::operator=(MinimalEnumeratedLiteral &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalEnumeratedLiteral::getCdrSerializedSize(const MinimalEnumeratedLiteral& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonEnumeratedLiteral::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += MinimalMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalEnumeratedLiteral::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid MinimalEnumeratedLiteral::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool MinimalEnumeratedLiteral::operator==(const MinimalEnumeratedLiteral& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nCommonEnumeratedHeader::CommonEnumeratedHeader()\n{\n}\n\nCommonEnumeratedHeader::~CommonEnumeratedHeader()\n{\n}\n\nCommonEnumeratedHeader::CommonEnumeratedHeader(const CommonEnumeratedHeader &x)\n{\n    m_bit_bound = x.m_bit_bound;\n}\n\nCommonEnumeratedHeader::CommonEnumeratedHeader(CommonEnumeratedHeader &&x)\n{\n    m_bit_bound = std::move(x.m_bit_bound);\n}\n\nCommonEnumeratedHeader& CommonEnumeratedHeader::operator=(const CommonEnumeratedHeader &x)\n{\n    m_bit_bound = x.m_bit_bound;\n\n    return *this;\n}\n\nCommonEnumeratedHeader& CommonEnumeratedHeader::operator=(CommonEnumeratedHeader &&x)\n{\n    m_bit_bound = std::move(x.m_bit_bound);\n\n    return *this;\n}\n\nsize_t CommonEnumeratedHeader::getCdrSerializedSize(const CommonEnumeratedHeader&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 2 + eprosima::fastcdr::Cdr::alignment(current_alignment, 2);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonEnumeratedHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bit_bound;\n}\n\nvoid CommonEnumeratedHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bit_bound;\n}\n\nbool CommonEnumeratedHeader::operator==(const CommonEnumeratedHeader& other) const\n{\n    return m_bit_bound == other.m_bit_bound;\n}\n\nCompleteEnumeratedHeader::CompleteEnumeratedHeader()\n{\n}\n\nCompleteEnumeratedHeader::~CompleteEnumeratedHeader()\n{\n}\n\nCompleteEnumeratedHeader::CompleteEnumeratedHeader(const CompleteEnumeratedHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteEnumeratedHeader::CompleteEnumeratedHeader(CompleteEnumeratedHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteEnumeratedHeader& CompleteEnumeratedHeader::operator=(const CompleteEnumeratedHeader &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteEnumeratedHeader& CompleteEnumeratedHeader::operator=(CompleteEnumeratedHeader &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteEnumeratedHeader::getCdrSerializedSize(const CompleteEnumeratedHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonEnumeratedHeader::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteEnumeratedHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteEnumeratedHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteEnumeratedHeader::operator==(const CompleteEnumeratedHeader& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalEnumeratedHeader::MinimalEnumeratedHeader()\n{\n}\n\nMinimalEnumeratedHeader::~MinimalEnumeratedHeader()\n{\n}\n\nMinimalEnumeratedHeader::MinimalEnumeratedHeader(const MinimalEnumeratedHeader &x)\n{\n    m_common = x.m_common;\n}\n\nMinimalEnumeratedHeader::MinimalEnumeratedHeader(MinimalEnumeratedHeader &&x)\n{\n    m_common = std::move(x.m_common);\n}\n\nMinimalEnumeratedHeader& MinimalEnumeratedHeader::operator=(const MinimalEnumeratedHeader &x)\n{\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalEnumeratedHeader& MinimalEnumeratedHeader::operator=(MinimalEnumeratedHeader &&x)\n{\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalEnumeratedHeader::getCdrSerializedSize(const MinimalEnumeratedHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonEnumeratedHeader::getCdrSerializedSize(data.common(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalEnumeratedHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n}\n\nvoid MinimalEnumeratedHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n}\n\nbool MinimalEnumeratedHeader::operator==(const MinimalEnumeratedHeader& other) const\n{\n    return m_common == other.m_common;\n}\n\nCompleteEnumeratedType::CompleteEnumeratedType()\n{\n}\n\nCompleteEnumeratedType::~CompleteEnumeratedType()\n{\n}\n\nCompleteEnumeratedType::CompleteEnumeratedType(const CompleteEnumeratedType &x)\n{\n    m_enum_flags = x.m_enum_flags;\n    m_header = x.m_header;\n    m_literal_seq = x.m_literal_seq;\n}\n\nCompleteEnumeratedType::CompleteEnumeratedType(CompleteEnumeratedType &&x)\n{\n    m_enum_flags = std::move(x.m_enum_flags);\n    m_header = std::move(x.m_header);\n    m_literal_seq = std::move(x.m_literal_seq);\n}\n\nCompleteEnumeratedType& CompleteEnumeratedType::operator=(const CompleteEnumeratedType &x)\n{\n    m_enum_flags = x.m_enum_flags;\n    m_header = x.m_header;\n    m_literal_seq = x.m_literal_seq;\n\n    return *this;\n}\n\nCompleteEnumeratedType& CompleteEnumeratedType::operator=(CompleteEnumeratedType &&x)\n{\n    m_enum_flags = std::move(x.m_enum_flags);\n    m_header = std::move(x.m_header);\n    m_literal_seq = std::move(x.m_literal_seq);\n\n    return *this;\n}\n\nsize_t CompleteEnumeratedType::getCdrSerializedSize(const CompleteEnumeratedType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += EnumTypeFlag::getCdrSerializedSize(data.enum_flags(), current_alignment);\n    current_alignment += CompleteEnumeratedHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.literal_seq().size(); ++a)\n    {\n        current_alignment += CompleteEnumeratedLiteral::getCdrSerializedSize(data.literal_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteEnumeratedType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_enum_flags;\n    scdr << m_header;\n    scdr << m_literal_seq;\n}\n\nvoid CompleteEnumeratedType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_enum_flags;\n    dcdr >> m_header;\n    dcdr >> m_literal_seq;\n}\n\nbool CompleteEnumeratedType::operator==(const CompleteEnumeratedType& other) const\n{\n    if(m_enum_flags == other.m_enum_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_literal_seq, other.m_literal_seq);\n    }\n    return false;\n}\n\nMinimalEnumeratedType::MinimalEnumeratedType()\n{\n}\n\nMinimalEnumeratedType::~MinimalEnumeratedType()\n{\n}\n\nMinimalEnumeratedType::MinimalEnumeratedType(const MinimalEnumeratedType &x)\n{\n    m_enum_flags = x.m_enum_flags;\n    m_header = x.m_header;\n    m_literal_seq = x.m_literal_seq;\n}\n\nMinimalEnumeratedType::MinimalEnumeratedType(MinimalEnumeratedType &&x)\n{\n    m_enum_flags = std::move(x.m_enum_flags);\n    m_header = std::move(x.m_header);\n    m_literal_seq = std::move(x.m_literal_seq);\n}\n\nMinimalEnumeratedType& MinimalEnumeratedType::operator=(const MinimalEnumeratedType &x)\n{\n    m_enum_flags = x.m_enum_flags;\n    m_header = x.m_header;\n    m_literal_seq = x.m_literal_seq;\n\n    return *this;\n}\n\nMinimalEnumeratedType& MinimalEnumeratedType::operator=(MinimalEnumeratedType &&x)\n{\n    m_enum_flags = std::move(x.m_enum_flags);\n    m_header = std::move(x.m_header);\n    m_literal_seq = std::move(x.m_literal_seq);\n\n    return *this;\n}\n\nsize_t MinimalEnumeratedType::getCdrSerializedSize(const MinimalEnumeratedType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += EnumTypeFlag::getCdrSerializedSize(data.enum_flags(), current_alignment);\n    current_alignment += MinimalEnumeratedHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.literal_seq().size(); ++a)\n    {\n        current_alignment += MinimalEnumeratedLiteral::getCdrSerializedSize(data.literal_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalEnumeratedType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_enum_flags;\n    scdr << m_header;\n    scdr << m_literal_seq;\n}\n\nvoid MinimalEnumeratedType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_enum_flags;\n    dcdr >> m_header;\n    dcdr >> m_literal_seq;\n}\n\nbool MinimalEnumeratedType::operator==(const MinimalEnumeratedType& other) const\n{\n    if(m_enum_flags == other.m_enum_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_literal_seq, other.m_literal_seq);\n    }\n    return false;\n}\n\nCommonBitflag::CommonBitflag()\n{\n}\n\nCommonBitflag::~CommonBitflag()\n{\n}\n\nCommonBitflag::CommonBitflag(const CommonBitflag &x)\n{\n    m_position = x.m_position;\n    m_flags = x.m_flags;\n}\n\nCommonBitflag::CommonBitflag(CommonBitflag &&x)\n{\n    m_position = std::move(x.m_position);\n    m_flags = std::move(x.m_flags);\n}\n\nCommonBitflag& CommonBitflag::operator=(const CommonBitflag &x)\n{\n    m_position = x.m_position;\n    m_flags = x.m_flags;\n\n    return *this;\n}\n\nCommonBitflag& CommonBitflag::operator=(CommonBitflag &&x)\n{\n    m_position = std::move(x.m_position);\n    m_flags = std::move(x.m_flags);\n\n    return *this;\n}\n\nsize_t CommonBitflag::getCdrSerializedSize(const CommonBitflag& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 2 + eprosima::fastcdr::Cdr::alignment(current_alignment, 2);\n    current_alignment += BitflagFlag::getCdrSerializedSize(data.flags(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonBitflag::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_position;\n    scdr << m_flags;\n}\n\nvoid CommonBitflag::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_position;\n    dcdr >> m_flags;\n}\n\nbool CommonBitflag::operator==(const CommonBitflag& other) const\n{\n    return m_position == other.m_position &&\n            m_flags == other.m_flags;\n}\n\nCompleteBitflag::CompleteBitflag()\n{\n}\n\nCompleteBitflag::~CompleteBitflag()\n{\n}\n\nCompleteBitflag::CompleteBitflag(const CompleteBitflag &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteBitflag::CompleteBitflag(CompleteBitflag &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteBitflag& CompleteBitflag::operator=(const CompleteBitflag &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteBitflag& CompleteBitflag::operator=(CompleteBitflag &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteBitflag::getCdrSerializedSize(const CompleteBitflag& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonBitflag::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteBitflag::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteBitflag::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteBitflag::operator==(const CompleteBitflag& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalBitflag::MinimalBitflag()\n{\n}\n\nMinimalBitflag::~MinimalBitflag()\n{\n}\n\nMinimalBitflag::MinimalBitflag(const MinimalBitflag &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nMinimalBitflag::MinimalBitflag(MinimalBitflag &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nMinimalBitflag& MinimalBitflag::operator=(const MinimalBitflag &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nMinimalBitflag& MinimalBitflag::operator=(MinimalBitflag &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t MinimalBitflag::getCdrSerializedSize(const MinimalBitflag& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonBitflag::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += MinimalMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalBitflag::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid MinimalBitflag::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool MinimalBitflag::operator==(const MinimalBitflag& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nCommonBitmaskHeader::CommonBitmaskHeader()\n{\n}\n\nCommonBitmaskHeader::~CommonBitmaskHeader()\n{\n}\n\nCommonBitmaskHeader::CommonBitmaskHeader(const CommonBitmaskHeader &x)\n{\n    m_bit_bound = x.m_bit_bound;\n}\n\nCommonBitmaskHeader::CommonBitmaskHeader(CommonBitmaskHeader &&x)\n{\n    m_bit_bound = std::move(x.m_bit_bound);\n}\n\nCommonBitmaskHeader& CommonBitmaskHeader::operator=(const CommonBitmaskHeader &x)\n{\n    m_bit_bound = x.m_bit_bound;\n\n    return *this;\n}\n\nCommonBitmaskHeader& CommonBitmaskHeader::operator=(CommonBitmaskHeader &&x)\n{\n    m_bit_bound = std::move(x.m_bit_bound);\n\n    return *this;\n}\n\nsize_t CommonBitmaskHeader::getCdrSerializedSize(const CommonBitmaskHeader&, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 2 + eprosima::fastcdr::Cdr::alignment(current_alignment, 2);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonBitmaskHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bit_bound;\n}\n\nvoid CommonBitmaskHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bit_bound;\n}\n\nbool CommonBitmaskHeader::operator==(const CommonBitmaskHeader& other) const\n{\n    return m_bit_bound == other.m_bit_bound;\n}\n\nCompleteBitmaskType::CompleteBitmaskType()\n{\n}\n\nCompleteBitmaskType::~CompleteBitmaskType()\n{\n}\n\nCompleteBitmaskType::CompleteBitmaskType(const CompleteBitmaskType &x)\n{\n    m_bitmask_flags = x.m_bitmask_flags;\n    m_header = x.m_header;\n    m_flag_seq = x.m_flag_seq;\n}\n\nCompleteBitmaskType::CompleteBitmaskType(CompleteBitmaskType &&x)\n{\n    m_bitmask_flags = std::move(x.m_bitmask_flags);\n    m_header = std::move(x.m_header);\n    m_flag_seq = std::move(x.m_flag_seq);\n}\n\nCompleteBitmaskType& CompleteBitmaskType::operator=(const CompleteBitmaskType &x)\n{\n    m_bitmask_flags = x.m_bitmask_flags;\n    m_header = x.m_header;\n    m_flag_seq = x.m_flag_seq;\n\n    return *this;\n}\n\nCompleteBitmaskType& CompleteBitmaskType::operator=(CompleteBitmaskType &&x)\n{\n    m_bitmask_flags = std::move(x.m_bitmask_flags);\n    m_header = std::move(x.m_header);\n    m_flag_seq = std::move(x.m_flag_seq);\n\n    return *this;\n}\n\nsize_t CompleteBitmaskType::getCdrSerializedSize(const CompleteBitmaskType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += BitmaskTypeFlag::getCdrSerializedSize(data.bitmask_flags(), current_alignment);\n    current_alignment += CompleteBitmaskHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.flag_seq().size(); ++a)\n    {\n        current_alignment += CompleteBitflag::getCdrSerializedSize(data.flag_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteBitmaskType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bitmask_flags;\n    scdr << m_header;\n    scdr << m_flag_seq;\n}\n\nvoid CompleteBitmaskType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bitmask_flags;\n    dcdr >> m_header;\n    dcdr >> m_flag_seq;\n}\n\nbool CompleteBitmaskType::operator==(const CompleteBitmaskType& other) const\n{\n    if(m_bitmask_flags == other.m_bitmask_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_flag_seq, other.m_flag_seq);\n    }\n    return false;\n}\n\nMinimalBitmaskType::MinimalBitmaskType()\n{\n}\n\nMinimalBitmaskType::~MinimalBitmaskType()\n{\n}\n\nMinimalBitmaskType::MinimalBitmaskType(const MinimalBitmaskType &x)\n{\n    m_bitmask_flags = x.m_bitmask_flags;\n    m_header = x.m_header;\n    m_flag_seq = x.m_flag_seq;\n}\n\nMinimalBitmaskType::MinimalBitmaskType(MinimalBitmaskType &&x)\n{\n    m_bitmask_flags = std::move(x.m_bitmask_flags);\n    m_header = std::move(x.m_header);\n    m_flag_seq = std::move(x.m_flag_seq);\n}\n\nMinimalBitmaskType& MinimalBitmaskType::operator=(const MinimalBitmaskType &x)\n{\n    m_bitmask_flags = x.m_bitmask_flags;\n    m_header = x.m_header;\n    m_flag_seq = x.m_flag_seq;\n\n    return *this;\n}\n\nMinimalBitmaskType& MinimalBitmaskType::operator=(MinimalBitmaskType &&x)\n{\n    m_bitmask_flags = std::move(x.m_bitmask_flags);\n    m_header = std::move(x.m_header);\n    m_flag_seq = std::move(x.m_flag_seq);\n\n    return *this;\n}\n\nsize_t MinimalBitmaskType::getCdrSerializedSize(const MinimalBitmaskType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += BitmaskTypeFlag::getCdrSerializedSize(data.bitmask_flags(), current_alignment);\n    current_alignment += MinimalBitmaskHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.flag_seq().size(); ++a)\n    {\n        current_alignment += MinimalBitflag::getCdrSerializedSize(data.flag_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalBitmaskType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bitmask_flags;\n    scdr << m_header;\n    scdr << m_flag_seq;\n}\n\nvoid MinimalBitmaskType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bitmask_flags;\n    dcdr >> m_header;\n    dcdr >> m_flag_seq;\n}\n\nbool MinimalBitmaskType::operator==(const MinimalBitmaskType& other) const\n{\n    if(m_bitmask_flags == other.m_bitmask_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_flag_seq, other.m_flag_seq);\n    }\n    return false;\n}\n\nCommonBitfield::CommonBitfield()\n{\n}\n\nCommonBitfield::~CommonBitfield()\n{\n}\n\nCommonBitfield::CommonBitfield(const CommonBitfield &x)\n{\n    m_position = x.m_position;\n    m_flags = x.m_flags;\n    m_bitcount = x.m_bitcount;\n    m_holder_type = x.m_holder_type;\n}\n\nCommonBitfield::CommonBitfield(CommonBitfield &&x)\n{\n    m_position = std::move(x.m_position);\n    m_flags = std::move(x.m_flags);\n    m_bitcount = std::move(x.m_bitcount);\n    m_holder_type = std::move(x.m_holder_type);\n}\n\nCommonBitfield& CommonBitfield::operator=(const CommonBitfield &x)\n{\n    m_position = x.m_position;\n    m_flags = x.m_flags;\n    m_bitcount = x.m_bitcount;\n    m_holder_type = x.m_holder_type;\n\n    return *this;\n}\n\nCommonBitfield& CommonBitfield::operator=(CommonBitfield &&x)\n{\n    m_position = std::move(x.m_position);\n    m_flags = std::move(x.m_flags);\n    m_bitcount = std::move(x.m_bitcount);\n    m_holder_type = std::move(x.m_holder_type);\n\n    return *this;\n}\n\nsize_t CommonBitfield::getCdrSerializedSize(const CommonBitfield& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 2 + eprosima::fastcdr::Cdr::alignment(current_alignment, 2);\n    current_alignment += BitsetMemberFlag::getCdrSerializedSize(data.flags(), current_alignment);\n    current_alignment += 1 + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n    current_alignment += 1 + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CommonBitfield::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_position;\n    scdr << m_flags;\n    scdr << m_bitcount;\n    scdr << m_holder_type;\n}\n\nvoid CommonBitfield::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_position;\n    dcdr >> m_flags;\n    dcdr >> m_bitcount;\n    dcdr >> m_holder_type;\n}\n\nbool CommonBitfield::operator==(const CommonBitfield& other) const\n{\n    return m_position == other.m_position &&\n            m_flags == other.m_flags &&\n            m_bitcount == other.m_bitcount &&\n            m_holder_type == other.m_holder_type;\n}\n\nCompleteBitfield::CompleteBitfield()\n{\n}\n\nCompleteBitfield::~CompleteBitfield()\n{\n}\n\nCompleteBitfield::CompleteBitfield(const CompleteBitfield &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n}\n\nCompleteBitfield::CompleteBitfield(CompleteBitfield &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n}\n\nCompleteBitfield& CompleteBitfield::operator=(const CompleteBitfield &x)\n{\n    m_common = x.m_common;\n    m_detail = x.m_detail;\n\n    return *this;\n}\n\nCompleteBitfield& CompleteBitfield::operator=(CompleteBitfield &&x)\n{\n    m_common = std::move(x.m_common);\n    m_detail = std::move(x.m_detail);\n\n    return *this;\n}\n\nsize_t CompleteBitfield::getCdrSerializedSize(const CompleteBitfield& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonBitfield::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += CompleteMemberDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteBitfield::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_detail;\n}\n\nvoid CompleteBitfield::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_detail;\n}\n\nbool CompleteBitfield::operator==(const CompleteBitfield& other) const\n{\n    return m_common == other.m_common &&\n            m_detail == other.m_detail;\n}\n\nMinimalBitfield::MinimalBitfield()\n{\n}\n\nMinimalBitfield::~MinimalBitfield()\n{\n}\n\nMinimalBitfield::MinimalBitfield(const MinimalBitfield &x)\n{\n    m_name_hash = x.m_name_hash;\n    m_common = x.m_common;\n}\n\nMinimalBitfield::MinimalBitfield(MinimalBitfield &&x)\n{\n    m_name_hash = std::move(x.m_name_hash);\n    m_common = std::move(x.m_common);\n}\n\nMinimalBitfield& MinimalBitfield::operator=(const MinimalBitfield &x)\n{\n    m_name_hash = x.m_name_hash;\n    m_common = x.m_common;\n\n    return *this;\n}\n\nMinimalBitfield& MinimalBitfield::operator=(MinimalBitfield &&x)\n{\n    m_name_hash = std::move(x.m_name_hash);\n    m_common = std::move(x.m_common);\n\n    return *this;\n}\n\nsize_t MinimalBitfield::getCdrSerializedSize(const MinimalBitfield& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += CommonBitfield::getCdrSerializedSize(data.common(), current_alignment);\n    current_alignment += ((4) * 1) + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalBitfield::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_common;\n    scdr << m_name_hash;\n}\n\nvoid MinimalBitfield::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_common;\n    dcdr >> m_name_hash;\n}\n\nbool MinimalBitfield::operator==(const MinimalBitfield& other) const\n{\n    return m_common == other.m_common &&\n            m_name_hash == other.m_name_hash;\n}\n\nCompleteBitsetHeader::CompleteBitsetHeader()\n{\n}\n\nCompleteBitsetHeader::~CompleteBitsetHeader()\n{\n}\n\nCompleteBitsetHeader::CompleteBitsetHeader(const CompleteBitsetHeader &x)\n{\n    m_detail = x.m_detail;\n    m_base_type = x.m_base_type;\n}\n\nCompleteBitsetHeader::CompleteBitsetHeader(CompleteBitsetHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n    m_base_type = std::move(x.m_base_type);\n}\n\nCompleteBitsetHeader& CompleteBitsetHeader::operator=(const CompleteBitsetHeader &x)\n{\n    m_detail = x.m_detail;\n    m_base_type = x.m_base_type;\n\n    return *this;\n}\n\nCompleteBitsetHeader& CompleteBitsetHeader::operator=(CompleteBitsetHeader &&x)\n{\n    m_detail = std::move(x.m_detail);\n    m_base_type = std::move(x.m_base_type);\n\n    return *this;\n}\n\nsize_t CompleteBitsetHeader::getCdrSerializedSize(const CompleteBitsetHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.base_type(), current_alignment);\n    current_alignment += CompleteTypeDetail::getCdrSerializedSize(data.detail(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteBitsetHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_base_type;\n    scdr << m_detail;\n}\n\nvoid CompleteBitsetHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_base_type;\n    dcdr >> m_detail;\n}\n\nbool CompleteBitsetHeader::operator==(const CompleteBitsetHeader& other) const\n{\n    return m_base_type == other.m_base_type &&\n            m_detail == other.m_detail;\n}\n\nMinimalBitsetHeader::MinimalBitsetHeader()\n{\n}\n\nMinimalBitsetHeader::~MinimalBitsetHeader()\n{\n}\n\nMinimalBitsetHeader::MinimalBitsetHeader(const MinimalBitsetHeader &x)\n{\n    m_base_type = x.m_base_type;\n}\n\nMinimalBitsetHeader::MinimalBitsetHeader(MinimalBitsetHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n}\n\nMinimalBitsetHeader& MinimalBitsetHeader::operator=(const MinimalBitsetHeader &x)\n{\n    m_base_type = x.m_base_type;\n    return *this;\n}\n\nMinimalBitsetHeader& MinimalBitsetHeader::operator=(MinimalBitsetHeader &&x)\n{\n    m_base_type = std::move(x.m_base_type);\n    return *this;\n}\n\nsize_t MinimalBitsetHeader::getCdrSerializedSize(const MinimalBitsetHeader& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.base_type(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalBitsetHeader::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_base_type;\n}\n\nvoid MinimalBitsetHeader::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_base_type;\n}\n\nbool MinimalBitsetHeader::operator==(const MinimalBitsetHeader& other) const\n{\n    return m_base_type == other.m_base_type;\n}\n\nCompleteBitsetType::CompleteBitsetType()\n{\n}\n\nCompleteBitsetType::~CompleteBitsetType()\n{\n}\n\nCompleteBitsetType::CompleteBitsetType(const CompleteBitsetType &x)\n{\n    m_bitset_flags = x.m_bitset_flags;\n    m_header = x.m_header;\n    m_field_seq = x.m_field_seq;\n}\n\nCompleteBitsetType::CompleteBitsetType(CompleteBitsetType &&x)\n{\n    m_bitset_flags = std::move(x.m_bitset_flags);\n    m_header = std::move(x.m_header);\n    m_field_seq = std::move(x.m_field_seq);\n}\n\nCompleteBitsetType& CompleteBitsetType::operator=(const CompleteBitsetType &x)\n{\n    m_bitset_flags = x.m_bitset_flags;\n    m_header = x.m_header;\n    m_field_seq = x.m_field_seq;\n\n    return *this;\n}\n\nCompleteBitsetType& CompleteBitsetType::operator=(CompleteBitsetType &&x)\n{\n    m_bitset_flags = std::move(x.m_bitset_flags);\n    m_header = std::move(x.m_header);\n    m_field_seq = std::move(x.m_field_seq);\n\n    return *this;\n}\n\nsize_t CompleteBitsetType::getCdrSerializedSize(const CompleteBitsetType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += BitsetTypeFlag::getCdrSerializedSize(data.bitset_flags(), current_alignment);\n    current_alignment += CompleteBitsetHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.field_seq().size(); ++a)\n    {\n        current_alignment += CompleteBitfield::getCdrSerializedSize(data.field_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteBitsetType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bitset_flags;\n    scdr << m_header;\n    scdr << m_field_seq;\n}\n\nvoid CompleteBitsetType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bitset_flags;\n    dcdr >> m_header;\n    dcdr >> m_field_seq;\n}\n\nbool CompleteBitsetType::operator==(const CompleteBitsetType& other) const\n{\n    if(m_bitset_flags == other.m_bitset_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_field_seq, other.m_field_seq);\n    }\n    return false;\n}\n\nMinimalBitsetType::MinimalBitsetType()\n{\n}\n\nMinimalBitsetType::~MinimalBitsetType()\n{\n}\n\nMinimalBitsetType::MinimalBitsetType(const MinimalBitsetType &x)\n{\n    m_bitset_flags = x.m_bitset_flags;\n    m_header = x.m_header;\n    m_field_seq = x.m_field_seq;\n}\n\nMinimalBitsetType::MinimalBitsetType(MinimalBitsetType &&x)\n{\n    m_bitset_flags = std::move(x.m_bitset_flags);\n    m_header = std::move(x.m_header);\n    m_field_seq = std::move(x.m_field_seq);\n}\n\nMinimalBitsetType& MinimalBitsetType::operator=(const MinimalBitsetType &x)\n{\n    m_bitset_flags = x.m_bitset_flags;\n    m_header = x.m_header;\n    m_field_seq = x.m_field_seq;\n\n    return *this;\n}\n\nMinimalBitsetType& MinimalBitsetType::operator=(MinimalBitsetType &&x)\n{\n    m_bitset_flags = std::move(x.m_bitset_flags);\n    m_header = std::move(x.m_header);\n    m_field_seq = std::move(x.m_field_seq);\n\n    return *this;\n}\n\nsize_t MinimalBitsetType::getCdrSerializedSize(const MinimalBitsetType& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += BitsetTypeFlag::getCdrSerializedSize(data.bitset_flags(), current_alignment);\n    current_alignment += MinimalBitsetHeader::getCdrSerializedSize(data.header(), current_alignment);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.field_seq().size(); ++a)\n    {\n        current_alignment += MinimalBitfield::getCdrSerializedSize(data.field_seq().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalBitsetType::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_bitset_flags;\n    scdr << m_header;\n    scdr << m_field_seq;\n}\n\nvoid MinimalBitsetType::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_bitset_flags;\n    dcdr >> m_header;\n    dcdr >> m_field_seq;\n}\n\nbool MinimalBitsetType::operator==(const MinimalBitsetType& other) const\n{\n    if(m_bitset_flags == other.m_bitset_flags &&\n        m_header == other.m_header)\n    {\n        return compareSequence(m_field_seq, other.m_field_seq);\n    }\n    return false;\n}\n\nCompleteExtendedType::CompleteExtendedType()\n{\n}\n\nCompleteExtendedType::~CompleteExtendedType()\n{\n}\n\nCompleteExtendedType::CompleteExtendedType(const CompleteExtendedType &)\n{\n}\n\nCompleteExtendedType::CompleteExtendedType(CompleteExtendedType &&)\n{\n}\n\nCompleteExtendedType& CompleteExtendedType::operator=(const CompleteExtendedType &)\n{\n    return *this;\n}\n\nCompleteExtendedType& CompleteExtendedType::operator=(CompleteExtendedType &&)\n{\n    return *this;\n}\n\nsize_t CompleteExtendedType::getCdrSerializedSize(const CompleteExtendedType& , size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteExtendedType::serialize(eprosima::fastcdr::Cdr &) const\n{\n}\n\nvoid CompleteExtendedType::deserialize(eprosima::fastcdr::Cdr &)\n{\n}\n\nMinimalExtendedType::MinimalExtendedType()\n{\n}\n\nMinimalExtendedType::~MinimalExtendedType()\n{\n}\n\nMinimalExtendedType::MinimalExtendedType(const MinimalExtendedType &)\n{\n}\n\nMinimalExtendedType::MinimalExtendedType(MinimalExtendedType &&)\n{\n}\n\nMinimalExtendedType& MinimalExtendedType::operator=(const MinimalExtendedType &)\n{\n    return *this;\n}\n\nMinimalExtendedType& MinimalExtendedType::operator=(MinimalExtendedType &&)\n{\n    return *this;\n}\n\nsize_t MinimalExtendedType::getCdrSerializedSize(const MinimalExtendedType& , size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalExtendedType::serialize(eprosima::fastcdr::Cdr &) const\n{\n}\n\nvoid MinimalExtendedType::deserialize(eprosima::fastcdr::Cdr &)\n{\n}\n\nTypeIdentifierTypeObjectPair::TypeIdentifierTypeObjectPair()\n{\n}\n\nTypeIdentifierTypeObjectPair::~TypeIdentifierTypeObjectPair()\n{\n}\n\nTypeIdentifierTypeObjectPair::TypeIdentifierTypeObjectPair(const TypeIdentifierTypeObjectPair &x)\n{\n    m_type_identifier = x.m_type_identifier;\n    m_type_object = x.m_type_object;\n}\n\nTypeIdentifierTypeObjectPair::TypeIdentifierTypeObjectPair(TypeIdentifierTypeObjectPair &&x)\n{\n    m_type_identifier = std::move(x.m_type_identifier);\n    m_type_object = std::move(x.m_type_object);\n}\n\nTypeIdentifierTypeObjectPair& TypeIdentifierTypeObjectPair::operator=(const TypeIdentifierTypeObjectPair &x)\n{\n    m_type_identifier = x.m_type_identifier;\n    m_type_object = x.m_type_object;\n\n    return *this;\n}\n\nTypeIdentifierTypeObjectPair& TypeIdentifierTypeObjectPair::operator=(TypeIdentifierTypeObjectPair &&x)\n{\n    m_type_identifier = std::move(x.m_type_identifier);\n    m_type_object = std::move(x.m_type_object);\n\n    return *this;\n}\n\nsize_t TypeIdentifierTypeObjectPair::getCdrSerializedSize(const TypeIdentifierTypeObjectPair& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_identifier(), current_alignment);\n    current_alignment += TypeObject::getCdrSerializedSize(data.type_object(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeIdentifierTypeObjectPair::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_type_identifier;\n    scdr << m_type_object;\n}\n\nvoid TypeIdentifierTypeObjectPair::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_type_identifier;\n    dcdr >> m_type_object;\n}\n\nTypeIdentifierPair::TypeIdentifierPair()\n{\n}\n\nTypeIdentifierPair::~TypeIdentifierPair()\n{\n}\n\nTypeIdentifierPair::TypeIdentifierPair(const TypeIdentifierPair &x)\n{\n    m_type_identifier1 = x.m_type_identifier1;\n    m_type_identifier2 = x.m_type_identifier2;\n}\n\nTypeIdentifierPair::TypeIdentifierPair(TypeIdentifierPair &&x)\n{\n    m_type_identifier1 = std::move(x.m_type_identifier1);\n    m_type_identifier2 = std::move(x.m_type_identifier2);\n}\n\nTypeIdentifierPair& TypeIdentifierPair::operator=(const TypeIdentifierPair &x)\n{\n    m_type_identifier1 = x.m_type_identifier1;\n    m_type_identifier2 = x.m_type_identifier2;\n\n    return *this;\n}\n\nTypeIdentifierPair& TypeIdentifierPair::operator=(TypeIdentifierPair &&x)\n{\n    m_type_identifier1 = std::move(x.m_type_identifier1);\n    m_type_identifier2 = std::move(x.m_type_identifier2);\n\n    return *this;\n}\n\nsize_t TypeIdentifierPair::getCdrSerializedSize(const TypeIdentifierPair& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_identifier1(), current_alignment);\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_identifier2(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeIdentifierPair::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_type_identifier1;\n    scdr << m_type_identifier2;\n}\n\nvoid TypeIdentifierPair::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_type_identifier1;\n    dcdr >> m_type_identifier2;\n}\n\nTypeIdentifierWithSize::TypeIdentifierWithSize()\n{\n}\n\nTypeIdentifierWithSize::~TypeIdentifierWithSize()\n{\n}\n\nTypeIdentifierWithSize::TypeIdentifierWithSize(const TypeIdentifierWithSize &x)\n{\n    m_type_id = x.m_type_id;\n    m_typeobject_serialized_size = x.m_typeobject_serialized_size;\n}\n\nTypeIdentifierWithSize::TypeIdentifierWithSize(TypeIdentifierWithSize &&x)\n{\n    m_type_id = std::move(x.m_type_id);\n    m_typeobject_serialized_size = std::move(x.m_typeobject_serialized_size);\n}\n\nTypeIdentifierWithSize& TypeIdentifierWithSize::operator=(const TypeIdentifierWithSize &x)\n{\n    m_type_id = x.m_type_id;\n    m_typeobject_serialized_size = x.m_typeobject_serialized_size;\n\n    return *this;\n}\n\nTypeIdentifierWithSize& TypeIdentifierWithSize::operator=(TypeIdentifierWithSize &&x)\n{\n    m_type_id = std::move(x.m_type_id);\n    m_typeobject_serialized_size = std::move(x.m_typeobject_serialized_size);\n\n    return *this;\n}\n\nsize_t TypeIdentifierWithSize::getCdrSerializedSize(const TypeIdentifierWithSize& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifier::getCdrSerializedSize(data.type_id(), current_alignment);\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeIdentifierWithSize::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_type_id;\n    scdr << m_typeobject_serialized_size;\n}\n\nvoid TypeIdentifierWithSize::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_type_id;\n    dcdr >> m_typeobject_serialized_size;\n}\n\nTypeIdentifierWithDependencies::TypeIdentifierWithDependencies()\n{\n}\n\nTypeIdentifierWithDependencies::~TypeIdentifierWithDependencies()\n{\n}\n\nTypeIdentifierWithDependencies::TypeIdentifierWithDependencies(const TypeIdentifierWithDependencies &x)\n{\n    m_typeid_with_size = x.m_typeid_with_size;\n    m_dependent_typeid_count = x.m_dependent_typeid_count;\n    m_dependent_typeids = x.m_dependent_typeids;\n}\n\nTypeIdentifierWithDependencies::TypeIdentifierWithDependencies(TypeIdentifierWithDependencies &&x)\n{\n    m_typeid_with_size = std::move(x.m_typeid_with_size);\n    m_dependent_typeid_count = std::move(x.m_dependent_typeid_count);\n    m_dependent_typeids = std::move(x.m_dependent_typeids);\n}\n\nTypeIdentifierWithDependencies& TypeIdentifierWithDependencies::operator=(const TypeIdentifierWithDependencies &x)\n{\n    m_typeid_with_size = x.m_typeid_with_size;\n    m_dependent_typeid_count = x.m_dependent_typeid_count;\n    m_dependent_typeids = x.m_dependent_typeids;\n\n    return *this;\n}\n\nTypeIdentifierWithDependencies& TypeIdentifierWithDependencies::operator=(TypeIdentifierWithDependencies &&x)\n{\n    m_typeid_with_size = std::move(x.m_typeid_with_size);\n    m_dependent_typeid_count = std::move(x.m_dependent_typeid_count);\n    m_dependent_typeids = std::move(x.m_dependent_typeids);\n\n    return *this;\n}\n\nsize_t TypeIdentifierWithDependencies::getCdrSerializedSize(const TypeIdentifierWithDependencies& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifierWithSize::getCdrSerializedSize(data.typeid_with_size(), current_alignment);\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n\n    current_alignment += 4 + eprosima::fastcdr::Cdr::alignment(current_alignment, 4);\n    for(size_t a = 0; a < data.dependent_typeids().size(); ++a)\n    {\n        current_alignment += TypeIdentifierWithSize::getCdrSerializedSize(data.dependent_typeids().at(a), current_alignment);\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeIdentifierWithDependencies::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_typeid_with_size;\n    scdr << m_dependent_typeid_count;\n    scdr << m_dependent_typeids;\n}\n\nvoid TypeIdentifierWithDependencies::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_typeid_with_size;\n    dcdr >> m_dependent_typeid_count;\n    dcdr >> m_dependent_typeids;\n}\n\n\n/////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nCompleteTypeObject::CompleteTypeObject()\n{\n    m__d = 0x00;\n}\n\nCompleteTypeObject::~CompleteTypeObject()\n{\n}\n\nCompleteTypeObject::CompleteTypeObject(const CompleteTypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n}\n\nCompleteTypeObject::CompleteTypeObject(CompleteTypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n}\n\nCompleteTypeObject& CompleteTypeObject::operator=(const CompleteTypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n    return *this;\n}\n\nCompleteTypeObject& CompleteTypeObject::operator=(CompleteTypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n    return *this;\n}\n\nvoid CompleteTypeObject::_d(octet __d) // Special case to ease... sets the current active member\n{\n    m__d = __d;\n    if(m__d != __d) throw BadParamException(\"Discriminator doesn't correspond with the selected union member\");\n}\n\noctet CompleteTypeObject::_d() const\n{\n    return m__d;\n}\n\noctet& CompleteTypeObject::_d()\n{\n    return m__d;\n}\n\nvoid CompleteTypeObject::alias_type(CompleteAliasType _alias_type)\n{\n    m_alias_type = _alias_type;\n    m__d = TK_ALIAS;\n}\n\nconst CompleteAliasType& CompleteTypeObject::alias_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n    return m_alias_type;\n}\n\nCompleteAliasType& CompleteTypeObject::alias_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_alias_type;\n}\n\nvoid CompleteTypeObject::annotation_type(CompleteAnnotationType _annotation_type)\n{\n    m_annotation_type = _annotation_type;\n    m__d = TK_ANNOTATION;\n}\n\nconst CompleteAnnotationType& CompleteTypeObject::annotation_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ANNOTATION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_annotation_type;\n}\n\nCompleteAnnotationType& CompleteTypeObject::annotation_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ANNOTATION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_annotation_type;\n}\n\nvoid CompleteTypeObject::struct_type(CompleteStructType _struct_type)\n{\n    m_struct_type = _struct_type;\n    m__d = TK_STRUCTURE;\n}\n\nconst CompleteStructType& CompleteTypeObject::struct_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_STRUCTURE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_struct_type;\n}\n\nCompleteStructType& CompleteTypeObject::struct_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_STRUCTURE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_struct_type;\n}\n\nvoid CompleteTypeObject::union_type(CompleteUnionType _union_type)\n{\n    m_union_type = _union_type;\n    m__d = TK_UNION;\n}\n\nconst CompleteUnionType& CompleteTypeObject::union_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_UNION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_union_type;\n}\n\nCompleteUnionType& CompleteTypeObject::union_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_UNION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_union_type;\n}\n\nvoid CompleteTypeObject::bitset_type(CompleteBitsetType _bitset_type)\n{\n    m_bitset_type = _bitset_type;\n    m__d = TK_BITSET;\n}\n\nconst CompleteBitsetType& CompleteTypeObject::bitset_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITSET:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitset_type;\n}\n\nCompleteBitsetType& CompleteTypeObject::bitset_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITSET:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitset_type;\n}\n\nvoid CompleteTypeObject::sequence_type(CompleteSequenceType _sequence_type)\n{\n    m_sequence_type = _sequence_type;\n    m__d = TK_SEQUENCE;\n}\n\nconst CompleteSequenceType& CompleteTypeObject::sequence_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_SEQUENCE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_sequence_type;\n}\n\nCompleteSequenceType& CompleteTypeObject::sequence_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_SEQUENCE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_sequence_type;\n}\n\nvoid CompleteTypeObject::array_type(CompleteArrayType _array_type)\n{\n    m_array_type = _array_type;\n    m__d = TK_ARRAY;\n}\n\nconst CompleteArrayType& CompleteTypeObject::array_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ARRAY:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_array_type;\n}\n\nCompleteArrayType& CompleteTypeObject::array_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ARRAY:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_array_type;\n}\n\nvoid CompleteTypeObject::map_type(CompleteMapType _map_type)\n{\n    m_map_type = _map_type;\n    m__d = TK_MAP;\n}\n\nconst CompleteMapType& CompleteTypeObject::map_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_MAP:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_map_type;\n}\n\nCompleteMapType& CompleteTypeObject::map_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_MAP:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_map_type;\n}\n\nvoid CompleteTypeObject::enumerated_type(CompleteEnumeratedType _enumerated_type)\n{\n    m_enumerated_type = _enumerated_type;\n    m__d = TK_ENUM;\n}\n\nconst CompleteEnumeratedType& CompleteTypeObject::enumerated_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ENUM:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_enumerated_type;\n}\n\nCompleteEnumeratedType& CompleteTypeObject::enumerated_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ENUM:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_enumerated_type;\n}\n\nvoid CompleteTypeObject::bitmask_type(CompleteBitmaskType _bitmask_type)\n{\n    m_bitmask_type = _bitmask_type;\n    m__d = TK_BITMASK;\n}\n\nconst CompleteBitmaskType& CompleteTypeObject::bitmask_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITMASK:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitmask_type;\n}\n\nCompleteBitmaskType& CompleteTypeObject::bitmask_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITMASK:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitmask_type;\n}\n\nvoid CompleteTypeObject::extended_type(CompleteExtendedType _extended_type)\n{\n    m_extended_type = _extended_type;\n    m__d = 0x00; // Default\n}\n\nconst CompleteExtendedType& CompleteTypeObject::extended_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        case TK_ANNOTATION:\n        case TK_STRUCTURE:\n        case TK_UNION:\n        case TK_BITSET:\n        case TK_SEQUENCE:\n        case TK_ARRAY:\n        case TK_MAP:\n        case TK_ENUM:\n        case TK_BITMASK:\n        break;\n        default:\n        b = true;\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_extended_type;\n}\n\nCompleteExtendedType& CompleteTypeObject::extended_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        case TK_ANNOTATION:\n        case TK_STRUCTURE:\n        case TK_UNION:\n        case TK_BITSET:\n        case TK_SEQUENCE:\n        case TK_ARRAY:\n        case TK_MAP:\n        case TK_ENUM:\n        case TK_BITMASK:\n        break;\n        default:\n        b = true;\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_extended_type;\n}\n\nsize_t CompleteTypeObject::getCdrSerializedSize(const CompleteTypeObject& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 1 + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n    switch(data.m__d)\n    {\n        case TK_ALIAS:\n        current_alignment += CompleteAliasType::getCdrSerializedSize(data.alias_type(), current_alignment);\n        break;\n        case TK_ANNOTATION:\n        current_alignment += CompleteAnnotationType::getCdrSerializedSize(data.annotation_type(), current_alignment);\n        break;\n        case TK_STRUCTURE:\n        current_alignment += CompleteStructType::getCdrSerializedSize(data.struct_type(), current_alignment);\n        break;\n        case TK_UNION:\n        current_alignment += CompleteUnionType::getCdrSerializedSize(data.union_type(), current_alignment);\n        break;\n        case TK_BITSET:\n        current_alignment += CompleteBitsetType::getCdrSerializedSize(data.bitset_type(), current_alignment);\n        break;\n        case TK_SEQUENCE:\n        current_alignment += CompleteSequenceType::getCdrSerializedSize(data.sequence_type(), current_alignment);\n        break;\n        case TK_ARRAY:\n        current_alignment += CompleteArrayType::getCdrSerializedSize(data.array_type(), current_alignment);\n        break;\n        case TK_MAP:\n        current_alignment += CompleteMapType::getCdrSerializedSize(data.map_type(), current_alignment);\n        break;\n        case TK_ENUM:\n        current_alignment += CompleteEnumeratedType::getCdrSerializedSize(data.enumerated_type(), current_alignment);\n        break;\n        case TK_BITMASK:\n        current_alignment += CompleteBitmaskType::getCdrSerializedSize(data.bitmask_type(), current_alignment);\n        break;\n        default:\n        current_alignment += CompleteExtendedType::getCdrSerializedSize(data.extended_type(), current_alignment);\n        break;\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid CompleteTypeObject::serialize(eprosima::fastcdr::Cdr &cdr) const\n{\n    cdr << m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        cdr << m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        cdr << m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        cdr << m_struct_type;\n        break;\n        case TK_UNION:\n        cdr << m_union_type;\n        break;\n        case TK_BITSET:\n        cdr << m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        cdr << m_sequence_type;\n        break;\n        case TK_ARRAY:\n        cdr << m_array_type;\n        break;\n        case TK_MAP:\n        cdr << m_map_type;\n        break;\n        case TK_ENUM:\n        cdr << m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        cdr << m_bitmask_type;\n        break;\n        default:\n        cdr << m_extended_type;\n        break;\n    }\n}\n\nvoid CompleteTypeObject::deserialize(eprosima::fastcdr::Cdr &cdr)\n{\n    cdr >> m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        cdr >> m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        cdr >> m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        cdr >> m_struct_type;\n        break;\n        case TK_UNION:\n        cdr >> m_union_type;\n        break;\n        case TK_BITSET:\n        cdr >> m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        cdr >> m_sequence_type;\n        break;\n        case TK_ARRAY:\n        cdr >> m_array_type;\n        break;\n        case TK_MAP:\n        cdr >> m_map_type;\n        break;\n        case TK_ENUM:\n        cdr >> m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        cdr >> m_bitmask_type;\n        break;\n        default:\n        cdr >> m_extended_type;\n        break;\n    }\n}\n\nbool CompleteTypeObject::operator==(const CompleteTypeObject& other) const\n{\n    if (m__d == other.m__d)\n    {\n        switch(m__d)\n        {\n            case TK_ALIAS:\n            return m_alias_type == other.m_alias_type;\n            break;\n            case TK_ANNOTATION:\n            return m_annotation_type == other.m_annotation_type;\n            break;\n            case TK_STRUCTURE:\n            return m_struct_type == other.m_struct_type;\n            break;\n            case TK_UNION:\n            return m_union_type == other.m_union_type;\n            break;\n            case TK_BITSET:\n            return m_bitset_type == other.m_bitset_type;\n            break;\n            case TK_SEQUENCE:\n            return m_sequence_type == other.m_sequence_type;\n            break;\n            case TK_ARRAY:\n            return m_array_type == other.m_array_type;\n            break;\n            case TK_MAP:\n            return m_map_type == other.m_map_type;\n            break;\n            case TK_ENUM:\n            return m_enumerated_type == other.m_enumerated_type;\n            break;\n            case TK_BITMASK:\n            return m_bitmask_type == other.m_bitmask_type;\n            break;\n            default:\n            return m_extended_type == other.m_extended_type;\n            break;\n        }\n    }\n    return false;\n}\n\n/****************************************************************************************************************/\n\nMinimalTypeObject::MinimalTypeObject()\n{\n    m__d = 0x00;\n}\n\nMinimalTypeObject::~MinimalTypeObject()\n{\n}\n\nMinimalTypeObject::MinimalTypeObject(const MinimalTypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n}\n\nMinimalTypeObject::MinimalTypeObject(MinimalTypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n}\n\nMinimalTypeObject& MinimalTypeObject::operator=(const MinimalTypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n    return *this;\n}\n\nMinimalTypeObject& MinimalTypeObject::operator=(MinimalTypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        m_alias_type = x.m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        m_annotation_type = x.m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        m_struct_type = x.m_struct_type;\n        break;\n        case TK_UNION:\n        m_union_type = x.m_union_type;\n        break;\n        case TK_BITSET:\n        m_bitset_type = x.m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        m_sequence_type = x.m_sequence_type;\n        break;\n        case TK_ARRAY:\n        m_array_type = x.m_array_type;\n        break;\n        case TK_MAP:\n        m_map_type = x.m_map_type;\n        break;\n        case TK_ENUM:\n        m_enumerated_type = x.m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        m_bitmask_type = x.m_bitmask_type;\n        break;\n        default:\n        m_extended_type = x.m_extended_type;\n        break;\n    }\n    return *this;\n}\n\nvoid MinimalTypeObject::_d(octet __d) // Special case to ease... sets the current active member\n{\n    m__d = __d;\n    if(m__d != __d) throw BadParamException(\"Discriminator doesn't correspond with the selected union member\");\n}\n\noctet MinimalTypeObject::_d() const\n{\n    return m__d;\n}\n\noctet& MinimalTypeObject::_d()\n{\n    return m__d;\n}\n\nvoid MinimalTypeObject::alias_type(MinimalAliasType _alias_type)\n{\n    m_alias_type = _alias_type;\n    m__d = TK_ALIAS;\n}\n\nconst MinimalAliasType& MinimalTypeObject::alias_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_alias_type;\n}\n\nMinimalAliasType& MinimalTypeObject::alias_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_alias_type;\n}\n\nvoid MinimalTypeObject::annotation_type(MinimalAnnotationType _annotation_type)\n{\n    m_annotation_type = _annotation_type;\n    m__d = TK_ANNOTATION;\n}\n\nconst MinimalAnnotationType& MinimalTypeObject::annotation_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ANNOTATION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_annotation_type;\n}\n\nMinimalAnnotationType& MinimalTypeObject::annotation_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ANNOTATION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_annotation_type;\n}\n\nvoid MinimalTypeObject::struct_type(MinimalStructType _struct_type)\n{\n    m_struct_type = _struct_type;\n    m__d = TK_STRUCTURE;\n}\n\nconst MinimalStructType& MinimalTypeObject::struct_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_STRUCTURE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_struct_type;\n}\n\nMinimalStructType& MinimalTypeObject::struct_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_STRUCTURE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_struct_type;\n}\n\nvoid MinimalTypeObject::union_type(MinimalUnionType _union_type)\n{\n    m_union_type = _union_type;\n    m__d = TK_UNION;\n}\n\nconst MinimalUnionType& MinimalTypeObject::union_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_UNION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_union_type;\n}\n\nMinimalUnionType& MinimalTypeObject::union_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_UNION:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_union_type;\n}\n\nvoid MinimalTypeObject::bitset_type(MinimalBitsetType _bitset_type)\n{\n    m_bitset_type = _bitset_type;\n    m__d = TK_BITSET;\n}\n\nconst MinimalBitsetType& MinimalTypeObject::bitset_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITSET:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitset_type;\n}\n\nMinimalBitsetType& MinimalTypeObject::bitset_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITSET:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitset_type;\n}\n\nvoid MinimalTypeObject::sequence_type(MinimalSequenceType _sequence_type)\n{\n    m_sequence_type = _sequence_type;\n    m__d = TK_SEQUENCE;\n}\n\nconst MinimalSequenceType& MinimalTypeObject::sequence_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_SEQUENCE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_sequence_type;\n}\n\nMinimalSequenceType& MinimalTypeObject::sequence_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_SEQUENCE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_sequence_type;\n}\n\nvoid MinimalTypeObject::array_type(MinimalArrayType _array_type)\n{\n    m_array_type = _array_type;\n    m__d = TK_ARRAY;\n}\n\nconst MinimalArrayType& MinimalTypeObject::array_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ARRAY:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_array_type;\n}\n\nMinimalArrayType& MinimalTypeObject::array_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ARRAY:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_array_type;\n}\n\nvoid MinimalTypeObject::map_type(MinimalMapType _map_type)\n{\n    m_map_type = _map_type;\n    m__d = TK_MAP;\n}\n\nconst MinimalMapType& MinimalTypeObject::map_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_MAP:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_map_type;\n}\n\nMinimalMapType& MinimalTypeObject::map_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_MAP:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_map_type;\n}\n\nvoid MinimalTypeObject::enumerated_type(MinimalEnumeratedType _enumerated_type)\n{\n    m_enumerated_type = _enumerated_type;\n    m__d = TK_ENUM;\n}\n\nconst MinimalEnumeratedType& MinimalTypeObject::enumerated_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ENUM:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_enumerated_type;\n}\n\nMinimalEnumeratedType& MinimalTypeObject::enumerated_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ENUM:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_enumerated_type;\n}\n\nvoid MinimalTypeObject::bitmask_type(MinimalBitmaskType _bitmask_type)\n{\n    m_bitmask_type = _bitmask_type;\n    m__d = TK_BITMASK;\n}\n\nconst MinimalBitmaskType& MinimalTypeObject::bitmask_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITMASK:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitmask_type;\n}\n\nMinimalBitmaskType& MinimalTypeObject::bitmask_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_BITMASK:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_bitmask_type;\n}\n\nvoid MinimalTypeObject::extended_type(MinimalExtendedType _extended_type)\n{\n    m_extended_type = _extended_type;\n    m__d = 0x00; // Default\n}\n\nconst MinimalExtendedType& MinimalTypeObject::extended_type() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        case TK_ANNOTATION:\n        case TK_STRUCTURE:\n        case TK_UNION:\n        case TK_BITSET:\n        case TK_SEQUENCE:\n        case TK_ARRAY:\n        case TK_MAP:\n        case TK_ENUM:\n        case TK_BITMASK:\n        break;\n        default:\n        b = true;\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_extended_type;\n}\n\nMinimalExtendedType& MinimalTypeObject::extended_type()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        case TK_ANNOTATION:\n        case TK_STRUCTURE:\n        case TK_UNION:\n        case TK_BITSET:\n        case TK_SEQUENCE:\n        case TK_ARRAY:\n        case TK_MAP:\n        case TK_ENUM:\n        case TK_BITMASK:\n        break;\n        default:\n        b = true;\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_extended_type;\n}\n\nsize_t MinimalTypeObject::getCdrSerializedSize(const MinimalTypeObject& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 1 + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n    switch(data.m__d)\n    {\n        case TK_ALIAS:\n        current_alignment += MinimalAliasType::getCdrSerializedSize(data.alias_type(), current_alignment);\n        break;\n        case TK_ANNOTATION:\n        current_alignment += MinimalAnnotationType::getCdrSerializedSize(data.annotation_type(), current_alignment);\n        break;\n        case TK_STRUCTURE:\n        current_alignment += MinimalStructType::getCdrSerializedSize(data.struct_type(), current_alignment);\n        break;\n        case TK_UNION:\n        current_alignment += MinimalUnionType::getCdrSerializedSize(data.union_type(), current_alignment);\n        break;\n        case TK_BITSET:\n        current_alignment += MinimalBitsetType::getCdrSerializedSize(data.bitset_type(), current_alignment);\n        break;\n        case TK_SEQUENCE:\n        current_alignment += MinimalSequenceType::getCdrSerializedSize(data.sequence_type(), current_alignment);\n        break;\n        case TK_ARRAY:\n        current_alignment += MinimalArrayType::getCdrSerializedSize(data.array_type(), current_alignment);\n        break;\n        case TK_MAP:\n        current_alignment += MinimalMapType::getCdrSerializedSize(data.map_type(), current_alignment);\n        break;\n        case TK_ENUM:\n        current_alignment += MinimalEnumeratedType::getCdrSerializedSize(data.enumerated_type(), current_alignment);\n        break;\n        case TK_BITMASK:\n        current_alignment += MinimalBitmaskType::getCdrSerializedSize(data.bitmask_type(), current_alignment);\n        break;\n        default:\n        current_alignment += MinimalExtendedType::getCdrSerializedSize(data.extended_type(), current_alignment);\n        break;\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid MinimalTypeObject::serialize(eprosima::fastcdr::Cdr &cdr) const\n{\n    cdr << m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        cdr << m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        cdr << m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        cdr << m_struct_type;\n        break;\n        case TK_UNION:\n        cdr << m_union_type;\n        break;\n        case TK_BITSET:\n        cdr << m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        cdr << m_sequence_type;\n        break;\n        case TK_ARRAY:\n        cdr << m_array_type;\n        break;\n        case TK_MAP:\n        cdr << m_map_type;\n        break;\n        case TK_ENUM:\n        cdr << m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        cdr << m_bitmask_type;\n        break;\n        default:\n        cdr << m_extended_type;\n        break;\n    }\n}\n\nvoid MinimalTypeObject::deserialize(eprosima::fastcdr::Cdr &cdr)\n{\n    cdr >> m__d;\n\n    switch(m__d)\n    {\n        case TK_ALIAS:\n        cdr >> m_alias_type;\n        break;\n        case TK_ANNOTATION:\n        cdr >> m_annotation_type;\n        break;\n        case TK_STRUCTURE:\n        cdr >> m_struct_type;\n        break;\n        case TK_UNION:\n        cdr >> m_union_type;\n        break;\n        case TK_BITSET:\n        cdr >> m_bitset_type;\n        break;\n        case TK_SEQUENCE:\n        cdr >> m_sequence_type;\n        break;\n        case TK_ARRAY:\n        cdr >> m_array_type;\n        break;\n        case TK_MAP:\n        cdr >> m_map_type;\n        break;\n        case TK_ENUM:\n        cdr >> m_enumerated_type;\n        break;\n        case TK_BITMASK:\n        cdr >> m_bitmask_type;\n        break;\n        default:\n        cdr >> m_extended_type;\n        break;\n    }\n}\n\nbool MinimalTypeObject::operator==(const MinimalTypeObject& other) const\n{\n    if (m__d == other.m__d)\n    {\n        switch(m__d)\n        {\n            case TK_ALIAS:\n            return m_alias_type == other.m_alias_type;\n            break;\n            case TK_ANNOTATION:\n            return m_annotation_type == other.m_annotation_type;\n            break;\n            case TK_STRUCTURE:\n            return m_struct_type == other.m_struct_type;\n            break;\n            case TK_UNION:\n            return m_union_type == other.m_union_type;\n            break;\n            case TK_BITSET:\n            return m_bitset_type == other.m_bitset_type;\n            break;\n            case TK_SEQUENCE:\n            return m_sequence_type == other.m_sequence_type;\n            break;\n            case TK_ARRAY:\n            return m_array_type == other.m_array_type;\n            break;\n            case TK_MAP:\n            return m_map_type == other.m_map_type;\n            break;\n            case TK_ENUM:\n            return m_enumerated_type == other.m_enumerated_type;\n            break;\n            case TK_BITMASK:\n            return m_bitmask_type == other.m_bitmask_type;\n            break;\n            default:\n            return m_extended_type == other.m_extended_type;\n            break;\n        }\n    }\n    return false;\n}\n\nTypeObject::TypeObject()\n{\n    m__d = 0x00; // Default\n}\n\nTypeObject::~TypeObject()\n{\n}\n\nTypeObject::TypeObject(const TypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        m_complete = x.m_complete;\n        break;\n        case EK_MINIMAL:\n        m_minimal = x.m_minimal;\n        break;\n        default:\n        break;\n    }\n}\n\nTypeObject::TypeObject(TypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        m_complete = std::move(x.m_complete);\n        break;\n        case EK_MINIMAL:\n        m_minimal = std::move(x.m_minimal);\n        break;\n        default:\n        break;\n    }\n}\n\nTypeObject& TypeObject::operator=(const TypeObject &x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        m_complete = x.m_complete;\n        break;\n        case EK_MINIMAL:\n        m_minimal = x.m_minimal;\n        break;\n        default:\n        break;\n    }\n\n    return *this;\n}\n\nTypeObject& TypeObject::operator=(TypeObject &&x)\n{\n    m__d = x.m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        m_complete = std::move(x.m_complete);\n        break;\n        case EK_MINIMAL:\n        m_minimal = std::move(x.m_minimal);\n        break;\n        default:\n        break;\n    }\n\n    return *this;\n}\n\nvoid TypeObject::_d(uint8_t __d) // Special case to ease... sets the current active member\n{\n    bool b = false;\n    m__d = __d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        switch(__d)\n        {\n            case EK_COMPLETE:\n            b = true;\n            break;\n            default:\n            break;\n        }\n        break;\n        case EK_MINIMAL:\n        switch(__d)\n        {\n            case EK_MINIMAL:\n            b = true;\n            break;\n            default:\n            break;\n        }\n        break;\n    }\n\n    if(!b) throw BadParamException(\"Discriminator doesn't correspond with the selected union member\");\n\n    m__d = __d;\n}\n\nuint8_t TypeObject::_d() const\n{\n    return m__d;\n}\n\nuint8_t& TypeObject::_d()\n{\n    return m__d;\n}\n\nvoid TypeObject::complete(const CompleteTypeObject &_complete)\n{\n    m_complete = _complete;\n    m__d = EK_COMPLETE;\n}\n\nvoid TypeObject::complete(CompleteTypeObject &&_complete)\n{\n    m_complete = std::move(_complete);\n    m__d = EK_COMPLETE;\n}\n\nconst CompleteTypeObject& TypeObject::complete() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_complete;\n}\n\nCompleteTypeObject& TypeObject::complete()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_complete;\n}\nvoid TypeObject::minimal(const MinimalTypeObject &_minimal)\n{\n    m_minimal = _minimal;\n    m__d = EK_MINIMAL;\n}\n\nvoid TypeObject::minimal(MinimalTypeObject &&_minimal)\n{\n    m_minimal = std::move(_minimal);\n    m__d = EK_MINIMAL;\n}\n\nconst MinimalTypeObject& TypeObject::minimal() const\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case EK_MINIMAL:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_minimal;\n}\n\nMinimalTypeObject& TypeObject::minimal()\n{\n    bool b = false;\n\n    switch(m__d)\n    {\n        case EK_MINIMAL:\n        b = true;\n        break;\n        default:\n        break;\n    }\n    if (!b)\n    {\n        throw BadParamException(\"This member hasn't been selected\");\n    }\n\n\n    return m_minimal;\n}\n\n// TODO(Ricardo) Review\nsize_t TypeObject::getCdrSerializedSize(const TypeObject& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += 1 + eprosima::fastcdr::Cdr::alignment(current_alignment, 1);\n\n    switch(data.m__d)\n    {\n        case EK_COMPLETE:\n        current_alignment += CompleteTypeObject::getCdrSerializedSize(data.complete(), current_alignment);\n        break;\n        case EK_MINIMAL:\n        current_alignment += MinimalTypeObject::getCdrSerializedSize(data.minimal(), current_alignment);\n        break;\n        default:\n        break;\n    }\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeObject::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        scdr << m_complete;\n        break;\n        case EK_MINIMAL:\n        scdr << m_minimal;\n        break;\n        default:\n        break;\n    }\n}\n\nvoid TypeObject::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m__d;\n\n    switch(m__d)\n    {\n        case EK_COMPLETE:\n        dcdr >> m_complete;\n        break;\n        case EK_MINIMAL:\n        dcdr >> m_minimal;\n        break;\n        default:\n        break;\n    }\n}\n\nbool TypeObject::operator==(const TypeObject& other) const\n{\n    if (m__d == other.m__d)\n    {\n        switch(m__d)\n        {\n            case EK_COMPLETE:\n            return m_complete == other.m_complete;\n            break;\n            case EK_MINIMAL:\n            return m_minimal == other.m_minimal;\n            break;\n            default:\n            break;\n        }\n    }\n    return false;\n}\n\nTypeInformation::TypeInformation()\n{\n}\n\nTypeInformation::~TypeInformation()\n{\n}\n\nTypeInformation::TypeInformation(const TypeInformation &x)\n{\n    m_minimal = x.m_minimal;\n    m_complete = x.m_complete;\n}\n\nTypeInformation::TypeInformation(TypeInformation &&x)\n{\n    m_minimal = std::move(x.m_minimal);\n    m_complete = std::move(x.m_complete);\n}\n\nTypeInformation& TypeInformation::operator=(const TypeInformation &x)\n{\n    m_minimal = x.m_minimal;\n    m_complete = x.m_complete;\n\n    return *this;\n}\n\nTypeInformation& TypeInformation::operator=(TypeInformation &&x)\n{\n    m_minimal = std::move(x.m_minimal);\n    m_complete = std::move(x.m_complete);\n\n    return *this;\n}\n\nsize_t TypeInformation::getCdrSerializedSize(const TypeInformation& data, size_t current_alignment)\n{\n    size_t initial_alignment = current_alignment;\n\n    current_alignment += TypeIdentifierWithDependencies::getCdrSerializedSize(data.minimal(), current_alignment);\n    current_alignment += TypeIdentifierWithDependencies::getCdrSerializedSize(data.complete(), current_alignment);\n\n    return current_alignment - initial_alignment;\n}\n\nvoid TypeInformation::serialize(eprosima::fastcdr::Cdr &scdr) const\n{\n    scdr << m_minimal;\n    scdr << m_complete;\n}\n\nvoid TypeInformation::deserialize(eprosima::fastcdr::Cdr &dcdr)\n{\n    dcdr >> m_minimal;\n    dcdr >> m_complete;\n}\n\n\n\n\n} // namespace types\n} // namespace fastrtps\n} // namespace eprosima\n", "idx": 57, "id": 15822, "msg": "", "proj": "eProsima-Fast-DDS", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -1297,26 +1297,32 @@ class Series(_Frame, IndexOpsMixin, Generic[T]):\n \n     tolist = to_list\n \n-    def drop_duplicates(self, inplace=False):\n+    def drop_duplicates(self, keep=\"first\", inplace=False):\n         \"\"\"\n-        Return koalas Series with duplicate values removed.\n+        Return Series with duplicate values removed.\n \n         Parameters\n         ----------\n-        inplace: bool, default False\n-            If True, performs operation inpalce and returns None.\n+        keep : {'first', 'last', ``False``}, default 'first'\n+            Method to handle dropping duplicates:\n+            - 'first' : Drop duplicates except for the first occurrence.\n+            - 'last' : Drop duplicates except for the last occurrence.\n+            - ``False`` : Drop all duplicates.\n+        inplace : bool, default ``False``\n+            If ``True``, performs operation inplace and returns None.\n \n         Returns\n         -------\n         Series\n-            Series with deplicates dropped.\n+            Series with duplicates dropped.\n \n         Examples\n         --------\n         Generate a Series with duplicated entries.\n+\n         >>> s = ks.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n         ...               name='animal')\n-        >>> s\n+        >>> s.sort_index()\n         0      lama\n         1       cow\n         2      lama", "y": 0, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\nA wrapper class for Spark Column to behave similar to pandas Series.\n\"\"\"\nimport re\nimport inspect\nfrom collections import Iterable\nfrom functools import partial, wraps, reduce\nfrom typing import Any, Generic, List, Optional, Tuple, TypeVar, Union\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.core.accessor import CachedAccessor\nfrom pandas.io.formats.printing import pprint_thing\n\nfrom databricks.koalas.typedef import as_python_type\nfrom pyspark import sql as spark\nfrom pyspark.sql import functions as F, Column\nfrom pyspark.sql.types import BooleanType, StructType\nfrom pyspark.sql.window import Window\n\nfrom databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.\nfrom databricks.koalas.config import get_option\nfrom databricks.koalas.base import IndexOpsMixin\nfrom databricks.koalas.frame import DataFrame\nfrom databricks.koalas.generic import _Frame\nfrom databricks.koalas.internal import IndexMap, _InternalFrame, SPARK_INDEX_NAME_FORMAT\nfrom databricks.koalas.missing.series import _MissingPandasLikeSeries\nfrom databricks.koalas.plot import KoalasSeriesPlotMethods\nfrom databricks.koalas.ml import corr\nfrom databricks.koalas.utils import (validate_arguments_and_invoke_function, scol_for,\n                                     combine_frames, name_like_string, validate_axis)\nfrom databricks.koalas.datetimes import DatetimeMethods\nfrom databricks.koalas.strings import StringMethods\n\n\n# This regular expression pattern is complied and defined here to avoid to compile the same\n# pattern every time it is used in _repr_ in Series.\n# This pattern basically seeks the footer string from Pandas'\nREPR_PATTERN = re.compile(r\"Length: (?P<length>[0-9]+)\")\n\n_flex_doc_SERIES = \"\"\"\nReturn {desc} of series and other, element-wise (binary operator `{op_name}`).\n\nEquivalent to ``{equiv}``\n\nParameters\n----------\nother : Series or scalar value\n\nReturns\n-------\nSeries\n    The result of the operation.\n\nSee Also\n--------\nSeries.{reverse}\n\n{series_examples}\n\"\"\"\n\n_add_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.add(df.b)\na    4.0\nb    NaN\nc    6.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.radd(df.b)\na    4.0\nb    NaN\nc    6.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_sub_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.subtract(df.b)\na    0.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rsub(df.b)\na    0.0\nb    NaN\nc   -2.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_mul_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.multiply(df.b)\na    4.0\nb    NaN\nc    8.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rmul(df.b)\na    4.0\nb    NaN\nc    8.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_div_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.divide(df.b)\na    1.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rdiv(df.b)\na    1.0\nb    NaN\nc    0.5\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_pow_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.pow(df.b)\na     4.0\nb     NaN\nc    16.0\nd     NaN\nName: a, dtype: float64\n\n>>> df.a.rpow(df.b)\na     4.0\nb     NaN\nc    16.0\nd     NaN\nName: a, dtype: float64\n\"\"\"\n\n_mod_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.mod(df.b)\na    0.0\nb    NaN\nc    0.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rmod(df.b)\na    0.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\n_floordiv_example_SERIES = \"\"\"\nExamples\n--------\n>>> df = ks.DataFrame({'a': [2, 2, 4, np.nan],\n...                    'b': [2, np.nan, 2, np.nan]},\n...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n>>> df\n     a    b\na  2.0  2.0\nb  2.0  NaN\nc  4.0  2.0\nd  NaN  NaN\n\n>>> df.a.floordiv(df.b)\na    1.0\nb    NaN\nc    2.0\nd    NaN\nName: a, dtype: float64\n\n>>> df.a.rfloordiv(df.b)\na    1.0\nb    NaN\nc    0.0\nd    NaN\nName: a, dtype: float64\n\"\"\"\n\nT = TypeVar(\"T\")\n\n# Needed to disambiguate Series.str and str type\nstr_type = str\n\n\nclass Series(_Frame, IndexOpsMixin, Generic[T]):\n    \"\"\"\n    Koalas Series that corresponds to Pandas Series logically. This holds Spark Column\n    internally.\n\n    :ivar _internal: an internal immutable Frame to manage metadata.\n    :type _internal: _InternalFrame\n    :ivar _kdf: Parent's Koalas DataFrame\n    :type _kdf: ks.DataFrame\n\n    Parameters\n    ----------\n    data : array-like, dict, or scalar value, Pandas Series\n        Contains data stored in Series\n        If data is a dict, argument order is maintained for Python 3.6\n        and later.\n        Note that if `data` is a Pandas Series, other arguments should not be used.\n    index : array-like or Index (1d)\n        Values must be hashable and have the same length as `data`.\n        Non-unique index values are allowed. Will default to\n        RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n        sequence are used, the index will override the keys found in the\n        dict.\n    dtype : numpy.dtype or None\n        If None, dtype will be inferred\n    copy : boolean, default False\n        Copy input data\n    \"\"\"\n\n    def __init__(self, data=None, index=None, dtype=None, name=None, copy=False, fastpath=False,\n                 anchor=None):\n        if isinstance(data, _InternalFrame):\n            assert dtype is None\n            assert name is None\n            assert not copy\n            assert not fastpath\n            IndexOpsMixin.__init__(self, data, anchor)\n        else:\n            assert anchor is None\n            if isinstance(data, pd.Series):\n                assert index is None\n                assert dtype is None\n                assert name is None\n                assert not copy\n                assert not fastpath\n                s = data\n            else:\n                s = pd.Series(\n                    data=data, index=index, dtype=dtype, name=name, copy=copy, fastpath=fastpath)\n            kdf = DataFrame(s)\n            IndexOpsMixin.__init__(self,\n                                   kdf._internal.copy(scol=kdf._internal.column_scols[0]), kdf)\n\n    def _with_new_scol(self, scol: spark.Column) -> 'Series':\n        \"\"\"\n        Copy Koalas Series with the new Spark Column.\n\n        :param scol: the new Spark Column\n        :return: the copied Series\n        \"\"\"\n        return Series(self._internal.copy(scol=scol), anchor=self._kdf)\n\n    @property\n    def dtypes(self):\n        \"\"\"Return the dtype object of the underlying data.\n\n        >>> s = ks.Series(list('abc'))\n        >>> s.dtype == s.dtypes\n        True\n        \"\"\"\n        return self.dtype\n\n    @property\n    def spark_type(self):\n        \"\"\" Returns the data type as defined by Spark, as a Spark DataType object.\"\"\"\n        return self._internal.spark_type_for(self._internal.column_index[0])\n\n    plot = CachedAccessor(\"plot\", KoalasSeriesPlotMethods)\n\n    # Arithmetic Operators\n    def add(self, other):\n        return (self + other).rename(self.name)\n\n    add.__doc__ = _flex_doc_SERIES.format(\n        desc='Addition',\n        op_name=\"+\",\n        equiv=\"series + other\",\n        reverse='radd',\n        series_examples=_add_example_SERIES)\n\n    def radd(self, other):\n        return (other + self).rename(self.name)\n\n    radd.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Addition',\n        op_name=\"+\",\n        equiv=\"other + series\",\n        reverse='add',\n        series_examples=_add_example_SERIES)\n\n    def div(self, other):\n        return (self / other).rename(self.name)\n\n    div.__doc__ = _flex_doc_SERIES.format(\n        desc='Floating division',\n        op_name=\"/\",\n        equiv=\"series / other\",\n        reverse='rdiv',\n        series_examples=_div_example_SERIES)\n\n    divide = div\n\n    def rdiv(self, other):\n        return (other / self).rename(self.name)\n\n    rdiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Floating division',\n        op_name=\"/\",\n        equiv=\"other / series\",\n        reverse='div',\n        series_examples=_div_example_SERIES)\n\n    def truediv(self, other):\n        return (self / other).rename(self.name)\n\n    truediv.__doc__ = _flex_doc_SERIES.format(\n        desc='Floating division',\n        op_name=\"/\",\n        equiv=\"series / other\",\n        reverse='rtruediv',\n        series_examples=_div_example_SERIES)\n\n    def rtruediv(self, other):\n        return (other / self).rename(self.name)\n\n    rtruediv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Floating division',\n        op_name=\"/\",\n        equiv=\"other / series\",\n        reverse='truediv',\n        series_examples=_div_example_SERIES)\n\n    def mul(self, other):\n        return (self * other).rename(self.name)\n\n    mul.__doc__ = _flex_doc_SERIES.format(\n        desc='Multiplication',\n        op_name=\"*\",\n        equiv=\"series * other\",\n        reverse='rmul',\n        series_examples=_mul_example_SERIES)\n\n    multiply = mul\n\n    def rmul(self, other):\n        return (other * self).rename(self.name)\n\n    rmul.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Multiplication',\n        op_name=\"*\",\n        equiv=\"other * series\",\n        reverse='mul',\n        series_examples=_mul_example_SERIES)\n\n    def sub(self, other):\n        return (self - other).rename(self.name)\n\n    sub.__doc__ = _flex_doc_SERIES.format(\n        desc='Subtraction',\n        op_name=\"-\",\n        equiv=\"series - other\",\n        reverse='rsub',\n        series_examples=_sub_example_SERIES)\n\n    subtract = sub\n\n    def rsub(self, other):\n        return (other - self).rename(self.name)\n\n    rsub.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Subtraction',\n        op_name=\"-\",\n        equiv=\"other - series\",\n        reverse='sub',\n        series_examples=_sub_example_SERIES)\n\n    def mod(self, other):\n        return (self % other).rename(self.name)\n\n    mod.__doc__ = _flex_doc_SERIES.format(\n        desc='Modulo',\n        op_name='%',\n        equiv='series % other',\n        reverse='rmod',\n        series_examples=_mod_example_SERIES)\n\n    def rmod(self, other):\n        return (other % self).rename(self.name)\n\n    rmod.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Modulo',\n        op_name='%',\n        equiv='other % series',\n        reverse='mod',\n        series_examples=_mod_example_SERIES)\n\n    def pow(self, other):\n        return (self ** other).rename(self.name)\n\n    pow.__doc__ = _flex_doc_SERIES.format(\n        desc='Exponential power of series',\n        op_name='**',\n        equiv='series ** other',\n        reverse='rpow',\n        series_examples=_pow_example_SERIES)\n\n    def rpow(self, other):\n        return (other ** self).rename(self.name)\n\n    rpow.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Exponential power',\n        op_name='**',\n        equiv='other ** series',\n        reverse='pow',\n        series_examples=_pow_example_SERIES)\n\n    def floordiv(self, other):\n        return (self // other).rename(self.name)\n\n    floordiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Integer division',\n        op_name='//',\n        equiv='series // other',\n        reverse='rfloordiv',\n        series_examples=_floordiv_example_SERIES)\n\n    def rfloordiv(self, other):\n        return (other // self).rename(self.name)\n\n    rfloordiv.__doc__ = _flex_doc_SERIES.format(\n        desc='Reverse Integer division',\n        op_name='//',\n        equiv='other // series',\n        reverse='floordiv',\n        series_examples=_floordiv_example_SERIES)\n\n    # Comparison Operators\n    def eq(self, other):\n        \"\"\"\n        Compare if the current value is equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a == 1\n        a     True\n        b    False\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.eq(1)\n        a     True\n        b    False\n        c     True\n        d    False\n        Name: b, dtype: bool\n        \"\"\"\n        return (self == other).rename(self.name)\n\n    equals = eq\n\n    def gt(self, other):\n        \"\"\"\n        Compare if the current value is greater than the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a > 1\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.gt(1)\n        a    False\n        b    False\n        c    False\n        d    False\n        Name: b, dtype: bool\n        \"\"\"\n        return (self > other).rename(self.name)\n\n    def ge(self, other):\n        \"\"\"\n        Compare if the current value is greater than or equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a >= 2\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.ge(2)\n        a    False\n        b    False\n        c    False\n        d    False\n        Name: b, dtype: bool\n        \"\"\"\n        return (self >= other).rename(self.name)\n\n    def lt(self, other):\n        \"\"\"\n        Compare if the current value is less than the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a < 1\n        a    False\n        b    False\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.lt(2)\n        a     True\n        b    False\n        c     True\n        d    False\n        Name: b, dtype: bool\n        \"\"\"\n        return (self < other).rename(self.name)\n\n    def le(self, other):\n        \"\"\"\n        Compare if the current value is less than or equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a <= 2\n        a     True\n        b     True\n        c    False\n        d    False\n        Name: a, dtype: bool\n\n        >>> df.b.le(2)\n        a     True\n        b    False\n        c     True\n        d    False\n        Name: b, dtype: bool\n        \"\"\"\n        return (self <= other).rename(self.name)\n\n    def ne(self, other):\n        \"\"\"\n        Compare if the current value is not equal to the other.\n\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4],\n        ...                    'b': [1, np.nan, 1, np.nan]},\n        ...                   index=['a', 'b', 'c', 'd'], columns=['a', 'b'])\n\n        >>> df.a != 1\n        a    False\n        b     True\n        c     True\n        d     True\n        Name: a, dtype: bool\n\n        >>> df.b.ne(1)\n        a    False\n        b     True\n        c    False\n        d     True\n        Name: b, dtype: bool\n        \"\"\"\n        return (self != other).rename(self.name)\n\n    def between(self, left, right, inclusive=True):\n        \"\"\"\n        Return boolean Series equivalent to left <= series <= right.\n        This function returns a boolean vector containing `True` wherever the\n        corresponding Series element is between the boundary values `left` and\n        `right`. NA values are treated as `False`.\n\n        Parameters\n        ----------\n        left : scalar or list-like\n            Left boundary.\n        right : scalar or list-like\n            Right boundary.\n        inclusive : bool, default True\n            Include boundaries.\n\n        Returns\n        -------\n        Series\n            Series representing whether each element is between left and\n            right (inclusive).\n\n        See Also\n        --------\n        Series.gt : Greater than of series and other.\n        Series.lt : Less than of series and other.\n\n        Notes\n        -----\n        This function is equivalent to ``(left <= ser) & (ser <= right)``\n\n        Examples\n        --------\n        >>> s = ks.Series([2, 0, 4, 8, np.nan])\n\n        Boundary values are included by default:\n\n        >>> s.between(1, 4)\n        0     True\n        1    False\n        2     True\n        3    False\n        4    False\n        Name: 0, dtype: bool\n\n        With `inclusive` set to ``False`` boundary values are excluded:\n\n        >>> s.between(1, 4, inclusive=False)\n        0     True\n        1    False\n        2    False\n        3    False\n        4    False\n        Name: 0, dtype: bool\n\n        `left` and `right` can be any scalar value:\n\n        >>> s = ks.Series(['Alice', 'Bob', 'Carol', 'Eve'])\n        >>> s.between('Anna', 'Daniel')\n        0    False\n        1     True\n        2     True\n        3    False\n        Name: 0, dtype: bool\n        \"\"\"\n        if inclusive:\n            lmask = self >= left\n            rmask = self <= right\n        else:\n            lmask = self > left\n            rmask = self < right\n\n        return (lmask & rmask)\n\n    # TODO: arg should support Series\n    # TODO: NaN and None\n    def map(self, arg):\n        \"\"\"\n        Map values of Series according to input correspondence.\n\n        Used for substituting each value in a Series with another value,\n        that may be derived from a function, a ``dict``.\n\n        .. note:: make sure the size of the dictionary is not huge because it could\n            downgrade the performance or throw OutOfMemoryError due to a huge\n            expression within Spark. Consider the input as a functions as an\n            alternative instead in this case.\n\n        Parameters\n        ----------\n        arg : function or dict\n            Mapping correspondence.\n\n        Returns\n        -------\n        Series\n            Same index as caller.\n\n        See Also\n        --------\n        Series.apply : For applying more complex functions on a Series.\n        DataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\n        Notes\n        -----\n        When ``arg`` is a dictionary, values in Series that are not in the\n        dictionary (as keys) are converted to ``None``. However, if the\n        dictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\n        provides a method for default values), then this default is used\n        rather than ``None``.\n\n        Examples\n        --------\n        >>> s = ks.Series(['cat', 'dog', None, 'rabbit'])\n        >>> s\n        0       cat\n        1       dog\n        2      None\n        3    rabbit\n        Name: 0, dtype: object\n\n        ``map`` accepts a ``dict``. Values that are not found\n        in the ``dict`` are converted to ``None``, unless the dict has a default\n        value (e.g. ``defaultdict``):\n\n        >>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n        0    kitten\n        1     puppy\n        2      None\n        3      None\n        Name: 0, dtype: object\n\n        It also accepts a function:\n\n        >>> def format(x) -> str:\n        ...     return 'I am a {}'.format(x)\n\n        >>> s.map(format)\n        0       I am a cat\n        1       I am a dog\n        2      I am a None\n        3    I am a rabbit\n        Name: 0, dtype: object\n        \"\"\"\n        if isinstance(arg, dict):\n            is_start = True\n            # In case dictionary is empty.\n            current = F.when(F.lit(False), F.lit(None).cast(self.spark_type))\n\n            for to_replace, value in arg.items():\n                if is_start:\n                    current = F.when(self._scol == F.lit(to_replace), value)\n                    is_start = False\n                else:\n                    current = current.when(self._scol == F.lit(to_replace), value)\n\n            if hasattr(arg, \"__missing__\"):\n                tmp_val = arg[np._NoValue]\n                del arg[np._NoValue]  # Remove in case it's set in defaultdict.\n                current = current.otherwise(F.lit(tmp_val))\n            else:\n                current = current.otherwise(F.lit(None).cast(self.spark_type))\n            return self._with_new_scol(current).rename(self.name)\n        else:\n            return self.apply(arg)\n\n    def astype(self, dtype) -> 'Series':\n        \"\"\"\n        Cast a Koalas object to a specified dtype ``dtype``.\n\n        Parameters\n        ----------\n        dtype : data type\n            Use a numpy.dtype or Python type to cast entire pandas object to\n            the same type.\n\n        Returns\n        -------\n        casted : same type as caller\n\n        See Also\n        --------\n        to_datetime : Convert argument to datetime.\n\n        Examples\n        --------\n        >>> ser = ks.Series([1, 2], dtype='int32')\n        >>> ser\n        0    1\n        1    2\n        Name: 0, dtype: int32\n\n        >>> ser.astype('int64')\n        0    1\n        1    2\n        Name: 0, dtype: int64\n        \"\"\"\n        from databricks.koalas.typedef import as_spark_type\n        spark_type = as_spark_type(dtype)\n        if not spark_type:\n            raise ValueError(\"Type {} not understood\".format(dtype))\n        return self._with_new_scol(self._scol.cast(spark_type))\n\n    def getField(self, name):\n        if not isinstance(self.spark_type, StructType):\n            raise AttributeError(\"Not a struct: {}\".format(self.spark_type))\n        else:\n            fnames = self.spark_type.fieldNames()\n            if name not in fnames:\n                raise AttributeError(\n                    \"Field {} not found, possible values are {}\".format(name, \", \".join(fnames)))\n            return self._with_new_scol(self._scol.getField(name))\n\n    def alias(self, name):\n        \"\"\"An alias for :meth:`Series.rename`.\"\"\"\n        return self.rename(name)\n\n    @property\n    def shape(self):\n        \"\"\"Return a tuple of the shape of the underlying data.\"\"\"\n        return len(self),\n\n    @property\n    def name(self) -> Union[str, Tuple[str, ...]]:\n        \"\"\"Return name of the Series.\"\"\"\n        name = self._internal.column_index[0]\n        if name is not None and len(name) == 1:\n            return name[0]\n        else:\n            return name\n\n    @name.setter\n    def name(self, name: Union[str, Tuple[str, ...]]):\n        self.rename(name, inplace=True)\n\n    # TODO: Functionality and documentation should be matched. Currently, changing index labels\n    # taking dictionary and function to change index are not supported.\n    def rename(self, index: Union[str, Tuple[str, ...]] = None, **kwargs):\n        \"\"\"\n        Alter Series name.\n\n        Parameters\n        ----------\n        index : scalar\n            Scalar will alter the ``Series.name`` attribute.\n\n        inplace : bool, default False\n            Whether to return a new Series. If True then value of copy is\n            ignored.\n\n        Returns\n        -------\n        Series\n            Series with name altered.\n\n        Examples\n        --------\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        >>> s.rename(\"my_name\")  # scalar, changes Series.name\n        0    1\n        1    2\n        2    3\n        Name: my_name, dtype: int64\n        \"\"\"\n        if index is None:\n            scol = self._scol\n        else:\n            scol = self._scol.alias(str(index))\n        internal = self._internal.copy(\n            scol=scol,\n            column_index=[index if index is None or isinstance(index, tuple) else (index,)])\n        if kwargs.get('inplace', False):\n            self._internal = internal\n            return self\n        else:\n            return Series(internal, anchor=self._kdf)\n\n    @property\n    def index(self):\n        \"\"\"The index (axis labels) Column of the Series.\n\n        See Also\n        --------\n        Index\n        \"\"\"\n        return self._kdf.index\n\n    @property\n    def is_unique(self):\n        \"\"\"\n        Return boolean if values in the object are unique\n\n        Returns\n        -------\n        is_unique : boolean\n\n        >>> ks.Series([1, 2, 3]).is_unique\n        True\n        >>> ks.Series([1, 2, 2]).is_unique\n        False\n        >>> ks.Series([1, 2, 3, None]).is_unique\n        True\n        \"\"\"\n        scol = self._scol\n\n        # Here we check:\n        #   1. the distinct count without nulls and count without nulls for non-null values\n        #   2. count null values and see if null is a distinct value.\n        #\n        # This workaround is in order to calculate the distinct count including nulls in\n        # single pass. Note that COUNT(DISTINCT expr) in Spark is designed to ignore nulls.\n        return self._internal._sdf.select(\n            (F.count(scol) == F.countDistinct(scol)) &\n            (F.count(F.when(scol.isNull(), 1).otherwise(None)) <= 1)\n        ).collect()[0][0]\n\n    def reset_index(self, level=None, drop=False, name=None, inplace=False):\n        \"\"\"\n        Generate a new DataFrame or Series with the index reset.\n\n        This is useful when the index needs to be treated as a column,\n        or when the index is meaningless and needs to be reset\n        to the default before another operation.\n\n        Parameters\n        ----------\n        level : int, str, tuple, or list, default optional\n            For a Series with a MultiIndex, only remove the specified levels from the index.\n            Removes all levels by default.\n        drop : bool, default False\n            Just reset the index, without inserting it as a column in the new DataFrame.\n        name : object, optional\n            The name to use for the column containing the original Series values.\n            Uses self.name by default. This argument is ignored when drop is True.\n        inplace : bool, default False\n            Modify the Series in place (do not create a new object).\n\n        Returns\n        -------\n        Series or DataFrame\n            When `drop` is False (the default), a DataFrame is returned.\n            The newly created columns will come first in the DataFrame,\n            followed by the original Series values.\n            When `drop` is True, a `Series` is returned.\n            In either case, if ``inplace=True``, no value is returned.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4], name='foo',\n        ...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\n        Generate a DataFrame with default index.\n\n        >>> s.reset_index()\n          idx  foo\n        0   a    1\n        1   b    2\n        2   c    3\n        3   d    4\n\n        To specify the name of the new column use `name`.\n\n        >>> s.reset_index(name='values')\n          idx  values\n        0   a       1\n        1   b       2\n        2   c       3\n        3   d       4\n\n        To generate a new Series with the default set `drop` to True.\n\n        >>> s.reset_index(drop=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n\n        To update the Series in place, without generating a new one\n        set `inplace` to True. Note that it also requires ``drop=True``.\n\n        >>> s.reset_index(inplace=True, drop=True)\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: foo, dtype: int64\n        \"\"\"\n        if inplace and not drop:\n            raise TypeError('Cannot reset_index inplace on a Series to create a DataFrame')\n\n        if name is not None:\n            kdf = self.rename(name).to_dataframe()\n        else:\n            kdf = self.to_dataframe()\n        kdf = kdf.reset_index(level=level, drop=drop)\n        if drop:\n            kseries = _col(kdf)\n            if inplace:\n                self._internal = kseries._internal\n                self._kdf = kseries._kdf\n            else:\n                return kseries\n        else:\n            return kdf\n\n    def to_frame(self, name: Union[str, Tuple[str, ...]] = None) -> spark.DataFrame:\n        \"\"\"\n        Convert Series to DataFrame.\n\n        Parameters\n        ----------\n        name : object, default None\n            The passed name should substitute for the series name (if it has\n            one).\n\n        Returns\n        -------\n        DataFrame\n            DataFrame representation of Series.\n\n        Examples\n        --------\n        >>> s = ks.Series([\"a\", \"b\", \"c\"])\n        >>> s.to_frame()\n           0\n        0  a\n        1  b\n        2  c\n\n        >>> s = ks.Series([\"a\", \"b\", \"c\"], name=\"vals\")\n        >>> s.to_frame()\n          vals\n        0    a\n        1    b\n        2    c\n        \"\"\"\n        if name is not None:\n            renamed = self.rename(name)\n        else:\n            renamed = self\n        sdf = renamed._internal.spark_internal_df\n        column_index = None  # type: Optional[List[Tuple[str, ...]]]\n        if renamed._internal.column_index[0] is None:\n            column_index = [('0',)]\n            column_index_names = None\n        else:\n            column_index = renamed._internal.column_index\n            column_index_names = renamed._internal.column_index_names\n        internal = _InternalFrame(sdf=sdf,\n                                  index_map=renamed._internal.index_map,\n                                  column_index=column_index,\n                                  column_scols=[scol_for(sdf, sdf.columns[-1])],\n                                  column_index_names=column_index_names)\n        return DataFrame(internal)\n\n    to_dataframe = to_frame\n\n    def to_string(self, buf=None, na_rep='NaN', float_format=None, header=True,\n                  index=True, length=False, dtype=False, name=False,\n                  max_rows=None):\n        \"\"\"\n        Render a string representation of the Series.\n\n        .. note:: This method should only be used if the resulting Pandas object is expected\n                  to be small, as all the data is loaded into the driver's memory. If the input\n                  is large, set max_rows parameter.\n\n        Parameters\n        ----------\n        buf : StringIO-like, optional\n            buffer to write to\n        na_rep : string, optional\n            string representation of NAN to use, default 'NaN'\n        float_format : one-parameter function, optional\n            formatter function to apply to columns' elements if they are floats\n            default None\n        header : boolean, default True\n            Add the Series header (index name)\n        index : bool, optional\n            Add index (row) labels, default True\n        length : boolean, default False\n            Add the Series length\n        dtype : boolean, default False\n            Add the Series dtype\n        name : boolean, default False\n            Add the Series name if not None\n        max_rows : int, optional\n            Maximum number of rows to show before truncating. If None, show\n            all.\n\n        Returns\n        -------\n        formatted : string (if not buffer passed)\n\n        Examples\n        --------\n        >>> df = ks.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], columns=['dogs', 'cats'])\n        >>> print(df['dogs'].to_string())\n        0    0.2\n        1    0.0\n        2    0.6\n        3    0.2\n\n        >>> print(df['dogs'].to_string(max_rows=2))\n        0    0.2\n        1    0.0\n        \"\"\"\n        # Make sure locals() call is at the top of the function so we don't capture local variables.\n        args = locals()\n        if max_rows is not None:\n            kseries = self.head(max_rows)\n        else:\n            kseries = self\n\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_string, pd.Series.to_string, args)\n\n    def to_clipboard(self, excel=True, sep=None, **kwargs):\n        # Docstring defined below by reusing DataFrame.to_clipboard's.\n        args = locals()\n        kseries = self\n\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_clipboard, pd.Series.to_clipboard, args)\n\n    to_clipboard.__doc__ = DataFrame.to_clipboard.__doc__\n\n    def to_dict(self, into=dict):\n        \"\"\"\n        Convert Series to {label -> value} dict or dict-like object.\n\n        .. note:: This method should only be used if the resulting Pandas DataFrame is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        Parameters\n        ----------\n        into : class, default dict\n            The collections.abc.Mapping subclass to use as the return\n            object. Can be the actual class or an empty\n            instance of the mapping type you want.  If you want a\n            collections.defaultdict, you must pass it initialized.\n\n        Returns\n        -------\n        collections.abc.Mapping\n            Key-value representation of Series.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s_dict = s.to_dict()\n        >>> sorted(s_dict.items())\n        [(0, 1), (1, 2), (2, 3), (3, 4)]\n\n        >>> from collections import OrderedDict, defaultdict\n        >>> s.to_dict(OrderedDict)\n        OrderedDict([(0, 1), (1, 2), (2, 3), (3, 4)])\n\n        >>> dd = defaultdict(list)\n        >>> s.to_dict(dd)  # doctest: +ELLIPSIS\n        defaultdict(<class 'list'>, {...})\n        \"\"\"\n        # Make sure locals() call is at the top of the function so we don't capture local variables.\n        args = locals()\n        kseries = self\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_dict, pd.Series.to_dict, args)\n\n    def to_latex(self, buf=None, columns=None, col_space=None, header=True, index=True,\n                 na_rep='NaN', formatters=None, float_format=None, sparsify=None, index_names=True,\n                 bold_rows=False, column_format=None, longtable=None, escape=None, encoding=None,\n                 decimal='.', multicolumn=None, multicolumn_format=None, multirow=None):\n\n        args = locals()\n        kseries = self\n        return validate_arguments_and_invoke_function(\n            kseries._to_internal_pandas(), self.to_latex, pd.Series.to_latex, args)\n\n    to_latex.__doc__ = DataFrame.to_latex.__doc__\n\n    def to_pandas(self):\n        \"\"\"\n        Return a pandas Series.\n\n        .. note:: This method should only be used if the resulting Pandas object is expected\n                  to be small, as all the data is loaded into the driver's memory.\n\n        Examples\n        --------\n        >>> df = ks.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], columns=['dogs', 'cats'])\n        >>> df['dogs'].to_pandas()\n        0    0.2\n        1    0.0\n        2    0.6\n        3    0.2\n        Name: dogs, dtype: float64\n        \"\"\"\n        return _col(self._internal.pandas_df.copy())\n\n    # Alias to maintain backward compatibility with Spark\n    toPandas = to_pandas\n\n    def to_list(self):\n        \"\"\"\n        Return a list of the values.\n\n        These are each a scalar type, which is a Python scalar\n        (for str, int, float) or a pandas scalar\n        (for Timestamp/Timedelta/Interval/Period)\n\n        .. note:: This method should only be used if the resulting list is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        \"\"\"\n        return self._to_internal_pandas().to_list()\n\n    tolist = to_list\n\n    def drop_duplicates(self, inplace=False):\n        \"\"\"\n        Return koalas Series with duplicate values removed.\n\n        Parameters\n        ----------\n        inplace: bool, default False\n            If True, performs operation inpalce and returns None.\n\n        Returns\n        -------\n        Series\n            Series with deplicates dropped.\n\n        Examples\n        --------\n        Generate a Series with duplicated entries.\n        >>> s = ks.Series(['lama', 'cow', 'lama', 'beetle', 'lama', 'hippo'],\n        ...               name='animal')\n        >>> s\n        0      lama\n        1       cow\n        2      lama\n        3    beetle\n        4      lama\n        5     hippo\n        Name: animal, dtype: object\n\n        >>> s.drop_duplicates()\n        1       cow\n        0      lama\n        5     hippo\n        3    beetle\n        Name: animal, dtype: object\n        \"\"\"\n        kseries = _col(self.to_frame().drop_duplicates())\n\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def fillna(self, value=None, method=None, axis=None, inplace=False, limit=None):\n        \"\"\"Fill NA/NaN values.\n\n        .. note:: the current implementation of 'method' parameter in fillna uses Spark's Window\n            without specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        value : scalar, dict, Series\n            Value to use to fill holes. alternately a dict/Series of values\n            specifying which value to use for each column.\n            DataFrame is not supported.\n        method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n            Method to use for filling holes in reindexed Series pad / ffill: propagate last valid\n            observation forward to next valid backfill / bfill:\n            use NEXT valid observation to fill gap\n        axis : {0 or `index`}\n            1 and `columns` are not supported.\n        inplace : boolean, default False\n            Fill in place (do not create a new object)\n        limit : int, default None\n            If method is specified, this is the maximum number of consecutive NaN values to\n            forward/backward fill. In other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. If method is not specified,\n            this is the maximum number of entries along the entire axis where NaNs will be filled.\n            Must be greater than 0 if not None\n\n        Returns\n        -------\n        Series\n            Series with NA entries filled.\n\n        Examples\n        --------\n        >>> s = ks.Series([np.nan, 2, 3, 4, np.nan, 6], name='x')\n        >>> s\n        0    NaN\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        Name: x, dtype: float64\n\n        Replace all NaN elements with 0s.\n\n        >>> s.fillna(0)\n        0    0.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    0.0\n        5    6.0\n        Name: x, dtype: float64\n\n        We can also propagate non-null values forward or backward.\n\n        >>> s.fillna(method='ffill')\n        0    NaN\n        1    2.0\n        2    3.0\n        3    4.0\n        4    4.0\n        5    6.0\n        Name: x, dtype: float64\n\n        >>> s = ks.Series([np.nan, 'a', 'b', 'c', np.nan], name='x')\n        >>> s.fillna(method='ffill')\n        0    None\n        1       a\n        2       b\n        3       c\n        4       c\n        Name: x, dtype: object\n        \"\"\"\n        return self._fillna(value, method, axis, inplace, limit)\n\n    def _fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, part_cols=()):\n        axis = validate_axis(axis)\n        if axis != 0:\n            raise NotImplementedError(\"fillna currently only works for axis=0 or axis='index'\")\n        if (value is None) and (method is None):\n            raise ValueError(\"Must specify a fillna 'value' or 'method' parameter.\")\n        if (method is not None) and (method not in ['ffill', 'pad', 'backfill', 'bfill']):\n            raise ValueError(\"Expecting 'pad', 'ffill', 'backfill' or 'bfill'.\")\n        if self.isnull().sum() == 0:\n            if inplace:\n                self._internal = self._internal.copy()\n                self._kdf = self._kdf.copy()\n            else:\n                return self\n\n        column_name = self.name\n        scol = self._scol\n\n        if value is not None:\n            if not isinstance(value, (float, int, str, bool)):\n                raise TypeError(\"Unsupported type %s\" % type(value))\n            if limit is not None:\n                raise ValueError('limit parameter for value is not support now')\n            scol = F.when(scol.isNull(), value).otherwise(scol)\n        else:\n            if method in ['ffill', 'pad']:\n                func = F.last\n                end = (Window.currentRow - 1)\n                if limit is not None:\n                    begin = Window.currentRow - limit\n                else:\n                    begin = Window.unboundedPreceding\n            elif method in ['bfill', 'backfill']:\n                func = F.first\n                begin = Window.currentRow + 1\n                if limit is not None:\n                    end = Window.currentRow + limit\n                else:\n                    end = Window.unboundedFollowing\n\n            window = Window.partitionBy(*part_cols).orderBy(self._internal.index_scols)\\\n                .rowsBetween(begin, end)\n            scol = F.when(scol.isNull(), func(scol, True).over(window)).otherwise(scol)\n        kseries = self._with_new_scol(scol).rename(column_name)\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def dropna(self, axis=0, inplace=False, **kwargs):\n        \"\"\"\n        Return a new Series with missing values removed.\n\n        Parameters\n        ----------\n        axis : {0 or 'index'}, default 0\n            There is only one axis to drop values from.\n        inplace : bool, default False\n            If True, do operation inplace and return None.\n        **kwargs\n            Not in use.\n\n        Returns\n        -------\n        Series\n            Series with NA entries dropped from it.\n\n        Examples\n        --------\n        >>> ser = ks.Series([1., 2., np.nan])\n        >>> ser\n        0    1.0\n        1    2.0\n        2    NaN\n        Name: 0, dtype: float64\n\n        Drop NA values from a Series.\n\n        >>> ser.dropna()\n        0    1.0\n        1    2.0\n        Name: 0, dtype: float64\n\n        Keep the Series with valid entries in the same variable.\n\n        >>> ser.dropna(inplace=True)\n        >>> ser\n        0    1.0\n        1    2.0\n        Name: 0, dtype: float64\n        \"\"\"\n        # TODO: last two examples from Pandas produce different results.\n        kseries = _col(self.to_dataframe().dropna(axis=axis, inplace=False))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n        else:\n            return kseries\n\n    def clip(self, lower: Union[float, int] = None, upper: Union[float, int] = None) -> 'Series':\n        \"\"\"\n        Trim values at input threshold(s).\n\n        Assigns values outside boundary to boundary values.\n\n        Parameters\n        ----------\n        lower : float or int, default None\n            Minimum threshold value. All values below this threshold will be set to it.\n        upper : float or int, default None\n            Maximum threshold value. All values above this threshold will be set to it.\n\n        Returns\n        -------\n        Series\n            Series with the values outside the clip boundaries replaced\n\n        Examples\n        --------\n        >>> ks.Series([0, 2, 4]).clip(1, 3)\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        Notes\n        -----\n        One difference between this implementation and pandas is that running\n        `pd.Series(['a', 'b']).clip(0, 1)` will crash with \"TypeError: '<=' not supported between\n        instances of 'str' and 'int'\" while `ks.Series(['a', 'b']).clip(0, 1)` will output the\n        original Series, simply ignoring the incompatible types.\n        \"\"\"\n        return _col(self.to_dataframe().clip(lower, upper))\n\n    def drop(self,\n             labels=None,\n             index: Union[str, Tuple[str, ...], List[str], List[Tuple[str, ...]]] = None,\n             level=None):\n        \"\"\"\n        Return Series with specified index labels removed.\n\n        Remove elements of a Series based on specifying the index labels.\n        When using a multi-index, labels on different levels can be removed by specifying the level.\n\n        Parameters\n        ----------\n        labels : single label or list-like\n            Index labels to drop.\n        index : None\n            Redundant for application on Series, but index can be used instead of labels.\n        level : int or level name, optional\n            For MultiIndex, level for which the labels will be removed.\n\n        Returns\n        -------\n        Series\n            Series with specified index labels removed.\n\n        See Also\n        --------\n        Series.dropna\n\n        Examples\n        --------\n        >>> s = ks.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A    0\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        Drop single label A\n\n        >>> s.drop('A')\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        Drop labels B and C\n\n        >>> s.drop(labels=['B', 'C'])\n        A    0\n        Name: 0, dtype: int64\n\n        With 'index' rather than 'labels' returns exactly same result.\n\n        >>> s.drop(index='A')\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        >>> s.drop(index=['B', 'C'])\n        A    0\n        Name: 0, dtype: int64\n\n        Also support for MultiIndex\n\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop(labels='weight', level=1)\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop(('lama', 'weight'))\n        lama    speed      45.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.drop([('lama', 'speed'), ('falcon', 'weight')])\n        lama    weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                length      0.3\n        Name: 0, dtype: float64\n        \"\"\"\n        level_param = level\n        if labels is not None:\n            if index is not None:\n                raise ValueError(\"Cannot specify both 'labels' and 'index'\")\n            return self.drop(index=labels, level=level)\n        if index is not None:\n            if not isinstance(index, (str, tuple, list)):\n                raise ValueError(\"'index' type should be one of str, list, tuple\")\n            if level is None:\n                level = 0\n            if level >= len(self._internal.index_scols):\n                raise ValueError(\"'level' should be less than the number of indexes\")\n\n            if isinstance(index, str):\n                index = [(index,)]  # type: ignore\n            elif isinstance(index, tuple):\n                index = [index]\n            else:\n                if not (all((isinstance(idxes, str) for idxes in index)) or\n                        all((isinstance(idxes, tuple) for idxes in index))):\n                    raise ValueError(\"If the given index is a list, it \"\n                                     \"should only contains names as strings, \"\n                                     \"or a list of tuples that contain \"\n                                     \"index names as strings\")\n                index = [idxes if isinstance(idxes, tuple) else (idxes,)  # type: ignore\n                         for idxes in index]\n\n            drop_index_scols = []\n            for idxes in index:\n                try:\n                    index_scols = [self._internal.index_scols[lvl] == idx\n                                   for lvl, idx in enumerate(idxes, level)]\n                except IndexError:\n                    if level_param is None:\n                        raise KeyError(\"Key length ({}) exceeds index depth ({})\"\n                                       .format(len(self._internal.index_scols), len(idxes)))\n                    else:\n                        return self\n                drop_index_scols.append(reduce(lambda x, y: x & y, index_scols))\n\n            sdf = self._internal.sdf.where(~reduce(lambda x, y: x | y, drop_index_scols))\n            return _col(DataFrame(self._internal.copy(sdf=sdf)))\n        else:\n            raise ValueError(\"Need to specify at least one of 'labels' or 'index'\")\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first n rows.\n\n        This function returns the first n rows for the object based on position.\n        It is useful for quickly testing if your object has the right type of data in it.\n\n        Parameters\n        ----------\n        n : Integer, default =  5\n\n        Returns\n        -------\n        The first n rows of the caller object.\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'animal':['alligator', 'bee', 'falcon', 'lion']})\n        >>> df.animal.head(2)  # doctest: +NORMALIZE_WHITESPACE\n        0     alligator\n        1     bee\n        Name: animal, dtype: object\n        \"\"\"\n        return _col(self.to_dataframe().head(n))\n\n    # TODO: Categorical type isn't supported (due to PySpark's limitation) and\n    # some doctests related with timestamps were not added.\n    def unique(self):\n        \"\"\"\n        Return unique values of Series object.\n\n        Uniques are returned in order of appearance. Hash table-based unique,\n        therefore does NOT sort.\n\n        .. note:: This method returns newly creased Series whereas Pandas returns\n                  the unique values as a NumPy array.\n\n        Returns\n        -------\n        Returns the unique values as a Series.\n\n        See Examples section.\n\n        Examples\n        --------\n        >>> kser = ks.Series([2, 1, 3, 3], name='A')\n        >>> kser.unique()\n        0    1\n        1    3\n        2    2\n        Name: A, dtype: int64\n\n        >>> ks.Series([pd.Timestamp('2016-01-01') for _ in range(3)]).unique()\n        0   2016-01-01\n        Name: 0, dtype: datetime64[ns]\n\n        >>> kser.name = ('x', 'a')\n        >>> kser.unique()\n        0    1\n        1    3\n        2    2\n        Name: (x, a), dtype: int64\n        \"\"\"\n        sdf = self._internal.sdf.select(self._scol).distinct()\n        internal = _InternalFrame(sdf=sdf,\n                                  column_index=[self._internal.column_index[0]],\n                                  column_scols=[scol_for(sdf, self._internal.data_columns[0])],\n                                  column_index_names=self._internal.column_index_names)\n        return _col(DataFrame(internal))\n\n    def nunique(self, dropna: bool = True, approx: bool = False, rsd: float = 0.05) -> int:\n        \"\"\"\n        Return number of unique elements in the object.\n\n        Excludes NA values by default.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don\u2019t include NaN in the count.\n        approx: bool, default False\n            If False, will use the exact algorithm and return the exact number of unique.\n            If True, it uses the HyperLogLog approximate algorithm, which is significantly faster\n            for large amount of data.\n            Note: This parameter is specific to Koalas and is not found in pandas.\n        rsd: float, default 0.05\n            Maximum estimation error allowed in the HyperLogLog algorithm.\n            Note: Just like ``approx`` this parameter is specific to Koalas.\n\n        Returns\n        -------\n        The number of unique values as an int.\n\n        Examples\n        --------\n        >>> ks.Series([1, 2, 3, np.nan]).nunique()\n        3\n\n        >>> ks.Series([1, 2, 3, np.nan]).nunique(dropna=False)\n        4\n\n        On big data, we recommend using the approximate algorithm to speed up this function.\n        The result will be very close to the exact unique count.\n\n        >>> ks.Series([1, 2, 3, np.nan]).nunique(approx=True)\n        3\n        \"\"\"\n        res = self._internal._sdf.select([self._nunique(dropna, approx, rsd)])\n        return res.collect()[0][0]\n\n    def _nunique(self, dropna=True, approx=False, rsd=0.05):\n        colname = self._internal.data_columns[0]\n        count_fn = partial(F.approx_count_distinct, rsd=rsd) if approx else F.countDistinct\n        if dropna:\n            return count_fn(self._scol).alias(colname)\n        else:\n            return (count_fn(self._scol) +\n                    F.when(F.count(F.when(self._scol.isNull(), 1)\n                                   .otherwise(None)) >= 1, 1).otherwise(0)).alias(colname)\n\n    def sort_values(self, ascending: bool = True, inplace: bool = False,\n                    na_position: str = 'last') -> Union['Series', None]:\n        \"\"\"\n        Sort by the values.\n\n        Sort a Series in ascending or descending order by some criterion.\n\n        Parameters\n        ----------\n        ascending : bool or list of bool, default True\n             Sort ascending vs. descending. Specify list for multiple sort\n             orders.  If this is a list of bools, must match the length of\n             the by.\n        inplace : bool, default False\n             if True, perform operation in-place\n        na_position : {'first', 'last'}, default 'last'\n             `first` puts NaNs at the beginning, `last` puts NaNs at the end\n\n        Returns\n        -------\n        sorted_obj : Series ordered by values.\n\n        Examples\n        --------\n        >>> s = ks.Series([np.nan, 1, 3, 10, 5])\n        >>> s\n        0     NaN\n        1     1.0\n        2     3.0\n        3    10.0\n        4     5.0\n        Name: 0, dtype: float64\n\n        Sort values ascending order (default behaviour)\n\n        >>> s.sort_values(ascending=True)\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values descending order\n\n        >>> s.sort_values(ascending=False)\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values inplace\n\n        >>> s.sort_values(ascending=False, inplace=True)\n        >>> s\n        3    10.0\n        4     5.0\n        2     3.0\n        1     1.0\n        0     NaN\n        Name: 0, dtype: float64\n\n        Sort values putting NAs first\n\n        >>> s.sort_values(na_position='first')\n        0     NaN\n        1     1.0\n        2     3.0\n        4     5.0\n        3    10.0\n        Name: 0, dtype: float64\n\n        Sort a series of strings\n\n        >>> s = ks.Series(['z', 'b', 'd', 'a', 'c'])\n        >>> s\n        0    z\n        1    b\n        2    d\n        3    a\n        4    c\n        Name: 0, dtype: object\n\n        >>> s.sort_values()\n        3    a\n        1    b\n        4    c\n        2    d\n        0    z\n        Name: 0, dtype: object\n        \"\"\"\n        kseries = _col(self.to_dataframe().sort_values(by=self.name, ascending=ascending,\n                                                       na_position=na_position))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n            return None\n        else:\n            return kseries\n\n    def sort_index(self, axis: int = 0,\n                   level: Optional[Union[int, List[int]]] = None, ascending: bool = True,\n                   inplace: bool = False, kind: str = None, na_position: str = 'last') \\\n            -> Optional['Series']:\n        \"\"\"\n        Sort object by labels (along an axis)\n\n        Parameters\n        ----------\n        axis : index, columns to direct sorting. Currently, only axis = 0 is supported.\n        level : int or level name or list of ints or list of level names\n            if not None, sort on values in specified index level(s)\n        ascending : boolean, default True\n            Sort ascending vs. descending\n        inplace : bool, default False\n            if True, perform operation in-place\n        kind : str, default None\n            Koalas does not allow specifying the sorting algorithm at the moment, default None\n        na_position : {\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\n            first puts NaNs at the beginning, last puts NaNs at the end. Not implemented for\n            MultiIndex.\n\n        Returns\n        -------\n        sorted_obj : Series\n\n        Examples\n        --------\n        >>> df = ks.Series([2, 1, np.nan], index=['b', 'a', np.nan])\n\n        >>> df.sort_index()\n        a      1.0\n        b      2.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(ascending=False)\n        b      2.0\n        a      1.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(na_position='first')\n        NaN    NaN\n        a      1.0\n        b      2.0\n        Name: 0, dtype: float64\n\n        >>> df.sort_index(inplace=True)\n        >>> df\n        a      1.0\n        b      2.0\n        NaN    NaN\n        Name: 0, dtype: float64\n\n        >>> df = ks.Series(range(4), index=[['b', 'b', 'a', 'a'], [1, 0, 1, 0]], name='0')\n\n        >>> df.sort_index()\n        a  0    3\n           1    2\n        b  0    1\n           1    0\n        Name: 0, dtype: int64\n\n        >>> df.sort_index(level=1)  # doctest: +SKIP\n        a  0    3\n        b  0    1\n        a  1    2\n        b  1    0\n        Name: 0, dtype: int64\n\n        >>> df.sort_index(level=[1, 0])\n        a  0    3\n        b  0    1\n        a  1    2\n        b  1    0\n        Name: 0, dtype: int64\n        \"\"\"\n        kseries = _col(self.to_dataframe().sort_index(axis=axis, level=level, ascending=ascending,\n                                                      kind=kind, na_position=na_position))\n        if inplace:\n            self._internal = kseries._internal\n            self._kdf = kseries._kdf\n            return None\n        else:\n            return kseries\n\n    def add_prefix(self, prefix):\n        \"\"\"\n        Prefix labels with string `prefix`.\n\n        For Series, the row labels are prefixed.\n        For DataFrame, the column labels are prefixed.\n\n        Parameters\n        ----------\n        prefix : str\n           The string to add before each label.\n\n        Returns\n        -------\n        Series\n           New Series with updated labels.\n\n        See Also\n        --------\n        Series.add_suffix: Suffix column labels with string `suffix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: 0, dtype: int64\n\n        >>> s.add_prefix('item_')\n        item_0    1\n        item_1    2\n        item_2    3\n        item_3    4\n        Name: 0, dtype: int64\n        \"\"\"\n        assert isinstance(prefix, str)\n        kdf = self.to_dataframe()\n        internal = kdf._internal\n        sdf = internal.sdf\n        sdf = sdf.select([F.concat(F.lit(prefix),\n                                   scol_for(sdf, index_column)).alias(index_column)\n                          for index_column in internal.index_columns] + internal.column_scols)\n        kdf._internal = internal.copy(sdf=sdf)\n        return _col(kdf)\n\n    def add_suffix(self, suffix):\n        \"\"\"\n        Suffix labels with string suffix.\n\n        For Series, the row labels are suffixed.\n        For DataFrame, the column labels are suffixed.\n\n        Parameters\n        ----------\n        suffix : str\n           The string to add after each label.\n\n        Returns\n        -------\n        Series\n           New Series with updated labels.\n\n        See Also\n        --------\n        Series.add_prefix: Prefix row labels with string `prefix`.\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s\n        0    1\n        1    2\n        2    3\n        3    4\n        Name: 0, dtype: int64\n\n        >>> s.add_suffix('_item')\n        0_item    1\n        1_item    2\n        2_item    3\n        3_item    4\n        Name: 0, dtype: int64\n        \"\"\"\n        assert isinstance(suffix, str)\n        kdf = self.to_dataframe()\n        internal = kdf._internal\n        sdf = internal.sdf\n        sdf = sdf.select([F.concat(scol_for(sdf, index_column),\n                                   F.lit(suffix)).alias(index_column)\n                          for index_column in internal.index_columns] + internal.column_scols)\n        kdf._internal = internal.copy(sdf=sdf)\n        return _col(kdf)\n\n    def corr(self, other, method='pearson'):\n        \"\"\"\n        Compute correlation with `other` Series, excluding missing values.\n\n        Parameters\n        ----------\n        other : Series\n        method : {'pearson', 'spearman'}\n            * pearson : standard correlation coefficient\n            * spearman : Spearman rank correlation\n\n        Returns\n        -------\n        correlation : float\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'s1': [.2, .0, .6, .2],\n        ...                    's2': [.3, .6, .0, .1]})\n        >>> s1 = df.s1\n        >>> s2 = df.s2\n        >>> s1.corr(s2, method='pearson')  # doctest: +ELLIPSIS\n        -0.851064...\n\n        >>> s1.corr(s2, method='spearman')  # doctest: +ELLIPSIS\n        -0.948683...\n\n        Notes\n        -----\n        There are behavior differences between Koalas and pandas.\n\n        * the `method` argument only accepts 'pearson', 'spearman'\n        * the data should not contain NaNs. Koalas will return an error.\n        * Koalas doesn't support the following argument(s).\n\n          * `min_periods` argument is not supported\n        \"\"\"\n        # This implementation is suboptimal because it computes more than necessary,\n        # but it should be a start\n        kdf = self._kdf.assign(corr_arg1=self, corr_arg2=other)[[\"corr_arg1\", \"corr_arg2\"]]\n        c = corr(kdf, method=method)\n        return c.loc[\"corr_arg1\", \"corr_arg2\"]\n\n    def nsmallest(self, n: int = 5) -> 'Series':\n        \"\"\"\n        Return the smallest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n            Return this many ascending sorted values.\n\n        Returns\n        -------\n        Series\n            The `n` smallest values in the Series, sorted in increasing order.\n\n        See Also\n        --------\n        Series.nlargest: Get the `n` largest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values().head(n)`` for small `n` relative to\n        the size of the ``Series`` object.\n        In Koalas, thanks to Spark's lazy execution and query optimizer,\n        the two would have same performance.\n\n        Examples\n        --------\n        >>> data = [1, 2, 3, 4, np.nan ,6, 7, 8]\n        >>> s = ks.Series(data)\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        6    7.0\n        7    8.0\n        Name: 0, dtype: float64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nsmallest()\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        5    6.0\n        Name: 0, dtype: float64\n\n        >>> s.nsmallest(3)\n        0    1.0\n        1    2.0\n        2    3.0\n        Name: 0, dtype: float64\n        \"\"\"\n        return _col(self.to_frame().nsmallest(n=n, columns=self.name))\n\n    def nlargest(self, n: int = 5) -> 'Series':\n        \"\"\"\n        Return the largest `n` elements.\n\n        Parameters\n        ----------\n        n : int, default 5\n\n        Returns\n        -------\n        Series\n            The `n` largest values in the Series, sorted in decreasing order.\n\n        See Also\n        --------\n        Series.nsmallest: Get the `n` smallest elements.\n        Series.sort_values: Sort Series by values.\n        Series.head: Return the first `n` rows.\n\n        Notes\n        -----\n        Faster than ``.sort_values(ascending=False).head(n)`` for small `n`\n        relative to the size of the ``Series`` object.\n\n        In Koalas, thanks to Spark's lazy execution and query optimizer,\n        the two would have same performance.\n\n        Examples\n        --------\n        >>> data = [1, 2, 3, 4, np.nan ,6, 7, 8]\n        >>> s = ks.Series(data)\n        >>> s\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        4    NaN\n        5    6.0\n        6    7.0\n        7    8.0\n        Name: 0, dtype: float64\n\n        The `n` largest elements where ``n=5`` by default.\n\n        >>> s.nlargest()\n        7    8.0\n        6    7.0\n        5    6.0\n        3    4.0\n        2    3.0\n        Name: 0, dtype: float64\n\n        >>> s.nlargest(n=3)\n        7    8.0\n        6    7.0\n        5    6.0\n        Name: 0, dtype: float64\n\n\n        \"\"\"\n        return _col(self.to_frame().nlargest(n=n, columns=self.name))\n\n    def count(self):\n        \"\"\"\n        Return number of non-NA/null observations in the Series.\n\n        Returns\n        -------\n        nobs : int\n\n        Examples\n        --------\n        Constructing DataFrame from a dictionary:\n\n        >>> df = ks.DataFrame({\"Person\":\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n        ...                    \"Age\": [24., np.nan, 21., 33, 26]})\n\n        Notice the uncounted NA values:\n\n        >>> df['Person'].count()\n        5\n\n        >>> df['Age'].count()\n        4\n        \"\"\"\n        return self._reduce_for_stat_function(_Frame._count_expr, name=\"count\")\n\n    def append(self, to_append: 'Series', ignore_index: bool = False,\n               verify_integrity: bool = False) -> 'Series':\n        \"\"\"\n        Concatenate two or more Series.\n\n        Parameters\n        ----------\n        to_append : Series or list/tuple of Series\n        ignore_index : boolean, default False\n            If True, do not use the index labels.\n        verify_integrity : boolean, default False\n            If True, raise Exception on creating index with duplicates\n\n        Returns\n        -------\n        appended : Series\n\n        Examples\n        --------\n        >>> s1 = ks.Series([1, 2, 3])\n        >>> s2 = ks.Series([4, 5, 6])\n        >>> s3 = ks.Series([4, 5, 6], index=[3,4,5])\n\n        >>> s1.append(s2)\n        0    1\n        1    2\n        2    3\n        0    4\n        1    5\n        2    6\n        Name: 0, dtype: int64\n\n        >>> s1.append(s3)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        Name: 0, dtype: int64\n\n        With ignore_index set to True:\n\n        >>> s1.append(s2, ignore_index=True)\n        0    1\n        1    2\n        2    3\n        3    4\n        4    5\n        5    6\n        Name: 0, dtype: int64\n        \"\"\"\n        return _col(self.to_dataframe().append(to_append.to_dataframe(), ignore_index,\n                                               verify_integrity))\n\n    def sample(self, n: Optional[int] = None, frac: Optional[float] = None, replace: bool = False,\n               random_state: Optional[int] = None) -> 'Series':\n        return _col(self.to_dataframe().sample(\n            n=n, frac=frac, replace=replace, random_state=random_state))\n\n    sample.__doc__ = DataFrame.sample.__doc__\n\n    def hist(self, bins=10, **kwds):\n        return self.plot.hist(bins, **kwds)\n\n    hist.__doc__ = KoalasSeriesPlotMethods.hist.__doc__\n\n    def apply(self, func, args=(), **kwds):\n        \"\"\"\n        Invoke function on values of Series.\n\n        Can be a Python function that only works on the Series.\n\n        .. note:: this API executes the function once to infer the type which is\n             potentially expensive, for instance, when the dataset is created after\n             aggregations or sorting.\n\n             To avoid this, specify return type in ``func``, for instance, as below:\n\n             >>> def square(x) -> np.int32:\n             ...     return x ** 2\n\n             Koalas uses return type hint and does not try to infer the type.\n\n        Parameters\n        ----------\n        func : function\n            Python function to apply. Note that type hint for return type is required.\n        args : tuple\n            Positional arguments passed to func after the series value.\n        **kwds\n            Additional keyword arguments passed to func.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        Create a Series with typical summer temperatures for each city.\n\n        >>> s = ks.Series([20, 21, 12],\n        ...               index=['London', 'New York', 'Helsinki'])\n        >>> s\n        London      20\n        New York    21\n        Helsinki    12\n        Name: 0, dtype: int64\n\n\n        Square the values by defining a function and passing it as an\n        argument to ``apply()``.\n\n        >>> def square(x) -> np.int64:\n        ...     return x ** 2\n        >>> s.apply(square)\n        London      400\n        New York    441\n        Helsinki    144\n        Name: 0, dtype: int64\n\n\n        Define a custom function that needs additional positional\n        arguments and pass these additional arguments using the\n        ``args`` keyword\n\n        >>> def subtract_custom_value(x, custom_value) -> np.int64:\n        ...     return x - custom_value\n\n        >>> s.apply(subtract_custom_value, args=(5,))\n        London      15\n        New York    16\n        Helsinki     7\n        Name: 0, dtype: int64\n\n\n        Define a custom function that takes keyword arguments\n        and pass these arguments to ``apply``\n\n        >>> def add_custom_values(x, **kwargs) -> np.int64:\n        ...     for month in kwargs:\n        ...         x += kwargs[month]\n        ...     return x\n\n        >>> s.apply(add_custom_values, june=30, july=20, august=25)\n        London      95\n        New York    96\n        Helsinki    87\n        Name: 0, dtype: int64\n\n\n        Use a function from the Numpy library\n\n        >>> def numpy_log(col) -> np.float64:\n        ...     return np.log(col)\n        >>> s.apply(numpy_log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        Name: 0, dtype: float64\n\n\n        You can omit the type hint and let Koalas infer its type.\n\n        >>> s.apply(np.log)\n        London      2.995732\n        New York    3.044522\n        Helsinki    2.484907\n        Name: 0, dtype: float64\n\n        \"\"\"\n        assert callable(func), \"the first argument should be a callable function.\"\n        try:\n            spec = inspect.getfullargspec(func)\n            return_sig = spec.annotations.get(\"return\", None)\n            should_infer_schema = return_sig is None\n        except TypeError:\n            # Falls back to schema inference if it fails to get signature.\n            should_infer_schema = True\n\n        apply_each = wraps(func)(lambda s, *a, **k: s.apply(func, args=a, **k))\n\n        if should_infer_schema:\n            # TODO: In this case, it avoids the shortcut for now (but only infers schema)\n            #  because it returns a series from a different DataFrame and it has a different\n            #  anchor. We should fix this to allow the shortcut or only allow to infer\n            #  schema.\n            limit = get_option(\"compute.shortcut_limit\")\n            pser = self.head(limit)._to_internal_pandas()\n            transformed = pser.apply(func, *args, **kwds)\n            kser = Series(transformed)\n\n            wrapped = ks.pandas_wraps(\n                return_col=as_python_type(kser.spark_type))(apply_each)\n        else:\n            wrapped = ks.pandas_wraps(return_col=return_sig)(apply_each)\n        return wrapped(self, *args, **kwds).rename(self.name)\n\n    # TODO: not all arguments are implemented comparing to Pandas' for now.\n    def aggregate(self, func: Union[str, List[str]]):\n        \"\"\"Aggregate using one or more operations over the specified axis.\n\n        Parameters\n        ----------\n        func : str or a list of str\n            function name(s) as string apply to series.\n\n        Returns\n        -------\n        scalar, Series\n            The return can be:\n            - scalar : when Series.agg is called with single function\n            - Series : when Series.agg is called with several functions\n\n        Notes\n        -----\n        `agg` is an alias for `aggregate`. Use the alias.\n\n        See Also\n        --------\n        databricks.koalas.Series.apply\n        databricks.koalas.Series.transform\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4])\n        >>> s.agg('min')\n        1\n\n        >>> s.agg(['min', 'max'])\n        max    4\n        min    1\n        Name: 0, dtype: int64\n        \"\"\"\n        if isinstance(func, list):\n            return self.to_frame().agg(func)[self.name]\n        elif isinstance(func, str):\n            return getattr(self, func)()\n        else:\n            raise ValueError(\"func must be a string or list of strings\")\n\n    agg = aggregate\n\n    def transpose(self, *args, **kwargs):\n        \"\"\"\n        Return the transpose, which is by definition self.\n\n        Examples\n        --------\n        It returns the same object as the transpose of the given series object, which is by\n        definition self.\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n\n        >>> s.transpose()\n        0    1\n        1    2\n        2    3\n        Name: 0, dtype: int64\n        \"\"\"\n        return Series(self._internal.copy(), anchor=self._kdf)\n\n    T = property(transpose)\n\n    def transform(self, func, *args, **kwargs):\n        \"\"\"\n        Call ``func`` producing the same type as `self` with transformed values\n        and that has the same axis length as input.\n\n        .. note:: this API executes the function once to infer the type which is\n             potentially expensive, for instance, when the dataset is created after\n             aggregations or sorting.\n\n             To avoid this, specify return type in ``func``, for instance, as below:\n\n             >>> def square(x) -> np.int32:\n             ...     return x ** 2\n\n             Koalas uses return type hint and does not try to infer the type.\n\n        Parameters\n        ----------\n        func : function or list\n            A function or a list of functions to use for transforming the data.\n        *args\n            Positional arguments to pass to `func`.\n        **kwargs\n            Keyword arguments to pass to `func`.\n\n        Returns\n        -------\n        An instance of the same type with `self` that must have the same length as input.\n\n        See Also\n        --------\n        Series.apply : Invoke function on Series.\n\n        Examples\n        --------\n\n        >>> s = ks.Series(range(3))\n        >>> s\n        0    0\n        1    1\n        2    2\n        Name: 0, dtype: int64\n\n        >>> def sqrt(x) -> float:\n        ...    return np.sqrt(x)\n        >>> s.transform(sqrt)\n        0    0.000000\n        1    1.000000\n        2    1.414214\n        Name: 0, dtype: float32\n\n        Even though the resulting instance must have the same length as the\n        input, it is possible to provide several input functions:\n\n        >>> def exp(x) -> float:\n        ...    return np.exp(x)\n        >>> s.transform([sqrt, exp])\n               sqrt       exp\n        0  0.000000  1.000000\n        1  1.000000  2.718282\n        2  1.414214  7.389056\n\n        You can omit the type hint and let Koalas infer its type.\n\n        >>> s.transform([np.sqrt, np.exp])\n               sqrt       exp\n        0  0.000000  1.000000\n        1  1.000000  2.718282\n        2  1.414214  7.389056\n        \"\"\"\n        if isinstance(func, list):\n            applied = []\n            for f in func:\n                applied.append(self.apply(f, args=args, **kwargs).rename(f.__name__))\n\n            sdf = self._internal._sdf.select(\n                self._internal.index_scols + [c._scol for c in applied])\n\n            internal = self.to_dataframe()._internal.copy(\n                sdf=sdf,\n                column_index=[c._internal.column_index[0] for c in applied],\n                column_scols=[scol_for(sdf, c._internal.data_columns[0]) for c in applied],\n                column_index_names=None)\n\n            return DataFrame(internal)\n        else:\n            return self.apply(func, args=args, **kwargs)\n\n    def round(self, decimals=0):\n        \"\"\"\n        Round each value in a Series to the given number of decimals.\n\n        Parameters\n        ----------\n        decimals : int\n            Number of decimal places to round to (default: 0).\n            If decimals is negative, it specifies the number of\n            positions to the left of the decimal point.\n\n        Returns\n        -------\n        Series object\n\n        See Also\n        --------\n        DataFrame.round\n\n        Examples\n        --------\n        >>> df = ks.Series([0.028208, 0.038683, 0.877076], name='x')\n        >>> df\n        0    0.028208\n        1    0.038683\n        2    0.877076\n        Name: x, dtype: float64\n\n        >>> df.round(2)\n        0    0.03\n        1    0.04\n        2    0.88\n        Name: x, dtype: float64\n        \"\"\"\n        if not isinstance(decimals, int):\n            raise ValueError(\"decimals must be an integer\")\n        column_name = self.name\n        scol = F.round(self._scol, decimals)\n        return self._with_new_scol(scol).rename(column_name)\n\n    # TODO: add 'interpolation' parameter.\n    def quantile(self, q=0.5, accuracy=10000):\n        \"\"\"\n        Return value at the given quantile.\n\n        .. note:: Unlike pandas', the quantile in Koalas is an approximated quantile based upon\n            approximate percentile computation because computing quantile across a large dataset\n            is extremely expensive.\n\n        Parameters\n        ----------\n        q : float or array-like, default 0.5 (50% quantile)\n            0 <= q <= 1, the quantile(s) to compute.\n        accuracy : int, optional\n            Default accuracy of approximation. Larger value means better accuracy.\n            The relative error can be deduced by 1.0 / accuracy.\n\n        Returns\n        -------\n        float or Series\n            If the current object is a Series and ``q`` is an array, a Series will be\n            returned where the index is ``q`` and the values are the quantiles, otherwise\n            a float will be returned.\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 3, 4, 5])\n        >>> s.quantile(.5)\n        3\n\n        >>> s.quantile([.25, .5, .75])\n        0.25    2\n        0.5     3\n        0.75    4\n        Name: 0, dtype: int64\n        \"\"\"\n        if not isinstance(accuracy, int):\n            raise ValueError(\"accuracy must be an integer; however, got [%s]\" % type(accuracy))\n\n        if isinstance(q, Iterable):\n            q = list(q)\n\n        for v in q if isinstance(q, list) else [q]:\n            if not isinstance(v, float):\n                raise ValueError(\n                    \"q must be a float of an array of floats; however, [%s] found.\" % type(v))\n            if v < 0.0 or v > 1.0:\n                raise ValueError(\n                    \"percentiles should all be in the interval [0, 1].\")\n\n        if isinstance(q, list):\n            quantiles = q\n            # TODO: avoid to use dataframe. After this, anchor will be lost.\n\n            # First calculate the percentiles and map it to each `quantiles`\n            # by creating each entry as a struct. So, it becomes an array of\n            # structs as below:\n            #\n            # +--------------------------------+\n            # | arrays                         |\n            # +--------------------------------+\n            # |[[0.25, 2], [0.5, 3], [0.75, 4]]|\n            # +--------------------------------+\n            sdf = self._internal._sdf\n            args = \", \".join(map(str, quantiles))\n            percentile_col = F.expr(\n                \"approx_percentile(`%s`, array(%s), %s)\" % (self.name, args, accuracy))\n            sdf = sdf.select(percentile_col.alias(\"percentiles\"))\n\n            internal_index_column = SPARK_INDEX_NAME_FORMAT(0)\n            value_column = \"value\"\n            cols = []\n            for i, quantile in enumerate(quantiles):\n                cols.append(F.struct(\n                    F.lit(\"%s\" % quantile).alias(internal_index_column),\n                    F.expr(\"percentiles[%s]\" % i).alias(value_column)))\n            sdf = sdf.select(F.array(*cols).alias(\"arrays\"))\n\n            # And then, explode it and manually set the index.\n            #\n            # +-----------------+-----+\n            # |__index_level_0__|value|\n            # +-----------------+-----+\n            # | 0.25            |    2|\n            # |  0.5            |    3|\n            # | 0.75            |    4|\n            # +-----------------+-----+\n            sdf = sdf.select(F.explode(F.col(\"arrays\"))).selectExpr(\"col.*\")\n\n            internal = _InternalFrame(\n                sdf=sdf,\n                index_map=[(internal_index_column, None)],\n                column_index=None,\n                column_scols=[scol_for(sdf, value_column)],\n                column_index_names=None)\n\n            return DataFrame(internal)[value_column].rename(self.name)\n        else:\n            return self._reduce_for_stat_function(\n                lambda _: F.expr(\"approx_percentile(`%s`, %s, %s)\" % (self.name, q, accuracy)),\n                name=\"median\")\n\n    # TODO: add axis, numeric_only, pct, na_option parameter\n    def rank(self, method='average', ascending=True):\n        \"\"\"\n        Compute numerical data ranks (1 through n) along axis. Equal values are\n        assigned a rank that is the average of the ranks of those values.\n\n        .. note:: the current implementation of rank uses Spark's Window without\n            specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        method : {'average', 'min', 'max', 'first', 'dense'}\n            * average: average rank of group\n            * min: lowest rank in group\n            * max: highest rank in group\n            * first: ranks assigned in order they appear in the array\n            * dense: like 'min', but rank always increases by 1 between groups\n        ascending : boolean, default True\n            False for ranks by high (1) to low (N)\n\n        Returns\n        -------\n        ranks : same type as caller\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2, 2, 3], name='A')\n        >>> s\n        0    1\n        1    2\n        2    2\n        3    3\n        Name: A, dtype: int64\n\n        >>> s.rank()\n        0    1.0\n        1    2.5\n        2    2.5\n        3    4.0\n        Name: A, dtype: float64\n\n        If method is set to 'min', it use lowest rank in group.\n\n        >>> s.rank(method='min')\n        0    1.0\n        1    2.0\n        2    2.0\n        3    4.0\n        Name: A, dtype: float64\n\n        If method is set to 'max', it use highest rank in group.\n\n        >>> s.rank(method='max')\n        0    1.0\n        1    3.0\n        2    3.0\n        3    4.0\n        Name: A, dtype: float64\n\n        If method is set to 'first', it is assigned rank in order without groups.\n\n        >>> s.rank(method='first')\n        0    1.0\n        1    2.0\n        2    3.0\n        3    4.0\n        Name: A, dtype: float64\n\n        If method is set to 'dense', it leaves no gaps in group.\n\n        >>> s.rank(method='dense')\n        0    1.0\n        1    2.0\n        2    2.0\n        3    3.0\n        Name: A, dtype: float64\n        \"\"\"\n        return self._rank(method, ascending)\n\n    def _rank(self, method='average', ascending=True, part_cols=()):\n        if method not in ['average', 'min', 'max', 'first', 'dense']:\n            msg = \"method must be one of 'average', 'min', 'max', 'first', 'dense'\"\n            raise ValueError(msg)\n\n        if len(self._internal.index_columns) > 1:\n            raise ValueError('rank do not support index now')\n\n        if ascending:\n            asc_func = spark.functions.asc\n        else:\n            asc_func = spark.functions.desc\n\n        index_column = self._internal.index_columns[0]\n        column_name = self._internal.data_columns[0]\n\n        if method == 'first':\n            window = Window.orderBy(\n                asc_func(column_name), asc_func(index_column)\n            ).partitionBy(*part_cols).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            scol = F.row_number().over(window)\n        elif method == 'dense':\n            window = Window.orderBy(asc_func(column_name)).partitionBy(*part_cols) \\\n                .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            scol = F.dense_rank().over(window)\n        else:\n            if method == 'average':\n                stat_func = F.mean\n            elif method == 'min':\n                stat_func = F.min\n            elif method == 'max':\n                stat_func = F.max\n            window1 = Window.orderBy(\n                asc_func(column_name)\n            ).partitionBy(*part_cols).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n            window2 = Window.partitionBy(\n                *[column_name] + list(part_cols)\n            ).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n            scol = stat_func(F.row_number().over(window1)).over(window2)\n        kser = self._with_new_scol(scol).rename(self.name)\n        return kser.astype(np.float64)\n\n    def describe(self, percentiles: Optional[List[float]] = None) -> 'Series':\n        return _col(self.to_dataframe().describe(percentiles))\n\n    describe.__doc__ = DataFrame.describe.__doc__\n\n    def diff(self, periods=1):\n        \"\"\"\n        First discrete difference of element.\n\n        Calculates the difference of a Series element compared with another element in the\n        DataFrame (default is the element in the same column of the previous row).\n\n        .. note:: the current implementation of diff uses Spark's Window without\n            specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for calculating difference, accepts negative values.\n\n        Returns\n        -------\n        diffed : DataFrame\n\n        Examples\n        --------\n        >>> df = ks.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n        ...                    'b': [1, 1, 2, 3, 5, 8],\n        ...                    'c': [1, 4, 9, 16, 25, 36]}, columns=['a', 'b', 'c'])\n        >>> df\n           a  b   c\n        0  1  1   1\n        1  2  1   4\n        2  3  2   9\n        3  4  3  16\n        4  5  5  25\n        5  6  8  36\n\n        >>> df.b.diff()\n        0    NaN\n        1    0.0\n        2    1.0\n        3    1.0\n        4    2.0\n        5    3.0\n        Name: b, dtype: float64\n\n        Difference with previous value\n\n        >>> df.c.diff(periods=3)\n        0     NaN\n        1     NaN\n        2     NaN\n        3    15.0\n        4    21.0\n        5    27.0\n        Name: c, dtype: float64\n\n        Difference with following value\n\n        >>> df.c.diff(periods=-1)\n        0    -3.0\n        1    -5.0\n        2    -7.0\n        3    -9.0\n        4   -11.0\n        5     NaN\n        Name: c, dtype: float64\n        \"\"\"\n        return self._diff(periods)\n\n    def _diff(self, periods, part_cols=()):\n        if not isinstance(periods, int):\n            raise ValueError('periods should be an int; however, got [%s]' % type(periods))\n        window = Window.partitionBy(*part_cols).orderBy(self._internal.index_scols)\\\n            .rowsBetween(-periods, -periods)\n        scol = self._scol - F.lag(self._scol, periods).over(window)\n        return self._with_new_scol(scol).rename(self.name)\n\n    def idxmax(self, skipna=True):\n        \"\"\"\n        Return the row label of the maximum value.\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Index\n            Label of the maximum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        Series.idxmin : Return index *label* of the first occurrence\n            of minimum of values.\n\n        Examples\n        --------\n        >>> s = ks.Series(data=[1, None, 4, 3, 5],\n        ...               index=['A', 'B', 'C', 'D', 'E'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    3.0\n        E    5.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmax()\n        'E'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmax(skipna=False)\n        nan\n\n        In case of multi-index, you get a tuple:\n\n        >>> index = pd.MultiIndex.from_arrays([\n        ...     ['a', 'a', 'b', 'b'], ['c', 'd', 'e', 'f']], names=('first', 'second'))\n        >>> s = ks.Series(data=[1, None, 4, 5], index=index)\n        >>> s\n        first  second\n        a      c         1.0\n               d         NaN\n        b      e         4.0\n               f         5.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmax()\n        ('b', 'f')\n\n        If multiple values equal the maximum, the first row label with that\n        value is returned.\n\n        >>> s = ks.Series([1, 100, 1, 100, 1, 100], index=[10, 3, 5, 2, 1, 8])\n        >>> s\n        10      1\n        3     100\n        5       1\n        2     100\n        1       1\n        8     100\n        Name: 0, dtype: int64\n\n        >>> s.idxmax()\n        3\n        \"\"\"\n        sdf = self._internal._sdf\n        scol = self._scol\n        index_scols = self._internal.index_scols\n        # desc_nulls_(last|first) is used via Py4J directly because\n        # it's not supported in Spark 2.3.\n        if skipna:\n            sdf = sdf.orderBy(Column(scol._jc.desc_nulls_last()), F.monotonically_increasing_id())\n        else:\n            sdf = sdf.orderBy(Column(scol._jc.desc_nulls_first()), F.monotonically_increasing_id())\n        results = sdf.select([scol] + index_scols).take(1)\n        if len(results) == 0:\n            raise ValueError(\"attempt to get idxmin of an empty sequence\")\n        if results[0][0] is None:\n            # This will only happens when skipna is False because we will\n            # place nulls first.\n            return np.nan\n        values = list(results[0][1:])\n        if len(values) == 1:\n            return values[0]\n        else:\n            return tuple(values)\n\n    def idxmin(self, skipna=True):\n        \"\"\"\n        Return the row label of the minimum value.\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        Parameters\n        ----------\n        skipna : bool, default True\n            Exclude NA/null values. If the entire Series is NA, the result\n            will be NA.\n\n        Returns\n        -------\n        Index\n            Label of the minimum value.\n\n        Raises\n        ------\n        ValueError\n            If the Series is empty.\n\n        See Also\n        --------\n        Series.idxmax : Return index *label* of the first occurrence\n            of maximum of values.\n\n        Notes\n        -----\n        This method is the Series version of ``ndarray.argmin``. This method\n        returns the label of the minimum, while ``ndarray.argmin`` returns\n        the position. To get the position, use ``series.values.argmin()``.\n\n        Examples\n        --------\n        >>> s = ks.Series(data=[1, None, 4, 0],\n        ...               index=['A', 'B', 'C', 'D'])\n        >>> s\n        A    1.0\n        B    NaN\n        C    4.0\n        D    0.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmin()\n        'D'\n\n        If `skipna` is False and there is an NA value in the data,\n        the function returns ``nan``.\n\n        >>> s.idxmin(skipna=False)\n        nan\n\n        In case of multi-index, you get a tuple:\n\n        >>> index = pd.MultiIndex.from_arrays([\n        ...     ['a', 'a', 'b', 'b'], ['c', 'd', 'e', 'f']], names=('first', 'second'))\n        >>> s = ks.Series(data=[1, None, 4, 0], index=index)\n        >>> s\n        first  second\n        a      c         1.0\n               d         NaN\n        b      e         4.0\n               f         0.0\n        Name: 0, dtype: float64\n\n        >>> s.idxmin()\n        ('b', 'f')\n\n        If multiple values equal the minimum, the first row label with that\n        value is returned.\n\n        >>> s = ks.Series([1, 100, 1, 100, 1, 100], index=[10, 3, 5, 2, 1, 8])\n        >>> s\n        10      1\n        3     100\n        5       1\n        2     100\n        1       1\n        8     100\n        Name: 0, dtype: int64\n\n        >>> s.idxmin()\n        10\n        \"\"\"\n        sdf = self._internal._sdf\n        scol = self._scol\n        index_scols = self._internal.index_scols\n        # asc_nulls_(last|first)is used via Py4J directly because\n        # it's not supported in Spark 2.3.\n        if skipna:\n            sdf = sdf.orderBy(Column(scol._jc.asc_nulls_last()), F.monotonically_increasing_id())\n        else:\n            sdf = sdf.orderBy(Column(scol._jc.asc_nulls_first()), F.monotonically_increasing_id())\n        results = sdf.select([scol] + index_scols).take(1)\n        if len(results) == 0:\n            raise ValueError(\"attempt to get idxmin of an empty sequence\")\n        if results[0][0] is None:\n            # This will only happens when skipna is False because we will\n            # place nulls first.\n            return np.nan\n        values = list(results[0][1:])\n        if len(values) == 1:\n            return values[0]\n        else:\n            return tuple(values)\n\n    def pop(self, item):\n        \"\"\"\n        Return item and drop from sereis.\n\n        Parameters\n        ----------\n        item : str\n            Label of index to be popped.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n        >>> s = ks.Series(data=np.arange(3), index=['A', 'B', 'C'])\n        >>> s\n        A    0\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        >>> s.pop('A')\n        0\n\n        >>> s\n        B    1\n        C    2\n        Name: 0, dtype: int64\n\n        >>> s = ks.Series(data=np.arange(3), index=['A', 'A', 'C'])\n        >>> s\n        A    0\n        A    1\n        C    2\n        Name: 0, dtype: int64\n\n        >>> s.pop('A')\n        A    0\n        A    1\n        Name: 0, dtype: int64\n\n        >>> s\n        C    2\n        Name: 0, dtype: int64\n\n        Also support for MultiIndex\n\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.pop('lama')\n        speed      45.0\n        weight    200.0\n        length      1.2\n        Name: 0, dtype: float64\n\n        >>> s\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        Also support for MultiIndex with several indexs.\n\n        >>> midx = pd.MultiIndex([['a', 'b', 'c'],\n        ...                       ['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 0, 0, 0, 1, 1, 1],\n        ...                       [0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 0, 2]]\n        ...  )\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...              index=midx)\n        >>> s\n        a  lama    speed      45.0\n                   weight    200.0\n                   length      1.2\n           cow     speed      30.0\n                   weight    250.0\n                   length      1.5\n        b  falcon  speed     320.0\n                   speed       1.0\n                   length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.pop(('a', 'lama'))\n        speed      45.0\n        weight    200.0\n        length      1.2\n        Name: 0, dtype: float64\n\n        >>> s\n        a  cow     speed      30.0\n                   weight    250.0\n                   length      1.5\n        b  falcon  speed     320.0\n                   speed       1.0\n                   length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.pop(('b', 'falcon', 'speed'))\n        (b, falcon, speed)    320.0\n        (b, falcon, speed)      1.0\n        Name: 0, dtype: float64\n        \"\"\"\n        if not isinstance(item, (str, tuple)):\n            raise ValueError(\"'key' should be string or tuple that contains strings\")\n        if isinstance(item, str):\n            item = (item,)\n        if not all(isinstance(index, str) for index in item):\n            raise ValueError(\"'key' should have index names as only strings \"\n                             \"or a tuple that contain index names as only strings\")\n        if len(self._internal._index_map) < len(item):\n            raise KeyError(\"Key length ({}) exceeds index depth ({})\"\n                           .format(len(item), len(self._internal.index_map)))\n\n        cols = (self._internal.index_scols[len(item):] +\n                [self._internal.scol_for(self._internal.column_index[0])])\n        rows = [self._internal.scols[level] == index\n                for level, index in enumerate(item)]\n        sdf = self._internal.sdf \\\n            .select(cols) \\\n            .where(reduce(lambda x, y: x & y, rows))\n\n        if len(self._internal._index_map) == len(item):\n            # if sdf has one column and one data, return data only without frame\n            pdf = sdf.limit(2).toPandas()\n            length = len(pdf)\n            if length == 1:\n                self._internal = self.drop(item)._internal\n                return pdf[self.name].iloc[0]\n\n            self._internal = self.drop(item)._internal\n            item_string = name_like_string(item)\n            sdf = sdf.withColumn(SPARK_INDEX_NAME_FORMAT(0), F.lit(str(item_string)))\n            internal = _InternalFrame(sdf=sdf, index_map=[(SPARK_INDEX_NAME_FORMAT(0), None)])\n            return _col(DataFrame(internal))\n\n        internal = self._internal.copy(\n            sdf=sdf,\n            index_map=self._internal._index_map[len(item):])\n\n        self._internal = self.drop(item)._internal\n\n        return _col(DataFrame(internal))\n\n    def copy(self) -> 'Series':\n        \"\"\"\n        Make a copy of this object's indices and data.\n\n        Returns\n        -------\n        copy : Series\n\n        Examples\n        --------\n        >>> s = ks.Series([1, 2], index=[\"a\", \"b\"])\n        >>> s\n        a    1\n        b    2\n        Name: 0, dtype: int64\n        >>> s_copy = s.copy()\n        >>> s_copy\n        a    1\n        b    2\n        Name: 0, dtype: int64\n        \"\"\"\n        return _col(DataFrame(self._internal.copy()))\n\n    def truncate(self, before=None, after=None, copy=True):\n        \"\"\"\n        Truncates a sorted Series before and/or after some particular index value.\n        Series should have sorted index.\n\n        .. note:: the current implementation of truncate uses is_monotonic_increasing internally\n            This leads to move all data into single partition in single machine and could cause\n            serious performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        before : string, int\n            Truncate all rows before this index value\n        after : string, int\n            Truncate all rows after this index value\n        copy : boolean, default is True,\n            return a copy of the truncated section\n\n        Returns\n        -------\n        truncated : Series\n\n        Examples\n        --------\n\n\n        A Series has index that sorted integers.\n\n        >>> s = ks.Series([10, 20, 30, 40, 50, 60, 70],\n        ...               index=[1, 2, 3, 4, 5, 6, 7])\n        >>> s\n        1    10\n        2    20\n        3    30\n        4    40\n        5    50\n        6    60\n        7    70\n        Name: 0, dtype: int64\n\n        >>> s.truncate(2, 5)\n        2    20\n        3    30\n        4    40\n        5    50\n        Name: 0, dtype: int64\n\n        A Series has index that sorted strings.\n\n        >>> s = ks.Series([10, 20, 30, 40, 50, 60, 70],\n        ...               index=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n        >>> s\n        a    10\n        b    20\n        c    30\n        d    40\n        e    50\n        f    60\n        g    70\n        Name: 0, dtype: int64\n\n        >>> s.truncate('b', 'e')\n        b    20\n        c    30\n        d    40\n        e    50\n        Name: 0, dtype: int64\n        \"\"\"\n        indexes = self.index\n        indexes_increasing = indexes.is_monotonic_increasing\n        if not indexes_increasing and not indexes.is_monotonic_decreasing:\n            raise ValueError(\"truncate requires a sorted index\")\n        if (before is None) and (after is None):\n            return self.copy() if copy else self\n        if (before is not None) and (after is not None):\n            if before > after:\n                raise ValueError(\"Truncate: %s must be after %s\" % (after, before))\n\n        if indexes_increasing:\n            result = _col(self.to_frame()[before:after])\n        else:\n            result = _col(self.to_frame()[after:before])\n\n        return result.copy() if copy else result\n\n    def mode(self, dropna=True) -> 'Series':\n        \"\"\"\n        Return the mode(s) of the dataset.\n\n        Always returns Series even if only one value is returned.\n\n        Parameters\n        ----------\n        dropna : bool, default True\n            Don't consider counts of NaN/NaT.\n\n        Returns\n        -------\n        Series\n            Modes of the Series.\n\n        Examples\n        --------\n        >>> s = ks.Series([0, 0, 1, 1, 1, np.nan, np.nan, np.nan])\n        >>> s\n        0    0.0\n        1    0.0\n        2    1.0\n        3    1.0\n        4    1.0\n        5    NaN\n        6    NaN\n        7    NaN\n        Name: 0, dtype: float64\n\n        >>> s.mode()\n        0    1.0\n        Name: 0, dtype: float64\n\n        If there are several same modes, all items are shown\n\n        >>> s = ks.Series([0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n        ...                np.nan, np.nan, np.nan])\n        >>> s\n        0     0.0\n        1     0.0\n        2     1.0\n        3     1.0\n        4     1.0\n        5     2.0\n        6     2.0\n        7     2.0\n        8     3.0\n        9     3.0\n        10    3.0\n        11    NaN\n        12    NaN\n        13    NaN\n        Name: 0, dtype: float64\n\n        >>> s.mode()\n        0    1.0\n        1    3.0\n        2    2.0\n        Name: 0, dtype: float64\n\n        With 'dropna' set to 'False', we can also see NaN in the result\n\n        >>> s.mode(False)\n        0    NaN\n        1    1.0\n        2    3.0\n        3    2.0\n        Name: 0, dtype: float64\n        \"\"\"\n        ser_count = self.value_counts(dropna=dropna, sort=False)\n        sdf_count = ser_count._internal.sdf\n        most_value = ser_count.max()\n        sdf_most_value = sdf_count.where(\"count == {}\".format(most_value))\n        sdf = sdf_most_value.select(\n            F.col(SPARK_INDEX_NAME_FORMAT(0)).alias('0'))\n        internal = _InternalFrame(sdf=sdf)\n\n        result = _col(DataFrame(internal))\n        result.name = self.name\n\n        return result\n\n    def keys(self):\n        \"\"\"\n        Return alias for index.\n\n        Returns\n        -------\n        Index\n            Index of the Series.\n\n        Examples\n        --------\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> kser = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n\n        >>> kser.keys()  # doctest: +SKIP\n        MultiIndex([(  'lama',  'speed'),\n                    (  'lama', 'weight'),\n                    (  'lama', 'length'),\n                    (   'cow',  'speed'),\n                    (   'cow', 'weight'),\n                    (   'cow', 'length'),\n                    ('falcon',  'speed'),\n                    ('falcon', 'weight'),\n                    ('falcon', 'length')],\n                   )\n        \"\"\"\n        return self.index\n\n    # TODO: 'regex', 'method' parameter\n    def replace(self, to_replace=None, value=None, regex=False) -> 'Series':\n        \"\"\"\n        Replace values given in to_replace with value.\n        Values of the Series are replaced with other values dynamically.\n\n        Parameters\n        ----------\n        to_replace : str, list, dict, Series, int, float, or None\n            How to find the values that will be replaced.\n            * numeric, str:\n\n                - numeric: numeric values equal to to_replace will be replaced with value\n                - str: string exactly matching to_replace will be replaced with value\n\n            * list of str or numeric:\n\n                - if to_replace and value are both lists, they must be the same length.\n                - str and numeric rules apply as above.\n\n            * dict:\n\n                - Dicts can be used to specify different replacement values for different\n                  existing values.\n                  For example, {'a': 'b', 'y': 'z'} replaces the value \u2018a\u2019 with \u2018b\u2019 and \u2018y\u2019\n                  with \u2018z\u2019. To use a dict in this way the value parameter should be None.\n                - For a DataFrame a dict can specify that different values should be replaced\n                  in different columns. For example, {'a': 1, 'b': 'z'} looks for the value 1\n                  in column \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with\n                  whatever is specified in value.\n                  The value parameter should not be None in this case.\n                  You can treat this as a special case of passing two lists except that you are\n                  specifying the column to search in.\n\n            See the examples section for examples of each of these.\n\n        value : scalar, dict, list, str default None\n            Value to replace any values matching to_replace with.\n            For a DataFrame a dict of values can be used to specify which value to use\n            for each column (columns not in the dict will not be filled).\n            Regular expressions, strings and lists or dicts of such objects are also allowed.\n\n        Returns\n        -------\n        Series\n            Object after replacement.\n\n        Examples\n        --------\n\n        Scalar `to_replace` and `value`\n\n        >>> s = ks.Series([0, 1, 2, 3, 4])\n        >>> s\n        0    0\n        1    1\n        2    2\n        3    3\n        4    4\n        Name: 0, dtype: int64\n\n        >>> s.replace(0, 5)\n        0    5\n        1    1\n        2    2\n        3    3\n        4    4\n        Name: 0, dtype: int64\n\n        List-like `to_replace`\n\n        >>> s.replace([0, 4], 5000)\n        0    5000\n        1       1\n        2       2\n        3       3\n        4    5000\n        Name: 0, dtype: int64\n\n        >>> s.replace([1, 2, 3], [10, 20, 30])\n        0     0\n        1    10\n        2    20\n        3    30\n        4     4\n        Name: 0, dtype: int64\n\n        Dict-like `to_replace`\n\n        >>> s.replace({1: 1000, 2: 2000, 3: 3000, 4: 4000})\n        0       0\n        1    1000\n        2    2000\n        3    3000\n        4    4000\n        Name: 0, dtype: int64\n\n        Also support for MultiIndex\n\n        >>> midx = pd.MultiIndex([['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        lama    speed      45.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace(45, 450)\n        lama    speed     450.0\n                weight    200.0\n                length      1.2\n        cow     speed      30.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace([45, 30, 320], 500)\n        lama    speed     500.0\n                weight    200.0\n                length      1.2\n        cow     speed     500.0\n                weight    250.0\n                length      1.5\n        falcon  speed     500.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n\n        >>> s.replace({45: 450, 30: 300})\n        lama    speed     450.0\n                weight    200.0\n                length      1.2\n        cow     speed     300.0\n                weight    250.0\n                length      1.5\n        falcon  speed     320.0\n                weight      1.0\n                length      0.3\n        Name: 0, dtype: float64\n        \"\"\"\n        if to_replace is None:\n            return self\n        if not isinstance(to_replace, (str, list, dict, int, float)):\n            raise ValueError(\n                \"'to_replace' should be one of str, list, dict, int, float\")\n        if regex:\n            raise NotImplementedError(\"replace currently not support for regex\")\n        if isinstance(to_replace, list) and isinstance(value, list):\n            if not len(to_replace) == len(value):\n                raise ValueError(\"Replacement lists must match in length. Expecting {} got {}\"\n                                 .format(len(to_replace), len(value)))\n            to_replace = {k: v for k, v in zip(to_replace, value)}\n        if isinstance(to_replace, dict):\n            is_start = True\n            if len(to_replace) == 0:\n                current = self._scol\n            else:\n                for to_replace_, value in to_replace.items():\n                    if is_start:\n                        current = F.when(self._scol == F.lit(to_replace_), value)\n                        is_start = False\n                    else:\n                        current = current.when(self._scol == F.lit(to_replace_), value)\n                current = current.otherwise(self._scol)\n        else:\n            current = F.when(self._scol.isin(to_replace), value).otherwise(self._scol)\n\n        return self._with_new_scol(current)\n\n    def update(self, other):\n        \"\"\"\n        Modify Series in place using non-NA values from passed Series. Aligns on index.\n\n        Parameters\n        ----------\n        other : Series\n\n        Examples\n        --------\n        >>> from databricks.koalas.config import set_option, reset_option\n        >>> set_option(\"compute.ops_on_diff_frames\", True)\n        >>> s = ks.Series([1, 2, 3])\n        >>> s.update(ks.Series([4, 5, 6]))\n        >>> s.sort_index()\n        0    4\n        1    5\n        2    6\n        Name: 0, dtype: int64\n\n        >>> s = ks.Series(['a', 'b', 'c'])\n        >>> s.update(ks.Series(['d', 'e'], index=[0, 2]))\n        >>> s.sort_index()\n        0    d\n        1    b\n        2    e\n        Name: 0, dtype: object\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s.update(ks.Series([4, 5, 6, 7, 8]))\n        >>> s.sort_index()\n        0    4\n        1    5\n        2    6\n        Name: 0, dtype: int64\n\n        >>> s = ks.Series([1, 2, 3], index=[10, 11, 12])\n        >>> s\n        10    1\n        11    2\n        12    3\n        Name: 0, dtype: int64\n\n        >>> s.update(ks.Series([4, 5, 6]))\n        >>> s.sort_index()\n        10    1\n        11    2\n        12    3\n        Name: 0, dtype: int64\n\n        >>> s.update(ks.Series([4, 5, 6], index=[11, 12, 13]))\n        >>> s.sort_index()\n        10    1\n        11    4\n        12    5\n        Name: 0, dtype: int64\n\n        If ``other`` contains NaNs the corresponding values are not updated\n        in the original Series.\n\n        >>> s = ks.Series([1, 2, 3])\n        >>> s.update(ks.Series([4, np.nan, 6]))\n        >>> s.sort_index()\n        0    4.0\n        1    2.0\n        2    6.0\n        Name: 0, dtype: float64\n\n        >>> reset_option(\"compute.ops_on_diff_frames\")\n        \"\"\"\n        if not isinstance(other, Series):\n            raise ValueError(\"'other' must be a Series\")\n\n        index_scol_names = [index_map[0] for index_map in self._internal.index_map]\n        combined = combine_frames(self.to_frame(), other.to_frame(), how='leftouter')\n        combined_sdf = combined._sdf\n        this_col = \"__this_%s\" % str(\n            self._internal.column_name_for(self._internal.column_index[0]))\n        that_col = \"__that_%s\" % str(\n            self._internal.column_name_for(other._internal.column_index[0]))\n        cond = F.when(scol_for(combined_sdf, that_col).isNotNull(),\n                      scol_for(combined_sdf, that_col)) \\\n                .otherwise(combined_sdf[this_col]) \\\n                .alias(str(self._internal.column_name_for(self._internal.column_index[0])))\n        internal = _InternalFrame(\n            sdf=combined_sdf.select(index_scol_names + [cond]),\n            index_map=self._internal.index_map,\n            column_index=self._internal.column_index)\n        self_updated = _col(ks.DataFrame(internal))\n        self._internal = self_updated._internal\n        self._kdf = self_updated._kdf\n\n    def where(self, cond, other=np.nan):\n        \"\"\"\n        Replace values where the condition is False.\n\n        Parameters\n        ----------\n        cond : boolean Series\n            Where cond is True, keep the original value. Where False,\n            replace with corresponding value from other.\n        other : scalar, Series\n            Entries where cond is False are replaced with corresponding value from other.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n\n        >>> from databricks.koalas.config import set_option, reset_option\n        >>> set_option(\"compute.ops_on_diff_frames\", True)\n        >>> s1 = ks.Series([0, 1, 2, 3, 4])\n        >>> s2 = ks.Series([100, 200, 300, 400, 500])\n        >>> s1.where(s1 > 0).sort_index()\n        0    NaN\n        1    1.0\n        2    2.0\n        3    3.0\n        4    4.0\n        Name: 0, dtype: float64\n\n        >>> s1.where(s1 > 1, 10).sort_index()\n        0    10\n        1    10\n        2     2\n        3     3\n        4     4\n        Name: 0, dtype: int64\n\n        >>> s1.where(s1 > 1, s1 + 100).sort_index()\n        0    100\n        1    101\n        2      2\n        3      3\n        4      4\n        Name: 0, dtype: int64\n\n        >>> s1.where(s1 > 1, s2).sort_index()\n        0    100\n        1    200\n        2      2\n        3      3\n        4      4\n        Name: 0, dtype: int64\n\n        >>> reset_option(\"compute.ops_on_diff_frames\")\n        \"\"\"\n        data_col_name = self._internal.column_name_for(self._internal.column_index[0])\n\n        assert isinstance(cond, Series)\n\n        # We should check the DataFrame from both `cond` and `other`.\n        should_try_ops_on_diff_frame = (\n            cond._kdf is not self._kdf or\n            (isinstance(other, Series) and other._kdf is not self._kdf))\n\n        if should_try_ops_on_diff_frame:\n            # Try to perform it with 'compute.ops_on_diff_frame' option.\n            kdf = self.to_frame()\n            kdf['__tmp_cond_col__'] = cond\n            kdf['__tmp_other_col__'] = other\n\n            sdf = kdf._sdf\n            # above logic makes a Spark DataFrame looks like below:\n            # +-----------------+---+----------------+-----------------+\n            # |__index_level_0__|  0|__tmp_cond_col__|__tmp_other_col__|\n            # +-----------------+---+----------------+-----------------+\n            # |                0|  0|           false|              100|\n            # |                1|  1|           false|              200|\n            # |                3|  3|            true|              400|\n            # |                2|  2|            true|              300|\n            # |                4|  4|            true|              500|\n            # +-----------------+---+----------------+-----------------+\n            condition = F.when(\n                sdf['__tmp_cond_col__'], sdf[data_col_name]\n            ).otherwise(sdf['__tmp_other_col__']).alias(data_col_name)\n\n            sdf = sdf.select(*self._internal.index_columns + [condition])\n            return _col(ks.DataFrame(_InternalFrame(\n                sdf=sdf,\n                index_map=self._internal.index_map,\n                column_index=self._internal.column_index,\n                column_index_names=self._internal.column_index_names)))\n        else:\n            if isinstance(other, Series):\n                other = other._scol\n            condition = F.when(\n                cond._scol, self._scol\n            ).otherwise(other).alias(data_col_name)\n            return self._with_new_scol(condition)\n\n    def mask(self, cond, other=np.nan):\n        \"\"\"\n        Replace values where the condition is True.\n\n        Parameters\n        ----------\n        cond : boolean Series\n            Where cond is False, keep the original value. Where True,\n            replace with corresponding value from other.\n        other : scalar, Series\n            Entries where cond is True are replaced with corresponding value from other.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n\n        >>> from databricks.koalas.config import set_option, reset_option\n        >>> set_option(\"compute.ops_on_diff_frames\", True)\n        >>> s1 = ks.Series([0, 1, 2, 3, 4])\n        >>> s2 = ks.Series([100, 200, 300, 400, 500])\n        >>> s1.mask(s1 > 0).sort_index()\n        0    0.0\n        1    NaN\n        2    NaN\n        3    NaN\n        4    NaN\n        Name: 0, dtype: float64\n\n        >>> s1.mask(s1 > 1, 10).sort_index()\n        0     0\n        1     1\n        2    10\n        3    10\n        4    10\n        Name: 0, dtype: int64\n\n        >>> s1.mask(s1 > 1, s1 + 100).sort_index()\n        0      0\n        1      1\n        2    102\n        3    103\n        4    104\n        Name: 0, dtype: int64\n\n        >>> s1.mask(s1 > 1, s2).sort_index()\n        0      0\n        1      1\n        2    300\n        3    400\n        4    500\n        Name: 0, dtype: int64\n\n        >>> reset_option(\"compute.ops_on_diff_frames\")\n        \"\"\"\n        return self.where(~cond, other)\n\n    def xs(self, key, level=None):\n        \"\"\"\n        Return cross-section from the Series.\n\n        This method takes a `key` argument to select data at a particular\n        level of a MultiIndex.\n\n        Parameters\n        ----------\n        key : label or tuple of label\n            Label contained in the index, or partially in a MultiIndex.\n        level : object, defaults to first n levels (n=1 or len(key))\n            In case of a key partially contained in a MultiIndex, indicate\n            which levels are used. Levels can be referred by label or position.\n\n        Returns\n        -------\n        Series\n            Cross-section from the original Series\n            corresponding to the selected index levels.\n\n        Examples\n        --------\n        >>> midx = pd.MultiIndex([['a', 'b', 'c'],\n        ...                       ['lama', 'cow', 'falcon'],\n        ...                       ['speed', 'weight', 'length']],\n        ...                      [[0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 0, 0, 1, 1, 1, 2, 2, 2],\n        ...                       [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n        >>> s = ks.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3],\n        ...               index=midx)\n        >>> s\n        a  lama    speed      45.0\n                   weight    200.0\n                   length      1.2\n        b  cow     speed      30.0\n                   weight    250.0\n                   length      1.5\n        c  falcon  speed     320.0\n                   weight      1.0\n                   length      0.3\n        Name: 0, dtype: float64\n\n        Get values at specified index\n\n        >>> s.xs('a')\n        lama  speed      45.0\n              weight    200.0\n              length      1.2\n        Name: 0, dtype: float64\n\n        Get values at several indexes\n\n        >>> s.xs(('a', 'lama'))\n        speed      45.0\n        weight    200.0\n        length      1.2\n        Name: 0, dtype: float64\n\n        Get values at specified index and level\n\n        >>> s.xs('lama', level=1)\n        a  speed      45.0\n           weight    200.0\n           length      1.2\n        Name: 0, dtype: float64\n        \"\"\"\n        if not isinstance(key, tuple):\n            key = (key,)\n        if level is None:\n            level = 0\n\n        cols = (self._internal.index_scols[:level] +\n                self._internal.index_scols[level+len(key):] +\n                [self._internal.scol_for(self._internal.column_index[0])])\n        rows = [self._internal.scols[lvl] == index\n                for lvl, index in enumerate(key, level)]\n        sdf = self._internal.sdf \\\n            .select(cols) \\\n            .where(reduce(lambda x, y: x & y, rows))\n\n        if len(self._internal._index_map) == len(key):\n            # if sdf has one column and one data, return data only without frame\n            pdf = sdf.limit(2).toPandas()\n            length = len(pdf)\n            if length == 1:\n                return pdf[self.name].iloc[0]\n\n        index_cols = [col for col in sdf.columns if col not in self._internal.data_columns]\n        index_map_dict = dict(self._internal.index_map)\n        internal = self._internal.copy(\n            sdf=sdf,\n            index_map=[(index_col, index_map_dict[index_col]) for index_col in index_cols])\n\n        return _col(DataFrame(internal))\n\n    def pct_change(self, periods=1):\n        \"\"\"\n        Percentage change between the current and a prior element.\n\n        .. note:: the current implementation of this API uses Spark's Window without\n            specifying partition specification. This leads to move all data into\n            single partition in single machine and could cause serious\n            performance degradation. Avoid this method against very large dataset.\n\n        Parameters\n        ----------\n        periods : int, default 1\n            Periods to shift for forming percent change.\n\n        Returns\n        -------\n        Series\n\n        Examples\n        --------\n\n        >>> kser = ks.Series([90, 91, 85], index=[2, 4, 1])\n        >>> kser\n        2    90\n        4    91\n        1    85\n        Name: 0, dtype: int64\n\n        >>> kser.pct_change()\n        2         NaN\n        4    0.011111\n        1   -0.065934\n        Name: 0, dtype: float64\n\n        >>> kser.sort_index().pct_change()\n        1         NaN\n        2    0.058824\n        4    0.011111\n        Name: 0, dtype: float64\n\n        >>> kser.pct_change(periods=2)\n        2         NaN\n        4         NaN\n        1   -0.055556\n        Name: 0, dtype: float64\n        \"\"\"\n        scol = self._internal.scol\n\n        window = Window.orderBy(F.monotonically_increasing_id()).rowsBetween(-periods, -periods)\n        prev_row = F.lag(scol, periods).over(window)\n\n        return self._with_new_scol((scol - prev_row) / prev_row)\n\n    def _cum(self, func, skipna, part_cols=()):\n        # This is used to cummin, cummax, cumsum, etc.\n        index_columns = self._internal.index_columns\n        window = Window.orderBy(\n            index_columns).partitionBy(*part_cols).rowsBetween(\n                Window.unboundedPreceding, Window.currentRow)\n\n        if skipna:\n            # There is a behavior difference between pandas and PySpark. In case of cummax,\n            #\n            # Input:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  1.0  0.0\n            # 3  2.0  4.0\n            # 4  4.0  9.0\n            #\n            # pandas:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n            #\n            # PySpark:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  1.0\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n\n            scol = F.when(\n                # Manually sets nulls given the column defined above.\n                self._scol.isNull(), F.lit(None)\n            ).otherwise(func(self._scol).over(window))\n        else:\n            # Here, we use two Windows.\n            # One for real data.\n            # The other one for setting nulls after the first null it meets.\n            #\n            # There is a behavior difference between pandas and PySpark. In case of cummax,\n            #\n            # Input:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  1.0  0.0\n            # 3  2.0  4.0\n            # 4  4.0  9.0\n            #\n            # pandas:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  NaN\n            # 2  5.0  NaN\n            # 3  5.0  NaN\n            # 4  5.0  NaN\n            #\n            # PySpark:\n            #      A    B\n            # 0  2.0  1.0\n            # 1  5.0  1.0\n            # 2  5.0  1.0\n            # 3  5.0  4.0\n            # 4  5.0  9.0\n            scol = F.when(\n                # By going through with max, it sets True after the first time it meets null.\n                F.max(self._scol.isNull()).over(window),\n                # Manually sets nulls given the column defined above.\n                F.lit(None)\n            ).otherwise(func(self._scol).over(window))\n\n        # cumprod uses exp(sum(log(...))) trick.\n        if func.__name__ == \"cumprod\":\n            scol = F.exp(scol)\n\n        return self._with_new_scol(scol).rename(self.name)\n\n    # ----------------------------------------------------------------------\n    # Accessor Methods\n    # ----------------------------------------------------------------------\n    dt = CachedAccessor(\"dt\", DatetimeMethods)\n    str = CachedAccessor(\"str\", StringMethods)\n\n    # ----------------------------------------------------------------------\n\n    def _reduce_for_stat_function(self, sfun, name, axis=None, numeric_only=None):\n        \"\"\"\n        Applies sfun to the column and returns a scalar\n\n        Parameters\n        ----------\n        sfun : the stats function to be used for aggregation\n        name : original pandas API name.\n        axis : used only for sanity check because series only support index axis.\n        numeric_only : not used by this implementation, but passed down by stats functions\n        \"\"\"\n        from inspect import signature\n        axis = validate_axis(axis)\n        if axis == 1:\n            raise ValueError(\"Series does not support columns axis.\")\n        num_args = len(signature(sfun).parameters)\n        col_sdf = self._scol\n        col_type = self.spark_type\n        if isinstance(col_type, BooleanType) and sfun.__name__ not in ('min', 'max'):\n            # Stat functions cannot be used with boolean values by default\n            # Thus, cast to integer (true to 1 and false to 0)\n            # Exclude the min and max methods though since those work with booleans\n            col_sdf = col_sdf.cast('integer')\n        if num_args == 1:\n            # Only pass in the column if sfun accepts only one arg\n            col_sdf = sfun(col_sdf)\n        else:  # must be 2\n            assert num_args == 2\n            # Pass in both the column and its data type if sfun accepts two args\n            col_sdf = sfun(col_sdf, col_type)\n        return _unpack_scalar(self._internal._sdf.select(col_sdf))\n\n    def __len__(self):\n        return len(self.to_dataframe())\n\n    def __getitem__(self, key):\n        if isinstance(key, Series) and isinstance(key.spark_type, BooleanType):\n            should_try_ops_on_diff_frame = key._kdf is not self._kdf\n\n            if should_try_ops_on_diff_frame:\n                kdf = self.to_frame()\n                kdf[\"__temp_col__\"] = key\n                sdf = kdf._sdf.filter(F.col(\"__temp_col__\")).drop(\"__temp_col__\")\n                return _col(ks.DataFrame(_InternalFrame(\n                    sdf=sdf,\n                    index_map=self._internal.index_map,\n                    column_index=self._internal.column_index,\n                    column_index_names=self._internal.column_index_names)))\n            else:\n                return _col(DataFrame(self._internal.copy(sdf=self._kdf._sdf.filter(key._scol))))\n\n        if not isinstance(key, tuple):\n            key = (key,)\n        if len(self._internal._index_map) < len(key):\n            raise KeyError(\"Key length ({}) exceeds index depth ({})\"\n                           .format(len(key), len(self._internal.index_map)))\n\n        cols = (self._internal.index_scols[len(key):] +\n                [self._internal.scol_for(self._internal.column_index[0])])\n        rows = [self._internal.scols[level] == index\n                for level, index in enumerate(key)]\n        sdf = self._internal.sdf \\\n            .select(cols) \\\n            .where(reduce(lambda x, y: x & y, rows))\n\n        if len(self._internal._index_map) == len(key):\n            # if sdf has one column and one data, return data only without frame\n            pdf = sdf.limit(2).toPandas()\n            length = len(pdf)\n            if length == 1:\n                return pdf[self.name].iloc[0]\n\n            key_string = name_like_string(key)\n            sdf = sdf.withColumn(SPARK_INDEX_NAME_FORMAT(0), F.lit(key_string))\n            internal = _InternalFrame(sdf=sdf, index_map=[(SPARK_INDEX_NAME_FORMAT(0), None)])\n            return _col(DataFrame(internal))\n\n        internal = self._internal.copy(\n            sdf=sdf,\n            index_map=self._internal._index_map[len(key):])\n\n        return _col(DataFrame(internal))\n\n    def __getattr__(self, item: str_type) -> Any:\n        if item.startswith(\"__\") or item.startswith(\"_pandas_\") or item.startswith(\"_spark_\"):\n            raise AttributeError(item)\n        if hasattr(_MissingPandasLikeSeries, item):\n            property_or_func = getattr(_MissingPandasLikeSeries, item)\n            if isinstance(property_or_func, property):\n                return property_or_func.fget(self)  # type: ignore\n            else:\n                return partial(property_or_func, self)\n        return self.getField(item)\n\n    def __str__(self):\n        return self._pandas_orig_repr()\n\n    def _to_internal_pandas(self):\n        \"\"\"\n        Return a pandas Series directly from _internal to avoid overhead of copy.\n\n        This method is for internal use only.\n        \"\"\"\n        return _col(self._internal.pandas_df)\n\n    def __repr__(self):\n        max_display_count = get_option(\"display.max_rows\")\n        if max_display_count is None:\n            return self._to_internal_pandas().to_string(name=self.name, dtype=self.dtype)\n\n        pser = self.head(max_display_count + 1)._to_internal_pandas()\n        pser_length = len(pser)\n        pser = pser.iloc[:max_display_count]\n        if pser_length > max_display_count:\n            repr_string = pser.to_string(length=True)\n            rest, prev_footer = repr_string.rsplit(\"\\n\", 1)\n            match = REPR_PATTERN.search(prev_footer)\n            if match is not None:\n                length = match.group(\"length\")\n                name = str(self.dtype.name)\n                footer = (\"\\nName: {name}, dtype: {dtype}\\nShowing only the first {length}\"\n                          .format(length=length, name=self.name, dtype=pprint_thing(name)))\n                return rest + footer\n        return pser.to_string(name=self.name, dtype=self.dtype)\n\n    def __dir__(self):\n        if not isinstance(self.spark_type, StructType):\n            fields = []\n        else:\n            fields = [f for f in self.spark_type.fieldNames() if ' ' not in f]\n        return super(Series, self).__dir__() + fields\n\n    def __iter__(self):\n        return _MissingPandasLikeSeries.__iter__(self)\n\n    def _pandas_orig_repr(self):\n        # TODO: figure out how to reuse the original one.\n        return 'Column<%s>' % self._scol._jc.toString().encode('utf8')\n\n    def _equals(self, other: 'Series') -> bool:\n        return self._scol._jc.equals(other._scol._jc)\n\n\ndef _unpack_scalar(sdf):\n    \"\"\"\n    Takes a dataframe that is supposed to contain a single row with a single scalar value,\n    and returns this value.\n    \"\"\"\n    l = sdf.head(2)\n    assert len(l) == 1, (sdf, l)\n    row = l[0]\n    l2 = list(row.asDict().values())\n    assert len(l2) == 1, (row, l2)\n    return l2[0]\n\n\ndef _col(df):\n    assert isinstance(df, (DataFrame, pd.DataFrame)), type(df)\n    return df[df.columns[0]]\n", "idx": 23, "id": 13576, "msg": "", "proj": "databricks-koalas", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -676,6 +676,7 @@ func (b *Bucket) NewWriter(ctx context.Context, key string, opts *WriterOptions)\n \t\tbuf:        bytes.NewBuffer([]byte{}),\n \t\tcontentMD5: opts.ContentMD5,\n \t\tmd5hash:    md5.New(),\n+\t\tprovider:   b.tracer.Provider,\n \t}, nil\n }\n ", "y": 0, "oldf": "// Copyright 2018 The Go Cloud Development Kit Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package blob provides an easy and portable way to interact with blobs\n// within a storage location, hereafter called a \"bucket\".\n//\n// It supports operations like reading and writing blobs (using standard\n// interfaces from the io package), deleting blobs, and listing blobs in a\n// bucket.\n//\n// Subpackages contain distinct implementations of blob for various providers,\n// including Cloud and on-prem solutions. For example, \"fileblob\" supports\n// blobs backed by a filesystem. Your application should import one of these\n// provider-specific subpackages and use its exported function(s) to create a\n// *Bucket; do not use the NewBucket function in this package. For example:\n//\n//  bucket, err := fileblob.OpenBucket(\"path/to/dir\", nil)\n//  if err != nil {\n//      return fmt.Errorf(\"could not open bucket: %v\", err)\n//  }\n//  buf, err := bucket.ReadAll(context.Background(), \"myfile.txt\")\n//  ...\n//\n// Then, write your application code using the *Bucket type. You can easily\n// reconfigure your initialization code to choose a different provider.\n// You can develop your application locally using fileblob, or deploy it to\n// multiple Cloud providers. You may find http://github.com/google/wire useful\n// for managing your initialization code.\n//\n// Alternatively, you can construct a *Bucket using blob.Open by providing\n// a URL that's supported by a blob subpackage that you have linked\n// in to your application.\n//\n//\n// Errors\n//\n// The errors returned from this package can be inspected in several ways:\n//\n// The Code function from gocloud.dev/gcerrors will return an error code, also\n// defined in that package, when invoked on an error.\n//\n// The Bucket.ErrorAs method can retrieve the driver error underlying the returned\n// error.\n//\n//\n// OpenCensus Integration\n//\n// OpenCensus supports tracing and metric collection for multiple languages and\n// backend providers. See https://opencensus.io.\n//\n// This API collects OpenCensus traces and metrics for the following methods:\n// - Attributes\n// - Delete\n// - NewRangeReader, from creation until the call to Close. (NewReader and ReadAll\n//   are included because they call NewRangeReader.)\n// - NewWriter, from creation until the call to Close.\n//\n// To enable trace collection in your application, see \"Configure Exporter\" at\n// https://opencensus.io/quickstart/go/tracing.\n// To enable metric collection in your application, see \"Exporting stats\" at\n// https://opencensus.io/quickstart/go/metrics.\npackage blob // import \"gocloud.dev/blob\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/md5\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"mime\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"gocloud.dev/blob/driver\"\n\t\"gocloud.dev/internal/gcerr\"\n\t\"gocloud.dev/internal/oc\"\n)\n\n// Reader reads bytes from a blob.\n// It implements io.ReadCloser, and must be closed after\n// reads are finished.\ntype Reader struct {\n\tb   driver.Bucket\n\tr   driver.Reader\n\tend func(error) // called at Close to finish trace and metric collection\n}\n\n// Read implements io.Reader (https://golang.org/pkg/io/#Reader).\nfunc (r *Reader) Read(p []byte) (int, error) {\n\tn, err := r.r.Read(p)\n\treturn n, wrapError(r.b, err)\n}\n\n// Close implements io.Closer (https://golang.org/pkg/io/#Closer).\nfunc (r *Reader) Close() error {\n\terr := wrapError(r.b, r.r.Close())\n\tr.end(err)\n\treturn err\n}\n\n// ContentType returns the MIME type of the blob.\nfunc (r *Reader) ContentType() string {\n\treturn r.r.Attributes().ContentType\n}\n\n// ModTime returns the time the blob was last modified.\nfunc (r *Reader) ModTime() time.Time {\n\treturn r.r.Attributes().ModTime\n}\n\n// Size returns the size of the blob content in bytes.\nfunc (r *Reader) Size() int64 {\n\treturn r.r.Attributes().Size\n}\n\n// As converts i to provider-specific types.\n// See Bucket.As for more details.\nfunc (r *Reader) As(i interface{}) bool {\n\treturn r.r.As(i)\n}\n\n// Attributes contains attributes about a blob.\ntype Attributes struct {\n\t// CacheControl specifies caching attributes that providers may use\n\t// when serving the blob.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control\n\tCacheControl string\n\t// ContentDisposition specifies whether the blob content is expected to be\n\t// displayed inline or as an attachment.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Disposition\n\tContentDisposition string\n\t// ContentEncoding specifies the encoding used for the blob's content, if any.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding\n\tContentEncoding string\n\t// ContentLanguage specifies the language used in the blob's content, if any.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Language\n\tContentLanguage string\n\t// ContentType is the MIME type of the blob. It will not be empty.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type\n\tContentType string\n\t// Metadata holds key/value pairs associated with the blob.\n\t// Keys are guaranteed to be in lowercase, even if the backend provider\n\t// has case-sensitive keys (although note that Metadata written via\n\t// this package will always be lowercased). If there are duplicate\n\t// case-insensitive keys (e.g., \"foo\" and \"FOO\"), only one value\n\t// will be kept, and it is undefined which one.\n\tMetadata map[string]string\n\t// ModTime is the time the blob was last modified.\n\tModTime time.Time\n\t// Size is the size of the blob's content in bytes.\n\tSize int64\n\t// MD5 is an MD5 hash of the blob contents or nil if not available.\n\tMD5 []byte\n\n\tasFunc func(interface{}) bool\n}\n\n// As converts i to provider-specific types.\n// See Bucket.As for more details.\nfunc (a *Attributes) As(i interface{}) bool {\n\tif a.asFunc == nil {\n\t\treturn false\n\t}\n\treturn a.asFunc(i)\n}\n\n// Writer writes bytes to a blob.\n//\n// It implements io.WriteCloser (https://golang.org/pkg/io/#Closer), and must be\n// closed after all writes are done.\ntype Writer struct {\n\tb          driver.Bucket\n\tw          driver.Writer\n\tend        func(error) // called at Close to finish trace and metric collection\n\tcancel     func()      // cancels the ctx provided to NewTypedWriter if contentMD5 verification fails\n\tcontentMD5 []byte\n\tmd5hash    hash.Hash\n\n\t// These fields exist only when w is not yet created.\n\t//\n\t// A ctx is stored in the Writer since we need to pass it into NewTypedWriter\n\t// when we finish detecting the content type of the blob and create the\n\t// underlying driver.Writer. This step happens inside Write or Close and\n\t// neither of them take a context.Context as an argument. The ctx is set\n\t// to nil after we have passed it to NewTypedWriter.\n\tctx  context.Context\n\tkey  string\n\topts *driver.WriterOptions\n\tbuf  *bytes.Buffer\n}\n\n// sniffLen is the byte size of Writer.buf used to detect content-type.\nconst sniffLen = 512\n\n// Write implements the io.Writer interface (https://golang.org/pkg/io/#Writer).\n//\n// Writes may happen asynchronously, so the returned error can be nil\n// even if the actual write eventually fails. The write is only guaranteed to\n// have succeeded if Close returns no error.\nfunc (w *Writer) Write(p []byte) (n int, err error) {\n\tif len(w.contentMD5) > 0 {\n\t\tif _, err := w.md5hash.Write(p); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\tif w.w != nil {\n\t\tn, err := w.w.Write(p)\n\t\treturn n, wrapError(w.b, err)\n\t}\n\n\t// If w is not yet created due to no content-type being passed in, try to sniff\n\t// the MIME type based on at most 512 bytes of the blob content of p.\n\n\t// Detect the content-type directly if the first chunk is at least 512 bytes.\n\tif w.buf.Len() == 0 && len(p) >= sniffLen {\n\t\treturn w.open(p)\n\t}\n\n\t// Store p in w.buf and detect the content-type when the size of content in\n\t// w.buf is at least 512 bytes.\n\tw.buf.Write(p)\n\tif w.buf.Len() >= sniffLen {\n\t\treturn w.open(w.buf.Bytes())\n\t}\n\treturn len(p), nil\n}\n\n// Close closes the blob writer. The write operation is not guaranteed to have succeeded until\n// Close returns with no error.\n// Close may return an error if the context provided to create the Writer is\n// canceled or reaches its deadline.\nfunc (w *Writer) Close() (err error) {\n\tdefer func() { w.end(err) }()\n\tif len(w.contentMD5) > 0 {\n\t\t// Verify the MD5 hash of what was written matches the ContentMD5 provided\n\t\t// by the user.\n\t\tmd5sum := w.md5hash.Sum(nil)\n\t\tif !bytes.Equal(md5sum, w.contentMD5) {\n\t\t\t// No match! Return an error, but first cancel the context and call the\n\t\t\t// driver's Close function to ensure the write is aborted.\n\t\t\tw.cancel()\n\t\t\tif w.w != nil {\n\t\t\t\t_ = w.w.Close()\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"blob: the ContentMD5 you specified (%X) did not match what was written (%X)\", w.contentMD5, md5sum)\n\t\t}\n\t}\n\n\tdefer w.cancel()\n\tif w.w != nil {\n\t\treturn wrapError(w.b, w.w.Close())\n\t}\n\tif _, err := w.open(w.buf.Bytes()); err != nil {\n\t\treturn err\n\t}\n\treturn wrapError(w.b, w.w.Close())\n}\n\n// open tries to detect the MIME type of p and write it to the blob.\n// The error it returns is wrapped.\nfunc (w *Writer) open(p []byte) (int, error) {\n\tct := http.DetectContentType(p)\n\tvar err error\n\tif w.w, err = w.b.NewTypedWriter(w.ctx, w.key, ct, w.opts); err != nil {\n\t\treturn 0, wrapError(w.b, err)\n\t}\n\tw.buf = nil\n\tw.ctx = nil\n\tw.key = \"\"\n\tw.opts = nil\n\tn, err := w.w.Write(p)\n\treturn n, wrapError(w.b, err)\n}\n\n// ListOptions sets options for listing blobs via Bucket.List.\ntype ListOptions struct {\n\t// Prefix indicates that only blobs with a key starting with this prefix\n\t// should be returned.\n\tPrefix string\n\t// Delimiter sets the delimiter used to define a hierarchical namespace,\n\t// like a filesystem with \"directories\".\n\t//\n\t// An empty delimiter means that the bucket is treated as a single flat\n\t// namespace.\n\t//\n\t// A non-empty delimiter means that any result with the delimiter in its key\n\t// after Prefix is stripped will be returned with ListObject.IsDir = true,\n\t// ListObject.Key truncated after the delimiter, and zero values for other\n\t// ListObject fields. These results represent \"directories\". Multiple results\n\t// in a \"directory\" are returned as a single result.\n\tDelimiter string\n\n\t// BeforeList is a callback that will be called before each call to the\n\t// the underlying provider's list functionality.\n\t// asFunc converts its argument to provider-specific types.\n\t// See Bucket.As for more details.\n\tBeforeList func(asFunc func(interface{}) bool) error\n}\n\n// ListIterator iterates over List results.\ntype ListIterator struct {\n\tb       driver.Bucket\n\topts    *driver.ListOptions\n\tpage    *driver.ListPage\n\tnextIdx int\n}\n\n// Next returns a *ListObject for the next blob. It returns (nil, io.EOF) if\n// there are no more.\nfunc (i *ListIterator) Next(ctx context.Context) (*ListObject, error) {\n\tif i.page != nil {\n\t\t// We've already got a page of results.\n\t\tif i.nextIdx < len(i.page.Objects) {\n\t\t\t// Next object is in the page; return it.\n\t\t\tdobj := i.page.Objects[i.nextIdx]\n\t\t\ti.nextIdx++\n\t\t\treturn &ListObject{\n\t\t\t\tKey:     dobj.Key,\n\t\t\t\tModTime: dobj.ModTime,\n\t\t\t\tSize:    dobj.Size,\n\t\t\t\tMD5:     dobj.MD5,\n\t\t\t\tIsDir:   dobj.IsDir,\n\t\t\t\tasFunc:  dobj.AsFunc,\n\t\t\t}, nil\n\t\t}\n\t\tif len(i.page.NextPageToken) == 0 {\n\t\t\t// Done with current page, and there are no more; return io.EOF.\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\t// We need to load the next page.\n\t\ti.opts.PageToken = i.page.NextPageToken\n\t}\n\t// Loading a new page.\n\tp, err := i.b.ListPaged(ctx, i.opts)\n\tif err != nil {\n\t\treturn nil, wrapError(i.b, err)\n\t}\n\ti.page = p\n\ti.nextIdx = 0\n\treturn i.Next(ctx)\n}\n\n// ListObject represents a single blob returned from List.\ntype ListObject struct {\n\t// Key is the key for this blob.\n\tKey string\n\t// ModTime is the time the blob was last modified.\n\tModTime time.Time\n\t// Size is the size of the blob's content in bytes.\n\tSize int64\n\t// MD5 is an MD5 hash of the blob contents or nil if not available.\n\tMD5 []byte\n\t// IsDir indicates that this result represents a \"directory\" in the\n\t// hierarchical namespace, ending in ListOptions.Delimiter. Key can be\n\t// passed as ListOptions.Prefix to list items in the \"directory\".\n\t// Fields other than Key and IsDir will not be set if IsDir is true.\n\tIsDir bool\n\n\tasFunc func(interface{}) bool\n}\n\n// As converts i to provider-specific types.\n// See Bucket.As for more details.\nfunc (o *ListObject) As(i interface{}) bool {\n\tif o.asFunc == nil {\n\t\treturn false\n\t}\n\treturn o.asFunc(i)\n}\n\n// Bucket provides an easy and portable way to interact with blobs\n// within a \"bucket\", including read, write, and list operations.\n// To create a Bucket, use constructors found in provider-specific\n// subpackages.\ntype Bucket struct {\n\tb      driver.Bucket\n\ttracer *oc.Tracer\n}\n\nconst pkgName = \"gocloud.dev/blob\"\n\nvar (\n\tlatencyMeasure = oc.LatencyMeasure(pkgName)\n\n\t// OpenCensusViews are predefined views for OpenCensus metrics.\n\t// The views include counts and latency distributions for API method calls.\n\t// See the example at https://godoc.org/go.opencensus.io/stats/view for usage.\n\tOpenCensusViews = oc.Views(pkgName, latencyMeasure)\n)\n\n// NewBucket is intended for use by provider implementations.\nvar NewBucket = newBucket\n\n// newBucket creates a new *Bucket based on a specific driver implementation.\n// End users should use subpackages to construct a *Bucket instead of this\n// function; see the package documentation for details.\nfunc newBucket(b driver.Bucket) *Bucket {\n\treturn &Bucket{\n\t\tb: b,\n\t\ttracer: &oc.Tracer{\n\t\t\tPackage:        pkgName,\n\t\t\tProvider:       oc.ProviderName(b),\n\t\t\tLatencyMeasure: latencyMeasure,\n\t\t},\n\t}\n}\n\n// As converts i to provider-specific types.\n//\n// This function (and the other As functions in this package) are inherently\n// provider-specific, and using them will make that part of your application\n// non-portable, so use with care.\n//\n// See the documentation for the subpackage used to instantiate Bucket to see\n// which type(s) are supported.\n//\n// Usage:\n//\n// 1. Declare a variable of the provider-specific type you want to access.\n//\n// 2. Pass a pointer to it to As.\n//\n// 3. If the type is supported, As will return true and copy the\n// provider-specific type into your variable. Otherwise, it will return false.\n//\n// Provider-specific types that are intended to be mutable will be exposed\n// as a pointer to the underlying type.\n//\n// See\n// https://github.com/google/go-cloud/blob/master/internal/docs/design.md#as\n// for more background.\nfunc (b *Bucket) As(i interface{}) bool {\n\tif i == nil {\n\t\treturn false\n\t}\n\treturn b.b.As(i)\n}\n\n// ErrorAs converts i to provider-specific types.\n// ErrorAs panics if i is nil or not a pointer.\n// See Bucket.As for more details.\nfunc (b *Bucket) ErrorAs(err error, i interface{}) bool {\n\tif i == nil || reflect.TypeOf(i).Kind() != reflect.Ptr {\n\t\tpanic(\"blob: ErrorAs i must be a non-nil pointer\")\n\t}\n\tif e, ok := err.(*gcerr.Error); ok {\n\t\treturn b.b.ErrorAs(e.Unwrap(), i)\n\t}\n\treturn b.b.ErrorAs(err, i)\n}\n\n// ReadAll is a shortcut for creating a Reader via NewReader with nil\n// ReaderOptions, and reading the entire blob.\nfunc (b *Bucket) ReadAll(ctx context.Context, key string) (_ []byte, err error) {\n\tr, err := b.NewReader(ctx, key, nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer r.Close()\n\treturn ioutil.ReadAll(r)\n}\n\n// List returns a ListIterator that can be used to iterate over blobs in a\n// bucket, in lexicographical order of UTF-8 encoded keys. The underlying\n// implementation fetches results in pages.\n//\n// A nil ListOptions is treated the same as the zero value.\n//\n// List is not guaranteed to include all recently-written blobs;\n// some providers are only eventually consistent.\nfunc (b *Bucket) List(opts *ListOptions) *ListIterator {\n\tif opts == nil {\n\t\topts = &ListOptions{}\n\t}\n\tdopts := &driver.ListOptions{\n\t\tPrefix:     opts.Prefix,\n\t\tDelimiter:  opts.Delimiter,\n\t\tBeforeList: opts.BeforeList,\n\t}\n\treturn &ListIterator{b: b.b, opts: dopts}\n}\n\n// Attributes returns attributes for the blob stored at key.\n//\n// If the blob does not exist, Attributes returns an error for which\n// gcerrors.Code will return gcerrors.NotFound.\nfunc (b *Bucket) Attributes(ctx context.Context, key string) (_ Attributes, err error) {\n\tctx = b.tracer.Start(ctx, \"Attributes\")\n\tdefer func() { b.tracer.End(ctx, err) }()\n\n\ta, err := b.b.Attributes(ctx, key)\n\tif err != nil {\n\t\treturn Attributes{}, wrapError(b.b, err)\n\t}\n\tvar md map[string]string\n\tif len(a.Metadata) > 0 {\n\t\t// Providers are inconsistent, but at least some treat keys\n\t\t// as case-insensitive. To make the behavior consistent, we\n\t\t// force-lowercase them when writing and reading.\n\t\tmd = make(map[string]string, len(a.Metadata))\n\t\tfor k, v := range a.Metadata {\n\t\t\tmd[strings.ToLower(k)] = v\n\t\t}\n\t}\n\treturn Attributes{\n\t\tCacheControl:       a.CacheControl,\n\t\tContentDisposition: a.ContentDisposition,\n\t\tContentEncoding:    a.ContentEncoding,\n\t\tContentLanguage:    a.ContentLanguage,\n\t\tContentType:        a.ContentType,\n\t\tMetadata:           md,\n\t\tModTime:            a.ModTime,\n\t\tSize:               a.Size,\n\t\tMD5:                a.MD5,\n\t\tasFunc:             a.AsFunc,\n\t}, nil\n}\n\n// NewReader is a shortcut for NewRangedReader with offset=0 and length=-1.\nfunc (b *Bucket) NewReader(ctx context.Context, key string, opts *ReaderOptions) (*Reader, error) {\n\treturn b.NewRangeReader(ctx, key, 0, -1, opts)\n}\n\n// NewRangeReader returns a Reader to read content from the blob stored at key.\n// It reads at most length bytes starting at offset (>= 0).\n// If length is negative, it will read till the end of the blob.\n//\n// If the blob does not exist, NewRangeReader returns an error for which\n// gcerrors.Code will return gcerrors.NotFound. Attributes is a lighter-weight way to\n// check for existence.\n//\n// A nil ReaderOptions is treated the same as the zero value.\n//\n// The caller must call Close on the returned Reader when done reading.\nfunc (b *Bucket) NewRangeReader(ctx context.Context, key string, offset, length int64, opts *ReaderOptions) (_ *Reader, err error) {\n\tif offset < 0 {\n\t\treturn nil, errors.New(\"blob.NewRangeReader: offset must be non-negative\")\n\t}\n\tif opts == nil {\n\t\topts = &ReaderOptions{}\n\t}\n\tdopts := &driver.ReaderOptions{}\n\ttctx := b.tracer.Start(ctx, \"NewRangeReader\")\n\tdefer func() {\n\t\t// If err == nil, we handed the end closure off to the returned *Writer; it\n\t\t// will be called when the Writer is Closed.\n\t\tif err != nil {\n\t\t\tb.tracer.End(tctx, err)\n\t\t}\n\t}()\n\tr, err := b.b.NewRangeReader(ctx, key, offset, length, dopts)\n\tif err != nil {\n\t\treturn nil, wrapError(b.b, err)\n\t}\n\tend := func(err error) { b.tracer.End(tctx, err) }\n\treturn &Reader{b: b.b, r: r, end: end}, nil\n}\n\n// WriteAll is a shortcut for creating a Writer via NewWriter and writing p.\nfunc (b *Bucket) WriteAll(ctx context.Context, key string, p []byte, opts *WriterOptions) (err error) {\n\tw, err := b.NewWriter(ctx, key, opts)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err := w.Write(p); err != nil {\n\t\t_ = w.Close()\n\t\treturn err\n\t}\n\treturn w.Close()\n}\n\n// NewWriter returns a Writer that writes to the blob stored at key.\n// A nil WriterOptions is treated the same as the zero value.\n//\n// If a blob with this key already exists, it will be replaced.\n// The blob being written is not guaranteed to be readable until Close\n// has been called; until then, any previous blob will still be readable.\n// Even after Close is called, newly written blobs are not guaranteed to be\n// returned from List; some providers are only eventually consistent.\n//\n// The returned Writer will store ctx for later use in Write and/or Close.\n// To abort a write, cancel ctx; otherwise, it must remain open until\n// Close is called.\n//\n// The caller must call Close on the returned Writer, even if the write is\n// aborted.\nfunc (b *Bucket) NewWriter(ctx context.Context, key string, opts *WriterOptions) (_ *Writer, err error) {\n\tvar dopts *driver.WriterOptions\n\tvar w driver.Writer\n\tif opts == nil {\n\t\topts = &WriterOptions{}\n\t}\n\tdopts = &driver.WriterOptions{\n\t\tCacheControl:       opts.CacheControl,\n\t\tContentDisposition: opts.ContentDisposition,\n\t\tContentEncoding:    opts.ContentEncoding,\n\t\tContentLanguage:    opts.ContentLanguage,\n\t\tContentMD5:         opts.ContentMD5,\n\t\tBufferSize:         opts.BufferSize,\n\t\tBeforeWrite:        opts.BeforeWrite,\n\t}\n\tif len(opts.Metadata) > 0 {\n\t\t// Providers are inconsistent, but at least some treat keys\n\t\t// as case-insensitive. To make the behavior consistent, we\n\t\t// force-lowercase them when writing and reading.\n\t\tmd := make(map[string]string, len(opts.Metadata))\n\t\tfor k, v := range opts.Metadata {\n\t\t\tif k == \"\" {\n\t\t\t\treturn nil, errors.New(\"blob.NewWriter: WriterOptions.Metadata keys may not be empty strings\")\n\t\t\t}\n\t\t\tlowerK := strings.ToLower(k)\n\t\t\tif _, found := md[lowerK]; found {\n\t\t\t\treturn nil, fmt.Errorf(\"blob.NewWriter: duplicate case-insensitive metadata key %q\", lowerK)\n\t\t\t}\n\t\t\tmd[lowerK] = v\n\t\t}\n\t\tdopts.Metadata = md\n\t}\n\tctx, cancel := context.WithCancel(ctx)\n\ttctx := b.tracer.Start(ctx, \"NewWriter\")\n\tend := func(err error) { b.tracer.End(tctx, err) }\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tend(err)\n\t\t}\n\t}()\n\n\tif opts.ContentType != \"\" {\n\t\tt, p, err := mime.ParseMediaType(opts.ContentType)\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t\treturn nil, err\n\t\t}\n\t\tct := mime.FormatMediaType(t, p)\n\t\tw, err = b.b.NewTypedWriter(ctx, key, ct, dopts)\n\t\tif err != nil {\n\t\t\tcancel()\n\t\t\treturn nil, wrapError(b.b, err)\n\t\t}\n\t\treturn &Writer{\n\t\t\tb:          b.b,\n\t\t\tw:          w,\n\t\t\tend:        end,\n\t\t\tcancel:     cancel,\n\t\t\tcontentMD5: opts.ContentMD5,\n\t\t\tmd5hash:    md5.New(),\n\t\t}, nil\n\t}\n\treturn &Writer{\n\t\tctx:        ctx,\n\t\tcancel:     cancel,\n\t\tb:          b.b,\n\t\tend:        end,\n\t\tkey:        key,\n\t\topts:       dopts,\n\t\tbuf:        bytes.NewBuffer([]byte{}),\n\t\tcontentMD5: opts.ContentMD5,\n\t\tmd5hash:    md5.New(),\n\t}, nil\n}\n\n// Delete deletes the blob stored at key.\n//\n// If the blob does not exist, Delete returns an error for which\n// gcerrors.Code will return gcerrors.NotFound.\nfunc (b *Bucket) Delete(ctx context.Context, key string) (err error) {\n\tctx = b.tracer.Start(ctx, \"Delete\")\n\tdefer func() { b.tracer.End(ctx, err) }()\n\treturn wrapError(b.b, b.b.Delete(ctx, key))\n}\n\n// SignedURL returns a URL that can be used to GET the blob for the duration\n// specified in opts.Expiry.\n//\n// A nil SignedURLOptions is treated the same as the zero value.\n//\n// It is valid to call SignedURL for a key that does not exist.\n//\n// If the provider implementation does not support this functionality, SignedURL\n// will return an error for which gcerrors.Code will return gcerrors.Unimplemented.\nfunc (b *Bucket) SignedURL(ctx context.Context, key string, opts *SignedURLOptions) (string, error) {\n\tif opts == nil {\n\t\topts = &SignedURLOptions{}\n\t}\n\tif opts.Expiry < 0 {\n\t\treturn \"\", errors.New(\"blob.SignedURL: SignedURLOptions.Expiry must be >= 0\")\n\t}\n\tif opts.Expiry == 0 {\n\t\topts.Expiry = DefaultSignedURLExpiry\n\t}\n\tdopts := driver.SignedURLOptions{\n\t\tExpiry: opts.Expiry,\n\t}\n\turl, err := b.b.SignedURL(ctx, key, &dopts)\n\treturn url, wrapError(b.b, err)\n}\n\n// DefaultSignedURLExpiry is the default duration for SignedURLOptions.Expiry.\nconst DefaultSignedURLExpiry = 1 * time.Hour\n\n// SignedURLOptions sets options for SignedURL.\ntype SignedURLOptions struct {\n\t// Expiry sets how long the returned URL is valid for.\n\t// Defaults to DefaultSignedURLExpiry.\n\tExpiry time.Duration\n}\n\n// ReaderOptions sets options for NewReader and NewRangedReader.\n// It is provided for future extensibility.\ntype ReaderOptions struct{}\n\n// WriterOptions sets options for NewWriter.\ntype WriterOptions struct {\n\t// BufferSize changes the default size in bytes of the chunks that\n\t// Writer will upload in a single request; larger blobs will be split into\n\t// multiple requests.\n\t//\n\t// This option may be ignored by some provider implementations.\n\t//\n\t// If 0, the provider implementation will choose a reasonable default.\n\t//\n\t// If the Writer is used to do many small writes concurrently, using a\n\t// smaller BufferSize may reduce memory usage.\n\tBufferSize int\n\n\t// CacheControl specifies caching attributes that providers may use\n\t// when serving the blob.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control\n\tCacheControl string\n\n\t// ContentDisposition specifies whether the blob content is expected to be\n\t// displayed inline or as an attachment.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Disposition\n\tContentDisposition string\n\n\t// ContentEncoding specifies the encoding used for the blob's content, if any.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding\n\tContentEncoding string\n\n\t// ContentLanguage specifies the language used in the blob's content, if any.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Language\n\tContentLanguage string\n\n\t// ContentType specifies the MIME type of the blob being written. If not set,\n\t// it will be inferred from the content using the algorithm described at\n\t// http://mimesniff.spec.whatwg.org/.\n\t// https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Type\n\tContentType string\n\n\t// ContentMD5 is used as a message integrity check.\n\t// If len(ContentMD5) > 0, the MD5 hash of the bytes written must match\n\t// ContentMD5, or Close will return an error without completing the write.\n\t// https://tools.ietf.org/html/rfc1864\n\tContentMD5 []byte\n\n\t// Metadata holds key/value strings to be associated with the blob, or nil.\n\t// Keys may not be empty, and are lowercased before being written.\n\t// Duplicate case-insensitive keys (e.g., \"foo\" and \"FOO\") will result in\n\t// an error.\n\tMetadata map[string]string\n\n\t// BeforeWrite is a callback that will be called exactly once, before\n\t// any data is written (unless NewWriter returns an error, in which case\n\t// it will not be called at all). Note that this is not necessarily during\n\t// or after the first Write call, as providers may buffer bytes before\n\t// sending an upload request.\n\t//\n\t// asFunc converts its argument to provider-specific types.\n\t// See Bucket.As for more details.\n\tBeforeWrite func(asFunc func(interface{}) bool) error\n}\n\n// FromURLFunc is intended for use by provider implementations.\n// It allows providers to convert a parsed URL from Open to a driver.Bucket.\ntype FromURLFunc func(context.Context, *url.URL) (driver.Bucket, error)\n\nvar (\n\t// registry maps scheme strings to provider-specific instantiation functions.\n\tregistry = map[string]FromURLFunc{}\n\t// registryMu protected registry.\n\tregistryMu sync.Mutex\n)\n\n// Register is for use by provider implementations. It allows providers to\n// register an instantiation function for URLs with the given scheme. It is\n// expected to be called from the provider implementation's package init\n// function.\n//\n// fn will be called from Open, with a bucket name and options parsed from\n// the URL. All option keys will be lowercased.\n//\n// Register panics if a provider has already registered for scheme.\nfunc Register(scheme string, fn FromURLFunc) {\n\tregistryMu.Lock()\n\tdefer registryMu.Unlock()\n\n\tif _, found := registry[scheme]; found {\n\t\tlog.Fatalf(\"a provider has already registered for scheme %q\", scheme)\n\t}\n\tregistry[scheme] = fn\n}\n\n// fromRegistry looks up the registered function for scheme.\n// It returns nil if scheme has not been registered for.\nfunc fromRegistry(scheme string) FromURLFunc {\n\tregistryMu.Lock()\n\tdefer registryMu.Unlock()\n\n\treturn registry[scheme]\n}\n\n// Open creates a *Bucket from a URL.\n// See the package documentation in provider-specific subpackages for more\n// details on supported scheme(s) and URL parameter(s).\nfunc Open(ctx context.Context, urlstr string) (*Bucket, error) {\n\tu, err := url.Parse(urlstr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif u.Scheme == \"\" {\n\t\treturn nil, fmt.Errorf(\"invalid URL %q, missing scheme\", urlstr)\n\t}\n\tfn := fromRegistry(u.Scheme)\n\tif fn == nil {\n\t\treturn nil, fmt.Errorf(\"no provider registered for scheme %q\", u.Scheme)\n\t}\n\tdrv, err := fn(ctx, u)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn NewBucket(drv), nil\n}\n\nfunc wrapError(b driver.Bucket, err error) error {\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif gcerr.DoNotWrap(err) {\n\t\treturn err\n\t}\n\treturn gcerr.New(b.ErrorCode(err), err, 2, \"blob\")\n}\n", "idx": 10, "id": 14205, "msg": "", "proj": "google-go-cloud", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -571,13 +571,17 @@ class L3PacketSocket(L2Socket):\n         sx = raw(ll(x))\n         x.sent_time = time.time()\n         try:\n-            self.outs.sendto(sx, sdto)\n+            return self.outs.sendto(sx, sdto)\n         except socket.error as msg:\n             if msg.errno == 22 and len(sx) < conf.min_pkt_size:\n-                self.outs.send(sx + b\"\\x00\" * (conf.min_pkt_size - len(sx)))\n+                return self.outs.send(\n+                    sx + b\"\\x00\" * (conf.min_pkt_size - len(sx))\n+                )\n             elif conf.auto_fragment and msg.errno == 90:\n+                i = 0\n                 for p in x.fragment():\n-                    self.outs.sendto(raw(ll(p)), sdto)\n+                    i += self.outs.sendto(raw(ll(p)), sdto)\n+                return i\n             else:\n                 raise\n ", "y": 0, "oldf": "# This file is part of Scapy\n# See http://www.secdev.org/projects/scapy for more information\n# Copyright (C) Philippe Biondi <phil@secdev.org>\n# This program is published under a GPLv2 license\n\n\"\"\"\nLinux specific functions.\n\"\"\"\n\nfrom __future__ import absolute_import\n\n\nimport array\nimport ctypes\nfrom fcntl import ioctl\nimport os\nfrom select import select\nimport socket\nimport struct\nimport sys\nimport time\n\nimport subprocess\n\nfrom scapy.compat import raw, plain_str\nfrom scapy.consts import LINUX\nimport scapy.utils\nimport scapy.utils6\nfrom scapy.arch.common import get_if, compile_filter, _iff_flags\nfrom scapy.config import conf\nfrom scapy.data import MTU, ETH_P_ALL, SOL_PACKET, SO_ATTACH_FILTER, \\\n    SO_TIMESTAMPNS\nfrom scapy.error import (\n    ScapyInvalidPlatformException,\n    Scapy_Exception,\n    log_loading,\n    log_runtime,\n    warning,\n)\nfrom scapy.interfaces import IFACES, InterfaceProvider, NetworkInterface, \\\n    network_name\nfrom scapy.libs.structures import sock_fprog\nfrom scapy.packet import Packet, Padding\nfrom scapy.pton_ntop import inet_ntop\nfrom scapy.supersocket import SuperSocket\n\nimport scapy.modules.six as six\nfrom scapy.modules.six.moves import range\n\nfrom scapy.arch.common import get_if_raw_hwaddr  # noqa: F401\n\n# From bits/ioctls.h\nSIOCGIFHWADDR = 0x8927          # Get hardware address\nSIOCGIFADDR = 0x8915          # get PA address\nSIOCGIFNETMASK = 0x891b          # get network PA mask\nSIOCGIFNAME = 0x8910          # get iface name\nSIOCSIFLINK = 0x8911          # set iface channel\nSIOCGIFCONF = 0x8912          # get iface list\nSIOCGIFFLAGS = 0x8913          # get flags\nSIOCSIFFLAGS = 0x8914          # set flags\nSIOCGIFINDEX = 0x8933          # name -> if_index mapping\nSIOCGIFCOUNT = 0x8938          # get number of devices\nSIOCGSTAMP = 0x8906          # get packet timestamp (as a timeval)\n\n# From if.h\nIFF_UP = 0x1               # Interface is up.\nIFF_BROADCAST = 0x2        # Broadcast address valid.\nIFF_DEBUG = 0x4            # Turn on debugging.\nIFF_LOOPBACK = 0x8         # Is a loopback net.\nIFF_POINTOPOINT = 0x10     # Interface is point-to-point link.\nIFF_NOTRAILERS = 0x20      # Avoid use of trailers.\nIFF_RUNNING = 0x40         # Resources allocated.\nIFF_NOARP = 0x80           # No address resolution protocol.\nIFF_PROMISC = 0x100        # Receive all packets.\n\n# From netpacket/packet.h\nPACKET_ADD_MEMBERSHIP = 1\nPACKET_DROP_MEMBERSHIP = 2\nPACKET_RECV_OUTPUT = 3\nPACKET_RX_RING = 5\nPACKET_STATISTICS = 6\nPACKET_MR_MULTICAST = 0\nPACKET_MR_PROMISC = 1\nPACKET_MR_ALLMULTI = 2\n\n# From net/route.h\nRTF_UP = 0x0001  # Route usable\nRTF_REJECT = 0x0200\n\n# From if_packet.h\nPACKET_HOST = 0  # To us\nPACKET_BROADCAST = 1  # To all\nPACKET_MULTICAST = 2  # To group\nPACKET_OTHERHOST = 3  # To someone else\nPACKET_OUTGOING = 4  # Outgoing of any type\nPACKET_LOOPBACK = 5  # MC/BRD frame looped back\nPACKET_USER = 6  # To user space\nPACKET_KERNEL = 7  # To kernel space\nPACKET_AUXDATA = 8\nPACKET_FASTROUTE = 6  # Fastrouted frame\n# Unused, PACKET_FASTROUTE and PACKET_LOOPBACK are invisible to user space\n\n# Utils\n\n\ndef get_if_raw_addr(iff):\n    r\"\"\"\n    Return the raw IPv4 address of an interface.\n    If unavailable, returns b\"\\0\\0\\0\\0\"\n    \"\"\"\n    try:\n        return get_if(iff, SIOCGIFADDR)[20:24]\n    except IOError:\n        return b\"\\0\\0\\0\\0\"\n\n\ndef _get_if_list():\n    \"\"\"\n    Function to read the interfaces from /proc/net/dev\n    \"\"\"\n    try:\n        f = open(\"/proc/net/dev\", \"rb\")\n    except IOError:\n        try:\n            f.close()\n        except Exception:\n            pass\n        log_loading.critical(\"Can't open /proc/net/dev !\")\n        return []\n    lst = []\n    f.readline()\n    f.readline()\n    for line in f:\n        line = plain_str(line)\n        lst.append(line.split(\":\")[0].strip())\n    f.close()\n    return lst\n\n\ndef attach_filter(sock, bpf_filter, iface):\n    \"\"\"\n    Compile bpf filter and attach it to a socket\n\n    :param sock: the python socket\n    :param bpf_filter: the bpf string filter to compile\n    :param iface: the interface used to compile\n    \"\"\"\n    bp = compile_filter(bpf_filter, iface)\n    if conf.use_pypy and sys.pypy_version_info <= (7, 3, 2):\n        # PyPy < 7.3.2 has a broken behavior\n        # https://foss.heptapod.net/pypy/pypy/-/issues/3298\n        bp = struct.pack(\n            'HL',\n            bp.bf_len, ctypes.addressof(bp.bf_insns.contents)\n        )\n    else:\n        bp = sock_fprog(bp.bf_len, bp.bf_insns)\n    sock.setsockopt(socket.SOL_SOCKET, SO_ATTACH_FILTER, bp)\n\n\ndef set_promisc(s, iff, val=1):\n    mreq = struct.pack(\"IHH8s\", get_if_index(iff), PACKET_MR_PROMISC, 0, b\"\")\n    if val:\n        cmd = PACKET_ADD_MEMBERSHIP\n    else:\n        cmd = PACKET_DROP_MEMBERSHIP\n    s.setsockopt(SOL_PACKET, cmd, mreq)\n\n\ndef get_alias_address(iface_name, ip_mask, gw_str, metric):\n    \"\"\"\n    Get the correct source IP address of an interface alias\n    \"\"\"\n\n    # Detect the architecture\n    if scapy.consts.IS_64BITS:\n        offset, name_len = 16, 40\n    else:\n        offset, name_len = 32, 32\n\n    # Retrieve interfaces structures\n    sck = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    names = array.array('B', b'\\0' * 4096)\n    ifreq = ioctl(sck.fileno(), SIOCGIFCONF,\n                  struct.pack(\"iL\", len(names), names.buffer_info()[0]))\n\n    # Extract interfaces names\n    out = struct.unpack(\"iL\", ifreq)[0]\n    names = names.tobytes() if six.PY3 else names.tostring()\n    names = [names[i:i + offset].split(b'\\0', 1)[0] for i in range(0, out, name_len)]  # noqa: E501\n\n    # Look for the IP address\n    for ifname in names:\n        # Only look for a matching interface name\n        if not ifname.decode(\"utf8\").startswith(iface_name):\n            continue\n\n        # Retrieve and convert addresses\n        ifreq = ioctl(sck, SIOCGIFADDR, struct.pack(\"16s16x\", ifname))\n        ifaddr = struct.unpack(\">I\", ifreq[20:24])[0]\n        ifreq = ioctl(sck, SIOCGIFNETMASK, struct.pack(\"16s16x\", ifname))\n        msk = struct.unpack(\">I\", ifreq[20:24])[0]\n\n        # Get the full interface name\n        ifname = plain_str(ifname)\n        if ':' in ifname:\n            ifname = ifname[:ifname.index(':')]\n        else:\n            continue\n\n        # Check if the source address is included in the network\n        if (ifaddr & msk) == ip_mask:\n            sck.close()\n            return (ifaddr & msk, msk, gw_str, ifname,\n                    scapy.utils.ltoa(ifaddr), metric)\n\n    sck.close()\n    return\n\n\ndef read_routes():\n    try:\n        f = open(\"/proc/net/route\", \"rb\")\n    except IOError:\n        log_loading.critical(\"Can't open /proc/net/route !\")\n        return []\n    routes = []\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        ifreq = ioctl(s, SIOCGIFADDR, struct.pack(\"16s16x\", conf.loopback_name.encode(\"utf8\")))  # noqa: E501\n        addrfamily = struct.unpack(\"h\", ifreq[16:18])[0]\n        if addrfamily == socket.AF_INET:\n            ifreq2 = ioctl(s, SIOCGIFNETMASK, struct.pack(\"16s16x\", conf.loopback_name.encode(\"utf8\")))  # noqa: E501\n            msk = socket.ntohl(struct.unpack(\"I\", ifreq2[20:24])[0])\n            dst = socket.ntohl(struct.unpack(\"I\", ifreq[20:24])[0]) & msk\n            ifaddr = scapy.utils.inet_ntoa(ifreq[20:24])\n            routes.append((dst, msk, \"0.0.0.0\", conf.loopback_name, ifaddr, 1))  # noqa: E501\n        else:\n            warning(\"Interface %s: unknown address family (%i)\" % (conf.loopback_name, addrfamily))  # noqa: E501\n    except IOError as err:\n        if err.errno == 99:\n            warning(\"Interface %s: no address assigned\" % conf.loopback_name)  # noqa: E501\n        else:\n            warning(\"Interface %s: failed to get address config (%s)\" % (conf.loopback_name, str(err)))  # noqa: E501\n\n    for line in f.readlines()[1:]:\n        line = plain_str(line)\n        iff, dst, gw, flags, _, _, metric, msk, _, _, _ = line.split()\n        flags = int(flags, 16)\n        if flags & RTF_UP == 0:\n            continue\n        if flags & RTF_REJECT:\n            continue\n        try:\n            ifreq = ioctl(s, SIOCGIFADDR, struct.pack(\"16s16x\", iff.encode(\"utf8\")))  # noqa: E501\n        except IOError:  # interface is present in routing tables but does not have any assigned IP  # noqa: E501\n            ifaddr = \"0.0.0.0\"\n            ifaddr_int = 0\n        else:\n            addrfamily = struct.unpack(\"h\", ifreq[16:18])[0]\n            if addrfamily == socket.AF_INET:\n                ifaddr = scapy.utils.inet_ntoa(ifreq[20:24])\n                ifaddr_int = struct.unpack(\"!I\", ifreq[20:24])[0]\n            else:\n                warning(\"Interface %s: unknown address family (%i)\", iff, addrfamily)  # noqa: E501\n                continue\n\n        # Attempt to detect an interface alias based on addresses inconsistencies  # noqa: E501\n        dst_int = socket.htonl(int(dst, 16)) & 0xffffffff\n        msk_int = socket.htonl(int(msk, 16)) & 0xffffffff\n        gw_str = scapy.utils.inet_ntoa(struct.pack(\"I\", int(gw, 16)))\n        metric = int(metric)\n\n        route = [dst_int, msk_int, gw_str, iff, ifaddr, metric]\n        if ifaddr_int & msk_int != dst_int:\n            tmp_route = get_alias_address(iff, dst_int, gw_str, metric)\n            if tmp_route:\n                route = tmp_route\n        routes.append(tuple(route))\n\n    f.close()\n    s.close()\n    return routes\n\n############\n#   IPv6   #\n############\n\n\ndef in6_getifaddr():\n    \"\"\"\n    Returns a list of 3-tuples of the form (addr, scope, iface) where\n    'addr' is the address of scope 'scope' associated to the interface\n    'iface'.\n\n    This is the list of all addresses of all interfaces available on\n    the system.\n    \"\"\"\n    ret = []\n    try:\n        fdesc = open(\"/proc/net/if_inet6\", \"rb\")\n    except IOError:\n        return ret\n    for line in fdesc:\n        # addr, index, plen, scope, flags, ifname\n        tmp = plain_str(line).split()\n        addr = scapy.utils6.in6_ptop(\n            b':'.join(\n                struct.unpack('4s4s4s4s4s4s4s4s', tmp[0].encode())\n            ).decode()\n        )\n        # (addr, scope, iface)\n        ret.append((addr, int(tmp[3], 16), tmp[5]))\n    fdesc.close()\n    return ret\n\n\ndef read_routes6():\n    try:\n        f = open(\"/proc/net/ipv6_route\", \"rb\")\n    except IOError:\n        return []\n    # 1. destination network\n    # 2. destination prefix length\n    # 3. source network displayed\n    # 4. source prefix length\n    # 5. next hop\n    # 6. metric\n    # 7. reference counter (?!?)\n    # 8. use counter (?!?)\n    # 9. flags\n    # 10. device name\n    routes = []\n\n    def proc2r(p):\n        ret = struct.unpack('4s4s4s4s4s4s4s4s', p)\n        ret = b':'.join(ret).decode()\n        return scapy.utils6.in6_ptop(ret)\n\n    lifaddr = in6_getifaddr()\n    for line in f.readlines():\n        d, dp, _, _, nh, metric, rc, us, fl, dev = line.split()\n        metric = int(metric, 16)\n        fl = int(fl, 16)\n        dev = plain_str(dev)\n\n        if fl & RTF_UP == 0:\n            continue\n        if fl & RTF_REJECT:\n            continue\n\n        d = proc2r(d)\n        dp = int(dp, 16)\n        nh = proc2r(nh)\n\n        cset = []  # candidate set (possible source addresses)\n        if dev == conf.loopback_name:\n            if d == '::':\n                continue\n            cset = ['::1']\n        else:\n            devaddrs = (x for x in lifaddr if x[2] == dev)\n            cset = scapy.utils6.construct_source_candidate_set(d, dp, devaddrs)\n\n        if len(cset) != 0:\n            routes.append((d, dp, nh, dev, cset, metric))\n    f.close()\n    return routes\n\n\ndef get_if_index(iff):\n    return int(struct.unpack(\"I\", get_if(iff, SIOCGIFINDEX)[16:20])[0])\n\n\nclass LinuxInterfaceProvider(InterfaceProvider):\n    name = \"sys\"\n\n    def _is_valid(self, dev):\n        return bool(dev.flags & IFF_UP)\n\n    def load(self):\n        from scapy.fields import FlagValue\n        data = {}\n        ips = in6_getifaddr()\n        for i in _get_if_list():\n            ifflags = struct.unpack(\"16xH14x\", get_if(i, SIOCGIFFLAGS))[0]\n            index = get_if_index(i)\n            mac = scapy.utils.str2mac(\n                get_if_raw_hwaddr(i, siocgifhwaddr=SIOCGIFHWADDR)[1]\n            )\n            ip = inet_ntop(socket.AF_INET, get_if_raw_addr(i))\n            if ip == \"0.0.0.0\":\n                ip = None\n            ifflags = FlagValue(ifflags, _iff_flags)\n            if_data = {\n                \"name\": i,\n                \"network_name\": i,\n                \"description\": i,\n                \"flags\": ifflags,\n                \"index\": index,\n                \"ip\": ip,\n                \"ips\": [x[0] for x in ips if x[2] == i] + [ip] if ip else [],\n                \"mac\": mac\n            }\n            data[i] = NetworkInterface(self, if_data)\n        return data\n\n\nIFACES.register_provider(LinuxInterfaceProvider)\n\nif os.uname()[4] in ['x86_64', 'aarch64']:\n    def get_last_packet_timestamp(sock):\n        ts = ioctl(sock, SIOCGSTAMP, \"1234567890123456\")\n        s, us = struct.unpack(\"QQ\", ts)\n        return s + us / 1000000.0\nelse:\n    def get_last_packet_timestamp(sock):\n        ts = ioctl(sock, SIOCGSTAMP, \"12345678\")\n        s, us = struct.unpack(\"II\", ts)\n        return s + us / 1000000.0\n\n\ndef _flush_fd(fd):\n    if hasattr(fd, 'fileno'):\n        fd = fd.fileno()\n    while True:\n        r, w, e = select([fd], [], [], 0)\n        if r:\n            os.read(fd, MTU)\n        else:\n            break\n\n\nclass L2Socket(SuperSocket):\n    desc = \"read/write packets at layer 2 using Linux PF_PACKET sockets\"\n\n    def __init__(self, iface=None, type=ETH_P_ALL, promisc=None, filter=None,\n                 nofilter=0, monitor=None):\n        self.iface = network_name(iface or conf.iface)\n        self.type = type\n        self.promisc = conf.sniff_promisc if promisc is None else promisc\n        if monitor is not None:\n            log_runtime.info(\n                \"The 'monitor' argument has no effect on native linux sockets.\"\n            )\n        self.ins = socket.socket(\n            socket.AF_PACKET, socket.SOCK_RAW, socket.htons(type))\n        self.ins.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 0)\n        if not nofilter:\n            if conf.except_filter:\n                if filter:\n                    filter = \"(%s) and not (%s)\" % (filter, conf.except_filter)\n                else:\n                    filter = \"not (%s)\" % conf.except_filter\n            if filter is not None:\n                try:\n                    attach_filter(self.ins, filter, iface)\n                except ImportError as ex:\n                    log_runtime.error(\"Cannot set filter: %s\", ex)\n        if self.promisc:\n            set_promisc(self.ins, self.iface)\n        self.ins.bind((self.iface, type))\n        _flush_fd(self.ins)\n        self.ins.setsockopt(\n            socket.SOL_SOCKET,\n            socket.SO_RCVBUF,\n            conf.bufsize\n        )\n        if not six.PY2:\n            # Receive Auxiliary Data (VLAN tags)\n            try:\n                self.ins.setsockopt(SOL_PACKET, PACKET_AUXDATA, 1)\n                self.ins.setsockopt(\n                    socket.SOL_SOCKET,\n                    SO_TIMESTAMPNS,\n                    1\n                )\n                self.auxdata_available = True\n            except OSError:\n                # Note: Auxiliary Data is only supported since\n                #       Linux 2.6.21\n                msg = \"Your Linux Kernel does not support Auxiliary Data!\"\n                log_runtime.info(msg)\n        if isinstance(self, L2ListenSocket):\n            self.outs = None\n        else:\n            self.outs = self.ins\n            self.outs.setsockopt(\n                socket.SOL_SOCKET,\n                socket.SO_SNDBUF,\n                conf.bufsize\n            )\n        sa_ll = self.ins.getsockname()\n        if sa_ll[3] in conf.l2types:\n            self.LL = conf.l2types[sa_ll[3]]\n            self.lvl = 2\n        elif sa_ll[1] in conf.l3types:\n            self.LL = conf.l3types[sa_ll[1]]\n            self.lvl = 3\n        else:\n            self.LL = conf.default_l2\n            self.lvl = 2\n            warning(\"Unable to guess type (interface=%s protocol=%#x family=%i). Using %s\", sa_ll[0], sa_ll[1], sa_ll[3], self.LL.name)  # noqa: E501\n\n    def close(self):\n        if self.closed:\n            return\n        try:\n            if self.promisc and self.ins:\n                set_promisc(self.ins, self.iface, 0)\n        except (AttributeError, OSError):\n            pass\n        SuperSocket.close(self)\n\n    def recv_raw(self, x=MTU):\n        \"\"\"Receives a packet, then returns a tuple containing (cls, pkt_data, time)\"\"\"  # noqa: E501\n        pkt, sa_ll, ts = self._recv_raw(self.ins, x)\n        if self.outs and sa_ll[2] == socket.PACKET_OUTGOING:\n            return None, None, None\n        if ts is None:\n            ts = get_last_packet_timestamp(self.ins)\n        return self.LL, pkt, ts\n\n    def send(self, x):\n        try:\n            return SuperSocket.send(self, x)\n        except socket.error as msg:\n            if msg.errno == 22 and len(x) < conf.min_pkt_size:\n                padding = b\"\\x00\" * (conf.min_pkt_size - len(x))\n                if isinstance(x, Packet):\n                    return SuperSocket.send(self, x / Padding(load=padding))\n                else:\n                    return SuperSocket.send(self, raw(x) + padding)\n            raise\n\n\nclass L2ListenSocket(L2Socket):\n    desc = \"read packets at layer 2 using Linux PF_PACKET sockets. Also receives the packets going OUT\"  # noqa: E501\n\n    def send(self, x):\n        raise Scapy_Exception(\"Can't send anything with L2ListenSocket\")\n\n\nclass L3PacketSocket(L2Socket):\n    desc = \"read/write packets at layer 3 using Linux PF_PACKET sockets\"\n\n    def recv(self, x=MTU):\n        pkt = SuperSocket.recv(self, x)\n        if pkt and self.lvl == 2:\n            pkt.payload.time = pkt.time\n            return pkt.payload\n        return pkt\n\n    def send(self, x):\n        iff = x.route()[0]\n        if iff is None:\n            iff = conf.iface\n        sdto = (iff, self.type)\n        self.outs.bind(sdto)\n        sn = self.outs.getsockname()\n        ll = lambda x: x\n        type_x = type(x)\n        if type_x in conf.l3types:\n            sdto = (iff, conf.l3types[type_x])\n        if sn[3] in conf.l2types:\n            ll = lambda x: conf.l2types[sn[3]]() / x\n        if self.lvl == 3 and type_x != self.LL:\n            warning(\"Incompatible L3 types detected using %s instead of %s !\",\n                    type_x, self.LL)\n            self.LL = type_x\n        sx = raw(ll(x))\n        x.sent_time = time.time()\n        try:\n            self.outs.sendto(sx, sdto)\n        except socket.error as msg:\n            if msg.errno == 22 and len(sx) < conf.min_pkt_size:\n                self.outs.send(sx + b\"\\x00\" * (conf.min_pkt_size - len(sx)))\n            elif conf.auto_fragment and msg.errno == 90:\n                for p in x.fragment():\n                    self.outs.sendto(raw(ll(p)), sdto)\n            else:\n                raise\n\n\nclass VEthPair(object):\n    \"\"\"\n    encapsulates a virtual Ethernet interface pair\n    \"\"\"\n\n    def __init__(self, iface_name, peer_name):\n\n        if not LINUX:\n            # ToDo: do we need a kernel version check here?\n            raise ScapyInvalidPlatformException(\n                'Virtual Ethernet interface pair only available on Linux'\n            )\n\n        self.ifaces = [iface_name, peer_name]\n\n    def iface(self):\n        return self.ifaces[0]\n\n    def peer(self):\n        return self.ifaces[1]\n\n    def setup(self):\n        \"\"\"\n        create veth pair links\n        :raises subprocess.CalledProcessError if operation fails\n        \"\"\"\n        subprocess.check_call(['ip', 'link', 'add', self.ifaces[0], 'type', 'veth', 'peer', 'name', self.ifaces[1]])  # noqa: E501\n\n    def destroy(self):\n        \"\"\"\n        remove veth pair links\n        :raises subprocess.CalledProcessError if operation fails\n        \"\"\"\n        subprocess.check_call(['ip', 'link', 'del', self.ifaces[0]])\n\n    def up(self):\n        \"\"\"\n        set veth pair links up\n        :raises subprocess.CalledProcessError if operation fails\n        \"\"\"\n        for idx in [0, 1]:\n            subprocess.check_call([\"ip\", \"link\", \"set\", self.ifaces[idx], \"up\"])  # noqa: E501\n\n    def down(self):\n        \"\"\"\n        set veth pair links down\n        :raises subprocess.CalledProcessError if operation fails\n        \"\"\"\n        for idx in [0, 1]:\n            subprocess.check_call([\"ip\", \"link\", \"set\", self.ifaces[idx], \"down\"])  # noqa: E501\n\n    def __enter__(self):\n        self.setup()\n        self.up()\n        conf.ifaces.reload()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.destroy()\n        conf.ifaces.reload()\n", "idx": 30, "id": 18696, "msg": "", "proj": "secdev-scapy", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -148,6 +148,43 @@ import (\n //    return syscall(SYS_bpf, cmd, &attr, sizeof(attr)) == 0 ? 0 : errno;\n // }\n //\n+// int bpf_map_load_multi(__u32 map_fd,\n+//                        void *current_key,\n+//                        int max_num,\n+//                        int key_stride,\n+//                        void *keys_out,\n+//                        int value_stride,\n+//                        void *values_out) {\n+//    int count = 0;\n+//    union bpf_attr attr = {};\n+//    attr.map_fd = map_fd;\n+//    attr.key = (__u64)(unsigned long)current_key;\n+//    for (int i = 0; i < max_num; i++) {\n+//      // Load the next key from the map.\n+//      attr.value = (__u64)(unsigned long)keys_out;\n+//      int rc = syscall(SYS_bpf, BPF_MAP_GET_NEXT_KEY, &attr, sizeof(attr));\n+//      if (rc != 0) {\n+//        if (errno == ENOENT) {\n+//          return count; // Reached end of map.\n+//        }\n+//        return -errno;\n+//      }\n+//      // Load the corresponding value.\n+//      attr.key = (__u64)(unsigned long)keys_out;\n+//      attr.value = (__u64)(unsigned long)values_out;\n+//\n+//      rc = syscall(SYS_bpf, BPF_MAP_LOOKUP_ELEM, &attr, sizeof(attr));\n+//      if (rc != 0) {\n+//        return -errno;\n+//      }\n+//\n+//      keys_out+=key_stride;\n+//      values_out+=value_stride;\n+//      count++;\n+//    }\n+//    return count;\n+// }\n+//\n import \"C\"\n \n func SyscallSupport() bool {", "y": 0, "oldf": "// Copyright (c) 2019-2020 Tigera, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage bpf\n\nimport (\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\t\"unsafe\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\n\t\"github.com/projectcalico/felix/bpf/asm\"\n\n\t\"golang.org/x/sys/unix\"\n)\n\n// #include <linux/bpf.h>\n// #include <stdlib.h>\n// #include <string.h>\n// #include <errno.h>\n// #include <unistd.h>\n// #include <sys/syscall.h>\n//\n// union bpf_attr *bpf_attr_alloc() {\n//    union bpf_attr *attr = malloc(sizeof(union bpf_attr));\n//    memset(attr, 0, sizeof(union bpf_attr));\n//    return attr;\n// }\n//\n// // bpf_attr_setup_obj_get sets up the bpf_attr union for use with BPF_OBJ_GET.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_obj_get(union bpf_attr *attr, char *path, __u32 flags) {\n//    attr->pathname = (__u64)(unsigned long)path;\n//    attr->bpf_fd = 0;\n//    attr->file_flags = flags;\n// }\n//\n// // bpf_attr_setup_obj_get_id sets up the bpf_attr union for use with BPF_XXX_GET_FD_BY_ID.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_obj_get_id(union bpf_attr *attr, __u32 id, __u32 flags) {\n//    attr->map_id = id;\n//    attr->open_flags = flags;\n// }\n//\n// // bpf_attr_setup_obj_pin sets up the bpf_attr union for use with BPF_OBJ_PIN.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_obj_pin(union bpf_attr *attr, char *path, __u32 fd, __u32 flags) {\n//    attr->pathname = (__u64)(unsigned long)path;\n//    attr->bpf_fd = fd;\n//    attr->file_flags = flags;\n// }\n//\n// // bpf_attr_setup_map_elem sets up the bpf_attr union for use with BPF_MAP_GET|UPDATE\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_map_elem(union bpf_attr *attr, __u32 map_fd, void *pointer_to_key, void *pointer_to_value, __u64 flags) {\n//    attr->map_fd = map_fd;\n//    attr->key = (__u64)(unsigned long)pointer_to_key;\n//    attr->value = (__u64)(unsigned long)pointer_to_value;\n//    attr->flags = flags;\n// }\n//\n// // bpf_attr_setup_map_get_next_key sets up the bpf_attr union for use with BPF_MAP_GET_NEXT_KEY\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_map_get_next_key(union bpf_attr *attr, __u32 map_fd, void *key, void *next_key, __u64 flags) {\n//    attr->map_fd = map_fd;\n//    attr->key = (__u64)(unsigned long)key;\n//    attr->next_key = (__u64)(unsigned long)next_key;\n//    attr->flags = flags;\n// }\n//\n// // bpf_attr_setup_map_elem_for_delete sets up the bpf_attr union for use with BPF_MAP_DELETE_ELEM\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_map_elem_for_delete(union bpf_attr *attr, __u32 map_fd, void *pointer_to_key) {\n//    attr->map_fd = map_fd;\n//    attr->key = (__u64)(unsigned long)pointer_to_key;\n// }\n//\n// // bpf_attr_setup_load_prog sets up the bpf_attr union for use with BPF_PROG_LOAD.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_load_prog(union bpf_attr *attr, __u32 prog_type, __u32 insn_count, void *insns, char *license, __u32 log_level, __u32 log_size, void *log_buf) {\n//    attr->prog_type = prog_type;\n//    attr->insn_cnt = insn_count;\n//    attr->insns = (__u64)(unsigned long)insns;\n//    attr->license = (__u64)(unsigned long)license;\n//    attr->log_level = log_level;\n//    attr->log_size = log_size;\n//    attr->log_buf = (__u64)(unsigned long)log_buf;\n//    attr->kern_version = 0;\n//    if (log_size > 0) ((char *)log_buf)[0] = 0;\n// }\n//\n// // bpf_attr_setup_prog_run sets up the bpf_attr union for use with BPF_PROG_TEST_RUN.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_prog_run(union bpf_attr *attr, __u32 prog_fd,\n//                              __u32 data_size_in, void *data_in,\n//                              __u32 data_size_out, void *data_out,\n//                              __u32 repeat) {\n//    attr->test.prog_fd = prog_fd;\n//    attr->test.data_size_in = data_size_in;\n//    attr->test.data_size_out = data_size_out;\n//    attr->test.data_in = (__u64)(unsigned long)data_in;\n//    attr->test.data_out = (__u64)(unsigned long)data_out;\n//    attr->test.repeat = repeat;\n// }\n//\n// // bpf_attr_setup_get_info sets up the bpf_attr union for use with BPF_OBJ_GET_INFO_BY_FD.\n// // A C function makes this easier because unions aren't easy to access from Go.\n// void bpf_attr_setup_get_info(union bpf_attr *attr, __u32 map_fd,\n//                              __u32 info_size, void *info) {\n//    attr->info.bpf_fd = map_fd;\n//    attr->info.info_len = info_size;\n//    attr->info.info = (__u64)(unsigned long)info;\n// }\n//\n// __u32 bpf_attr_prog_run_retval(union bpf_attr *attr) {\n//    return attr->test.retval;\n// }\n//\n// __u32 bpf_attr_prog_run_data_out_size(union bpf_attr *attr) {\n//    return attr->test.data_size_out;\n// }\n//\n// __u32 bpf_attr_prog_run_duration(union bpf_attr *attr) {\n//    return attr->test.duration;\n// }\n//\n// int bpf_map_call(int cmd, __u32 map_fd, void *pointer_to_key, void *pointer_to_value, __u64 flags) {\n//    union bpf_attr attr = {};\n//\n//    attr.map_fd = map_fd;\n//    attr.key = (__u64)(unsigned long)pointer_to_key;\n//    attr.value = (__u64)(unsigned long)pointer_to_value;\n//    attr.flags = flags;\n//\n//    return syscall(SYS_bpf, cmd, &attr, sizeof(attr)) == 0 ? 0 : errno;\n// }\n//\nimport \"C\"\n\nfunc SyscallSupport() bool {\n\treturn true\n}\n\nfunc GetMapFDByPin(filename string) (MapFD, error) {\n\tlog.Debugf(\"GetMapFDByPin(%v)\", filename)\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tcFilename := C.CString(filename)\n\tdefer C.free(unsafe.Pointer(cFilename))\n\n\tC.bpf_attr_setup_obj_get(bpfAttr, cFilename, 0)\n\tfd, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_OBJ_GET, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\tif errno != 0 {\n\t\treturn 0, errno\n\t}\n\n\treturn MapFD(fd), nil\n}\n\nfunc GetMapFDByID(mapID int) (MapFD, error) {\n\tlog.Debugf(\"GetMapFDByID(%v)\", mapID)\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tC.bpf_attr_setup_obj_get_id(bpfAttr, C.uint(mapID), 0)\n\tfd, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_MAP_GET_FD_BY_ID, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\tif errno != 0 {\n\t\treturn 0, errno\n\t}\n\n\treturn MapFD(fd), nil\n}\n\nconst defaultLogSize = 1024 * 1024\nconst maxLogSize = 128 * 1024 * 1024\n\nfunc LoadBPFProgramFromInsns(insns asm.Insns, license string) (ProgFD, error) {\n\tlog.Debugf(\"LoadBPFProgramFromInsns(%v, %v)\", insns, license)\n\tincreaseLockedMemoryQuota()\n\n\t// By default, try to load the program with logging disabled, for performance.\n\tfd, err := tryLoadBPFProgramFromInsns(insns, license, 0)\n\tif err == nil {\n\t\tlog.WithField(\"fd\", fd).Debug(\"Loaded program successfully\")\n\t\treturn fd, nil\n\t}\n\n\t// After a failure, retry, passing a log buffer to get the diagnostics from the kernel.\n\tlog.WithError(err).Warn(\"Failed to load BPF program; collecting diagnostics...\")\n\tvar logSize uint = defaultLogSize\n\tfor {\n\t\tfd, err2 := tryLoadBPFProgramFromInsns(insns, license, logSize)\n\t\tif err2 == nil {\n\t\t\t// Unexpected but we'll take it.\n\t\t\tlog.Warn(\"Retry succeeded.\")\n\t\t\treturn fd, nil\n\t\t}\n\t\tif err2 == unix.ENOSPC && logSize < maxLogSize {\n\t\t\t// Log buffer was too small.\n\t\t\tlog.Warn(\"Diagnostics buffer was too small, trying again with a larger buffer.\")\n\t\t\tlogSize *= 2\n\t\t\tcontinue\n\t\t}\n\t\treturn 0, err\n\t}\n}\n\nfunc tryLoadBPFProgramFromInsns(insns asm.Insns, license string, logSize uint) (ProgFD, error) {\n\tlog.Debugf(\"tryLoadBPFProgramFromInsns(..., %v, %v)\", license, logSize)\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tcInsnBytes := C.CBytes(insns.AsBytes())\n\tdefer C.free(cInsnBytes)\n\tcLicense := C.CString(license)\n\tdefer C.free(unsafe.Pointer(cLicense))\n\n\tvar logBuf unsafe.Pointer\n\tvar logLevel uint\n\tif logSize > 0 {\n\t\tlogLevel = 1\n\t\tlogBuf = C.malloc((C.size_t)(logSize))\n\t\tdefer C.free(logBuf)\n\t}\n\n\tC.bpf_attr_setup_load_prog(bpfAttr, unix.BPF_PROG_TYPE_SCHED_CLS, C.uint(len(insns)), cInsnBytes, cLicense, (C.uint)(logLevel), (C.uint)(logSize), logBuf)\n\tfd, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_PROG_LOAD, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\n\tif errno != 0 && errno != unix.ENOSPC /* log buffer too small */ {\n\t\tgoLog := strings.TrimSpace(C.GoString((*C.char)(logBuf)))\n\t\tlog.WithError(errno).Error(\"BPF_PROG_LOAD failed\")\n\t\tif len(goLog) > 0 {\n\t\t\tfor _, l := range strings.Split(goLog, \"\\n\") {\n\t\t\t\tlog.Error(\"BPF Verifier:    \", l)\n\t\t\t}\n\t\t} else if logSize > 0 {\n\t\t\tlog.Error(\"Verifier log was empty.\")\n\t\t}\n\t}\n\n\tif errno != 0 {\n\t\treturn 0, errno\n\t}\n\treturn ProgFD(fd), nil\n}\n\nvar memLockOnce sync.Once\n\nfunc increaseLockedMemoryQuota() {\n\tmemLockOnce.Do(func() {\n\t\terr := unix.Setrlimit(unix.RLIMIT_MEMLOCK, &unix.Rlimit{Cur: unix.RLIM_INFINITY, Max: unix.RLIM_INFINITY})\n\t\tif err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to increase RLIMIT_MEMLOCK, loading BPF programs may fail\")\n\t\t}\n\t})\n}\n\nfunc RunBPFProgram(fd ProgFD, dataIn []byte, repeat int) (pr ProgResult, err error) {\n\tlog.Debugf(\"RunBPFProgram(%v, ..., %v)\", fd, repeat)\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tcDataIn := C.CBytes(dataIn)\n\tdefer C.free(cDataIn)\n\tconst dataOutBufSize = 4096\n\tcDataOut := C.malloc(dataOutBufSize)\n\tdefer C.free(cDataOut)\n\n\tC.bpf_attr_setup_prog_run(bpfAttr, C.uint(fd), C.uint(len(dataIn)), cDataIn, C.uint(dataOutBufSize), cDataOut, C.uint(repeat))\n\t_, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_PROG_TEST_RUN, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\n\tif errno != 0 {\n\t\terr = errno\n\t\treturn\n\t}\n\n\tpr.RC = int32(C.bpf_attr_prog_run_retval(bpfAttr))\n\tdataOutSize := C.bpf_attr_prog_run_data_out_size(bpfAttr)\n\tpr.Duration = time.Duration(C.bpf_attr_prog_run_data_out_size(bpfAttr))\n\tpr.DataOut = C.GoBytes(cDataOut, C.int(dataOutSize))\n\treturn\n}\n\nfunc PinBPFProgram(fd ProgFD, filename string) error {\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tcFilename := C.CString(filename)\n\tdefer C.free(unsafe.Pointer(cFilename))\n\n\tC.bpf_attr_setup_obj_pin(bpfAttr, cFilename, C.uint(fd), 0)\n\t_, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_OBJ_PIN, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\tif errno != 0 {\n\t\treturn errno\n\t}\n\n\treturn nil\n}\n\nfunc UpdateMapEntry(mapFD MapFD, k, v []byte) error {\n\tlog.Debugf(\"UpdateMapEntry(%v, %v, %v)\", mapFD, k, v)\n\n\terr := checkMapIfDebug(mapFD, len(k), len(v))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\n\tcK := C.CBytes(k)\n\tdefer C.free(cK)\n\tcV := C.CBytes(v)\n\tdefer C.free(cV)\n\n\tC.bpf_attr_setup_map_elem(bpfAttr, C.uint(mapFD), cK, cV, unix.BPF_ANY)\n\n\t_, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_MAP_UPDATE_ELEM, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\n\tif errno != 0 {\n\t\treturn errno\n\t}\n\treturn nil\n}\n\nfunc GetMapEntry(mapFD MapFD, k []byte, valueSize int) ([]byte, error) {\n\tlog.Debugf(\"GetMapEntry(%v, %v, %v)\", mapFD, k, valueSize)\n\n\terr := checkMapIfDebug(mapFD, len(k), valueSize)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tval := make([]byte, valueSize)\n\n\terrno := C.bpf_map_call(unix.BPF_MAP_LOOKUP_ELEM, C.uint(mapFD),\n\t\tunsafe.Pointer(&k[0]), unsafe.Pointer(&val[0]), 0)\n\tif errno != 0 {\n\t\treturn nil, unix.Errno(errno)\n\t}\n\n\treturn val, nil\n}\n\nfunc checkMapIfDebug(mapFD MapFD, keySize, valueSize int) error {\n\tif log.GetLevel() >= log.DebugLevel {\n\t\tmapInfo, err := GetMapInfo(mapFD)\n\t\tif err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to read map information\")\n\t\t\treturn err\n\t\t}\n\t\tlog.WithField(\"mapInfo\", mapInfo).Debug(\"Map metadata\")\n\t\tif keySize != mapInfo.KeySize {\n\t\t\tlog.WithField(\"mapInfo\", mapInfo).WithField(\"keyLen\", keySize).Panic(\"Incorrect key length\")\n\t\t}\n\t\tif valueSize >= 0 && valueSize != mapInfo.ValueSize {\n\t\t\tlog.WithField(\"mapInfo\", mapInfo).WithField(\"valueLen\", valueSize).Panic(\"Incorrect value length\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc GetMapInfo(fd MapFD) (*MapInfo, error) {\n\tbpfAttr := C.bpf_attr_alloc()\n\tdefer C.free(unsafe.Pointer(bpfAttr))\n\tvar bpfMapInfo *C.struct_bpf_map_info = (*C.struct_bpf_map_info)(C.malloc(C.sizeof_struct_bpf_map_info))\n\tdefer C.free(unsafe.Pointer(bpfMapInfo))\n\n\tC.bpf_attr_setup_get_info(bpfAttr, C.uint(fd), C.sizeof_struct_bpf_map_info, unsafe.Pointer(bpfMapInfo))\n\t_, _, errno := unix.Syscall(unix.SYS_BPF, unix.BPF_OBJ_GET_INFO_BY_FD, uintptr(unsafe.Pointer(bpfAttr)), C.sizeof_union_bpf_attr)\n\n\tif errno != 0 {\n\t\treturn nil, errno\n\t}\n\treturn &MapInfo{\n\t\tType:      int(bpfMapInfo._type),\n\t\tKeySize:   int(bpfMapInfo.key_size),\n\t\tValueSize: int(bpfMapInfo.value_size),\n\t}, nil\n}\n\nfunc DeleteMapEntry(mapFD MapFD, k []byte, valueSize int) error {\n\tlog.Debugf(\"DeleteMapEntry(%v, %v, %v)\", mapFD, k, valueSize)\n\n\terr := checkMapIfDebug(mapFD, len(k), valueSize)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terrno := C.bpf_map_call(unix.BPF_MAP_DELETE_ELEM, C.uint(mapFD),\n\t\tunsafe.Pointer(&k[0]), unsafe.Pointer(nil), 0)\n\tif errno != 0 {\n\t\treturn unix.Errno(errno)\n\t}\n\n\treturn nil\n}\n\n// GetMapNextKey returns the next key for the given key if the current key exists.\n// Otherwise it returns the first key. Order is implemention / map type\n// dependent.\n//\n// Start iterating by passing a nil key.\nfunc GetMapNextKey(mapFD MapFD, k []byte, keySize int) ([]byte, error) {\n\tlog.Debugf(\"GetMapNextKey(%v, %v, %v)\", mapFD, k, keySize)\n\n\tif log.GetLevel() >= log.DebugLevel && keySize == 0 && len(k) != keySize && len(k) != 0 {\n\t\tlog.WithField(\"keySize\", keySize).WithField(\"keyLen\", len(k)).Panic(\"keySize != len(k)\")\n\t}\n\terr := checkMapIfDebug(mapFD, keySize, -1)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar cK unsafe.Pointer\n\tif len(k) > 0 {\n\t\tcK = unsafe.Pointer(&k[0])\n\t}\n\n\tnext := make([]byte, keySize)\n\n\terrno := C.bpf_map_call(unix.BPF_MAP_GET_NEXT_KEY, C.uint(mapFD), cK, unsafe.Pointer(&next[0]), 0)\n\tif errno != 0 {\n\t\treturn nil, unix.Errno(errno)\n\t}\n\n\treturn next, nil\n}\n", "idx": 2, "id": 18252, "msg": "", "proj": "projectcalico-felix", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -50,6 +50,24 @@\n namespace Kokkos {\n namespace Profiling {\n \n+static initFunction initProfileLibrary = NULL;\n+static finalizeFunction finalizeProfileLibrary = NULL;\n+\n+static beginFunction beginForCallee = NULL;\n+static beginFunction beginScanCallee = NULL;\n+static beginFunction beginReduceCallee = NULL;\n+static endFunction endForCallee = NULL;\n+static endFunction endScanCallee = NULL;\n+static endFunction endReduceCallee = NULL;\n+\n+static pushFunction pushRegionCallee = NULL;\n+static popFunction popRegionCallee = NULL;\n+\n+static allocateDataFunction allocateDataCallee = NULL;\n+static deallocateDataFunction deallocateDataCallee = NULL;\n+\n+static deepCopyFunction deepCopyCallee = NULL;\n+\n SpaceHandle::SpaceHandle(const char* space_name) {\n   strncpy(name,space_name,64);\n }", "y": 1, "oldf": "/*\n //@HEADER\n // ************************************************************************\n //\n //                        Kokkos v. 2.0\n //              Copyright (2014) Sandia Corporation\n //\n // Under the terms of Contract DE-AC04-94AL85000 with Sandia Corporation,\n // the U.S. Government retains certain rights in this software.\n //\n // Redistribution and use in source and binary forms, with or without\n // modification, are permitted provided that the following conditions are\n // met:\n //\n // 1. Redistributions of source code must retain the above copyright\n // notice, this list of conditions and the following disclaimer.\n //\n // 2. Redistributions in binary form must reproduce the above copyright\n // notice, this list of conditions and the following disclaimer in the\n // documentation and/or other materials provided with the distribution.\n //\n // 3. Neither the name of the Corporation nor the names of the\n // contributors may be used to endorse or promote products derived from\n // this software without specific prior written permission.\n //\n // THIS SOFTWARE IS PROVIDED BY SANDIA CORPORATION \"AS IS\" AND ANY\n // EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n // IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n // PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SANDIA CORPORATION OR THE\n // CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n // EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n // PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n // PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n // LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n // NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n // SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n //\n // Questions? Contact  H. Carter Edwards (hcedwar@sandia.gov)\n //\n // ************************************************************************\n //@HEADER\n */\n\n#include <Kokkos_Macros.hpp>\n#if defined(KOKKOS_ENABLE_PROFILING)\n\n#include <impl/Kokkos_Profiling_Interface.hpp>\n#include <cstring>\n\nnamespace Kokkos {\nnamespace Profiling {\n\nSpaceHandle::SpaceHandle(const char* space_name) {\n  strncpy(name,space_name,64);\n}\n\nbool profileLibraryLoaded() {\n  return (NULL != initProfileLibrary);\n}\n\nvoid beginParallelFor(const std::string& kernelPrefix, const uint32_t devID, uint64_t* kernelID) {\n  if(NULL != beginForCallee) {\n    Kokkos::fence();\n    (*beginForCallee)(kernelPrefix.c_str(), devID, kernelID);\n  }\n}\n\nvoid endParallelFor(const uint64_t kernelID) {\n  if(NULL != endForCallee) {\n    Kokkos::fence();\n    (*endForCallee)(kernelID);\n  }\n}\n\nvoid beginParallelScan(const std::string& kernelPrefix, const uint32_t devID, uint64_t* kernelID) {\n  if(NULL != beginScanCallee) {\n    Kokkos::fence();\n    (*beginScanCallee)(kernelPrefix.c_str(), devID, kernelID);\n  }\n}\n\nvoid endParallelScan(const uint64_t kernelID) {\n  if(NULL != endScanCallee) {\n    Kokkos::fence();\n    (*endScanCallee)(kernelID);\n  }\n}\n\nvoid beginParallelReduce(const std::string& kernelPrefix, const uint32_t devID, uint64_t* kernelID) {\n  if(NULL != beginReduceCallee) {\n    Kokkos::fence();\n    (*beginReduceCallee)(kernelPrefix.c_str(), devID, kernelID);\n  }\n}\n\nvoid endParallelReduce(const uint64_t kernelID) {\n  if(NULL != endReduceCallee) {\n    Kokkos::fence();\n    (*endReduceCallee)(kernelID);\n  }\n}\n\n\nvoid pushRegion(const std::string& kName) {\n  if( NULL != pushRegionCallee ) {\n    Kokkos::fence();\n    (*pushRegionCallee)(kName.c_str());\n  }\n}\n\nvoid popRegion() {\n  if( NULL != popRegionCallee ) {\n    Kokkos::fence();\n    (*popRegionCallee)();\n  }\n}\n\nvoid allocateData(const SpaceHandle space, const std::string label, const void* ptr, const uint64_t size) {\n  if(NULL != allocateDataCallee) {\n    (*allocateDataCallee)(space,label.c_str(),ptr,size);\n  }\n}\n\nvoid deallocateData(const SpaceHandle space, const std::string label, const void* ptr, const uint64_t size) {\n  if(NULL != allocateDataCallee) {\n    (*deallocateDataCallee)(space,label.c_str(),ptr,size);\n  }\n}\n\nvoid initialize() {\n\n  // Make sure initialize calls happens only once\n  static int is_initialized = 0;\n  if(is_initialized) return;\n  is_initialized = 1;\n\n  void* firstProfileLibrary;\n\n  char* envProfileLibrary  = getenv(\"KOKKOS_PROFILE_LIBRARY\");\n\n  // If we do not find a profiling library in the environment then exit\n  // early.\n  if( NULL == envProfileLibrary ) {\n    return ;\n  }\n\n  char* envProfileCopy = (char*) malloc(sizeof(char) * (strlen(envProfileLibrary) + 1));\n  sprintf(envProfileCopy, \"%s\", envProfileLibrary);\n\n  char* profileLibraryName = strtok(envProfileCopy, \";\");\n\n  if( (NULL != profileLibraryName) && (strcmp(profileLibraryName, \"\") != 0) ) {\n    firstProfileLibrary = dlopen(profileLibraryName, RTLD_NOW | RTLD_GLOBAL);\n\n    if(NULL == firstProfileLibrary) {\n      std::cerr << \"Error: Unable to load KokkosP library: \" <<\n        profileLibraryName << std::endl;\n    } else {\n#ifdef KOKKOS_ENABLE_PROFILING_LOAD_PRINT\n      std::cout << \"KokkosP: Library Loaded: \" << profileLibraryName << std::endl;\n#endif\n\n      // dlsym returns a pointer to an object, while we want to assign to pointer to function\n      // A direct cast will give warnings hence, we have to workaround the issue by casting pointer to pointers.\n      auto p1 = dlsym(firstProfileLibrary, \"kokkosp_begin_parallel_for\");\n      beginForCallee = *((beginFunction*) &p1);\n      auto p2 = dlsym(firstProfileLibrary, \"kokkosp_begin_parallel_scan\");\n      beginScanCallee = *((beginFunction*) &p2);\n      auto p3 = dlsym(firstProfileLibrary, \"kokkosp_begin_parallel_reduce\");\n      beginReduceCallee = *((beginFunction*) &p3);\n\n      auto p4 = dlsym(firstProfileLibrary, \"kokkosp_end_parallel_scan\");\n      endScanCallee = *((endFunction*) &p4);\n      auto p5 = dlsym(firstProfileLibrary, \"kokkosp_end_parallel_for\");\n      endForCallee = *((endFunction*) &p5);\n      auto p6 = dlsym(firstProfileLibrary, \"kokkosp_end_parallel_reduce\");\n      endReduceCallee = *((endFunction*) &p6);\n\n      auto p7 = dlsym(firstProfileLibrary, \"kokkosp_init_library\");\n      initProfileLibrary = *((initFunction*) &p7);\n      auto p8 = dlsym(firstProfileLibrary, \"kokkosp_finalize_library\");\n      finalizeProfileLibrary = *((finalizeFunction*) &p8);\n\n      auto p9 = dlsym(firstProfileLibrary, \"kokkosp_push_profile_region\");\n      pushRegionCallee = *((pushFunction*) &p9);\n      auto p10 = dlsym(firstProfileLibrary, \"kokkosp_pop_profile_region\");\n      popRegionCallee = *((popFunction*) &p10);\n\n      auto p11 = dlsym(firstProfileLibrary, \"kokkosp_allocate_data\");\n      allocateDataCallee = *((allocateDataFunction*) &p11);\n      auto p12 = dlsym(firstProfileLibrary, \"kokkosp_deallocate_data\");\n      deallocateDataCallee = *((deallocateDataFunction*) &p12);\n\n    }\n  }\n\n  if(NULL != initProfileLibrary) {\n    (*initProfileLibrary)(0,\n        (uint64_t) KOKKOSP_INTERFACE_VERSION,\n        (uint32_t) 0,\n        NULL);\n  }\n\n  free(envProfileCopy);\n}\n\nvoid finalize() {\n  // Make sure finalize calls happens only once\n  static int is_finalized = 0;\n  if(is_finalized) return;\n  is_finalized = 1;\n\n  if(NULL != finalizeProfileLibrary) {\n    (*finalizeProfileLibrary)();\n\n    // Set all profile hooks to NULL to prevent\n    // any additional calls. Once we are told to\n    // finalize, we mean it\n    initProfileLibrary = NULL;\n    finalizeProfileLibrary = NULL;\n\n    beginForCallee = NULL;\n    beginScanCallee = NULL;\n    beginReduceCallee = NULL;\n    endScanCallee = NULL;\n    endForCallee = NULL;\n    endReduceCallee = NULL;\n\n    pushRegionCallee = NULL;\n    popRegionCallee = NULL;\n\n    allocateDataCallee = NULL;\n    deallocateDataCallee = NULL;\n\n  }\n}\n}\n}\n\n#else\nvoid KOKKOS_CORE_SRC_IMPL_PROFILING_INTERFACE_PREVENT_LINK_ERROR() {}\n#endif\n\n", "idx": 1, "id": 16696, "msg": "use 'nullptr' instead of NULL", "proj": "kokkos-kokkos", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -508,12 +508,23 @@ def _copy_file_list_internal(file_list, message, callback):\n                     if not futures:\n                         break\n                     future = futures.popleft()\n-                future.result()\n+                if future.cancelled():\n+                    continue\n+                try:\n+                    future.result()\n+                except ClientError:\n+                    with lock:\n+                        idx = future_to_idx[future]\n+                        futures_to_cancel = idx_to_futures[idx]\n+                        for f in futures_to_cancel:\n+                            f.cancel()\n+                        futures_to_cancel.clear()\n         finally:\n             # Make sure all tasks exit quickly if the main thread exits before they're done.\n             stopped = True\n \n-    assert all(results)\n+    if not all(results):\n+        raise QuiltException(\"Unable to copy some files.\")\n \n     return results\n ", "y": 0, "oldf": "from collections import deque\nfrom codecs import iterdecode\nfrom concurrent.futures import ThreadPoolExecutor\nfrom enum import Enum\nimport hashlib\nimport pathlib\nimport shutil\nfrom threading import Lock\nfrom typing import List, Tuple\nimport warnings\n\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\nfrom botocore.exceptions import ClientError, ConnectionError, HTTPClientError, ReadTimeoutError\nimport boto3\nfrom boto3.s3.transfer import TransferConfig\nfrom s3transfer.utils import ChunksizeAdjuster, OSUtils, signal_transferring, signal_not_transferring\n\nimport jsonlines\nfrom tqdm import tqdm\n\nfrom .session import create_botocore_session\nfrom .util import PhysicalKey, QuiltException\n\n\n\n\n\nclass S3Api(Enum):\n    GET_OBJECT = \"GET_OBJECT\"\n    HEAD_OBJECT = \"HEAD_OBJECT\"\n    LIST_OBJECT_VERSIONS = \"LIST_OBJECT_VERSIONS\"\n    LIST_OBJECTS_V2 = \"LIST_OBJECTS_V2\"\n\n\nclass S3NoValidClientError(Exception):\n    def __init__(self, message, **kwargs):\n        # We use NewError(\"Prefix: \" + str(error)) a lot.\n        # To be consistent across Python 2.7 and 3.x:\n        # 1) This `super` call must exist, or 2.7 will have no text for str(error)\n        # 2) This `super` call must have only one argument (the message) or str(error) will be a repr of args\n        super(S3NoValidClientError, self).__init__(message)\n        self.message = message\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\nclass S3ClientProvider:\n    \"\"\"\n    An s3_client is either signed with standard credentials or unsigned. This class exists to dynamically provide the\n    correct s3 client (either standard_client or unsigned_client) for a bucket. This means if standard credentials\n    can't read from the bucket, check if the bucket is public in which case we should be using an unsigned client.\n    This check is expensive at scale so the class also keeps track of which client to use for each bucket+api_call.\n\n    If there are no credentials available at all (i.e. you don't have AWS credentials and you don't have a\n    Quilt-provided role from quilt3.login()), the standard client will also be unsigned so that users can still\n    access public s3 buckets.\n\n    We assume that public buckets are read-only: write operations should always use S3ClientProvider.standard_client\n    \"\"\"\n\n    def __init__(self):\n        self._use_unsigned_client = {}  # f'{action}/{bucket}' -> use_unsigned_client_bool\n        self._standard_client = None\n        self._unsigned_client = None\n\n    @property\n    def standard_client(self):\n        if self._standard_client is None:\n            self._build_standard_client()\n        return self._standard_client\n\n    @property\n    def unsigned_client(self):\n        if self._unsigned_client is None:\n            self._build_unsigned_client()\n        return self._unsigned_client\n\n    def get_correct_client(self, action: S3Api, bucket: str):\n        if not self.client_type_known(action, bucket):\n            raise RuntimeError(\"get_correct_client was called, but the correct client type is not known. Only call \"\n                               \"get_correct_client() after checking if client_type_known()\")\n\n        if self.should_use_unsigned_client(action, bucket):\n            return self.unsigned_client\n        else:\n            return self.standard_client\n\n    def key(self, action: S3Api, bucket: str):\n        return f\"{action}/{bucket}\"\n\n    def set_cache(self, action: S3Api, bucket: str, use_unsigned: bool):\n        self._use_unsigned_client[self.key(action, bucket)] = use_unsigned\n\n    def should_use_unsigned_client(self, action: S3Api, bucket: str):\n        # True if should use unsigned, False if should use standard, None if don't know yet\n        return self._use_unsigned_client.get(self.key(action, bucket))\n\n    def client_type_known(self, action: S3Api, bucket: str):\n        return self.should_use_unsigned_client(action, bucket) is not None\n\n    def find_correct_client(self, api_type, bucket, param_dict):\n        if self.client_type_known(api_type, bucket):\n            return self.get_correct_client(api_type, bucket)\n        else:\n            check_fn_mapper = {\n                S3Api.GET_OBJECT: check_get_object_works_for_client,\n                S3Api.HEAD_OBJECT: check_head_object_works_for_client,\n                S3Api.LIST_OBJECTS_V2: check_list_objects_v2_works_for_client,\n                S3Api.LIST_OBJECT_VERSIONS: check_list_object_versions_works_for_client\n            }\n            assert api_type in check_fn_mapper.keys(), f\"Only certain APIs are supported with unsigned_client. The \" \\\n                f\"API '{api_type}' is not current supported. You may want to use S3ClientProvider.standard_client \" \\\n                f\"instead \"\n            check_fn = check_fn_mapper[api_type]\n            if check_fn(self.standard_client, param_dict):\n                self.set_cache(api_type, bucket, use_unsigned=False)\n                return self.standard_client\n            else:\n                if check_fn(self.unsigned_client, param_dict):\n                    self.set_cache(api_type, bucket, use_unsigned=True)\n                    return self.unsigned_client\n                else:\n                    raise S3NoValidClientError(f\"S3 AccessDenied for {api_type} on bucket: {bucket}\")\n\n    def get_boto_session(self):\n        botocore_session = create_botocore_session()\n        boto_session = boto3.Session(botocore_session=botocore_session)\n        return boto_session\n\n\n    def register_signals(self, s3_client):\n        # Enable/disable file read callbacks when uploading files.\n        # Copied from https://github.com/boto/s3transfer/blob/develop/s3transfer/manager.py#L501\n        event_name = 'request-created.s3'\n        s3_client.meta.events.register_first(\n                event_name, signal_not_transferring,\n                unique_id='datatransfer-not-transferring')\n        s3_client.meta.events.register_last(\n                event_name, signal_transferring,\n                unique_id='datatransfer-transferring')\n\n    def _build_standard_client(self):\n        boto_session = self.get_boto_session()\n\n        config = None\n        if boto_session.get_credentials() is None:\n            config = Config(signature_version=UNSIGNED)\n\n        s3_client = boto_session.client('s3', config=config)\n        self.register_signals(s3_client)\n        self._standard_client = s3_client\n\n\n    def _build_unsigned_client(self):\n        boto_session = self.get_boto_session()\n        s3_client = boto_session.client('s3', config=Config(signature_version=UNSIGNED))\n        self.register_signals(s3_client)\n        self._unsigned_client = s3_client\n\n\n\ndef check_list_object_versions_works_for_client(s3_client, params):\n    try:\n        s3_client.list_object_versions(**params, MaxKeys=1)  # Make this as fast as possible\n    except ClientError as e:\n        return e.response[\"Error\"][\"Code\"] != \"AccessDenied\"\n    return True\n\ndef check_list_objects_v2_works_for_client(s3_client, params):\n    try:\n        s3_client.list_objects_v2(**params, MaxKeys=1)  # Make this as fast as possible\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"AccessDenied\":\n            return False\n    return True\n\ndef check_get_object_works_for_client(s3_client, params):\n    try:\n        head_args = dict(\n                Bucket=params[\"Bucket\"],\n                Key=params[\"Key\"]\n        )\n        if \"VersionId\" in params:\n            head_args[\"VersionId\"] = params[\"VersionId\"]\n\n        s3_client.head_object(**head_args)  # HEAD/GET share perms, but HEAD always fast\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"403\":\n            # This can also happen if you have full get_object access, but not list_objects_v2, and the object does not\n            # exist. Instead of returning a 404, S3 will return a 403.\n            return False\n\n    return True\n\ndef check_head_object_works_for_client(s3_client, params):\n    try:\n        s3_client.head_object(**params)\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"403\":\n            # This can also happen if you have full get_object access, but not list_objects_v2, and the object does not\n            # exist. Instead of returning a 404, S3 will return a 403.\n            return False\n    return True\n\n\n\ns3_transfer_config = TransferConfig()\n\n# When uploading files at least this size, compare the ETags first and skip the upload if they're equal;\n# copy the remote file onto itself if the metadata changes.\nUPLOAD_ETAG_OPTIMIZATION_THRESHOLD = 1024\n\n\ndef _copy_local_file(ctx, size, src_path, dest_path):\n    pathlib.Path(dest_path).parent.mkdir(parents=True, exist_ok=True)\n\n    # TODO(dima): More detailed progress.\n    shutil.copyfile(src_path, dest_path)\n    ctx.progress(size)\n    shutil.copymode(src_path, dest_path)\n\n    ctx.done(PhysicalKey.from_path(dest_path))\n\n\ndef _upload_file(ctx, size, src_path, dest_bucket, dest_key):\n    s3_client = ctx.s3_client_provider.standard_client\n\n    if size < s3_transfer_config.multipart_threshold:\n        with OSUtils().open_file_chunk_reader(src_path, 0, size, [ctx.progress]) as fd:\n            resp = s3_client.put_object(\n                Body=fd,\n                Bucket=dest_bucket,\n                Key=dest_key,\n            )\n\n        version_id = resp.get('VersionId')  # Absent in unversioned buckets.\n        ctx.done(PhysicalKey(dest_bucket, dest_key, version_id))\n    else:\n        resp = s3_client.create_multipart_upload(\n            Bucket=dest_bucket,\n            Key=dest_key,\n        )\n        upload_id = resp['UploadId']\n\n        adjuster = ChunksizeAdjuster()\n        chunksize = adjuster.adjust_chunksize(s3_transfer_config.multipart_chunksize, size)\n\n        chunk_offsets = list(range(0, size, chunksize))\n\n        lock = Lock()\n        remaining = len(chunk_offsets)\n        parts = [None] * remaining\n\n        def upload_part(i, start, end):\n            nonlocal remaining\n            part_id = i + 1\n            with OSUtils().open_file_chunk_reader(src_path, start, end-start, [ctx.progress]) as fd:\n                part = s3_client.upload_part(\n                    Body=fd,\n                    Bucket=dest_bucket,\n                    Key=dest_key,\n                    UploadId=upload_id,\n                    PartNumber=part_id\n                )\n            with lock:\n                parts[i] = {\"PartNumber\": part_id, \"ETag\": part[\"ETag\"]}\n                remaining -= 1\n                done = remaining == 0\n\n            if done:\n                resp = s3_client.complete_multipart_upload(\n                    Bucket=dest_bucket,\n                    Key=dest_key,\n                    UploadId=upload_id,\n                    MultipartUpload={\"Parts\": parts}\n                )\n                version_id = resp.get('VersionId')  # Absent in unversioned buckets.\n                ctx.done(PhysicalKey(dest_bucket, dest_key, version_id))\n\n        for i, start in enumerate(chunk_offsets):\n            end = min(start + chunksize, size)\n            ctx.run(upload_part, i, start, end)\n\n\ndef _download_file(ctx, src_bucket, src_key, src_version, dest_path):\n    dest_file = pathlib.Path(dest_path)\n    if dest_file.is_reserved():\n        raise ValueError(\"Cannot download to %r: reserved file name\" % dest_path)\n\n    params = dict(Bucket=src_bucket, Key=src_key)\n    s3_client = ctx.s3_client_provider.find_correct_client(S3Api.GET_OBJECT, src_bucket, params)\n\n    dest_file.parent.mkdir(parents=True, exist_ok=True)\n\n\n    if src_version is not None:\n        params.update(dict(VersionId=src_version))\n    resp = s3_client.get_object(**params)\n\n    body = resp['Body']\n    with open(dest_path, 'wb') as fd:\n        while True:\n            chunk = body.read(64 * 1024)\n            if not chunk:\n                break\n            fd.write(chunk)\n            ctx.progress(len(chunk))\n\n    ctx.done(PhysicalKey.from_path(dest_path))\n\n\ndef _copy_remote_file(ctx, size, src_bucket, src_key, src_version,\n                      dest_bucket, dest_key, extra_args=None):\n    src_params = dict(\n        Bucket=src_bucket,\n        Key=src_key\n    )\n    if src_version is not None:\n        src_params.update(\n            VersionId=src_version\n        )\n\n    s3_client = ctx.s3_client_provider.standard_client\n\n    if size < s3_transfer_config.multipart_threshold:\n        params = dict(\n            CopySource=src_params,\n            Bucket=dest_bucket,\n            Key=dest_key,\n        )\n\n        if extra_args:\n            params.update(extra_args)\n\n        resp = s3_client.copy_object(**params)\n        ctx.progress(size)\n        version_id = resp.get('VersionId')  # Absent in unversioned buckets.\n        ctx.done(PhysicalKey(dest_bucket, dest_key, version_id))\n    else:\n        resp = s3_client.create_multipart_upload(\n            Bucket=dest_bucket,\n            Key=dest_key,\n        )\n        upload_id = resp['UploadId']\n\n        adjuster = ChunksizeAdjuster()\n        chunksize = adjuster.adjust_chunksize(s3_transfer_config.multipart_chunksize, size)\n\n        chunk_offsets = list(range(0, size, chunksize))\n\n        lock = Lock()\n        remaining = len(chunk_offsets)\n        parts = [None] * remaining\n\n        def upload_part(i, start, end):\n            nonlocal remaining\n            part_id = i + 1\n            part = s3_client.upload_part_copy(\n                CopySource=src_params,\n                CopySourceRange=f'bytes={start}-{end-1}',\n                Bucket=dest_bucket,\n                Key=dest_key,\n                UploadId=upload_id,\n                PartNumber=part_id\n            )\n            with lock:\n                parts[i] = {\"PartNumber\": part_id, \"ETag\": part[\"CopyPartResult\"][\"ETag\"]}\n                remaining -= 1\n                done = remaining == 0\n\n            ctx.progress(end - start)\n\n            if done:\n                resp = s3_client.complete_multipart_upload(\n                    Bucket=dest_bucket,\n                    Key=dest_key,\n                    UploadId=upload_id,\n                    MultipartUpload={\"Parts\": parts}\n                )\n                version_id = resp.get('VersionId')  # Absent in unversioned buckets.\n                ctx.done(PhysicalKey(dest_bucket, dest_key, version_id))\n\n        for i, start in enumerate(chunk_offsets):\n            end = min(start + chunksize, size)\n            ctx.run(upload_part, i, start, end)\n\n\ndef _upload_or_copy_file(ctx, size, src_path, dest_bucket, dest_path):\n\n\n\n    # Optimization: check if the remote file already exists and has the right ETag,\n    # and skip the upload.\n    if size >= UPLOAD_ETAG_OPTIMIZATION_THRESHOLD:\n        try:\n            params = dict(Bucket=dest_bucket, Key=dest_path)\n            s3_client = ctx.s3_client_provider.find_correct_client(S3Api.HEAD_OBJECT, dest_bucket, params)\n            resp = s3_client.head_object(**params)\n        except ClientError:\n            # Destination doesn't exist, so fall through to the normal upload.\n            pass\n        except S3NoValidClientError:\n            # S3ClientProvider can't currently distinguish between a user that has PUT but not LIST permissions and a\n            # user that has no permissions. If we can't find a valid client, proceed to the upload stage anyway.\n            pass\n        else:\n            # Check the ETag.\n            dest_size = resp['ContentLength']\n            dest_etag = resp['ETag']\n            dest_version_id = resp.get('VersionId')\n            if size == dest_size:\n                src_etag = _calculate_etag(src_path)\n                if src_etag == dest_etag:\n                    # Nothing more to do. We should not attempt to copy the object because\n                    # that would cause the \"copy object to itself\" error.\n                    ctx.progress(size)\n                    ctx.done(PhysicalKey(dest_bucket, dest_path, dest_version_id))\n                    return  # Optimization succeeded.\n\n    # If the optimization didn't happen, do the normal upload.\n    _upload_file(ctx, size, src_path, dest_bucket, dest_path)\n\n\nclass WorkerContext(object):\n    def __init__(self, s3_client_provider, progress, done, run):\n        self.s3_client_provider = s3_client_provider\n        self.progress = progress\n        self.done = done\n        self.run = run\n\n\ndef _copy_file_list_internal(file_list, message, callback):\n    \"\"\"\n    Takes a list of tuples (src, dest, size) and copies the data in parallel.\n    Returns versioned URLs for S3 destinations and regular file URLs for files.\n    \"\"\"\n    if not file_list:\n        return []\n\n    total_size = sum(size for _, _, size in file_list)\n\n    lock = Lock()\n    futures = deque()\n    results = [None] * len(file_list)\n\n    stopped = False\n\n    s3_client_provider = S3ClientProvider()  # Share provider across threads to reduce redundant public bucket checks\n\n    with tqdm(desc=message, total=total_size, unit='B', unit_scale=True) as progress, \\\n         ThreadPoolExecutor(s3_transfer_config.max_request_concurrency) as executor:\n\n        def progress_callback(bytes_transferred):\n            if stopped:\n                raise Exception(\"Interrupted\")\n            with lock:\n                progress.update(bytes_transferred)\n\n        def run_task(func, *args):\n            future = executor.submit(func, *args)\n            with lock:\n                futures.append(future)\n\n        def worker(idx, src, dest, size):\n            if stopped:\n                raise Exception(\"Interrupted\")\n\n            def done_callback(value):\n                assert value is not None\n                with lock:\n                    assert results[idx] is None\n                    results[idx] = value\n                if callback is not None:\n                    callback(src, dest, size)\n\n            ctx = WorkerContext(s3_client_provider=s3_client_provider,\n                                progress=progress_callback,\n                                done=done_callback,\n                                run=run_task)\n\n            if dest.version_id:\n                raise ValueError(\"Cannot set VersionId on destination\")\n\n            if src.is_local():\n                if dest.is_local():\n                    _copy_local_file(ctx, size, src.path, dest.path)\n                else:\n                    if dest.version_id:\n                        raise ValueError(\"Cannot set VersionId on destination\")\n                    _upload_or_copy_file(ctx, size, src.path, dest.bucket, dest.path)\n            else:\n                if dest.is_local():\n                    _download_file(ctx, src.bucket, src.path, src.version_id, dest.path)\n                else:\n                    _copy_remote_file(ctx, size, src.bucket, src.path, src.version_id,\n                                      dest.bucket, dest.path)\n\n        try:\n            for idx, args in enumerate(file_list):\n                run_task(worker, idx, *args)\n\n            # ThreadPoolExecutor does not appear to have a way to just wait for everything to complete.\n            # Shutting it down will cause it to wait - but will prevent any new tasks from starting.\n            # So, manually wait for all tasks to complete.\n            # This will also raise any exception that happened in a worker thread.\n            while True:\n                with lock:\n                    if not futures:\n                        break\n                    future = futures.popleft()\n                future.result()\n        finally:\n            # Make sure all tasks exit quickly if the main thread exits before they're done.\n            stopped = True\n\n    assert all(results)\n\n    return results\n\n\ndef _calculate_etag(file_path):\n    \"\"\"\n    Attempts to calculate a local file's ETag the way S3 does:\n    - Normal uploads: MD5 of the file\n    - Multi-part uploads: MD5 of the (binary) MD5s of the parts, dash, number of parts\n    We can't know how the file was actually uploaded - but we're assuming it was done using\n    the default settings, which we get from `s3_transfer_config`.\n    \"\"\"\n    size = pathlib.Path(file_path).stat().st_size\n    with open(file_path, 'rb') as fd:\n        if size <= s3_transfer_config.multipart_threshold:\n            contents = fd.read()\n            etag = hashlib.md5(contents).hexdigest()\n        else:\n            adjuster = ChunksizeAdjuster()\n            chunksize = adjuster.adjust_chunksize(s3_transfer_config.multipart_chunksize, size)\n\n            hashes = []\n            while True:\n                contents = fd.read(chunksize)\n                if not contents:\n                    break\n                hashes.append(hashlib.md5(contents).digest())\n            etag = '%s-%d' % (hashlib.md5(b''.join(hashes)).hexdigest(), len(hashes))\n    return '\"%s\"' % etag\n\n\ndef delete_object(bucket, key):\n    s3_client = S3ClientProvider().standard_client\n\n    s3_client.head_object(Bucket=bucket, Key=key)  # Make sure it exists\n    s3_client.delete_object(Bucket=bucket, Key=key)  # Actually delete it\n\n\ndef list_object_versions(bucket, prefix, recursive=True):\n    if prefix and not prefix.endswith('/'):\n        raise ValueError(\"Prefix must end with /\")\n\n    list_obj_params = dict(Bucket=bucket,\n                           Prefix=prefix\n                          )\n    if not recursive:\n        # Treat '/' as a directory separator and only return one level of files instead of everything.\n        list_obj_params.update(dict(Delimiter='/'))\n\n    # TODO: make this a generator?\n    versions = []\n    delete_markers = []\n    prefixes = []\n\n    s3_client = S3ClientProvider().find_correct_client(S3Api.LIST_OBJECT_VERSIONS, bucket, list_obj_params)\n    paginator = s3_client.get_paginator('list_object_versions')\n\n    for response in paginator.paginate(**list_obj_params):\n        versions += response.get('Versions', [])\n        delete_markers += response.get('DeleteMarkers', [])\n        prefixes += response.get('CommonPrefixes', [])\n\n    if recursive:\n        return versions, delete_markers\n    else:\n        return prefixes, versions, delete_markers\n\n\n\ndef list_objects(bucket, prefix, recursive=True):\n    if prefix and not prefix.endswith('/'):\n        raise ValueError(\"Prefix must end with /\")\n\n    objects = []\n    prefixes = []\n    list_obj_params = dict(Bucket=bucket,\n                           Prefix=prefix)\n    if not recursive:\n        # Treat '/' as a directory separator and only return one level of files instead of everything.\n        list_obj_params.update(dict(Delimiter='/'))\n\n    s3_client = S3ClientProvider().find_correct_client(S3Api.LIST_OBJECTS_V2, bucket, list_obj_params)\n    paginator = s3_client.get_paginator('list_objects_v2')\n\n    for response in paginator.paginate(**list_obj_params):\n        objects += response.get('Contents', [])\n        prefixes += response.get('CommonPrefixes', [])\n\n    if recursive:\n        return objects\n    else:\n        return prefixes, objects\n\n\ndef _looks_like_dir(pk: PhysicalKey):\n    return pk.basename() == ''\n\n\ndef list_url(src: PhysicalKey):\n    if src.is_local():\n        src_file = pathlib.Path(src.path)\n\n        for f in src_file.rglob('*'):\n            try:\n                if f.is_file():\n                    size = f.stat().st_size\n                    yield f.relative_to(src_file).as_posix(), size\n            except FileNotFoundError:\n                # If a file does not exist, is it really a file?\n                pass\n    else:\n        if src.version_id is not None:\n            raise ValueError(f\"Directories cannot have version IDs: {src}\")\n        src_path = src.path\n        if not _looks_like_dir(src):\n            src_path += '/'\n        list_obj_params = dict(Bucket=src.bucket, Prefix=src_path)\n        s3_client = S3ClientProvider().find_correct_client(S3Api.LIST_OBJECTS_V2, src.bucket, list_obj_params)\n        paginator = s3_client.get_paginator('list_objects_v2')\n        for response in paginator.paginate(**list_obj_params):\n            for obj in response.get('Contents', []):\n                key = obj['Key']\n                if not key.startswith(src_path):\n                    raise ValueError(\"Unexpected key: %r\" % key)\n                yield key[len(src_path):], obj['Size']\n\n\ndef delete_url(src: PhysicalKey):\n    \"\"\"Deletes the given URL.\n    Follows S3 semantics even for local files:\n    - If the URL does not exist, it's a no-op.\n    - If it's a non-empty directory, it's also a no-op.\n    \"\"\"\n    if src.is_local():\n        src_file = pathlib.Path(src.path)\n\n        if src_file.is_dir():\n            try:\n                src_file.rmdir()\n            except OSError:\n                # Ignore non-empty directories, for consistency with S3\n                pass\n        else:\n            try:\n                src_file.unlink()\n            except FileExistsError:\n                pass\n    else:\n        s3_client = S3ClientProvider().standard_client\n        s3_client.delete_object(Bucket=src.bucket, Key=src.path)\n\n\ndef copy_file_list(file_list, message=None, callback=None):\n    \"\"\"\n    Takes a list of tuples (src, dest, size) and copies them in parallel.\n    URLs must be regular files, not directories.\n    Returns versioned URLs for S3 destinations and regular file URLs for files.\n    \"\"\"\n    for src, dest, _ in file_list:\n        if _looks_like_dir(src) or _looks_like_dir(dest):\n            raise ValueError(\"Directories are not allowed\")\n\n    return _copy_file_list_internal(file_list, message, callback)\n\n\ndef copy_file(src: PhysicalKey, dest: PhysicalKey, size=None, message=None, callback=None):\n    \"\"\"\n    Copies a single file or directory.\n    If src is a file, dest can be a file or a directory.\n    If src is a directory, dest must be a directory.\n    \"\"\"\n    def sanity_check(rel_path):\n        for part in rel_path.split('/'):\n            if part in ('', '.', '..'):\n                raise ValueError(\"Invalid relative path: %r\" % rel_path)\n\n    url_list = []\n    if _looks_like_dir(src):\n        if not _looks_like_dir(dest):\n            raise ValueError(\"Destination path must end in /\")\n        if size is not None:\n            raise ValueError(\"`size` does not make sense for directories\")\n\n        for rel_path, size in list_url(src):\n            sanity_check(rel_path)\n            url_list.append((src.join(rel_path), dest.join(rel_path), size))\n        if not url_list:\n            raise QuiltException(\"No objects to download.\")\n    else:\n        if _looks_like_dir(dest):\n            dest = dest.join(src.basename())\n        if size is None:\n            size, _ = get_size_and_version(src)\n        url_list.append((src, dest, size))\n\n    _copy_file_list_internal(url_list, message, callback)\n\n\ndef put_bytes(data: bytes, dest: PhysicalKey):\n    if _looks_like_dir(dest):\n        raise ValueError(\"Invalid path: %r\" % dest.path)\n\n    if dest.is_local():\n        dest_file = pathlib.Path(dest.path)\n        dest_file.parent.mkdir(parents=True, exist_ok=True)\n        dest_file.write_bytes(data)\n    else:\n        if dest.version_id is not None:\n            raise ValueError(\"Cannot set VersionId on destination\")\n        s3_client = S3ClientProvider().standard_client\n        s3_client.put_object(\n            Bucket=dest.bucket,\n            Key=dest.path,\n            Body=data,\n        )\n\ndef get_bytes(src: PhysicalKey):\n    if src.is_local():\n        src_file = pathlib.Path(src.path)\n        data = src_file.read_bytes()\n    else:\n        params = dict(Bucket=src.bucket, Key=src.path)\n        if src.version_id is not None:\n            params.update(dict(VersionId=src.version_id))\n        s3_client = S3ClientProvider().find_correct_client(S3Api.GET_OBJECT, src.bucket, params)\n        resp = s3_client.get_object(**params)\n        data = resp['Body'].read()\n    return data\n\ndef get_size_and_version(src: PhysicalKey):\n    \"\"\"\n    Gets size and version for the object at a given URL.\n\n    Returns:\n        size, version(str)\n    \"\"\"\n    if _looks_like_dir(src):\n        raise QuiltException(\"Invalid path: %r; cannot be a directory\" % src.path)\n\n    version = None\n    if src.is_local():\n        src_file = pathlib.Path(src.path)\n        if not src_file.is_file():\n            raise QuiltException(\"Not a file: %r\" % str(src_file))\n        size = src_file.stat().st_size\n    else:\n        params = dict(\n            Bucket=src.bucket,\n            Key=src.path\n        )\n        if src.version_id is not None:\n            params.update(dict(VersionId=src.version_id))\n        s3_client = S3ClientProvider().find_correct_client(S3Api.HEAD_OBJECT, src.bucket, params)\n        resp = s3_client.head_object(**params)\n        size = resp['ContentLength']\n        version = resp.get('VersionId')\n    return size, version\n\ndef calculate_sha256(src_list: List[PhysicalKey], sizes: List[int]):\n    assert len(src_list) == len(sizes)\n\n    total_size = sum(sizes)\n    lock = Lock()\n\n    with tqdm(desc=\"Hashing\", total=total_size, unit='B', unit_scale=True) as progress:\n        def _process_url(src, size):\n            hash_obj = hashlib.sha256()\n            if src.is_local():\n                with open(src.path, 'rb') as fd:\n                    while True:\n                        chunk = fd.read(64 * 1024)\n                        if not chunk:\n                            break\n                        hash_obj.update(chunk)\n                        with lock:\n                            progress.update(len(chunk))\n\n                    current_file_size = fd.tell()\n                    if current_file_size != size:\n                        warnings.warn(\n                            f\"Expected the package entry at {src!r} to be {size} B in size, but \"\n                            f\"found an object which is {current_file_size} B instead. This \"\n                            f\"indicates that the content of the file changed in between when you \"\n                            f\"included this  entry in the package (via set or set_dir) and now. \"\n                            f\"This should be avoided if possible.\"\n                        )\n\n            else:\n                params = dict(Bucket=src.bucket, Key=src.path)\n                if src.version_id is not None:\n                    params.update(dict(VersionId=src.version_id))\n                try:\n                    s3_client = S3ClientProvider().find_correct_client(S3Api.GET_OBJECT, src.bucket, params)\n\n                    resp = s3_client.get_object(**params)\n                    body = resp['Body']\n                    for chunk in body:\n                        hash_obj.update(chunk)\n                        with lock:\n                            progress.update(len(chunk))\n                except (ConnectionError, HTTPClientError, ReadTimeoutError):\n                    # TODO: Find a better way to warn users that we failed to compute this hash\n                    return None\n            return hash_obj.hexdigest()\n\n        with ThreadPoolExecutor() as executor:\n            results = executor.map(_process_url, src_list, sizes)\n\n    return results\n\n\ndef select(src, query, meta=None, raw=False, **kwargs):\n    \"\"\"Perform an S3 Select SQL query, return results as a Pandas DataFrame\n\n    The data returned by Boto3 for S3 Select is fairly convoluted, to say the\n    least.  This function returns the result as a dataframe instead.  It also\n    performs the following actions, for convenience:\n\n    * If quilt metadata is given, necessary info to handle the select query is\n      pulled from the format metadata.\n    * If no metadata is present, but the URL indicates an object with a known\n      extension, the file format (and potentially compression) are determeined\n      by that extension.\n      * Extension may include a compresssion extension in cases where that is\n        supported by AWS -- I.e, for queries on JSON or CSV files, .bz2 and\n        .gz are supported.\n      * Parquet files must not be compressed as a whole, and should not have\n        a compression extension.  However, columnar GZIP and Snappy are\n        transparently supported.\n\n    Args:\n        src(PhysicalKey):  S3 PhysicalKey of the object to query\n        query(str): An SQL query using the 'SELECT' directive. See examples at\n            https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html\n        meta: Quilt Object Metadata\n        raw(bool):  True to return the raw Boto3 response object\n        **kwargs:  s3_client.select() kwargs override.\n            All kwargs specified passed to S3 client directly, overriding\n            matching default/generated kwargs for `select_object_content()`.\n            Note that this will also override the bucket and key specified in\n            the URL if `Bucket` and `Key` are passed as kwargs.\n\n    Returns: pandas.DataFrame | dict\n        dict is returned if 'raw' is True or if OutputSerialization is set to\n            something other than JSON Lines.\n\n    \"\"\"\n    # We don't process any other kind of response at this time.\n    output_serialization = {'JSON': {}}\n    query_type = \"SQL\"  # AWS S3 doesn't currently support anything else.\n    meta = meta if meta is not None else {}\n\n    # Internal Format Name <--> S3 Format Name\n    valid_s3_select_formats = {\n        'parquet': 'Parquet',\n        'json': 'JSON',\n        'jsonl': 'JSON',\n        'csv': 'CSV',\n        }\n    # S3 Format Name <--> S3-Acceptable compression types\n    format_compression = {\n        'Parquet': ['NONE'],  # even if column-level compression has been used.\n        'JSON': ['NONE', 'BZIP2', 'GZIP'],\n        'CSV': ['NONE', 'BZIP2', 'GZIP'],\n        }\n    # File extension <--> S3-Acceptable compression type\n    # For compression type, when not specified in metadata.  Guess by extension.\n    accepted_compression = {\n        '.bz2': 'BZIP2',\n        '.gz': 'GZIP'\n        }\n    # Extension <--> Internal Format Name\n    # For file type, when not specified in metadata. Guess by extension.\n    ext_formats = {\n        '.parquet': 'parquet',\n        '.json': 'json',\n        '.jsonl': 'jsonl',\n        '.csv': 'csv',\n        '.tsv': 'csv',\n        '.ssv': 'csv',\n        }\n    delims = {'.tsv': '\\t', '.ssv': ';'}\n\n    assert not src.is_local(), \"src must be an S3 URL\"\n\n    # TODO: what about version_id???\n\n    # TODO: Use formats lib for this stuff\n    # use metadata to get format and compression\n    compression = None\n    format = meta.get('target')\n    if format is None:\n        format = meta.get('format', {}).get('name')\n        if format in ('bzip2', 'gzip'):\n            compression = format.upper()\n            format = meta.get('format', {}).get('contained_format', {}).get('name')\n\n    # use file extensions to get compression info, if none is present\n    exts = pathlib.Path(src.path).suffixes  # last of e.g. ['.periods', '.in', '.name', '.json', '.gz']\n    if exts and not compression:\n        if exts[-1].lower() in accepted_compression:\n            compression = accepted_compression[exts.pop(-1)]   # remove e.g. '.gz'\n    compression = compression if compression else 'NONE'\n\n    # use remaining file extensions to get format info, if none is present\n    csv_delim = None\n    if exts and not format:\n        ext = exts[-1].lower()    # last of e.g. ['.periods', '.in', '.name', '.json']\n        if ext in ext_formats:\n            format = ext_formats[ext]\n            csv_delim = delims.get(ext)\n            s3_format = valid_s3_select_formats[format]\n            ok_compression = format_compression[s3_format]\n            if compression not in ok_compression:\n                raise QuiltException(\"Compression {!r} not valid for select on format {!r}: \"\n                                     \"Expected {!r}\".format(compression, s3_format, ok_compression))\n    if not format:\n        raise QuiltException(\"Unable to discover format for select on {}\".format(src))\n\n    # At this point, we have a known format and enough information to use it.\n    s3_format = valid_s3_select_formats[format]\n\n    # Create InputSerialization section if not user-specified.\n    input_serialization = None\n    if 'InputSerialization' not in kwargs:\n        input_serialization = {'CompressionType': compression}\n        format_spec = input_serialization.setdefault(s3_format, {})\n\n        if s3_format == 'JSON':\n            format_spec['Type'] = \"LINES\" if format == 'jsonl' else \"DOCUMENT\"\n        elif s3_format == 'CSV':\n            if csv_delim is not None:\n                format_spec['FieldDelimiter'] = csv_delim\n\n    # These are processed and/or default args.\n    select_kwargs = dict(\n        Bucket=src.bucket,\n        Key=src.path,\n        Expression=query,\n        ExpressionType=query_type,\n        InputSerialization=input_serialization,\n        OutputSerialization=output_serialization,\n    )\n    # Include user-specified passthrough options, overriding other options\n    select_kwargs.update(kwargs)\n\n    # S3 Select does not support anonymous access (as of Jan 2019)\n    # https://docs.aws.amazon.com/AmazonS3/latest/API/API_SelectObjectContent.html\n    s3_client = S3ClientProvider().standard_client\n    response = s3_client.select_object_content(**select_kwargs)\n\n    # we don't want multiple copies of large chunks of data hanging around.\n    # ..iteration ftw.  It's what we get from amazon, anyways..\n    def iter_chunks(resp):\n        for item in resp['Payload']:\n            chunk = item.get('Records', {}).get('Payload')\n            if chunk is None:\n                continue\n            yield chunk\n\n    def iter_lines(resp, delimiter):\n        # S3 may break chunks off at any point, so we need to find line endings and handle\n        # line breaks manually.\n        # Note: this isn't reliable for CSV, because CSV may have a quoted line ending,\n        # whereas line endings in JSONLines content will be encoded cleanly.\n        lastline = ''\n        for chunk in iterdecode(iter_chunks(resp), 'utf-8'):\n            lines = chunk.split(delimiter)\n            lines[0] = lastline + lines[0]\n            lastline = lines.pop(-1)\n            for line in lines:\n                yield line + delimiter\n        yield lastline\n\n    if not raw:\n        # JSON used for processed content as it doesn't have the ambiguity of CSV.\n        if 'JSON' in select_kwargs[\"OutputSerialization\"]:\n            delimiter = select_kwargs['OutputSerialization']['JSON'].get('RecordDelimiter', '\\n')\n            reader = jsonlines.Reader(line.strip() for line in iter_lines(response, delimiter)\n                                      if line.strip())\n            # noinspection PyPackageRequirements\n            from pandas import DataFrame   # Lazy import for slow module\n            # !! if this response type is modified, update related docstrings on Bucket.select().\n            return DataFrame.from_records(x for x in reader)\n        # If there's some need, we could implement some other OutputSerialization format here.\n        # If they've specified an OutputSerialization key we don't handle, just give them the\n        # raw response.\n    # !! if this response type is modified, update related docstrings on Bucket.select().\n    return response\n", "idx": 8, "id": 18389, "msg": "", "proj": "quiltdata-quilt", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -236,9 +236,7 @@ void ImageDataLayer<Dtype>::CreatePrefetchThread() {\n   phase_ = Caffe::phase();\n   const bool prefetch_needs_rand =\n       this->layer_param_.image_data_param().shuffle() ||\n-          ((phase_ == Caffe::TRAIN) &&\n-           (this->layer_param_.image_data_param().mirror() ||\n-            this->layer_param_.image_data_param().crop_size()));\n+      this->layer_param_.image_data_param().crop_size();\n   if (prefetch_needs_rand) {\n     const unsigned int prefetch_rng_seed = caffe_rng_rand();\n     prefetch_rng_.reset(new Caffe::RNG(prefetch_rng_seed));", "y": 1, "oldf": "// Copyright 2014 BVLC and contributors.\n\n#include <stdint.h>\n#include <leveldb/db.h>\n#include <pthread.h>\n\n#include <string>\n#include <vector>\n#include <iostream>  // NOLINT(readability/streams)\n#include <fstream>  // NOLINT(readability/streams)\n#include <utility>\n\n#include \"caffe/layer.hpp\"\n#include \"caffe/util/io.hpp\"\n#include \"caffe/util/math_functions.hpp\"\n#include \"caffe/util/rng.hpp\"\n#include \"caffe/vision_layers.hpp\"\n\nusing std::iterator;\nusing std::string;\nusing std::pair;\n\nnamespace caffe {\n\ntemplate <typename Dtype>\nvoid* ImageDataLayerPrefetch(void* layer_pointer) {\n  CHECK(layer_pointer);\n  ImageDataLayer<Dtype>* layer =\n      reinterpret_cast<ImageDataLayer<Dtype>*>(layer_pointer);\n  CHECK(layer);\n  Datum datum;\n  CHECK(layer->prefetch_data_);\n  Dtype* top_data = layer->prefetch_data_->mutable_cpu_data();\n  Dtype* top_label = layer->prefetch_label_->mutable_cpu_data();\n  ImageDataParameter image_data_param = layer->layer_param_.image_data_param();\n  const Dtype scale = image_data_param.scale();\n  const int batch_size = image_data_param.batch_size();\n  const int crop_size = image_data_param.crop_size();\n  const bool mirror = image_data_param.mirror();\n  const int new_height = image_data_param.new_height();\n  const int new_width = image_data_param.new_width();\n\n  if (mirror && crop_size == 0) {\n    LOG(FATAL) << \"Current implementation requires mirror and crop_size to be \"\n        << \"set at the same time.\";\n  }\n  // datum scales\n  const int channels = layer->datum_channels_;\n  const int height = layer->datum_height_;\n  const int width = layer->datum_width_;\n  const int size = layer->datum_size_;\n  const int lines_size = layer->lines_.size();\n  const Dtype* mean = layer->data_mean_.cpu_data();\n  for (int item_id = 0; item_id < batch_size; ++item_id) {\n    // get a blob\n    CHECK_GT(lines_size, layer->lines_id_);\n    if (!ReadImageToDatum(layer->lines_[layer->lines_id_].first,\n          layer->lines_[layer->lines_id_].second,\n          new_height, new_width, &datum)) {\n      continue;\n    }\n    const string& data = datum.data();\n    if (crop_size) {\n      CHECK(data.size()) << \"Image cropping only support uint8 data\";\n      int h_off, w_off;\n      // We only do random crop when we do training.\n      if (layer->phase_ == Caffe::TRAIN) {\n        h_off = layer->PrefetchRand() % (height - crop_size);\n        w_off = layer->PrefetchRand() % (width - crop_size);\n      } else {\n        h_off = (height - crop_size) / 2;\n        w_off = (width - crop_size) / 2;\n      }\n      if (mirror && layer->PrefetchRand() % 2) {\n        // Copy mirrored version\n        for (int c = 0; c < channels; ++c) {\n          for (int h = 0; h < crop_size; ++h) {\n            for (int w = 0; w < crop_size; ++w) {\n              int top_index = ((item_id * channels + c) * crop_size + h)\n                              * crop_size + (crop_size - 1 - w);\n              int data_index = (c * height + h + h_off) * width + w + w_off;\n              Dtype datum_element =\n                  static_cast<Dtype>(static_cast<uint8_t>(data[data_index]));\n              top_data[top_index] = (datum_element - mean[data_index]) * scale;\n            }\n          }\n        }\n      } else {\n        // Normal copy\n        for (int c = 0; c < channels; ++c) {\n          for (int h = 0; h < crop_size; ++h) {\n            for (int w = 0; w < crop_size; ++w) {\n              int top_index = ((item_id * channels + c) * crop_size + h)\n                              * crop_size + w;\n              int data_index = (c * height + h + h_off) * width + w + w_off;\n              Dtype datum_element =\n                  static_cast<Dtype>(static_cast<uint8_t>(data[data_index]));\n              top_data[top_index] = (datum_element - mean[data_index]) * scale;\n            }\n          }\n        }\n      }\n    } else {\n      // Just copy the whole data\n      if (data.size()) {\n        for (int j = 0; j < size; ++j) {\n          Dtype datum_element =\n              static_cast<Dtype>(static_cast<uint8_t>(data[j]));\n          top_data[item_id * size + j] = (datum_element - mean[j]) * scale;\n        }\n      } else {\n        for (int j = 0; j < size; ++j) {\n          top_data[item_id * size + j] =\n              (datum.float_data(j) - mean[j]) * scale;\n        }\n      }\n    }\n\n    top_label[item_id] = datum.label();\n    // go to the next iter\n    layer->lines_id_++;\n    if (layer->lines_id_ >= lines_size) {\n      // We have reached the end. Restart from the first.\n      DLOG(INFO) << \"Restarting data prefetching from start.\";\n      layer->lines_id_ = 0;\n      if (layer->layer_param_.image_data_param().shuffle()) {\n        layer->ShuffleImages();\n      }\n    }\n  }\n\n  return reinterpret_cast<void*>(NULL);\n}\n\ntemplate <typename Dtype>\nImageDataLayer<Dtype>::~ImageDataLayer<Dtype>() {\n  JoinPrefetchThread();\n}\n\ntemplate <typename Dtype>\nvoid ImageDataLayer<Dtype>::SetUp(const vector<Blob<Dtype>*>& bottom,\n      vector<Blob<Dtype>*>* top) {\n  Layer<Dtype>::SetUp(bottom, top);\n  const int new_height  = this->layer_param_.image_data_param().new_height();\n  const int new_width  = this->layer_param_.image_data_param().new_height();\n  CHECK((new_height == 0 && new_width == 0) ||\n      (new_height > 0 && new_width > 0)) << \"Current implementation requires \"\n      \"new_height and new_width to be set at the same time.\";\n  // Read the file with filenames and labels\n  const string& source = this->layer_param_.image_data_param().source();\n  LOG(INFO) << \"Opening file \" << source;\n  std::ifstream infile(source.c_str());\n  string filename;\n  int label;\n  while (infile >> filename >> label) {\n    lines_.push_back(std::make_pair(filename, label));\n  }\n\n  if (this->layer_param_.image_data_param().shuffle()) {\n    // randomly shuffle data\n    LOG(INFO) << \"Shuffling data\";\n    const unsigned int prefetch_rng_seed = caffe_rng_rand();\n    prefetch_rng_.reset(new Caffe::RNG(prefetch_rng_seed));\n    ShuffleImages();\n  }\n  LOG(INFO) << \"A total of \" << lines_.size() << \" images.\";\n\n  lines_id_ = 0;\n  // Check if we would need to randomly skip a few data points\n  if (this->layer_param_.image_data_param().rand_skip()) {\n    unsigned int skip = caffe_rng_rand() %\n        this->layer_param_.image_data_param().rand_skip();\n    LOG(INFO) << \"Skipping first \" << skip << \" data points.\";\n    CHECK_GT(lines_.size(), skip) << \"Not enough points to skip\";\n    lines_id_ = skip;\n  }\n  // Read a data point, and use it to initialize the top blob.\n  Datum datum;\n  CHECK(ReadImageToDatum(lines_[lines_id_].first, lines_[lines_id_].second,\n                         new_height, new_width, &datum));\n  // image\n  const int crop_size = this->layer_param_.image_data_param().crop_size();\n  const int batch_size = this->layer_param_.image_data_param().batch_size();\n  const string& mean_file = this->layer_param_.image_data_param().mean_file();\n  if (crop_size > 0) {\n    (*top)[0]->Reshape(batch_size, datum.channels(), crop_size, crop_size);\n    prefetch_data_.reset(new Blob<Dtype>(batch_size, datum.channels(),\n                                         crop_size, crop_size));\n  } else {\n    (*top)[0]->Reshape(batch_size, datum.channels(), datum.height(),\n                       datum.width());\n    prefetch_data_.reset(new Blob<Dtype>(batch_size, datum.channels(),\n                                         datum.height(), datum.width()));\n  }\n  LOG(INFO) << \"output data size: \" << (*top)[0]->num() << \",\"\n      << (*top)[0]->channels() << \",\" << (*top)[0]->height() << \",\"\n      << (*top)[0]->width();\n  // label\n  (*top)[1]->Reshape(batch_size, 1, 1, 1);\n  prefetch_label_.reset(new Blob<Dtype>(batch_size, 1, 1, 1));\n  // datum size\n  datum_channels_ = datum.channels();\n  datum_height_ = datum.height();\n  datum_width_ = datum.width();\n  datum_size_ = datum.channels() * datum.height() * datum.width();\n  CHECK_GT(datum_height_, crop_size);\n  CHECK_GT(datum_width_, crop_size);\n  // check if we want to have mean\n  if (this->layer_param_.image_data_param().has_mean_file()) {\n    BlobProto blob_proto;\n    LOG(INFO) << \"Loading mean file from\" << mean_file;\n    ReadProtoFromBinaryFile(mean_file.c_str(), &blob_proto);\n    data_mean_.FromProto(blob_proto);\n    CHECK_EQ(data_mean_.num(), 1);\n    CHECK_EQ(data_mean_.channels(), datum_channels_);\n    CHECK_EQ(data_mean_.height(), datum_height_);\n    CHECK_EQ(data_mean_.width(), datum_width_);\n  } else {\n    // Simply initialize an all-empty mean.\n    data_mean_.Reshape(1, datum_channels_, datum_height_, datum_width_);\n  }\n  // Now, start the prefetch thread. Before calling prefetch, we make two\n  // cpu_data calls so that the prefetch thread does not accidentally make\n  // simultaneous cudaMalloc calls when the main thread is running. In some\n  // GPUs this seems to cause failures if we do not so.\n  prefetch_data_->mutable_cpu_data();\n  prefetch_label_->mutable_cpu_data();\n  data_mean_.cpu_data();\n  DLOG(INFO) << \"Initializing prefetch\";\n  CreatePrefetchThread();\n  DLOG(INFO) << \"Prefetch initialized.\";\n}\n\ntemplate <typename Dtype>\nvoid ImageDataLayer<Dtype>::CreatePrefetchThread() {\n  phase_ = Caffe::phase();\n  const bool prefetch_needs_rand =\n      this->layer_param_.image_data_param().shuffle() ||\n          ((phase_ == Caffe::TRAIN) &&\n           (this->layer_param_.image_data_param().mirror() ||\n            this->layer_param_.image_data_param().crop_size()));\n  if (prefetch_needs_rand) {\n    const unsigned int prefetch_rng_seed = caffe_rng_rand();\n    prefetch_rng_.reset(new Caffe::RNG(prefetch_rng_seed));\n  } else {\n    prefetch_rng_.reset();\n  }\n  // Create the thread.\n  CHECK(!pthread_create(&thread_, NULL, ImageDataLayerPrefetch<Dtype>,\n        static_cast<void*>(this))) << \"Pthread execution failed.\";\n}\n\ntemplate <typename Dtype>\nvoid ImageDataLayer<Dtype>::ShuffleImages() {\n  const int num_images = lines_.size();\n  for (int i = 0; i < num_images; ++i) {\n    const int max_rand_index = num_images - i;\n    const int rand_index = PrefetchRand() % max_rand_index;\n    pair<string, int> item = lines_[rand_index];\n    lines_.erase(lines_.begin() + rand_index);\n    lines_.push_back(item);\n  }\n}\n\ntemplate <typename Dtype>\nvoid ImageDataLayer<Dtype>::JoinPrefetchThread() {\n  CHECK(!pthread_join(thread_, NULL)) << \"Pthread joining failed.\";\n}\n\ntemplate <typename Dtype>\nunsigned int ImageDataLayer<Dtype>::PrefetchRand() {\n  caffe::rng_t* prefetch_rng =\n      static_cast<caffe::rng_t*>(prefetch_rng_->generator());\n  return (*prefetch_rng)();\n}\n\ntemplate <typename Dtype>\nDtype ImageDataLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,\n      vector<Blob<Dtype>*>* top) {\n  // First, join the thread\n  JoinPrefetchThread();\n  // Copy the data\n  caffe_copy(prefetch_data_->count(), prefetch_data_->cpu_data(),\n             (*top)[0]->mutable_cpu_data());\n  caffe_copy(prefetch_label_->count(), prefetch_label_->cpu_data(),\n             (*top)[1]->mutable_cpu_data());\n  // Start a new prefetch thread\n  CreatePrefetchThread();\n  return Dtype(0.);\n}\n\nINSTANTIATE_CLASS(ImageDataLayer);\n\n}  // namespace caffe\n", "idx": 1, "id": 28747, "msg": "Is this change intended? I guess it is fine to initialize a RNG and not use it, but this modification seems unnecessary...", "proj": "BVLC-caffe", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -135,7 +135,7 @@ func patchDuplicateKeyUsage(usages []cmapi.KeyUsage) []cmapi.KeyUsage {\n \t\t\tnewUsages = append(newUsages, cmapi.UsageDigitalSignature)\n \t\t\t// prevent having 2 UsageDigitalSignature in the slice\n \t\t\thasUsageSigning = true\n-\t\t} else {\n+\t\t} else if usage != cmapi.UsageSigning && usage != cmapi.UsageDigitalSignature {\n \t\t\tnewUsages = append(newUsages, usage)\n \t\t}\n \t}", "y": 1, "oldf": "/*\nCopyright 2019 The Jetstack cert-manager contributors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage validation\n\nimport (\n\t\"crypto/x509\"\n\t\"encoding/asn1\"\n\t\"fmt\"\n\n\t\"github.com/jetstack/cert-manager/pkg/util\"\n\n\t\"reflect\"\n\n\t\"github.com/kr/pretty\"\n\n\t\"k8s.io/apimachinery/pkg/runtime\"\n\t\"k8s.io/apimachinery/pkg/util/validation/field\"\n\n\tcmapi \"github.com/jetstack/cert-manager/pkg/internal/apis/certmanager\"\n\t\"github.com/jetstack/cert-manager/pkg/util/pki\"\n)\n\nvar defaultInternalKeyUsages = []cmapi.KeyUsage{cmapi.UsageDigitalSignature, cmapi.UsageKeyEncipherment}\n\nfunc ValidateCertificateRequest(obj runtime.Object) field.ErrorList {\n\tcr := obj.(*cmapi.CertificateRequest)\n\tallErrs := ValidateCertificateRequestSpec(&cr.Spec, field.NewPath(\"spec\"), true)\n\treturn allErrs\n}\n\nfunc ValidateUpdateCertificateRequest(oldObj, obj runtime.Object) field.ErrorList {\n\tcr := obj.(*cmapi.CertificateRequest)\n\t// do not check the CSR content here not to break existing resources on upgrade\n\tallErrs := ValidateCertificateRequestSpec(&cr.Spec, field.NewPath(\"spec\"), false)\n\treturn allErrs\n}\n\nfunc ValidateCertificateRequestSpec(crSpec *cmapi.CertificateRequestSpec, fldPath *field.Path, validateCSRContent bool) field.ErrorList {\n\tel := field.ErrorList{}\n\n\tel = append(el, validateIssuerRef(crSpec.IssuerRef, fldPath)...)\n\n\tif len(crSpec.Request) == 0 {\n\t\tel = append(el, field.Required(fldPath.Child(\"request\"), \"must be specified\"))\n\t} else {\n\t\tcsr, err := pki.DecodeX509CertificateRequestBytes(crSpec.Request)\n\t\tif err != nil {\n\t\t\tel = append(el, field.Invalid(fldPath.Child(\"request\"), crSpec.Request, fmt.Sprintf(\"failed to decode csr: %s\", err)))\n\t\t} else {\n\t\t\t// only compare usages if set on CR and in the CSR\n\t\t\tif len(crSpec.Usages) > 0 && len(csr.Extensions) > 0 && validateCSRContent && !reflect.DeepEqual(crSpec.Usages, defaultInternalKeyUsages) {\n\t\t\t\tif crSpec.IsCA {\n\t\t\t\t\tcrSpec.Usages = ensureCertSignIsSet(crSpec.Usages)\n\t\t\t\t}\n\t\t\t\tcsrUsages, err := getCSRKeyUsage(crSpec, fldPath, csr, el)\n\t\t\t\tif len(err) > 0 {\n\t\t\t\t\tel = append(el, err...)\n\t\t\t\t} else if len(csrUsages) > 0 && !isUsageEqual(csrUsages, crSpec.Usages) && !isUsageEqual(csrUsages, defaultInternalKeyUsages) {\n\t\t\t\t\tel = append(el, field.Invalid(fldPath.Child(\"request\"), crSpec.Request, fmt.Sprintf(\"csr key usages do not match specified usages, these should match if both are set: %s\", pretty.Diff(patchDuplicateKeyUsage(csrUsages), patchDuplicateKeyUsage(crSpec.Usages)))))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn el\n}\n\nfunc getCSRKeyUsage(crSpec *cmapi.CertificateRequestSpec, fldPath *field.Path, csr *x509.CertificateRequest, el field.ErrorList) ([]cmapi.KeyUsage, field.ErrorList) {\n\tvar ekus []x509.ExtKeyUsage\n\tvar ku x509.KeyUsage\n\n\tfor _, extension := range csr.Extensions {\n\t\tif extension.Id.String() == asn1.ObjectIdentifier(pki.OIDExtensionExtendedKeyUsage).String() {\n\t\t\tvar asn1ExtendedUsages []asn1.ObjectIdentifier\n\t\t\t_, err := asn1.Unmarshal(extension.Value, &asn1ExtendedUsages)\n\t\t\tif err != nil {\n\t\t\t\tel = append(el, field.Invalid(fldPath.Child(\"request\"), crSpec.Request, fmt.Sprintf(\"failed to decode csr extended usages: %s\", err)))\n\t\t\t} else {\n\t\t\t\tfor _, asnExtUsage := range asn1ExtendedUsages {\n\t\t\t\t\teku, ok := pki.ExtKeyUsageFromOID(asnExtUsage)\n\t\t\t\t\tif ok {\n\t\t\t\t\t\tekus = append(ekus, eku)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif extension.Id.String() == asn1.ObjectIdentifier(pki.OIDExtensionKeyUsage).String() {\n\t\t\t// RFC 5280, 4.2.1.3\n\t\t\tvar asn1bits asn1.BitString\n\t\t\t_, err := asn1.Unmarshal(extension.Value, &asn1bits)\n\t\t\tif err != nil {\n\t\t\t\tel = append(el, field.Invalid(fldPath.Child(\"request\"), crSpec.Request, fmt.Sprintf(\"failed to decode csr usages: %s\", err)))\n\t\t\t} else {\n\t\t\t\tvar usage int\n\t\t\t\tfor i := 0; i < 9; i++ {\n\t\t\t\t\tif asn1bits.At(i) != 0 {\n\t\t\t\t\t\tusage |= 1 << uint(i)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tku = x509.KeyUsage(usage)\n\t\t\t}\n\t\t}\n\t}\n\n\t// convert usages to the internal API\n\tvar out []cmapi.KeyUsage\n\tfor _, usage := range pki.BuildCertManagerKeyUsages(ku, ekus) {\n\t\tout = append(out, cmapi.KeyUsage(usage))\n\t}\n\treturn out, el\n}\n\nfunc patchDuplicateKeyUsage(usages []cmapi.KeyUsage) []cmapi.KeyUsage {\n\t// usage signing and digital signature are the same key use in x509\n\t// we should patch this for proper validation\n\n\tnewUsages := []cmapi.KeyUsage(nil)\n\thasUsageSigning := false\n\tfor _, usage := range usages {\n\t\tif (usage == cmapi.UsageSigning || usage == cmapi.UsageDigitalSignature) && !hasUsageSigning {\n\t\t\tnewUsages = append(newUsages, cmapi.UsageDigitalSignature)\n\t\t\t// prevent having 2 UsageDigitalSignature in the slice\n\t\t\thasUsageSigning = true\n\t\t} else {\n\t\t\tnewUsages = append(newUsages, usage)\n\t\t}\n\t}\n\n\treturn newUsages\n}\n\nfunc isUsageEqual(a, b []cmapi.KeyUsage) bool {\n\ta = patchDuplicateKeyUsage(a)\n\tb = patchDuplicateKeyUsage(b)\n\n\tvar aStrings, bStrings []string\n\n\tfor _, usage := range a {\n\t\taStrings = append(aStrings, string(usage))\n\t}\n\n\tfor _, usage := range b {\n\t\tbStrings = append(bStrings, string(usage))\n\t}\n\n\treturn util.EqualUnsorted(aStrings, bStrings)\n}\n\n// ensureCertSignIsSet adds UsageCertSign in case it is not set\n// TODO: add a mutating webhook to make sure this is always set\n// when isCA is true.\nfunc ensureCertSignIsSet(list []cmapi.KeyUsage) []cmapi.KeyUsage {\n\tfor _, usage := range list {\n\t\tif usage == cmapi.UsageCertSign {\n\t\t\treturn list\n\t\t}\n\t}\n\n\treturn append(list, cmapi.UsageCertSign)\n}\n", "idx": 1, "id": 23658, "msg": "nit: Did you consider moving the `hasUsageSigning` check into a nested `if statement`? Then this `else if` would not be necessary.", "proj": "jetstack-cert-manager", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -155,7 +155,10 @@ public final class AnimationID\n \tpublic static final int SAND_COLLECTION = 895;\n \tpublic static final int PISCARILIUS_CRANE_REPAIR = 7199;\n \tpublic static final int HOME_MAKE_TABLET = 4067;\n-\n+\tpublic static final int THIEVING_STALL = 832;\n+\tpublic static final int PICKPOCKET_SUCCESS = 881;\n+\tpublic static final int PICKPOCKET_FAIL = 424;\n+\t\n \t//block animations for players and perhaps npcs as well?\n \tpublic static final int BLOCK_DEFENDER = 4177;\n \tpublic static final int BLOCK_NO_SHIELD = 420;", "y": 1, "oldf": "/*\n * Copyright (c) 2016-2017, Abel Briggs\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * 1. Redistributions of source code must retain the above copyright notice, this\n *    list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright notice,\n *    this list of conditions and the following disclaimer in the documentation\n *    and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\npackage net.runelite.api;\n\n/**\n * Utility class used for mapping animation IDs.\n * <p>\n * Note: This class is not complete and may not contain a specific animation\n * required.\n */\npublic final class AnimationID\n{\n\tpublic static final int IDLE = -1;\n\tpublic static final int HERBLORE_PESTLE_AND_MORTAR = 364;\n\tpublic static final int WOODCUTTING_BRONZE = 879;\n\tpublic static final int WOODCUTTING_IRON = 877;\n\tpublic static final int WOODCUTTING_STEEL = 875;\n\tpublic static final int WOODCUTTING_BLACK = 873;\n\tpublic static final int WOODCUTTING_MITHRIL = 871;\n\tpublic static final int WOODCUTTING_ADAMANT = 869;\n\tpublic static final int WOODCUTTING_RUNE = 867;\n\tpublic static final int WOODCUTTING_DRAGON = 2846;\n\tpublic static final int WOODCUTTING_INFERNAL = 2117;\n\tpublic static final int WOODCUTTING_3A_AXE = 7264;\n\tpublic static final int CONSUMING = 829; // consuming consumables\n\tpublic static final int FIREMAKING = 733;\n\tpublic static final int DEATH = 836;\n\tpublic static final int COOKING_FIRE = 897;\n\tpublic static final int COOKING_RANGE = 896;\n\tpublic static final int COOKING_WINE = 7529;\n\tpublic static final int FLETCHING_BOW_CUTTING = 1248;\n\tpublic static final int HUNTER_LAY_BOXTRAP_BIRDSNARE = 5208; //same for laying bird snares and box traps\n\tpublic static final int HUNTER_LAY_DEADFALLTRAP = 5212; //setting up deadfall trap\n\tpublic static final int HUNTER_LAY_NETTRAP = 5215; //setting up net trap\n\tpublic static final int HUNTER_LAY_MANIACAL_MONKEY_BOULDER_TRAP = 7259; // setting up maniacal monkey boulder trap\n\tpublic static final int HUNTER_CHECK_BIRD_SNARE = 5207;\n\tpublic static final int HUNTER_CHECK_BOX_TRAP = 5212;\n\tpublic static final int HERBLORE_MAKE_TAR = 5249;\n\tpublic static final int FLETCHING_STRING_NORMAL_SHORTBOW = 6678;\n\tpublic static final int FLETCHING_STRING_NORMAL_LONGBOW = 6684;\n\tpublic static final int FLETCHING_STRING_OAK_SHORTBOW = 6679;\n\tpublic static final int FLETCHING_STRING_OAK_LONGBOW = 6685;\n\tpublic static final int FLETCHING_STRING_WILLOW_SHORTBOW = 6680;\n\tpublic static final int FLETCHING_STRING_WILLOW_LONGBOW = 6686;\n\tpublic static final int FLETCHING_STRING_MAPLE_SHORTBOW = 6681;\n\tpublic static final int FLETCHING_STRING_MAPLE_LONGBOW = 6687;\n\tpublic static final int FLETCHING_STRING_YEW_SHORTBOW = 6682;\n\tpublic static final int FLETCHING_STRING_YEW_LONGBOW = 6688;\n\tpublic static final int FLETCHING_STRING_MAGIC_SHORTBOW = 6683;\n\tpublic static final int FLETCHING_STRING_MAGIC_LONGBOW = 6689;\n\tpublic static final int GEM_CUTTING_OPAL = 890;\n\tpublic static final int GEM_CUTTING_JADE = 891;\n\tpublic static final int GEM_CUTTING_REDTOPAZ = 892;\n\tpublic static final int GEM_CUTTING_SAPPHIRE = 888;\n\tpublic static final int GEM_CUTTING_EMERALD = 889;\n\tpublic static final int GEM_CUTTING_RUBY = 887;\n\tpublic static final int GEM_CUTTING_DIAMOND = 886;\n\tpublic static final int GEM_CUTTING_AMETHYST = 6295;\n\tpublic static final int CRAFTING_LEATHER = 1249;\n\tpublic static final int CRAFTING_GLASSBLOWING = 884;\n\tpublic static final int CRAFTING_SPINNING = 894;\n\tpublic static final int CRAFTING_POTTERS_WHEEL = 883;\n\tpublic static final int CRAFTING_POTTERY_OVEN = 24975;\n\tpublic static final int SMITHING_SMELTING = 899;\n\tpublic static final int SMITHING_CANNONBALL = 827; //cball smithing uses this and SMITHING_SMELTING\n\tpublic static final int SMITHING_ANVIL = 898;\n\tpublic static final int FISHING_BIG_NET = 620;\n\tpublic static final int FISHING_NET = 621;\n\tpublic static final int FISHING_POLE_CAST = 623; // pole is in the water\n\tpublic static final int FISHING_CAGE = 619;\n\tpublic static final int FISHING_HARPOON = 618;\n\tpublic static final int FISHING_BARBTAIL_HARPOON = 5108;\n\tpublic static final int FISHING_DRAGON_HARPOON = 7401;\n\tpublic static final int FISHING_INFERNAL_HARPOON = 7402;\n\tpublic static final int FISHING_OILY_ROD = 622;\n\tpublic static final int FISHING_KARAMBWAN = 1193;\n\tpublic static final int FISHING_CRUSHING_INFERNAL_EELS = 7553;\n\tpublic static final int FISHING_CUTTING_SACRED_EELS = 7151;\n\tpublic static final int FISHING_BAREHAND = 6709;\n\tpublic static final int MINING_BRONZE_PICKAXE = 625;\n\tpublic static final int MINING_IRON_PICKAXE = 626;\n\tpublic static final int MINING_STEEL_PICKAXE = 627;\n\tpublic static final int MINING_BLACK_PICKAXE = 3873;\n\tpublic static final int MINING_MITHRIL_PICKAXE = 629;\n\tpublic static final int MINING_ADAMANT_PICKAXE = 628;\n\tpublic static final int MINING_RUNE_PICKAXE = 624;\n\tpublic static final int MINING_DRAGON_PICKAXE = 7139;\n\tpublic static final int MINING_DRAGON_PICKAXE_ORN = 642;\n\tpublic static final int MINING_INFERNAL_PICKAXE = 4482;\n\tpublic static final int MINING_3A_PICKAXE = 7283;\n\tpublic static final int MINING_MOTHERLODE_BRONZE = 6753;\n\tpublic static final int MINING_MOTHERLODE_IRON = 6754;\n\tpublic static final int MINING_MOTHERLODE_STEEL = 6755;\n\tpublic static final int MINING_MOTHERLODE_BLACK = 3866;\n\tpublic static final int MINING_MOTHERLODE_MITHRIL = 6757;\n\tpublic static final int MINING_MOTHERLODE_ADAMANT = 6756;\n\tpublic static final int MINING_MOTHERLODE_RUNE = 6752;\n\tpublic static final int MINING_MOTHERLODE_DRAGON = 6758;\n\tpublic static final int MINING_MOTHERLODE_DRAGON_ORN = 335;\n\tpublic static final int MINING_MOTHERLODE_INFERNAL = 4481;\n\tpublic static final int MINING_MOTHERLODE_3A = 7282;\n\tpublic static final int DENSE_ESSENCE_CHIPPING = 7201;\n\tpublic static final int DENSE_ESSENCE_CHISELING = 7202;\n\tpublic static final int HERBLORE_POTIONMAKING = 363; //used for both herb and secondary\n\tpublic static final int MAGIC_CHARGING_ORBS = 726;\n\tpublic static final int MAGIC_MAKE_TABLET = 4068;\n\tpublic static final int MAGIC_ENCHANTING_JEWELRY = 931;\n\tpublic static final int MAGIC_ENCHANTING_AMULET_1 = 719; // sapphire, opal, diamond\n\tpublic static final int MAGIC_ENCHANTING_AMULET_2 = 720; // emerald, jade, dragonstone\n\tpublic static final int MAGIC_ENCHANTING_AMULET_3 = 721; // ruby, topaz, onyx, zenyte\n\tpublic static final int BURYING_BONES = 827;\n\tpublic static final int USING_GILDED_ALTAR = 3705;\n\tpublic static final int LOOKING_INTO = 832;\n\tpublic static final int DIG = 830;\n\tpublic static final int DEMONIC_GORILLA_MAGIC_ATTACK = 7225;\n\tpublic static final int DEMONIC_GORILLA_MELEE_ATTACK = 7226;\n\tpublic static final int DEMONIC_GORILLA_RANGED_ATTACK = 7227;\n\tpublic static final int DEMONIC_GORILLA_AOE_ATTACK = 7228;\n\tpublic static final int DEMONIC_GORILLA_PRAYER_SWITCH = 7228;\n\tpublic static final int DEMONIC_GORILLA_DEFEND = 7224;\n\tpublic static final int BOOK_HOME_TELEPORT_1 = 4847;\n\tpublic static final int BOOK_HOME_TELEPORT_2 = 4850;\n\tpublic static final int BOOK_HOME_TELEPORT_3 = 4853;\n\tpublic static final int BOOK_HOME_TELEPORT_4 = 4855;\n\tpublic static final int BOOK_HOME_TELEPORT_5 = 4857;\n\tpublic static final int COW_HOME_TELEPORT_1 = 1696;\n\tpublic static final int COW_HOME_TELEPORT_2 = 1697;\n\tpublic static final int COW_HOME_TELEPORT_3 = 1698;\n\tpublic static final int COW_HOME_TELEPORT_4 = 1699;\n\tpublic static final int COW_HOME_TELEPORT_5 = 1700;\n\tpublic static final int COW_HOME_TELEPORT_6 = 1701;\n\tpublic static final int CONSTRUCTION = 3676;\n\tpublic static final int SAND_COLLECTION = 895;\n\tpublic static final int PISCARILIUS_CRANE_REPAIR = 7199;\n\tpublic static final int HOME_MAKE_TABLET = 4067;\n\n\t//block animations for players and perhaps npcs as well?\n\tpublic static final int BLOCK_DEFENDER = 4177;\n\tpublic static final int BLOCK_NO_SHIELD = 420;\n\tpublic static final int BLOCK_SHIELD = 1156;\n\tpublic static final int BLOCK_SWORD = 388;\n\tpublic static final int BLOCK_UNARMED = 424;\n\n\t// NPC animations\n\tpublic static final int TZTOK_JAD_MAGIC_ATTACK = 2656;\n\tpublic static final int TZTOK_JAD_RANGE_ATTACK = 2652;\n\tpublic static final int HELLHOUND_DEFENCE = 6566;\n\tpublic static final int VORKATH_WAKE_UP = 7950;\n\tpublic static final int VORKATH_DEATH = 7949;\n\tpublic static final int VORKATH_SLASH_ATTACK = 7951;\n\tpublic static final int VORKATH_ATTACK = 7952;\n\tpublic static final int VORKATH_FIRE_BOMB_ATTACK = 7960;\n\tpublic static final int VORKATH_ACID_ATTACK = 7957;\n\tpublic static final int BLACKJACK_KO = 838;\n\tpublic static final int VETION_EARTHQUAKE = 5507;\n\tpublic static final int ZULRAH_DEATH = 5804;\n\n\t// Farming\n\tpublic static final int FARMING_HARVEST_FRUIT_TREE = 2280;\n\tpublic static final int FARMING_HARVEST_BUSH = 2281;\n\tpublic static final int FARMING_HARVEST_HERB = 2282;\n\tpublic static final int FARMING_USE_COMPOST = 2283;\n\tpublic static final int FARMING_CURE_WITH_POTION = 2288;\n\tpublic static final int FARMING_PLANT_SEED = 2291;\n\tpublic static final int FARMING_HARVEST_FLOWER = 2292;\n\tpublic static final int FARMING_MIX_ULTRACOMPOST = 7699;\n\n\t// Lunar spellbook\n\tpublic static final int ENERGY_TRANSFER_VENGEANCE_OTHER = 4411;\n\tpublic static final int MAGIC_LUNAR_SHARED = 4413; // Utilized by Fertile Soil, Boost/Stat Potion Share, NPC Contact, Bake Pie\n\tpublic static final int MAGIC_LUNAR_CURE_PLANT = 4432;\n\tpublic static final int MAGIC_LUNAR_GEOMANCY = 7118;\n\tpublic static final int MAGIC_LUNAR_PLANK_MAKE = 6298;\n\tpublic static final int MAGIC_LUNAR_STRING_JEWELRY = 4412;\n\n\t// Arceuus spellbook\n\tpublic static final int MAGIC_ARCEUUS_RESURRECT_CROPS = 7118;\n\n\t// Battlestaff Crafting\n\tpublic static final int CRAFTING_BATTLESTAVES = 7531;\n\n\t// Death Animations\n\tpublic static final int CAVE_KRAKEN_DEATH = 3993;\n\tpublic static final int WIZARD_DEATH = 2553;\n\tpublic static final int GARGOYLE_DEATH = 1520;\n\tpublic static final int MARBLE_GARGOYLE_DEATH = 7813;\n\tpublic static final int LIZARD_DEATH = 2778;\n\tpublic static final int ROCKSLUG_DEATH = 1568;\n\tpublic static final int ZYGOMITE_DEATH = 3327;\n\tpublic static final int IMP_DEATH = 172;\n\n\t// POH Animations\n\tpublic static final int INCENSE_BURNER = 3687;\n\tpublic static final int LOW_LEVEL_MAGIC_ATTACK = 1162;\n\tpublic static final int HIGH_LEVEL_MAGIC_ATTACK = 1167;\n\tpublic static final int BLOWPIPE_ATTACK = 5061;\n\n\t// Hydra\n\tpublic static final int HYDRA_POISON_1 = 8234;\n\tpublic static final int HYDRA_RANGED_1 = 8235;\n\tpublic static final int HYDRA_MAGIC_1 = 8236;\n\tpublic static final int HYDRA_1_1 = 8237;\n\tpublic static final int HYDRA_1_2 = 8238;\n\tpublic static final int HYDRA_LIGHTNING = 8241;\n\tpublic static final int HYDRA_RANGED_2 = 8242;\n\tpublic static final int HYDRA_MAGIC_2 = 8243;\n\tpublic static final int HYDRA_2_1 = 8244;\n\tpublic static final int HYDRA_2_2 = 8245;\n\tpublic static final int HYDRA_FIRE = 8248;\n\tpublic static final int HYDRA_RANGED_3 = 8249;\n\tpublic static final int HYDRA_MAGIC_3 = 8250;\n\tpublic static final int HYDRA_3_1 = 8251;\n\tpublic static final int HYDRA_3_2 = 8252;\n\tpublic static final int HYDRA_MAGIC_4 = 8254;\n\tpublic static final int HYDRA_POISON_4 = 8254;\n\tpublic static final int HYDRA_RANGED_4 = 8255;\n\tpublic static final int HYDRA_4_1 = 8257;\n\tpublic static final int HYDRA_4_2 = 8258;\n}\n", "idx": 1, "id": 14962, "msg": "PICKPOCKET_FAIL is the same as BLOCK_UNARMED", "proj": "open-osrs-runelite", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -267,10 +267,10 @@ void contourAndDrawGaussians(MolDraw2D &drawer,\n       maxP.y = std::max(loc.y, maxP.y);\n     }\n     Point2D dims = maxP - minP;\n-    minP.x -= drawer.drawOptions().padding * dims.x;\n-    minP.y -= drawer.drawOptions().padding * dims.y;\n-    maxP.x += drawer.drawOptions().padding * dims.x;\n-    maxP.y += drawer.drawOptions().padding * dims.y;\n+    minP.x -= fabs(drawer.drawOptions().padding) * dims.x;\n+    minP.y -= fabs(drawer.drawOptions().padding) * dims.y;\n+    maxP.x += fabs(drawer.drawOptions().padding) * dims.x;\n+    maxP.y += fabs(drawer.drawOptions().padding) * dims.y;\n \n     if (params.extraGridPadding > 0) {\n       Point2D p1(0, 0), p2(params.extraGridPadding, 0);", "y": 1, "oldf": "//\n//  Copyright (C) 2016-2019 Greg Landrum\n//\n//   @@ All Rights Reserved @@\n//  This file is part of the RDKit.\n//  The contents are covered by the terms of the BSD license\n//  which is included in the file license.txt, found at the root\n//  of the RDKit source tree.\n//\n#include <GraphMol/MolDraw2D/MolDraw2D.h>\n#include <GraphMol/MolDraw2D/MolDraw2DUtils.h>\n\n#include <GraphMol/RWMol.h>\n#include <GraphMol/MolOps.h>\n#include <GraphMol/Depictor/RDDepictor.h>\n#include <GraphMol/FileParsers/MolFileStereochem.h>\n\n#include <RDGeneral/BoostStartInclude.h>\n#include <boost/foreach.hpp>\n#include <boost/lexical_cast.hpp>\n#include <boost/property_tree/ptree.hpp>\n#include <boost/property_tree/json_parser.hpp>\n#include <RDGeneral/BoostEndInclude.h>\n#include <limits>\n#include <cmath>\n#include <Numerics/Conrec.h>\n\nnamespace RDKit {\nnamespace MolDraw2DUtils {\n\nnamespace {\nbool isAtomCandForChiralH(const RWMol &mol, const Atom *atom) {\n  // conditions for needing a chiral H:\n  //   - stereochem specified\n  //   - in at least two rings\n  if ((!mol.getRingInfo()->isInitialized() ||\n       mol.getRingInfo()->numAtomRings(atom->getIdx()) > 1) &&\n      (atom->getChiralTag() == Atom::CHI_TETRAHEDRAL_CCW ||\n       atom->getChiralTag() == Atom::CHI_TETRAHEDRAL_CW)) {\n    return true;\n  }\n  return false;\n}\n}  // end of anonymous namespace\n\nvoid prepareMolForDrawing(RWMol &mol, bool kekulize, bool addChiralHs,\n                          bool wedgeBonds, bool forceCoords) {\n  if (kekulize) {\n    MolOps::Kekulize(mol, false);  // kekulize, but keep the aromatic flags!\n  }\n  if (addChiralHs) {\n    std::vector<unsigned int> chiralAts;\n    for (RWMol::AtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n         ++atIt) {\n      if (isAtomCandForChiralH(mol, *atIt)) {\n        chiralAts.push_back((*atIt)->getIdx());\n      }\n    }\n    if (chiralAts.size()) {\n      bool addCoords = false;\n      if (!forceCoords && mol.getNumConformers()) addCoords = true;\n      MolOps::addHs(mol, false, addCoords, &chiralAts);\n    }\n  }\n  if (forceCoords || !mol.getNumConformers()) {\n    // compute 2D coordinates in a standard orientation:\n    const bool canonOrient = true;\n    RDDepict::compute2DCoords(mol, nullptr, canonOrient);\n  }\n  if (wedgeBonds) {\n    WedgeMolBonds(mol, &mol.getConformer());\n  }\n}\n\nvoid prepareAndDrawMolecule(MolDraw2D &drawer, const ROMol &mol,\n                            const std::string &legend,\n                            const std::vector<int> *highlight_atoms,\n                            const std::vector<int> *highlight_bonds,\n                            const std::map<int, DrawColour> *highlight_atom_map,\n                            const std::map<int, DrawColour> *highlight_bond_map,\n                            const std::map<int, double> *highlight_radii,\n                            int confId) {\n  RWMol cpy(mol);\n  prepareMolForDrawing(cpy);\n  drawer.drawMolecule(cpy, legend, highlight_atoms, highlight_bonds,\n                      highlight_atom_map, highlight_bond_map, highlight_radii,\n                      confId);\n}\n\nvoid updateDrawerParamsFromJSON(MolDraw2D &drawer, const char *json) {\n  PRECONDITION(json, \"no parameter string\");\n  updateDrawerParamsFromJSON(drawer, std::string(json));\n};\n#define PT_OPT_GET(opt) opts.opt = pt.get(#opt, opts.opt)\n\nvoid get_colour_option(boost::property_tree::ptree *pt, const char *pnm,\n                       DrawColour &colour) {\n  PRECONDITION(pnm && strlen(pnm), \"bad property name\");\n  if (pt->find(pnm) == pt->not_found()) return;\n\n  boost::property_tree::ptree::const_iterator itm = pt->get_child(pnm).begin();\n  colour.r = itm->second.get_value<float>();\n  ++itm;\n  colour.g = itm->second.get_value<float>();\n  ++itm;\n  colour.b = itm->second.get_value<float>();\n  ++itm;\n}\n\nvoid updateDrawerParamsFromJSON(MolDraw2D &drawer, const std::string &json) {\n  if (json == \"\") return;\n  std::istringstream ss;\n  ss.str(json);\n  MolDrawOptions &opts = drawer.drawOptions();\n  boost::property_tree::ptree pt;\n  boost::property_tree::read_json(ss, pt);\n  PT_OPT_GET(atomLabelDeuteriumTritium);\n  PT_OPT_GET(dummiesAreAttachments);\n  PT_OPT_GET(circleAtoms);\n  PT_OPT_GET(continuousHighlight);\n  PT_OPT_GET(flagCloseContactsDist);\n  PT_OPT_GET(includeAtomTags);\n  PT_OPT_GET(clearBackground);\n  PT_OPT_GET(legendFontSize);\n  PT_OPT_GET(multipleBondOffset);\n  PT_OPT_GET(padding);\n  PT_OPT_GET(additionalAtomLabelPadding);\n  get_colour_option(&pt, \"highlightColour\", opts.highlightColour);\n  get_colour_option(&pt, \"backgroundColour\", opts.backgroundColour);\n  get_colour_option(&pt, \"legendColour\", opts.legendColour);\n  if (pt.find(\"atomLabels\") != pt.not_found()) {\n    BOOST_FOREACH (boost::property_tree::ptree::value_type const &item,\n                   pt.get_child(\"atomLabels\")) {\n      opts.atomLabels[boost::lexical_cast<int>(item.first)] =\n          item.second.get_value<std::string>();\n    }\n  }\n}\n\nvoid contourAndDrawGrid(MolDraw2D &drawer, const double *grid,\n                        const std::vector<double> &xcoords,\n                        const std::vector<double> &ycoords, size_t nContours,\n                        std::vector<double> &levels,\n                        const ContourParams &params) {\n  PRECONDITION(grid, \"no data\");\n  PRECONDITION(params.colourMap.size() > 1,\n               \"colourMap must have at least two entries\");\n\n  if (params.setScale) {\n    Point2D minP = {xcoords[0], ycoords[0]};\n    Point2D maxP = {xcoords.back(), ycoords.back()};\n    drawer.setScale(drawer.width(), drawer.height(), minP, maxP);\n  }\n\n  size_t nX = xcoords.size();\n  size_t nY = ycoords.size();\n  double minV = std::numeric_limits<double>::max();\n  double maxV = -std::numeric_limits<double>::max();\n  if (!levels.size() || params.fillGrid) {\n    for (size_t i = 0; i < nX; ++i) {\n      for (size_t j = 0; j < nY; ++j) {\n        minV = std::min(minV, grid[i * nY + j]);\n        maxV = std::max(maxV, grid[i * nY + j]);\n      }\n    }\n    if (!levels.size()) {\n      levels.resize(nContours);\n      for (size_t i = 0; i < nContours; ++i) {\n        levels[i] = minV + i * (maxV - minV) / (nContours - 1);\n      }\n    }\n  }\n  if (maxV <= minV) return;\n\n  const auto olw = drawer.lineWidth();\n  const auto odash = drawer.dash();\n  const auto ocolor = drawer.colour();\n  const auto ofill = drawer.fillPolys();\n  const auto owidth = drawer.lineWidth();\n  if (params.fillGrid) {\n    drawer.setFillPolys(true);\n    drawer.setLineWidth(1);\n    auto delta = (maxV - minV);\n    if (params.colourMap.size() > 2) {\n      // need to find how fractionally far we are from zero, not the min\n      if (-minV > maxV)\n        delta = -minV;\n      else\n        delta = maxV;\n    }\n    for (size_t i = 0; i < nX - 1; ++i) {\n      for (size_t j = 0; j < nY - 1; ++j) {\n        auto gridV = grid[i * nY + j];\n        auto fracV = (gridV - minV) / delta;\n        if (params.colourMap.size() > 2) {\n          // need to find how fractionally far we are from zero, not the min\n          fracV = gridV / delta;\n          if (fracV < 0) fracV *= -1;\n        }\n        DrawColour fillColour;\n        auto c1 = (gridV < 0 || params.colourMap.size() == 2)\n                      ? params.colourMap[1]\n                      : params.colourMap[1];\n        auto c2 = (gridV < 0 || params.colourMap.size() == 2)\n                      ? params.colourMap[0]\n                      : params.colourMap[2];\n        auto c = c1 + (c2 - c1) * fracV;\n        // don't bother drawing boxes that are the same as the background color:\n        double tol = 0.01;\n        if (c.feq(drawer.drawOptions().backgroundColour, tol)) {\n          continue;\n        }\n        drawer.setColour(c);\n        Point2D p1 = {xcoords[i], ycoords[j]};\n        Point2D p2 = {xcoords[i + 1], ycoords[j + 1]};\n        drawer.drawRect(p1, p2);\n      }\n    }\n  }\n\n  if (nContours) {\n    if(nContours > levels.size()){\n      throw ValueErrorException(\"nContours larger than the size of the level list\");\n    }\n    std::vector<conrec::ConrecSegment> segs;\n    conrec::Contour(grid, 0, nX - 1, 0, nY - 1, xcoords.data(), ycoords.data(),\n                    nContours, levels.data(), segs);\n    static DashPattern negDash = {2, 6};\n    static DashPattern posDash;\n    drawer.setColour(params.contourColour);\n    drawer.setLineWidth(params.contourWidth);\n    for (const auto &seg : segs) {\n      if (params.dashNegative && seg.isoVal < 0) {\n        drawer.setDash(negDash);\n      } else {\n        drawer.setDash(posDash);\n      }\n      drawer.drawLine(seg.p1, seg.p2);\n    }\n  }\n\n  drawer.setDash(odash);\n  drawer.setLineWidth(olw);\n  drawer.setColour(ocolor);\n  drawer.setFillPolys(ofill);\n  drawer.setLineWidth(owidth);\n};\n\nvoid contourAndDrawGaussians(MolDraw2D &drawer,\n                             const std::vector<Point2D> &locs,\n                             const std::vector<double> &weights,\n                             const std::vector<double> &widths,\n                             size_t nContours, std::vector<double> &levels,\n                             const ContourParams &params) {\n  PRECONDITION(locs.size() == weights.size(), \"size mismatch\");\n  PRECONDITION(locs.size() == widths.size(), \"size mismatch\");\n\n  // start by setting up the grid\n  if (params.setScale) {\n    Point2D minP, maxP;\n    minP.x = minP.y = std::numeric_limits<double>::max();\n    maxP.x = maxP.y = -std::numeric_limits<double>::max();\n    for (const auto &loc : locs) {\n      minP.x = std::min(loc.x, minP.x);\n      minP.y = std::min(loc.y, minP.y);\n      maxP.x = std::max(loc.x, maxP.x);\n      maxP.y = std::max(loc.y, maxP.y);\n    }\n    Point2D dims = maxP - minP;\n    minP.x -= drawer.drawOptions().padding * dims.x;\n    minP.y -= drawer.drawOptions().padding * dims.y;\n    maxP.x += drawer.drawOptions().padding * dims.x;\n    maxP.y += drawer.drawOptions().padding * dims.y;\n\n    if (params.extraGridPadding > 0) {\n      Point2D p1(0, 0), p2(params.extraGridPadding, 0);\n      double pad =\n          fabs(drawer.getDrawCoords(p2).x - drawer.getDrawCoords(p1).x);\n      minP.x -= pad;\n      minP.y -= pad;\n      maxP.x += pad;\n      maxP.y += pad;\n    }\n\n    drawer.setScale(drawer.width(), drawer.height(), minP, maxP);\n  }\n\n  size_t nx = (size_t)ceil(drawer.range().x / params.gridResolution) + 1;\n  size_t ny = (size_t)ceil(drawer.range().y / params.gridResolution) + 1;\n  std::vector<double> xcoords(nx);\n  for (size_t i = 0; i < nx; ++i) {\n    xcoords[i] = drawer.minPt().x + i * params.gridResolution;\n  }\n  std::vector<double> ycoords(ny);\n  for (size_t i = 0; i < ny; ++i) {\n    ycoords[i] = drawer.minPt().y + i * params.gridResolution;\n  }\n  std::unique_ptr<double[]> grid(new double[nx * ny]);\n\n  // populate the grid from the gaussians:\n  for (size_t ix = 0; ix < nx; ++ix) {\n    auto px = drawer.minPt().x + ix * params.gridResolution;\n    for (size_t iy = 0; iy < ny; ++iy) {\n      auto py = drawer.minPt().y + iy * params.gridResolution;\n      Point2D pt(px, py);\n      double accum = 0.0;\n      for (size_t ig = 0; ig < locs.size(); ++ig) {\n        auto d2 = (pt - locs[ig]).lengthSq();\n        auto contrib = weights[ig] / widths[ig] *\n                       exp(-0.5 * d2 / (widths[ig] * widths[ig]));\n        accum += contrib;\n      }\n      grid[ix * ny + iy] = accum / (2 * M_PI);\n    }\n  }\n\n  // and render it:\n  ContourParams paramsCopy = params;\n  paramsCopy.setScale = false;  // if scaling was needed, we did it already\n  contourAndDrawGrid(drawer, grid.get(), xcoords, ycoords, nContours, levels,\n                     paramsCopy);\n};\n}  // namespace MolDraw2DUtils\n}  // namespace RDKit\n", "idx": 1, "id": 20053, "msg": "Are these changes connected to the bug? It's not clear to me that they are correct. It seems to me that if the padding is negative (for some reason), it should have the corresponding effect.", "proj": "rdkit-rdkit", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -141,32 +141,33 @@ func (v *VolumeSnapshotRecv) Execute() ([]byte, error) {\n \t\treturn nil, err\n \t}\n \t// execute command here\n-\treturn exec.Command(bin.ZFS, v.Command).CombinedOutput()\n+\treturn exec.Command(bin.BASH, \"-c\", v.Command).CombinedOutput()\n }\n \n // Build returns the VolumeSnapshotRecv object generated by builder\n func (v *VolumeSnapshotRecv) Build() (*VolumeSnapshotRecv, error) {\n \tvar c strings.Builder\n \tv = v.Validate()\n+\tv.appendCommand(&c, bin.ZFS)\n \n-\tv.appendCommand(c, fmt.Sprintf(\" %s \", Operation))\n+\tv.appendCommand(&c, fmt.Sprintf(\" %s \", Operation))\n \tif IsDedupSet()(v) {\n-\t\tv.appendCommand(c, fmt.Sprintf(\" -D \"))\n+\t\tv.appendCommand(&c, fmt.Sprintf(\" -D \"))\n \t}\n \n \tif IsLastSnapshotSet()(v) {\n-\t\tv.appendCommand(c, fmt.Sprintf(\" -i @%s \", v.LastSnapshot))\n+\t\tv.appendCommand(&c, fmt.Sprintf(\" -i @%s \", v.LastSnapshot))\n \t}\n \n-\tv.appendCommand(c, fmt.Sprintf(\" %s@%s \", v.Dataset, v.Snapshot))\n-\tv.appendCommand(c, fmt.Sprintf(\" | nc %s\", v.Target))\n+\tv.appendCommand(&c, fmt.Sprintf(\" %s@%s \", v.Dataset, v.Snapshot))\n+\tv.appendCommand(&c, fmt.Sprintf(\" | nc %s\", v.Target))\n \n \tv.Command = c.String()\n \treturn v, v.err\n }\n \n // appendCommand append string to given string builder\n-func (v *VolumeSnapshotRecv) appendCommand(c strings.Builder, cmd string) {\n+func (v *VolumeSnapshotRecv) appendCommand(c *strings.Builder, cmd string) {\n \t_, err := c.WriteString(cmd)\n \tif err != nil {\n \t\tv.err = errors.Wrapf(v.err, \"Failed to append cmd{%s} : %s\", cmd, err.Error())", "y": 1, "oldf": "/*\nCopyright 2019 The OpenEBS Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage vsnapshotrecv\n\nimport (\n\t\"fmt\"\n\t\"os/exec\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\n\t\"github.com/openebs/maya/pkg/zfs/cmd/v1alpha1/bin\"\n\t\"github.com/pkg/errors\"\n)\n\nconst (\n\t// Operation defines type of zfs operation\n\tOperation = \"receive\"\n)\n\n//VolumeSnapshotRecv defines structure for volume 'Snapshot' operation\ntype VolumeSnapshotRecv struct {\n\t//name of snapshot\n\tSnapshot string\n\n\t//name of dataset on which snapshot should be taken\n\tDataset string\n\n\t//remote destination for snapshot send/recv using nc\n\tTarget string\n\n\t// to send incremental snapshot\n\tLastSnapshot string\n\n\t// Generate a deduplicated stream\n\tDedup bool\n\n\t// dry-run\n\tDryRun bool\n\n\t// use compression for zfs send\n\tEnableCompression bool\n\n\t// command string\n\tCommand string\n\n\t// checks is list of predicate function used for validating object\n\tchecks []PredicateFunc\n\n\t// error\n\terr error\n}\n\n// NewVolumeSnapshotRecv returns new instance of object VolumeSnapshotRecv\nfunc NewVolumeSnapshotRecv() *VolumeSnapshotRecv {\n\treturn &VolumeSnapshotRecv{}\n}\n\n// WithCheck add given check to checks list\nfunc (v *VolumeSnapshotRecv) WithCheck(check ...PredicateFunc) *VolumeSnapshotRecv {\n\tv.checks = append(v.checks, check...)\n\treturn v\n}\n\n// WithSnapshot method fills the Snapshot field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithSnapshot(Snapshot string) *VolumeSnapshotRecv {\n\tv.Snapshot = Snapshot\n\treturn v\n}\n\n// WithDataset method fills the Dataset field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithDataset(Dataset string) *VolumeSnapshotRecv {\n\tv.Dataset = Dataset\n\treturn v\n}\n\n// WithTarget method fills the Target field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithTarget(Target string) *VolumeSnapshotRecv {\n\tv.Target = Target\n\treturn v\n}\n\n// WithDedup method fills the Dedup field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithDedup(Dedup bool) *VolumeSnapshotRecv {\n\tv.Dedup = Dedup\n\treturn v\n}\n\n// WithLastSnapshot method fills the LastSnapshot field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithLastSnapshot(LastSnapshot string) *VolumeSnapshotRecv {\n\tv.LastSnapshot = LastSnapshot\n\treturn v\n}\n\n// WithDryRun method fills the DryRun field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithDryRun(DryRun bool) *VolumeSnapshotRecv {\n\tv.DryRun = DryRun\n\treturn v\n}\n\n// WithEnableCompression method fills the EnableCompression field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithEnableCompression(EnableCompression bool) *VolumeSnapshotRecv {\n\tv.EnableCompression = EnableCompression\n\treturn v\n}\n\n// WithCommand method fills the Command field of VolumeSnapshotRecv object.\nfunc (v *VolumeSnapshotRecv) WithCommand(Command string) *VolumeSnapshotRecv {\n\tv.Command = Command\n\treturn v\n}\n\n// Validate is to validate generated VolumeSnapshotRecv object by builder\nfunc (v *VolumeSnapshotRecv) Validate() *VolumeSnapshotRecv {\n\tfor _, check := range v.checks {\n\t\tif !check(v) {\n\t\t\tv.err = errors.Wrapf(v.err, \"validation failed {%v}\", runtime.FuncForPC(reflect.ValueOf(check).Pointer()).Name())\n\t\t}\n\t}\n\treturn v\n}\n\n// Execute is to execute generated VolumeSnapshotRecv object\nfunc (v *VolumeSnapshotRecv) Execute() ([]byte, error) {\n\tv, err := v.Build()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// execute command here\n\treturn exec.Command(bin.ZFS, v.Command).CombinedOutput()\n}\n\n// Build returns the VolumeSnapshotRecv object generated by builder\nfunc (v *VolumeSnapshotRecv) Build() (*VolumeSnapshotRecv, error) {\n\tvar c strings.Builder\n\tv = v.Validate()\n\n\tv.appendCommand(c, fmt.Sprintf(\" %s \", Operation))\n\tif IsDedupSet()(v) {\n\t\tv.appendCommand(c, fmt.Sprintf(\" -D \"))\n\t}\n\n\tif IsLastSnapshotSet()(v) {\n\t\tv.appendCommand(c, fmt.Sprintf(\" -i @%s \", v.LastSnapshot))\n\t}\n\n\tv.appendCommand(c, fmt.Sprintf(\" %s@%s \", v.Dataset, v.Snapshot))\n\tv.appendCommand(c, fmt.Sprintf(\" | nc %s\", v.Target))\n\n\tv.Command = c.String()\n\treturn v, v.err\n}\n\n// appendCommand append string to given string builder\nfunc (v *VolumeSnapshotRecv) appendCommand(c strings.Builder, cmd string) {\n\t_, err := c.WriteString(cmd)\n\tif err != nil {\n\t\tv.err = errors.Wrapf(v.err, \"Failed to append cmd{%s} : %s\", cmd, err.Error())\n\t}\n}\n", "idx": 1, "id": 16753, "msg": "G204: Subprocess launching should be audited (from `gosec`)", "proj": "openebs-maya", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -105,6 +105,7 @@ func NewWebAPI(\n \t\tdeploymentProjectCache:    memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n \t\tpipedProjectCache:         memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n \t\tenvProjectCache:           memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n+\t\tpipedStatCache:            psc,\n \t\tinsightCache:              rediscache.NewTTLCache(rd, 3*time.Hour),\n \t\tredis:                     rd,\n \t\tlogger:                    logger.Named(\"web-api\"),", "y": 0, "oldf": "// Copyright 2020 The PipeCD Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage grpcapi\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/gob\"\n\t\"errors\"\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\n\t\"github.com/pipe-cd/pipe/pkg/app/api/applicationlivestatestore\"\n\t\"github.com/pipe-cd/pipe/pkg/app/api/commandstore\"\n\t\"github.com/pipe-cd/pipe/pkg/app/api/service/webservice\"\n\t\"github.com/pipe-cd/pipe/pkg/app/api/stagelogstore\"\n\t\"github.com/pipe-cd/pipe/pkg/cache\"\n\t\"github.com/pipe-cd/pipe/pkg/cache/memorycache\"\n\t\"github.com/pipe-cd/pipe/pkg/cache/rediscache\"\n\t\"github.com/pipe-cd/pipe/pkg/config\"\n\t\"github.com/pipe-cd/pipe/pkg/datastore\"\n\t\"github.com/pipe-cd/pipe/pkg/filestore\"\n\t\"github.com/pipe-cd/pipe/pkg/insight/insightstore\"\n\t\"github.com/pipe-cd/pipe/pkg/model\"\n\t\"github.com/pipe-cd/pipe/pkg/redis\"\n\t\"github.com/pipe-cd/pipe/pkg/rpc/rpcauth\"\n)\n\ntype encrypter interface {\n\tEncrypt(text string) (string, error)\n}\n\n// WebAPI implements the behaviors for the gRPC definitions of WebAPI.\ntype WebAPI struct {\n\tapplicationStore          datastore.ApplicationStore\n\tenvironmentStore          datastore.EnvironmentStore\n\tdeploymentStore           datastore.DeploymentStore\n\tpipedStore                datastore.PipedStore\n\tprojectStore              datastore.ProjectStore\n\tapiKeyStore               datastore.APIKeyStore\n\tstageLogStore             stagelogstore.Store\n\tapplicationLiveStateStore applicationlivestatestore.Store\n\tcommandStore              commandstore.Store\n\tinsightStore              insightstore.Store\n\tencrypter                 encrypter\n\n\tappProjectCache        cache.Cache\n\tdeploymentProjectCache cache.Cache\n\tpipedProjectCache      cache.Cache\n\tenvProjectCache        cache.Cache\n\tinsightCache           cache.Cache\n\tredis                  redis.Redis\n\n\tprojectsInConfig map[string]config.ControlPlaneProject\n\tlogger           *zap.Logger\n}\n\n// NewWebAPI creates a new WebAPI instance.\nfunc NewWebAPI(\n\tctx context.Context,\n\tds datastore.DataStore,\n\tfs filestore.Store,\n\tsls stagelogstore.Store,\n\talss applicationlivestatestore.Store,\n\tcmds commandstore.Store,\n\tis insightstore.Store,\n\trd redis.Redis,\n\tprojs map[string]config.ControlPlaneProject,\n\tencrypter encrypter,\n\tlogger *zap.Logger) *WebAPI {\n\ta := &WebAPI{\n\t\tapplicationStore:          datastore.NewApplicationStore(ds),\n\t\tenvironmentStore:          datastore.NewEnvironmentStore(ds),\n\t\tdeploymentStore:           datastore.NewDeploymentStore(ds),\n\t\tpipedStore:                datastore.NewPipedStore(ds),\n\t\tprojectStore:              datastore.NewProjectStore(ds),\n\t\tapiKeyStore:               datastore.NewAPIKeyStore(ds),\n\t\tstageLogStore:             sls,\n\t\tapplicationLiveStateStore: alss,\n\t\tcommandStore:              cmds,\n\t\tinsightStore:              is,\n\t\tprojectsInConfig:          projs,\n\t\tencrypter:                 encrypter,\n\t\tappProjectCache:           memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n\t\tdeploymentProjectCache:    memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n\t\tpipedProjectCache:         memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n\t\tenvProjectCache:           memorycache.NewTTLCache(ctx, 24*time.Hour, 3*time.Hour),\n\t\tinsightCache:              rediscache.NewTTLCache(rd, 3*time.Hour),\n\t\tredis:                     rd,\n\t\tlogger:                    logger.Named(\"web-api\"),\n\t}\n\treturn a\n}\n\n// Register registers all handling of this service into the specified gRPC server.\nfunc (a *WebAPI) Register(server *grpc.Server) {\n\twebservice.RegisterWebServiceServer(server, a)\n}\n\nfunc (a *WebAPI) AddEnvironment(ctx context.Context, req *webservice.AddEnvironmentRequest) (*webservice.AddEnvironmentResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tenv := model.Environment{\n\t\tId:        uuid.New().String(),\n\t\tName:      req.Name,\n\t\tDesc:      req.Desc,\n\t\tProjectId: claims.Role.ProjectId,\n\t}\n\terr = a.environmentStore.AddEnvironment(ctx, &env)\n\tif errors.Is(err, datastore.ErrAlreadyExists) {\n\t\treturn nil, status.Error(codes.AlreadyExists, \"The environment already exists\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to create environment\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to create environment\")\n\t}\n\n\treturn &webservice.AddEnvironmentResponse{}, nil\n}\n\nfunc (a *WebAPI) UpdateEnvironmentDesc(ctx context.Context, req *webservice.UpdateEnvironmentDescRequest) (*webservice.UpdateEnvironmentDescResponse, error) {\n\treturn nil, status.Error(codes.Unimplemented, \"\")\n}\n\nfunc (a *WebAPI) ListEnvironments(ctx context.Context, req *webservice.ListEnvironmentsRequest) (*webservice.ListEnvironmentsResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\topts := datastore.ListOptions{\n\t\tFilters: []datastore.ListFilter{\n\t\t\t{\n\t\t\t\tField:    \"ProjectId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    claims.Role.ProjectId,\n\t\t\t},\n\t\t},\n\t}\n\tenvs, err := a.environmentStore.ListEnvironments(ctx, opts)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get environments\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get environments\")\n\t}\n\n\treturn &webservice.ListEnvironmentsResponse{\n\t\tEnvironments: envs,\n\t}, nil\n}\n\nfunc (a *WebAPI) EnableEnvironment(ctx context.Context, req *webservice.EnableEnvironmentRequest) (*webservice.EnableEnvironmentResponse, error) {\n\tif err := a.updateEnvironmentEnable(ctx, req.EnvironmentId, true); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.EnableEnvironmentResponse{}, nil\n}\n\nfunc (a *WebAPI) DisableEnvironment(ctx context.Context, req *webservice.DisableEnvironmentRequest) (*webservice.DisableEnvironmentResponse, error) {\n\tif err := a.updateEnvironmentEnable(ctx, req.EnvironmentId, false); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.DisableEnvironmentResponse{}, nil\n}\n\n// DeleteEnvironment deletes the given environment and all applications that belong to it.\n// It returns a FailedPrecondition error if any Piped is still using that environment.\nfunc (a *WebAPI) DeleteEnvironment(ctx context.Context, req *webservice.DeleteEnvironmentRequest) (*webservice.DeleteEnvironmentResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif err := a.validateEnvBelongsToProject(ctx, req.EnvironmentId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\t// Check if no Piped has permission to the given environment.\n\tpipeds, err := a.pipedStore.ListPipeds(ctx, datastore.ListOptions{\n\t\tFilters: []datastore.ListFilter{\n\t\t\t{\n\t\t\t\tField:    \"ProjectId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    claims.Role.ProjectId,\n\t\t\t},\n\t\t\t{\n\t\t\t\tField:    \"EnvIds\",\n\t\t\t\tOperator: datastore.OperatorContains,\n\t\t\t\tValue:    req.EnvironmentId,\n\t\t\t},\n\t\t\t{\n\t\t\t\tField:    \"Disabled\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    false,\n\t\t\t},\n\t\t},\n\t})\n\tif err != nil {\n\t\ta.logger.Error(\"failed to fetch Pipeds linked to the given environment\",\n\t\t\tzap.String(\"env-id\", req.EnvironmentId),\n\t\t\tzap.Error(err),\n\t\t)\n\t\treturn nil, status.Error(codes.Internal, \"Failed to validate the deletion operation\")\n\t}\n\tif len(pipeds) > 0 {\n\t\tpipedNames := make([]string, 0, len(pipeds))\n\t\tfor _, p := range pipeds {\n\t\t\tpipedNames = append(pipedNames, p.Name)\n\t\t}\n\t\treturn nil, status.Errorf(\n\t\t\tcodes.FailedPrecondition,\n\t\t\t\"Found Pipeds linked the environment to be deleted. Please remove this environment from all Pipeds (%s) on the Piped settings page\",\n\t\t\tstrings.Join(pipedNames, \",\"),\n\t\t)\n\t}\n\n\t// Delete all applications that belongs to the given env.\n\tapps, _, err := a.applicationStore.ListApplications(ctx, datastore.ListOptions{\n\t\tFilters: []datastore.ListFilter{\n\t\t\t{\n\t\t\t\tField:    \"ProjectId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    claims.Role.ProjectId,\n\t\t\t},\n\t\t\t{\n\t\t\t\tField:    \"EnvId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    req.EnvironmentId,\n\t\t\t},\n\t\t},\n\t})\n\tif err != nil {\n\t\ta.logger.Error(\"failed to fetch applications that belongs to the given environment\",\n\t\t\tzap.String(\"env-id\", req.EnvironmentId),\n\t\t\tzap.Error(err),\n\t\t)\n\t\treturn nil, status.Error(codes.Internal, \"Failed to fetch applications that belongs to the given environment\")\n\t}\n\tfor _, app := range apps {\n\t\tif app.ProjectId != claims.Role.ProjectId {\n\t\t\tcontinue\n\t\t}\n\t\terr := a.applicationStore.DeleteApplication(ctx, app.Id)\n\t\tif err == nil {\n\t\t\tcontinue\n\t\t}\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn nil, status.Error(codes.Internal, \"The application is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn nil, status.Error(codes.InvalidArgument, \"Invalid value to delete\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to delete the application\",\n\t\t\t\tzap.String(\"application-id\", app.Id),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn nil, status.Error(codes.Internal, \"Failed to delete the application\")\n\t\t}\n\t}\n\n\tif err := a.environmentStore.DeleteEnvironment(ctx, req.EnvironmentId); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn nil, status.Error(codes.NotFound, \"The environment is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn nil, status.Error(codes.InvalidArgument, \"Invalid value to delete\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to delete the environment\",\n\t\t\t\tzap.String(\"env-id\", req.EnvironmentId),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn nil, status.Error(codes.Internal, \"Failed to delete the environment\")\n\t\t}\n\t}\n\n\treturn &webservice.DeleteEnvironmentResponse{}, nil\n}\n\nfunc (a *WebAPI) updateEnvironmentEnable(ctx context.Context, envID string, enable bool) error {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn err\n\t}\n\n\tif err := a.validateEnvBelongsToProject(ctx, envID, claims.Role.ProjectId); err != nil {\n\t\treturn err\n\t}\n\n\tvar updater func(context.Context, string) error\n\tif enable {\n\t\tupdater = a.environmentStore.EnableEnvironment\n\t} else {\n\t\tupdater = a.environmentStore.DisableEnvironment\n\t}\n\n\tif err := updater(ctx, envID); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn status.Error(codes.NotFound, \"The environment is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn status.Error(codes.InvalidArgument, \"Invalid value for update\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to update the environment\",\n\t\t\t\tzap.String(\"env-id\", envID),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn status.Error(codes.Internal, \"Failed to update the environment\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// validateEnvBelongsToProject checks if the given piped belongs to the given project.\n// It gives back error unless the env belongs to the project.\nfunc (a *WebAPI) validateEnvBelongsToProject(ctx context.Context, envID, projectID string) error {\n\teid, err := a.envProjectCache.Get(envID)\n\tif err == nil {\n\t\tif projectID != eid {\n\t\t\treturn status.Error(codes.PermissionDenied, \"Requested environment doesn't belong to the project you logged in\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tenv, err := getEnvironment(ctx, a.environmentStore, envID, a.logger)\n\tif err != nil {\n\t\treturn err\n\t}\n\ta.envProjectCache.Put(envID, env.ProjectId)\n\n\tif projectID != env.ProjectId {\n\t\treturn status.Error(codes.PermissionDenied, \"Requested environment doesn't belong to the project you logged in\")\n\t}\n\treturn nil\n}\n\nfunc (a *WebAPI) RegisterPiped(ctx context.Context, req *webservice.RegisterPipedRequest) (*webservice.RegisterPipedResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tkey, keyHash, err := model.GeneratePipedKey()\n\tif err != nil {\n\t\ta.logger.Error(\"failed to generate piped key\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to generate the piped key\")\n\t}\n\n\tpiped := model.Piped{\n\t\tId:        uuid.New().String(),\n\t\tName:      req.Name,\n\t\tDesc:      req.Desc,\n\t\tProjectId: claims.Role.ProjectId,\n\t\tEnvIds:    req.EnvIds,\n\t\tStatus:    model.Piped_OFFLINE,\n\t}\n\tif err := piped.AddKey(keyHash, claims.Subject, time.Now()); err != nil {\n\t\treturn nil, status.Error(codes.FailedPrecondition, fmt.Sprintf(\"Failed to create key: %v\", err))\n\t}\n\n\terr = a.pipedStore.AddPiped(ctx, &piped)\n\tif errors.Is(err, datastore.ErrAlreadyExists) {\n\t\treturn nil, status.Error(codes.AlreadyExists, \"The piped already exists\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to register piped\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to register piped\")\n\t}\n\treturn &webservice.RegisterPipedResponse{\n\t\tId:  piped.Id,\n\t\tKey: key,\n\t}, nil\n}\n\nfunc (a *WebAPI) UpdatePiped(ctx context.Context, req *webservice.UpdatePipedRequest) (*webservice.UpdatePipedResponse, error) {\n\tupdater := func(ctx context.Context, pipedID string) error {\n\t\treturn a.pipedStore.UpdatePiped(ctx, req.PipedId, func(p *model.Piped) error {\n\t\t\tp.Name = req.Name\n\t\t\tp.Desc = req.Desc\n\t\t\tp.EnvIds = req.EnvIds\n\t\t\treturn nil\n\t\t})\n\t}\n\tif err := a.updatePiped(ctx, req.PipedId, updater); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.UpdatePipedResponse{}, nil\n}\n\nfunc (a *WebAPI) RecreatePipedKey(ctx context.Context, req *webservice.RecreatePipedKeyRequest) (*webservice.RecreatePipedKeyResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tkey, keyHash, err := model.GeneratePipedKey()\n\tif err != nil {\n\t\ta.logger.Error(\"failed to generate piped key\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to generate the piped key\")\n\t}\n\n\tupdater := func(ctx context.Context, pipedID string) error {\n\t\treturn a.pipedStore.AddKey(ctx, pipedID, keyHash, claims.Subject, time.Now())\n\t}\n\tif err := a.updatePiped(ctx, req.Id, updater); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.RecreatePipedKeyResponse{\n\t\tKey: key,\n\t}, nil\n}\n\nfunc (a *WebAPI) DeleteOldPipedKeys(ctx context.Context, req *webservice.DeleteOldPipedKeysRequest) (*webservice.DeleteOldPipedKeysResponse, error) {\n\tif _, err := rpcauth.ExtractClaims(ctx); err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tupdater := func(ctx context.Context, pipedID string) error {\n\t\treturn a.pipedStore.DeleteOldKeys(ctx, pipedID)\n\t}\n\tif err := a.updatePiped(ctx, req.PipedId, updater); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.DeleteOldPipedKeysResponse{}, nil\n}\n\nfunc (a *WebAPI) EnablePiped(ctx context.Context, req *webservice.EnablePipedRequest) (*webservice.EnablePipedResponse, error) {\n\tif err := a.updatePiped(ctx, req.PipedId, a.pipedStore.EnablePiped); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.EnablePipedResponse{}, nil\n}\n\nfunc (a *WebAPI) DisablePiped(ctx context.Context, req *webservice.DisablePipedRequest) (*webservice.DisablePipedResponse, error) {\n\tif err := a.updatePiped(ctx, req.PipedId, a.pipedStore.DisablePiped); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.DisablePipedResponse{}, nil\n}\n\nfunc (a *WebAPI) updatePiped(ctx context.Context, pipedID string, updater func(context.Context, string) error) error {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn err\n\t}\n\n\tif err := a.validatePipedBelongsToProject(ctx, pipedID, claims.Role.ProjectId); err != nil {\n\t\treturn err\n\t}\n\n\tif err := updater(ctx, pipedID); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn status.Error(codes.InvalidArgument, \"The piped is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn status.Error(codes.InvalidArgument, \"Invalid value for update\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to update the piped\",\n\t\t\t\tzap.String(\"piped-id\", pipedID),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\t// TODO: Improve error handling, instead of considering all as Internal error like this\n\t\t\t// we should check the error type to decide to pass its message to the web client or just a generic message.\n\t\t\treturn status.Error(codes.Internal, \"Failed to update the piped\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// TODO: Consider using piped-stats to decide piped connection status.\nfunc (a *WebAPI) ListPipeds(ctx context.Context, req *webservice.ListPipedsRequest) (*webservice.ListPipedsResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\topts := datastore.ListOptions{\n\t\tFilters: []datastore.ListFilter{\n\t\t\t{\n\t\t\t\tField:    \"ProjectId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    claims.Role.ProjectId,\n\t\t\t},\n\t\t},\n\t}\n\n\tif req.Options != nil {\n\t\tif req.Options.Enabled != nil {\n\t\t\topts.Filters = append(opts.Filters, datastore.ListFilter{\n\t\t\t\tField:    \"Disabled\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    !req.Options.Enabled.GetValue(),\n\t\t\t})\n\t\t}\n\t}\n\n\tpipeds, err := a.pipedStore.ListPipeds(ctx, opts)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get pipeds\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get pipeds\")\n\t}\n\n\t// Redact all sensitive data inside piped message before sending to the client.\n\tfor i := range pipeds {\n\t\tpipeds[i].RedactSensitiveData()\n\t}\n\n\treturn &webservice.ListPipedsResponse{\n\t\tPipeds: pipeds,\n\t}, nil\n}\n\nfunc (a *WebAPI) GetPiped(ctx context.Context, req *webservice.GetPipedRequest) (*webservice.GetPipedResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tpiped, err := getPiped(ctx, a.pipedStore, req.PipedId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := a.validatePipedBelongsToProject(ctx, req.PipedId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Redact all sensitive data inside piped message before sending to the client.\n\tpiped.RedactSensitiveData()\n\n\treturn &webservice.GetPipedResponse{\n\t\tPiped: piped,\n\t}, nil\n}\n\nfunc (a *WebAPI) UpdatePipedDesiredVersion(ctx context.Context, req *webservice.UpdatePipedDesiredVersionRequest) (*webservice.UpdatePipedDesiredVersionResponse, error) {\n\tupdater := func(ctx context.Context, pipedID string) error {\n\t\treturn a.pipedStore.UpdatePiped(ctx, pipedID, func(p *model.Piped) error {\n\t\t\tp.DesiredVersion = req.Version\n\t\t\treturn nil\n\t\t})\n\t}\n\tfor _, pipedID := range req.PipedIds {\n\t\tif err := a.updatePiped(ctx, pipedID, updater); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn &webservice.UpdatePipedDesiredVersionResponse{}, nil\n}\n\n// validatePipedBelongsToProject checks if the given piped belongs to the given project.\n// It gives back error unless the piped belongs to the project.\nfunc (a *WebAPI) validatePipedBelongsToProject(ctx context.Context, pipedID, projectID string) error {\n\tpid, err := a.pipedProjectCache.Get(pipedID)\n\tif err == nil {\n\t\tif pid != projectID {\n\t\t\treturn status.Error(codes.PermissionDenied, \"Requested piped doesn't belong to the project you logged in\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tpiped, err := getPiped(ctx, a.pipedStore, pipedID, a.logger)\n\tif err != nil {\n\t\treturn err\n\t}\n\ta.pipedProjectCache.Put(pipedID, piped.ProjectId)\n\n\tif piped.ProjectId != projectID {\n\t\treturn status.Error(codes.PermissionDenied, \"Requested piped doesn't belong to the project you logged in\")\n\t}\n\treturn nil\n}\n\nfunc (a *WebAPI) ListUnregisteredApplications(ctx context.Context, _ *webservice.ListUnregisteredApplicationsRequest) (*webservice.ListUnregisteredApplicationsResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\t// Collect all apps that belong to the project.\n\tkey := makeUnregisteredAppsCacheKey(claims.Role.ProjectId)\n\tc := rediscache.NewHashCache(a.redis, key)\n\t// pipedToApps assumes to be a map[\"piped-id\"][]byte(slice of *model.ApplicationInfo encoded by encoding/gob)\n\tpipedToApps, err := c.GetAll()\n\tif errors.Is(err, cache.ErrNotFound) {\n\t\treturn &webservice.ListUnregisteredApplicationsResponse{}, nil\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get unregistered apps\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get unregistered apps\")\n\t}\n\n\t// Integrate all apps cached for each Piped.\n\tallApps := make([]*model.ApplicationInfo, 0)\n\tfor _, as := range pipedToApps {\n\t\tb, ok := as.([]byte)\n\t\tif !ok {\n\t\t\treturn nil, status.Error(codes.Internal, \"Unexpected data cached\")\n\t\t}\n\t\tdec := gob.NewDecoder(bytes.NewReader(b))\n\t\tvar apps []*model.ApplicationInfo\n\t\tif err := dec.Decode(&apps); err != nil {\n\t\t\ta.logger.Error(\"failed to decode the unregistered apps\", zap.Error(err))\n\t\t\treturn nil, status.Error(codes.Internal, \"failed to decode the unregistered apps\")\n\t\t}\n\t\tallApps = append(allApps, apps...)\n\t}\n\tif len(allApps) == 0 {\n\t\treturn &webservice.ListUnregisteredApplicationsResponse{}, nil\n\t}\n\n\tsort.Slice(allApps, func(i, j int) bool {\n\t\treturn allApps[i].Path < allApps[j].Path\n\t})\n\treturn &webservice.ListUnregisteredApplicationsResponse{\n\t\tApplications: allApps,\n\t}, nil\n}\n\n// TODO: Validate the specified piped to ensure that it belongs to the specified environment.\nfunc (a *WebAPI) AddApplication(ctx context.Context, req *webservice.AddApplicationRequest) (*webservice.AddApplicationResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tpiped, err := getPiped(ctx, a.pipedStore, req.PipedId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif piped.ProjectId != claims.Role.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested piped does not belong to your project\")\n\t}\n\n\tgitpath, err := makeGitPath(\n\t\treq.GitPath.Repo.Id,\n\t\treq.GitPath.Path,\n\t\treq.GitPath.ConfigFilename,\n\t\tpiped,\n\t\ta.logger,\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tapp := model.Application{\n\t\tId:            uuid.New().String(),\n\t\tName:          req.Name,\n\t\tEnvId:         req.EnvId,\n\t\tPipedId:       req.PipedId,\n\t\tProjectId:     claims.Role.ProjectId,\n\t\tGitPath:       gitpath,\n\t\tKind:          req.Kind,\n\t\tCloudProvider: req.CloudProvider,\n\t\tDescription:   req.Description,\n\t}\n\terr = a.applicationStore.AddApplication(ctx, &app)\n\tif errors.Is(err, datastore.ErrAlreadyExists) {\n\t\treturn nil, status.Error(codes.AlreadyExists, \"The application already exists\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to create application\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to create application\")\n\t}\n\n\treturn &webservice.AddApplicationResponse{\n\t\tApplicationId: app.Id,\n\t}, nil\n}\n\nfunc (a *WebAPI) UpdateApplication(ctx context.Context, req *webservice.UpdateApplicationRequest) (*webservice.UpdateApplicationResponse, error) {\n\tupdater := func(app *model.Application) error {\n\t\tapp.Name = req.Name\n\t\tapp.EnvId = req.EnvId\n\t\tapp.PipedId = req.PipedId\n\t\tapp.Kind = req.Kind\n\t\tapp.CloudProvider = req.CloudProvider\n\t\treturn nil\n\t}\n\n\tif err := a.updateApplication(ctx, req.ApplicationId, req.PipedId, updater); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.UpdateApplicationResponse{}, nil\n}\n\nfunc (a *WebAPI) UpdateApplicationDescription(ctx context.Context, req *webservice.UpdateApplicationDescriptionRequest) (*webservice.UpdateApplicationDescriptionResponse, error) {\n\tupdater := func(app *model.Application) error {\n\t\tapp.Description = req.Description\n\t\treturn nil\n\t}\n\n\tif err := a.updateApplication(ctx, req.ApplicationId, \"\", updater); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.UpdateApplicationDescriptionResponse{}, nil\n}\n\nfunc (a *WebAPI) updateApplication(ctx context.Context, id, pipedID string, updater func(app *model.Application) error) error {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn err\n\t}\n\n\t// Ensure that the specified piped is assignable for this application.\n\tif pipedID != \"\" {\n\t\tpiped, err := getPiped(ctx, a.pipedStore, pipedID, a.logger)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif piped.ProjectId != claims.Role.ProjectId {\n\t\t\treturn status.Error(codes.InvalidArgument, \"Requested piped does not belong to your project\")\n\t\t}\n\t}\n\n\terr = a.applicationStore.UpdateApplication(ctx, id, updater)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to update application\", zap.Error(err))\n\t\treturn status.Error(codes.Internal, \"Failed to update application\")\n\t}\n\n\treturn nil\n}\n\nfunc (a *WebAPI) EnableApplication(ctx context.Context, req *webservice.EnableApplicationRequest) (*webservice.EnableApplicationResponse, error) {\n\tif err := a.updateApplicationEnable(ctx, req.ApplicationId, true); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.EnableApplicationResponse{}, nil\n}\n\nfunc (a *WebAPI) DisableApplication(ctx context.Context, req *webservice.DisableApplicationRequest) (*webservice.DisableApplicationResponse, error) {\n\tif err := a.updateApplicationEnable(ctx, req.ApplicationId, false); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &webservice.DisableApplicationResponse{}, nil\n}\n\nfunc (a *WebAPI) DeleteApplication(ctx context.Context, req *webservice.DeleteApplicationRequest) (*webservice.DeleteApplicationResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif err := a.validateAppBelongsToProject(ctx, req.ApplicationId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := a.applicationStore.DeleteApplication(ctx, req.ApplicationId); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn nil, status.Error(codes.NotFound, \"The application is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn nil, status.Error(codes.InvalidArgument, \"Invalid value to delete\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to delete the application\",\n\t\t\t\tzap.String(\"application-id\", req.ApplicationId),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn nil, status.Error(codes.Internal, \"Failed to delete the application\")\n\t\t}\n\t}\n\n\treturn &webservice.DeleteApplicationResponse{}, nil\n}\n\nfunc (a *WebAPI) updateApplicationEnable(ctx context.Context, appID string, enable bool) error {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn err\n\t}\n\n\tif err := a.validateAppBelongsToProject(ctx, appID, claims.Role.ProjectId); err != nil {\n\t\treturn err\n\t}\n\n\tvar updater func(context.Context, string) error\n\tif enable {\n\t\tupdater = a.applicationStore.EnableApplication\n\t} else {\n\t\tupdater = a.applicationStore.DisableApplication\n\t}\n\n\tif err := updater(ctx, appID); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn status.Error(codes.NotFound, \"The application is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn status.Error(codes.InvalidArgument, \"Invalid value for update\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to update the application\",\n\t\t\t\tzap.String(\"application-id\", appID),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn status.Error(codes.Internal, \"Failed to update the application\")\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (a *WebAPI) ListApplications(ctx context.Context, req *webservice.ListApplicationsRequest) (*webservice.ListApplicationsResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\torders := []datastore.Order{\n\t\t{\n\t\t\tField:     \"UpdatedAt\",\n\t\t\tDirection: datastore.Desc,\n\t\t},\n\t\t{\n\t\t\tField:     \"Id\",\n\t\t\tDirection: datastore.Asc,\n\t\t},\n\t}\n\tfilters := []datastore.ListFilter{\n\t\t{\n\t\t\tField:    \"ProjectId\",\n\t\t\tOperator: datastore.OperatorEqual,\n\t\t\tValue:    claims.Role.ProjectId,\n\t\t},\n\t}\n\tif o := req.Options; o != nil {\n\t\tif o.Enabled != nil {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"Disabled\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    !o.Enabled.GetValue(),\n\t\t\t})\n\t\t}\n\t\t// Allowing multiple so that it can do In Query later.\n\t\t// Currently only the first value is used.\n\t\tif len(o.Kinds) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"Kind\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.Kinds[0],\n\t\t\t})\n\t\t}\n\t\tif len(o.SyncStatuses) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"SyncState.Status\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.SyncStatuses[0],\n\t\t\t})\n\t\t}\n\t\tif len(o.EnvIds) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"EnvId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.EnvIds[0],\n\t\t\t})\n\t\t}\n\t\tif o.Name != \"\" {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"Name\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.Name,\n\t\t\t})\n\t\t}\n\t}\n\n\tapps, _, err := a.applicationStore.ListApplications(ctx, datastore.ListOptions{\n\t\tFilters: filters,\n\t\tOrders:  orders,\n\t})\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get applications\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get applications\")\n\t}\n\n\tif len(req.Options.Labels) == 0 {\n\t\treturn &webservice.ListApplicationsResponse{\n\t\t\tApplications: apps,\n\t\t}, nil\n\t}\n\n\t// NOTE: Filtering by labels is done by the application-side because we need to create composite indexes for every combination in the filter.\n\tfiltered := make([]*model.Application, 0, len(apps))\n\tfor _, a := range apps {\n\t\tif a.ContainLabels(req.Options.Labels) {\n\t\t\tfiltered = append(filtered, a)\n\t\t}\n\t}\n\treturn &webservice.ListApplicationsResponse{\n\t\tApplications: filtered,\n\t}, nil\n}\n\nfunc (a *WebAPI) SyncApplication(ctx context.Context, req *webservice.SyncApplicationRequest) (*webservice.SyncApplicationResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tapp, err := getApplication(ctx, a.applicationStore, req.ApplicationId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif claims.Role.ProjectId != app.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested application does not belong to your project\")\n\t}\n\n\tcmd := model.Command{\n\t\tId:            uuid.New().String(),\n\t\tPipedId:       app.PipedId,\n\t\tApplicationId: app.Id,\n\t\tProjectId:     app.ProjectId,\n\t\tType:          model.Command_SYNC_APPLICATION,\n\t\tCommander:     claims.Subject,\n\t\tSyncApplication: &model.Command_SyncApplication{\n\t\t\tApplicationId: app.Id,\n\t\t\tSyncStrategy:  req.SyncStrategy,\n\t\t},\n\t}\n\tif err := addCommand(ctx, a.commandStore, &cmd, a.logger); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.SyncApplicationResponse{\n\t\tCommandId: cmd.Id,\n\t}, nil\n}\n\nfunc (a *WebAPI) GetApplication(ctx context.Context, req *webservice.GetApplicationRequest) (*webservice.GetApplicationResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tapp, err := getApplication(ctx, a.applicationStore, req.ApplicationId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif app.ProjectId != claims.Role.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested application does not belong to your project\")\n\t}\n\n\treturn &webservice.GetApplicationResponse{\n\t\tApplication: app,\n\t}, nil\n}\n\nfunc (a *WebAPI) GenerateApplicationSealedSecret(ctx context.Context, req *webservice.GenerateApplicationSealedSecretRequest) (*webservice.GenerateApplicationSealedSecretResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tpiped, err := getPiped(ctx, a.pipedStore, req.PipedId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := a.validatePipedBelongsToProject(ctx, req.PipedId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\n\tse := model.GetSecretEncryptionInPiped(piped)\n\tpubkey, err := getEncriptionKey(se)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tciphertext, err := encrypt(req.Data, pubkey, req.Base64Encoding, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.GenerateApplicationSealedSecretResponse{\n\t\tData: ciphertext,\n\t}, nil\n}\n\n// validateAppBelongsToProject checks if the given application belongs to the given project.\n// It gives back error unless the application belongs to the project.\nfunc (a *WebAPI) validateAppBelongsToProject(ctx context.Context, appID, projectID string) error {\n\tpid, err := a.appProjectCache.Get(appID)\n\tif err == nil {\n\t\tif pid != projectID {\n\t\t\treturn status.Error(codes.PermissionDenied, \"Requested application doesn't belong to the project you logged in\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tapp, err := getApplication(ctx, a.applicationStore, appID, a.logger)\n\tif err != nil {\n\t\treturn err\n\t}\n\ta.appProjectCache.Put(appID, app.ProjectId)\n\n\tif app.ProjectId != projectID {\n\t\treturn status.Error(codes.PermissionDenied, \"Requested application doesn't belong to the project you logged in\")\n\t}\n\treturn nil\n}\n\nfunc (a *WebAPI) ListDeployments(ctx context.Context, req *webservice.ListDeploymentsRequest) (*webservice.ListDeploymentsResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\torders := []datastore.Order{\n\t\t{\n\t\t\tField:     \"UpdatedAt\",\n\t\t\tDirection: datastore.Desc,\n\t\t},\n\t\t{\n\t\t\tField:     \"Id\",\n\t\t\tDirection: datastore.Asc,\n\t\t},\n\t}\n\tfilters := []datastore.ListFilter{\n\t\t{\n\t\t\tField:    \"ProjectId\",\n\t\t\tOperator: datastore.OperatorEqual,\n\t\t\tValue:    claims.Role.ProjectId,\n\t\t},\n\t\t{\n\t\t\tField:    \"UpdatedAt\",\n\t\t\tOperator: datastore.OperatorGreaterThanOrEqual,\n\t\t\tValue:    req.PageMinUpdatedAt,\n\t\t},\n\t}\n\tif o := req.Options; o != nil {\n\t\t// Allowing multiple so that it can do In Query later.\n\t\t// Currently only the first value is used.\n\t\tif len(o.Statuses) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"Status\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.Statuses[0],\n\t\t\t})\n\t\t}\n\t\tif len(o.Kinds) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"Kind\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.Kinds[0],\n\t\t\t})\n\t\t}\n\t\tif len(o.ApplicationIds) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"ApplicationId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.ApplicationIds[0],\n\t\t\t})\n\t\t}\n\t\tif len(o.EnvIds) > 0 {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"EnvId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.EnvIds[0],\n\t\t\t})\n\t\t}\n\t\tif o.ApplicationName != \"\" {\n\t\t\tfilters = append(filters, datastore.ListFilter{\n\t\t\t\tField:    \"ApplicationName\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    o.ApplicationName,\n\t\t\t})\n\t\t}\n\t}\n\n\tpageSize := int(req.PageSize)\n\toptions := datastore.ListOptions{\n\t\tFilters: filters,\n\t\tOrders:  orders,\n\t\tLimit:   pageSize,\n\t\tCursor:  req.Cursor,\n\t}\n\tdeployments, cursor, err := a.deploymentStore.ListDeployments(ctx, options)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get deployments\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get deployments\")\n\t}\n\tlabels := req.Options.Labels\n\tif len(labels) == 0 || len(deployments) == 0 {\n\t\treturn &webservice.ListDeploymentsResponse{\n\t\t\tDeployments: deployments,\n\t\t\tCursor:      cursor,\n\t\t}, nil\n\t}\n\n\t// Start filtering them by labels.\n\t//\n\t// NOTE: Filtering by labels is done by the application-side because we need to create composite indexes for every combination in the filter.\n\t// We don't want to depend on any other search engine, that's why it filters here.\n\tfiltered := make([]*model.Deployment, 0, len(deployments))\n\tfor _, d := range deployments {\n\t\tif d.ContainLabels(labels) {\n\t\t\tfiltered = append(filtered, d)\n\t\t}\n\t}\n\t// Stop running additional queries for more data, and return filtered deployments immediately with\n\t// current cursor if the size before filtering is already less than the page size.\n\tif len(deployments) < pageSize {\n\t\treturn &webservice.ListDeploymentsResponse{\n\t\t\tDeployments: filtered,\n\t\t\tCursor:      cursor,\n\t\t}, nil\n\t}\n\t// Repeat the query until the number of filtered deployments reaches the page size,\n\t// or until it finishes scanning to page_min_updated_at.\n\tfor len(filtered) < pageSize {\n\t\toptions.Cursor = cursor\n\t\tdeployments, cursor, err = a.deploymentStore.ListDeployments(ctx, options)\n\t\tif err != nil {\n\t\t\ta.logger.Error(\"failed to get deployments\", zap.Error(err))\n\t\t\treturn nil, status.Error(codes.Internal, \"Failed to get deployments\")\n\t\t}\n\t\tif len(deployments) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tfor _, d := range deployments {\n\t\t\tif d.ContainLabels(labels) {\n\t\t\t\tfiltered = append(filtered, d)\n\t\t\t}\n\t\t}\n\t\t// We've already specified UpdatedAt >= req.PageMinUpdatedAt, so we need to check just equality.\n\t\tif deployments[len(deployments)-1].UpdatedAt == req.PageMinUpdatedAt {\n\t\t\tbreak\n\t\t}\n\t}\n\t// TODO: Think about possibility that the response of ListDeployments exceeds the page size\n\treturn &webservice.ListDeploymentsResponse{\n\t\tDeployments: filtered,\n\t\tCursor:      cursor,\n\t}, nil\n}\n\nfunc (a *WebAPI) GetDeployment(ctx context.Context, req *webservice.GetDeploymentRequest) (*webservice.GetDeploymentResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tdeployment, err := getDeployment(ctx, a.deploymentStore, req.DeploymentId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif claims.Role.ProjectId != deployment.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested deployment does not belong to your project\")\n\t}\n\n\treturn &webservice.GetDeploymentResponse{\n\t\tDeployment: deployment,\n\t}, nil\n}\n\n// validateDeploymentBelongsToProject checks if the given deployment belongs to the given project.\n// It gives back error unless the deployment belongs to the project.\nfunc (a *WebAPI) validateDeploymentBelongsToProject(ctx context.Context, deploymentID, projectID string) error {\n\tpid, err := a.deploymentProjectCache.Get(deploymentID)\n\tif err == nil {\n\t\tif pid != projectID {\n\t\t\treturn status.Error(codes.PermissionDenied, \"Requested deployment doesn't belong to the project you logged in\")\n\t\t}\n\t\treturn nil\n\t}\n\n\tdeployment, err := getDeployment(ctx, a.deploymentStore, deploymentID, a.logger)\n\tif err != nil {\n\t\treturn err\n\t}\n\ta.deploymentProjectCache.Put(deploymentID, deployment.ProjectId)\n\n\tif deployment.ProjectId != projectID {\n\t\treturn status.Error(codes.PermissionDenied, \"Requested deployment doesn't belong to the project you logged in\")\n\t}\n\treturn nil\n}\n\nfunc (a *WebAPI) GetStageLog(ctx context.Context, req *webservice.GetStageLogRequest) (*webservice.GetStageLogResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif err := a.validateDeploymentBelongsToProject(ctx, req.DeploymentId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\n\tblocks, completed, err := a.stageLogStore.FetchLogs(ctx, req.DeploymentId, req.StageId, req.RetriedCount, req.OffsetIndex)\n\tif errors.Is(err, stagelogstore.ErrNotFound) {\n\t\treturn nil, status.Error(codes.NotFound, \"The stage log not found\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get stage logs\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get stage logs\")\n\t}\n\n\treturn &webservice.GetStageLogResponse{\n\t\tBlocks:    blocks,\n\t\tCompleted: completed,\n\t}, nil\n}\n\nfunc (a *WebAPI) CancelDeployment(ctx context.Context, req *webservice.CancelDeploymentRequest) (*webservice.CancelDeploymentResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tdeployment, err := getDeployment(ctx, a.deploymentStore, req.DeploymentId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif claims.Role.ProjectId != deployment.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested deployment does not belong to your project\")\n\t}\n\n\tif model.IsCompletedDeployment(deployment.Status) {\n\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"could not cancel the deployment because it was already completed\")\n\t}\n\n\tcmd := model.Command{\n\t\tId:            uuid.New().String(),\n\t\tPipedId:       deployment.PipedId,\n\t\tApplicationId: deployment.ApplicationId,\n\t\tProjectId:     deployment.ProjectId,\n\t\tDeploymentId:  req.DeploymentId,\n\t\tType:          model.Command_CANCEL_DEPLOYMENT,\n\t\tCommander:     claims.Subject,\n\t\tCancelDeployment: &model.Command_CancelDeployment{\n\t\t\tDeploymentId:    req.DeploymentId,\n\t\t\tForceRollback:   req.ForceRollback,\n\t\t\tForceNoRollback: req.ForceNoRollback,\n\t\t},\n\t}\n\tif err := addCommand(ctx, a.commandStore, &cmd, a.logger); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.CancelDeploymentResponse{\n\t\tCommandId: cmd.Id,\n\t}, nil\n}\n\nfunc (a *WebAPI) ApproveStage(ctx context.Context, req *webservice.ApproveStageRequest) (*webservice.ApproveStageResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tdeployment, err := getDeployment(ctx, a.deploymentStore, req.DeploymentId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := validateApprover(deployment.Stages, claims.Subject, req.StageId); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := a.validateDeploymentBelongsToProject(ctx, req.DeploymentId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\tstage, ok := deployment.StageStatusMap()[req.StageId]\n\tif !ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"The stage was not found in the deployment\")\n\t}\n\tif model.IsCompletedStage(stage) {\n\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"Could not approve the stage because it was already completed\")\n\t}\n\n\tcommandID := uuid.New().String()\n\tcmd := model.Command{\n\t\tId:            commandID,\n\t\tPipedId:       deployment.PipedId,\n\t\tApplicationId: deployment.ApplicationId,\n\t\tProjectId:     deployment.ProjectId,\n\t\tDeploymentId:  req.DeploymentId,\n\t\tStageId:       req.StageId,\n\t\tType:          model.Command_APPROVE_STAGE,\n\t\tCommander:     claims.Subject,\n\t\tApproveStage: &model.Command_ApproveStage{\n\t\t\tDeploymentId: req.DeploymentId,\n\t\t\tStageId:      req.StageId,\n\t\t},\n\t}\n\tif err := addCommand(ctx, a.commandStore, &cmd, a.logger); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.ApproveStageResponse{\n\t\tCommandId: commandID,\n\t}, nil\n}\n\n// No error means that the given commander is valid.\nfunc validateApprover(stages []*model.PipelineStage, commander, stageID string) error {\n\tvar approvers []string\n\tfor _, s := range stages {\n\t\tif s.Id != stageID {\n\t\t\tcontinue\n\t\t}\n\t\tif as := s.Metadata[\"Approvers\"]; as != \"\" {\n\t\t\tapprovers = strings.Split(as, \",\")\n\t\t}\n\t\tbreak\n\t}\n\tif len(approvers) == 0 {\n\t\t// Anyone can approve the deployment pipeline\n\t\treturn nil\n\t}\n\tfor _, ap := range approvers {\n\t\tif ap == commander {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn status.Error(codes.PermissionDenied, fmt.Sprintf(\"You can't approve this deployment because you (%s) are not in the approver list: %v\", commander, approvers))\n}\n\nfunc (a *WebAPI) GetApplicationLiveState(ctx context.Context, req *webservice.GetApplicationLiveStateRequest) (*webservice.GetApplicationLiveStateResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif err := a.validateAppBelongsToProject(ctx, req.ApplicationId, claims.Role.ProjectId); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsnapshot, err := a.applicationLiveStateStore.GetStateSnapshot(ctx, req.ApplicationId)\n\tif errors.Is(err, filestore.ErrNotFound) {\n\t\treturn nil, status.Error(codes.NotFound, \"Application live state not found\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get application live state\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get application live state\")\n\t}\n\treturn &webservice.GetApplicationLiveStateResponse{\n\t\tSnapshot: snapshot,\n\t}, nil\n}\n\n// GetProject gets the specified porject without sensitive data.\nfunc (a *WebAPI) GetProject(ctx context.Context, req *webservice.GetProjectRequest) (*webservice.GetProjectResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tproject, err := a.getProject(ctx, claims.Role.ProjectId)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Redact all sensitive data inside project message before sending to the client.\n\tproject.RedactSensitiveData()\n\n\treturn &webservice.GetProjectResponse{\n\t\tProject: project,\n\t}, nil\n}\n\nfunc (a *WebAPI) getProject(ctx context.Context, projectID string) (*model.Project, error) {\n\tif p, ok := a.projectsInConfig[projectID]; ok {\n\t\treturn &model.Project{\n\t\t\tId:   p.Id,\n\t\t\tDesc: p.Desc,\n\t\t\tStaticAdmin: &model.ProjectStaticUser{\n\t\t\t\tUsername:     p.StaticAdmin.Username,\n\t\t\t\tPasswordHash: p.StaticAdmin.PasswordHash,\n\t\t\t},\n\t\t}, nil\n\t}\n\n\tproject, err := a.projectStore.GetProject(ctx, projectID)\n\tif errors.Is(err, datastore.ErrNotFound) {\n\t\treturn nil, status.Error(codes.NotFound, \"The project is not found\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to get project\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to get project\")\n\t}\n\treturn project, nil\n}\n\n// UpdateProjectStaticAdmin updates the static admin user settings.\nfunc (a *WebAPI) UpdateProjectStaticAdmin(ctx context.Context, req *webservice.UpdateProjectStaticAdminRequest) (*webservice.UpdateProjectStaticAdminResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif _, ok := a.projectsInConfig[claims.Role.ProjectId]; ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"Failed to update a debug project specified in the control-plane configuration\")\n\t}\n\n\tif err := a.projectStore.UpdateProjectStaticAdmin(ctx, claims.Role.ProjectId, req.Username, req.Password); err != nil {\n\t\ta.logger.Error(\"failed to update static admin\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to update static admin\")\n\t}\n\treturn &webservice.UpdateProjectStaticAdminResponse{}, nil\n}\n\n// EnableStaticAdmin enables static admin login.\nfunc (a *WebAPI) EnableStaticAdmin(ctx context.Context, req *webservice.EnableStaticAdminRequest) (*webservice.EnableStaticAdminResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif _, ok := a.projectsInConfig[claims.Role.ProjectId]; ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"Failed to update a debug project specified in the control-plane configuration\")\n\t}\n\n\tif err := a.projectStore.EnableStaticAdmin(ctx, claims.Role.ProjectId); err != nil {\n\t\ta.logger.Error(\"failed to enable static admin login\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to enable static admin login\")\n\t}\n\treturn &webservice.EnableStaticAdminResponse{}, nil\n}\n\n// DisableStaticAdmin disables static admin login.\nfunc (a *WebAPI) DisableStaticAdmin(ctx context.Context, req *webservice.DisableStaticAdminRequest) (*webservice.DisableStaticAdminResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif _, ok := a.projectsInConfig[claims.Role.ProjectId]; ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"Failed to update a debug project specified in the control-plane configuration\")\n\t}\n\n\tif err := a.projectStore.DisableStaticAdmin(ctx, claims.Role.ProjectId); err != nil {\n\t\ta.logger.Error(\"failed to disenable static admin login\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to disenable static admin login\")\n\t}\n\treturn &webservice.DisableStaticAdminResponse{}, nil\n}\n\n// UpdateProjectSSOConfig updates the sso settings.\nfunc (a *WebAPI) UpdateProjectSSOConfig(ctx context.Context, req *webservice.UpdateProjectSSOConfigRequest) (*webservice.UpdateProjectSSOConfigResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif _, ok := a.projectsInConfig[claims.Role.ProjectId]; ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"Failed to update a debug project specified in the control-plane configuration\")\n\t}\n\n\tif err := req.Sso.Encrypt(a.encrypter); err != nil {\n\t\ta.logger.Error(\"failed to encrypt sensitive data in sso configurations\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to encrypt sensitive data in sso configurations\")\n\t}\n\n\tif err := a.projectStore.UpdateProjectSSOConfig(ctx, claims.Role.ProjectId, req.Sso); err != nil {\n\t\ta.logger.Error(\"failed to update project single sign on settings\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to update project single sign on settings\")\n\t}\n\treturn &webservice.UpdateProjectSSOConfigResponse{}, nil\n}\n\n// UpdateProjectRBACConfig updates the sso settings.\nfunc (a *WebAPI) UpdateProjectRBACConfig(ctx context.Context, req *webservice.UpdateProjectRBACConfigRequest) (*webservice.UpdateProjectRBACConfigResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif _, ok := a.projectsInConfig[claims.Role.ProjectId]; ok {\n\t\treturn nil, status.Error(codes.FailedPrecondition, \"Failed to update a debug project specified in the control-plane configuration\")\n\t}\n\n\tif err := a.projectStore.UpdateProjectRBACConfig(ctx, claims.Role.ProjectId, req.Rbac); err != nil {\n\t\ta.logger.Error(\"failed to update project single sign on settings\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to update project single sign on settings\")\n\t}\n\treturn &webservice.UpdateProjectRBACConfigResponse{}, nil\n}\n\n// GetMe gets information about the current user.\nfunc (a *WebAPI) GetMe(ctx context.Context, req *webservice.GetMeRequest) (*webservice.GetMeResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\treturn &webservice.GetMeResponse{\n\t\tSubject:     claims.Subject,\n\t\tAvatarUrl:   claims.AvatarURL,\n\t\tProjectId:   claims.Role.ProjectId,\n\t\tProjectRole: claims.Role.ProjectRole,\n\t}, nil\n}\n\nfunc (a *WebAPI) GetCommand(ctx context.Context, req *webservice.GetCommandRequest) (*webservice.GetCommandResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tcmd, err := getCommand(ctx, a.commandStore, req.CommandId, a.logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif claims.Role.ProjectId != cmd.ProjectId {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"Requested command does not belong to your project\")\n\t}\n\n\treturn &webservice.GetCommandResponse{\n\t\tCommand: cmd,\n\t}, nil\n}\n\nfunc (a *WebAPI) GenerateAPIKey(ctx context.Context, req *webservice.GenerateAPIKeyRequest) (*webservice.GenerateAPIKeyResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tid := uuid.New().String()\n\tkey, hash, err := model.GenerateAPIKey(id)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to generate API key\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to generate API key\")\n\t}\n\n\tapiKey := model.APIKey{\n\t\tId:        id,\n\t\tName:      req.Name,\n\t\tKeyHash:   hash,\n\t\tProjectId: claims.Role.ProjectId,\n\t\tRole:      req.Role,\n\t\tCreator:   claims.Subject,\n\t}\n\n\terr = a.apiKeyStore.AddAPIKey(ctx, &apiKey)\n\tif errors.Is(err, datastore.ErrAlreadyExists) {\n\t\treturn nil, status.Error(codes.AlreadyExists, \"The API key already exists\")\n\t}\n\tif err != nil {\n\t\ta.logger.Error(\"failed to create API key\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to create API key\")\n\t}\n\n\treturn &webservice.GenerateAPIKeyResponse{\n\t\tKey: key,\n\t}, nil\n}\n\nfunc (a *WebAPI) DisableAPIKey(ctx context.Context, req *webservice.DisableAPIKeyRequest) (*webservice.DisableAPIKeyResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tif err := a.apiKeyStore.DisableAPIKey(ctx, req.Id, claims.Role.ProjectId); err != nil {\n\t\tswitch err {\n\t\tcase datastore.ErrNotFound:\n\t\t\treturn nil, status.Error(codes.InvalidArgument, \"The API key is not found\")\n\t\tcase datastore.ErrInvalidArgument:\n\t\t\treturn nil, status.Error(codes.InvalidArgument, \"Invalid value for update\")\n\t\tdefault:\n\t\t\ta.logger.Error(\"failed to disable the API key\",\n\t\t\t\tzap.String(\"apikey-id\", req.Id),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t\treturn nil, status.Error(codes.Internal, \"Failed to disable the API key\")\n\t\t}\n\t}\n\n\treturn &webservice.DisableAPIKeyResponse{}, nil\n}\n\nfunc (a *WebAPI) ListAPIKeys(ctx context.Context, req *webservice.ListAPIKeysRequest) (*webservice.ListAPIKeysResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\topts := datastore.ListOptions{\n\t\tFilters: []datastore.ListFilter{\n\t\t\t{\n\t\t\t\tField:    \"ProjectId\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    claims.Role.ProjectId,\n\t\t\t},\n\t\t},\n\t}\n\n\tif req.Options != nil {\n\t\tif req.Options.Enabled != nil {\n\t\t\topts.Filters = append(opts.Filters, datastore.ListFilter{\n\t\t\t\tField:    \"Disabled\",\n\t\t\t\tOperator: datastore.OperatorEqual,\n\t\t\t\tValue:    !req.Options.Enabled.GetValue(),\n\t\t\t})\n\t\t}\n\t}\n\n\tapiKeys, err := a.apiKeyStore.ListAPIKeys(ctx, opts)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to list API keys\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"Failed to list API keys\")\n\t}\n\n\t// Redact all sensitive data inside API key before sending to the client.\n\tfor i := range apiKeys {\n\t\tapiKeys[i].RedactSensitiveData()\n\t}\n\n\treturn &webservice.ListAPIKeysResponse{\n\t\tKeys: apiKeys,\n\t}, nil\n}\n\n// GetInsightData returns the accumulated insight data.\nfunc (a *WebAPI) GetInsightData(ctx context.Context, req *webservice.GetInsightDataRequest) (*webservice.GetInsightDataResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\tcount := int(req.DataPointCount)\n\tfrom := time.Unix(req.RangeFrom, 0)\n\n\tchunks, err := insightstore.LoadChunksFromCache(a.insightCache, claims.Role.ProjectId, req.ApplicationId, req.MetricsKind, req.Step, from, count)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to load chunks from cache\", zap.Error(err))\n\n\t\tchunks, err = a.insightStore.LoadChunks(ctx, claims.Role.ProjectId, req.ApplicationId, req.MetricsKind, req.Step, from, count)\n\t\tif err != nil {\n\t\t\ta.logger.Error(\"failed to load chunks from insightstore\", zap.Error(err))\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := insightstore.PutChunksToCache(a.insightCache, chunks); err != nil {\n\t\t\ta.logger.Error(\"failed to put chunks to cache\", zap.Error(err))\n\t\t}\n\t}\n\n\tidp, err := chunks.ExtractDataPoints(req.Step, from, count)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to extract data points from chunks\", zap.Error(err))\n\t}\n\n\tvar updateAt int64\n\tfor _, c := range chunks {\n\t\taccumulatedTo := c.GetAccumulatedTo()\n\t\tif accumulatedTo > updateAt {\n\t\t\tupdateAt = accumulatedTo\n\t\t}\n\t}\n\n\treturn &webservice.GetInsightDataResponse{\n\t\tUpdatedAt:  updateAt,\n\t\tDataPoints: idp,\n\t\tType:       model.InsightResultType_MATRIX,\n\t\tMatrix: []*model.InsightSampleStream{\n\t\t\t{\n\t\t\t\tDataPoints: idp,\n\t\t\t},\n\t\t},\n\t}, nil\n}\n\nfunc (a *WebAPI) GetInsightApplicationCount(ctx context.Context, req *webservice.GetInsightApplicationCountRequest) (*webservice.GetInsightApplicationCountResponse, error) {\n\tclaims, err := rpcauth.ExtractClaims(ctx)\n\tif err != nil {\n\t\ta.logger.Error(\"failed to authenticate the current user\", zap.Error(err))\n\t\treturn nil, err\n\t}\n\n\t// TODO: Cache application counts in the cache service.\n\tc, err := a.insightStore.LoadApplicationCounts(ctx, claims.Role.ProjectId)\n\tif err != nil {\n\t\tif err == filestore.ErrNotFound {\n\t\t\treturn nil, status.Error(codes.NotFound, \"Not found\")\n\t\t}\n\t\ta.logger.Error(\"failed to load application counts\", zap.Error(err))\n\t\treturn nil, status.Error(codes.Internal, \"failed to load application counts\")\n\t}\n\n\tcounts := make([]*model.InsightApplicationCount, 0, len(c.Counts))\n\tfor i := range c.Counts {\n\t\tcounts = append(counts, &c.Counts[i])\n\t}\n\n\treturn &webservice.GetInsightApplicationCountResponse{\n\t\tCounts:    counts,\n\t\tUpdatedAt: c.UpdatedAt,\n\t}, nil\n}\n", "idx": 3, "id": 23233, "msg": "", "proj": "pipe-cd-pipe", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -57,6 +57,7 @@ type accountingPeer struct {\n type Options struct {\n \tPaymentThreshold uint64\n \tPaymentTolerance uint64\n+\tEarlyPayment     uint64\n \tLogger           logging.Logger\n \tStore            storage.StateStorer\n \tSettlement       settlement.Interface", "y": 0, "oldf": "// Copyright 2020 The Swarm Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage accounting\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/p2p\"\n\t\"github.com/ethersphere/bee/pkg/settlement\"\n\t\"github.com/ethersphere/bee/pkg/storage\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n)\n\nvar (\n\t_              Interface = (*Accounting)(nil)\n\tbalancesPrefix string    = \"balance_\"\n)\n\n// Interface is the Accounting interface.\ntype Interface interface {\n\t// Reserve reserves a portion of the balance for peer. Returns an error if\n\t// the operation risks exceeding the disconnect threshold.\n\t//\n\t// This should be called (always in combination with Release) before a\n\t// Credit action to prevent overspending in case of concurrent requests.\n\tReserve(peer swarm.Address, price uint64) error\n\t// Release releases the reserved funds.\n\tRelease(peer swarm.Address, price uint64)\n\t// Credit increases the balance the peer has with us (we \"pay\" the peer).\n\tCredit(peer swarm.Address, price uint64) error\n\t// Debit increases the balance we have with the peer (we get \"paid\" back).\n\tDebit(peer swarm.Address, price uint64) error\n\t// Balance returns the current balance for the given peer.\n\tBalance(peer swarm.Address) (int64, error)\n\t// Balances returns balances for all known peers.\n\tBalances() (map[string]int64, error)\n}\n\n// accountingPeer holds all in-memory accounting information for one peer.\ntype accountingPeer struct {\n\t// Lock to be held during any accounting action for this peer.\n\tlock sync.Mutex\n\t// Amount currently reserved for active peer interaction\n\treservedBalance uint64\n}\n\n// Options are options provided to Accounting.\ntype Options struct {\n\tPaymentThreshold uint64\n\tPaymentTolerance uint64\n\tLogger           logging.Logger\n\tStore            storage.StateStorer\n\tSettlement       settlement.Interface\n}\n\n// Accounting is the main implementation of the accounting interface.\ntype Accounting struct {\n\t// Mutex for accessing the accountingPeers map.\n\taccountingPeersMu sync.Mutex\n\taccountingPeers   map[string]*accountingPeer\n\tlogger            logging.Logger\n\tstore             storage.StateStorer\n\t// The payment threshold in BZZ we communicate to our peers.\n\tpaymentThreshold uint64\n\t// The amount in BZZ we let peers exceed the payment threshold before we\n\t// disconnect them.\n\tpaymentTolerance uint64\n\tsettlement       settlement.Interface\n\tmetrics          metrics\n}\n\nvar (\n\t// ErrOverdraft denotes the expected debt in Reserve would exceed the payment thresholds.\n\tErrOverdraft = errors.New(\"attempted overdraft\")\n\t// ErrDisconnectThresholdExceeded denotes a peer has exceeded the disconnect threshold.\n\tErrDisconnectThresholdExceeded = errors.New(\"disconnect threshold exceeded\")\n\t// ErrInvalidPaymentTolerance denotes the payment tolerance is too high\n\t// compared to the payment threshold.\n\tErrInvalidPaymentTolerance = errors.New(\"payment tolerance must be less than half the payment threshold\")\n\t// ErrPeerNoBalance is the error returned if no balance in store exists for a peer\n\tErrPeerNoBalance = errors.New(\"no balance for peer\")\n\t// ErrOverflow denotes an arithmetic operation overflowed.\n\tErrOverflow = errors.New(\"overflow error\")\n)\n\n// NewAccounting creates a new Accounting instance with the provided options.\nfunc NewAccounting(o Options) (*Accounting, error) {\n\tif o.PaymentTolerance+o.PaymentThreshold > math.MaxInt64 {\n\t\treturn nil, fmt.Errorf(\"tolerance plus threshold too big: %w\", ErrOverflow)\n\t}\n\n\tif o.PaymentTolerance > o.PaymentThreshold/2 {\n\t\treturn nil, ErrInvalidPaymentTolerance\n\t}\n\n\treturn &Accounting{\n\t\taccountingPeers:  make(map[string]*accountingPeer),\n\t\tpaymentThreshold: o.PaymentThreshold,\n\t\tpaymentTolerance: o.PaymentTolerance,\n\t\tlogger:           o.Logger,\n\t\tstore:            o.Store,\n\t\tsettlement:       o.Settlement,\n\t\tmetrics:          newMetrics(),\n\t}, nil\n}\n\n// Reserve reserves a portion of the balance for peer.\nfunc (a *Accounting) Reserve(peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Subtract already reserved amount from actual balance, to get expected balance\n\texpectedBalance, err := subtractI64mU64(currentBalance, accountingPeer.reservedBalance)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Determine if we owe anything to the peer, if we owe less than 0, we conclude we owe nothing\n\t// This conversion is made safe by previous subtractI64mU64 not allowing MinInt64\n\texpectedDebt := -expectedBalance\n\tif expectedDebt < 0 {\n\t\texpectedDebt = 0\n\t}\n\n\t// Check if the expected debt is already over the payment threshold.\n\tif uint64(expectedDebt) > a.paymentThreshold {\n\t\ta.metrics.AccountingBlocksCount.Inc()\n\t\treturn ErrOverdraft\n\t}\n\n\t// Check for safety of increase of reservedBalance by price\n\tif accountingPeer.reservedBalance+price < accountingPeer.reservedBalance {\n\t\treturn ErrOverflow\n\t}\n\n\taccountingPeer.reservedBalance += price\n\n\treturn nil\n}\n\n// Release releases reserved funds.\nfunc (a *Accounting) Release(peer swarm.Address, price uint64) {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\ta.logger.Errorf(\"cannot release balance for peer: %v\", err)\n\t\treturn\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\t// NOTE: this should never happen if Reserve and Release calls are paired.\n\tif price > accountingPeer.reservedBalance {\n\t\ta.logger.Error(\"attempting to release more balance than was reserved for peer\")\n\t\taccountingPeer.reservedBalance = 0\n\t} else {\n\t\taccountingPeer.reservedBalance -= price\n\t}\n}\n\n// Credit increases the amount of credit we have with the given peer\n// (and decreases existing debt).\nfunc (a *Accounting) Credit(peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Calculate next balance by safely decreasing current balance with the price we credit\n\tnextBalance, err := subtractI64mU64(currentBalance, price)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ta.logger.Tracef(\"crediting peer %v with price %d, new balance is %d\", peer, price, nextBalance)\n\n\t// Get expectedbalance by safely decreasing current balance with reserved amounts\n\texpectedBalance, err := subtractI64mU64(currentBalance, accountingPeer.reservedBalance)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Compute expected debt before update because reserve still includes the\n\t// amount that is deducted from the balance.\n\t// This conversion is made safe by previous subtractI64mU64 not allowing MinInt64\n\texpectedDebt := -expectedBalance\n\tif expectedDebt < 0 {\n\t\texpectedDebt = 0\n\t}\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\ta.metrics.TotalCreditedAmount.Add(float64(price))\n\ta.metrics.CreditEventsCount.Inc()\n\n\t// If our expected debt exceeds our payment threshold (which we assume is\n\t// also the peers payment threshold), trigger settlement.\n\tif uint64(expectedDebt) >= a.paymentThreshold {\n\t\terr = a.settle(peer, accountingPeer)\n\t\tif err != nil {\n\t\t\ta.logger.Errorf(\"failed to settle with peer %v: %v\", peer, err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Settle all debt with a peer. The lock on the accountingPeer must be held when\n// called.\nfunc (a *Accounting) settle(peer swarm.Address, balance *accountingPeer) error {\n\toldBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Don't do anything if there is no actual debt.\n\t// This might be the case if the peer owes us and the total reserve for a\n\t// peer exceeds the payment treshold.\n\tif oldBalance >= 0 {\n\t\treturn nil\n\t}\n\n\t// check safety of the following -1 * int64 conversion, all negative int64 have positive int64 equals except MinInt64\n\tif oldBalance == math.MinInt64 {\n\t\treturn ErrOverflow\n\t}\n\n\t// This is safe because of the earlier check for oldbalance < 0 and the check for != MinInt64\n\tpaymentAmount := uint64(-oldBalance)\n\tnextBalance := 0\n\n\t// Try to save the next balance first.\n\t// Otherwise we might pay and then not be able to save, forcing us to pay\n\t// again after restart.\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\terr = a.settlement.Pay(context.Background(), peer, paymentAmount)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"settlement for amount %d failed: %w\", paymentAmount, err)\n\t\t// If the payment didn't succeed we should restore the old balance in\n\t\t// the state store.\n\t\tif storeErr := a.store.Put(peerBalanceKey(peer), oldBalance); storeErr != nil {\n\t\t\ta.logger.Errorf(\"failed to restore balance after failed settlement for peer %v: %v\", peer, storeErr)\n\t\t}\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Debit increases the amount of debt we have with the given peer (and decreases\n// existing credit).\nfunc (a *Accounting) Debit(peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Get nextBalance by safely increasing current balance with price\n\tnextBalance, err := addI64pU64(currentBalance, price)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ta.logger.Tracef(\"debiting peer %v with price %d, new balance is %d\", peer, price, nextBalance)\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\ta.metrics.TotalDebitedAmount.Add(float64(price))\n\ta.metrics.DebitEventsCount.Inc()\n\n\tif nextBalance >= int64(a.paymentThreshold+a.paymentTolerance) {\n\t\t// peer too much in debt\n\t\ta.metrics.AccountingDisconnectsCount.Inc()\n\t\treturn p2p.NewBlockPeerError(10000*time.Hour, ErrDisconnectThresholdExceeded)\n\t}\n\n\treturn nil\n}\n\n// Balance returns the current balance for the given peer.\nfunc (a *Accounting) Balance(peer swarm.Address) (balance int64, err error) {\n\terr = a.store.Get(peerBalanceKey(peer), &balance)\n\n\tif err != nil {\n\t\tif errors.Is(err, storage.ErrNotFound) {\n\t\t\treturn 0, ErrPeerNoBalance\n\t\t}\n\t\treturn 0, err\n\t}\n\n\treturn balance, nil\n}\n\n// peerBalanceKey returns the balance storage key for the given peer.\nfunc peerBalanceKey(peer swarm.Address) string {\n\treturn fmt.Sprintf(\"%s%s\", balancesPrefix, peer.String())\n}\n\n// getAccountingPeer returns the accountingPeer for a given swarm address.\n// If not found in memory it will initialize it.\nfunc (a *Accounting) getAccountingPeer(peer swarm.Address) (*accountingPeer, error) {\n\ta.accountingPeersMu.Lock()\n\tdefer a.accountingPeersMu.Unlock()\n\n\tpeerData, ok := a.accountingPeers[peer.String()]\n\tif !ok {\n\t\tpeerData = &accountingPeer{\n\t\t\treservedBalance: 0,\n\t\t}\n\t\ta.accountingPeers[peer.String()] = peerData\n\t}\n\n\treturn peerData, nil\n}\n\n// Balances gets balances for all peers from store.\nfunc (a *Accounting) Balances() (map[string]int64, error) {\n\ts := make(map[string]int64)\n\n\terr := a.store.Iterate(balancesPrefix, func(key, val []byte) (stop bool, err error) {\n\t\taddr, err := balanceKeyPeer(key)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"parse address from key: %s: %v\", string(key), err)\n\t\t}\n\n\t\tif _, ok := s[addr.String()]; !ok {\n\t\t\tvar storevalue int64\n\t\t\terr = a.store.Get(peerBalanceKey(addr), &storevalue)\n\t\t\tif err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"get peer %s balance: %v\", addr.String(), err)\n\t\t\t}\n\n\t\t\ts[addr.String()] = storevalue\n\t\t}\n\n\t\treturn false, nil\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n\n// balanceKeyPeer returns the embedded peer from the balance storage key.\nfunc balanceKeyPeer(key []byte) (swarm.Address, error) {\n\tk := string(key)\n\n\tsplit := strings.SplitAfter(k, balancesPrefix)\n\tif len(split) != 2 {\n\t\treturn swarm.ZeroAddress, errors.New(\"no peer in key\")\n\t}\n\n\taddr, err := swarm.ParseHexAddress(split[1])\n\tif err != nil {\n\t\treturn swarm.ZeroAddress, err\n\t}\n\n\treturn addr, nil\n}\n\n// NotifyPayment implements the PaymentObserver interface. It is called by\n// Settlement when we receive a payment.\nfunc (a *Accounting) NotifyPayment(peer swarm.Address, amount uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tnextBalance, err := subtractI64mU64(currentBalance, amount)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Don't allow a payment to put use more into debt than the tolerance.\n\t// This is to prevent another node tricking us into settling by settling\n\t// first (e.g. send a bouncing cheque to trigger an honest cheque in swap).\n\tif nextBalance < -int64(a.paymentTolerance) {\n\t\treturn fmt.Errorf(\"refusing to accept payment which would put us too much in debt, new balance would have been %d\", nextBalance)\n\t}\n\n\ta.logger.Tracef(\"crediting peer %v with amount %d due to payment, new balance is %d\", peer, amount, nextBalance)\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// subtractI64mU64 is a helper function for safe subtraction of Int64 - Uint64\n// It checks for\n//   - overflow safety in conversion of uint64 to int64\n//   - safety of the arithmetic\n//   - whether ( -1 * result ) is still Int64, as MinInt64 in absolute sense is 1 larger than MaxInt64\n// If result is MinInt64, we also return overflow error, for two reasons:\n//   - in some cases we are going to use -1 * result in the following operations, which is secured by this check\n//   - we also do not want to possibly store this value as balance, even if ( -1 * result ) is not used immediately afterwards, because it could\n//\t\tdisable settleing for this amount as the value would create overflow\nfunc subtractI64mU64(base int64, subtracted uint64) (result int64, err error) {\n\tif subtracted > math.MaxInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tresult = base - int64(subtracted)\n\tif result > base {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tif result == math.MinInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\treturn result, nil\n}\n\n// addI64pU64 is a helper function for safe addition of Int64 + Uint64\n// It checks for\n//   - overflow safety in conversion of uint64 to int64\n//   - safety of the arithmetic\nfunc addI64pU64(a int64, b uint64) (result int64, err error) {\n\tif b > math.MaxInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tresult = a + int64(b)\n\tif result < a {\n\t\treturn 0, ErrOverflow\n\t}\n\n\treturn result, nil\n}\n", "idx": 1, "id": 12659, "msg": "", "proj": "ethersphere-bee", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -169,8 +169,7 @@ func (s *Server) HandleRPC(w http.ResponseWriter, r *http.Request) {\n \t\t\t\"req_id\", GetReqID(ctx),\n \t\t\t\"err\", err,\n \t\t)\n-\t\twriteRPCError(ctx, w, req.ID, err)\n-\t\treturn\n+\t\treturn NewRPCErrorRes(req.ID, err)\n \t}\n \n \tif backendRes.Error == nil {", "y": 0, "oldf": "package proxyd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/ethereum/go-ethereum/log\"\n\t\"github.com/gorilla/mux\"\n\t\"github.com/gorilla/websocket\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/rs/cors\"\n)\n\nconst (\n\tContextKeyAuth          = \"authorization\"\n\tContextKeyReqID         = \"req_id\"\n\tContextKeyXForwardedFor = \"x_forwarded_for\"\n)\n\ntype Server struct {\n\tbackendGroups      map[string]*BackendGroup\n\twsBackendGroup     *BackendGroup\n\twsMethodWhitelist  *StringSet\n\trpcMethodMappings  map[string]string\n\tmaxBodySize        int64\n\tauthenticatedPaths map[string]string\n\tupgrader           *websocket.Upgrader\n\trpcServer          *http.Server\n\twsServer           *http.Server\n\tcache              RPCCache\n}\n\nfunc NewServer(\n\tbackendGroups map[string]*BackendGroup,\n\twsBackendGroup *BackendGroup,\n\twsMethodWhitelist *StringSet,\n\trpcMethodMappings map[string]string,\n\tmaxBodySize int64,\n\tauthenticatedPaths map[string]string,\n\tcache RPCCache,\n) *Server {\n\tif cache == nil {\n\t\tcache = &NoopRPCCache{}\n\t}\n\treturn &Server{\n\t\tbackendGroups:      backendGroups,\n\t\twsBackendGroup:     wsBackendGroup,\n\t\twsMethodWhitelist:  wsMethodWhitelist,\n\t\trpcMethodMappings:  rpcMethodMappings,\n\t\tmaxBodySize:        maxBodySize,\n\t\tauthenticatedPaths: authenticatedPaths,\n\t\tcache:              cache,\n\t\tupgrader: &websocket.Upgrader{\n\t\t\tHandshakeTimeout: 5 * time.Second,\n\t\t},\n\t}\n}\n\nfunc (s *Server) RPCListenAndServe(host string, port int) error {\n\thdlr := mux.NewRouter()\n\thdlr.HandleFunc(\"/healthz\", s.HandleHealthz).Methods(\"GET\")\n\thdlr.HandleFunc(\"/\", s.HandleRPC).Methods(\"POST\")\n\thdlr.HandleFunc(\"/{authorization}\", s.HandleRPC).Methods(\"POST\")\n\tc := cors.New(cors.Options{\n\t\tAllowedOrigins: []string{\"*\"},\n\t})\n\taddr := fmt.Sprintf(\"%s:%d\", host, port)\n\ts.rpcServer = &http.Server{\n\t\tHandler: instrumentedHdlr(c.Handler(hdlr)),\n\t\tAddr:    addr,\n\t}\n\tlog.Info(\"starting HTTP server\", \"addr\", addr)\n\treturn s.rpcServer.ListenAndServe()\n}\n\nfunc (s *Server) WSListenAndServe(host string, port int) error {\n\thdlr := mux.NewRouter()\n\thdlr.HandleFunc(\"/\", s.HandleWS)\n\thdlr.HandleFunc(\"/{authorization}\", s.HandleWS)\n\tc := cors.New(cors.Options{\n\t\tAllowedOrigins: []string{\"*\"},\n\t})\n\taddr := fmt.Sprintf(\"%s:%d\", host, port)\n\ts.wsServer = &http.Server{\n\t\tHandler: instrumentedHdlr(c.Handler(hdlr)),\n\t\tAddr:    addr,\n\t}\n\tlog.Info(\"starting WS server\", \"addr\", addr)\n\treturn s.wsServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown() {\n\tif s.rpcServer != nil {\n\t\ts.rpcServer.Shutdown(context.Background())\n\t}\n\tif s.wsServer != nil {\n\t\ts.wsServer.Shutdown(context.Background())\n\t}\n}\n\nfunc (s *Server) HandleHealthz(w http.ResponseWriter, r *http.Request) {\n\tw.Write([]byte(\"OK\"))\n}\n\nfunc (s *Server) HandleRPC(w http.ResponseWriter, r *http.Request) {\n\tctx := s.populateContext(w, r)\n\tif ctx == nil {\n\t\treturn\n\t}\n\n\tlog.Info(\n\t\t\"received RPC request\",\n\t\t\"req_id\", GetReqID(ctx),\n\t\t\"auth\", GetAuthCtx(ctx),\n\t\t\"user_agent\", r.Header.Get(\"user-agent\"),\n\t)\n\n\tbodyReader := &recordLenReader{Reader: io.LimitReader(r.Body, s.maxBodySize)}\n\treq, err := ParseRPCReq(bodyReader)\n\tif err != nil {\n\t\tlog.Info(\"rejected request with bad rpc request\", \"source\", \"rpc\", \"err\", err)\n\t\tRecordRPCError(ctx, BackendProxyd, MethodUnknown, err)\n\t\twriteRPCError(ctx, w, nil, err)\n\t\treturn\n\t}\n\tRecordRequestPayloadSize(ctx, req.Method, bodyReader.Len)\n\n\tgroup := s.rpcMethodMappings[req.Method]\n\tif group == \"\" {\n\t\t// use unknown below to prevent DOS vector that fills up memory\n\t\t// with arbitrary method names.\n\t\tlog.Info(\n\t\t\t\"blocked request for non-whitelisted method\",\n\t\t\t\"source\", \"rpc\",\n\t\t\t\"req_id\", GetReqID(ctx),\n\t\t\t\"method\", req.Method,\n\t\t)\n\t\tRecordRPCError(ctx, BackendProxyd, MethodUnknown, ErrMethodNotWhitelisted)\n\t\twriteRPCError(ctx, w, req.ID, ErrMethodNotWhitelisted)\n\t\treturn\n\t}\n\n\tvar backendRes *RPCRes\n\tbackendRes, err = s.cache.GetRPC(ctx, req)\n\tif err == nil && backendRes != nil {\n\t\twriteRPCRes(ctx, w, backendRes)\n\t\treturn\n\t}\n\tif err != nil {\n\t\tlog.Warn(\n\t\t\t\"cache lookup error\",\n\t\t\t\"req_id\", GetReqID(ctx),\n\t\t\t\"err\", err,\n\t\t)\n\t}\n\n\tbackendRes, err = s.backendGroups[group].Forward(ctx, req)\n\tif err != nil {\n\t\tlog.Error(\n\t\t\t\"error forwarding RPC request\",\n\t\t\t\"method\", req.Method,\n\t\t\t\"req_id\", GetReqID(ctx),\n\t\t\t\"err\", err,\n\t\t)\n\t\twriteRPCError(ctx, w, req.ID, err)\n\t\treturn\n\t}\n\n\tif backendRes.Error == nil {\n\t\tif err = s.cache.PutRPC(ctx, req, backendRes); err != nil {\n\t\t\tlog.Warn(\n\t\t\t\t\"cache put error\",\n\t\t\t\t\"req_id\", GetReqID(ctx),\n\t\t\t\t\"err\", err,\n\t\t\t)\n\t\t}\n\t}\n\n\twriteRPCRes(ctx, w, backendRes)\n}\n\nfunc (s *Server) HandleWS(w http.ResponseWriter, r *http.Request) {\n\tctx := s.populateContext(w, r)\n\tif ctx == nil {\n\t\treturn\n\t}\n\n\tlog.Info(\"received WS connection\", \"req_id\", GetReqID(ctx))\n\n\tclientConn, err := s.upgrader.Upgrade(w, r, nil)\n\tif err != nil {\n\t\tlog.Error(\"error upgrading client conn\", \"auth\", GetAuthCtx(ctx), \"req_id\", GetReqID(ctx), \"err\", err)\n\t\treturn\n\t}\n\n\tproxier, err := s.wsBackendGroup.ProxyWS(ctx, clientConn, s.wsMethodWhitelist)\n\tif err != nil {\n\t\tif errors.Is(err, ErrNoBackends) {\n\t\t\tRecordUnserviceableRequest(ctx, RPCRequestSourceWS)\n\t\t}\n\t\tlog.Error(\"error dialing ws backend\", \"auth\", GetAuthCtx(ctx), \"req_id\", GetReqID(ctx), \"err\", err)\n\t\tclientConn.Close()\n\t\treturn\n\t}\n\n\tactiveClientWsConnsGauge.WithLabelValues(GetAuthCtx(ctx)).Inc()\n\tgo func() {\n\t\t// Below call blocks so run it in a goroutine.\n\t\tif err := proxier.Proxy(ctx); err != nil {\n\t\t\tlog.Error(\"error proxying websocket\", \"auth\", GetAuthCtx(ctx), \"req_id\", GetReqID(ctx), \"err\", err)\n\t\t}\n\t\tactiveClientWsConnsGauge.WithLabelValues(GetAuthCtx(ctx)).Dec()\n\t}()\n\n\tlog.Info(\"accepted WS connection\", \"auth\", GetAuthCtx(ctx), \"req_id\", GetReqID(ctx))\n}\n\nfunc (s *Server) populateContext(w http.ResponseWriter, r *http.Request) context.Context {\n\tvars := mux.Vars(r)\n\tauthorization := vars[\"authorization\"]\n\n\tif s.authenticatedPaths == nil {\n\t\t// handle the edge case where auth is disabled\n\t\t// but someone sends in an auth key anyway\n\t\tif authorization != \"\" {\n\t\t\tlog.Info(\"blocked authenticated request against unauthenticated proxy\")\n\t\t\thttpResponseCodesTotal.WithLabelValues(\"404\").Inc()\n\t\t\tw.WriteHeader(404)\n\t\t\treturn nil\n\t\t}\n\t\treturn context.WithValue(\n\t\t\tr.Context(),\n\t\t\tContextKeyReqID,\n\t\t\trandStr(10),\n\t\t)\n\t}\n\n\tif authorization == \"\" || s.authenticatedPaths[authorization] == \"\" {\n\t\tlog.Info(\"blocked unauthorized request\", \"authorization\", authorization)\n\t\thttpResponseCodesTotal.WithLabelValues(\"401\").Inc()\n\t\tw.WriteHeader(401)\n\t\treturn nil\n\t}\n\n\txff := r.Header.Get(\"X-Forwarded-For\")\n\tif xff == \"\" {\n\t\tipPort := strings.Split(r.RemoteAddr, \":\")\n\t\tif len(ipPort) == 2 {\n\t\t\txff = ipPort[0]\n\t\t}\n\t}\n\n\tctx := context.WithValue(r.Context(), ContextKeyAuth, s.authenticatedPaths[authorization])\n\tctx = context.WithValue(ctx, ContextKeyXForwardedFor, xff)\n\treturn context.WithValue(\n\t\tctx,\n\t\tContextKeyReqID,\n\t\trandStr(10),\n\t)\n}\n\nfunc writeRPCError(ctx context.Context, w http.ResponseWriter, id json.RawMessage, err error) {\n\tvar res *RPCRes\n\tif r, ok := err.(*RPCErr); ok {\n\t\tres = NewRPCErrorRes(id, r)\n\t} else {\n\t\tres = NewRPCErrorRes(id, ErrInternal)\n\t}\n\twriteRPCRes(ctx, w, res)\n}\n\nfunc writeRPCRes(ctx context.Context, w http.ResponseWriter, res *RPCRes) {\n\tstatusCode := 200\n\tif res.IsError() && res.Error.HTTPErrorCode != 0 {\n\t\tstatusCode = res.Error.HTTPErrorCode\n\t}\n\n\tw.WriteHeader(statusCode)\n\tww := &recordLenWriter{Writer: w}\n\tenc := json.NewEncoder(ww)\n\tif err := enc.Encode(res); err != nil {\n\t\tlog.Error(\"error writing rpc response\", \"err\", err)\n\t\tRecordRPCError(ctx, BackendProxyd, MethodUnknown, err)\n\t\treturn\n\t}\n\thttpResponseCodesTotal.WithLabelValues(strconv.Itoa(statusCode)).Inc()\n\tRecordResponsePayloadSize(ctx, ww.Len)\n}\n\nfunc instrumentedHdlr(h http.Handler) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\trespTimer := prometheus.NewTimer(httpRequestDurationSumm)\n\t\th.ServeHTTP(w, r)\n\t\trespTimer.ObserveDuration()\n\t}\n}\n\nfunc GetAuthCtx(ctx context.Context) string {\n\tauthUser, ok := ctx.Value(ContextKeyAuth).(string)\n\tif !ok {\n\t\treturn \"none\"\n\t}\n\n\treturn authUser\n}\n\nfunc GetReqID(ctx context.Context) string {\n\treqId, ok := ctx.Value(ContextKeyReqID).(string)\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn reqId\n}\n\nfunc GetXForwardedFor(ctx context.Context) string {\n\txff, ok := ctx.Value(ContextKeyXForwardedFor).(string)\n\tif !ok {\n\t\treturn \"\"\n\t}\n\treturn xff\n}\n\ntype recordLenReader struct {\n\tio.Reader\n\tLen int\n}\n\nfunc (r *recordLenReader) Read(p []byte) (n int, err error) {\n\tn, err = r.Reader.Read(p)\n\tr.Len += n\n\treturn\n}\n\ntype recordLenWriter struct {\n\tio.Writer\n\tLen int\n}\n\nfunc (w *recordLenWriter) Write(p []byte) (n int, err error) {\n\tn, err = w.Writer.Write(p)\n\tw.Len += n\n\treturn\n}\n\ntype NoopRPCCache struct{}\n\nfunc (n *NoopRPCCache) GetRPC(context.Context, *RPCReq) (*RPCRes, error) {\n\treturn nil, nil\n}\n\nfunc (n *NoopRPCCache) PutRPC(context.Context, *RPCReq, *RPCRes) error {\n\treturn nil\n}\n", "idx": 7, "id": 21696, "msg": "", "proj": "ethereum-optimism-optimism", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -355,16 +355,6 @@ namespace OpenTelemetry.Trace\n         /// <inheritdoc/>\n         public void SetAttribute(string key, string value)\n         {\n-            if (key == null)\n-            {\n-                throw new ArgumentNullException(nameof(key));\n-            }\n-\n-            if (key == null)\n-            {\n-                throw new ArgumentNullException(nameof(key));\n-            }\n-\n             if (!this.IsRecordingEvents)\n             {\n                 return;", "y": 0, "oldf": "\ufeff// <copyright file=\"Span.cs\" company=\"OpenTelemetry Authors\">\n// Copyright 2018, OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n// </copyright>\nusing System;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.Linq;\nusing OpenTelemetry.Context.Propagation;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace.Configuration;\nusing OpenTelemetry.Trace.Export;\nusing OpenTelemetry.Trace.Internal;\nusing OpenTelemetry.Utils;\n\nnamespace OpenTelemetry.Trace\n{\n    /// <summary>\n    /// Span implementation.\n    /// </summary>\n    public sealed class Span : ISpan\n    {\n        private readonly TracerConfiguration tracerConfiguration;\n        private readonly SpanProcessor spanProcessor;\n        private readonly DateTimeOffset startTimestamp;\n        private readonly object @lock = new object();\n        private EvictingQueue<KeyValuePair<string, object>> attributes;\n        private EvictingQueue<Event> events;\n        private EvictingQueue<Link> links;\n        private Status status;\n        private DateTimeOffset endTimestamp;\n        private bool hasEnded;\n\n        private Span(\n            string name,\n            SpanContext parentSpanContext,\n            ActivityAndTracestate activityAndTracestate,\n            bool ownsActivity,\n            SpanKind spanKind,\n            DateTimeOffset startTimestamp,\n            IEnumerable<Link> links,\n            TracerConfiguration tracerConfiguration,\n            SpanProcessor spanProcessor,\n            Resource libraryResource)\n        {\n            this.Name = name;\n            this.LibraryResource = libraryResource;\n            this.startTimestamp = startTimestamp;\n            this.tracerConfiguration = tracerConfiguration;\n            this.spanProcessor = spanProcessor;\n            this.Kind = spanKind;\n            this.OwnsActivity = ownsActivity;\n            this.Activity = activityAndTracestate.Activity;\n\n            var tracestate = activityAndTracestate.Tracestate;\n\n            this.IsRecordingEvents = MakeSamplingDecision(\n                parentSpanContext,\n                name,\n                null,\n                links, // we'll enumerate again, but double enumeration over small collection is cheaper than allocation\n                this.Activity.TraceId,\n                this.Activity.SpanId,\n                this.tracerConfiguration);\n\n            if (this.IsRecordingEvents)\n            {\n                this.Activity.ActivityTraceFlags |= ActivityTraceFlags.Recorded;\n                \n                if (links != null)\n                {\n                    foreach (var link in links)\n                    {\n                        this.AddLink(link);\n                    }\n                }\n\n                this.spanProcessor.OnStart(this);\n            }\n            else\n            {\n                this.Activity.ActivityTraceFlags &= ~ActivityTraceFlags.Recorded;\n            }\n\n            this.Context = new SpanContext(this.Activity.TraceId, this.Activity.SpanId, this.Activity.ActivityTraceFlags, tracestate);\n        }\n\n        public SpanContext Context { get; private set; }\n\n        public string Name { get; private set; }\n\n        /// <inheritdoc/>\n        public Status Status\n        {\n            get\n            {\n                lock (this.@lock)\n                {\n                    return this.StatusWithDefault;\n                }\n            }\n\n            set\n            {\n                if (!this.IsRecordingEvents)\n                {\n                    return;\n                }\n\n                lock (this.@lock)\n                {\n                    this.status = value.IsValid ? value : throw new ArgumentException(nameof(value));\n                }\n            }\n        }\n\n        public ActivitySpanId ParentSpanId => this.Activity.ParentSpanId;\n\n        /// <inheritdoc/>\n        public bool IsRecordingEvents { get; }\n\n        /// <summary>\n        /// Gets attributes.\n        /// </summary>\n        public IEnumerable<KeyValuePair<string, object>> Attributes => this.attributes ?? Enumerable.Empty<KeyValuePair<string, object>>();\n\n        /// <summary>\n        /// Gets events.\n        /// </summary>\n        public IEnumerable<Event> Events => this.events ?? Enumerable.Empty<Event>();\n\n        /// <summary>\n        /// Gets links.\n        /// </summary>\n        public IEnumerable<Link> Links => this.links ?? Enumerable.Empty<Link>();\n\n        /// <summary>\n        /// Gets span start timestamp.\n        /// </summary>\n        public DateTimeOffset StartTimestamp => this.startTimestamp;\n\n        /// <summary>\n        /// Gets span end timestamp.\n        /// </summary>\n        public DateTimeOffset EndTimestamp => this.endTimestamp;\n\n        /// <summary>\n        /// Gets the span kind.\n        /// </summary>\n        public SpanKind? Kind { get; }\n\n        /// <summary>\n        /// Gets the \"Library Resource\" (name + version) associated with the Tracer that produced this span.\n        /// </summary>\n        public Resource LibraryResource { get; }\n        \n        internal bool OwnsActivity { get; }\n\n        internal Activity Activity { get; }\n\n        private Status StatusWithDefault => this.status.IsValid ? this.status : Status.Ok;\n\n        /// <inheritdoc />\n        public void UpdateName(string name)\n        {\n            this.Name = name ?? throw new ArgumentNullException(nameof(name));\n        }\n\n        /// <inheritdoc/>\n        public void SetAttribute(KeyValuePair<string, object> keyValuePair)\n        {\n            if (keyValuePair.Key == null || keyValuePair.Value == null)\n            {\n                throw new ArgumentNullException(nameof(keyValuePair));\n            }\n\n            if (!this.IsRecordingEvents || this.hasEnded)\n            {\n                return;\n            }\n\n            lock (this.@lock)\n            {\n                if (this.attributes == null)\n                {\n                    this.attributes = new EvictingQueue<KeyValuePair<string, object>>(this.tracerConfiguration.MaxNumberOfAttributes);\n                }\n\n                this.attributes.AddEvent(new KeyValuePair<string, object>(keyValuePair.Key, keyValuePair.Value));\n            }\n        }\n\n        /// <inheritdoc/>\n        public void AddEvent(string name)\n        {\n            if (name == null)\n            {\n                throw new ArgumentNullException(nameof(name));\n            }\n\n            if (!this.IsRecordingEvents || this.hasEnded)\n            {\n                return;\n            }\n\n            lock (this.@lock)\n            {\n                if (this.events == null)\n                {\n                    this.events =\n                        new EvictingQueue<Event>(this.tracerConfiguration.MaxNumberOfEvents);\n                }\n\n                this.events.AddEvent(new Event(name, PreciseTimestamp.GetUtcNow()));\n            }\n        }\n\n        /// <inheritdoc/>\n        public void AddEvent(string name, IDictionary<string, object> eventAttributes)\n        {\n            if (name == null)\n            {\n                throw new ArgumentNullException(nameof(name));\n            }\n\n            if (eventAttributes == null)\n            {\n                throw new ArgumentNullException(nameof(eventAttributes));\n            }\n\n            if (!this.IsRecordingEvents || this.hasEnded)\n            {\n                return;\n            }\n\n            lock (this.@lock)\n            {\n                if (this.hasEnded)\n                {\n                    // logger.log(Level.FINE, \"Calling AddEvent() on an ended Span.\");\n                    return;\n                }\n\n                if (this.events == null)\n                {\n                    this.events =\n                        new EvictingQueue<Event>(this.tracerConfiguration.MaxNumberOfEvents);\n                }\n\n                this.events.AddEvent(new Event(name, PreciseTimestamp.GetUtcNow(), eventAttributes));\n            }\n        }\n\n        /// <inheritdoc/>\n        public void AddEvent(Event addEvent)\n        {\n            if (addEvent == null)\n            {\n                throw new ArgumentNullException(nameof(addEvent));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            lock (this.@lock)\n            {\n                if (this.hasEnded)\n                {\n                    // logger.log(Level.FINE, \"Calling AddEvent() on an ended Span.\");\n                    return;\n                }\n\n                if (this.events == null)\n                {\n                    this.events =\n                        new EvictingQueue<Event>(this.tracerConfiguration.MaxNumberOfEvents);\n                }\n\n                this.events.AddEvent(addEvent);\n            }\n        }\n\n        /// <inheritdoc/>\n        public void AddLink(Link link)\n        {\n            if (link == null)\n            {\n                throw new ArgumentNullException(nameof(link));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            lock (this.@lock)\n            {\n                if (this.hasEnded)\n                {\n                    // logger.log(Level.FINE, \"Calling addLink() on an ended Span.\");\n                    return;\n                }\n\n                if (this.links == null)\n                {\n                    this.links = new EvictingQueue<Link>(this.tracerConfiguration.MaxNumberOfLinks);\n                }\n\n                this.links.AddEvent(link);\n            }\n        }\n\n        /// <inheritdoc/>\n        public void End()\n        {\n            this.End(PreciseTimestamp.GetUtcNow());\n        }\n\n        public void End(DateTimeOffset endTimestamp)\n        {\n            if (this.hasEnded)\n            {\n                return;\n            }\n\n            this.hasEnded = true;\n            this.endTimestamp = endTimestamp;\n            if (this.OwnsActivity && this.Activity == Activity.Current)\n            {\n                // TODO log if current is not span activity\n                this.Activity.Stop();\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            this.spanProcessor.OnEnd(this);\n        }\n\n        /// <inheritdoc/>\n        public void SetAttribute(string key, string value)\n        {\n            if (key == null)\n            {\n                throw new ArgumentNullException(nameof(key));\n            }\n\n            if (key == null)\n            {\n                throw new ArgumentNullException(nameof(key));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            this.SetAttribute(new KeyValuePair<string, object>(key, value));\n        }\n\n        /// <inheritdoc/>\n        public void SetAttribute(string key, long value)\n        {\n            if (key == null)\n            {\n                throw new ArgumentNullException(nameof(key));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            this.SetAttribute(new KeyValuePair<string, object>(key, value));\n        }\n\n        /// <inheritdoc/>\n        public void SetAttribute(string key, double value)\n        {\n            if (key == null)\n            {\n                throw new ArgumentNullException(nameof(key));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            this.SetAttribute(new KeyValuePair<string, object>(key, value));\n        }\n\n        /// <inheritdoc/>\n        public void SetAttribute(string key, bool value)\n        {\n            if (key == null)\n            {\n                throw new ArgumentNullException(nameof(key));\n            }\n\n            if (!this.IsRecordingEvents)\n            {\n                return;\n            }\n\n            this.SetAttribute(new KeyValuePair<string, object>(key, value));\n        }\n\n        internal static Span CreateFromParentSpan(\n            string name,\n            ISpan parentSpan,\n            SpanKind spanKind,\n            DateTimeOffset startTimestamp,\n            IEnumerable<Link> links,\n            TracerConfiguration tracerConfiguration,\n            SpanProcessor spanProcessor,\n            Resource libraryResource)\n        {\n            if (parentSpan.Context.IsValid)\n            {\n                return new Span(\n                    name,\n                    parentSpan.Context,\n                    FromParentSpan(name, parentSpan),\n                    true,\n                    spanKind,\n                    startTimestamp,\n                    links,\n                    tracerConfiguration,\n                    spanProcessor,\n                    libraryResource);\n            }\n\n            var currentActivity = Activity.Current;\n            if (currentActivity == null)\n            {\n                return new Span(\n                    name,\n                    SpanContext.Blank,\n                    CreateRoot(name),\n                    true,\n                    spanKind,\n                    startTimestamp,\n                    links,\n                    tracerConfiguration,\n                    spanProcessor,\n                    libraryResource);\n            }\n\n            return new Span(\n                name,\n                new SpanContext(\n                    currentActivity.TraceId,\n                    currentActivity.SpanId,\n                    currentActivity.ActivityTraceFlags),\n                FromCurrentParentActivity(name, currentActivity),\n                true,\n                spanKind,\n                startTimestamp,\n                links,\n                tracerConfiguration,\n                spanProcessor,\n                libraryResource);\n        }\n\n        internal static Span CreateFromParentContext(\n            string name,\n            SpanContext parentContext,\n            SpanKind spanKind,\n            DateTimeOffset startTimestamp,\n            IEnumerable<Link> links,\n            TracerConfiguration tracerConfiguration,\n            SpanProcessor spanProcessor,\n            Resource libraryResource)\n        {\n            return new Span(\n                name,\n                parentContext,\n                FromParentSpanContext(name, parentContext),\n                true,\n                spanKind,\n                startTimestamp,\n                links,\n                tracerConfiguration,\n                spanProcessor,\n                libraryResource);\n        }\n\n        internal static Span CreateRoot(\n            string name,\n            SpanKind spanKind,\n            DateTimeOffset startTimestamp,\n            IEnumerable<Link> links,\n            TracerConfiguration tracerConfiguration,\n            SpanProcessor spanProcessor,\n            Resource libraryResource)\n        {\n            return new Span(\n                name,\n                SpanContext.Blank,\n                CreateRoot(name),\n                true,\n                spanKind,\n                startTimestamp,\n                links,\n                tracerConfiguration,\n                spanProcessor,\n                libraryResource);\n        }\n\n        internal static Span CreateFromActivity(\n            string name,\n            Activity activity,\n            SpanKind spanKind,\n            IEnumerable<Link> links,\n            TracerConfiguration tracerConfiguration,\n            SpanProcessor spanProcessor,\n            Resource libraryResource)\n        {\n            return new Span(\n                name,\n                ParentContextFromActivity(activity),\n                FromActivity(name, activity),\n                false,\n                spanKind,\n                new DateTimeOffset(activity.StartTimeUtc),\n                links,\n                tracerConfiguration,\n                spanProcessor,\n                libraryResource);\n        }\n\n        private static bool MakeSamplingDecision(\n            SpanContext parent,\n            string name,\n            ISampler sampler,\n            IEnumerable<Link> parentLinks,\n            ActivityTraceId traceId,\n            ActivitySpanId spanId,\n            TracerConfiguration tracerConfiguration)\n        {\n            // If users set a specific sampler in the SpanBuilder, use it.\n            if (sampler != null)\n            {\n                return sampler.ShouldSample(parent, traceId, spanId, name, parentLinks).IsSampled;\n            }\n\n            // Use the default sampler if this is a root Span or this is an entry point Span (has remote\n            // parent).\n            if (parent == null || !parent.IsValid)\n            {\n                return tracerConfiguration\n                    .Sampler\n                    .ShouldSample(parent, traceId, spanId, name, parentLinks).IsSampled;\n            }\n\n            // Parent is always different than null because otherwise we use the default sampler.\n            return (parent.TraceOptions & ActivityTraceFlags.Recorded) != 0 || IsAnyParentLinkSampled(parentLinks);\n        }\n\n        private static bool IsAnyParentLinkSampled(IEnumerable<Link> parentLinks)\n        {\n            if (parentLinks != null)\n            {\n                foreach (var parentLink in parentLinks)\n                {\n                    if ((parentLink.Context.TraceOptions & ActivityTraceFlags.Recorded) != 0)\n                    {\n                        return true;\n                    }\n                }\n            }\n\n            return false;\n        }\n\n        private static ActivityAndTracestate FromCurrentParentActivity(string spanName, Activity current)\n        {\n            var activity = new Activity(spanName);\n            activity.SetIdFormat(ActivityIdFormat.W3C);\n\n            activity.Start();\n            Activity.Current = current;\n\n            List<KeyValuePair<string, string>> tracestate = null;\n            if (activity.TraceStateString != null)\n            {\n                tracestate = new List<KeyValuePair<string, string>>();\n                TracestateUtils.AppendTracestate(activity.TraceStateString, tracestate);\n            }\n\n            return new ActivityAndTracestate(activity, tracestate);\n        }\n\n        private static ActivityAndTracestate FromParentSpan(string spanName, ISpan parentSpan)\n        {\n            if (parentSpan is Span parentSpanImpl && parentSpanImpl.Activity == Activity.Current)\n            {\n                var activity = new Activity(spanName);\n                activity.SetIdFormat(ActivityIdFormat.W3C);\n                activity.TraceStateString = parentSpanImpl.Activity.TraceStateString;\n\n                var originalActivity = Activity.Current;\n                activity.Start();\n                Activity.Current = originalActivity;\n\n                return new ActivityAndTracestate(activity, parentSpan.Context.Tracestate);\n            }\n\n            return FromParentSpanContext(spanName, parentSpan.Context);\n        }\n\n        private static ActivityAndTracestate FromParentSpanContext(string spanName, SpanContext parentContext)\n        {\n            var activity = new Activity(spanName);\n\n            IEnumerable<KeyValuePair<string, string>> tracestate = null;\n            if (parentContext != null && parentContext.IsValid)\n            {\n                activity.SetParentId(parentContext.TraceId,\n                    parentContext.SpanId,\n                    parentContext.TraceOptions);\n                if (parentContext.Tracestate != null && parentContext.Tracestate.Any())\n                {\n                    activity.TraceStateString = TracestateUtils.GetString(parentContext.Tracestate);\n                    tracestate = parentContext.Tracestate;\n                }\n            }\n\n            activity.SetIdFormat(ActivityIdFormat.W3C);\n\n            var originalActivity = Activity.Current;\n            activity.Start();\n            Activity.Current = originalActivity;\n\n            return new ActivityAndTracestate(activity, tracestate);\n        }\n\n        private static ActivityAndTracestate CreateRoot(string spanName)\n        {\n            var activity = new Activity(spanName);\n            activity.SetIdFormat(ActivityIdFormat.W3C);\n\n            var originalActivity = Activity.Current;\n            if (originalActivity != null)\n            {\n                activity.SetParentId(\" \");\n            }\n\n            activity.Start();\n            Activity.Current = originalActivity;\n\n            return new ActivityAndTracestate(activity, null);\n        }\n\n        private static ActivityAndTracestate FromActivity(string spanName, Activity activity)\n        {\n            List<KeyValuePair<string, string>> tracestate = null;\n            if (activity.TraceStateString != null)\n            {\n                tracestate = new List<KeyValuePair<string, string>>();\n                TracestateUtils.AppendTracestate(activity.TraceStateString, tracestate);\n            }\n\n            return new ActivityAndTracestate(activity, tracestate);\n        }\n\n        private static SpanContext ParentContextFromActivity(Activity activity)\n        {\n            if (activity.TraceId != default && activity.ParentSpanId != default)\n            {\n                return new SpanContext(\n                    activity.TraceId,\n                    activity.ParentSpanId,\n                    activity.ActivityTraceFlags);\n            }\n\n            return null;\n        }\n\n        private readonly struct ActivityAndTracestate\n        {\n            public readonly Activity Activity;\n            public readonly IEnumerable<KeyValuePair<string, string>> Tracestate;\n\n            public ActivityAndTracestate(Activity activity, IEnumerable<KeyValuePair<string, string>> tracestate)\n            {\n                this.Activity = activity;\n                this.Tracestate = tracestate;\n            }\n        }\n    }\n}\n", "idx": 8, "id": 12478, "msg": "", "proj": "open-telemetry-opentelemetry-dotnet", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -274,15 +274,19 @@ class SwigInputGenerator(object):\n         return t.decl_string\n \n     def renameTypesInSTL(self, s):\n-        if s.startswith(\"std::\") and \\\n-                pygccxml.declarations.templates.is_instantiation(s):\n+        if s.startswith(\"std::\") and pygccxml.declarations.templates.is_instantiation(\n+            s\n+        ):\n             args = []\n             for arg in pygccxml.declarations.templates.args(s):\n                 t, d = SwigInputGenerator.typeAndDecorators(arg)\n                 args.append(self.renameTypesInSTL(self.get_alias(t)) + d)\n-            return pygccxml.declarations.templates.join(\n-                pygccxml.declarations.templates.name(s),\n-                args) + SwigInputGenerator.typeAndDecorators(s)[1]\n+            return (\n+                pygccxml.declarations.templates.join(\n+                    pygccxml.declarations.templates.name(s), args\n+                )\n+                + SwigInputGenerator.typeAndDecorators(s)[1]\n+            )\n         return s\n \n     @staticmethod", "y": 0, "oldf": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport re\nfrom argparse import ArgumentParser\nfrom io import StringIO\n\n\ndef getType(v):\n    if hasattr(v, \"decl_type\"):\n        return getType(v.decl_type)\n    if hasattr(v, \"declaration\"):\n        return getType(v.declaration)\n    return v\n\n\nclass IdxGenerator(object):\n    \"\"\"Generates a the .idx file for an ITK wrapping submodule (which usually\n    corresponds to a class).\"\"\"\n\n    def __init__(self, moduleName):\n        self.moduleName = moduleName\n        # the output file\n        self.outputFile = StringIO()\n\n    def create_idxfile(self, idxFilePath, wrappersNamespace):\n        # iterate over all the typedefs in the _wrapping_::wrappers namespace\n        for typedef in wrappersNamespace.typedefs():\n            n = typedef.name\n            s = getType(typedef).decl_string\n            # drop the :: prefix - it make swig produce invalid code\n            if s.startswith(\"::\"):\n                s = s[2:]\n            self.outputFile.write(\"{%s} {%s} {%s}\\n\" % (s, n, self.moduleName))\n\n        content = self.outputFile.getvalue()\n\n        with open(idxFilePath, \"w\") as f:\n            f.write(content)\n\n\nclass SwigInputGenerator(object):\n    \"\"\"Generates a swig input .i file for an ITK module.\"\"\"\n\n    notWrapped = [\n        \"std::_Deque_alloc<.+>\",\n        \"itk::AtomicInt<.+>\",\n        \"itk::MapContainer< unsigned long, itk::CellInterface<.+>\",\n        \"itk::VectorContainer< unsigned long, itk::CellInterface<.+>\",\n        \"itk::CellInterface< double, itk::QuadEdgeMeshCellTraitsInfo<.+>\",\n        \"itk::QuadEdgeMeshLineCell< itk::CellInterface<.+>\",\n        \"itk::LibHandle\",\n        \"itk::NeighborhoodAllocator<.+>\",\n        # to avoid wrapping all the region for all the dims\n        \"itk::ImageRegion<.+>\",\n        \"itk::ImportImageContainer<.+>\",\n        \"itk::DefaultPixelAccessor<.+>\",\n        \"itk::NeighborhoodAccessorFunctor<.+>\",\n        \"itk::DefaultVectorPixelAccessor<.+>\",\n        \"itk::VectorImageNeighborhoodAccessorFunctor<.+>\",\n        \"itk::.*Iterator.*\",  # TODO: remove this one ?\n        \"itk::Neighborhood<.+>\",  # TODO: remove this one\n        \"itk::ThreadFunctionType\",\n        \"itk::Functor::.+\",\n        \"itk::SmartPointer< itk::Functor::.+\",\n        \"itk::Function::.+\",\n        \"itk::.+Function.*\",  # Level set functions\n        \"itk::watershed::.+\",  # ignore the internal classes of the watershed\n        # require to wrap too more type\n        \"itk::SmartPointer< itk::VoronoiDiagram2D<.+> >\",\n        # used internally in ImageToImageMetric\n        \"itk::Image< itk::CovariantVector< double, \\d+u >, \\d+u >\",\n        \"itk::FixedArray< itk::SmartPointer.+ >\",\n        # used internally in itkTransformBase\n        \"itk::SmartPointer< itk::Transform.+ >\",\n        # used internally in itkMattesMutualInformationImageToImageMetric\n        \"itk::SmartPointer< itk::Image.+ >\",\n        \"itk::ObjectFactoryBasePrivate\",\n        \"itk::ThreadPoolGlobals\",\n        \"itk::MultiThreaderBaseGlobals\",\n\n        \".+[(][*][)][(].+\" # functor functions\n    ]\n\n    forceSnakeCase = [\n      \"ImageDuplicator\"\n    ]\n\n    notWrappedRegExp = re.compile(\"|\".join([\"^\" + s + \"$\" for s in notWrapped]))\n\n    # stdcomplex code\n\n    stdcomplex_headers = {\n        \"D\": \"\"\" class stdcomplexD {\n       public:\n         ~stdcomplexD();\n         stdcomplexD & operator=(stdcomplexD const & arg0);\n         stdcomplexD(stdcomplexD const & arg0);\n         stdcomplexD(stdcomplexD __z);\n         stdcomplexD(double __r = 0.0, double __i = 0.0);\n         stdcomplexD(stdcomplexF const & __z);\n         double real();\n         double const real() const;\n         double imag();\n         double const imag() const;\n         stdcomplexD & operator=(double __d);\n         stdcomplexD & operator+=(double __d);\n         stdcomplexD & operator-=(double __d);\n         stdcomplexD & operator*=(double __d);\n         stdcomplexD & operator/=(double __d);\n         // stdcomplexD const & __rep() const;\n       private:\n       protected:\n     };\n    \"\"\",\n\n        \"F\": \"\"\"class stdcomplexF {\n       public:\n         ~stdcomplexF();\n         stdcomplexF & operator=(stdcomplexF const & arg0);\n         stdcomplexF(stdcomplexF const & arg0);\n         stdcomplexF(stdcomplexF __z);\n         stdcomplexF(float r = 0.0f, float i = 0.0f);\n         stdcomplexF(stdcomplexD const & __z);\n         float real();\n         float const real() const;\n         float imag();\n         float const imag() const;\n         stdcomplexF & operator=(float __f);\n         stdcomplexF & operator+=(float __f);\n         stdcomplexF & operator-=(float __f);\n         stdcomplexF & operator*=(float __f);\n         stdcomplexF & operator/=(float __f);\n         // stdcomplexF const & __rep() const;\n       private:\n       protected:\n     };\n    \"\"\"}\n\n    new_override = '''\n// some changes in the New() method\n%rename(__New_orig__) {class_name}::New;\n%extend {class_name} {{\n%pythoncode %{{\n    def New(*args, **kargs):\n        \"\"\"New() -> {class_name}\n\n        Create a new object of the class {class_name} and set the input and the parameters if some\n        named or non-named arguments are passed to that method.\n\n        New() tries to assign all the non named parameters to the input of the new objects - the\n        first non named parameter in the first input, etc.\n\n        The named parameters are used by calling the method with the same name prefixed by 'Set'.\n\n        Ex:\n\n          {class_name}.New(reader, threshold=10)\n\n        is (most of the time) equivalent to:\n\n          obj = {class_name}.New()\n          obj.SetInput(0, reader.GetOutput())\n          obj.SetThreshold(10)\n        \"\"\"\n        obj = {class_name}.__New_orig__()\n        import itkTemplate\n        itkTemplate.New(obj, *args, **kargs)\n        return obj\n    New = staticmethod(New)\n  %}}\n}}\n%pythoncode %{{\n    def {class_name}_New():\n        return {class_name}.New()\n%}}\n\n\n'''\n\n    new_override_pycommand = '''\n// some changes in the New() method\n%rename(__New_orig__) {class_name}::New;\n%extend {class_name} {{\n%pythoncode %{{\n    def New(*args, **kargs):\n        \"\"\"New() -> {class_name}\n        \"\"\"\n        obj = {class_name}.__New_orig__()\n        import itk\n        itk.set_inputs(obj, *args, **kargs)\n        return obj\n    New = staticmethod(New)\n%}}\n}}\n%pythoncode %{{\n    def {class_name}_New():\n        return {class_name}.New()\n%}}\n\n\n'''\n\n\n\n    def __init__(self, moduleName, options):\n        self.moduleName = moduleName\n        self.options = options\n\n        self.outputFile = StringIO()\n        self.applyFileNames = []\n\n        # a dict to let us use the alias name instead of the full c++ name. Without\n        # that, in many cases, swig don't know that's the same type\n        self.aliases = {}\n\n        # a set of used types\n        self.usedTypes = set()\n\n        # a dict to store the file where the def comes from\n        self.typedefSource = {}\n\n        self.warnings = set()\n\n        self.mdx_loaded = set()\n\n        self.verbose = options.verbose\n\n        self.snakeCaseProcessObjectFunctions = set()\n\n\n    def warn(self, identifier, msg, doWarn=True):\n        if not doWarn:\n            # don't warn for anything\n            return\n        if str(identifier) not in self.options.warnings:\n            if not self.verbose and (identifier, msg) in self.warnings:\n                # just do nothing\n                return\n            self.warnings.add((identifier, msg))\n            if self.verbose:\n                if self.options.warningError:\n                    print(\"error(%s): %s\" % (str(identifier), msg), file=sys.stderr)\n                else:\n                    print(\"warning(%s): %s\" % (str(identifier), msg), file=sys.stderr)\n            else:\n                if self.options.warningError:\n                    print(\n                        \"%s: error(%s): %s\" %\n                        (self.moduleName, str(identifier), msg), file=sys.stderr)\n                else:\n                    print(\n                        \"%s: warning(%s): %s\" %\n                        (self.moduleName, str(identifier), msg), file=sys.stderr)\n\n    def info(self, msg):\n        if self.verbose:\n            print(\"info: %s\" % msg, file=sys.stderr)\n\n    @staticmethod\n    def getDeclarationString(t):\n        t = getType(t)\n        if t.decl_string == \"::PyObject *\":\n            # don't go further - we want to keep that one as is\n            return \"::PyObject *\"\n        if isinstance(t, pygccxml.declarations.cpptypes.pointer_t):\n            return SwigInputGenerator.getDeclarationString(getType(t.base)) + \" *\"\n        elif isinstance(t, pygccxml.declarations.cpptypes.const_t):\n            return SwigInputGenerator.getDeclarationString(getType(t.base)) + \" const\"\n        elif isinstance(t, pygccxml.declarations.cpptypes.reference_t):\n            return SwigInputGenerator.getDeclarationString(getType(t.base)) + \" &\"\n        return t.decl_string\n\n    def renameTypesInSTL(self, s):\n        if s.startswith(\"std::\") and \\\n                pygccxml.declarations.templates.is_instantiation(s):\n            args = []\n            for arg in pygccxml.declarations.templates.args(s):\n                t, d = SwigInputGenerator.typeAndDecorators(arg)\n                args.append(self.renameTypesInSTL(self.get_alias(t)) + d)\n            return pygccxml.declarations.templates.join(\n                pygccxml.declarations.templates.name(s),\n                args) + SwigInputGenerator.typeAndDecorators(s)[1]\n        return s\n\n    @staticmethod\n    def removeStdAllocator(s):\n        if pygccxml.declarations.templates.is_instantiation(s):\n            args = []\n            for arg in pygccxml.declarations.templates.args(s):\n                if not arg.startswith(\"std::allocator\"):\n                    t, d = SwigInputGenerator.typeAndDecorators(arg)\n                    args.append(SwigInputGenerator.removeStdAllocator(t) + d)\n            return pygccxml.declarations.templates.join(\n                pygccxml.declarations.templates.name(s),\n                args) + SwigInputGenerator.typeAndDecorators(s)[1]\n        return s\n\n    @staticmethod\n    def typeAndDecorators(s):\n        end = \"\"\n        s = s.strip()\n        ends = [\" \", \"*\", \"&\", \"const\"]\n        needToContinue = True\n        while needToContinue:\n            needToContinue = False\n            for e in ends:\n                if s.endswith(e):\n                    end = e + end\n                    s = s[:-len(e)]\n                    needToContinue = True\n        return (s, end)\n\n    _firstCapRE = re.compile(r'(.)([A-Z][a-z]+)')\n    _allCapRE = re.compile('([a-z0-9])([A-Z])')\n    @staticmethod\n    def camelCaseToSnakeCase(camelCase):\n        substitution = SwigInputGenerator._firstCapRE.sub(r'\\1_\\2', camelCase)\n        return SwigInputGenerator._allCapRE.sub(r'\\1_\\2', substitution).lower().replace('__', '_')\n\n    def get_alias(self, decl_string, w=True):\n        s = str(decl_string)\n\n        # drop the :: prefix - it make swig produce invalid code\n        if s.startswith(\"::\"):\n            s = s[2:]\n\n        # normalize string\n        s = SwigInputGenerator.normalize(s)\n\n        # workaround a bug - or is it a feature ? - somewhere\n        s = s.replace(\"complex float\", \"std::complex<float>\")\n        s = s.replace(\"complex double\", \"std::complex<double>\")\n        s = s.replace(\"complex long double\", \"std::complex<long double>\")\n\n        (s, end) = SwigInputGenerator.typeAndDecorators(s)\n\n        if s in self.aliases:\n            self.usedTypes.add(self.aliases[s])\n            return self.aliases[s] + end\n\n        if s.startswith(\"itk::Templates::\"):\n            # that's a explicitly instantiated type. The name is the same than\n            # the WrapITK one, so lets use it as a base for WrapITK\n            # Ex: itk::Templates::RGBPixelUC\n            # don't store the new string in s, because we need it unchanged if\n            # the type is explicitly instantiated, but not wrapped\n            new_s = s.replace(\"::Templates::\", \"\")\n            if new_s.split(\"::\")[0] in self.aliases.values():\n                self.usedTypes.add(new_s)\n                return new_s + end\n\n        if s[:s.rfind(\"::\")] in self.aliases:\n            # take care of subtypes/enum/...\n            alias = self.aliases[s[:s.rfind(\"::\")]] + s[s.rfind(\"::\"):]\n            self.usedTypes.add(alias)\n            return alias + end\n\n        # replace the types defined in this type, to support\n        # std::vector<itkDataObject> for example\n        s = self.renameTypesInSTL(s)\n\n        # drop the allocator part of the type, because it is not supported by the\n        # %template directive with some generators (like tcl)\n        s = SwigInputGenerator.removeStdAllocator(s)\n\n        # rename basic_string to std::string to make name shorter\n        s = s.replace(\"std::basic_string< char >\", \"std::string\")\n        s = s.replace(\n            \"std::basic_string< char, std::char_traits< char > >\",\n            \"std::string\")\n        s = s.replace(\n            \"std::basic_ostream< char, std::char_traits< char > >\",\n            \"std::ostream\")\n        s = s.replace(\n            \"std::basic_istream< char, std::char_traits< char > >\",\n            \"std::istream\")\n        s = s.replace(\n            \"std::basic_ofstream< char, std::char_traits< char > >\",\n            \"std::ostream\")\n        s = s.replace(\n            \"std::basic_ifstream< char, std::char_traits< char > >\",\n            \"std::istream\")\n\n        # rename some types not renamed by gccxml (why ?)\n        s = s.replace(\"itk::SerieUIDContainer\", \"std::vector< std::string >\")\n        s = s.replace(\"itk::FilenamesContainer\", \"std::vector< std::string >\")\n\n        if s.startswith(\"itk::\") and not self.notWrappedRegExp.match(s):\n            self.warn(\n                4,\n                \"ITK type not wrapped, or currently not known: %s\" %\n                s,\n                w)\n\n        self.usedTypes.add(s)\n        return s + end\n\n    def load_idx(self, file_name):\n        with open(file_name, \"r\") as f:\n            for line in f:\n                (full_name, alias, module) = \\\n                    re.findall(r'{(.*)} {(.*)} {(.*)}', line)[0]\n                # workaround lack of :: prefix in idx files\n                # TODO: would it be better to remove the :: prefix in the output of\n                # pygccxml ?\n                # full_name = \"::\"+full_name\n                # normalize some basic type names\n                full_name = self.normalize(full_name)\n\n                if full_name in self.aliases:\n                    # If the full_name key alreay exists, do not overwrite the\n                    # value. load_idx() is called once before load_mdx(), making\n                    # sure the first aliases loaded are the ones belonging to\n                    # the current submodule (and the next load_idx() calls\n                    # should not overwrite these aliases.\n                    continue\n\n                self.aliases[full_name] = alias\n                # store the source of the def\n                if alias in self.typedefSource and file_name != self.typedefSource[alias]:\n                    self.warn(\n                        7, \"%s in %s is already defined in %s.\" %\n                        (alias, file_name, self.typedefSource[alias]))\n                else:\n                    self.typedefSource[alias] = file_name\n\n    def load_mdx(self, file_name):\n        if file_name in self.mdx_loaded:\n            # already loaded - no need to do it again\n            return\n        self.mdx_loaded.add(file_name)\n        with open(file_name, \"r\") as f:\n            lines = f.readlines()\n        for line in lines:\n            line_stripped = line.strip()\n            if line.startswith('%') or line.isspace():\n                # exclude the lines which are starting with % - that's not the idx\n                # files\n                pass\n            elif line_stripped.endswith(\".mdx\"):\n                self.load_mdx(os.path.dirname(file_name) + os.sep + line_stripped)\n            elif line_stripped[:-4] == self.moduleName:\n                continue\n            else:\n                self.load_idx(os.path.dirname(file_name) + os.sep + line_stripped)\n\n    @staticmethod\n    def normalize(name):\n        name = name.replace(\"short unsigned int\", \"unsigned short\")\n        name = name.replace(\"long unsigned int\", \"unsigned long\")\n        name = name.replace(\"long long unsigned int\", \"unsigned long long\")\n        name = name.replace(\"short int\", \"short\")\n        name = name.replace(\"long int\", \"long\")\n        name = name.replace(\"long long int\", \"long long\")\n    #  name = name.replace(\"unsigned int\", \"unsigned\")\n        # normalize spaces\n        name = \" \".join(name.replace(',', ', ').split())\n        return name\n\n    def generate_class(self, typedef, indent=0):\n        self.info(\"Generating interface for %s.\" % typedef.name)\n\n        decls = pygccxml.declarations\n\n        if not typedef.name.startswith(\"stdcomplex\"):\n            for member in getType(typedef).get_members(access=pygccxml.declarations.ACCESS_TYPES.PUBLIC):\n                if isinstance(member, decls.member_function_t) and member.name == 'New' and not typedef.name == 'itkLightObject':\n                    if typedef.name == 'itkPyCommand':\n                        self.outputFile.write(self.new_override_pycommand.format(class_name=typedef.name))\n                    else:\n                        self.outputFile.write(self.new_override.format(class_name=typedef.name))\n                    self.outputFile.write(\"\\n\")\n                    break\n\n            super_classes = []\n            for super_class in getType(typedef).bases:\n                super_classes.append(\n                    \"%s %s\" %\n                    (super_class.access,\n                     self.get_alias(\n                         super_class.related_class.decl_string)))\n            s = \"\"\n            if super_classes:\n                s = \" : \" + \", \".join(super_classes)\n            self.outputFile.write(\"  \" * indent)\n            self.outputFile.write(\"class %s%s {\\n\" % (typedef.name, s))\n\n            # iterate over access\n            for access in decls.ACCESS_TYPES.ALL:\n\n                # the access type\n                self.outputFile.write(\"  \" * indent)\n                self.outputFile.write(\"  %s:\\n\" % access)\n\n                # warnings or no warning?\n                w = access not in self.options.access_warnings\n\n                # iterate over the members\n                for member in getType(typedef).get_members(access=access):\n                    if isinstance(member, decls.typedef.typedef_t):\n                        self.warn(\n                            51,\n                            \"Member typedef are not supported: %s\" %\n                            member.name,\n                            w)\n                    elif isinstance(member, decls.member_function_t):\n                        self.generate_method(typedef, member, indent, w)\n                    elif isinstance(member, decls.constructor_t):\n                        self.generate_constructor(typedef, member, indent, w)\n                    elif isinstance(member, decls.member_operator_t):\n                        self.generate_method(typedef, member, indent, w)\n                    elif isinstance(member, decls.destructor_t):\n                        self.generate_destructor(typedef, member, indent, w)\n                    elif isinstance(member, decls.enumeration_t):\n                        self.generate_nested_enum(typedef, member, indent, w)\n                    elif isinstance(member, decls.variable_t):\n                        self.warn(\n                            52,\n                            \"Member variables are not supported: %s\" %\n                            member.name,\n                            w)\n                    elif isinstance(member, decls.class_declaration.class_t):\n                        self.warn(\n                            53,\n                            \"Member classes are not supported: %s\" %\n                            member.name,\n                            w)\n                    elif isinstance(\n                            member, decls.class_declaration.class_declaration_t):\n                        self.warn(\n                            53,\n                            \"Member classes are not supported: %s\" %\n                            member.name,\n                            w)\n                    elif isinstance(member, decls.casting_operator_t):\n                        self.warn(\n                            54,\n                            \"Member casting operators are not supported: %s\" %\n                            member.name,\n                            w)\n                    else:\n                        self.warn(\n                            50,\n                            \"Unknown member type: %s\" %\n                            repr(member),\n                            w)\n\n            # finally, close the class\n            self.outputFile.write(\"  \" * indent)\n            self.outputFile.write(\"};\\n\\n\")\n\n        elif typedef.name == \"stdcomplexD\":\n            self.outputFile.write(self.stdcomplex_headers[\"D\"] + '\\n')\n        elif typedef.name == \"stdcomplexF\":\n            self.outputFile.write(self.stdcomplex_headers[\"F\"] + '\\n')\n        else:\n            print('stdcomplex', typedef.name)\n            # stdcomplex is too difficult to wrap in some cases. Only wrap the\n            # constructor.\n            self.outputFile.write(\"  \" * indent)\n            self.outputFile.write(\"class %s%s {\\n\" % (typedef.name, s))\n\n            # iterate over access\n            for access in pygccxml.declarations.ACCESS_TYPES.ALL:\n\n                # the access type\n                self.outputFile.write(\"  \" * indent)\n                self.outputFile.write(\"  %s:\\n\" % access)\n\n                # warnings or no warning?\n                w = access not in self.options.access_warnings\n                for member in getType(typedef).get_members(access=access):\n                    if isinstance(member, decls.constructor_t):\n                        self.generate_constructor(typedef, member, indent, w)\n                    elif isinstance(member, decls.destructor_t):\n                        self.generate_destructor(typedef, member, indent, w)\n            # finally, close the class\n            self.outputFile.write(\"  \" * indent)\n            self.outputFile.write(\"};\\n\\n\\n\")\n\n    def generate_process_object_snake_case_functions(self, typedefs):\n        self.info(\"Generating snake case functions\")\n        processObjects = set()\n        for typedef in typedefs:\n            classType = getType(typedef)\n            bases = [base.related_class.name for base in classType.recursive_bases]\n            isProcessObject = 'ProcessObject' in bases\n            short_name = classType.name.split('<')[0]\n            if isProcessObject or short_name in self.forceSnakeCase:\n                processObjects.add(short_name)\n        if len(processObjects) > 0:\n            self.outputFile.write(\"\\n\\n#ifdef SWIGPYTHON\\n\")\n            self.outputFile.write('%pythoncode %{\\n')\n            for processObject in processObjects:\n                snakeCase = self.camelCaseToSnakeCase(processObject)\n                self.snakeCaseProcessObjectFunctions.add(snakeCase)\n                self.outputFile.write('import itkHelpers\\n')\n                self.outputFile.write('@itkHelpers.accept_numpy_array_like_xarray\\n')\n                self.outputFile.write('def %s(*args, **kwargs):\\n' % snakeCase)\n                self.outputFile.write('    \"\"\"Procedural interface for %s\"\"\"\\n' % processObject)\n                self.outputFile.write('    import itk\\n')\n                self.outputFile.write('    instance = itk.%s.New(*args, **kwargs)\\n' % processObject)\n                self.outputFile.write('    return instance.__internal_call__()\\n\\n')\n                self.outputFile.write('def %s_init_docstring():\\n' % snakeCase)\n                self.outputFile.write('    import itk\\n')\n                self.outputFile.write('    import itkTemplate\\n')\n                self.outputFile.write('    import itkHelpers\\n')\n                self.outputFile.write('    if isinstance(itk.%s, itkTemplate.itkTemplate):\\n' % processObject)\n                self.outputFile.write('        filter_object = itk.%s.values()[0]\\n' % (processObject))\n                self.outputFile.write('    else:\\n')\n                self.outputFile.write('        filter_object = itk.%s\\n\\n' % (processObject))\n                self.outputFile.write('    %s.__doc__ = filter_object.__doc__\\n' % (snakeCase))\n                self.outputFile.write('    %s.__doc__ += \"\\\\n Args are Input(s) to the filter.\\\\n\"\\n' % (snakeCase))\n                self.outputFile.write('    %s.__doc__ += \"\\\\n Available Keyword Arguments:\\\\n\"\\n' % (snakeCase))\n                self.outputFile.write('    if isinstance(itk.%s, itkTemplate.itkTemplate):\\n' % processObject)\n                self.outputFile.write('        %s.__doc__ += itkHelpers.filter_args(filter_object)[0]\\n' % (snakeCase))\n                self.outputFile.write('        %s.__doc__ += \"\\\\n\"\\n' % (snakeCase))\n                self.outputFile.write('        %s.__doc__ += itkHelpers.filter_args(filter_object)[1]\\n' % (snakeCase))\n                self.outputFile.write('    else:\\n')\n                self.outputFile.write('        %s.__doc__ += \"\".join([\\n' % (snakeCase))\n                self.outputFile.write('            \"  \" + itkHelpers.camel_to_snake_case(item[3:]) + \"\\\\n\"\\n')\n                self.outputFile.write('            for item in dir(filter_object)\\n')\n                self.outputFile.write('            if item.startswith(\"Set\")])\\n')\n\n            self.outputFile.write('%}\\n')\n            self.outputFile.write(\"#endif\\n\")\n\n    def generate_constructor(self, typedef, constructor, indent, w):\n        # iterate over the arguments\n        args = []\n        for arg in constructor.arguments:\n            s = \"%s %s\" % (self.get_alias(self.getDeclarationString(arg), w), arg.name)\n            if 'unknown' in s:\n                continue\n            # append the default value if it exists\n            if arg.default_value:\n                s += \" = %s\" % arg.default_value\n            # and add the string to the arg list\n            args.append(s)\n        self.outputFile.write(\"  \" * indent)\n        self.outputFile.write(\"    %s(%s);\\n\" % (typedef.name, \", \".join(args)))\n\n    def generate_destructor(self, typedef, destructor, indent, w):\n        self.outputFile.write(\"  \" * indent)\n        self.outputFile.write(\"    ~%s();\\n\" % typedef.name)\n\n    def generate_enum(self, typedef):\n        name = typedef.name\n        enum = getType(typedef)\n        decl_string = typedef.decl_type.decl_string\n        # extract the namespace to put it in c++ code. Without that, the code\n        # generated by swig\n        # is wrong because it doesn't include the namespace\n        ns = \"::\".join(decl_string.split(\"::\")[:-1])\n        self.outputFile.write(\"%{\\n\")\n        self.outputFile.write(\"using namespace %s;\\n\" % ns)\n        self.outputFile.write(\"%}\\n\")\n        content = [\" %s\" % (key,) for key, value in enum.values]\n        self.outputFile.write(\"enum class %s: uint8_t { %s };\\n\\n\" % (name, \", \".join(content)))\n\n    def generate_nested_enum(self, typedef, enum, indent, w):\n        content = [\" %s\" % (key,) for key, value in enum.values]\n        self.outputFile.write(\"  \" * indent)\n        self.outputFile.write(\"    enum class %s: uint8_t { %s };\\n\\n\" % (enum.name, \", \".join(content)))\n\n    def generate_method(self, typedef, method, indent, w):\n        self.info(\"Generating interface for method  '%s::%s'.\" %\n            (typedef.name, method.name))\n        # avoid the apply method for the class vnl_c_vector: the signature is\n        # quite strange and currently confuse swig :-/\n        if \"(\" in getType(method.return_type).decl_string:\n            self.warn(\n                1, \"ignoring method not supported by swig '%s::%s'.\" %\n                (typedef.name, method.name), w)\n            return\n\n        names = [\n            \"rBegin\",\n            \"rEnd\",\n            \"GetSpacingCallback\",\n            \"GetOriginCallback\",\n            \"Begin\",\n            \"End\"]\n\n        if ((typedef.name.startswith('vnl_') and method.name in [\"as_ref\"])\n                or (typedef.name.startswith('itk') and method.name in names)):\n            self.warn(\n                3, \"ignoring black listed method '%s::%s'.\" %\n                (typedef.name, method.name), w)\n            return\n\n        # iterate over the arguments\n        args = []\n        for arg in method.arguments:\n            s = \"%s %s\" % (self.get_alias(self.getDeclarationString(arg), w), arg.name)\n            if 'unknown' in s:\n                continue\n            if \"(\" in s:\n                self.warn(\n                    1, \"ignoring method not supported by swig '%s::%s'.\" %\n                    (typedef.name, method.name), w)\n                return\n            # append the default value if it exists\n            if arg.default_value:\n                s += \" = %s\" % arg.default_value\n            # and add the string to the arg list\n            args.append(s)\n\n        # find the method decorators\n        static = \"\"\n        const = \"\"\n        if method.has_static:\n            static = \"static \"\n        if method.has_const:\n            const = \" const\"\n        if method.virtuality != \"not virtual\":\n            static += \"virtual \"\n        if method.virtuality == \"pure virtual\":\n            const += \" = 0\"\n\n        self.outputFile.write(\"  \" * indent)\n        self.outputFile.write(\n            \"    %s%s %s(%s)%s;\\n\" %\n            (static,\n             self.get_alias(\n                 self.getDeclarationString(\n                     method.return_type),\n                 w),\n                method.name,\n                \", \".join(args),\n                const))\n\n        # Check the method arguments for std::string passed by reference.\n        # In this case, save the name of the argument in the applyFileNames list\n        # for further usage.\n        for arg in method.arguments:\n            dtype = arg.decl_type\n            if pygccxml.declarations.is_reference(dtype) and \\\n                pygccxml.declarations.is_const(\n                    pygccxml.declarations.remove_reference(dtype)) is False and \\\n                    pygccxml.declarations.is_std_string(dtype):\n                    self.applyFileNames.append(arg.name)\n\n\n    def generate_headerfile(self, idxFile, wrappersNamespace):\n        # and begin to write the output\n        headerFile = StringIO()\n        headerFile.write(\"// This file is automatically generated.\\n\")\n        headerFile.write(\"// Do not modify this file manually.\\n\\n\\n\")\n\n        langs = [\n            # \"CHICKEN\",\n            # \"CSHARP\",\n            # \"GUILE\",\n            # \"JAVA\",\n            # \"LUA\",\n            # \"MODULA3\",\n            # \"MZSCHEME\",\n            # \"OCAML\",\n            # \"PERL\",\n            # \"PERL5\",\n            # \"PHP\",\n            # \"PHP4\",\n            # \"PHP5\",\n            # \"PIKE\",\n            \"PYTHON\",\n            # \"R\",\n            # \"RUBY\",\n            # \"SEXP\",\n            # \"TCL\",\n            # \"XML\",\n            ]\n\n        # first, define the module\n        # [1:-1] is there to drop the quotes\n        for lang in langs:\n            headerFile.write(\"#ifdef SWIG%s\\n\" % lang)\n            if lang == \"PYTHON\":\n                # Also, release the GIL\n                headerFile.write(\"%%module(package=\\\"itk\\\",threads=\\\"1\\\") %s%s\\n\" % (self.moduleName, lang.title()))\n                headerFile.write('%feature(\"nothreadallow\");\\n')\n            else:\n                headerFile.write(\"%%module %s%s\\n\" % (self.moduleName, lang.title()))\n            headerFile.write(\"#endif\\n\")\n        headerFile.write('\\n')\n\n        # add the includes\n        # use a set to avoid putting many times the same include\n        s = set()\n        headerFile.write(\"%{\\n\")\n        # the include files passed in option\n        include = self.moduleName + 'SwigInterface.h'\n        i = '#include \"%s\"' % include\n        if i not in s:\n            headerFile.write(i + '\\n')\n            s.add(i)\n        headerFile.write(\"%}\\n\\n\\n\")\n\n        # load the aliases files\n        headerFile.write(\"%{\\n\")\n        self.load_idx(idxFile)\n        # and the idx files in the mdx ones\n        for f in self.options.mdx:\n            self.load_mdx(f)\n        # iterate over all the typedefs in the _wrapping_::wrappers namespace\n        # to fill the alias dict\n        for typedef in wrappersNamespace.typedefs():  # allow_empty=True):\n            s = getType(typedef).decl_string\n            # drop the :: prefix - it make swig produce invalid code\n            if s.startswith(\"::\"):\n                s = s[2:]\n            if s not in self.aliases:\n                self.warn(\n                    2, \"%s (%s) should be already defined in the idx files.\" %\n                    (s, typedef.name))\n                self.aliases[s] = typedef.name\n                # declare the typedef\n                headerFile.write(\"typedef %s %s;\\n\" % (s, typedef.name))\n\n        headerFile.write(\"%}\\n\\n\\n\")\n\n        return headerFile\n\n    def generate_importfile(self, usedSources):\n        # add the imports\n        importFile = StringIO()\n        for f in self.options.imports:\n            importFile.write(\"%%import %s\\n\" % f)\n        importFile.write(\"\\n\\n\")\n\n        for src in usedSources:\n            importFile.write(\"%%import %s.i\\n\" % src)\n        importFile.write('\\n\\n')\n        return importFile\n\n    def generate_includefile(self):\n        # add the swig includes\n        includeFile = StringIO()\n        includeFile.write(\"%include itk.i\\n\")\n        for f in options.swig_includes:\n            includeFile.write(\"%%include %s\\n\" % f)\n        includeFile.write(\"%%include %s\\n\" % (self.moduleName + \"_ext.i\"))\n        includeFile.write('\\n\\n')\n        return includeFile\n\n    def generate_applyfile(self):\n        # When a std::string is passed by reference, we need to add the %apply\n        # line with the argument name, and the INOUT command.\n        # Use a set() to remove duplicates, this will work event if we got\n        # multiple functions with the same argument name in the same .i file\n        # (swig should take care of it).\n        applyFileNames = set(self.applyFileNames)\n        # Apply file, for passing std::string as reference in methods\n        applyFile = StringIO()\n        for name in applyFileNames:\n            applyFile.write(\n                \"%apply (std::string& INOUT) { std::string & \" + name + \"};\\n\")\n        applyFile.write(\"\\n\\n\")\n        return applyFile\n\n    def create_typedefheader(self, usedSources):\n        # create the typedef header\n        typedefFile = StringIO()\n        typedefFile.write(\"#ifndef __%sSwigInterface_h\\n\" % self.moduleName)\n        typedefFile.write(\"#define __%sSwigInterface_h\\n\" % self.moduleName)\n        typedefInput = os.path.join(options.library_output_dir,\n                                    self.moduleName + 'SwigInterface.h.in')\n        with open(typedefInput, \"r\") as f:\n            typedefFile.write(f.read() + '\\n')\n        for src in usedSources:\n            typedefFile.write('#include \"%sSwigInterface.h\"\\n' % src)\n        typedefFile.write(\"#endif\\n\")\n        typedefOutput = os.path.join(options.interface_output_dir,\n                                     self.moduleName + 'SwigInterface.h')\n        with open(typedefOutput, \"w\") as f:\n            f.write(typedefFile.getvalue())\n\n    def create_interfacefile(self, interfaceFile, idxFile, wrappersNamespace):\n        headerFile = self.generate_headerfile(idxFile, wrappersNamespace)\n\n        # iterate over all the typedefs in the _wrapping_::wrappers namespace\n        # to build a list of classes with the dependecies\n        # classes :: [(name, [dep_name], typedef)]\n        classes = []\n        for typedef in wrappersNamespace.typedefs():\n            # begin a new class\n            if isinstance(\n                    getType(typedef),\n                    pygccxml.declarations.class_declaration.class_t):\n\n                classes.append((\n                    typedef.name,\n                    [self.get_alias(super_class.related_class.decl_string) for\n                        super_class in getType(typedef).bases], typedef))\n            elif isinstance(\n                    getType(typedef),\n                    pygccxml.declarations.enumeration.enumeration_t):\n                # warn( 6, \"Enum are currently supported only nested in a\n                # class.\" )\n                self.generate_enum(typedef)\n            else:\n                self.warn(\n                    5, \"Unknown type type: %s\" % str(typedef.decl_type.declaration))\n\n        # copy the classes in a new ordered list, according to the dependencies\n        # classes is sorted to be sure to always get the same result everywhere\n        name_local_classes = [c[0] for c in classes]\n        classes = sorted(classes)\n        name_already_in_typedefs = []\n        typedefs = []\n        while len(classes) != 0:\n            nclasses = []\n            for name, deps, typedef in classes:\n                ok = True\n                for d in deps:\n                    if d in name_local_classes and d not in name_already_in_typedefs:\n                        ok = False\n                if ok:\n                    name_already_in_typedefs.append(name)\n                    typedefs.append(typedef)\n                else:\n                    nclasses.append((name, deps, typedef))\n            classes = nclasses\n\n        # now really generate the swig interface\n        for typedef in typedefs:\n            # begin a new class\n            self.generate_class(typedef)\n\n        self.generate_process_object_snake_case_functions(typedefs)\n\n        if len(self.warnings) > 0 and self.options.warningError:\n            sys.exit(1)\n\n        # search the files to import\n        usedSources = set()\n        for alias in self.usedTypes:\n            if alias.rfind(\"Enums::\") != -1:\n                alias = alias[:alias.rfind(\"Enums::\")+5]\n            if alias in self.typedefSource:\n                idxName = os.path.basename(self.typedefSource[alias])\n                iName = idxName[:-len(\".idx\")]\n                usedSources.add(iName)\n        outputFileName = os.path.basename(interfaceFile)\n        if outputFileName in usedSources:\n            usedSources.remove(outputFileName)\n\n        importFile = self.generate_importfile(usedSources)\n        includeFile = self.generate_includefile()\n        applyFile = self.generate_applyfile()\n\n        self.create_typedefheader(usedSources)\n\n        # finally, really write the output\n        content = headerFile.getvalue() + importFile.getvalue() + \\\n            includeFile.getvalue() + applyFile.getvalue() + self.outputFile.getvalue()\n\n        if self.options.keep and os.path.exists(interfaceFile):\n            with open(interfaceFile, \"r\") as f:\n                filecontent = f.read()\n\n        if self.options.keep and os.path.exists(interfaceFile) and \\\n                filecontent == content:\n            self.info(\"%s unchanged.\" % interfaceFile)\n        else:\n            self.info(\"Writing %s.\" % interfaceFile)\n            with open(interfaceFile, \"w\") as f:\n                f.write(content)\n\n\nif __name__ == '__main__':\n    argParser = ArgumentParser()\n    argParser.add_argument(\n        \"--mdx\",\n        action=\"append\",\n        dest=\"mdx\",\n        default=[],\n        metavar=\"FILE\",\n        help=\"master idx file to be used.\")\n    argParser.add_argument(\n        \"--import\",\n        action=\"append\",\n        dest=\"imports\",\n        default=[],\n        metavar=\"FILE\",\n        help=\"File to be imported in the generated interface file.\")\n    argParser.add_argument(\n        \"--swig-include\",\n        action=\"append\",\n        dest=\"swig_includes\",\n        default=[],\n        metavar=\"FILE\",\n        help=(\n            \"File to be included by swig (%include) in the generated \"\n            \"interface file.\"))\n    argParser.add_argument(\n        \"-w\",\n        \"--disable-warning\",\n        action=\"append\",\n        dest=\"warnings\",\n        default=[],\n        metavar=\"WARNING\",\n        help=\"Warning to be disabled.\")\n    argParser.add_argument(\n        \"-A\",\n        \"--disable-access-warning\",\n        action=\"append\",\n        dest=\"access_warnings\",\n        default=[],\n        metavar=\"LEVEL\",\n        help=(\n            \"Access level where warnings are disabled \"\n            \"(public, protected, private).\"))\n    argParser.add_argument(\n        \"-W\",\n        \"--warning-error\",\n        action=\"store_true\",\n        dest=\"warningError\",\n        help=\"Treat warnings as errors.\")\n    argParser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        dest=\"verbose\",\n        help=\"Log what is currently done.\")\n    argParser.add_argument(\n        \"-k\",\n        \"--keep\",\n        action=\"store_true\",\n        dest=\"keep\",\n        help=\"Don't rewrite the output file if the content is unchanged.\")\n    argParser.add_argument(\n        \"-p\",\n        \"--pygccxml-path\",\n        action=\"store\",\n        dest=\"pygccxml_path\",\n        help=\"Path to pygccxml\")\n    argParser.add_argument(\n        \"-g\",\n        \"--castxml-path\",\n        action=\"store\",\n        dest=\"castxml_path\",\n        help=\"Path to castxml\")\n    argParser.add_argument(\n        \"-o\",\n        \"--interface-output-dir\",\n        action=\"store\",\n        dest=\"interface_output_dir\",\n        help=\"Directory to write the Swig input files\")\n    argParser.add_argument(\n        \"-l\",\n        \"--library-output-dir\",\n        action=\"store\",\n        dest=\"library_output_dir\",\n        help=\"Directory to read the xml abstract syntax tree input files\")\n    argParser.add_argument(\n        \"-s\",\n        \"--submodule-order\",\n        action=\"store\",\n        dest=\"submodule_order\",\n        help=\"List of submodules that must be wrapped in the given order\")\n    options = argParser.parse_args()\n\n    sys.path.insert(1, options.pygccxml_path)\n    import pygccxml\n    import logging\n    # init the pygccxml stuff\n    pygccxml.utils.loggers.cxx_parser.setLevel(logging.CRITICAL)\n    pygccxml.declarations.scopedef_t.RECURSIVE_DEFAULT = False\n    pygccxml.declarations.scopedef_t.ALLOW_EMPTY_MDECL_WRAPPER = True\n\n    pygccxml_config = pygccxml.parser.config.xml_generator_configuration_t(\n        xml_generator_path=options.castxml_path,\n        xml_generator=\"castxml\")\n\n    moduleNames = []\n    # The first mdx file is the master index file for this module.\n    with open(options.mdx[0], 'r') as ff:\n        for line in ff.readlines():\n            stripped = line.strip()\n            if line.startswith('%') or line.isspace():\n                # exclude the lines which are starting with % - that's not the idx\n                # files\n                pass\n            elif stripped.endswith(\".mdx\"):\n                pass\n            else:\n                moduleName = stripped.rsplit('.')[0]\n                if moduleName.startswith('(const char*)'):\n                    moduleName = moduleName[len('(const char*)'):]\n                moduleName = moduleName.strip('\"')\n                moduleNames.append(moduleName)\n\n    def generate_wrapping_namespace(moduleName):\n        xmlFilePath = os.path.join(options.library_output_dir,\n                                   moduleName + '.xml')\n        pygccxml_reader = pygccxml.parser.source_reader.source_reader_t(\n            pygccxml_config)\n        abstractSyntaxTree = pygccxml_reader.read_xml_file(xmlFilePath)\n        globalNamespace = pygccxml.declarations.get_global_namespace(abstractSyntaxTree)\n        wrappingNamespace = globalNamespace.namespace('_wrapping_')\n        return wrappingNamespace.namespace('wrappers')\n\n    wrappingNamespaces = dict()\n    # Limit the number of cached, parsed abstract syntax trees to avoid very\n    # high memory usage\n    wrappingCacheLength = min(len(moduleNames), 20)\n    for ii in range(wrappingCacheLength):\n        moduleName = moduleNames[ii]\n        wrappingNamespace = generate_wrapping_namespace(moduleName)\n        wrappingNamespaces[moduleName] = wrappingNamespace\n\n    for moduleName in moduleNames:\n        if moduleName in wrappingNamespaces:\n            wrappersNamespace = wrappingNamespaces[moduleName]\n        else:\n            wrappersNamespace = generate_wrapping_namespace(moduleName)\n\n        idxFilePath = os.path.join(options.interface_output_dir,\n                                   moduleName + '.idx')\n        idx_generator = IdxGenerator(moduleName)\n        idx_generator.create_idxfile(idxFilePath, wrappersNamespace)\n\n    snake_case_process_object_functions = set()\n    def generate_swig_input(moduleName):\n        if moduleName in wrappingNamespaces:\n            wrappersNamespace = wrappingNamespaces[moduleName]\n        else:\n            wrappersNamespace = generate_wrapping_namespace(moduleName)\n\n        idxFilePath = os.path.join(options.interface_output_dir,\n                                   moduleName + '.idx')\n        swigInputFilePath = os.path.join(options.interface_output_dir,\n                                   moduleName + '.i')\n\n        swig_input_generator = SwigInputGenerator(moduleName, options)\n        swig_input_generator.create_interfacefile(swigInputFilePath, idxFilePath,\n                wrappersNamespace)\n        snake_case_process_object_functions.update(swig_input_generator.snakeCaseProcessObjectFunctions)\n\n    if options.submodule_order:\n        for moduleName in options.submodule_order.split(';'):\n            generate_swig_input(moduleName)\n            moduleNames.remove(moduleName)\n    for moduleName in moduleNames:\n        generate_swig_input(moduleName)\n\n    config_file = os.path.join(options.library_output_dir, 'Generators',\n            'Python', 'Configuration', os.path.basename(options.mdx[0])[:-4] + 'Config.py')\n    with open(config_file, 'a') as ff:\n        ff.write('snake_case_functions = (')\n        for function in snake_case_process_object_functions:\n            ff.write(\"'\" + function + \"', \")\n        ff.write(')\\n')\n", "idx": 7, "id": 13943, "msg": "", "proj": "InsightSoftwareConsortium-ITK", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,39 @@\n+require 'spec_helper'\n+\n+describe Travis::Build do\n+  describe 'by_lang' do\n+    it 'maps anything java-ish to PureJava' do\n+      %w(java-woot java7 javaZOMBIES).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::PureJava)\n+      end\n+    end\n+\n+    it 'maps c++, cpp, and cplusplus to Cpp' do\n+      %w(c++ cpp cplusplus).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Cpp)\n+      end\n+    end\n+\n+    it 'maps objective-c to ObjectiveC' do\n+      expect(subject.by_lang('objective-c')).to eq(Travis::Build::Script::ObjectiveC)\n+    end\n+\n+    it 'maps bash, sh, and shell to Generic' do\n+      %w(bash sh shell).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Generic)\n+      end\n+    end\n+\n+    it 'maps known languages to their implementations' do\n+      %w(android c cpp clojure erlang go groovy haskell node_js perl php python rust scala).each do |lang|\n+        expect(subject.by_lang(lang)).to_not eq(Travis::Build::Script::Ruby)\n+      end\n+    end\n+\n+    it 'maps unknown languages to Ruby' do\n+      %w(brainfudge objective-d rubby).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Ruby)\n+      end\n+    end\n+  end\n+end", "y": 1, "oldf": "", "idx": 1, "id": 12061, "msg": "IMHO positive tests will be better. Also `cpp` is tested before.", "proj": "travis-ci-travis-build", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -176,8 +176,8 @@ def _get_mapped_solids_dict(\n     with user_code_error_boundary(\n         DagsterConfigMappingFunctionError, _get_error_lambda(current_stack)\n     ):\n-        mapped_solids_config = graph_def.config_mapping.config_fn(\n-            config_mapped_solid_config.value.get(\"config\", {})\n+        mapped_solids_config = graph_def.config_mapping.resolve(\n+            config_mapped_solid_config.value.get(\"config\", {}), config_has_been_validated=True\n         )\n \n     # Dynamically construct the type that the output of the config mapping function will", "y": 1, "oldf": "from collections import namedtuple\n\nfrom dagster import check\nfrom dagster.config.evaluate_value_result import EvaluateValueResult\nfrom dagster.config.validate import process_config\nfrom dagster.core.definitions.dependency import NodeHandle\nfrom dagster.core.definitions.graph import GraphDefinition\nfrom dagster.core.definitions.pipeline import PipelineDefinition\nfrom dagster.core.definitions.resource import ResourceDefinition\nfrom dagster.core.definitions.run_config import define_solid_dictionary_cls\nfrom dagster.core.definitions.solid import SolidDefinition\nfrom dagster.core.errors import (\n    DagsterConfigMappingFunctionError,\n    DagsterInvalidConfigError,\n    user_code_error_boundary,\n)\nfrom dagster.core.system_config.objects import SolidConfig\nfrom dagster.utils.merger import merge_dicts\n\n\nclass SolidConfigEntry(namedtuple(\"_SolidConfigEntry\", \"handle solid_config\")):\n    def __new__(cls, handle, solid_config):\n        return super(SolidConfigEntry, cls).__new__(\n            cls,\n            check.inst_param(handle, \"handle\", NodeHandle),\n            check.inst_param(solid_config, \"solid_config\", SolidConfig),\n        )\n\n\nclass DescentStack(namedtuple(\"_DescentStack\", \"pipeline_def handle\")):\n    def __new__(cls, pipeline_def, handle):\n        return super(DescentStack, cls).__new__(\n            cls,\n            pipeline_def=check.inst_param(pipeline_def, \"pipeline_def\", PipelineDefinition),\n            handle=check.opt_inst_param(handle, \"handle\", NodeHandle),\n        )\n\n    @property\n    def current_container(self):\n        return self.current_solid.definition if self.handle else self.pipeline_def\n\n    @property\n    def current_solid(self):\n        check.invariant(self.handle)\n        return self.pipeline_def.get_solid(self.handle)\n\n    @property\n    def current_handle_str(self):\n        check.invariant(self.handle)\n        return self.handle.to_string()\n\n    def descend(self, solid):\n        return self._replace(handle=NodeHandle(solid.name, parent=self.handle))\n\n\ndef composite_descent(pipeline_def, solids_config, resource_defs):\n    \"\"\"\n    This function is responsible for constructing the dictionary\n    of SolidConfig (indexed by handle) that will be passed into the\n    ResolvedRunConfig. Critically this is the codepath that manages config mapping,\n    where the runtime calls into user-defined config mapping functions to\n    produce config for child solids of composites.\n\n    Args:\n        pipeline_def (PipelineDefintion): PipelineDefinition\n        solids_config (dict): Configuration for the solids in the pipeline. The \"solids\" entry\n            of the run_config. Assumed to have already been validated.\n\n    Returns:\n        Dict[str, SolidConfig]: A dictionary mapping string representations of NodeHandles to\n            SolidConfig objects. It includes an entry for solids at every level of the\n            composite tree - i.e. not just leaf solids, but composite solids as well\n    \"\"\"\n    check.inst_param(pipeline_def, \"pipeline_def\", PipelineDefinition)\n    check.dict_param(solids_config, \"solids_config\")\n    check.dict_param(resource_defs, \"resource_defs\", key_type=str, value_type=ResourceDefinition)\n\n    return {\n        handle.to_string(): solid_config\n        for handle, solid_config in _composite_descent(\n            parent_stack=DescentStack(pipeline_def, None),\n            solids_config_dict=solids_config,\n            resource_defs=resource_defs,\n        )\n    }\n\n\ndef _composite_descent(parent_stack, solids_config_dict, resource_defs):\n    \"\"\"\n    The core implementation of composite_descent. This yields a stream of\n    SolidConfigEntry. This is used by composite_descent to construct a\n    dictionary.\n\n    It descends over the entire solid hierarchy, constructing an entry\n    for every handle. If it encounters a composite solid instance\n    with a config mapping, it will invoke that config mapping fn,\n    producing the config that is necessary to configure the child solids.\n\n    This process unrolls recursively as you descend down the tree.\n    \"\"\"\n\n    for solid in parent_stack.current_container.solids:\n\n        current_stack = parent_stack.descend(solid)\n        current_handle = current_stack.handle\n\n        current_solid_config = solids_config_dict.get(solid.name, {})\n\n        # the base case\n        if isinstance(solid.definition, SolidDefinition):\n            config_mapped_solid_config = solid.definition.apply_config_mapping(\n                {\"config\": current_solid_config.get(\"config\")}\n            )\n            if not config_mapped_solid_config.success:\n                raise DagsterInvalidConfigError(\n                    \"Error in config for solid {}\".format(solid.name),\n                    config_mapped_solid_config.errors,\n                    config_mapped_solid_config,\n                )\n\n            complete_config_object = merge_dicts(\n                current_solid_config, config_mapped_solid_config.value\n            )\n            yield SolidConfigEntry(current_handle, SolidConfig.from_dict(complete_config_object))\n            continue\n\n        graph_def = check.inst(solid.definition, GraphDefinition)\n\n        yield SolidConfigEntry(\n            current_handle,\n            SolidConfig.from_dict(\n                {\n                    \"inputs\": current_solid_config.get(\"inputs\"),\n                    \"outputs\": current_solid_config.get(\"outputs\"),\n                }\n            ),\n        )\n\n        # If there is a config mapping, invoke it and get the descendent solids\n        # config that way. Else just grabs the solids entry of the current config\n        solids_dict = (\n            _get_mapped_solids_dict(\n                solid, graph_def, current_stack, current_solid_config, resource_defs\n            )\n            if graph_def.config_mapping\n            else current_solid_config.get(\"solids\", {})\n        )\n\n        yield from _composite_descent(current_stack, solids_dict, resource_defs)\n\n\ndef _get_mapped_solids_dict(\n    composite, graph_def, current_stack, current_solid_config, resource_defs\n):\n    # the spec of the config mapping function is that it takes the dictionary at:\n    # solid_name:\n    #    config: {dict_passed_to_user}\n\n    # and it returns the dictionary rooted at solids\n    # solid_name:\n    #    solids: {return_value_of_config_fn}\n\n    # We must call the config mapping function and then validate it against\n    # the child schema.\n\n    # apply @configured config mapping to the composite's incoming config before we get to the\n    # composite's own config mapping process\n    config_mapped_solid_config = graph_def.apply_config_mapping(current_solid_config)\n    if not config_mapped_solid_config.success:\n        raise DagsterInvalidConfigError(\n            \"Error in config for composite solid {}\".format(composite.name),\n            config_mapped_solid_config.errors,\n            config_mapped_solid_config,\n        )\n\n    with user_code_error_boundary(\n        DagsterConfigMappingFunctionError, _get_error_lambda(current_stack)\n    ):\n        mapped_solids_config = graph_def.config_mapping.config_fn(\n            config_mapped_solid_config.value.get(\"config\", {})\n        )\n\n    # Dynamically construct the type that the output of the config mapping function will\n    # be evaluated against\n\n    type_to_evaluate_against = define_solid_dictionary_cls(\n        solids=graph_def.solids,\n        ignored_solids=None,\n        dependency_structure=graph_def.dependency_structure,\n        parent_handle=current_stack.handle,\n        resource_defs=resource_defs,\n    )\n\n    # process against that new type\n\n    evr = process_config(type_to_evaluate_against, mapped_solids_config)\n\n    if not evr.success:\n        raise_composite_descent_config_error(current_stack, mapped_solids_config, evr)\n\n    return evr.value\n\n\ndef _get_error_lambda(current_stack):\n    return lambda: (\n        \"The config mapping function on the composite solid definition \"\n        '\"{definition_name}\" at solid \"{solid_name}\" in pipeline \"{pipeline_name}\" '\n        \"has thrown an unexpected error during its execution. The definition is \"\n        'instantiated at stack \"{stack_str}\".'\n    ).format(\n        definition_name=current_stack.current_solid.definition.name,\n        solid_name=current_stack.current_solid.name,\n        pipeline_name=current_stack.pipeline_def.name,\n        stack_str=\":\".join(current_stack.handle.path),\n    )\n\n\ndef raise_composite_descent_config_error(descent_stack, failed_config_value, evr):\n    check.inst_param(descent_stack, \"descent_stack\", DescentStack)\n    check.inst_param(evr, \"evr\", EvaluateValueResult)\n\n    solid = descent_stack.current_solid\n    message = \"In pipeline {pipeline_name} at stack {stack}: \\n\".format(\n        pipeline_name=descent_stack.pipeline_def.name,\n        stack=\":\".join(descent_stack.handle.path),\n    )\n    message += (\n        'Solid \"{solid_name}\" with definition \"{solid_def_name}\" has a '\n        \"configuration error. \"\n        \"It has produced config a via its config_fn that fails to \"\n        \"pass validation in the solids that it contains. \"\n        \"This indicates an error in the config mapping function itself. It must \"\n        \"produce correct config for its constiuent solids in all cases. The correct \"\n        \"resolution is to fix the mapping function. Details on the error (and the paths \"\n        'on this error are relative to config mapping function \"root\", not the entire document): '\n    ).format(\n        solid_name=solid.name,\n        solid_def_name=solid.definition.name,\n    )\n\n    raise DagsterInvalidConfigError(message, evr.errors, failed_config_value)\n", "idx": 1, "id": 14561, "msg": "how do we handle enums in composite descent? is it under test at all?", "proj": "dagster-io-dagster", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -41,7 +41,13 @@ namespace PrepareRelease\n                 frameworkMoniker: \"net45\",\n                 groupDirectory: \"net45.GAC\",\n                 filePrefix: \"net45_GAC_\",\n-                isGac: true);\n+                GacStatus.Net45);\n+            CreateWixFile(\n+                groupId: \"Files.Managed.Net461.GAC\",\n+                frameworkMoniker: \"net461\",\n+                groupDirectory: \"net461.GAC\",\n+                filePrefix: \"net461_GAC_\",\n+                GacStatus.Net461);\n             CreateWixFile(\n                 groupId: \"Files.Managed.Net45\",\n                 frameworkMoniker: \"net45\");", "y": 0, "oldf": "using System;\nusing System.IO;\nusing System.Text;\nusing Datadog.Core.Tools;\nusing Datadog.Trace.TestHelpers;\n\nnamespace PrepareRelease\n{\n    public class SyncMsiContent\n    {\n        private const string FileNameTemplate = @\"{{file_name}}\";\n        private const string ComponentListTemplate = @\"{{component_list}}\";\n        private const string ComponentGroupIdTemplate = @\"{{component_group_id}}\";\n        private const string ComponentGroupDirectoryTemplate = @\"{{component_group_directory}}\";\n        private const string FileIdPrefixTemplate = @\"{{file_id_prefix}}\";\n        private const string FrameworkMonikerTemplate = @\"{{framework_moniker}}\";\n\n        private static readonly string Gac45ItemTemplate = $@\"\n      <Component Win64=\"\"$(var.Win64)\"\">\n        <File Id=\"\"{FileIdPrefixTemplate}{FileNameTemplate}\"\"\n              Source=\"\"$(var.TracerHomeDirectory)\\{FrameworkMonikerTemplate}\\{FileNameTemplate}\"\"\n              KeyPath=\"\"yes\"\" Checksum=\"\"yes\"\" Assembly=\"\".net\"\"/>\n      </Component>\";\n\n        private static readonly string Gac45FileTemplate = $@\"<?xml version=\"\"1.0\"\" encoding=\"\"UTF-8\"\"?>\n\n<Wix xmlns=\"\"http://schemas.microsoft.com/wix/2006/wi\"\"\n     xmlns:util=\"\"http://schemas.microsoft.com/wix/UtilExtension\"\">\n  <?include $(sys.CURRENTDIR)\\Config.wxi?>\n  <Fragment>\n    <ComponentGroup Id=\"\"{ComponentGroupIdTemplate}\"\" Directory=\"\"{ComponentGroupDirectoryTemplate}\"\">{ComponentListTemplate}\n    </ComponentGroup>\n  </Fragment>\n</Wix>\n\";\n\n        public static void Run()\n        {\n            CreateWixFile(\n                groupId: \"Files.Managed.Net45.GAC\",\n                frameworkMoniker: \"net45\",\n                groupDirectory: \"net45.GAC\",\n                filePrefix: \"net45_GAC_\",\n                isGac: true);\n            CreateWixFile(\n                groupId: \"Files.Managed.Net45\",\n                frameworkMoniker: \"net45\");\n            CreateWixFile(\n                groupId: \"Files.Managed.Net461\",\n                frameworkMoniker: \"net461\");\n            CreateWixFile(\n                groupId: \"Files.Managed.NetStandard20\",\n                frameworkMoniker: \"netstandard2.0\");\n            CreateWixFile(\n                groupId: \"Files.Managed.Netcoreapp31\",\n                frameworkMoniker: \"netcoreapp3.1\");\n        }\n\n        private static void CreateWixFile(\n            string groupId,\n            string frameworkMoniker,\n            string groupDirectory = null,\n            string filePrefix = null,\n            bool isGac = false)\n        {\n            Console.WriteLine($\"Creating the {groupId} Group\");\n\n            groupDirectory = groupDirectory ?? $\"{frameworkMoniker}\";\n            filePrefix = filePrefix ?? $\"{frameworkMoniker.Replace(\".\", string.Empty)}_\";\n\n            var solutionDirectory = EnvironmentHelper.GetSolutionDirectory();\n\n            var wixProjectRoot =\n                Path.Combine(\n                    solutionDirectory,\n                    \"deploy\",\n                    \"Datadog.Trace.ClrProfiler.WindowsInstaller\");\n\n            var filePaths = DependencyHelpers.GetTracerBinContent(frameworkMoniker);\n\n            var components = string.Empty;\n\n            foreach (var filePath in filePaths)\n            {\n                var fileName = Path.GetFileName(filePath);\n                var component =\n                        Gac45ItemTemplate\n                           .Replace(FileIdPrefixTemplate, filePrefix)\n                           .Replace(FrameworkMonikerTemplate, frameworkMoniker)\n                           .Replace(FileNameTemplate, fileName);\n\n                if (!isGac)\n                {\n                    component = component.Replace(@\" Assembly=\"\".net\"\"\", string.Empty);\n                }\n\n                components += component;\n            }\n\n            var wixFileContent =\n                Gac45FileTemplate\n                   .Replace(ComponentGroupDirectoryTemplate, groupDirectory)\n                   .Replace(ComponentGroupIdTemplate, groupId)\n                   .Replace(ComponentListTemplate, components);\n\n            var wixFilePath = Path.Combine(wixProjectRoot, groupId + \".wxs\");\n\n            File.WriteAllText(wixFilePath, wixFileContent, new UTF8Encoding(encoderShouldEmitUTF8Identifier: false));\n\n            Console.WriteLine($\"{groupId} Group successfully created.\");\n        }\n    }\n}\n", "idx": 3, "id": 17883, "msg": "", "proj": "DataDog-dd-trace-dotnet", "lang": ".cs", "sampling_weight": 0.04491010151092091}
{"patch": "@@ -23,10 +23,10 @@ func testMemoryNotification(t *testing.T, evName string, notify notifyFunc, targ\n \t}\n \tevFile := filepath.Join(memoryPath, evName)\n \teventPath := filepath.Join(memoryPath, \"cgroup.event_control\")\n-\tif err := ioutil.WriteFile(evFile, []byte{}, 0o700); err != nil {\n+\tif err := ioutil.WriteFile(evFile, []byte{}, 0o600); err != nil {\n \t\tt.Fatal(err)\n \t}\n-\tif err := ioutil.WriteFile(eventPath, []byte{}, 0o700); err != nil {\n+\tif err := ioutil.WriteFile(eventPath, []byte{}, 0o600); err != nil {\n \t\tt.Fatal(err)\n \t}\n \tch, err := notify(memoryPath)", "y": 1, "oldf": "// +build linux\n\npackage libcontainer\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\t\"time\"\n\n\t\"golang.org/x/sys/unix\"\n)\n\ntype notifyFunc func(path string) (<-chan struct{}, error)\n\nfunc testMemoryNotification(t *testing.T, evName string, notify notifyFunc, targ string) {\n\tmemoryPath, err := ioutil.TempDir(\"\", \"testmemnotification-\"+evName)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tevFile := filepath.Join(memoryPath, evName)\n\teventPath := filepath.Join(memoryPath, \"cgroup.event_control\")\n\tif err := ioutil.WriteFile(evFile, []byte{}, 0o700); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := ioutil.WriteFile(eventPath, []byte{}, 0o700); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tch, err := notify(memoryPath)\n\tif err != nil {\n\t\tt.Fatal(\"expected no error, got:\", err)\n\t}\n\n\tdata, err := ioutil.ReadFile(eventPath)\n\tif err != nil {\n\t\tt.Fatal(\"couldn't read event control file:\", err)\n\t}\n\n\tvar eventFd, evFd int\n\tvar arg string\n\tif targ != \"\" {\n\t\t_, err = fmt.Sscanf(string(data), \"%d %d %s\", &eventFd, &evFd, &arg)\n\t} else {\n\t\t_, err = fmt.Sscanf(string(data), \"%d %d\", &eventFd, &evFd)\n\t}\n\tif err != nil || arg != targ {\n\t\tt.Fatalf(\"invalid control data %q: %s\", data, err)\n\t}\n\n\t// dup the eventfd\n\tefd, err := unix.Dup(eventFd)\n\tif err != nil {\n\t\tt.Fatal(\"unable to dup eventfd:\", err)\n\t}\n\tdefer unix.Close(efd)\n\n\tbuf := make([]byte, 8)\n\tbinary.LittleEndian.PutUint64(buf, 1)\n\n\tif _, err := unix.Write(efd, buf); err != nil {\n\t\tt.Fatal(\"unable to write to eventfd:\", err)\n\t}\n\n\tselect {\n\tcase <-ch:\n\tcase <-time.After(100 * time.Millisecond):\n\t\tt.Fatal(\"no notification on channel after 100ms\")\n\t}\n\n\t// simulate what happens when a cgroup is destroyed by cleaning up and then\n\t// writing to the eventfd.\n\tif err := os.RemoveAll(memoryPath); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif _, err := unix.Write(efd, buf); err != nil {\n\t\tt.Fatal(\"unable to write to eventfd:\", err)\n\t}\n\n\t// give things a moment to shut down\n\tselect {\n\tcase _, ok := <-ch:\n\t\tif ok {\n\t\t\tt.Fatal(\"expected no notification to be triggered\")\n\t\t}\n\tcase <-time.After(100 * time.Millisecond):\n\t\tt.Fatal(\"channel not closed after 100ms\")\n\t}\n\n\tif _, _, err := unix.Syscall(unix.SYS_FCNTL, uintptr(evFd), unix.F_GETFD, 0); err != unix.EBADF {\n\t\tt.Errorf(\"expected event control to be closed, but received error %s\", err.Error())\n\t}\n\n\tif _, _, err := unix.Syscall(unix.SYS_FCNTL, uintptr(eventFd), unix.F_GETFD, 0); err != unix.EBADF {\n\t\tt.Errorf(\"expected event fd to be closed, but received error %s\", err.Error())\n\t}\n}\n\nfunc TestNotifyOnOOM(t *testing.T) {\n\tf := func(path string) (<-chan struct{}, error) {\n\t\treturn notifyOnOOM(path)\n\t}\n\n\ttestMemoryNotification(t, \"memory.oom_control\", f, \"\")\n}\n\nfunc TestNotifyMemoryPressure(t *testing.T) {\n\ttests := map[PressureLevel]string{\n\t\tLowPressure:      \"low\",\n\t\tMediumPressure:   \"medium\",\n\t\tCriticalPressure: \"critical\",\n\t}\n\n\tfor level, arg := range tests {\n\t\tf := func(path string) (<-chan struct{}, error) {\n\t\t\treturn notifyMemoryPressure(path, level)\n\t\t}\n\n\t\ttestMemoryNotification(t, \"memory.pressure_level\", f, arg)\n\t}\n}\n", "idx": 1, "id": 22628, "msg": "Should this also use the const? (`cgEvCtlFile`)", "proj": "opencontainers-runc", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -0,0 +1,45 @@\n+<?php\n+\n+/**\n+ * Copyright \u00a9 Bold Brand Commerce Sp. z o.o. All rights reserved.\n+ * See LICENSE.txt for license details.\n+ */\n+\n+declare(strict_types=1);\n+\n+namespace Ergonode\\BatchAction\\Infrastructure\\Handler;\n+\n+use Ergonode\\BatchAction\\Domain\\Command\\EndBatchActionProcessCommand;\n+use Ergonode\\BatchAction\\Domain\\Event\\BatchActionFinishedEvent;\n+use Ergonode\\BatchAction\\Domain\\Repository\\BatchActionRepositoryInterface;\n+use Ergonode\\SharedKernel\\Domain\\Bus\\CommandBusInterface;\n+use Ergonode\\SharedKernel\\Domain\\Bus\\EventBusInterface;\n+\n+class EndBatchActionProcessCommandHandler\n+{\n+    private BatchActionRepositoryInterface $repository;\n+\n+    private EventBusInterface $eventBus;\n+\n+    private CommandBusInterface $commandBus;\n+\n+    public function __construct(\n+        BatchActionRepositoryInterface $repository,\n+        EventBusInterface $eventBus,\n+        CommandBusInterface $commandBus\n+    ) {\n+        $this->repository = $repository;\n+        $this->eventBus = $eventBus;\n+        $this->commandBus = $commandBus;\n+    }\n+\n+    public function __invoke(EndBatchActionProcessCommand $command): void\n+    {\n+        if ($this->repository->isProcessEnded($command->getId())) {\n+            $event = new BatchActionFinishedEvent($command->getId(), $command->getType());\n+            $this->eventBus->dispatch($event);\n+        } else {\n+            $this->commandBus->dispatch($command, true);\n+        }\n+    }\n+}", "y": 1, "oldf": "", "idx": 1, "id": 9319, "msg": "Please sync naming - once it's `ended`, other time `finished`", "proj": "ergonode-backend", "lang": "php", "sampling_weight": 0.0739150857232327}
{"patch": "@@ -24,11 +24,13 @@ import (\n \t\"github.com/aws/amazon-ecs-agent/agent/api\"\n \tapierrors \"github.com/aws/amazon-ecs-agent/agent/api/errors\"\n \t\"github.com/aws/amazon-ecs-agent/agent/api/mocks\"\n+\t\"github.com/aws/amazon-ecs-agent/agent/ecs_client/model/ecs\"\n \t\"github.com/aws/amazon-ecs-agent/agent/engine/dockerstate/mocks\"\n \t\"github.com/aws/amazon-ecs-agent/agent/statechange\"\n \t\"github.com/aws/amazon-ecs-agent/agent/statemanager\"\n \t\"github.com/aws/amazon-ecs-agent/agent/utils\"\n \t\"github.com/aws/amazon-ecs-agent/agent/utils/mocks\"\n+\t\"github.com/aws/aws-sdk-go/aws/awserr\"\n \t\"github.com/golang/mock/gomock\"\n \t\"github.com/pkg/errors\"\n \t\"github.com/stretchr/testify/assert\"", "y": 0, "oldf": "// Copyright 2014-2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n// not use this file except in compliance with the License. A copy of the\n// License is located at\n//\n//\thttp://aws.amazon.com/apache2.0/\n//\n// or in the \"license\" file accompanying this file. This file is distributed\n// on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n// express or implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage eventhandler\n\nimport (\n\t\"container/list\"\n\t\"context\"\n\t\"strconv\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/aws/amazon-ecs-agent/agent/api\"\n\tapierrors \"github.com/aws/amazon-ecs-agent/agent/api/errors\"\n\t\"github.com/aws/amazon-ecs-agent/agent/api/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/dockerstate/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/statechange\"\n\t\"github.com/aws/amazon-ecs-agent/agent/statemanager\"\n\t\"github.com/aws/amazon-ecs-agent/agent/utils\"\n\t\"github.com/aws/amazon-ecs-agent/agent/utils/mocks\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nconst taskARN = \"taskarn\"\n\nfunc TestSendsEventsOneContainer(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\t// Trivial: one container, no errors\n\tcontEvent1 := containerEvent(taskARN)\n\tcontEvent2 := containerEvent(taskARN)\n\ttaskEvent2 := taskEvent(taskARN)\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\tassert.Equal(t, 2, len(change.Containers))\n\t\tassert.Equal(t, taskARN, change.Containers[0].TaskArn)\n\t\tassert.Equal(t, taskARN, change.Containers[1].TaskArn)\n\t\twg.Done()\n\t})\n\n\thandler.AddStateChangeEvent(contEvent1, client)\n\thandler.AddStateChangeEvent(contEvent2, client)\n\thandler.AddStateChangeEvent(taskEvent2, client)\n\n\twg.Wait()\n}\n\nfunc TestSendsEventsOneEventRetries(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\n\tretriable := apierrors.NewRetriableError(apierrors.NewRetriable(true), errors.New(\"test\"))\n\ttaskEvent := taskEvent(taskARN)\n\n\tgomock.InOrder(\n\t\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Return(retriable).Do(func(interface{}) { wg.Done() }),\n\t\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Return(nil).Do(func(interface{}) { wg.Done() }),\n\t)\n\n\thandler.AddStateChangeEvent(taskEvent, client)\n\n\twg.Wait()\n}\n\nfunc TestSendsEventsConcurrentLimit(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\tcompleteStateChange := make(chan bool, concurrentEventCalls+1)\n\tvar wg sync.WaitGroup\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Times(concurrentEventCalls + 1).Do(func(interface{}) {\n\t\twg.Done()\n\t\t<-completeStateChange\n\t})\n\n\t// Test concurrency; ensure it doesn't attempt to send more than\n\t// concurrentEventCalls at once\n\twg.Add(concurrentEventCalls)\n\n\t// Put on N+1 events\n\tfor i := 0; i < concurrentEventCalls+1; i++ {\n\t\thandler.AddStateChangeEvent(taskEvent(\"concurrent_\"+strconv.Itoa(i)), client)\n\t}\n\twg.Wait()\n\n\t// accept a single change event\n\twg.Add(1)\n\tcompleteStateChange <- true\n\twg.Wait()\n\n\t// ensure the remaining requests are completed\n\tfor i := 0; i < concurrentEventCalls; i++ {\n\t\tcompleteStateChange <- true\n\t}\n}\n\nfunc TestSendsEventsContainerDifferences(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\t// Test container event replacement doesn't happen\n\tcontEvent1 := containerEvent(taskARN)\n\tcontEvent2 := containerEventStopped(taskARN)\n\ttaskEvent := taskEvent(taskARN)\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\tassert.Equal(t, 2, len(change.Containers))\n\t\tassert.Equal(t, taskARN, change.Containers[0].TaskArn)\n\t\tassert.Equal(t, api.ContainerRunning, change.Containers[0].Status)\n\t\tassert.Equal(t, taskARN, change.Containers[1].TaskArn)\n\t\tassert.Equal(t, api.ContainerStopped, change.Containers[1].Status)\n\t\twg.Done()\n\t})\n\n\thandler.AddStateChangeEvent(contEvent1, client)\n\thandler.AddStateChangeEvent(contEvent2, client)\n\thandler.AddStateChangeEvent(taskEvent, client)\n\n\twg.Wait()\n}\n\nfunc TestSendsEventsTaskDifferences(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\ttaskARNA := \"taskarnA\"\n\ttaskARNB := \"taskarnB\"\n\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\n\tvar wgAddEvent sync.WaitGroup\n\twgAddEvent.Add(1)\n\n\t// Test task event replacement doesn't happen\n\ttaskEventA := taskEvent(taskARNA)\n\tcontEventA1 := containerEvent(taskARNA)\n\n\tcontEventB1 := containerEvent(taskARNB)\n\tcontEventB2 := containerEventStopped(taskARNB)\n\ttaskEventB := taskEventStopped(taskARNB)\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\tassert.Equal(t, taskARNA, change.TaskARN)\n\t\twgAddEvent.Done()\n\t\twg.Done()\n\t})\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\tassert.Equal(t, taskARNB, change.TaskARN)\n\t\twg.Done()\n\t})\n\n\thandler.AddStateChangeEvent(contEventB1, client)\n\thandler.AddStateChangeEvent(contEventA1, client)\n\thandler.AddStateChangeEvent(contEventB2, client)\n\n\thandler.AddStateChangeEvent(taskEventA, client)\n\twgAddEvent.Wait()\n\n\thandler.AddStateChangeEvent(taskEventB, client)\n\n\twg.Wait()\n}\n\nfunc TestSendsEventsDedupe(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\ttaskARNA := \"taskarnA\"\n\ttaskARNB := \"taskarnB\"\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\t// Verify that a task doesn't get sent if we already have 'sent' it\n\ttask1 := taskEvent(taskARNA)\n\ttask1.(api.TaskStateChange).Task.SetSentStatus(api.TaskRunning)\n\tcont1 := containerEvent(taskARNA)\n\tcont1.(api.ContainerStateChange).Container.SetSentStatus(api.ContainerRunning)\n\n\thandler.AddStateChangeEvent(cont1, client)\n\thandler.AddStateChangeEvent(task1, client)\n\n\ttask2 := taskEvent(taskARNB)\n\ttask2.(api.TaskStateChange).Task.SetSentStatus(api.TaskStatusNone)\n\tcont2 := containerEvent(taskARNB)\n\tcont2.(api.ContainerStateChange).Container.SetSentStatus(api.ContainerRunning)\n\n\t// Expect to send a task status but not a container status\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\tassert.Equal(t, 1, len(change.Containers))\n\t\tassert.Equal(t, taskARNB, change.Containers[0].TaskArn)\n\t\tassert.Equal(t, taskARNB, change.TaskARN)\n\t\twg.Done()\n\t})\n\n\thandler.AddStateChangeEvent(cont2, client)\n\thandler.AddStateChangeEvent(task2, client)\n\n\twg.Wait()\n}\n\n// TestCleanupTaskEventAfterSubmit tests the map of task event is removed after\n// calling submittaskstatechange\nfunc TestCleanupTaskEventAfterSubmit(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tstateManager := statemanager.NewNoopStateManager()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, stateManager, nil, client)\n\tdefer cancel()\n\n\ttaskARN2 := \"taskarn2\"\n\n\tvar wg sync.WaitGroup\n\twg.Add(3)\n\n\ttaskEvent1 := taskEvent(taskARN)\n\ttaskEvent2 := taskEvent(taskARN)\n\ttaskEvent3 := taskEvent(taskARN2)\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(\n\t\tfunc(change api.TaskStateChange) {\n\t\t\twg.Done()\n\t\t}).Times(3)\n\n\thandler.AddStateChangeEvent(taskEvent1, client)\n\thandler.AddStateChangeEvent(taskEvent2, client)\n\thandler.AddStateChangeEvent(taskEvent3, client)\n\n\twg.Wait()\n\n\t// Wait for task events to be removed from the tasksToEvents map\n\tfor {\n\t\tif handler.getTasksToEventsLen() == 0 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(time.Millisecond)\n\t}\n}\n\nfunc containerEvent(arn string) statechange.Event {\n\treturn api.ContainerStateChange{TaskArn: arn, ContainerName: \"containerName\", Status: api.ContainerRunning, Container: &api.Container{}}\n}\n\nfunc containerEventStopped(arn string) statechange.Event {\n\treturn api.ContainerStateChange{TaskArn: arn, ContainerName: \"containerName\", Status: api.ContainerStopped, Container: &api.Container{}}\n}\n\nfunc taskEvent(arn string) statechange.Event {\n\treturn api.TaskStateChange{TaskARN: arn, Status: api.TaskRunning, Task: &api.Task{}}\n}\n\nfunc taskEventStopped(arn string) statechange.Event {\n\treturn api.TaskStateChange{TaskARN: arn, Status: api.TaskStopped, Task: &api.Task{}}\n}\n\nfunc TestENISentStatusChange(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tclient := mock_api.NewMockECSClient(ctrl)\n\n\ttask := &api.Task{\n\t\tArn: taskARN,\n\t}\n\n\teniAttachment := &api.ENIAttachment{\n\t\tTaskARN:          taskARN,\n\t\tAttachStatusSent: false,\n\t\tExpiresAt:        time.Now().Add(time.Second),\n\t}\n\ttimeoutFunc := func() {\n\t\teniAttachment.AttachStatusSent = true\n\t}\n\tassert.NoError(t, eniAttachment.StartTimer(timeoutFunc))\n\n\tsendableTaskEvent := newSendableTaskEvent(api.TaskStateChange{\n\t\tAttachment: eniAttachment,\n\t\tTaskARN:    taskARN,\n\t\tStatus:     api.TaskStatusNone,\n\t\tTask:       task,\n\t})\n\n\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Return(nil)\n\n\tevents := list.New()\n\tevents.PushBack(sendableTaskEvent)\n\tctx, cancel := context.WithCancel(context.Background())\n\thandler := NewTaskHandler(ctx, statemanager.NewNoopStateManager(), nil, client)\n\tdefer cancel()\n\thandler.submitTaskEvents(&taskSendableEvents{\n\t\tevents: events,\n\t}, client, taskARN)\n\n\tassert.True(t, eniAttachment.AttachStatusSent)\n}\n\nfunc TestGetBatchedContainerEvents(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tstate := mock_dockerstate.NewMockTaskEngineState(ctrl)\n\n\thandler := &TaskHandler{\n\t\ttasksToContainerStates: map[string][]api.ContainerStateChange{\n\t\t\t\"t1\": []api.ContainerStateChange{},\n\t\t\t\"t2\": []api.ContainerStateChange{},\n\t\t},\n\t\tstate: state,\n\t}\n\n\tstate.EXPECT().TaskByArn(\"t1\").Return(&api.Task{Arn: \"t1\", KnownStatusUnsafe: api.TaskRunning}, true)\n\tstate.EXPECT().TaskByArn(\"t2\").Return(nil, false)\n\n\tevents := handler.taskStateChangesToSend()\n\tassert.Len(t, events, 1)\n\tassert.Equal(t, \"t1\", events[0].TaskARN)\n}\n\nfunc TestGetBatchedContainerEventsStoppedTask(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tstate := mock_dockerstate.NewMockTaskEngineState(ctrl)\n\n\thandler := &TaskHandler{\n\t\ttasksToContainerStates: map[string][]api.ContainerStateChange{\n\t\t\t\"t1\": []api.ContainerStateChange{},\n\t\t},\n\t\tstate: state,\n\t}\n\n\tstate.EXPECT().TaskByArn(\"t1\").Return(&api.Task{Arn: \"t1\", KnownStatusUnsafe: api.TaskStopped}, true)\n\n\tevents := handler.taskStateChangesToSend()\n\tassert.Len(t, events, 0)\n}\n\nfunc TestSubmitTaskEventsWhenSubmittingTaskRunningAfterStopped(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tstate := mock_dockerstate.NewMockTaskEngineState(ctrl)\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\thandler := &TaskHandler{\n\t\tstate:                  state,\n\t\tsubmitSemaphore:        utils.NewSemaphore(concurrentEventCalls),\n\t\ttasksToEvents:          make(map[string]*taskSendableEvents),\n\t\ttasksToContainerStates: make(map[string][]api.ContainerStateChange),\n\t\tstateSaver:             stateManager,\n\t\tclient:                 client,\n\t}\n\n\ttaskEvents := &taskSendableEvents{events: list.New(),\n\t\tsending:   false,\n\t\tcreatedAt: time.Now(),\n\t\ttaskARN:   taskARN,\n\t}\n\n\tbackoff := mock_utils.NewMockBackoff(ctrl)\n\tok, err := taskEvents.submitFirstEvent(handler, backoff)\n\tassert.True(t, ok)\n\tassert.NoError(t, err)\n\n\ttask := &api.Task{}\n\ttaskEvents.events.PushBack(newSendableTaskEvent(api.TaskStateChange{\n\t\tTaskARN: taskARN,\n\t\tStatus:  api.TaskStopped,\n\t\tTask:    task,\n\t}))\n\ttaskEvents.events.PushBack(newSendableTaskEvent(api.TaskStateChange{\n\t\tTaskARN: taskARN,\n\t\tStatus:  api.TaskRunning,\n\t\tTask:    task,\n\t}))\n\thandler.tasksToEvents[taskARN] = taskEvents\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgomock.InOrder(\n\t\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\t\tassert.Equal(t, api.TaskStopped, change.Status)\n\t\t}),\n\t\tbackoff.EXPECT().Reset().Do(func() {\n\t\t\twg.Done()\n\t\t}),\n\t)\n\tok, err = taskEvents.submitFirstEvent(handler, backoff)\n\t// We have an unsent event for the TaskRunning transition. Hence, send() returns false\n\tassert.False(t, ok)\n\tassert.NoError(t, err)\n\twg.Wait()\n\n\t// The unsent transition is deleted from the task list. send() returns true as it\n\t// does not have any more events to process\n\tok, err = taskEvents.submitFirstEvent(handler, backoff)\n\tassert.NoError(t, err)\n\tassert.True(t, ok)\n}\n\nfunc TestSubmitTaskEventsWhenSubmittingTaskStoppedAfterRunning(t *testing.T) {\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tstate := mock_dockerstate.NewMockTaskEngineState(ctrl)\n\tclient := mock_api.NewMockECSClient(ctrl)\n\tstateManager := statemanager.NewNoopStateManager()\n\n\thandler := &TaskHandler{\n\t\tstate:                  state,\n\t\tsubmitSemaphore:        utils.NewSemaphore(concurrentEventCalls),\n\t\ttasksToEvents:          make(map[string]*taskSendableEvents),\n\t\ttasksToContainerStates: make(map[string][]api.ContainerStateChange),\n\t\tstateSaver:             stateManager,\n\t\tclient:                 client,\n\t}\n\n\ttaskEvents := &taskSendableEvents{events: list.New(),\n\t\tsending:   false,\n\t\tcreatedAt: time.Now(),\n\t\ttaskARN:   taskARN,\n\t}\n\n\tbackoff := mock_utils.NewMockBackoff(ctrl)\n\tok, err := taskEvents.submitFirstEvent(handler, backoff)\n\tassert.True(t, ok)\n\tassert.NoError(t, err)\n\n\ttask := &api.Task{}\n\ttaskEvents.events.PushBack(newSendableTaskEvent(api.TaskStateChange{\n\t\tTaskARN: taskARN,\n\t\tStatus:  api.TaskRunning,\n\t\tTask:    task,\n\t}))\n\ttaskEvents.events.PushBack(newSendableTaskEvent(api.TaskStateChange{\n\t\tTaskARN: taskARN,\n\t\tStatus:  api.TaskStopped,\n\t\tTask:    task,\n\t}))\n\thandler.tasksToEvents[taskARN] = taskEvents\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgomock.InOrder(\n\t\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\t\tassert.Equal(t, api.TaskRunning, change.Status)\n\t\t}),\n\t\tbackoff.EXPECT().Reset().Do(func() {\n\t\t\twg.Done()\n\t\t}),\n\t)\n\t// We have an unsent event for the TaskStopped transition. Hence, send() returns false\n\tok, err = taskEvents.submitFirstEvent(handler, backoff)\n\tassert.False(t, ok)\n\tassert.NoError(t, err)\n\twg.Wait()\n\n\twg.Add(1)\n\tgomock.InOrder(\n\t\tclient.EXPECT().SubmitTaskStateChange(gomock.Any()).Do(func(change api.TaskStateChange) {\n\t\t\tassert.Equal(t, api.TaskStopped, change.Status)\n\t\t}),\n\t\tbackoff.EXPECT().Reset().Do(func() {\n\t\t\twg.Done()\n\t\t}),\n\t)\n\t// The unsent transition is send and deleted from the task list. send() returns true as it\n\t// does not have any more events to process\n\tok, err = taskEvents.submitFirstEvent(handler, backoff)\n\tassert.True(t, ok)\n\tassert.NoError(t, err)\n\twg.Wait()\n}\n", "idx": 1, "id": 19695, "msg": "", "proj": "aws-amazon-ecs-agent", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -811,7 +811,18 @@ module Beaker\n       ssh_string = public_key()\n       region.key_pairs.import(pair_name, ssh_string)\n       kp = region.key_pairs[pair_name]\n-      if kp.exists?\n+\n+      exists = false\n+      for tries in 1..10\n+        if kp.exists?\n+          exists = true\n+          break\n+        end\n+        @logger.debug(\"AWS key pair doesn't appear to exist yet, sleeping before retry \")\n+        backoff_sleep(tries)\n+      end\n+\n+      if exists\n         @logger.debug(\"aws-sdk: key pair #{pair_name} imported\")\n         kp\n       else", "y": 1, "oldf": "require 'aws/ec2'\nrequire 'set'\nrequire 'zlib'\nrequire 'beaker/hypervisor/ec2_helper'\n\nmodule Beaker\n  # This is an alternate EC2 driver that implements direct API access using\n  # Amazon's AWS-SDK library: {http://aws.amazon.com/documentation/sdkforruby/ SDK For Ruby}\n  #\n  # It is built for full control, to reduce any other layers beyond the pure\n  # vendor API.\n  class AwsSdk < Beaker::Hypervisor\n    ZOMBIE = 3 #anything older than 3 hours is considered a zombie\n    PING_SECURITY_GROUP_NAME = 'beaker-ping'\n\n    # Initialize AwsSdk hypervisor driver\n    #\n    # @param [Array<Beaker::Host>] hosts Array of Beaker::Host objects\n    # @param [Hash<String, String>] options Options hash\n    def initialize(hosts, options)\n      @hosts = hosts\n      @options = options\n      @logger = options[:logger]\n\n      # Get AWS credentials\n      creds = load_credentials()\n\n      config = {\n        :access_key_id => creds[:access_key],\n        :secret_access_key => creds[:secret_key],\n        :logger => Logger.new($stdout),\n        :log_level => :debug,\n        :log_formatter => AWS::Core::LogFormatter.colored,\n        :max_retries => 12,\n      }\n      AWS.config(config)\n\n      @ec2 = AWS::EC2.new()\n    end\n\n    # Provision all hosts on EC2 using the AWS::EC2 API\n    #\n    # @return [void]\n    def provision\n      start_time = Time.now\n\n      # Perform the main launch work\n      launch_all_nodes()\n\n      wait_for_status_netdev()\n\n      # Add metadata tags to each instance\n      add_tags()\n\n      # Grab the ip addresses and dns from EC2 for each instance to use for ssh\n      populate_dns()\n\n      #enable root if user is not root\n      enable_root_on_hosts()\n\n      # Set the hostname for each box\n      set_hostnames()\n\n      # Configure /etc/hosts on each host\n      configure_hosts()\n\n      @logger.notify(\"aws-sdk: Provisioning complete in #{Time.now - start_time} seconds\")\n\n      nil #void\n    end\n\n    # Kill all instances.\n    #\n    # @param instances [Enumerable<EC2::Instance>]\n    # @return [void]\n    def kill_instances(instances)\n      instances.each do |instance|\n        if !instance.nil? and instance.exists?\n          @logger.notify(\"aws-sdk: killing EC2 instance #{instance.id}\")\n          instance.terminate\n        end\n      end\n      nil\n    end\n\n    # Cleanup all earlier provisioned hosts on EC2 using the AWS::EC2 library\n    #\n    # It goes without saying, but a #cleanup does nothing without a #provision\n    # method call first.\n    #\n    # @return [void]\n    def cleanup\n      # Provisioning should have set the host 'instance' values.\n      kill_instances(@hosts.map{|h| h['instance']}.select{|x| !x.nil?})\n      delete_key_pair_all_regions()\n      nil\n    end\n\n    # Print instances to the logger. Instances will be from all regions\n    # associated with provided key name and limited by regex compared to\n    # instance status. Defaults to running instances.\n    #\n    # @param [String] key The key_name to match for\n    # @param [Regex] status The regular expression to match against the instance's status\n    def log_instances(key = key_name, status = /running/)\n      instances = []\n      @ec2.regions.each do |region|\n        @logger.debug \"Reviewing: #{region.name}\"\n        @ec2.regions[region.name].instances.each do |instance|\n          if (instance.key_name =~ /#{key}/) and (instance.status.to_s =~ status)\n            instances << instance\n          end\n        end\n      end\n      output = \"\"\n      instances.each do |instance|\n        output << \"#{instance.id} keyname: #{instance.key_name}, dns name: #{instance.dns_name}, private ip: #{instance.private_ip_address}, ip: #{instance.ip_address}, launch time #{instance.launch_time}, status: #{instance.status}\\n\"\n      end\n      @logger.notify(\"aws-sdk: List instances (keyname: #{key})\")\n      @logger.notify(\"#{output}\")\n    end\n\n    # Provided an id return an instance object.\n    # Instance object will respond to methods described here: {http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/EC2/Instance.html AWS Instance Object}.\n    # @param [String] id The id of the instance to return\n    # @return [AWS::EC2::Instance] An AWS::EC2 instance object\n    def instance_by_id(id)\n      @ec2.instances[id]\n    end\n\n    # Return all instances currently on ec2.\n    # @see AwsSdk#instance_by_id\n    # @return [AWS::EC2::InstanceCollection] An array of AWS::EC2 instance objects\n    def instances\n      @ec2.instances\n    end\n\n    # Provided an id return a VPC object.\n    # VPC object will respond to methods described here: {http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/EC2/VPC.html AWS VPC Object}.\n    # @param [String] id The id of the VPC to return\n    # @return [AWS::EC2::VPC] An AWS::EC2 vpc object\n    def vpc_by_id(id)\n      @ec2.vpcs[id]\n    end\n\n    # Return all VPCs currently on ec2.\n    # @see AwsSdk#vpc_by_id\n    # @return [AWS::EC2::VPCCollection] An array of AWS::EC2 vpc objects\n    def vpcs\n      @ec2.vpcs\n    end\n\n    # Provided an id return a security group object\n    # Security object will respond to methods described here: {http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/EC2/SecurityGroup.html AWS SecurityGroup Object}.\n    # @param [String] id The id of the security group to return\n    # @return [AWS::EC2::SecurityGroup] An AWS::EC2 security group object\n    def security_group_by_id(id)\n      @ec2.security_groups[id]\n    end\n\n    # Return all security groups currently on ec2.\n    # @see AwsSdk#security_goup_by_id\n    # @return [AWS::EC2::SecurityGroupCollection] An array of AWS::EC2 security group objects\n    def security_groups\n      @ec2.security_groups\n    end\n\n    # Shutdown and destroy ec2 instances idenfitied by key that have been alive\n    # longer than ZOMBIE hours.\n    #\n    # @param [Integer] max_age The age in hours that a machine needs to be older than to be considered a zombie\n    # @param [String] key The key_name to match for\n    def kill_zombies(max_age = ZOMBIE, key = key_name)\n      @logger.notify(\"aws-sdk: Kill Zombies! (keyname: #{key}, age: #{max_age} hrs)\")\n      #examine all available regions\n      kill_count = 0\n      time_now = Time.now.getgm #ec2 uses GM time\n      @ec2.regions.each do |region|\n        @logger.debug \"Reviewing: #{region.name}\"\n        # Note: don't use instances.each here as that funtion doesn't allow proper rescue from error states\n        instances = @ec2.regions[region.name].instances\n        instances.each do |instance|\n          begin\n            if (instance.key_name =~ /#{key}/)\n              @logger.debug \"Examining #{instance.id} (keyname: #{instance.key_name}, launch time: #{instance.launch_time}, status: #{instance.status})\"\n              if ((time_now - instance.launch_time) >  max_age*60*60) and instance.status.to_s !~ /terminated/\n                @logger.debug \"Kill! #{instance.id}: #{instance.key_name} (Current status: #{instance.status})\"\n                instance.terminate()\n                kill_count += 1\n              end\n            end\n          rescue AWS::Core::Resource::NotFound, AWS::EC2::Errors => e\n            @logger.debug \"Failed to remove instance: #{instance.id}, #{e}\"\n          end\n        end\n      end\n      delete_key_pair_all_regions(key_name_prefix)\n\n      @logger.notify \"#{key}: Killed #{kill_count} instance(s)\"\n    end\n\n    # Destroy any volumes marked 'available', INCLUDING THOSE YOU DON'T OWN!  Use with care.\n    def kill_zombie_volumes\n      # Occasionaly, tearing down ec2 instances leaves orphaned EBS volumes behind -- these stack up quickly.\n      # This simply looks for EBS volumes that are not in use\n      # Note: don't use volumes.each here as that funtion doesn't allow proper rescue from error states\n      @logger.notify(\"aws-sdk: Kill Zombie Volumes!\")\n      volume_count = 0\n      @ec2.regions.each do |region|\n        @logger.debug \"Reviewing: #{region.name}\"\n        volumes = @ec2.regions[region.name].volumes.map { |vol| vol.id }\n        volumes.each do |vol|\n          begin\n            vol = @ec2.regions[region.name].volumes[vol]\n            if ( vol.status.to_s =~ /available/ )\n              @logger.debug \"Tear down available volume: #{vol.id}\"\n              vol.delete()\n              volume_count += 1\n            end\n          rescue AWS::EC2::Errors::InvalidVolume::NotFound => e\n            @logger.debug \"Failed to remove volume: #{vol.id}, #{e}\"\n          end\n        end\n      end\n      @logger.notify \"Freed #{volume_count} volume(s)\"\n\n    end\n\n    # Create an EC2 instance for host, tag it, and return it.\n    #\n    # @return [void]\n    # @api private\n    def create_instance(host, ami_spec, subnet_id)\n      amitype = host['vmname'] || host['platform']\n      amisize = host['amisize'] || 'm1.small'\n      vpc_id = host['vpc_id'] || @options['vpc_id'] || nil\n\n      if vpc_id and !subnet_id\n        raise RuntimeError, \"A subnet_id must be provided with a vpc_id\"\n      end\n\n      # Use snapshot provided for this host\n      image_type = host['snapshot']\n      if not image_type\n        raise RuntimeError, \"No snapshot/image_type provided for EC2 provisioning\"\n      end\n      ami = ami_spec[amitype]\n      ami_region = ami[:region]\n\n      # Main region object for ec2 operations\n      region = @ec2.regions[ami_region]\n\n      # If we haven't defined a vpc_id then we use the default vpc for the provided region\n      if !vpc_id\n        @logger.notify(\"aws-sdk: filtering available vpcs in region by 'isDefault\")\n        filtered_vpcs = region.client.describe_vpcs(:filters => [{:name => 'isDefault', :values => ['true']}])\n        if !filtered_vpcs[:vpc_set].empty?\n          vpc_id = filtered_vpcs[:vpc_set].first[:vpc_id]\n        else #there's no default vpc, use nil\n          vpc_id = nil\n        end\n      end\n\n      # Grab the vpc object based upon provided id\n      vpc = vpc_id ? region.vpcs[vpc_id] : nil\n\n      # Grab image object\n      image_id = ami[:image][image_type.to_sym]\n      @logger.notify(\"aws-sdk: Checking image #{image_id} exists and getting its root device\")\n      image = region.images[image_id]\n      if image.nil? and not image.exists?\n        raise RuntimeError, \"Image not found: #{image_id}\"\n      end\n\n      @logger.notify(\"Image Storage Type: #{image.root_device_type}\")\n\n      # Transform the images block_device_mappings output into a format\n      # ready for a create.\n      block_device_mappings = []\n      if image.root_device_type == :ebs\n        orig_bdm = image.block_device_mappings()\n        @logger.notify(\"aws-sdk: Image block_device_mappings: #{orig_bdm.to_hash}\")\n        orig_bdm.each do |device_name, rest|\n          block_device_mappings << {\n            :device_name => device_name,\n            :ebs => {\n              # Change the default size of the root volume.\n              :volume_size => host['volume_size'] || rest[:volume_size],\n              # This is required to override the images default for\n              # delete_on_termination, forcing all volumes to be deleted once the\n              # instance is terminated.\n              :delete_on_termination => true,\n            }\n          }\n        end\n      end\n\n      security_group = ensure_group(vpc || region, Beaker::EC2Helper.amiports(host))\n      #check if ping is enabled\n      ping_security_group = ensure_ping_group(vpc || region)\n\n      msg = \"aws-sdk: launching %p on %p using %p/%p%s\" %\n            [host.name, amitype, amisize, image_type,\n             subnet_id ? (\"in %p\" % subnet_id) : '']\n      @logger.notify(msg)\n      config = {\n        :count => 1,\n        :image_id => image_id,\n        :monitoring_enabled => true,\n        :key_pair => ensure_key_pair(region),\n        :security_groups => [security_group, ping_security_group],\n        :instance_type => amisize,\n        :disable_api_termination => false,\n        :instance_initiated_shutdown_behavior => \"terminate\",\n        :subnet => subnet_id,\n      }\n      config[:block_device_mappings] = block_device_mappings if image.root_device_type == :ebs\n      region.instances.create(config)\n    end\n\n    # For each host, create an EC2 instance in one of the specified\n    # subnets and push it onto instances_created.  Each subnet will be\n    # tried at most once for each host, and more than one subnet may\n    # be tried if capacity constraints are encountered.  Each Hash in\n    # instances_created will contain an :instance and :host value.\n    #\n    # @param hosts [Enumerable<Host>]\n    # @param subnets [Enumerable<String>]\n    # @param ami_spec [Hash]\n    # @param instances_created Enumerable<Hash{Symbol=>EC2::Instance,Host}>\n    # @return [void]\n    # @api private\n    def launch_nodes_on_some_subnet(hosts, subnets, ami_spec, instances_created)\n      # Shuffle the subnets so we don't always hit the same one\n      # first, and cycle though the subnets independently of the\n      # host, so we stick with one that's working.  Try each subnet\n      # once per-host.\n      if subnets.nil? or subnets.empty?\n        return\n      end\n      subnet_i = 0\n      shuffnets = subnets.shuffle\n      hosts.each do |host|\n        instance = nil\n        shuffnets.length.times do\n          begin\n            subnet_id = shuffnets[subnet_i]\n            instance = create_instance(host, ami_spec, subnet_id)\n            instances_created.push({:instance => instance, :host => host})\n            break\n          rescue AWS::EC2::Errors::InsufficientInstanceCapacity => ex\n            @logger.notify(\"aws-sdk: hit #{subnet_id} capacity limit; moving on\")\n            subnet_i = (subnet_i + 1) % shuffnets.length\n          end\n        end\n        if instance.nil?\n          raise RuntimeError, \"unable to launch host in any requested subnet\"\n        end\n      end\n    end\n\n    # Create EC2 instances for all hosts, tag them, and wait until\n    # they're running.  When a host provides a subnet_id, create the\n    # instance in that subnet, otherwise prefer a CONFIG subnet_id.\n    # If neither are set but there is a CONFIG subnet_ids list,\n    # attempt to create the host in each specified subnet, which might\n    # fail due to capacity constraints, for example.  Specifying both\n    # a CONFIG subnet_id and subnet_ids will provoke an error.\n    #\n    # @return [void]\n    # @api private\n    def launch_all_nodes\n      @logger.notify(\"aws-sdk: launch all hosts in configuration\")\n      ami_spec = YAML.load_file(@options[:ec2_yaml])[\"AMI\"]\n      global_subnet_id = @options['subnet_id']\n      global_subnets = @options['subnet_ids']\n      if global_subnet_id and global_subnets\n        raise RuntimeError, 'Config specifies both subnet_id and subnet_ids'\n      end\n      no_subnet_hosts = []\n      specific_subnet_hosts = []\n      some_subnet_hosts = []\n      @hosts.each do |host|\n        if global_subnet_id or host['subnet_id']\n          specific_subnet_hosts.push(host)\n        elsif global_subnets\n          some_subnet_hosts.push(host)\n        else\n          no_subnet_hosts.push(host)\n        end\n      end\n      instances = [] # Each element is {:instance => i, :host => h}\n      begin\n        @logger.notify(\"aws-sdk: launch instances not particular about subnet\")\n        launch_nodes_on_some_subnet(some_subnet_hosts, global_subnets, ami_spec,\n                                    instances)\n        @logger.notify(\"aws-sdk: launch instances requiring a specific subnet\")\n        specific_subnet_hosts.each do |host|\n          subnet_id = host['subnet_id'] || global_subnet_id\n          instance = create_instance(host, ami_spec, subnet_id)\n          instances.push({:instance => instance, :host => host})\n        end\n        @logger.notify(\"aws-sdk: launch instances requiring no subnet\")\n        no_subnet_hosts.each do |host|\n          instance = create_instance(host, ami_spec, nil)\n          instances.push({:instance => instance, :host => host})\n        end\n        wait_for_status(:running, instances)\n      rescue Exception => ex\n        @logger.notify(\"aws-sdk: exception #{ex.class}: #{ex}\")\n        kill_instances(instances.map{|x| x[:instance]})\n        raise ex\n      end\n      # At this point, all instances should be running since wait\n      # either returns on success or throws an exception.\n      if instances.empty?\n        raise RuntimeError, \"Didn't manage to launch any EC2 instances\"\n      end\n      # Assign the now known running instances to their hosts.\n      instances.each {|x| x[:host]['instance'] = x[:instance]}\n      nil\n    end\n\n    # Wait until all instances reach the desired state.  Each Hash in\n    # instances must contain an :instance and :host value.\n    #\n    # @param status [Symbol] EC2 state to wait for, :running :stopped etc.\n    # @param instances Enumerable<Hash{Symbol=>EC2::Instance,Host}>\n    # @param block [Proc] more complex checks can be made by passing a\n    #                     block in.  This overrides the status parameter.\n    #                     EC2::Instance objects from the hosts will be\n    #                     yielded to the passed block\n    # @return [void]\n    # @api private\n    def wait_for_status(status, instances, &block)\n      # Wait for each node to reach status :running\n      @logger.notify(\"aws-sdk: Waiting for all hosts to be #{status}\")\n      instances.each do |x|\n        name = x[:name]\n        instance = x[:instance]\n        @logger.notify(\"aws-sdk: Wait for node #{name} to be #{status}\")\n        # Here we keep waiting for the machine state to reach ':running' with an\n        # exponential backoff for each poll.\n        # TODO: should probably be a in a shared method somewhere\n        for tries in 1..10\n          begin\n            if block_given?\n              test_result = yield instance\n            else\n              test_result = instance.status == status\n            end\n            if test_result\n              # Always sleep, so the next command won't cause a throttle\n              backoff_sleep(tries)\n              break\n            elsif tries == 10\n              raise \"Instance never reached state #{status}\"\n            end\n          rescue AWS::EC2::Errors::InvalidInstanceID::NotFound => e\n            @logger.debug(\"Instance #{name} not yet available (#{e})\")\n          end\n          backoff_sleep(tries)\n        end\n      end\n    end\n\n    # Handles special checks needed for netdev platforms.\n    #\n    # @note if any host is an netdev one, these checks will happen once across all\n    #   of the hosts, and then we'll exit\n    #\n    # @return [void]\n    # @api private\n    def wait_for_status_netdev()\n      @hosts.each do |host|\n        if host['platform'] =~ /f5-|netscaler/\n          wait_for_status(:running, @hosts)\n\n          wait_for_status(nil, @hosts) do |instance|\n            instance_status_collection = instance.client.describe_instance_status({:instance_ids => [instance.id]})\n            first_instance = instance_status_collection[:instance_status_set].first\n            first_instance[:system_status][:status] == \"ok\"\n          end\n\n          break\n        end\n      end\n    end\n\n    # Add metadata tags to all instances\n    #\n    # @return [void]\n    # @api private\n    def add_tags\n      @hosts.each do |host|\n        instance = host['instance']\n\n        # Define tags for the instance\n        @logger.notify(\"aws-sdk: Add tags for #{host.name}\")\n        instance.add_tag(\"jenkins_build_url\", :value => @options[:jenkins_build_url])\n        instance.add_tag(\"Name\", :value => host.name)\n        instance.add_tag(\"department\", :value => @options[:department])\n        instance.add_tag(\"project\", :value => @options[:project])\n        instance.add_tag(\"created_by\", :value => @options[:created_by])\n\n        host[:host_tags].each do |name, val|\n          instance.add_tag(name.to_s, :value => val)\n        end\n      end\n\n      nil\n    end\n\n    # Populate the hosts IP address from the EC2 dns_name\n    #\n    # @return [void]\n    # @api private\n    def populate_dns\n      # Obtain the IP addresses and dns_name for each host\n      @hosts.each do |host|\n        @logger.notify(\"aws-sdk: Populate DNS for #{host.name}\")\n        instance = host['instance']\n        host['ip'] = instance.ip_address ? instance.ip_address : instance.private_ip_address\n        host['private_ip'] = instance.private_ip_address\n        host['dns_name'] = instance.dns_name\n        @logger.notify(\"aws-sdk: name: #{host.name} ip: #{host['ip']} private_ip: #{host['private_ip']} dns_name: #{instance.dns_name}\")\n      end\n\n      nil\n    end\n\n    # Return a valid /etc/hosts line for a given host\n    #\n    # @param [Beaker::Host] host Beaker::Host object for generating /etc/hosts entry\n    # @param [Symbol] interface Symbol identifies which ip should be used for host\n    # @return [String] formatted hosts entry for host\n    # @api private\n    def etc_hosts_entry(host, interface = :ip)\n      name = host.name\n      domain = get_domain_name(host)\n      ip = host[interface.to_s]\n      \"#{ip}\\t#{name} #{name}.#{domain} #{host['dns_name']}\\n\"\n    end\n\n    # Configure /etc/hosts for each node\n    #\n    # @note f5 hosts are skipped since this isn't a valid step there\n    #\n    # @return [void]\n    # @api private\n    def configure_hosts\n      non_netdev_hosts = @hosts.select{ |h| !(h['platform'] =~ /f5-|netscaler/) }\n      non_netdev_hosts.each do |host|\n        host_entries = non_netdev_hosts.map do |h|\n          h == host ? etc_hosts_entry(h, :private_ip) : etc_hosts_entry(h)\n        end\n        host_entries.unshift \"127.0.0.1\\tlocalhost localhost.localdomain\\n\"\n        set_etc_hosts(host, host_entries.join(''))\n      end\n      nil\n    end\n\n    # Enables root for instances with custom username like ubuntu-amis\n    #\n    # @return [void]\n    # @api private\n    def enable_root_on_hosts\n      @hosts.each do |host|\n        enable_root(host)\n      end\n    end\n\n    # Enables root access for a host when username is not root\n    #\n    # @return [void]\n    # @api private\n    def enable_root(host)\n      if host['user'] != 'root'\n        if host['platform'] =~ /f5-/\n          enable_root_f5(host)\n        elsif host['platform'] =~ /netscaler/\n          enable_root_netscaler(host)\n        else\n          copy_ssh_to_root(host, @options)\n          enable_root_login(host, @options)\n          host['user'] = 'root'\n        end\n        host.close\n      end\n    end\n\n    # Enables root access for a host on an f5 platform\n    # @note This method does not support other platforms\n    #\n    # @return nil\n    # @api private\n    def enable_root_f5(host)\n      for tries in 1..10\n        begin\n          #This command is problematic as the F5 is not always done loading\n          if host.exec(Command.new(\"modify sys db systemauth.disablerootlogin value false\"), :acceptable_exit_codes => [0,1]).exit_code == 0 \\\n              and host.exec(Command.new(\"modify sys global-settings gui-setup disabled\"), :acceptable_exit_codes => [0,1]).exit_code == 0 \\\n              and host.exec(Command.new(\"save sys config\"), :acceptable_exit_codes => [0,1]).exit_code == 0\n            backoff_sleep(tries)\n            break\n          elsif tries == 10\n            raise \"Instance was unable to be configured\"\n          end\n        rescue Beaker::Host::CommandFailure => e\n          @logger.debug(\"Instance not yet configured (#{e})\")\n        end\n        backoff_sleep(tries)\n      end\n      host['user'] = 'root'\n      host.close\n      sha256 = Digest::SHA256.new\n      password = sha256.hexdigest((1..50).map{(rand(86)+40).chr}.join.gsub(/\\\\/,'\\&\\&'))\n      host['ssh'] = {:password => password}\n      host.exec(Command.new(\"echo -e '#{password}\\\\n#{password}' | tmsh modify auth password admin\"))\n      @logger.notify(\"f5: Configured admin password to be #{password}\")\n    end\n\n    # Enables root access for a host on an netscaler platform\n    # @note This method does not support other platforms\n    #\n    # @return nil\n    # @api private\n    def enable_root_netscaler(host)\n      host['ssh'] = {:password => host['instance'].id}\n      @logger.notify(\"netscaler: nsroot password is #{host['instance'].id}\")\n    end\n\n    # Set the :vmhostname for each host object to be the dns_name, which is accessible\n    # publicly. Then configure each ec2 machine to that dns_name, so that when facter\n    # is installed the facts for hostname and domain match the dns_name.\n    #\n    # if :use_beaker_hostnames: is true, set the :vmhostname and hostname of each ec2\n    # machine to the host[:name] from the beaker hosts file.\n    #\n    # @return [@hosts]\n    # @api private\n    def set_hostnames\n      if @options[:use_beaker_hostnames]\n        @hosts.each do |host|\n          host[:vmhostname] = host[:name]\n          if host['platform'] =~ /el-7/\n            # on el-7 hosts, the hostname command doesn't \"stick\" randomly\n            host.exec(Command.new(\"hostnamectl set-hostname #{host.name}\"))\n          else\n            next if host['platform'] =~ /netscaler/\n            host.exec(Command.new(\"hostname #{host.name}\"))\n          end\n        end\n      else\n        @hosts.each do |host|\n          host[:vmhostname] = host[:dns_name]\n          if host['platform'] =~ /el-7/\n            # on el-7 hosts, the hostname command doesn't \"stick\" randomly\n            host.exec(Command.new(\"hostnamectl set-hostname #{host.hostname}\"))\n          else\n            next if host['platform'] =~ /netscaler/\n            host.exec(Command.new(\"hostname #{host.hostname}\"))\n          end\n        end\n      end\n    end\n\n    # Calculates and waits a back-off period based on the number of tries\n    #\n    # Logs each backupoff time and retry value to the console.\n    #\n    # @param tries [Number] number of tries to calculate back-off period\n    # @return [void]\n    # @api private\n    def backoff_sleep(tries)\n      # Exponential with some randomization\n      sleep_time = 2 ** tries\n      @logger.notify(\"aws-sdk: Sleeping #{sleep_time} seconds for attempt #{tries}.\")\n      sleep sleep_time\n      nil\n    end\n\n    # Retrieve the public key locally from the executing users ~/.ssh directory\n    #\n    # @return [String] contents of public key\n    # @api private\n    def public_key\n      keys = Array(@options[:ssh][:keys])\n      keys << '~/.ssh/id_rsa'\n      keys << '~/.ssh/id_dsa'\n      key_file = nil\n      keys.each do |key|\n        key_filename = File.expand_path(key + '.pub')\n        key_file = key_filename if File.exists?(key_filename)\n      end\n\n      if key_file\n        @logger.debug(\"Using public key: #{key_file}\")\n      else\n        raise RuntimeError, \"Expected to find a public key, but couldn't in #{keys}\"\n      end\n      File.read(key_file)\n    end\n\n    # Generate a key prefix for key pair names\n    #\n    # @note This is the part of the key that will stay static between Beaker\n    #   runs on the same host.\n    #\n    # @return [String] Beaker key pair name based on sanitized hostname\n    def key_name_prefix\n      safe_hostname = Socket.gethostname.gsub('.', '-')\n      \"Beaker-#{local_user}-#{safe_hostname}\"\n    end\n\n    # Generate a reusable key name from the local hosts hostname\n    #\n    # @return [String] safe key name for current host\n    # @api private\n    def key_name\n      \"#{key_name_prefix}-#{@options[:aws_keyname_modifier]}-#{@options[:timestamp].strftime(\"%F_%H_%M_%S_%N\")}\"\n    end\n\n    # Returns the local user running this tool\n    #\n    # @return [String] username of local user\n    # @api private\n    def local_user\n      ENV['USER']\n    end\n\n    # Creates the KeyPair for this test run\n    #\n    # @param region [AWS::EC2::Region] region to create the key pair in\n    # @return [AWS::EC2::KeyPair] created key_pair\n    # @api private\n    def ensure_key_pair(region)\n      pair_name = key_name()\n      delete_key_pair(region, pair_name)\n      create_new_key_pair(region, pair_name)\n    end\n\n    # Deletes key pairs from all regions\n    #\n    # @param [String] keypair_name_filter if given, will get all keypairs that match\n    #   a simple {::String#start_with?} filter. If no filter is given, the basic key\n    #   name returned by {#key_name} will be used.\n    #\n    # @return nil\n    # @api private\n    def delete_key_pair_all_regions(keypair_name_filter=nil)\n      region_keypairs_hash = my_key_pairs(keypair_name_filter)\n      region_keypairs_hash.each_pair do |region, keypair_name_array|\n        keypair_name_array.each do |keypair_name|\n          delete_key_pair(region, keypair_name)\n        end\n      end\n    end\n\n    # Gets the Beaker user's keypairs by region\n    #\n    # @param [String] name_filter if given, will get all keypairs that match\n    #   a simple {::String#start_with?} filter. If no filter is given, the basic key\n    #   name returned by {#key_name} will be used.\n    #\n    # @return [Hash{AWS::EC2::Region=>Array[String]}] a hash of region instance to\n    #   an array of the keypair names that match for the filter\n    # @api private\n    def my_key_pairs(name_filter=nil)\n      keypairs_by_region = {}\n      keyname_default = key_name()\n      keyname_filtered = \"#{name_filter}-*\"\n      @ec2.regions.each do |region|\n        if name_filter\n          aws_name_filter = keyname_filtered\n        else\n          aws_name_filter = keyname_default\n        end\n        keypair_collection = region.key_pairs.filter('key-name', aws_name_filter)\n        keypair_collection.each do |keypair|\n          keypairs_by_region[region] ||= []\n          keypairs_by_region[region] << keypair.name\n        end\n      end\n      keypairs_by_region\n    end\n\n    # Deletes a given key pair\n    #\n    # @param [AWS::EC2::Region] region the region the key belongs to\n    # @param [String] pair_name the name of the key to be deleted\n    #\n    # @api private\n    def delete_key_pair(region, pair_name)\n      kp = region.key_pairs[pair_name]\n      if kp.exists?\n        @logger.debug(\"aws-sdk: delete key pair in region: #{region.name}\")\n        kp.delete()\n      end\n    end\n\n    # Create a new key pair for a given Beaker run\n    #\n    # @param [AWS::EC2::Region] region the region the key pair will be imported into\n    # @param [String] pair_name the name of the key to be created\n    #\n    # @return [AWS::EC2::KeyPair] key pair created\n    # @raise [RuntimeError] raised if AWS keypair not created\n    def create_new_key_pair(region, pair_name)\n      @logger.debug(\"aws-sdk: importing new key pair: #{pair_name}\")\n      ssh_string = public_key()\n      region.key_pairs.import(pair_name, ssh_string)\n      kp = region.key_pairs[pair_name]\n      if kp.exists?\n        @logger.debug(\"aws-sdk: key pair #{pair_name} imported\")\n        kp\n      else\n        raise RuntimeError, \"AWS key pair #{pair_name} can not be queried, even after import\"\n      end\n    end\n\n    # Return a reproducable security group identifier based on input ports\n    #\n    # @param ports [Array<Number>] array of port numbers\n    # @return [String] group identifier\n    # @api private\n    def group_id(ports)\n      if ports.nil? or ports.empty?\n        raise ArgumentError, \"Ports list cannot be nil or empty\"\n      end\n\n      unless ports.is_a? Set\n        ports = Set.new(ports)\n      end\n\n      # Lolwut, #hash is inconsistent between ruby processes\n      \"Beaker-#{Zlib.crc32(ports.inspect)}\"\n    end\n\n    # Return an existing group, or create new one\n    #\n    # Accepts a VPC as input for checking & creation.\n    #\n    # @param vpc [AWS::EC2::VPC] the AWS vpc control object\n    # @return [AWS::EC2::SecurityGroup] created security group\n    # @api private\n    def ensure_ping_group(vpc)\n      @logger.notify(\"aws-sdk: Ensure security group exists that enables ping, create if not\")\n\n      group = vpc.security_groups.filter('group-name', PING_SECURITY_GROUP_NAME).first\n\n      if group.nil?\n        group = create_ping_group(vpc)\n      end\n\n      group\n    end\n\n    # Return an existing group, or create new one\n    #\n    # Accepts a VPC as input for checking & creation.\n    #\n    # @param vpc [AWS::EC2::VPC] the AWS vpc control object\n    # @param ports [Array<Number>] an array of port numbers\n    # @return [AWS::EC2::SecurityGroup] created security group\n    # @api private\n    def ensure_group(vpc, ports)\n      @logger.notify(\"aws-sdk: Ensure security group exists for ports #{ports.to_s}, create if not\")\n      name = group_id(ports)\n\n      group = vpc.security_groups.filter('group-name', name).first\n\n      if group.nil?\n        group = create_group(vpc, ports)\n      end\n\n      group\n    end\n\n    # Create a new ping enabled security group\n    #\n    # Accepts a region or VPC for group creation.\n    #\n    # @param rv [AWS::EC2::Region, AWS::EC2::VPC] the AWS region or vpc control object\n    # @return [AWS::EC2::SecurityGroup] created security group\n    # @api private\n    def create_ping_group(rv)\n      @logger.notify(\"aws-sdk: Creating group #{PING_SECURITY_GROUP_NAME}\")\n      group = rv.security_groups.create(PING_SECURITY_GROUP_NAME,\n                                        :description => \"Custom Beaker security group to enable ping\")\n\n      group.allow_ping\n\n      group\n    end\n\n    # Create a new security group\n    #\n    # Accepts a region or VPC for group creation.\n    #\n    # @param rv [AWS::EC2::Region, AWS::EC2::VPC] the AWS region or vpc control object\n    # @param ports [Array<Number>] an array of port numbers\n    # @return [AWS::EC2::SecurityGroup] created security group\n    # @api private\n    def create_group(rv, ports)\n      name = group_id(ports)\n      @logger.notify(\"aws-sdk: Creating group #{name} for ports #{ports.to_s}\")\n      group = rv.security_groups.create(name,\n                                        :description => \"Custom Beaker security group for #{ports.to_a}\")\n\n      unless ports.is_a? Set\n        ports = Set.new(ports)\n      end\n\n      ports.each do |port|\n        group.authorize_ingress(:tcp, port)\n      end\n\n      group\n    end\n\n    # Return a hash containing AWS credentials\n    #\n    # @return [Hash<Symbol, String>] AWS credentials\n    # @api private\n    def load_credentials\n      return load_env_credentials unless load_env_credentials.empty?\n      load_fog_credentials(@options[:dot_fog])\n    end\n\n    # Return AWS credentials loaded from environment variables\n    #\n    # @param prefix [String] environment variable prefix\n    # @return [Hash<Symbol, String>] ec2 credentials\n    # @api private\n    def load_env_credentials(prefix='AWS')\n      provider = AWS::Core::CredentialProviders::ENVProvider.new prefix\n\n      if provider.set?\n        {\n          :access_key => provider.access_key_id,\n          :secret_key => provider.secret_access_key,\n        }\n      else\n        {}\n      end\n    end\n    # Return a hash containing the fog credentials for EC2\n    #\n    # @param dot_fog [String] dot fog path\n    # @return [Hash<Symbol, String>] ec2 credentials\n    # @api private\n    def load_fog_credentials(dot_fog = '.fog')\n      fog = YAML.load_file( dot_fog )\n      default = fog[:default]\n\n      raise \"You must specify an aws_access_key_id in your .fog file (#{dot_fog}) for ec2 instances!\" unless default[:aws_access_key_id]\n      raise \"You must specify an aws_secret_access_key in your .fog file (#{dot_fog}) for ec2 instances!\" unless default[:aws_secret_access_key]\n\n      {\n        :access_key => default[:aws_access_key_id],\n        :secret_key => default[:aws_secret_access_key],\n      }\n    end\n  end\nend\n", "idx": 1, "id": 14367, "msg": "The exponential backoff here seems a little high; @mullr have you seen how long it takes when this needs to wait? 2^10 is an extra 25 minutes from 2^8, which is about 8 minutes of waiting and retrying.", "proj": "voxpupuli-beaker", "lang": "rb", "sampling_weight": 0.06140236423892906}
{"patch": "@@ -4103,7 +4103,7 @@ func (mb *msgBlock) removeSeqPerSubject(subj string, seq uint64) {\n \t}\n \t// TODO(dlc) - Might want to optimize this.\n \tfor tseq := seq + 1; tseq <= ss.Last; tseq++ {\n-\t\tif sm, _ := mb.cacheLookupWithLock(tseq); sm != nil {\n+\t\tif sm, _ := mb.cacheLookup(tseq); sm != nil {\n \t\t\tif sm.subj == subj {\n \t\t\t\tss.First = tseq\n \t\t\t\treturn", "y": 0, "oldf": "// Copyright 2019-2021 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"archive/tar\"\n\t\"bytes\"\n\t\"crypto/cipher\"\n\t\"crypto/rand\"\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\tmrand \"math/rand\"\n\n\t\"github.com/klauspost/compress/s2\"\n\t\"github.com/minio/highwayhash\"\n\t\"golang.org/x/crypto/chacha20\"\n\t\"golang.org/x/crypto/chacha20poly1305\"\n)\n\ntype FileStoreConfig struct {\n\t// Where the parent directory for all storage will be located.\n\tStoreDir string\n\t// BlockSize is the file block size. This also represents the maximum overhead size.\n\tBlockSize uint64\n\t// CacheExpire is how long with no activity until we expire the cache.\n\tCacheExpire time.Duration\n\t// SyncInterval is how often we sync to disk in the background.\n\tSyncInterval time.Duration\n\t// AsyncFlush allows async flush to batch write operations.\n\tAsyncFlush bool\n}\n\n// FileStreamInfo allows us to remember created time.\ntype FileStreamInfo struct {\n\tCreated time.Time\n\tStreamConfig\n}\n\n// File ConsumerInfo is used for creating consumer stores.\ntype FileConsumerInfo struct {\n\tCreated time.Time\n\tName    string\n\tConsumerConfig\n}\n\n// Default file and directory permissions.\nconst (\n\tdefaultDirPerms  = os.FileMode(0750)\n\tdefaultFilePerms = os.FileMode(0640)\n)\n\ntype fileStore struct {\n\tmu      sync.RWMutex\n\tstate   StreamState\n\tld      *LostStreamData\n\tscb     StorageUpdateHandler\n\tageChk  *time.Timer\n\tsyncTmr *time.Timer\n\tcfg     FileStreamInfo\n\tfcfg    FileStoreConfig\n\tprf     keyGen\n\taek     cipher.AEAD\n\tlmb     *msgBlock\n\tblks    []*msgBlock\n\thh      hash.Hash64\n\tqch     chan struct{}\n\tcfs     []*consumerFileStore\n\tsips    int\n\tclosed  bool\n\tfip     bool\n\ttms     bool\n}\n\n// Represents a message store block and its data.\ntype msgBlock struct {\n\t// Here for 32bit systems and atomic.\n\tfirst   msgId\n\tlast    msgId\n\tmu      sync.RWMutex\n\tfs      *fileStore\n\taek     cipher.AEAD\n\tbek     *chacha20.Cipher\n\tseed    []byte\n\tnonce   []byte\n\tmfn     string\n\tmfd     *os.File\n\tifn     string\n\tifd     *os.File\n\tliwsz   int64\n\tindex   uint64\n\tbytes   uint64 // User visible bytes count.\n\trbytes  uint64 // Total bytes (raw) including deleted. Used for rolling to new blk.\n\tmsgs    uint64 // User visible message count.\n\tfss     map[string]*SimpleState\n\tsfn     string\n\tlwits   int64\n\tlwts    int64\n\tllts    int64\n\tlrts    int64\n\tllseq   uint64\n\thh      hash.Hash64\n\tcache   *cache\n\tcloads  uint64\n\tcexp    time.Duration\n\tctmr    *time.Timer\n\twerr    error\n\tloading bool\n\tflusher bool\n\tdmap    map[uint64]struct{}\n\tfch     chan struct{}\n\tqch     chan struct{}\n\tlchk    [8]byte\n\tclosed  bool\n}\n\n// Write through caching layer that is also used on loading messages.\ntype cache struct {\n\tbuf   []byte\n\toff   int\n\twp    int\n\tidx   []uint32\n\tlrl   uint32\n\tfseq  uint64\n\tflush bool\n}\n\ntype msgId struct {\n\tseq uint64\n\tts  int64\n}\n\ntype fileStoredMsg struct {\n\tsubj string\n\thdr  []byte\n\tmsg  []byte\n\tseq  uint64\n\tts   int64 // nanoseconds\n\tmb   *msgBlock\n\toff  int64 // offset into block file\n}\n\nconst (\n\t// Magic is used to identify the file store files.\n\tmagic = uint8(22)\n\t// Version\n\tversion = uint8(1)\n\t// hdrLen\n\thdrLen = 2\n\t// This is where we keep the streams.\n\tstreamsDir = \"streams\"\n\t// This is where we keep the message store blocks.\n\tmsgDir = \"msgs\"\n\t// This is where we temporarily move the messages dir.\n\tpurgeDir = \"__msgs__\"\n\t// used to scan blk file names.\n\tblkScan = \"%d.blk\"\n\t// used for compacted blocks that are staged.\n\tnewScan = \"%d.new\"\n\t// used to scan index file names.\n\tindexScan = \"%d.idx\"\n\t// used to load per subject meta information.\n\tfssScan = \"%d.fss\"\n\t// used to store our block encryption key.\n\tkeyScan = \"%d.key\"\n\t// This is where we keep state on consumers.\n\tconsumerDir = \"obs\"\n\t// Index file for a consumer.\n\tconsumerState = \"o.dat\"\n\t// This is where we keep state on templates.\n\ttmplsDir = \"templates\"\n\t// Maximum size of a write buffer we may consider for re-use.\n\tmaxBufReuse = 2 * 1024 * 1024\n\t// default cache buffer expiration\n\tdefaultCacheBufferExpiration = 5 * time.Second\n\t// cache idx expiration\n\tdefaultCacheIdxExpiration = 5 * time.Minute\n\t// default sync interval\n\tdefaultSyncInterval = 60 * time.Second\n\t// coalesceMinimum\n\tcoalesceMinimum = 16 * 1024\n\t// maxFlushWait is maximum we will wait to gather messages to flush.\n\tmaxFlushWait = 8 * time.Millisecond\n\n\t// Metafiles for streams and consumers.\n\tJetStreamMetaFile    = \"meta.inf\"\n\tJetStreamMetaFileSum = \"meta.sum\"\n\tJetStreamMetaFileKey = \"meta.key\"\n\n\t// AEK key sizes\n\tmetaKeySize = 72\n\tblkKeySize  = 72\n\n\t// Default stream block size.\n\tdefaultStreamBlockSize = 16 * 1024 * 1024 // 16MB\n\t// Default for workqueue or interest based.\n\tdefaultOtherBlockSize = 8 * 1024 * 1024 // 8MB\n\t// Default for KV based\n\tdefaultKVBlockSize = 8 * 1024 * 1024 // 8MB\n\t// max block size for now.\n\tmaxBlockSize = defaultStreamBlockSize\n\t// Compact minimum threshold.\n\tcompactMinimum = 2 * 1024 * 1024 // 2MB\n\t// FileStoreMinBlkSize is minimum size we will do for a blk size.\n\tFileStoreMinBlkSize = 32 * 1000 // 32kib\n\t// FileStoreMaxBlkSize is maximum size we will do for a blk size.\n\tFileStoreMaxBlkSize = maxBlockSize\n)\n\nfunc newFileStore(fcfg FileStoreConfig, cfg StreamConfig) (*fileStore, error) {\n\treturn newFileStoreWithCreated(fcfg, cfg, time.Now().UTC(), nil)\n}\n\nfunc newFileStoreWithCreated(fcfg FileStoreConfig, cfg StreamConfig, created time.Time, prf keyGen) (*fileStore, error) {\n\tif cfg.Name == _EMPTY_ {\n\t\treturn nil, fmt.Errorf(\"name required\")\n\t}\n\tif cfg.Storage != FileStorage {\n\t\treturn nil, fmt.Errorf(\"fileStore requires file storage type in config\")\n\t}\n\t// Default values.\n\tif fcfg.BlockSize == 0 {\n\t\tfcfg.BlockSize = dynBlkSize(cfg.Retention, cfg.MaxBytes)\n\t}\n\tif fcfg.BlockSize > maxBlockSize {\n\t\treturn nil, fmt.Errorf(\"filestore max block size is %s\", friendlyBytes(maxBlockSize))\n\t}\n\tif fcfg.CacheExpire == 0 {\n\t\tfcfg.CacheExpire = defaultCacheBufferExpiration\n\t}\n\tif fcfg.SyncInterval == 0 {\n\t\tfcfg.SyncInterval = defaultSyncInterval\n\t}\n\n\t// Check the directory\n\tif stat, err := os.Stat(fcfg.StoreDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(fcfg.StoreDir, defaultDirPerms); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"could not create storage directory - %v\", err)\n\t\t}\n\t} else if stat == nil || !stat.IsDir() {\n\t\treturn nil, fmt.Errorf(\"storage directory is not a directory\")\n\t}\n\ttmpfile, err := ioutil.TempFile(fcfg.StoreDir, \"_test_\")\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"storage directory is not writable\")\n\t}\n\ttmpfile.Close()\n\tos.Remove(tmpfile.Name())\n\n\tfs := &fileStore{\n\t\tfcfg: fcfg,\n\t\tcfg:  FileStreamInfo{Created: created, StreamConfig: cfg},\n\t\tprf:  prf,\n\t\tqch:  make(chan struct{}),\n\t}\n\n\t// Set flush in place to AsyncFlush which by default is false.\n\tfs.fip = !fcfg.AsyncFlush\n\n\t// Check if this is a new setup.\n\tmdir := path.Join(fcfg.StoreDir, msgDir)\n\todir := path.Join(fcfg.StoreDir, consumerDir)\n\tif err := os.MkdirAll(mdir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create message storage directory - %v\", err)\n\t}\n\tif err := os.MkdirAll(odir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create consumer storage directory - %v\", err)\n\t}\n\n\t// Create highway hash for message blocks. Use sha256 of directory as key.\n\tkey := sha256.Sum256([]byte(cfg.Name))\n\tfs.hh, err = highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\n\t// Always track per subject information.\n\tfs.tms = true\n\n\t// Recover our message state.\n\tif err := fs.recoverMsgs(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Write our meta data iff does not exist.\n\tmeta := path.Join(fcfg.StoreDir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && os.IsNotExist(err) {\n\t\tif err := fs.writeStreamMeta(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\t// If we expect to be encrypted check that what we are restoring is not plaintext.\n\t// This can happen on snapshot restores or conversions.\n\tif fs.prf != nil {\n\t\tkeyFile := path.Join(fs.fcfg.StoreDir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && os.IsNotExist(err) {\n\t\t\tif err := fs.writeStreamMeta(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tfs.syncTmr = time.AfterFunc(fs.fcfg.SyncInterval, fs.syncBlocks)\n\n\treturn fs, nil\n}\n\nfunc (fs *fileStore) UpdateConfig(cfg *StreamConfig) error {\n\tif fs.isClosed() {\n\t\treturn ErrStoreClosed\n\t}\n\tif cfg.Name == _EMPTY_ {\n\t\treturn fmt.Errorf(\"name required\")\n\t}\n\tif cfg.Storage != FileStorage {\n\t\treturn fmt.Errorf(\"fileStore requires file storage type in config\")\n\t}\n\n\tfs.mu.Lock()\n\tnew_cfg := FileStreamInfo{Created: fs.cfg.Created, StreamConfig: *cfg}\n\told_cfg := fs.cfg\n\tfs.cfg = new_cfg\n\tif err := fs.writeStreamMeta(); err != nil {\n\t\tfs.cfg = old_cfg\n\t\tfs.mu.Unlock()\n\t\treturn err\n\t}\n\n\t// Limits checks and enforcement.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Do age timers.\n\tif fs.ageChk == nil && fs.cfg.MaxAge != 0 {\n\t\tfs.startAgeChk()\n\t}\n\tif fs.ageChk != nil && fs.cfg.MaxAge == 0 {\n\t\tfs.ageChk.Stop()\n\t\tfs.ageChk = nil\n\t}\n\tfs.mu.Unlock()\n\n\tif cfg.MaxAge != 0 {\n\t\tfs.expireMsgs()\n\t}\n\treturn nil\n}\n\nfunc dynBlkSize(retention RetentionPolicy, maxBytes int64) uint64 {\n\tif maxBytes > 0 {\n\t\tblkSize := (maxBytes / 4) + 1 // (25% overhead)\n\t\t// Round up to nearest 100\n\t\tif m := blkSize % 100; m != 0 {\n\t\t\tblkSize += 100 - m\n\t\t}\n\t\tif blkSize < FileStoreMinBlkSize {\n\t\t\tblkSize = FileStoreMinBlkSize\n\t\t}\n\t\tif blkSize > FileStoreMaxBlkSize {\n\t\t\tblkSize = FileStoreMaxBlkSize\n\t\t}\n\t\treturn uint64(blkSize)\n\t}\n\n\tif retention == LimitsPolicy {\n\t\t// TODO(dlc) - Make the blocksize relative to this if set.\n\t\treturn defaultStreamBlockSize\n\t} else {\n\t\t// TODO(dlc) - Make the blocksize relative to this if set.\n\t\treturn defaultOtherBlockSize\n\t}\n}\n\n// Generate an asset encryption key from the context and server PRF.\nfunc (fs *fileStore) genEncryptionKeys(context string) (aek cipher.AEAD, bek *chacha20.Cipher, seed, encrypted []byte, err error) {\n\tif fs.prf == nil {\n\t\treturn nil, nil, nil, nil, errNoEncryption\n\t}\n\t// Generate key encryption key.\n\trb, err := fs.prf([]byte(context))\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\tkek, err := chacha20poly1305.NewX(rb)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\t// Generate random asset encryption key seed.\n\tseed = make([]byte, 32)\n\tif n, err := rand.Read(seed); err != nil || n != 32 {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\taek, err = chacha20poly1305.NewX(seed)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\t// Generate our nonce. Use same buffer to hold encrypted seed.\n\tnonce := make([]byte, kek.NonceSize(), kek.NonceSize()+len(seed)+kek.Overhead())\n\tmrand.Read(nonce)\n\tbek, err = chacha20.NewUnauthenticatedCipher(seed[:], nonce)\n\tif err != nil {\n\t\treturn nil, nil, nil, nil, err\n\t}\n\n\treturn aek, bek, seed, kek.Seal(nonce, nonce, seed, nil), nil\n}\n\n// Write out meta and the checksum.\n// Lock should be held.\nfunc (fs *fileStore) writeStreamMeta() error {\n\tif fs.prf != nil && fs.aek == nil {\n\t\tkey, _, _, encrypted, err := fs.genEncryptionKeys(fs.cfg.Name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfs.aek = key\n\t\tkeyFile := path.Join(fs.fcfg.StoreDir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\tif err := ioutil.WriteFile(keyFile, encrypted, defaultFilePerms); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tmeta := path.Join(fs.fcfg.StoreDir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\tb, err := json.Marshal(fs.cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Encrypt if needed.\n\tif fs.aek != nil {\n\t\tnonce := make([]byte, fs.aek.NonceSize(), fs.aek.NonceSize()+len(b)+fs.aek.Overhead())\n\t\tmrand.Read(nonce)\n\t\tb = fs.aek.Seal(nonce, nonce, b, nil)\n\t}\n\n\tif err := ioutil.WriteFile(meta, b, defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\tfs.hh.Reset()\n\tfs.hh.Write(b)\n\tchecksum := hex.EncodeToString(fs.hh.Sum(nil))\n\tsum := path.Join(fs.fcfg.StoreDir, JetStreamMetaFileSum)\n\tif err := ioutil.WriteFile(sum, []byte(checksum), defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nconst msgHdrSize = 22\nconst checksumSize = 8\n\n// This is the max room needed for index header.\nconst indexHdrSize = 7*binary.MaxVarintLen64 + hdrLen + checksumSize\n\nfunc (fs *fileStore) recoverMsgBlock(fi os.FileInfo, index uint64) (*msgBlock, error) {\n\tmb := &msgBlock{fs: fs, index: index, cexp: fs.fcfg.CacheExpire}\n\n\tmdir := path.Join(fs.fcfg.StoreDir, msgDir)\n\tmb.mfn = path.Join(mdir, fi.Name())\n\tmb.ifn = path.Join(mdir, fmt.Sprintf(indexScan, index))\n\tmb.sfn = path.Join(mdir, fmt.Sprintf(fssScan, index))\n\n\tif mb.hh == nil {\n\t\tkey := sha256.Sum256(fs.hashKeyForBlock(index))\n\t\tmb.hh, _ = highwayhash.New64(key[:])\n\t}\n\n\tvar createdKeys bool\n\n\t// Check if encryption is enabled.\n\tif fs.prf != nil {\n\t\tekey, err := ioutil.ReadFile(path.Join(mdir, fmt.Sprintf(keyScan, mb.index)))\n\t\tif err != nil {\n\t\t\t// We do not seem to have keys even though we should. Could be a plaintext conversion.\n\t\t\t// Create the keys and we will double check below.\n\t\t\tif err := fs.genEncryptionKeysForBlock(mb); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tcreatedKeys = true\n\t\t} else {\n\t\t\tif len(ekey) != blkKeySize {\n\t\t\t\treturn nil, errBadKeySize\n\t\t\t}\n\t\t\t// Recover key encryption key.\n\t\t\trb, err := fs.prf([]byte(fmt.Sprintf(\"%s:%d\", fs.cfg.Name, mb.index)))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tkek, err := chacha20poly1305.NewX(rb)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tns := kek.NonceSize()\n\t\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tmb.seed, mb.nonce = seed, ekey[:ns]\n\t\t\tif mb.aek, err = chacha20poly1305.NewX(seed); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif mb.bek, err = chacha20.NewUnauthenticatedCipher(seed, ekey[:ns]); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// If we created keys here, let's check the data and if it is plaintext convert here.\n\tif createdKeys {\n\t\tbuf, err := mb.loadBlock(nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := mb.indexCacheBuf(buf); err != nil {\n\t\t\t// This likely indicates this was already encrypted or corrupt.\n\t\t\tmb.cache = nil\n\t\t\treturn nil, err\n\t\t}\n\t\t// Undo cache from above for later.\n\t\tmb.cache = nil\n\t\twbek, err := chacha20.NewUnauthenticatedCipher(mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\twbek.XORKeyStream(buf, buf)\n\t\tif err := ioutil.WriteFile(mb.mfn, buf, defaultFilePerms); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif buf, err = ioutil.ReadFile(mb.ifn); err == nil && len(buf) > 0 {\n\t\t\tif err := checkHeader(buf); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tbuf = mb.aek.Seal(buf[:0], mb.nonce, buf, nil)\n\t\t\tif err := ioutil.WriteFile(mb.ifn, buf, defaultFilePerms); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Open up the message file, but we will try to recover from the index file.\n\t// We will check that the last checksums match.\n\tfile, err := os.Open(mb.mfn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer file.Close()\n\n\tif fi, err := file.Stat(); fi != nil {\n\t\tmb.rbytes = uint64(fi.Size())\n\t} else {\n\t\treturn nil, err\n\t}\n\t// Grab last checksum from main block file.\n\tvar lchk [8]byte\n\tfile.ReadAt(lchk[:], fi.Size()-8)\n\tfile.Close()\n\n\t// Read our index file. Use this as source of truth if possible.\n\tif err := mb.readIndexInfo(); err == nil {\n\t\t// Quick sanity check here.\n\t\t// Note this only checks that the message blk file is not newer then this file.\n\t\tif bytes.Equal(lchk[:], mb.lchk[:]) {\n\t\t\tif fs.tms {\n\t\t\t\tmb.readPerSubjectInfo()\n\t\t\t}\n\t\t\tfs.blks = append(fs.blks, mb)\n\t\t\treturn mb, nil\n\t\t}\n\t}\n\n\t// If we get data loss rebuilding the message block state record that with the fs itself.\n\tif ld, _ := mb.rebuildState(); ld != nil {\n\t\tfs.rebuildState(ld)\n\t}\n\n\t// Rewrite this to make sure we are sync'd.\n\tmb.writeIndexInfo()\n\tfs.blks = append(fs.blks, mb)\n\tfs.lmb = mb\n\treturn mb, nil\n}\n\nfunc (fs *fileStore) lostData() *LostStreamData {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\tif fs.ld == nil {\n\t\treturn nil\n\t}\n\tnld := *fs.ld\n\treturn &nld\n}\n\nfunc (fs *fileStore) rebuildState(ld *LostStreamData) {\n\tif fs.ld != nil {\n\t\tfs.ld.Msgs = append(fs.ld.Msgs, ld.Msgs...)\n\t\tmsgs := fs.ld.Msgs\n\t\tsort.Slice(msgs, func(i, j int) bool { return msgs[i] < msgs[j] })\n\t\tfs.ld.Bytes += ld.Bytes\n\t} else {\n\t\tfs.ld = ld\n\t}\n\tfs.state.Msgs, fs.state.Bytes = 0, 0\n\tfs.state.FirstSeq, fs.state.LastSeq = 0, 0\n\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tfs.state.Msgs += mb.msgs\n\t\tfs.state.Bytes += mb.bytes\n\t\tif fs.state.FirstSeq == 0 || mb.first.seq < fs.state.FirstSeq {\n\t\t\tfs.state.FirstSeq = mb.first.seq\n\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t}\n\t\tfs.state.LastSeq = mb.last.seq\n\t\tfs.state.LastTime = time.Unix(0, mb.last.ts).UTC()\n\t\tmb.mu.RUnlock()\n\t}\n}\n\nfunc (mb *msgBlock) rebuildState() (*LostStreamData, error) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.rebuildStateLocked()\n}\n\nfunc (mb *msgBlock) rebuildStateLocked() (*LostStreamData, error) {\n\tstartLastSeq := mb.last.seq\n\n\t// Clear state we need to rebuild.\n\tmb.msgs, mb.bytes, mb.rbytes, mb.fss = 0, 0, 0, nil\n\tmb.last.seq, mb.last.ts = 0, 0\n\tfirstNeedsSet := true\n\n\tbuf, err := mb.loadBlock(nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Check if we need to decrypt.\n\tif mb.bek != nil && len(buf) > 0 {\n\t\t// Recreate to reset counter.\n\t\tmb.bek, err = chacha20.NewUnauthenticatedCipher(mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmb.bek.XORKeyStream(buf, buf)\n\t}\n\n\tmb.rbytes = uint64(len(buf))\n\n\taddToDmap := func(seq uint64) {\n\t\tif seq == 0 {\n\t\t\treturn\n\t\t}\n\t\tif mb.dmap == nil {\n\t\t\tmb.dmap = make(map[uint64]struct{})\n\t\t}\n\t\tmb.dmap[seq] = struct{}{}\n\t}\n\n\tvar le = binary.LittleEndian\n\n\ttruncate := func(index uint32) {\n\t\tvar fd *os.File\n\t\tif mb.mfd != nil {\n\t\t\tfd = mb.mfd\n\t\t} else {\n\t\t\tfd, err = os.OpenFile(mb.mfn, os.O_RDWR, defaultFilePerms)\n\t\t\tif err != nil {\n\t\t\t\tdefer fd.Close()\n\t\t\t}\n\t\t}\n\t\tif fd == nil {\n\t\t\treturn\n\t\t}\n\t\tif err := fd.Truncate(int64(index)); err == nil {\n\t\t\t// Update our checksum.\n\t\t\tif index >= 8 {\n\t\t\t\tvar lchk [8]byte\n\t\t\t\tfd.ReadAt(lchk[:], int64(index-8))\n\t\t\t\tcopy(mb.lchk[0:], lchk[:])\n\t\t\t}\n\t\t\tfd.Sync()\n\t\t}\n\t}\n\n\tgatherLost := func(lb uint32) *LostStreamData {\n\t\tvar ld LostStreamData\n\t\tfor seq := mb.last.seq + 1; seq <= startLastSeq; seq++ {\n\t\t\tld.Msgs = append(ld.Msgs, seq)\n\t\t}\n\t\tld.Bytes = uint64(lb)\n\t\treturn &ld\n\t}\n\n\t// Rebuild per subject info.\n\tif mb.fs.tms {\n\t\tmb.fss = make(map[string]*SimpleState)\n\t}\n\n\tfor index, lbuf := uint32(0), uint32(len(buf)); index < lbuf; {\n\t\tif index+msgHdrSize >= lbuf {\n\t\t\ttruncate(index)\n\t\t\treturn gatherLost(lbuf - index), nil\n\t\t}\n\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl := le.Uint32(hdr[0:])\n\t\tslen := le.Uint16(hdr[20:])\n\n\t\thasHeaders := rl&hbit != 0\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || int(slen) > dlen || dlen > int(rl) {\n\t\t\ttruncate(index)\n\t\t\treturn gatherLost(lbuf - index), errBadMsg\n\t\t}\n\n\t\tif index+rl > lbuf {\n\t\t\ttruncate(index)\n\t\t\treturn gatherLost(lbuf - index), errBadMsg\n\t\t}\n\n\t\tseq := le.Uint64(hdr[4:])\n\t\tts := int64(le.Uint64(hdr[12:]))\n\n\t\t// This is an old erased message, or a new one that we can track.\n\t\tif seq == 0 || seq&ebit != 0 || seq < mb.first.seq {\n\t\t\tseq = seq &^ ebit\n\t\t\taddToDmap(seq)\n\t\t\tindex += rl\n\t\t\tcontinue\n\t\t}\n\n\t\t// This is for when we have index info that adjusts for deleted messages\n\t\t// at the head. So the first.seq will be already set here. If this is larger\n\t\t// replace what we have with this seq.\n\t\tif firstNeedsSet && seq > mb.first.seq {\n\t\t\tfirstNeedsSet = false\n\t\t\tmb.first.seq = seq\n\t\t\tmb.first.ts = ts\n\t\t}\n\n\t\tvar deleted bool\n\t\tif mb.dmap != nil {\n\t\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\t\tdeleted = true\n\t\t\t}\n\t\t}\n\n\t\tif !deleted {\n\t\t\tdata := buf[index+msgHdrSize : index+rl]\n\t\t\tif hh := mb.hh; hh != nil {\n\t\t\t\thh.Reset()\n\t\t\t\thh.Write(hdr[4:20])\n\t\t\t\thh.Write(data[:slen])\n\t\t\t\tif hasHeaders {\n\t\t\t\t\thh.Write(data[slen+4 : dlen-8])\n\t\t\t\t} else {\n\t\t\t\t\thh.Write(data[slen : dlen-8])\n\t\t\t\t}\n\t\t\t\tchecksum := hh.Sum(nil)\n\t\t\t\tif !bytes.Equal(checksum, data[len(data)-8:]) {\n\t\t\t\t\ttruncate(index)\n\t\t\t\t\treturn gatherLost(lbuf - index), errBadMsg\n\t\t\t\t}\n\t\t\t\tcopy(mb.lchk[0:], checksum)\n\t\t\t}\n\n\t\t\tif firstNeedsSet {\n\t\t\t\tfirstNeedsSet = false\n\t\t\t\tmb.first.seq = seq\n\t\t\t\tmb.first.ts = ts\n\t\t\t}\n\t\t\tmb.last.seq = seq\n\t\t\tmb.last.ts = ts\n\n\t\t\tmb.msgs++\n\t\t\tmb.bytes += uint64(rl)\n\n\t\t\t// Do per subject info.\n\t\t\tif mb.fss != nil {\n\t\t\t\tif subj := string(data[:slen]); len(subj) > 0 {\n\t\t\t\t\tif ss := mb.fss[subj]; ss != nil {\n\t\t\t\t\t\tss.Msgs++\n\t\t\t\t\t\tss.Last = seq\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmb.fss[subj] = &SimpleState{Msgs: 1, First: seq, Last: seq}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Advance to next record.\n\t\tindex += rl\n\t}\n\n\t// For empty msg blocks make sure we recover last seq correctly based off of first.\n\tif mb.msgs == 0 && mb.first.seq > 0 {\n\t\tmb.last.seq = mb.first.seq - 1\n\t}\n\n\treturn nil, nil\n}\n\nfunc (fs *fileStore) recoverMsgs() error {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Check for any left over purged messages.\n\tpdir := path.Join(fs.fcfg.StoreDir, purgeDir)\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\n\tmdir := path.Join(fs.fcfg.StoreDir, msgDir)\n\tfis, err := ioutil.ReadDir(mdir)\n\tif err != nil {\n\t\treturn errNotReadable\n\t}\n\n\t// Recover all of the msg blocks.\n\t// These can come in a random order, so account for that.\n\tfor _, fi := range fis {\n\t\tvar index uint64\n\t\tif n, err := fmt.Sscanf(fi.Name(), blkScan, &index); err == nil && n == 1 {\n\t\t\tif mb, err := fs.recoverMsgBlock(fi, index); err == nil && mb != nil {\n\t\t\t\tif fs.state.FirstSeq == 0 || mb.first.seq < fs.state.FirstSeq {\n\t\t\t\t\tfs.state.FirstSeq = mb.first.seq\n\t\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t\t}\n\t\t\t\tif mb.last.seq > fs.state.LastSeq {\n\t\t\t\t\tfs.state.LastSeq = mb.last.seq\n\t\t\t\t\tfs.state.LastTime = time.Unix(0, mb.last.ts).UTC()\n\t\t\t\t}\n\t\t\t\tfs.state.Msgs += mb.msgs\n\t\t\t\tfs.state.Bytes += mb.bytes\n\t\t\t} else {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now make sure to sort blks for efficient lookup later with selectMsgBlock().\n\tif len(fs.blks) > 0 {\n\t\tsort.Slice(fs.blks, func(i, j int) bool { return fs.blks[i].index < fs.blks[j].index })\n\t\tfs.lmb = fs.blks[len(fs.blks)-1]\n\t\terr = fs.enableLastMsgBlockForWriting()\n\t} else {\n\t\t_, err = fs.newMsgBlockForWrite()\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Limits checks and enforcement.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Do age checks too, make sure to call in place.\n\tif fs.cfg.MaxAge != 0 {\n\t\tfs.expireMsgsOnRecover()\n\t\tfs.startAgeChk()\n\t}\n\n\treturn nil\n}\n\n// Will expire msgs that have aged out on restart.\n// We will treat this differently in case we have a recovery\n// that will expire alot of messages on startup. Should only be called\n// on startup. Lock should be held.\nfunc (fs *fileStore) expireMsgsOnRecover() {\n\tif fs.state.Msgs == 0 {\n\t\treturn\n\t}\n\n\tvar minAge = time.Now().UnixNano() - int64(fs.cfg.MaxAge)\n\tvar purged, bytes uint64\n\tvar deleted int\n\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t\tif minAge < mb.first.ts {\n\t\t\tmb.mu.Unlock()\n\t\t\tbreak\n\t\t}\n\t\t// Can we remove whole block here?\n\t\tif mb.last.ts <= minAge {\n\t\t\tpurged += mb.msgs\n\t\t\tbytes += mb.bytes\n\t\t\tmb.dirtyCloseWithRemove(true)\n\t\t\tnewFirst := mb.last.seq + 1\n\t\t\tmb.mu.Unlock()\n\t\t\t// Update fs first here as well.\n\t\t\tfs.state.FirstSeq = newFirst\n\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\tdeleted++\n\t\t\tcontinue\n\t\t}\n\n\t\t// If we are here we have to process the interior messages of this blk.\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\tbreak\n\t\t}\n\n\t\t// Walk messages and remove if expired.\n\t\tfor seq := mb.first.seq; seq <= mb.last.seq; seq++ {\n\t\t\tsm, err := mb.cacheLookupWithLock(seq)\n\t\t\t// Process interior deleted msgs.\n\t\t\tif err == errDeletedMsg {\n\t\t\t\t// Update dmap.\n\t\t\t\tif len(mb.dmap) > 0 {\n\t\t\t\t\tdelete(mb.dmap, seq)\n\t\t\t\t\tif len(mb.dmap) == 0 {\n\t\t\t\t\t\tmb.dmap = nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Break on other errors.\n\t\t\tif err != nil || sm == nil {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// No error and sm != nil from here onward.\n\n\t\t\t// Check for done.\n\t\t\tif sm.ts > minAge {\n\t\t\t\tmb.first.seq = sm.seq\n\t\t\t\tmb.first.ts = sm.ts\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// Delete the message here.\n\t\t\tsz := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\tmb.bytes -= sz\n\t\t\tbytes += sz\n\t\t\tmb.msgs--\n\t\t\tpurged++\n\t\t\t// Update fss\n\t\t\tmb.removeSeqPerSubject(sm.subj, seq)\n\t\t}\n\n\t\t// Check if empty after processing, could happen if tail of messages are all deleted.\n\t\tisEmpty := mb.msgs == 0\n\t\tif isEmpty {\n\t\t\tmb.dirtyCloseWithRemove(true)\n\t\t\t// Update fs first here as well.\n\t\t\tfs.state.FirstSeq = mb.last.seq + 1\n\t\t\tfs.state.FirstTime = time.Time{}\n\t\t\tdeleted++\n\t\t} else {\n\t\t\t// Update fs first seq and time.\n\t\t\tfs.state.FirstSeq = mb.first.seq\n\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t}\n\t\tmb.mu.Unlock()\n\n\t\tif !isEmpty {\n\t\t\t// Make sure to write out our index info.\n\t\t\tmb.writeIndexInfo()\n\t\t}\n\t\tbreak\n\t}\n\n\tif deleted > 0 {\n\t\t// Update blks slice.\n\t\tfs.blks = append(fs.blks[:0:0], fs.blks[deleted:]...)\n\t\tif lb := len(fs.blks); lb == 0 {\n\t\t\tfs.lmb = nil\n\t\t} else {\n\t\t\tfs.lmb = fs.blks[lb-1]\n\t\t\tfs.enableLastMsgBlockForWriting()\n\t\t}\n\t}\n\t// Update top level accounting.\n\tfs.state.Msgs -= purged\n\tfs.state.Bytes -= bytes\n}\n\n// GetSeqFromTime looks for the first sequence number that has\n// the message with >= timestamp.\n// FIXME(dlc) - inefficient, and dumb really. Make this better.\nfunc (fs *fileStore) GetSeqFromTime(t time.Time) uint64 {\n\tfs.mu.RLock()\n\tlastSeq := fs.state.LastSeq\n\tclosed := fs.closed\n\tfs.mu.RUnlock()\n\n\tif closed {\n\t\treturn 0\n\t}\n\n\tmb := fs.selectMsgBlockForStart(t)\n\tif mb == nil {\n\t\treturn lastSeq + 1\n\t}\n\n\tmb.mu.RLock()\n\tfseq := mb.first.seq\n\tlseq := mb.last.seq\n\tmb.mu.RUnlock()\n\n\t// Linear search, hence the dumb part..\n\tts := t.UnixNano()\n\tfor seq := fseq; seq <= lseq; seq++ {\n\t\tsm, _ := mb.fetchMsg(seq)\n\t\tif sm != nil && sm.ts >= ts {\n\t\t\treturn sm.seq\n\t\t}\n\t}\n\treturn 0\n}\n\n// This will traverse a message block and generate the filtered pending.\nfunc (mb *msgBlock) filteredPending(subj string, wc bool, seq uint64) (total, first, last uint64) {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.filteredPendingLocked(subj, wc, seq)\n}\n\n// This will traverse a message block and generate the filtered pending.\n// Lock should be held.\nfunc (mb *msgBlock) filteredPendingLocked(subj string, wc bool, seq uint64) (total, first, last uint64) {\n\tif mb.fss == nil {\n\t\treturn 0, 0, 0\n\t}\n\n\tsubs := []string{subj}\n\t// If we have a wildcard match against all tracked subjects we know about.\n\tif wc {\n\t\tsubs = subs[:0]\n\t\tfor fsubj := range mb.fss {\n\t\t\tif subjectIsSubsetMatch(fsubj, subj) {\n\t\t\t\tsubs = append(subs, fsubj)\n\t\t\t}\n\t\t}\n\t}\n\t// If we load the cache for a linear scan we want to expire that cache upon exit.\n\tvar shouldExpire bool\n\n\tupdate := func(ss *SimpleState) {\n\t\ttotal += ss.Msgs\n\t\tif first == 0 || ss.First < first {\n\t\t\tfirst = ss.First\n\t\t}\n\t\tif ss.Last > last {\n\t\t\tlast = ss.Last\n\t\t}\n\t}\n\n\tfor i, subj := range subs {\n\t\t// If the starting seq is less then or equal that means we want all and we do not need to load any messages.\n\t\tss := mb.fss[subj]\n\t\tif ss == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// If the seq we are starting at is less then the simple state's first sequence we can just return the total msgs.\n\t\tif seq <= ss.First {\n\t\t\tupdate(ss)\n\t\t\tcontinue\n\t\t}\n\n\t\t// We may need to scan this one block since we have a partial set to consider.\n\t\t// If we are all inclusive then we can do simple math and avoid the scan.\n\t\tif allInclusive := ss.Msgs == ss.Last-ss.First+1; allInclusive {\n\t\t\tupdate(ss)\n\t\t\t// Make sure to compensate for the diff from the head.\n\t\t\tif seq > ss.First {\n\t\t\t\tfirst, total = seq, total-(seq-ss.First)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// We need to scan this block to compute the correct number of pending for this block.\n\t\t// We want to only do this once so we will adjust subs and test against them all here.\n\n\t\tif !mb.cacheAlreadyLoaded() {\n\t\t\tmb.loadMsgsWithLock()\n\t\t\tshouldExpire = true\n\t\t}\n\n\t\tsubs = subs[i:]\n\t\tvar all, lseq uint64\n\t\t// Grab last applicable sequence as a union of all applicable subjects.\n\t\tfor _, subj := range subs {\n\t\t\tif ss := mb.fss[subj]; ss != nil {\n\t\t\t\tall += ss.Msgs\n\t\t\t\tif ss.Last > lseq {\n\t\t\t\t\tlseq = ss.Last\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tnumScanIn, numScanOut := lseq-seq, seq-mb.first.seq\n\n\t\tisMatch := func(seq uint64) bool {\n\t\t\tif sm, _ := mb.cacheLookupWithLock(seq); sm != nil {\n\t\t\t\tif len(subs) == 1 && sm.subj == subs[0] {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t\tfor _, subj := range subs {\n\t\t\t\t\tif sm.subj == subj {\n\t\t\t\t\t\treturn true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn false\n\t\t}\n\n\t\t// Decide on whether to scan those included or those excluded based on which scan amount is less.\n\t\tif numScanIn < numScanOut {\n\t\t\tfor tseq := seq; tseq <= lseq; tseq++ {\n\t\t\t\tif isMatch(tseq) {\n\t\t\t\t\ttotal++\n\t\t\t\t\tif first == 0 || tseq < first {\n\t\t\t\t\t\tfirst = tseq\n\t\t\t\t\t}\n\t\t\t\t\tlast = tseq\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// Here its more efficient to scan the out nodes.\n\t\t\tvar discard uint64\n\t\t\tfor tseq := mb.first.seq; tseq < seq; tseq++ {\n\t\t\t\tif isMatch(tseq) {\n\t\t\t\t\tdiscard++\n\t\t\t\t}\n\t\t\t}\n\t\t\ttotal += (all - discard)\n\t\t\t// Now make sure we match our first\n\t\t\tfor tseq := seq; tseq <= lseq; tseq++ {\n\t\t\t\tif isMatch(tseq) {\n\t\t\t\t\tfirst = tseq\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// We can bail since we scanned all remaining in this pass.\n\t\tbreak\n\t}\n\n\t// If we loaded this block for this operation go ahead and expire it here.\n\tif shouldExpire {\n\t\tmb.expireCacheLocked()\n\t}\n\n\treturn total, first, last\n}\n\n// FilteredState will return the SimpleState associated with the filtered subject and a proposed starting sequence.\nfunc (fs *fileStore) FilteredState(sseq uint64, subj string) SimpleState {\n\tfs.mu.RLock()\n\tlseq := fs.state.LastSeq\n\tif sseq < fs.state.FirstSeq {\n\t\tsseq = fs.state.FirstSeq\n\t}\n\tfs.mu.RUnlock()\n\n\tvar ss SimpleState\n\n\t// If past the end no results.\n\tif sseq > lseq {\n\t\treturn ss\n\t}\n\n\t// If subj is empty or we are not tracking multiple subjects.\n\tif subj == _EMPTY_ || subj == fwcs || !fs.tms {\n\t\ttotal := lseq - sseq + 1\n\t\tif state := fs.State(); len(state.Deleted) > 0 {\n\t\t\tfor _, dseq := range state.Deleted {\n\t\t\t\tif dseq >= sseq && dseq <= lseq {\n\t\t\t\t\ttotal--\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tss.Msgs, ss.First, ss.Last = total, sseq, lseq\n\t\treturn ss\n\t}\n\n\twc := subjectHasWildcard(subj)\n\t// Are we tracking multiple subject states?\n\tif fs.tms {\n\t\tfor _, mb := range fs.blks {\n\t\t\t// Skip blocks that are less than our starting sequence.\n\t\t\tif sseq > atomic.LoadUint64(&mb.last.seq) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt, f, l := mb.filteredPending(subj, wc, sseq)\n\t\t\tss.Msgs += t\n\t\t\tif ss.First == 0 || (f > 0 && f < ss.First) {\n\t\t\t\tss.First = f\n\t\t\t}\n\t\t\tif l > ss.Last {\n\t\t\t\tss.Last = l\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// Fallback to linear scan.\n\t\teq := compareFn(subj)\n\t\tfor seq := sseq; seq <= lseq; seq++ {\n\t\t\tif sm, _ := fs.msgForSeq(seq); sm != nil && eq(sm.subj, subj) {\n\t\t\t\tss.Msgs++\n\t\t\t\tif ss.First == 0 {\n\t\t\t\t\tss.First = seq\n\t\t\t\t}\n\t\t\t\tss.Last = seq\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ss\n}\n\n// Will gather complete filtered state for the subject.\n// Lock should be held.\nfunc (fs *fileStore) perSubjectState(subj string) (total, first, last uint64) {\n\tif !fs.tms {\n\t\treturn\n\t}\n\twc := subjectHasWildcard(subj)\n\tfor _, mb := range fs.blks {\n\t\tt, f, l := mb.filteredPending(subj, wc, 1)\n\t\ttotal += t\n\t\tif first == 0 || (f > 0 && f < first) {\n\t\t\tfirst = f\n\t\t}\n\t\tif l > last {\n\t\t\tlast = l\n\t\t}\n\t}\n\treturn total, first, last\n}\n\n// SubjectsState returns a map of SimpleState for all matching subjects.\nfunc (fs *fileStore) SubjectsState(subject string) map[string]SimpleState {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tif !fs.tms || fs.state.Msgs == 0 {\n\t\treturn nil\n\t}\n\n\tfss := make(map[string]SimpleState)\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tfor subj, ss := range mb.fss {\n\t\t\tif subject == _EMPTY_ || subject == fwcs || subjectIsSubsetMatch(subj, subject) {\n\t\t\t\toss := fss[subj]\n\t\t\t\tif oss.First == 0 { // New\n\t\t\t\t\tfss[subj] = *ss\n\t\t\t\t} else {\n\t\t\t\t\t// Merge here.\n\t\t\t\t\toss.Last, oss.Msgs = ss.Last, oss.Msgs+ss.Msgs\n\t\t\t\t\tfss[subj] = oss\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tmb.mu.RUnlock()\n\t}\n\treturn fss\n}\n\n// RegisterStorageUpdates registers a callback for updates to storage changes.\n// It will present number of messages and bytes as a signed integer and an\n// optional sequence number of the message if a single.\nfunc (fs *fileStore) RegisterStorageUpdates(cb StorageUpdateHandler) {\n\tfs.mu.Lock()\n\tfs.scb = cb\n\tbsz := fs.state.Bytes\n\tfs.mu.Unlock()\n\tif cb != nil && bsz > 0 {\n\t\tcb(0, int64(bsz), 0, _EMPTY_)\n\t}\n}\n\n// Helper to get hash key for specific message block.\n// Lock should be held\nfunc (fs *fileStore) hashKeyForBlock(index uint64) []byte {\n\treturn []byte(fmt.Sprintf(\"%s-%d\", fs.cfg.Name, index))\n}\n\nfunc (mb *msgBlock) setupWriteCache(buf []byte) {\n\t// Make sure we have a cache setup.\n\tif mb.cache != nil {\n\t\treturn\n\t}\n\t// Setup simple cache.\n\tmb.cache = &cache{buf: buf}\n\t// Make sure we set the proper cache offset if we have existing data.\n\tvar fi os.FileInfo\n\tif mb.mfd != nil {\n\t\tfi, _ = mb.mfd.Stat()\n\t} else if mb.mfn != _EMPTY_ {\n\t\tfi, _ = os.Stat(mb.mfn)\n\t}\n\tif fi != nil {\n\t\tmb.cache.off = int(fi.Size())\n\t}\n\tmb.startCacheExpireTimer()\n}\n\n// This rolls to a new append msg block.\n// Lock should be held.\nfunc (fs *fileStore) newMsgBlockForWrite() (*msgBlock, error) {\n\tindex := uint64(1)\n\tvar rbuf []byte\n\n\tif lmb := fs.lmb; lmb != nil {\n\t\tindex = lmb.index + 1\n\n\t\t// Determine if we can reclaim resources here.\n\t\tif fs.fip {\n\t\t\t// Reset write timestamp and see if we can expire this cache.\n\t\t\tlmb.mu.Lock()\n\t\t\tlmb.closeFDsLocked()\n\t\t\tif lmb.cache != nil {\n\t\t\t\tlmb.lwts = 0\n\t\t\t\tbuf, llts := lmb.cache.buf, lmb.llts\n\t\t\t\tlmb.expireCacheLocked()\n\t\t\t\t// We could check for a certain time since last load, but to be safe just reuse if no loads.\n\t\t\t\tif llts == 0 && (lmb.cache == nil || lmb.cache.buf == nil) {\n\t\t\t\t\trbuf = buf\n\t\t\t\t}\n\t\t\t}\n\t\t\tlmb.mu.Unlock()\n\t\t}\n\t}\n\n\tmb := &msgBlock{fs: fs, index: index, cexp: fs.fcfg.CacheExpire}\n\n\t// Lock should be held to quiet race detector.\n\tmb.mu.Lock()\n\tmb.setupWriteCache(rbuf)\n\tif fs.tms {\n\t\tmb.fss = make(map[string]*SimpleState)\n\t}\n\tmb.mu.Unlock()\n\n\t// Now do local hash.\n\tkey := sha256.Sum256(fs.hashKeyForBlock(index))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\tmb.hh = hh\n\n\tmdir := path.Join(fs.fcfg.StoreDir, msgDir)\n\tmb.mfn = path.Join(mdir, fmt.Sprintf(blkScan, mb.index))\n\tmfd, err := os.OpenFile(mb.mfn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\tif err != nil {\n\t\tmb.dirtyCloseWithRemove(true)\n\t\treturn nil, fmt.Errorf(\"Error creating msg block file [%q]: %v\", mb.mfn, err)\n\t}\n\tmb.mfd = mfd\n\n\tmb.ifn = path.Join(mdir, fmt.Sprintf(indexScan, mb.index))\n\tifd, err := os.OpenFile(mb.ifn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\tif err != nil {\n\t\tmb.dirtyCloseWithRemove(true)\n\t\treturn nil, fmt.Errorf(\"Error creating msg index file [%q]: %v\", mb.mfn, err)\n\t}\n\tmb.ifd = ifd\n\n\t// For subject based info.\n\tmb.sfn = path.Join(mdir, fmt.Sprintf(fssScan, mb.index))\n\n\t// Check if encryption is enabled.\n\tif fs.prf != nil {\n\t\tif err := fs.genEncryptionKeysForBlock(mb); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Set cache time to creation time to start.\n\tts := time.Now().UnixNano()\n\t// Race detector wants these protected.\n\tmb.mu.Lock()\n\tmb.llts, mb.lwts = 0, ts\n\tmb.mu.Unlock()\n\n\t// Remember our last sequence number.\n\tmb.first.seq = fs.state.LastSeq + 1\n\tmb.last.seq = fs.state.LastSeq\n\n\t// If we know we will need this so go ahead and spin up.\n\tif !fs.fip {\n\t\tmb.spinUpFlushLoop()\n\t}\n\n\t// Add to our list of blocks and mark as last.\n\tfs.blks = append(fs.blks, mb)\n\tfs.lmb = mb\n\n\treturn mb, nil\n}\n\n// Generate the keys for this message block and write them out.\nfunc (fs *fileStore) genEncryptionKeysForBlock(mb *msgBlock) error {\n\tif mb == nil {\n\t\treturn nil\n\t}\n\tkey, bek, seed, encrypted, err := fs.genEncryptionKeys(fmt.Sprintf(\"%s:%d\", fs.cfg.Name, mb.index))\n\tif err != nil {\n\t\treturn err\n\t}\n\tmb.aek, mb.bek, mb.seed, mb.nonce = key, bek, seed, encrypted[:key.NonceSize()]\n\tmdir := path.Join(fs.fcfg.StoreDir, msgDir)\n\tkeyFile := path.Join(mdir, fmt.Sprintf(keyScan, mb.index))\n\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\tif err := ioutil.WriteFile(keyFile, encrypted, defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Make sure we can write to the last message block.\n// Lock should be held.\nfunc (fs *fileStore) enableLastMsgBlockForWriting() error {\n\tmb := fs.lmb\n\tif mb == nil {\n\t\treturn fmt.Errorf(\"no last message block assigned, can not enable for writing\")\n\t}\n\tif mb.mfd != nil {\n\t\treturn nil\n\t}\n\tmfd, err := os.OpenFile(mb.mfn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error opening msg block file [%q]: %v\", mb.mfn, err)\n\t}\n\tmb.mfd = mfd\n\n\t// Spin up our flusher loop if needed.\n\tif !fs.fip {\n\t\tmb.spinUpFlushLoop()\n\t}\n\n\treturn nil\n}\n\n// Stores a raw message with expected sequence number and timestamp.\n// Lock should be held.\nfunc (fs *fileStore) storeRawMsg(subj string, hdr, msg []byte, seq uint64, ts int64) error {\n\tif fs.closed {\n\t\treturn ErrStoreClosed\n\t}\n\n\t// Check if we are discarding new messages when we reach the limit.\n\tif fs.cfg.Discard == DiscardNew {\n\t\tif fs.cfg.MaxMsgs > 0 && fs.state.Msgs >= uint64(fs.cfg.MaxMsgs) {\n\t\t\treturn ErrMaxMsgs\n\t\t}\n\t\tif fs.cfg.MaxBytes > 0 && fs.state.Bytes+uint64(len(msg)+len(hdr)) >= uint64(fs.cfg.MaxBytes) {\n\t\t\treturn ErrMaxBytes\n\t\t}\n\t\tif fs.cfg.MaxMsgsPer > 0 && len(subj) > 0 {\n\t\t\tif msgs, _, _ := fs.perSubjectState(subj); msgs >= uint64(fs.cfg.MaxMsgsPer) {\n\t\t\t\treturn ErrMaxMsgsPerSubject\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check sequence.\n\tif seq != fs.state.LastSeq+1 {\n\t\tif seq > 0 {\n\t\t\treturn ErrSequenceMismatch\n\t\t}\n\t\tseq = fs.state.LastSeq + 1\n\t}\n\n\t// Write msg record.\n\tn, err := fs.writeMsgRecord(seq, ts, subj, hdr, msg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Adjust first if needed.\n\tnow := time.Unix(0, ts).UTC()\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq = seq\n\t\tfs.state.FirstTime = now\n\t}\n\n\tfs.state.Msgs++\n\tfs.state.Bytes += n\n\tfs.state.LastSeq = seq\n\tfs.state.LastTime = now\n\n\t// Enforce per message limits.\n\tif fs.cfg.MaxMsgsPer > 0 && len(subj) > 0 {\n\t\tfs.enforcePerSubjectLimit(subj)\n\t}\n\n\t// Limits checks and enforcement.\n\t// If they do any deletions they will update the\n\t// byte count on their own, so no need to compensate.\n\tfs.enforceMsgLimit()\n\tfs.enforceBytesLimit()\n\n\t// Check if we have and need the age expiration timer running.\n\tif fs.ageChk == nil && fs.cfg.MaxAge != 0 {\n\t\tfs.startAgeChk()\n\t}\n\n\treturn nil\n}\n\n// StoreRawMsg stores a raw message with expected sequence number and timestamp.\nfunc (fs *fileStore) StoreRawMsg(subj string, hdr, msg []byte, seq uint64, ts int64) error {\n\tfs.mu.Lock()\n\terr := fs.storeRawMsg(subj, hdr, msg, seq, ts)\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif err == nil && cb != nil {\n\t\tcb(1, int64(fileStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn err\n}\n\n// Store stores a message. We hold the main filestore lock for any write operation.\nfunc (fs *fileStore) StoreMsg(subj string, hdr, msg []byte) (uint64, int64, error) {\n\tfs.mu.Lock()\n\tseq, ts := fs.state.LastSeq+1, time.Now().UnixNano()\n\terr := fs.storeRawMsg(subj, hdr, msg, seq, ts)\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif err != nil {\n\t\tseq, ts = 0, 0\n\t} else if cb != nil {\n\t\tcb(1, int64(fileStoreMsgSize(subj, hdr, msg)), seq, subj)\n\t}\n\n\treturn seq, ts, err\n}\n\n// skipMsg will update this message block for a skipped message.\n// If we do not have any messages, just update the metadata, otherwise\n// we will place and empty record marking the sequence as used. The\n// sequence will be marked erased.\n// fs lock should be held.\nfunc (mb *msgBlock) skipMsg(seq uint64, now time.Time) {\n\tif mb == nil {\n\t\treturn\n\t}\n\tvar needsRecord bool\n\n\tmb.mu.Lock()\n\t// If we are empty can just do meta.\n\tif mb.msgs == 0 {\n\t\tmb.last.seq = seq\n\t\tmb.last.ts = now.UnixNano()\n\t\tmb.first.seq = seq + 1\n\t\tmb.first.ts = now.UnixNano()\n\t} else {\n\t\tneedsRecord = true\n\t\tif mb.dmap == nil {\n\t\t\tmb.dmap = make(map[uint64]struct{})\n\t\t}\n\t\tmb.dmap[seq] = struct{}{}\n\t\tmb.msgs--\n\t\tmb.bytes -= emptyRecordLen\n\t}\n\tmb.mu.Unlock()\n\n\tif needsRecord {\n\t\tmb.writeMsgRecord(emptyRecordLen, seq|ebit, _EMPTY_, nil, nil, now.UnixNano(), true)\n\t} else {\n\t\tmb.kickFlusher()\n\t}\n}\n\n// SkipMsg will use the next sequence number but not store anything.\nfunc (fs *fileStore) SkipMsg() uint64 {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\t// Grab time.\n\tnow := time.Now().UTC()\n\tseq := fs.state.LastSeq + 1\n\tfs.state.LastSeq = seq\n\tfs.state.LastTime = now\n\tif fs.state.Msgs == 0 {\n\t\tfs.state.FirstSeq = seq\n\t\tfs.state.FirstTime = now\n\t}\n\tif seq == fs.state.FirstSeq {\n\t\tfs.state.FirstSeq = seq + 1\n\t\tfs.state.FirstTime = now\n\t}\n\tfs.lmb.skipMsg(seq, now)\n\n\treturn seq\n}\n\n// Lock should be held.\nfunc (fs *fileStore) rebuildFirst() {\n\tif len(fs.blks) == 0 {\n\t\treturn\n\t}\n\tif fmb := fs.blks[0]; fmb != nil {\n\t\tfmb.removeIndexFile()\n\t\tfmb.rebuildState()\n\t\tfmb.writeIndexInfo()\n\t\tfs.selectNextFirst()\n\t}\n}\n\n// Will check the msg limit for this tracked subject.\n// Lock should be held.\nfunc (fs *fileStore) enforcePerSubjectLimit(subj string) {\n\tif fs.closed || fs.sips > 0 || fs.cfg.MaxMsgsPer < 0 || !fs.tms {\n\t\treturn\n\t}\n\tfor {\n\t\tmsgs, first, _ := fs.perSubjectState(subj)\n\t\tif msgs <= uint64(fs.cfg.MaxMsgsPer) {\n\t\t\treturn\n\t\t}\n\t\tif ok, _ := fs.removeMsg(first, false, false); !ok {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// Will check the msg limit and drop firstSeq msg if needed.\n// Lock should be held.\nfunc (fs *fileStore) enforceMsgLimit() {\n\tif fs.cfg.MaxMsgs <= 0 || fs.state.Msgs <= uint64(fs.cfg.MaxMsgs) {\n\t\treturn\n\t}\n\tfor nmsgs := fs.state.Msgs; nmsgs > uint64(fs.cfg.MaxMsgs); nmsgs = fs.state.Msgs {\n\t\tif removed, err := fs.deleteFirstMsg(); err != nil || !removed {\n\t\t\tfs.rebuildFirst()\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Will check the bytes limit and drop msgs if needed.\n// Lock should be held.\nfunc (fs *fileStore) enforceBytesLimit() {\n\tif fs.cfg.MaxBytes <= 0 || fs.state.Bytes <= uint64(fs.cfg.MaxBytes) {\n\t\treturn\n\t}\n\tfor bs := fs.state.Bytes; bs > uint64(fs.cfg.MaxBytes); bs = fs.state.Bytes {\n\t\tif removed, err := fs.deleteFirstMsg(); err != nil || !removed {\n\t\t\tfs.rebuildFirst()\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Lock should be held on entry but will be released during actual remove.\nfunc (fs *fileStore) deleteFirstMsg() (bool, error) {\n\tfs.mu.Unlock()\n\tdefer fs.mu.Lock()\n\treturn fs.removeMsg(fs.state.FirstSeq, false, true)\n}\n\n// RemoveMsg will remove the message from this store.\n// Will return the number of bytes removed.\nfunc (fs *fileStore) RemoveMsg(seq uint64) (bool, error) {\n\treturn fs.removeMsg(seq, false, true)\n}\n\nfunc (fs *fileStore) EraseMsg(seq uint64) (bool, error) {\n\treturn fs.removeMsg(seq, true, true)\n}\n\n// Remove a message, optionally rewriting the mb file.\nfunc (fs *fileStore) removeMsg(seq uint64, secure, needFSLock bool) (bool, error) {\n\tfsLock := func() {\n\t\tif needFSLock {\n\t\t\tfs.mu.Lock()\n\t\t}\n\t}\n\tfsUnlock := func() {\n\t\tif needFSLock {\n\t\t\tfs.mu.Unlock()\n\t\t}\n\t}\n\n\tfsLock()\n\n\tif fs.closed {\n\t\tfsUnlock()\n\t\treturn false, ErrStoreClosed\n\t}\n\tif fs.sips > 0 {\n\t\tfsUnlock()\n\t\treturn false, ErrStoreSnapshotInProgress\n\t}\n\t// If in encrypted mode negate secure rewrite here.\n\tif secure && fs.prf != nil {\n\t\tsecure = false\n\t}\n\tmb := fs.selectMsgBlock(seq)\n\tif mb == nil {\n\t\tvar err = ErrStoreEOF\n\t\tif seq <= fs.state.LastSeq {\n\t\t\terr = ErrStoreMsgNotFound\n\t\t}\n\t\tfsUnlock()\n\t\treturn false, err\n\t}\n\n\t// If we have a callback grab the message since we need the subject.\n\t// TODO(dlc) - This will cause whole buffer to be loaded which I was trying\n\t// to avoid. Maybe use side cache for subjects or understand when we really need them.\n\t// Meaning if the stream above is only a single subject no need to store, this is just\n\t// for updating stream pending for consumers.\n\tvar sm *fileStoredMsg\n\tif fs.scb != nil {\n\t\tsm, _ = mb.fetchMsg(seq)\n\t}\n\n\tmb.mu.Lock()\n\n\t// Check cache. This should be very rare.\n\tif mb.cache == nil || mb.cache.idx == nil || seq < mb.cache.fseq && mb.cache.off > 0 {\n\t\tmb.mu.Unlock()\n\t\tfsUnlock()\n\t\tif err := mb.loadMsgs(); err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tfsLock()\n\t\tmb.mu.Lock()\n\t}\n\n\t// See if the sequence numbers is still relevant. Check first and cache first.\n\tif seq < mb.first.seq || seq < mb.cache.fseq || (seq-mb.cache.fseq) >= uint64(len(mb.cache.idx)) {\n\t\tmb.mu.Unlock()\n\t\tfsUnlock()\n\t\treturn false, nil\n\t}\n\n\t// Now check dmap if it is there.\n\tif mb.dmap != nil {\n\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\tmb.mu.Unlock()\n\t\t\tfsUnlock()\n\t\t\treturn false, nil\n\t\t}\n\t}\n\n\t// Set cache timestamp for last remove.\n\tmb.lrts = time.Now().UnixNano()\n\n\t// Grab record length from idx.\n\tslot := seq - mb.cache.fseq\n\tri, rl, _, _ := mb.slotInfo(int(slot))\n\tmsz := uint64(rl)\n\n\t// Global stats\n\tfs.state.Msgs--\n\tfs.state.Bytes -= msz\n\n\t// Now local mb updates.\n\tmb.msgs--\n\tmb.bytes -= msz\n\n\t// If we are tracking multiple subjects here make sure we update that accounting.\n\tif mb.fss != nil {\n\t\tif sm == nil {\n\t\t\tif !mb.cacheAlreadyLoaded() {\n\t\t\t\tmb.loadMsgsWithLock()\n\t\t\t}\n\t\t\tsm, _ = mb.cacheLookupWithLock(seq)\n\t\t}\n\t\tif sm != nil {\n\t\t\tmb.removeSeqPerSubject(sm.subj, seq)\n\t\t}\n\t}\n\n\tvar shouldWriteIndex, firstSeqNeedsUpdate bool\n\n\tif secure {\n\t\tmb.eraseMsg(seq, int(ri), int(rl))\n\t}\n\n\t// Optimize for FIFO case.\n\tif seq == mb.first.seq {\n\t\tmb.selectNextFirst()\n\t\tif mb.isEmpty() {\n\t\t\tfs.removeMsgBlock(mb)\n\t\t\tfirstSeqNeedsUpdate = seq == fs.state.FirstSeq\n\t\t} else {\n\t\t\tshouldWriteIndex = true\n\t\t\tif seq == fs.state.FirstSeq {\n\t\t\t\tfs.state.FirstSeq = mb.first.seq // new one.\n\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// Check if we are empty first, as long as not the last message block.\n\t\tif isLast := mb != fs.lmb; isLast && mb.msgs == 0 {\n\t\t\tfs.removeMsgBlock(mb)\n\t\t\tfirstSeqNeedsUpdate = seq == fs.state.FirstSeq\n\t\t} else {\n\t\t\t// Out of order delete.\n\t\t\tshouldWriteIndex = true\n\t\t\tif mb.dmap == nil {\n\t\t\t\tmb.dmap = make(map[uint64]struct{})\n\t\t\t}\n\t\t\tmb.dmap[seq] = struct{}{}\n\t\t\t// Check if <50% utilization and minimum size met.\n\t\t\tif mb.rbytes > compactMinimum && mb.rbytes>>1 > mb.bytes {\n\t\t\t\tmb.compact()\n\t\t\t}\n\t\t}\n\t}\n\n\tvar qch, fch chan struct{}\n\tif shouldWriteIndex {\n\t\tqch, fch = mb.qch, mb.fch\n\t}\n\tcb := fs.scb\n\tmb.mu.Unlock()\n\n\tif secure {\n\t\tmb.flushPendingMsgs()\n\t}\n\n\t// Kick outside of lock.\n\tif shouldWriteIndex {\n\t\tif !fs.fip {\n\t\t\tif qch == nil {\n\t\t\t\tmb.spinUpFlushLoop()\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase fch <- struct{}{}:\n\t\t\tdefault:\n\t\t\t}\n\t\t} else {\n\t\t\tmb.writeIndexInfo()\n\t\t}\n\t}\n\n\t// If we emptied the current message block and the seq was state.First.Seq\n\t// then we need to jump message blocks.\n\tif firstSeqNeedsUpdate {\n\t\tfs.selectNextFirst()\n\t}\n\tfs.mu.Unlock()\n\n\t// Storage updates.\n\tif cb != nil {\n\t\tsubj := _EMPTY_\n\t\tif sm != nil {\n\t\t\tsubj = sm.subj\n\t\t}\n\t\tdelta := int64(msz)\n\t\tcb(-1, -delta, seq, subj)\n\t}\n\n\tif !needFSLock {\n\t\tfs.mu.Lock()\n\t}\n\n\treturn true, nil\n}\n\n// This will compact and rewrite this block. This should only be called when we know we want to rewrite this block.\n// This should not be called on the lmb since we will prune tail deleted messages which could cause issues with\n// writing new messages. We will silently bail on any issues with the underlying block and let someone else detect.\n// Write lock needs to be held.\nfunc (mb *msgBlock) compact() {\n\tif !mb.cacheAlreadyLoaded() {\n\t\tif err := mb.loadMsgsWithLock(); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tbuf := mb.cache.buf\n\tnbuf := make([]byte, 0, len(buf))\n\n\tvar le = binary.LittleEndian\n\tvar firstSet bool\n\n\tisDeleted := func(seq uint64) bool {\n\t\tif seq == 0 || seq&ebit != 0 || seq < mb.first.seq {\n\t\t\treturn true\n\t\t}\n\t\tif mb.dmap != nil {\n\t\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\n\tfor index, lbuf := uint32(0), uint32(len(buf)); index < lbuf; {\n\t\tif index+msgHdrSize >= lbuf {\n\t\t\treturn\n\t\t}\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl, slen := le.Uint32(hdr[0:]), le.Uint16(hdr[20:])\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || int(slen) > dlen || dlen > int(rl) || rl > 32*1024*1024 || index+rl > lbuf {\n\t\t\treturn\n\t\t}\n\t\t// Only need to process non-deleted messages.\n\t\tif seq := le.Uint64(hdr[4:]); !isDeleted(seq) {\n\t\t\t// Normal message here.\n\t\t\tnbuf = append(nbuf, buf[index:index+rl]...)\n\t\t\tif !firstSet {\n\t\t\t\tfirstSet = true\n\t\t\t\tmb.first.seq = seq\n\t\t\t}\n\t\t\tmb.last.seq = seq\n\t\t}\n\t\t// Advance to next record.\n\t\tindex += rl\n\t}\n\n\t// Check for encryption.\n\tif mb.bek != nil && len(nbuf) > 0 {\n\t\t// Recreate to reset counter.\n\t\trbek, err := chacha20.NewUnauthenticatedCipher(mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\trbek.XORKeyStream(nbuf, nbuf)\n\t}\n\n\t// Close FDs first.\n\tmb.closeFDsLocked()\n\n\t// We will write to a new file and mv/rename it in case of failure.\n\tmfn := path.Join(path.Join(mb.fs.fcfg.StoreDir, msgDir), fmt.Sprintf(newScan, mb.index))\n\tdefer os.Remove(mfn)\n\tif err := ioutil.WriteFile(mfn, nbuf, defaultFilePerms); err != nil {\n\t\treturn\n\t}\n\tos.Rename(mfn, mb.mfn)\n\n\t// Close cache and open FDs and index file.\n\tmb.clearCacheAndOffset()\n\tmb.removeIndexFileLocked()\n\tmb.deleteDmap()\n\tmb.rebuildStateLocked()\n}\n\n// Nil out our dmap.\nfunc (mb *msgBlock) deleteDmap() {\n\tmb.dmap = nil\n}\n\n// Grab info from a slot.\n// Lock should be held.\nfunc (mb *msgBlock) slotInfo(slot int) (uint32, uint32, bool, error) {\n\tif mb.cache == nil || slot >= len(mb.cache.idx) {\n\t\treturn 0, 0, false, errPartialCache\n\t}\n\tbi := mb.cache.idx[slot]\n\tri := (bi &^ hbit)\n\thashChecked := (bi & hbit) != 0\n\t// Determine record length\n\tvar rl uint32\n\tif len(mb.cache.idx) > slot+1 {\n\t\tni := mb.cache.idx[slot+1] &^ hbit\n\t\trl = ni - ri\n\t} else {\n\t\trl = mb.cache.lrl\n\t}\n\tif rl < msgHdrSize {\n\t\treturn 0, 0, false, errBadMsg\n\t}\n\treturn uint32(ri), rl, hashChecked, nil\n}\n\nfunc (fs *fileStore) isClosed() bool {\n\tfs.mu.RLock()\n\tclosed := fs.closed\n\tfs.mu.RUnlock()\n\treturn closed\n}\n\n// Will spin up our flush loop.\nfunc (mb *msgBlock) spinUpFlushLoop() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\t// Are we already running?\n\tif mb.flusher {\n\t\treturn\n\t}\n\tmb.flusher = true\n\tmb.fch = make(chan struct{}, 1)\n\tmb.qch = make(chan struct{})\n\tfch, qch := mb.fch, mb.qch\n\n\tgo mb.flushLoop(fch, qch)\n}\n\n// Raw low level kicker for flush loops.\nfunc kickFlusher(fch chan struct{}) {\n\tif fch != nil {\n\t\tselect {\n\t\tcase fch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n}\n\n// Kick flusher for this message block.\nfunc (mb *msgBlock) kickFlusher() {\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\tkickFlusher(mb.fch)\n}\n\nfunc (mb *msgBlock) setInFlusher() {\n\tmb.mu.Lock()\n\tmb.flusher = true\n\tmb.mu.Unlock()\n}\n\nfunc (mb *msgBlock) clearInFlusher() {\n\tmb.mu.Lock()\n\tmb.flusher = false\n\tmb.mu.Unlock()\n}\n\n// flushLoop watches for messages, index info, or recently closed msg block updates.\nfunc (mb *msgBlock) flushLoop(fch, qch chan struct{}) {\n\tmb.setInFlusher()\n\tdefer mb.clearInFlusher()\n\n\t// Will use to test if we have meta data updates.\n\tvar firstSeq, lastSeq uint64\n\tvar dmapLen int\n\n\tinfoChanged := func() bool {\n\t\tmb.mu.RLock()\n\t\tdefer mb.mu.RUnlock()\n\t\tvar changed bool\n\t\tif firstSeq != mb.first.seq || lastSeq != mb.last.seq || dmapLen != len(mb.dmap) {\n\t\t\tchanged = true\n\t\t\tfirstSeq, lastSeq = mb.first.seq, mb.last.seq\n\t\t\tdmapLen = len(mb.dmap)\n\t\t}\n\t\treturn changed\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-fch:\n\t\t\t// If we have pending messages process them first.\n\t\t\tif waiting := mb.pendingWriteSize(); waiting != 0 {\n\t\t\t\tts := 1 * time.Millisecond\n\t\t\t\tvar waited time.Duration\n\n\t\t\t\tfor waiting < coalesceMinimum {\n\t\t\t\t\ttime.Sleep(ts)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-qch:\n\t\t\t\t\t\treturn\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t\tnewWaiting := mb.pendingWriteSize()\n\t\t\t\t\tif waited = waited + ts; waited > maxFlushWait || newWaiting <= waiting {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\twaiting = newWaiting\n\t\t\t\t\tts *= 2\n\t\t\t\t}\n\t\t\t\tmb.flushPendingMsgsAndWait()\n\t\t\t\t// Check if we are no longer the last message block. If we are\n\t\t\t\t// not we can close FDs and exit.\n\t\t\t\tmb.fs.mu.RLock()\n\t\t\t\tnotLast := mb != mb.fs.lmb\n\t\t\t\tmb.fs.mu.RUnlock()\n\t\t\t\tif notLast {\n\t\t\t\t\tif err := mb.closeFDs(); err == nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif infoChanged() {\n\t\t\t\tmb.writeIndexInfo()\n\t\t\t}\n\t\tcase <-qch:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) eraseMsg(seq uint64, ri, rl int) error {\n\tvar le = binary.LittleEndian\n\tvar hdr [msgHdrSize]byte\n\n\tle.PutUint32(hdr[0:], uint32(rl))\n\tle.PutUint64(hdr[4:], seq|ebit)\n\tle.PutUint64(hdr[12:], 0)\n\tle.PutUint16(hdr[20:], 0)\n\n\t// Randomize record\n\tdata := make([]byte, rl-emptyRecordLen)\n\tmrand.Read(data)\n\n\t// Now write to underlying buffer.\n\tvar b bytes.Buffer\n\tb.Write(hdr[:])\n\tb.Write(data)\n\n\t// Calculate hash.\n\tmb.hh.Reset()\n\tmb.hh.Write(hdr[4:20])\n\tmb.hh.Write(data)\n\tchecksum := mb.hh.Sum(nil)\n\t// Write to msg record.\n\tb.Write(checksum)\n\n\t// Update both cache and disk.\n\tnbytes := b.Bytes()\n\n\t// Cache\n\tif ri >= mb.cache.off {\n\t\tli := ri - mb.cache.off\n\t\tbuf := mb.cache.buf[li : li+rl]\n\t\tcopy(buf, nbytes)\n\t}\n\n\t// Disk\n\tif mb.cache.off+mb.cache.wp > ri {\n\t\tmfd, err := os.OpenFile(mb.mfn, os.O_RDWR, defaultFilePerms)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer mfd.Close()\n\t\tif _, err = mfd.WriteAt(nbytes, int64(ri)); err == nil {\n\t\t\tmfd.Sync()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Truncate this message block to the storedMsg.\nfunc (mb *msgBlock) truncate(sm *fileStoredMsg) (nmsgs, nbytes uint64, err error) {\n\t// Make sure we are loaded to process messages etc.\n\tif err := mb.loadMsgs(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// Calculate new eof using slot info from our new last sm.\n\tri, rl, _, err := mb.slotInfo(int(sm.seq - mb.cache.fseq))\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\t// Calculate new eof.\n\teof := int64(ri + rl)\n\n\tvar purged, bytes uint64\n\n\tmb.mu.Lock()\n\tcheckDmap := len(mb.dmap) > 0\n\tfor seq := mb.last.seq; seq > sm.seq; seq-- {\n\t\tif checkDmap {\n\t\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\t\t// Delete and skip to next.\n\t\t\t\tdelete(mb.dmap, seq)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// We should have a valid msg to calculate removal stats.\n\t\t_, rl, _, err := mb.slotInfo(int(seq - mb.cache.fseq))\n\t\tif err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\treturn 0, 0, err\n\t\t}\n\t\tpurged++\n\t\tbytes += uint64(rl)\n\t}\n\n\t// Truncate our msgs and close file.\n\tif mb.mfd != nil {\n\t\tmb.mfd.Truncate(eof)\n\t\tmb.mfd.Sync()\n\t\t// Update our checksum.\n\t\tvar lchk [8]byte\n\t\tmb.mfd.ReadAt(lchk[:], eof-8)\n\t\tcopy(mb.lchk[0:], lchk[:])\n\t} else {\n\t\tmb.mu.Unlock()\n\t\treturn 0, 0, fmt.Errorf(\"failed to truncate msg block %d, file not open\", mb.index)\n\t}\n\n\t// Do local mb stat updates.\n\tmb.msgs -= purged\n\tmb.bytes -= bytes\n\tmb.rbytes -= bytes\n\n\t// Update our last msg.\n\tmb.last.seq = sm.seq\n\tmb.last.ts = sm.ts\n\n\t// Clear our cache.\n\tmb.clearCacheAndOffset()\n\tmb.mu.Unlock()\n\n\t// Write our index file.\n\tmb.writeIndexInfo()\n\t// Load msgs again.\n\tmb.loadMsgs()\n\n\treturn purged, bytes, nil\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) isEmpty() bool {\n\treturn mb.first.seq > mb.last.seq\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) selectNextFirst() {\n\tvar seq uint64\n\tfor seq = mb.first.seq + 1; seq <= mb.last.seq; seq++ {\n\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\t// We will move past this so we can delete the entry.\n\t\t\tdelete(mb.dmap, seq)\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\t// Set new first sequence.\n\tmb.first.seq = seq\n\n\t// Check if we are empty..\n\tif mb.isEmpty() {\n\t\tmb.first.ts = 0\n\t\treturn\n\t}\n\n\t// Need to get the timestamp.\n\t// We will try the cache direct and fallback if needed.\n\tsm, _ := mb.cacheLookupWithLock(seq)\n\tif sm == nil {\n\t\t// Slow path, need to unlock.\n\t\tmb.mu.Unlock()\n\t\tsm, _ = mb.fetchMsg(seq)\n\t\tmb.mu.Lock()\n\t}\n\tif sm != nil {\n\t\tmb.first.ts = sm.ts\n\t} else {\n\t\tmb.first.ts = 0\n\t}\n}\n\n// Select the next FirstSeq\nfunc (fs *fileStore) selectNextFirst() {\n\tif len(fs.blks) > 0 {\n\t\tmb := fs.blks[0]\n\t\tmb.mu.RLock()\n\t\tfs.state.FirstSeq = mb.first.seq\n\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\tmb.mu.RUnlock()\n\t} else {\n\t\t// Could not find anything, so treat like purge\n\t\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\t\tfs.state.FirstTime = time.Time{}\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) resetCacheExpireTimer(td time.Duration) {\n\tif td == 0 {\n\t\ttd = mb.cexp\n\t}\n\tif mb.ctmr == nil {\n\t\tmb.ctmr = time.AfterFunc(td, mb.expireCache)\n\t} else {\n\t\tmb.ctmr.Reset(td)\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) startCacheExpireTimer() {\n\tmb.resetCacheExpireTimer(0)\n}\n\n// Used when we load in a message block.\n// Lock should be held.\nfunc (mb *msgBlock) clearCacheAndOffset() {\n\tif mb.cache != nil {\n\t\tmb.cache.off = 0\n\t\tmb.cache.wp = 0\n\t}\n\tmb.clearCache()\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) clearCache() {\n\tif mb.ctmr != nil {\n\t\tmb.ctmr.Stop()\n\t\tmb.ctmr = nil\n\t}\n\tif mb.cache == nil {\n\t\treturn\n\t}\n\n\tif mb.cache.off == 0 {\n\t\tmb.cache = nil\n\t} else {\n\t\t// Clear msgs and index.\n\t\tmb.cache.buf = nil\n\t\tmb.cache.idx = nil\n\t\tmb.cache.wp = 0\n\t}\n}\n\n// Called to possibly expire a message block cache.\nfunc (mb *msgBlock) expireCache() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.expireCacheLocked()\n}\n\nfunc (mb *msgBlock) expireCacheLocked() {\n\tif mb.cache == nil {\n\t\tif mb.ctmr != nil {\n\t\t\tmb.ctmr.Stop()\n\t\t\tmb.ctmr = nil\n\t\t}\n\t\treturn\n\t}\n\n\t// Can't expire if we are flushing or still have pending.\n\tif mb.cache.flush || (len(mb.cache.buf)-int(mb.cache.wp) > 0) {\n\t\tmb.resetCacheExpireTimer(mb.cexp)\n\t\treturn\n\t}\n\n\t// Grab timestamp to compare.\n\ttns := time.Now().UnixNano()\n\n\t// For the core buffer of messages, we care about reads and writes, but not removes.\n\tbufts := mb.llts\n\tif mb.lwts > bufts {\n\t\tbufts = mb.lwts\n\t}\n\n\t// Check for activity on the cache that would prevent us from expiring.\n\tif tns-bufts <= int64(mb.cexp) {\n\t\tmb.resetCacheExpireTimer(mb.cexp - time.Duration(tns-bufts))\n\t\treturn\n\t}\n\n\t// If we are here we will at least expire the core msg buffer.\n\t// We need to capture offset in case we do a write next before a full load.\n\tmb.cache.off += len(mb.cache.buf)\n\tmb.cache.buf = nil\n\tmb.cache.wp = 0\n\n\t// The idx is used in removes, and will have a longer timeframe.\n\t// See if we should also remove the idx.\n\tif tns-mb.lrts > int64(defaultCacheIdxExpiration) {\n\t\tmb.clearCache()\n\t} else {\n\t\tmb.resetCacheExpireTimer(mb.cexp)\n\t}\n}\n\nfunc (fs *fileStore) startAgeChk() {\n\tif fs.ageChk == nil && fs.cfg.MaxAge != 0 {\n\t\tfs.ageChk = time.AfterFunc(fs.cfg.MaxAge, fs.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) resetAgeChk(delta int64) {\n\tfireIn := fs.cfg.MaxAge\n\tif delta > 0 {\n\t\tfireIn = time.Duration(delta)\n\t}\n\tif fs.ageChk != nil {\n\t\tfs.ageChk.Reset(fireIn)\n\t} else {\n\t\tfs.ageChk = time.AfterFunc(fireIn, fs.expireMsgs)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) cancelAgeChk() {\n\tif fs.ageChk != nil {\n\t\tfs.ageChk.Stop()\n\t\tfs.ageChk = nil\n\t}\n}\n\n// Will expire msgs that are too old.\nfunc (fs *fileStore) expireMsgs() {\n\t// We need to delete one by one here and can not optimize for the time being.\n\t// Reason is that we need more information to adjust ack pending in consumers.\n\tvar sm *fileStoredMsg\n\tminAge := time.Now().UnixNano() - int64(fs.cfg.MaxAge)\n\tfor sm, _ = fs.msgForSeq(0); sm != nil && sm.ts <= minAge; sm, _ = fs.msgForSeq(0) {\n\t\tfs.removeMsg(sm.seq, false, true)\n\t}\n\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\tif sm == nil {\n\t\tfs.cancelAgeChk()\n\t} else {\n\t\tfs.resetAgeChk(sm.ts - minAge)\n\t}\n}\n\n// Lock should be held.\nfunc (fs *fileStore) checkAndFlushAllBlocks() {\n\tfor _, mb := range fs.blks {\n\t\tif mb.pendingWriteSize() > 0 {\n\t\t\tmb.flushPendingMsgsAndWait()\n\t\t}\n\t\tmb.writeIndexInfo()\n\t}\n}\n\n// This will check all the checksums on messages and report back any sequence numbers with errors.\nfunc (fs *fileStore) checkMsgs() *LostStreamData {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\n\tfs.checkAndFlushAllBlocks()\n\n\tfor _, mb := range fs.blks {\n\t\tif ld, err := mb.rebuildState(); err != nil && ld != nil {\n\t\t\t// Rebuild fs state too.\n\t\t\tmb.fs.rebuildState(ld)\n\t\t}\n\t}\n\treturn fs.ld\n}\n\n// Will write the message record to the underlying message block.\n// filestore lock will be held.\nfunc (mb *msgBlock) writeMsgRecord(rl, seq uint64, subj string, mhdr, msg []byte, ts int64, flush bool) error {\n\tmb.mu.Lock()\n\t// Make sure we have a cache setup.\n\tif mb.cache == nil {\n\t\tmb.setupWriteCache(nil)\n\t}\n\n\t// Indexing\n\tindex := len(mb.cache.buf) + int(mb.cache.off)\n\n\t// Formats\n\t// Format with no header\n\t// total_len(4) sequence(8) timestamp(8) subj_len(2) subj msg hash(8)\n\t// With headers, high bit on total length will be set.\n\t// total_len(4) sequence(8) timestamp(8) subj_len(2) subj hdr_len(4) hdr msg hash(8)\n\n\t// First write header, etc.\n\tvar le = binary.LittleEndian\n\tvar hdr [msgHdrSize]byte\n\n\tl := uint32(rl)\n\thasHeaders := len(mhdr) > 0\n\tif hasHeaders {\n\t\tl |= hbit\n\t}\n\n\tle.PutUint32(hdr[0:], l)\n\tle.PutUint64(hdr[4:], seq)\n\tle.PutUint64(hdr[12:], uint64(ts))\n\tle.PutUint16(hdr[20:], uint16(len(subj)))\n\n\t// Now write to underlying buffer.\n\tmb.cache.buf = append(mb.cache.buf, hdr[:]...)\n\tmb.cache.buf = append(mb.cache.buf, subj...)\n\n\tif hasHeaders {\n\t\tvar hlen [4]byte\n\t\tle.PutUint32(hlen[0:], uint32(len(mhdr)))\n\t\tmb.cache.buf = append(mb.cache.buf, hlen[:]...)\n\t\tmb.cache.buf = append(mb.cache.buf, mhdr...)\n\t}\n\tmb.cache.buf = append(mb.cache.buf, msg...)\n\n\t// Calculate hash.\n\tmb.hh.Reset()\n\tmb.hh.Write(hdr[4:20])\n\tmb.hh.Write([]byte(subj))\n\tif hasHeaders {\n\t\tmb.hh.Write(mhdr)\n\t}\n\tmb.hh.Write(msg)\n\tchecksum := mb.hh.Sum(nil)\n\t// Grab last checksum\n\tcopy(mb.lchk[0:], checksum)\n\n\t// Update write through cache.\n\t// Write to msg record.\n\tmb.cache.buf = append(mb.cache.buf, checksum...)\n\t// Write index\n\tmb.cache.idx = append(mb.cache.idx, uint32(index)|hbit)\n\tmb.cache.lrl = uint32(rl)\n\tif mb.cache.fseq == 0 {\n\t\tmb.cache.fseq = seq\n\t}\n\n\t// Set cache timestamp for last store.\n\tmb.lwts = ts\n\t// Decide if we write index info if flushing in place.\n\twriteIndex := ts-mb.lwits > int64(2*time.Second)\n\n\t// Accounting\n\tmb.updateAccounting(seq, ts, rl)\n\n\t// Check if we are tracking per subject for our simple state.\n\tif len(subj) > 0 && mb.fss != nil {\n\t\tif ss := mb.fss[subj]; ss != nil {\n\t\t\tss.Msgs++\n\t\t\tss.Last = seq\n\t\t} else {\n\t\t\tmb.fss[subj] = &SimpleState{Msgs: 1, First: seq, Last: seq}\n\t\t}\n\t}\n\n\tfch, werr := mb.fch, mb.werr\n\tmb.mu.Unlock()\n\n\t// If we should be flushing in place do so here. We will also flip to flushing in place if we\n\t// had a write error.\n\tif flush || werr != nil {\n\t\tif err := mb.flushPendingMsgsAndWait(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif writeIndex {\n\t\t\tif err := mb.writeIndexInfo(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// Kick the flusher here.\n\t\tkickFlusher(fch)\n\t}\n\n\treturn nil\n}\n\n// How many bytes pending to be written for this message block.\nfunc (mb *msgBlock) pendingWriteSize() int {\n\tif mb == nil {\n\t\treturn 0\n\t}\n\tvar pending int\n\tmb.mu.RLock()\n\tif mb.mfd != nil && mb.cache != nil {\n\t\tpending = len(mb.cache.buf) - int(mb.cache.wp)\n\t}\n\tmb.mu.RUnlock()\n\treturn pending\n}\n\n// Lock should NOT be held.\nfunc (mb *msgBlock) clearFlushing() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tif mb.cache != nil {\n\t\tmb.cache.flush = false\n\t}\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) setFlushing() {\n\tif mb.cache != nil {\n\t\tmb.cache.flush = true\n\t}\n}\n\n// Try to close our FDs if we can.\nfunc (mb *msgBlock) closeFDs() error {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.closeFDsLocked()\n}\n\nfunc (mb *msgBlock) closeFDsLocked() error {\n\tif buf, err := mb.bytesPending(); err == errFlushRunning || len(buf) > 0 {\n\t\treturn errPendingData\n\t}\n\tif mb.mfd != nil {\n\t\tmb.mfd.Close()\n\t\tmb.mfd = nil\n\t}\n\tif mb.ifd != nil {\n\t\tmb.ifd.Close()\n\t\tmb.ifd = nil\n\t}\n\treturn nil\n}\n\n// bytesPending returns the buffer to be used for writing to the underlying file.\n// This marks we are in flush and will return nil if asked again until cleared.\n// Lock should be held.\nfunc (mb *msgBlock) bytesPending() ([]byte, error) {\n\tif mb == nil || mb.mfd == nil {\n\t\treturn nil, errNoPending\n\t}\n\tif mb.cache == nil {\n\t\treturn nil, errNoCache\n\t}\n\tif mb.cache.flush {\n\t\treturn nil, errFlushRunning\n\t}\n\tbuf := mb.cache.buf[mb.cache.wp:]\n\tif len(buf) == 0 {\n\t\treturn nil, errNoPending\n\t}\n\treturn buf, nil\n}\n\n// Returns the current blkSize including deleted msgs etc.\nfunc (mb *msgBlock) blkSize() uint64 {\n\tmb.mu.RLock()\n\tnb := mb.rbytes\n\tmb.mu.RUnlock()\n\treturn nb\n}\n\n// Update accounting on a write msg.\n// Lock should be held.\nfunc (mb *msgBlock) updateAccounting(seq uint64, ts int64, rl uint64) {\n\tif mb.first.seq == 0 || mb.first.ts == 0 {\n\t\tmb.first.seq = seq\n\t\tmb.first.ts = ts\n\t}\n\t// Need atomics here for selectMsgBlock speed.\n\tatomic.StoreUint64(&mb.last.seq, seq)\n\tmb.last.ts = ts\n\tmb.bytes += rl\n\tmb.rbytes += rl\n\tmb.msgs++\n}\n\n// Lock should be held.\nfunc (fs *fileStore) writeMsgRecord(seq uint64, ts int64, subj string, hdr, msg []byte) (uint64, error) {\n\tvar err error\n\n\t// Get size for this message.\n\trl := fileStoreMsgSize(subj, hdr, msg)\n\tif rl&hbit != 0 {\n\t\treturn 0, ErrMsgTooLarge\n\t}\n\t// Grab our current last message block.\n\tmb := fs.lmb\n\tif mb == nil || mb.blkSize()+rl > fs.fcfg.BlockSize {\n\t\tif mb, err = fs.newMsgBlockForWrite(); err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\t// Ask msg block to store in write through cache.\n\terr = mb.writeMsgRecord(rl, seq, subj, hdr, msg, ts, fs.fip)\n\n\treturn rl, err\n}\n\n// Sync msg and index files as needed. This is called from a timer.\nfunc (fs *fileStore) syncBlocks() {\n\tfs.mu.RLock()\n\tif fs.closed {\n\t\tfs.mu.RUnlock()\n\t\treturn\n\t}\n\tblks := append([]*msgBlock(nil), fs.blks...)\n\tfs.mu.RUnlock()\n\n\tfor _, mb := range blks {\n\t\tmb.mu.RLock()\n\t\tmfd := mb.mfd\n\t\tifd := mb.ifd\n\t\tliwsz := mb.liwsz\n\t\tmb.mu.RUnlock()\n\n\t\tif mfd != nil {\n\t\t\tmfd.Sync()\n\t\t}\n\t\tif ifd != nil {\n\t\t\tifd.Truncate(liwsz)\n\t\t\tifd.Sync()\n\t\t}\n\t}\n\n\tfs.mu.Lock()\n\tfs.syncTmr = time.AfterFunc(fs.fcfg.SyncInterval, fs.syncBlocks)\n\tfs.mu.Unlock()\n}\n\n// Select the message block where this message should be found.\n// Return nil if not in the set.\n// Read lock should be held.\nfunc (fs *fileStore) selectMsgBlock(seq uint64) *msgBlock {\n\t// Check for out of range.\n\tif seq < fs.state.FirstSeq || seq > fs.state.LastSeq {\n\t\treturn nil\n\t}\n\t// blks are sorted in ascending order.\n\t// TODO(dlc) - Can be smarter here, when lots of blks maybe use binary search.\n\t// For now this is cache friendly for small to medium numbers of blks.\n\tfor _, mb := range fs.blks {\n\t\tif seq <= atomic.LoadUint64(&mb.last.seq) {\n\t\t\treturn mb\n\t\t}\n\t}\n\treturn nil\n}\n\n// Select the message block where this message should be found.\n// Return nil if not in the set.\nfunc (fs *fileStore) selectMsgBlockForStart(minTime time.Time) *msgBlock {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\n\tt := minTime.UnixNano()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tfound := t <= mb.last.ts\n\t\tmb.mu.RUnlock()\n\t\tif found {\n\t\t\treturn mb\n\t\t}\n\t}\n\treturn nil\n}\n\n// Index a raw msg buffer.\n// Lock should be held.\nfunc (mb *msgBlock) indexCacheBuf(buf []byte) error {\n\tvar le = binary.LittleEndian\n\n\tvar fseq uint64\n\tvar idx []uint32\n\tvar index uint32\n\n\tif mb.cache == nil {\n\t\t// Approximation, may adjust below.\n\t\tfseq = mb.first.seq\n\t\tidx = make([]uint32, 0, mb.msgs)\n\t\tmb.cache = &cache{}\n\t} else {\n\t\tfseq = mb.cache.fseq\n\t\tidx = mb.cache.idx\n\t\tif len(idx) == 0 {\n\t\t\tidx = make([]uint32, 0, mb.msgs)\n\t\t}\n\t\tindex = uint32(len(mb.cache.buf))\n\t\tbuf = append(mb.cache.buf, buf...)\n\t}\n\n\tlbuf := uint32(len(buf))\n\n\tfor index < lbuf {\n\t\tif index+msgHdrSize >= lbuf {\n\t\t\treturn errCorruptState\n\t\t}\n\t\thdr := buf[index : index+msgHdrSize]\n\t\trl := le.Uint32(hdr[0:])\n\t\tseq := le.Uint64(hdr[4:])\n\t\tslen := le.Uint16(hdr[20:])\n\n\t\t// Clear any headers bit that could be set.\n\t\trl &^= hbit\n\t\tdlen := int(rl) - msgHdrSize\n\n\t\t// Do some quick sanity checks here.\n\t\tif dlen < 0 || int(slen) > dlen || dlen > int(rl) || rl > 32*1024*1024 {\n\t\t\t// This means something is off.\n\t\t\t// TODO(dlc) - Add into bad list?\n\t\t\treturn errCorruptState\n\t\t}\n\t\t// Clear erase bit.\n\t\tseq = seq &^ ebit\n\t\t// Adjust if we guessed wrong.\n\t\tif seq != 0 && seq < fseq {\n\t\t\tfseq = seq\n\t\t}\n\t\t// We defer checksum checks to individual msg cache lookups to amortorize costs and\n\t\t// not introduce latency for first message from a newly loaded block.\n\t\tidx = append(idx, index)\n\t\tmb.cache.lrl = uint32(rl)\n\t\tindex += mb.cache.lrl\n\t}\n\tmb.cache.buf = buf\n\tmb.cache.idx = idx\n\tmb.cache.fseq = fseq\n\tmb.cache.wp += int(lbuf)\n\n\treturn nil\n}\n\nfunc (mb *msgBlock) quitChan() chan struct{} {\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\treturn mb.qch\n}\n\n// When called directly, flushPending could be busy already and return errFlushRunning.\n// This function is called for in place flushing so we need to wait.\nfunc (mb *msgBlock) flushPendingMsgsAndWait() error {\n\tvar err error\n\tvar t *time.Timer\n\tconst delay = time.Millisecond\n\n\t// If we are in flush wait for that to clear.\n\tfor err = mb.flushPendingMsgs(); err == errFlushRunning; err = mb.flushPendingMsgs() {\n\t\tqch := mb.quitChan()\n\t\tif t == nil {\n\t\t\tt = time.NewTimer(delay)\n\t\t\tdefer t.Stop()\n\t\t} else {\n\t\t\tt.Reset(delay)\n\t\t}\n\t\tselect {\n\t\tcase <-qch:\n\t\t\treturn nil\n\t\tcase <-t.C:\n\t\t}\n\t}\n\treturn err\n}\n\n// flushPendingMsgs writes out any messages for this message block.\nfunc (mb *msgBlock) flushPendingMsgs() error {\n\t// We will not hold the lock across I/O so we can add more messages\n\t// in parallel but we allow only one flush to be running.\n\tmb.mu.Lock()\n\tif mb.cache == nil || mb.mfd == nil {\n\t\tmb.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// bytesPending will return with errFlushRunning\n\t// if we are already flushing this message block.\n\tbuf, err := mb.bytesPending()\n\t// If we got an error back return here.\n\tif err != nil {\n\t\tmb.mu.Unlock()\n\t\t// No pending data to be written is not an error.\n\t\tif err == errNoPending || err == errNoCache {\n\t\t\terr = nil\n\t\t}\n\t\treturn err\n\t}\n\n\twoff := int64(mb.cache.off + mb.cache.wp)\n\tlob := len(buf)\n\n\t// Only one can be flushing at a time.\n\tmb.setFlushing()\n\t// Clear on exit.\n\tdefer mb.clearFlushing()\n\n\tmfd := mb.mfd\n\tmb.mu.Unlock()\n\n\tvar n int\n\n\t// Check if we need to encrypt.\n\tif mb.bek != nil && lob > 0 {\n\t\tconst rsz = 32 * 1024 // 32k\n\t\tvar rdst [rsz]byte\n\t\tvar dst []byte\n\t\tif lob > rsz {\n\t\t\tdst = make([]byte, lob)\n\t\t} else {\n\t\t\tdst = rdst[:lob]\n\t\t}\n\t\t// Need to leave original alone.\n\t\tmb.bek.XORKeyStream(dst, buf)\n\t\tbuf = dst\n\t}\n\n\t// Append new data to the message block file.\n\tfor lbb := lob; lbb > 0; lbb = len(buf) {\n\t\tn, err = mfd.WriteAt(buf, woff)\n\t\tif err != nil {\n\t\t\tmb.removeIndexFile()\n\t\t\tmb.dirtyClose()\n\t\t\tif !isOutOfSpaceErr(err) {\n\t\t\t\tif ld, err := mb.rebuildState(); err != nil && ld != nil {\n\t\t\t\t\t// Rebuild fs state too.\n\t\t\t\t\tmb.fs.mu.Lock()\n\t\t\t\t\tmb.fs.rebuildState(ld)\n\t\t\t\t\tmb.fs.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\t// Partial write.\n\t\tif n != lbb {\n\t\t\tbuf = buf[n:]\n\t\t} else {\n\t\t\t// Done.\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Update our write offset.\n\twoff += int64(lob)\n\n\t// We did a successful write.\n\t// Re-acquire lock to update.\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\t// set write err to any error.\n\tmb.werr = err\n\n\t// Cache may be gone.\n\tif mb.cache == nil || mb.mfd == nil {\n\t\treturn mb.werr\n\t}\n\n\t// Check for additional writes while we were writing to the disk.\n\tmoreBytes := len(mb.cache.buf) - mb.cache.wp - lob\n\n\t// Decide what we want to do with the buffer in hand. If we have load interest\n\t// we will hold onto the whole thing, otherwise empty the buffer, possibly reusing it.\n\tif ts := time.Now().UnixNano(); ts < mb.llts || (ts-mb.llts) <= int64(mb.cexp) {\n\t\tmb.cache.wp += lob\n\t} else {\n\t\tif cap(mb.cache.buf) <= maxBufReuse {\n\t\t\tbuf = mb.cache.buf[:0]\n\t\t} else {\n\t\t\tbuf = nil\n\t\t}\n\t\tif moreBytes > 0 {\n\t\t\tnbuf := mb.cache.buf[len(mb.cache.buf)-moreBytes:]\n\t\t\tif moreBytes > (len(mb.cache.buf)/4*3) && cap(nbuf) <= maxBufReuse {\n\t\t\t\tbuf = nbuf\n\t\t\t} else {\n\t\t\t\tbuf = append(buf, nbuf...)\n\t\t\t}\n\t\t}\n\t\t// Update our cache offset.\n\t\tmb.cache.off = int(woff)\n\t\t// Reset write pointer.\n\t\tmb.cache.wp = 0\n\t\t// Place buffer back in the cache structure.\n\t\tmb.cache.buf = buf\n\t}\n\n\treturn mb.werr\n}\n\n//  Lock should be held.\nfunc (mb *msgBlock) clearLoading() {\n\tmb.loading = false\n}\n\n// Will load msgs from disk.\nfunc (mb *msgBlock) loadMsgs() error {\n\t// We hold the lock here the whole time by design.\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.loadMsgsWithLock()\n}\n\n// Lock should be held.\nfunc (mb *msgBlock) cacheAlreadyLoaded() bool {\n\treturn mb.cache != nil && len(mb.cache.idx) == int(mb.msgs) && mb.cache.off == 0 && len(mb.cache.buf) > 0\n}\n\n// Used to load in the block contents.\n// Lock should be held and all conditionals satisfied prior.\nfunc (mb *msgBlock) loadBlock(buf []byte) ([]byte, error) {\n\tf, err := os.Open(mb.mfn)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\tvar sz int\n\tif info, err := f.Stat(); err == nil {\n\t\tsz64 := info.Size()\n\t\tif int64(int(sz64)) == sz64 {\n\t\t\tsz = int(sz64)\n\t\t}\n\t}\n\n\tif sz > cap(buf) {\n\t\tbuf = make([]byte, sz)\n\t} else {\n\t\tbuf = buf[:sz]\n\t}\n\n\tn, err := io.ReadFull(f, buf)\n\treturn buf[:n], err\n}\n\nfunc (mb *msgBlock) loadMsgsWithLock() error {\n\t// Check to see if we are loading already.\n\tif mb.loading {\n\t\treturn nil\n\t}\n\n\t// Set loading status.\n\tmb.loading = true\n\tdefer mb.clearLoading()\n\n\tvar nchecks int\n\ncheckCache:\n\tnchecks++\n\tif nchecks > 8 {\n\t\treturn errCorruptState\n\t}\n\n\t// Check to see if we have a full cache.\n\tif mb.cacheAlreadyLoaded() {\n\t\treturn nil\n\t}\n\n\tmb.llts = time.Now().UnixNano()\n\n\t// FIXME(dlc) - We could be smarter here.\n\tif mb.cache != nil && len(mb.cache.buf)-mb.cache.wp > 0 {\n\t\tmb.mu.Unlock()\n\t\terr := mb.flushPendingMsgsAndWait()\n\t\tmb.mu.Lock()\n\t\tif err != nil && err != errFlushRunning {\n\t\t\treturn err\n\t\t}\n\t\tgoto checkCache\n\t}\n\n\t// Load in the whole block. We want to hold the mb lock here to avoid any changes to\n\t// state.\n\tbuf, err := mb.loadBlock(nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Reset the cache since we just read everything in.\n\t// Make sure this is cleared in case we had a partial when we started.\n\tmb.clearCacheAndOffset()\n\n\t// Check if we need to decrypt.\n\tif mb.bek != nil && len(buf) > 0 {\n\t\trbek, err := chacha20.NewUnauthenticatedCipher(mb.seed, mb.nonce)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trbek.XORKeyStream(buf, buf)\n\t}\n\n\tif err := mb.indexCacheBuf(buf); err != nil {\n\t\tif err == errCorruptState {\n\t\t\tfs := mb.fs\n\t\t\tmb.mu.Unlock()\n\t\t\tvar ld *LostStreamData\n\t\t\tif ld, err = mb.rebuildState(); ld != nil {\n\t\t\t\tfs.mu.Lock()\n\t\t\t\tfs.rebuildState(ld)\n\t\t\t\tfs.mu.Unlock()\n\t\t\t}\n\t\t\tmb.mu.Lock()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgoto checkCache\n\t}\n\n\tif len(buf) > 0 {\n\t\tmb.cloads++\n\t\tmb.startCacheExpireTimer()\n\t}\n\n\treturn nil\n}\n\n// Fetch a message from this block, possibly reading in and caching the messages.\n// We assume the block was selected and is correct, so we do not do range checks.\nfunc (mb *msgBlock) fetchMsg(seq uint64) (*fileStoredMsg, error) {\n\tvar sm *fileStoredMsg\n\n\tsm, err := mb.cacheLookup(seq)\n\tif err == nil || (err != errNoCache && err != errPartialCache) {\n\t\treturn sm, err\n\t}\n\n\t// We have a cache miss here.\n\tif err := mb.loadMsgs(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn mb.cacheLookup(seq)\n}\n\nvar (\n\terrNoCache      = errors.New(\"no message cache\")\n\terrBadMsg       = errors.New(\"malformed or corrupt message\")\n\terrDeletedMsg   = errors.New(\"deleted message\")\n\terrPartialCache = errors.New(\"partial cache\")\n\terrNoPending    = errors.New(\"message block does not have pending data\")\n\terrNotReadable  = errors.New(\"storage directory not readable\")\n\terrFlushRunning = errors.New(\"flush is already running\")\n\terrCorruptState = errors.New(\"corrupt state file\")\n\terrPendingData  = errors.New(\"pending data still present\")\n\terrNoEncryption = errors.New(\"encryption not enabled\")\n\terrBadKeySize   = errors.New(\"encryption bad key size\")\n)\n\n// Used for marking messages that have had their checksums checked.\n// Used to signal a message record with headers.\nconst hbit = 1 << 31\n\n// Used for marking erased messages sequences.\nconst ebit = 1 << 63\n\n// Will do a lookup from the cache.\nfunc (mb *msgBlock) cacheLookup(seq uint64) (*fileStoredMsg, error) {\n\t// Currently grab the write lock for optional use of mb.hh. Prefer this for now\n\t// vs read lock and promote. Also defer based on 1.14 performance.\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\treturn mb.cacheLookupWithLock(seq)\n}\n\n// Will do a lookup from cache assuming lock is held.\nfunc (mb *msgBlock) cacheLookupWithLock(seq uint64) (*fileStoredMsg, error) {\n\tif mb.cache == nil || len(mb.cache.idx) == 0 {\n\t\treturn nil, errNoCache\n\t}\n\n\tif seq < mb.first.seq || seq < mb.cache.fseq || seq > mb.last.seq {\n\t\treturn nil, ErrStoreMsgNotFound\n\t}\n\n\t// If we have a delete map check it.\n\tif mb.dmap != nil {\n\t\tif _, ok := mb.dmap[seq]; ok {\n\t\t\treturn nil, errDeletedMsg\n\t\t}\n\t}\n\n\tif mb.cache.off > 0 {\n\t\treturn nil, errPartialCache\n\t}\n\n\tbi, _, hashChecked, err := mb.slotInfo(int(seq - mb.cache.fseq))\n\tif err != nil {\n\t\treturn nil, errPartialCache\n\t}\n\n\t// Update cache activity.\n\tmb.llts = time.Now().UnixNano()\n\tmb.llseq = seq\n\n\t// We use the high bit to denote we have already checked the checksum.\n\tvar hh hash.Hash64\n\tif !hashChecked {\n\t\thh = mb.hh // This will force the hash check in msgFromBuf.\n\t\tmb.cache.idx[seq-mb.cache.fseq] = (bi | hbit)\n\t}\n\n\tli := int(bi) - mb.cache.off\n\tbuf := mb.cache.buf[li:]\n\n\t// Parse from the raw buffer.\n\tsubj, hdr, msg, mseq, ts, err := msgFromBuf(buf, hh)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif seq != mseq {\n\t\treturn nil, fmt.Errorf(\"sequence numbers for cache load did not match, %d vs %d\", seq, mseq)\n\t}\n\tsm := &fileStoredMsg{\n\t\tsubj: subj,\n\t\thdr:  hdr,\n\t\tmsg:  msg,\n\t\tseq:  seq,\n\t\tts:   ts,\n\t\tmb:   mb,\n\t\toff:  int64(bi),\n\t}\n\n\treturn sm, nil\n}\n\n// Will return message for the given sequence number.\nfunc (fs *fileStore) msgForSeq(seq uint64) (*fileStoredMsg, error) {\n\t// TODO(dlc) - Since Store, Remove, Skip all hold the write lock on fs this will\n\t// be stalled. Need another lock if want to happen in parallel.\n\tfs.mu.RLock()\n\tif fs.closed {\n\t\tfs.mu.RUnlock()\n\t\treturn nil, ErrStoreClosed\n\t}\n\t// Indicates we want first msg.\n\tif seq == 0 {\n\t\tseq = fs.state.FirstSeq\n\t}\n\t// Make sure to snapshot here.\n\tlseq := fs.state.LastSeq\n\tmb, lmb := fs.selectMsgBlock(seq), fs.lmb\n\tfs.mu.RUnlock()\n\n\tif mb == nil {\n\t\tvar err = ErrStoreEOF\n\t\tif seq <= lseq {\n\t\t\terr = ErrStoreMsgNotFound\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// Check to see if we are the last seq for this message block and are doing\n\t// a linear scan. If that is true and we are not the last message block we can\n\t// try to expire the cache.\n\tmb.mu.RLock()\n\tshouldTryExpire := mb != lmb && seq == mb.last.seq && mb.llseq == seq-1\n\tmb.mu.RUnlock()\n\n\t// TODO(dlc) - older design had a check to prefetch when we knew we were\n\t// loading in order and getting close to end of current mb. Should add\n\t// something like it back in.\n\tfsm, err := mb.fetchMsg(seq)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// We detected a linear scan and access to the last message.\n\tif shouldTryExpire {\n\t\tmb.mu.Lock()\n\t\tmb.llts = 0\n\t\tmb.expireCacheLocked()\n\t\tmb.mu.Unlock()\n\t}\n\n\treturn fsm, nil\n}\n\n// Internal function to return msg parts from a raw buffer.\nfunc msgFromBuf(buf []byte, hh hash.Hash64) (string, []byte, []byte, uint64, int64, error) {\n\tif len(buf) < msgHdrSize {\n\t\treturn _EMPTY_, nil, nil, 0, 0, errBadMsg\n\t}\n\tvar le = binary.LittleEndian\n\n\thdr := buf[:msgHdrSize]\n\trl := le.Uint32(hdr[0:])\n\thasHeaders := rl&hbit != 0\n\trl &^= hbit // clear header bit\n\tdlen := int(rl) - msgHdrSize\n\tslen := int(le.Uint16(hdr[20:]))\n\t// Simple sanity check.\n\tif dlen < 0 || slen > dlen || int(rl) > len(buf) {\n\t\treturn _EMPTY_, nil, nil, 0, 0, errBadMsg\n\t}\n\tdata := buf[msgHdrSize : msgHdrSize+dlen]\n\t// Do checksum tests here if requested.\n\tif hh != nil {\n\t\thh.Reset()\n\t\thh.Write(hdr[4:20])\n\t\thh.Write(data[:slen])\n\t\tif hasHeaders {\n\t\t\thh.Write(data[slen+4 : dlen-8])\n\t\t} else {\n\t\t\thh.Write(data[slen : dlen-8])\n\t\t}\n\t\tif !bytes.Equal(hh.Sum(nil), data[len(data)-8:]) {\n\t\t\treturn _EMPTY_, nil, nil, 0, 0, errBadMsg\n\t\t}\n\t}\n\tseq := le.Uint64(hdr[4:])\n\tif seq&ebit != 0 {\n\t\tseq = 0\n\t}\n\tts := int64(le.Uint64(hdr[12:]))\n\t// FIXME(dlc) - We need to not allow appends to the underlying buffer, so we will\n\t// fix the capacity. This will cause a copy though in stream:internalSendLoop when\n\t// we append CRLF but this was causing a race. Need to rethink more to avoid this copy.\n\tend := dlen - 8\n\tvar mhdr, msg []byte\n\tif hasHeaders {\n\t\thl := le.Uint32(data[slen:])\n\t\tbi := slen + 4\n\t\tli := bi + int(hl)\n\t\tmhdr = data[bi:li:li]\n\t\tmsg = data[li:end:end]\n\t} else {\n\t\tmsg = data[slen:end:end]\n\t}\n\treturn string(data[:slen]), mhdr, msg, seq, ts, nil\n}\n\n// LoadMsg will lookup the message by sequence number and return it if found.\nfunc (fs *fileStore) LoadMsg(seq uint64) (string, []byte, []byte, int64, error) {\n\tsm, err := fs.msgForSeq(seq)\n\tif sm != nil {\n\t\treturn sm.subj, sm.hdr, sm.msg, sm.ts, nil\n\t}\n\treturn _EMPTY_, nil, nil, 0, err\n}\n\n// LoadLastMsg will return the last message we have that matches a given subject.\n// The subject can be a wildcard.\nfunc (fs *fileStore) LoadLastMsg(subject string) (subj string, seq uint64, hdr, msg []byte, ts int64, err error) {\n\tvar sm *fileStoredMsg\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tsm, _ = fs.msgForSeq(fs.lastSeq())\n\t} else if ss := fs.FilteredState(1, subject); ss.Msgs > 0 {\n\t\tsm, _ = fs.msgForSeq(ss.Last)\n\t}\n\tif sm == nil {\n\t\treturn _EMPTY_, 0, nil, nil, 0, ErrStoreMsgNotFound\n\t}\n\treturn sm.subj, sm.seq, sm.hdr, sm.msg, sm.ts, nil\n}\n\n// Type returns the type of the underlying store.\nfunc (fs *fileStore) Type() StorageType {\n\treturn FileStorage\n}\n\n// FastState will fill in state with only the following.\n// Msgs, Bytes, FirstSeq, LastSeq\nfunc (fs *fileStore) FastState(state *StreamState) {\n\tfs.mu.RLock()\n\tstate.Msgs = fs.state.Msgs\n\tstate.Bytes = fs.state.Bytes\n\tstate.FirstSeq = fs.state.FirstSeq\n\tstate.LastSeq = fs.state.LastSeq\n\tfs.mu.RUnlock()\n}\n\n// State returns the current state of the stream.\nfunc (fs *fileStore) State() StreamState {\n\tfs.mu.RLock()\n\tstate := fs.state\n\tstate.Consumers = len(fs.cfs)\n\tstate.Deleted = nil // make sure.\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t\tfseq := mb.first.seq\n\t\tfor seq := range mb.dmap {\n\t\t\tif seq <= fseq {\n\t\t\t\tdelete(mb.dmap, seq)\n\t\t\t} else {\n\t\t\t\tstate.Deleted = append(state.Deleted, seq)\n\t\t\t}\n\t\t}\n\t\tmb.mu.Unlock()\n\t}\n\tfs.mu.RUnlock()\n\n\tstate.Lost = fs.lostData()\n\n\t// Can not be guaranteed to be sorted.\n\tif len(state.Deleted) > 0 {\n\t\tsort.Slice(state.Deleted, func(i, j int) bool {\n\t\t\treturn state.Deleted[i] < state.Deleted[j]\n\t\t})\n\t\tstate.NumDeleted = len(state.Deleted)\n\t}\n\treturn state\n}\n\nfunc (fs *fileStore) Utilization() (total, reported uint64, err error) {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\treported += mb.bytes\n\t\ttotal += mb.rbytes\n\t\tmb.mu.RUnlock()\n\t}\n\treturn total, reported, nil\n}\n\nconst emptyRecordLen = 22 + 8\n\nfunc fileStoreMsgSize(subj string, hdr, msg []byte) uint64 {\n\tif len(hdr) == 0 {\n\t\t// length of the message record (4bytes) + seq(8) + ts(8) + subj_len(2) + subj + msg + hash(8)\n\t\treturn uint64(22 + len(subj) + len(msg) + 8)\n\t}\n\t// length of the message record (4bytes) + seq(8) + ts(8) + subj_len(2) + subj + hdr_len(4) + hdr + msg + hash(8)\n\treturn uint64(22 + len(subj) + 4 + len(hdr) + len(msg) + 8)\n}\n\nfunc fileStoreMsgSizeEstimate(slen, maxPayload int) uint64 {\n\treturn uint64(emptyRecordLen + slen + 4 + maxPayload)\n}\n\n// Write index info to the appropriate file.\n// Lock should be held.\nfunc (mb *msgBlock) writeIndexInfo() error {\n\t// HEADER: magic version msgs bytes fseq fts lseq lts ndel checksum\n\tvar hdr [indexHdrSize]byte\n\n\t// Write header\n\thdr[0] = magic\n\thdr[1] = version\n\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tn := hdrLen\n\tn += binary.PutUvarint(hdr[n:], mb.msgs)\n\tn += binary.PutUvarint(hdr[n:], mb.bytes)\n\tn += binary.PutUvarint(hdr[n:], mb.first.seq)\n\tn += binary.PutVarint(hdr[n:], mb.first.ts)\n\tn += binary.PutUvarint(hdr[n:], mb.last.seq)\n\tn += binary.PutVarint(hdr[n:], mb.last.ts)\n\tn += binary.PutUvarint(hdr[n:], uint64(len(mb.dmap)))\n\tbuf := append(hdr[:n], mb.lchk[:]...)\n\n\t// Append a delete map if needed\n\tif len(mb.dmap) > 0 {\n\t\tbuf = append(buf, mb.genDeleteMap()...)\n\t}\n\tvar err error\n\tif mb.ifd == nil {\n\t\tifd, err := os.OpenFile(mb.ifn, os.O_CREATE|os.O_RDWR, defaultFilePerms)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmb.ifd = ifd\n\t}\n\n\tmb.lwits = time.Now().UnixNano()\n\n\t// Encrypt if needed.\n\tif mb.aek != nil {\n\t\tbuf = mb.aek.Seal(buf[:0], mb.nonce, buf, nil)\n\t}\n\n\tif n, err = mb.ifd.WriteAt(buf, 0); err == nil {\n\t\tmb.liwsz = int64(n)\n\t\tmb.werr = nil\n\t} else {\n\t\tmb.werr = err\n\t}\n\n\treturn err\n}\n\n// readIndexInfo will read in the index information for the message block.\nfunc (mb *msgBlock) readIndexInfo() error {\n\tbuf, err := ioutil.ReadFile(mb.ifn)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Decrypt if needed.\n\tif mb.aek != nil {\n\t\tbuf, err = mb.aek.Open(buf[:0], mb.nonce, buf, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := checkHeader(buf); err != nil {\n\t\tdefer os.Remove(mb.ifn)\n\t\treturn fmt.Errorf(\"bad index file\")\n\t}\n\n\tbi := hdrLen\n\n\t// Helpers, will set i to -1 on error.\n\treadSeq := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tseq, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn seq &^ ebit\n\t}\n\treadCount := readSeq\n\treadTimeStamp := func() int64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tts, n := binary.Varint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn -1\n\t\t}\n\t\tbi += n\n\t\treturn ts\n\t}\n\tmb.msgs = readCount()\n\tmb.bytes = readCount()\n\tmb.first.seq = readSeq()\n\tmb.first.ts = readTimeStamp()\n\tmb.last.seq = readSeq()\n\tmb.last.ts = readTimeStamp()\n\tdmapLen := readCount()\n\n\t// Check if this is a short write index file.\n\tif bi < 0 || bi+checksumSize > len(buf) {\n\t\tdefer os.Remove(mb.ifn)\n\t\treturn fmt.Errorf(\"short index file\")\n\t}\n\n\t// Checksum\n\tcopy(mb.lchk[0:], buf[bi:bi+checksumSize])\n\tbi += checksumSize\n\n\t// Now check for presence of a delete map\n\tif dmapLen > 0 {\n\t\tmb.dmap = make(map[uint64]struct{}, dmapLen)\n\t\tfor i := 0; i < int(dmapLen); i++ {\n\t\t\tseq := readSeq()\n\t\t\tif seq == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tmb.dmap[seq+mb.first.seq] = struct{}{}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (mb *msgBlock) genDeleteMap() []byte {\n\tif len(mb.dmap) == 0 {\n\t\treturn nil\n\t}\n\tbuf := make([]byte, len(mb.dmap)*binary.MaxVarintLen64)\n\t// We use first seq as an offset to cut down on size.\n\tfseq, n := uint64(mb.first.seq), 0\n\tfor seq := range mb.dmap {\n\t\t// This is for lazy cleanup as the first sequence moves up.\n\t\tif seq <= fseq {\n\t\t\tdelete(mb.dmap, seq)\n\t\t} else {\n\t\t\tn += binary.PutUvarint(buf[n:], seq-fseq)\n\t\t}\n\t}\n\treturn buf[:n]\n}\n\nfunc syncAndClose(mfd, ifd *os.File) {\n\tif mfd != nil {\n\t\tmfd.Sync()\n\t\tmfd.Close()\n\t}\n\tif ifd != nil {\n\t\tifd.Sync()\n\t\tifd.Close()\n\t}\n}\n\n// Will return total number of cache loads.\nfunc (fs *fileStore) cacheLoads() uint64 {\n\tvar tl uint64\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\ttl += mb.cloads\n\t}\n\tfs.mu.RUnlock()\n\treturn tl\n}\n\n// Will return total number of cached bytes.\nfunc (fs *fileStore) cacheSize() uint64 {\n\tvar sz uint64\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.RLock()\n\t\tif mb.cache != nil {\n\t\t\tsz += uint64(len(mb.cache.buf))\n\t\t}\n\t\tmb.mu.RUnlock()\n\t}\n\tfs.mu.RUnlock()\n\treturn sz\n}\n\n// Will return total number of dmapEntries for all msg blocks.\nfunc (fs *fileStore) dmapEntries() int {\n\tvar total int\n\tfs.mu.RLock()\n\tfor _, mb := range fs.blks {\n\t\ttotal += len(mb.dmap)\n\t}\n\tfs.mu.RUnlock()\n\treturn total\n}\n\n// Fixed helper for iterating.\nfunc subjectsEqual(a, b string) bool {\n\treturn a == b\n}\n\nfunc subjectsAll(a, b string) bool {\n\treturn true\n}\n\nfunc compareFn(subject string) func(string, string) bool {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\treturn subjectsAll\n\t}\n\tif subjectHasWildcard(subject) {\n\t\treturn subjectIsSubsetMatch\n\t}\n\treturn subjectsEqual\n}\n\n// PurgeEx will remove messages based on subject filters, sequence and number of messages to keep.\n// Will return the number of purged messages.\nfunc (fs *fileStore) PurgeEx(subject string, sequence, keep uint64) (purged uint64, err error) {\n\tif subject == _EMPTY_ || subject == fwcs {\n\t\tif keep == 0 && (sequence == 0 || sequence == 1) {\n\t\t\treturn fs.Purge()\n\t\t}\n\t\tif sequence > 1 {\n\t\t\treturn fs.Compact(sequence)\n\t\t} else if keep > 0 {\n\t\t\tfs.mu.RLock()\n\t\t\tmsgs, lseq := fs.state.Msgs, fs.state.LastSeq\n\t\t\tfs.mu.RUnlock()\n\t\t\tif keep >= msgs {\n\t\t\t\treturn 0, nil\n\t\t\t}\n\t\t\treturn fs.Compact(lseq - keep + 1)\n\t\t}\n\t\treturn 0, nil\n\t}\n\n\teq, wc := compareFn(subject), subjectHasWildcard(subject)\n\tvar firstSeqNeedsUpdate bool\n\n\t// If we have a \"keep\" designation need to get full filtered state so we know how many to purge.\n\tvar maxp uint64\n\tif keep > 0 {\n\t\tss := fs.FilteredState(1, subject)\n\t\tif keep >= ss.Msgs {\n\t\t\treturn 0, nil\n\t\t}\n\t\tmaxp = ss.Msgs - keep\n\t}\n\n\tfs.mu.Lock()\n\tfor _, mb := range fs.blks {\n\t\tmb.mu.Lock()\n\t\tt, f, l := mb.filteredPendingLocked(subject, wc, mb.first.seq)\n\t\tif t == 0 {\n\t\t\tmb.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tvar shouldExpire bool\n\t\tif !mb.cacheAlreadyLoaded() {\n\t\t\tmb.loadMsgsWithLock()\n\t\t\tshouldExpire = true\n\t\t}\n\t\tif sequence > 0 && sequence <= l {\n\t\t\tl = sequence - 1\n\t\t}\n\t\tfor seq := f; seq <= l; seq++ {\n\t\t\tif sm, _ := mb.cacheLookupWithLock(seq); sm != nil && eq(sm.subj, subject) {\n\t\t\t\trl := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\t\t// Do fast in place remove.\n\t\t\t\t// Stats\n\t\t\t\tfs.state.Msgs--\n\t\t\t\tfs.state.Bytes -= rl\n\t\t\t\tmb.msgs--\n\t\t\t\tmb.bytes -= rl\n\t\t\t\t// FSS updates.\n\t\t\t\tmb.removeSeqPerSubject(sm.subj, seq)\n\t\t\t\t// Check for first message.\n\t\t\t\tif seq == mb.first.seq {\n\t\t\t\t\tmb.selectNextFirst()\n\t\t\t\t\tif mb.isEmpty() {\n\t\t\t\t\t\tfs.removeMsgBlock(mb)\n\t\t\t\t\t\tfirstSeqNeedsUpdate = seq == fs.state.FirstSeq\n\t\t\t\t\t} else if seq == fs.state.FirstSeq {\n\t\t\t\t\t\tfs.state.FirstSeq = mb.first.seq // new one.\n\t\t\t\t\t\tfs.state.FirstTime = time.Unix(0, mb.first.ts).UTC()\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// Out of order delete.\n\t\t\t\t\tif mb.dmap == nil {\n\t\t\t\t\t\tmb.dmap = make(map[uint64]struct{})\n\t\t\t\t\t}\n\t\t\t\t\tmb.dmap[seq] = struct{}{}\n\t\t\t\t}\n\t\t\t\tpurged++\n\t\t\t\tif maxp > 0 && purged >= maxp {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Expire if we were responsible for loading.\n\t\tif shouldExpire {\n\t\t\t// Expire this cache before moving on.\n\t\t\tmb.llts = 0\n\t\t\tmb.expireCacheLocked()\n\t\t}\n\n\t\tmb.mu.Unlock()\n\t\t// Update our index info on disk.\n\t\tmb.writeIndexInfo()\n\t}\n\tif firstSeqNeedsUpdate {\n\t\tfs.selectNextFirst()\n\t}\n\n\tfs.mu.Unlock()\n\treturn purged, nil\n}\n\n// Purge will remove all messages from this store.\n// Will return the number of purged messages.\nfunc (fs *fileStore) Purge() (uint64, error) {\n\treturn fs.purge(0)\n}\n\nfunc (fs *fileStore) purge(fseq uint64) (uint64, error) {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn 0, ErrStoreClosed\n\t}\n\n\tpurged := fs.state.Msgs\n\trbytes := int64(fs.state.Bytes)\n\n\tfs.state.FirstSeq = fs.state.LastSeq + 1\n\tfs.state.FirstTime = time.Time{}\n\n\tfs.state.Bytes = 0\n\tfs.state.Msgs = 0\n\n\tfor _, mb := range fs.blks {\n\t\tmb.dirtyClose()\n\t}\n\n\tfs.blks = nil\n\tfs.lmb = nil\n\n\t// Move the msgs directory out of the way, will delete out of band.\n\t// FIXME(dlc) - These can error and we need to change api above to propagate?\n\tmdir := path.Join(fs.fcfg.StoreDir, msgDir)\n\tpdir := path.Join(fs.fcfg.StoreDir, purgeDir)\n\t// If purge directory still exists then we need to wait\n\t// in place and remove since rename would fail.\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\tos.Rename(mdir, pdir)\n\tgo os.RemoveAll(pdir)\n\t// Create new one.\n\tos.MkdirAll(mdir, defaultDirPerms)\n\n\t// Make sure we have a lmb to write to.\n\tif _, err := fs.newMsgBlockForWrite(); err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn purged, err\n\t}\n\n\t// Check if we need to set the first seq to a new number.\n\tif fseq > fs.state.FirstSeq {\n\t\tfs.state.FirstSeq = fseq\n\t\tfs.state.LastSeq = fseq - 1\n\t}\n\tfs.lmb.first.seq = fs.state.FirstSeq\n\tfs.lmb.last.seq = fs.state.LastSeq\n\tfs.lmb.writeIndexInfo()\n\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -rbytes, 0, _EMPTY_)\n\t}\n\n\treturn purged, nil\n}\n\n// Compact will remove all messages from this store up to\n// but not including the seq parameter.\n// Will return the number of purged messages.\nfunc (fs *fileStore) Compact(seq uint64) (uint64, error) {\n\tif seq == 0 || seq > fs.lastSeq() {\n\t\treturn fs.purge(seq)\n\t}\n\n\tvar purged, bytes uint64\n\n\t// We have to delete interior messages.\n\tfs.mu.Lock()\n\tsmb := fs.selectMsgBlock(seq)\n\tif smb == nil {\n\t\tfs.mu.Unlock()\n\t\treturn 0, nil\n\t}\n\tif err := smb.loadMsgs(); err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn 0, err\n\t}\n\n\t// All msgblocks up to this one can be thrown away.\n\tvar deleted int\n\tfor _, mb := range fs.blks {\n\t\tif mb == smb {\n\t\t\tbreak\n\t\t}\n\t\tmb.mu.Lock()\n\t\tpurged += mb.msgs\n\t\tbytes += mb.bytes\n\t\tmb.dirtyCloseWithRemove(true)\n\t\tmb.mu.Unlock()\n\t\tdeleted++\n\t}\n\n\tsmb.mu.Lock()\n\tfor mseq := smb.first.seq; mseq < seq; mseq++ {\n\t\tsm, err := smb.cacheLookupWithLock(mseq)\n\t\tif err == errDeletedMsg {\n\t\t\t// Update dmap.\n\t\t\tif len(smb.dmap) > 0 {\n\t\t\t\tdelete(smb.dmap, seq)\n\t\t\t\tif len(smb.dmap) == 0 {\n\t\t\t\t\tsmb.dmap = nil\n\t\t\t\t}\n\t\t\t}\n\t\t} else if sm != nil {\n\t\t\tsz := fileStoreMsgSize(sm.subj, sm.hdr, sm.msg)\n\t\t\tsmb.bytes -= sz\n\t\t\tbytes += sz\n\t\t\tsmb.msgs--\n\t\t\tpurged++\n\t\t\t// Update fss\n\t\t\tsmb.removeSeqPerSubject(sm.subj, mseq)\n\t\t}\n\t}\n\n\t// Check if empty after processing, could happen if tail of messages are all deleted.\n\tisEmpty := smb.msgs == 0\n\tif isEmpty {\n\t\tsmb.dirtyCloseWithRemove(true)\n\t\t// Update fs first here as well.\n\t\tfs.state.FirstSeq = smb.last.seq + 1\n\t\tfs.state.FirstTime = time.Time{}\n\t\tdeleted++\n\t} else {\n\t\t// Update fs first seq and time.\n\t\tsmb.first.seq = seq - 1 // Just for start condition for selectNextFirst.\n\t\tsmb.selectNextFirst()\n\t\tfs.state.FirstSeq = smb.first.seq\n\t\tfs.state.FirstTime = time.Unix(0, smb.first.ts).UTC()\n\t}\n\tsmb.mu.Unlock()\n\n\tif !isEmpty {\n\t\t// Make sure to write out our index info.\n\t\tsmb.writeIndexInfo()\n\t}\n\n\tif deleted > 0 {\n\t\t// Update blks slice.\n\t\tfs.blks = append(fs.blks[:0:0], fs.blks[deleted:]...)\n\t}\n\n\t// Update top level accounting.\n\tfs.state.Msgs -= purged\n\tfs.state.Bytes -= bytes\n\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn purged, nil\n}\n\n// Truncate will truncate a stream store up to and including seq. Sequence needs to be valid.\nfunc (fs *fileStore) Truncate(seq uint64) error {\n\tfs.mu.Lock()\n\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreClosed\n\t}\n\tif fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreSnapshotInProgress\n\t}\n\n\tnlmb := fs.selectMsgBlock(seq)\n\tif nlmb == nil {\n\t\tfs.mu.Unlock()\n\t\treturn ErrInvalidSequence\n\t}\n\tlsm, _ := nlmb.fetchMsg(seq)\n\tif lsm == nil {\n\t\tfs.mu.Unlock()\n\t\treturn ErrInvalidSequence\n\t}\n\n\t// Set lmb to nlmb and make sure writeable.\n\tfs.lmb = nlmb\n\tfs.enableLastMsgBlockForWriting()\n\n\tvar purged, bytes uint64\n\n\t// Truncate our new last message block.\n\tnmsgs, nbytes, err := nlmb.truncate(lsm)\n\tif err != nil {\n\t\tfs.mu.Unlock()\n\t\treturn err\n\t}\n\t// Account for the truncated msgs and bytes.\n\tpurged += nmsgs\n\tbytes += nbytes\n\n\t// Remove any left over msg blocks.\n\tgetLastMsgBlock := func() *msgBlock { return fs.blks[len(fs.blks)-1] }\n\tfor mb := getLastMsgBlock(); mb != nlmb; mb = getLastMsgBlock() {\n\t\tmb.mu.Lock()\n\t\tpurged += mb.msgs\n\t\tbytes += mb.bytes\n\t\tfs.removeMsgBlock(mb)\n\t\tmb.mu.Unlock()\n\t}\n\n\t// Reset last.\n\tfs.state.LastSeq = lsm.seq\n\tfs.state.LastTime = time.Unix(0, lsm.ts).UTC()\n\t// Update msgs and bytes.\n\tfs.state.Msgs -= purged\n\tfs.state.Bytes -= bytes\n\n\tcb := fs.scb\n\tfs.mu.Unlock()\n\n\tif cb != nil {\n\t\tcb(-int64(purged), -int64(bytes), 0, _EMPTY_)\n\t}\n\n\treturn nil\n}\n\nfunc (fs *fileStore) lastSeq() uint64 {\n\tfs.mu.RLock()\n\tseq := fs.state.LastSeq\n\tfs.mu.RUnlock()\n\treturn seq\n}\n\n// Returns number of msg blks.\nfunc (fs *fileStore) numMsgBlocks() int {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn len(fs.blks)\n}\n\n// Will remove our index file.\nfunc (mb *msgBlock) removeIndexFile() {\n\tmb.mu.RLock()\n\tdefer mb.mu.RUnlock()\n\tmb.removeIndexFileLocked()\n}\n\nfunc (mb *msgBlock) removeIndexFileLocked() {\n\tif mb.ifd != nil {\n\t\tmb.ifd.Close()\n\t\tmb.ifd = nil\n\t}\n\tif mb.ifn != _EMPTY_ {\n\t\tos.Remove(mb.ifn)\n\t}\n}\n\n// Removes the msgBlock\n// Both locks should be held.\nfunc (fs *fileStore) removeMsgBlock(mb *msgBlock) {\n\tmb.dirtyCloseWithRemove(true)\n\n\t// Remove from list.\n\tfor i, omb := range fs.blks {\n\t\tif mb == omb {\n\t\t\tblks := append(fs.blks[:i], fs.blks[i+1:]...)\n\t\t\tfs.blks = append(blks[:0:0], blks...)\n\t\t\tbreak\n\t\t}\n\t}\n\t// Check for us being last message block\n\tif mb == fs.lmb {\n\t\t// Creating a new message write block requires that the lmb lock is not held.\n\t\tmb.mu.Unlock()\n\t\tfs.newMsgBlockForWrite()\n\t\tmb.mu.Lock()\n\t}\n}\n\n// Called by purge to simply get rid of the cache and close our fds.\n// Lock should not be held.\nfunc (mb *msgBlock) dirtyClose() {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\tmb.dirtyCloseWithRemove(false)\n}\n\n// Should be called with lock held.\nfunc (mb *msgBlock) dirtyCloseWithRemove(remove bool) {\n\tif mb == nil {\n\t\treturn\n\t}\n\t// Close cache\n\tmb.clearCacheAndOffset()\n\t// Quit our loops.\n\tif mb.qch != nil {\n\t\tclose(mb.qch)\n\t\tmb.qch = nil\n\t}\n\tif mb.mfd != nil {\n\t\tmb.mfd.Close()\n\t\tmb.mfd = nil\n\t}\n\tif mb.ifd != nil {\n\t\tmb.ifd.Close()\n\t\tmb.ifd = nil\n\t}\n\tif remove {\n\t\tif mb.ifn != _EMPTY_ {\n\t\t\tos.Remove(mb.ifn)\n\t\t\tmb.ifn = _EMPTY_\n\t\t}\n\t\tif mb.mfn != _EMPTY_ {\n\t\t\tos.Remove(mb.mfn)\n\t\t\tmb.mfn = _EMPTY_\n\t\t}\n\t}\n}\n\n// Remove a seq from the fss and select new first.\n// Lock should be held.\nfunc (mb *msgBlock) removeSeqPerSubject(subj string, seq uint64) {\n\tss := mb.fss[subj]\n\tif ss == nil {\n\t\treturn\n\t}\n\tif ss.Msgs == 1 {\n\t\tdelete(mb.fss, subj)\n\t\treturn\n\t}\n\n\tss.Msgs--\n\tif seq != ss.First {\n\t\treturn\n\t}\n\t// TODO(dlc) - Might want to optimize this.\n\tfor tseq := seq + 1; tseq <= ss.Last; tseq++ {\n\t\tif sm, _ := mb.cacheLookupWithLock(tseq); sm != nil {\n\t\t\tif sm.subj == subj {\n\t\t\t\tss.First = tseq\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// generatePerSubjectInfo will generate the per subject info via the raw msg block.\nfunc (mb *msgBlock) generatePerSubjectInfo() error {\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tvar shouldExpire bool\n\tif !mb.cacheAlreadyLoaded() {\n\t\tmb.loadMsgsWithLock()\n\t\tshouldExpire = true\n\t}\n\tif mb.fss == nil {\n\t\tmb.fss = make(map[string]*SimpleState)\n\t}\n\tfseq, lseq := mb.first.seq, mb.last.seq\n\tfor seq := fseq; seq <= lseq; seq++ {\n\t\tif sm, _ := mb.cacheLookupWithLock(seq); sm != nil && len(sm.subj) > 0 {\n\t\t\tif ss := mb.fss[sm.subj]; ss != nil {\n\t\t\t\tss.Msgs++\n\t\t\t\tss.Last = seq\n\t\t\t} else {\n\t\t\t\tmb.fss[sm.subj] = &SimpleState{Msgs: 1, First: seq, Last: seq}\n\t\t\t}\n\t\t}\n\t}\n\tif shouldExpire {\n\t\t// Expire this cache before moving on.\n\t\tmb.llts = 0\n\t\tmb.expireCacheLocked()\n\t}\n\treturn nil\n}\n\n// readPerSubjectInfo will attempt to restore the per subject information.\nfunc (mb *msgBlock) readPerSubjectInfo() error {\n\t// Remove after processing regardless.\n\tdefer os.Remove(mb.sfn)\n\n\tconst (\n\t\tfileHashIndex = 16\n\t\tmbHashIndex   = 8\n\t\tminFileSize   = 24\n\t)\n\n\tbuf, err := ioutil.ReadFile(mb.sfn)\n\n\tif err != nil || len(buf) < minFileSize || checkHeader(buf) != nil {\n\t\treturn mb.generatePerSubjectInfo()\n\t}\n\n\t// Check that we did not have any bit flips.\n\tmb.hh.Reset()\n\tmb.hh.Write(buf[0 : len(buf)-fileHashIndex])\n\tfhash := buf[len(buf)-fileHashIndex : len(buf)-mbHashIndex]\n\tif checksum := mb.hh.Sum(nil); !bytes.Equal(checksum, fhash) {\n\t\treturn mb.generatePerSubjectInfo()\n\t}\n\n\tif !bytes.Equal(buf[len(buf)-mbHashIndex:], mb.lchk[:]) {\n\t\treturn mb.generatePerSubjectInfo()\n\t}\n\n\tfss := make(map[string]*SimpleState)\n\n\tbi := hdrLen\n\treadU64 := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tnum, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn num\n\t}\n\n\tfor i, numEntries := uint64(0), readU64(); i < numEntries; i++ {\n\t\tlsubj := readU64()\n\t\tsubj := buf[bi : bi+int(lsubj)]\n\t\tbi += int(lsubj)\n\t\tmsgs, first, last := readU64(), readU64(), readU64()\n\t\tfss[string(subj)] = &SimpleState{Msgs: msgs, First: first, Last: last}\n\t}\n\tmb.mu.Lock()\n\tmb.fss = fss\n\tmb.mu.Unlock()\n\treturn nil\n}\n\n// writePerSubjectInfo will write out per subject information if we are tracking per subject.\n// Lock should be held.\nfunc (mb *msgBlock) writePerSubjectInfo() error {\n\t// Raft groups do not have any subjects.\n\tif len(mb.fss) == 0 {\n\t\treturn nil\n\t}\n\tvar scratch [4 * binary.MaxVarintLen64]byte\n\tvar b bytes.Buffer\n\tb.WriteByte(magic)\n\tb.WriteByte(version)\n\tn := binary.PutUvarint(scratch[0:], uint64(len(mb.fss)))\n\tb.Write(scratch[0:n])\n\tfor subj, ss := range mb.fss {\n\t\tn := binary.PutUvarint(scratch[0:], uint64(len(subj)))\n\t\tb.Write(scratch[0:n])\n\t\tb.WriteString(subj)\n\t\t// Encode all three parts of our simple state into same scratch buffer.\n\t\tn = binary.PutUvarint(scratch[0:], ss.Msgs)\n\t\tn += binary.PutUvarint(scratch[n:], ss.First)\n\t\tn += binary.PutUvarint(scratch[n:], ss.Last)\n\t\tb.Write(scratch[0:n])\n\t}\n\t// Calculate hash for this information.\n\tmb.hh.Reset()\n\tmb.hh.Write(b.Bytes())\n\tb.Write(mb.hh.Sum(nil))\n\t// Now copy over checksum from the block itself, this allows us to know if we are in sync.\n\tb.Write(mb.lchk[:])\n\n\treturn ioutil.WriteFile(mb.sfn, b.Bytes(), defaultFilePerms)\n}\n\nfunc (mb *msgBlock) close(sync bool) {\n\tif mb == nil {\n\t\treturn\n\t}\n\tmb.mu.Lock()\n\tdefer mb.mu.Unlock()\n\n\tif mb.closed {\n\t\treturn\n\t}\n\tmb.closed = true\n\n\t// Check if we are tracking by subject.\n\tif mb.fss != nil {\n\t\tmb.writePerSubjectInfo()\n\t}\n\n\t// Close cache\n\tmb.clearCacheAndOffset()\n\t// Quit our loops.\n\tif mb.qch != nil {\n\t\tclose(mb.qch)\n\t\tmb.qch = nil\n\t}\n\tif sync {\n\t\tsyncAndClose(mb.mfd, mb.ifd)\n\t} else {\n\t\tif mb.mfd != nil {\n\t\t\tmb.mfd.Close()\n\t\t}\n\t\tif mb.ifd != nil {\n\t\t\tmb.ifd.Close()\n\t\t}\n\t}\n\tmb.mfd = nil\n\tmb.ifd = nil\n}\n\nfunc (fs *fileStore) closeAllMsgBlocks(sync bool) {\n\tfor _, mb := range fs.blks {\n\t\tmb.close(sync)\n\t}\n}\n\nfunc (fs *fileStore) Delete() error {\n\tif fs.isClosed() {\n\t\treturn ErrStoreClosed\n\t}\n\tfs.Purge()\n\n\tpdir := path.Join(fs.fcfg.StoreDir, purgeDir)\n\t// If purge directory still exists then we need to wait\n\t// in place and remove since rename would fail.\n\tif _, err := os.Stat(pdir); err == nil {\n\t\tos.RemoveAll(pdir)\n\t}\n\n\tif err := fs.Stop(); err != nil {\n\t\treturn err\n\t}\n\n\terr := os.RemoveAll(fs.fcfg.StoreDir)\n\tif err == nil {\n\t\treturn nil\n\t}\n\tttl := time.Now().Add(time.Second)\n\tfor time.Now().Before(ttl) {\n\t\ttime.Sleep(10 * time.Millisecond)\n\t\tif err = os.RemoveAll(fs.fcfg.StoreDir); err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn err\n}\n\n// Lock should be held.\nfunc (fs *fileStore) cancelSyncTimer() {\n\tif fs.syncTmr != nil {\n\t\tfs.syncTmr.Stop()\n\t\tfs.syncTmr = nil\n\t}\n}\n\nfunc (fs *fileStore) Stop() error {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn ErrStoreClosed\n\t}\n\tfs.closed = true\n\tfs.lmb = nil\n\n\tfs.checkAndFlushAllBlocks()\n\tfs.closeAllMsgBlocks(false)\n\n\tfs.cancelSyncTimer()\n\tfs.cancelAgeChk()\n\n\tvar _cfs [256]*consumerFileStore\n\tcfs := append(_cfs[:0], fs.cfs...)\n\tfs.cfs = nil\n\tfs.mu.Unlock()\n\n\tfor _, o := range cfs {\n\t\to.Stop()\n\t}\n\n\treturn nil\n}\n\nconst errFile = \"errors.txt\"\n\n// Stream our snapshot through S2 compression and tar.\nfunc (fs *fileStore) streamSnapshot(w io.WriteCloser, state *StreamState, includeConsumers bool) {\n\tdefer w.Close()\n\n\tenc := s2.NewWriter(w)\n\tdefer enc.Close()\n\n\ttw := tar.NewWriter(enc)\n\tdefer tw.Close()\n\n\tdefer func() {\n\t\tfs.mu.Lock()\n\t\tfs.sips--\n\t\tfs.mu.Unlock()\n\t}()\n\n\tmodTime := time.Now().UTC()\n\n\twriteFile := func(name string, buf []byte) error {\n\t\thdr := &tar.Header{\n\t\t\tName:    name,\n\t\t\tMode:    0600,\n\t\t\tModTime: modTime,\n\t\t\tUname:   \"nats\",\n\t\t\tGname:   \"nats\",\n\t\t\tSize:    int64(len(buf)),\n\t\t\tFormat:  tar.FormatPAX,\n\t\t}\n\t\tif err := tw.WriteHeader(hdr); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := tw.Write(buf); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\n\twriteErr := func(err string) {\n\t\twriteFile(errFile, []byte(err))\n\t}\n\n\tfs.mu.Lock()\n\tblks := fs.blks\n\t// Grab our general meta data.\n\t// We do this now instead of pulling from files since they could be encrypted.\n\tmeta, err := json.Marshal(fs.cfg)\n\tif err != nil {\n\t\tfs.mu.Unlock()\n\t\twriteErr(fmt.Sprintf(\"Could not gather stream meta file: %v\", err))\n\t\treturn\n\t}\n\tfs.hh.Reset()\n\tfs.hh.Write(meta)\n\tsum := []byte(hex.EncodeToString(fs.hh.Sum(nil)))\n\tfs.mu.Unlock()\n\n\t// Meta first.\n\tif writeFile(JetStreamMetaFile, meta) != nil {\n\t\treturn\n\t}\n\tif writeFile(JetStreamMetaFileSum, sum) != nil {\n\t\treturn\n\t}\n\n\t// Can't use join path here, tar only recognizes relative paths with forward slashes.\n\tmsgPre := msgDir + \"/\"\n\n\tvar bbuf []byte\n\n\t// Now do messages themselves.\n\tfor _, mb := range blks {\n\t\tif mb.pendingWriteSize() > 0 {\n\t\t\tmb.flushPendingMsgsAndWait()\n\t\t\tmb.writeIndexInfo()\n\t\t}\n\t\tmb.mu.Lock()\n\t\tbuf, err := ioutil.ReadFile(mb.ifn)\n\t\tif err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not read message block [%d] index file: %v\", mb.index, err))\n\t\t\treturn\n\t\t}\n\t\t// Check for encryption.\n\t\tif mb.aek != nil && len(buf) > 0 {\n\t\t\tbuf, err = mb.aek.Open(buf[:0], mb.nonce, buf, nil)\n\t\t\tif err != nil {\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\twriteErr(fmt.Sprintf(\"Could not decrypt message block [%d] index file: %v\", mb.index, err))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tif writeFile(msgPre+fmt.Sprintf(indexScan, mb.index), buf) != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\treturn\n\t\t}\n\t\t// We could stream but don't want to hold the lock and prevent changes, so just read in and\n\t\t// release the lock for now.\n\t\tbbuf, err = mb.loadBlock(bbuf)\n\t\tif err != nil {\n\t\t\tmb.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not read message block [%d]: %v\", mb.index, err))\n\t\t\treturn\n\t\t}\n\t\t// Check for encryption.\n\t\tif mb.bek != nil && len(bbuf) > 0 {\n\t\t\trbek, err := chacha20.NewUnauthenticatedCipher(mb.seed, mb.nonce)\n\t\t\tif err != nil {\n\t\t\t\tmb.mu.Unlock()\n\t\t\t\twriteErr(fmt.Sprintf(\"Could not create encryption key for message block [%d]: %v\", mb.index, err))\n\t\t\t\treturn\n\t\t\t}\n\t\t\trbek.XORKeyStream(bbuf, bbuf)\n\t\t}\n\t\tmb.mu.Unlock()\n\t\t// Do this one unlocked.\n\t\tif writeFile(msgPre+fmt.Sprintf(blkScan, mb.index), bbuf) != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Bail if no consumers requested.\n\tif !includeConsumers {\n\t\treturn\n\t}\n\n\t// Do consumers' state last.\n\tfs.mu.Lock()\n\tcfs := fs.cfs\n\tfs.mu.Unlock()\n\n\tfor _, o := range cfs {\n\t\to.mu.Lock()\n\t\t// Grab our general meta data.\n\t\t// We do this now instead of pulling from files since they could be encrypted.\n\t\tmeta, err := json.Marshal(o.cfg)\n\t\tif err != nil {\n\t\t\to.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not gather consumer meta file for %q: %v\", o.name, err))\n\t\t\treturn\n\t\t}\n\t\to.hh.Reset()\n\t\to.hh.Write(meta)\n\t\tsum := []byte(hex.EncodeToString(o.hh.Sum(nil)))\n\n\t\t// We can have the running state directly encoded now.\n\t\tstate, err := o.encodeState()\n\t\tif err != nil {\n\t\t\to.mu.Unlock()\n\t\t\twriteErr(fmt.Sprintf(\"Could not encode consumer state for %q: %v\", o.name, err))\n\t\t\treturn\n\t\t}\n\t\todirPre := consumerDir + \"/\" + o.name\n\t\to.mu.Unlock()\n\n\t\t// Write all the consumer files.\n\t\tif writeFile(path.Join(odirPre, JetStreamMetaFile), meta) != nil {\n\t\t\treturn\n\t\t}\n\t\tif writeFile(path.Join(odirPre, JetStreamMetaFileSum), sum) != nil {\n\t\t\treturn\n\t\t}\n\t\twriteFile(path.Join(odirPre, consumerState), state)\n\t}\n}\n\n// Create a snapshot of this stream and its consumer's state along with messages.\nfunc (fs *fileStore) Snapshot(deadline time.Duration, checkMsgs, includeConsumers bool) (*SnapshotResult, error) {\n\tfs.mu.Lock()\n\tif fs.closed {\n\t\tfs.mu.Unlock()\n\t\treturn nil, ErrStoreClosed\n\t}\n\t// Only allow one at a time.\n\tif fs.sips > 0 {\n\t\tfs.mu.Unlock()\n\t\treturn nil, ErrStoreSnapshotInProgress\n\t}\n\t// Mark us as snapshotting\n\tfs.sips += 1\n\tfs.mu.Unlock()\n\n\t// We can add to our stream while snapshotting but not delete anything.\n\tstate := fs.State()\n\n\tif checkMsgs {\n\t\tld := fs.checkMsgs()\n\t\tif ld != nil && len(ld.Msgs) > 0 {\n\t\t\treturn nil, fmt.Errorf(\"snapshot check detected %d bad messages\", len(ld.Msgs))\n\t\t}\n\t}\n\n\tpr, pw := net.Pipe()\n\n\t// Set a write deadline here to protect ourselves.\n\tif deadline > 0 {\n\t\tpw.SetWriteDeadline(time.Now().Add(deadline))\n\t}\n\t// Stream in separate Go routine.\n\tgo fs.streamSnapshot(pw, &state, includeConsumers)\n\n\treturn &SnapshotResult{pr, state}, nil\n}\n\n// Helper to return the config.\nfunc (fs *fileStore) fileStoreConfig() FileStoreConfig {\n\tfs.mu.RLock()\n\tdefer fs.mu.RUnlock()\n\treturn fs.fcfg\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Consumers\n////////////////////////////////////////////////////////////////////////////////\n\ntype consumerFileStore struct {\n\tmu      sync.Mutex\n\tfs      *fileStore\n\tcfg     *FileConsumerInfo\n\tprf     keyGen\n\taek     cipher.AEAD\n\tname    string\n\todir    string\n\tifn     string\n\thh      hash.Hash64\n\tstate   ConsumerState\n\tfch     chan struct{}\n\tqch     chan struct{}\n\tflusher bool\n\twriting bool\n\tdirty   bool\n\tclosed  bool\n}\n\nfunc (fs *fileStore) ConsumerStore(name string, cfg *ConsumerConfig) (ConsumerStore, error) {\n\tif fs == nil {\n\t\treturn nil, fmt.Errorf(\"filestore is nil\")\n\t}\n\tif fs.isClosed() {\n\t\treturn nil, ErrStoreClosed\n\t}\n\tif cfg == nil || name == _EMPTY_ {\n\t\treturn nil, fmt.Errorf(\"bad consumer config\")\n\t}\n\todir := path.Join(fs.fcfg.StoreDir, consumerDir, name)\n\tif err := os.MkdirAll(odir, defaultDirPerms); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create consumer directory - %v\", err)\n\t}\n\tcsi := &FileConsumerInfo{ConsumerConfig: *cfg}\n\to := &consumerFileStore{\n\t\tfs:   fs,\n\t\tcfg:  csi,\n\t\tprf:  fs.prf,\n\t\tname: name,\n\t\todir: odir,\n\t\tifn:  path.Join(odir, consumerState),\n\t}\n\tkey := sha256.Sum256([]byte(fs.cfg.Name + \"/\" + name))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create hash: %v\", err)\n\t}\n\to.hh = hh\n\n\t// Check for encryption.\n\tif o.prf != nil {\n\t\tif ekey, err := ioutil.ReadFile(path.Join(odir, JetStreamMetaFileKey)); err == nil {\n\t\t\t// Recover key encryption key.\n\t\t\trb, err := fs.prf([]byte(fs.cfg.Name + tsep + o.name))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tkek, err := chacha20poly1305.NewX(rb)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tns := kek.NonceSize()\n\t\t\tseed, err := kek.Open(nil, ekey[:ns], ekey[ns:], nil)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif o.aek, err = chacha20poly1305.NewX(seed); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Write our meta data iff does not exist.\n\tmeta := path.Join(odir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && os.IsNotExist(err) {\n\t\tcsi.Created = time.Now().UTC()\n\t\tif err := o.writeConsumerMeta(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// If we expect to be encrypted check that what we are restoring is not plaintext.\n\t// This can happen on snapshot restores or conversions.\n\tif o.prf != nil {\n\t\tkeyFile := path.Join(odir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && os.IsNotExist(err) {\n\t\t\tif err := o.writeConsumerMeta(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// Redo the state file as well here if we have one and we can tell it was plaintext.\n\t\t\tif buf, err := ioutil.ReadFile(o.ifn); err == nil {\n\t\t\t\tif _, err := decodeConsumerState(buf); err == nil {\n\t\t\t\t\tif err := ioutil.WriteFile(o.ifn, o.encryptState(buf), defaultFilePerms); err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Create channels to control our flush go routine.\n\to.fch = make(chan struct{}, 1)\n\to.qch = make(chan struct{})\n\tgo o.flushLoop()\n\n\tfs.mu.Lock()\n\tfs.cfs = append(fs.cfs, o)\n\tfs.mu.Unlock()\n\n\treturn o, nil\n}\n\n// Kick flusher for this consumer.\n// Lock should be held.\nfunc (o *consumerFileStore) kickFlusher() {\n\tif o.fch != nil {\n\t\tselect {\n\t\tcase o.fch <- struct{}{}:\n\t\tdefault:\n\t\t}\n\t}\n\to.dirty = true\n}\n\n// Set in flusher status\nfunc (o *consumerFileStore) setInFlusher() {\n\to.mu.Lock()\n\to.flusher = true\n\to.mu.Unlock()\n}\n\n// Clear in flusher status\nfunc (o *consumerFileStore) clearInFlusher() {\n\to.mu.Lock()\n\to.flusher = false\n\to.mu.Unlock()\n}\n\n// Report in flusher status\nfunc (o *consumerFileStore) inFlusher() bool {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\treturn o.flusher\n}\n\n// flushLoop watches for consumer updates and the quit channel.\nfunc (o *consumerFileStore) flushLoop() {\n\to.mu.Lock()\n\tfch, qch := o.fch, o.qch\n\to.mu.Unlock()\n\n\to.setInFlusher()\n\tdefer o.clearInFlusher()\n\n\t// Maintain approximately 10 updates per second per consumer under load.\n\tconst minTime = 100 * time.Millisecond\n\tvar lastWrite time.Time\n\tvar dt *time.Timer\n\n\tsetDelayTimer := func(addWait time.Duration) {\n\t\tif dt == nil {\n\t\t\tdt = time.NewTimer(addWait)\n\t\t\treturn\n\t\t}\n\t\tif !dt.Stop() {\n\t\t\tselect {\n\t\t\tcase <-dt.C:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t\tdt.Reset(addWait)\n\t}\n\n\tfor {\n\t\tselect {\n\t\tcase <-fch:\n\t\t\tif ts := time.Since(lastWrite); ts < minTime {\n\t\t\t\tsetDelayTimer(minTime - ts)\n\t\t\t\tselect {\n\t\t\t\tcase <-dt.C:\n\t\t\t\tcase <-qch:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\to.mu.Lock()\n\t\t\tif o.closed {\n\t\t\t\to.mu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tbuf, err := o.encodeState()\n\t\t\to.mu.Unlock()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// TODO(dlc) - if we error should start failing upwards.\n\t\t\to.writeState(buf)\n\t\t\tlastWrite = time.Now()\n\t\tcase <-qch:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// UpdateDelivered is called whenever a new message has been delivered.\nfunc (o *consumerFileStore) UpdateDelivered(dseq, sseq, dc uint64, ts int64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif dc != 1 && o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.Delivered.Consumer {\n\t\treturn nil\n\t}\n\n\t// See if we expect an ack for this.\n\tif o.cfg.AckPolicy != AckNone {\n\t\t// Need to create pending records here.\n\t\tif o.state.Pending == nil {\n\t\t\to.state.Pending = make(map[uint64]*Pending)\n\t\t}\n\t\tvar p *Pending\n\t\t// Check for an update to a message already delivered.\n\t\tif sseq <= o.state.Delivered.Stream {\n\t\t\tif p = o.state.Pending[sseq]; p != nil {\n\t\t\t\tp.Sequence, p.Timestamp = dseq, ts\n\t\t\t}\n\t\t}\n\t\t// Add to pending if needed.\n\t\tif p == nil {\n\t\t\to.state.Pending[sseq] = &Pending{dseq, ts}\n\t\t}\n\t\t// Update delivered as needed.\n\t\tif dseq > o.state.Delivered.Consumer {\n\t\t\to.state.Delivered.Consumer = dseq\n\t\t}\n\t\tif sseq > o.state.Delivered.Stream {\n\t\t\to.state.Delivered.Stream = sseq\n\t\t}\n\n\t\tif dc > 1 {\n\t\t\tif o.state.Redelivered == nil {\n\t\t\t\to.state.Redelivered = make(map[uint64]uint64)\n\t\t\t}\n\t\t\to.state.Redelivered[sseq] = dc - 1\n\t\t}\n\t} else {\n\t\t// For AckNone just update delivered and ackfloor at the same time.\n\t\to.state.Delivered.Consumer = dseq\n\t\to.state.Delivered.Stream = sseq\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\t}\n\t// Make sure we flush to disk.\n\to.kickFlusher()\n\n\treturn nil\n}\n\n// UpdateAcks is called whenever a consumer with explicit ack or ack all acks a message.\nfunc (o *consumerFileStore) UpdateAcks(dseq, sseq uint64) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tif o.cfg.AckPolicy == AckNone {\n\t\treturn ErrNoAckPolicy\n\t}\n\tif len(o.state.Pending) == 0 || o.state.Pending[sseq] == nil {\n\t\treturn ErrStoreMsgNotFound\n\t}\n\n\t// On restarts the old leader may get a replay from the raft logs that are old.\n\tif dseq <= o.state.AckFloor.Consumer {\n\t\treturn nil\n\t}\n\n\t// Check for AckAll here.\n\tif o.cfg.AckPolicy == AckAll {\n\t\tsgap := sseq - o.state.AckFloor.Stream\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\t\tfor seq := sseq; seq > sseq-sgap; seq-- {\n\t\t\tdelete(o.state.Pending, seq)\n\t\t\tif len(o.state.Redelivered) > 0 {\n\t\t\t\tdelete(o.state.Redelivered, seq)\n\t\t\t}\n\t\t}\n\t\to.kickFlusher()\n\t\treturn nil\n\t}\n\n\t// AckExplicit\n\n\t// First delete from our pending state.\n\tif p, ok := o.state.Pending[sseq]; ok {\n\t\tdelete(o.state.Pending, sseq)\n\t\tdseq = p.Sequence // Use the original.\n\t}\n\t// Now remove from redelivered.\n\tif len(o.state.Redelivered) > 0 {\n\t\tdelete(o.state.Redelivered, sseq)\n\t}\n\n\tif len(o.state.Pending) == 0 {\n\t\to.state.AckFloor.Consumer = o.state.Delivered.Consumer\n\t\to.state.AckFloor.Stream = o.state.Delivered.Stream\n\t} else if dseq == o.state.AckFloor.Consumer+1 {\n\t\tfirst := o.state.AckFloor.Consumer == 0\n\t\to.state.AckFloor.Consumer = dseq\n\t\to.state.AckFloor.Stream = sseq\n\n\t\tif !first && o.state.Delivered.Consumer > dseq {\n\t\t\tfor ss := sseq + 1; ss < o.state.Delivered.Stream; ss++ {\n\t\t\t\tif p, ok := o.state.Pending[ss]; ok {\n\t\t\t\t\tif p.Sequence > 0 {\n\t\t\t\t\t\to.state.AckFloor.Consumer = p.Sequence - 1\n\t\t\t\t\t\to.state.AckFloor.Stream = ss - 1\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\to.kickFlusher()\n\treturn nil\n}\n\nconst seqsHdrSize = 6*binary.MaxVarintLen64 + hdrLen\n\n// Encode our consumer state, version 2.\n// Lock should be held.\nfunc (o *consumerFileStore) encodeState() ([]byte, error) {\n\tif o.closed {\n\t\treturn nil, ErrStoreClosed\n\t}\n\treturn encodeConsumerState(&o.state), nil\n}\n\nfunc encodeConsumerState(state *ConsumerState) []byte {\n\tvar hdr [seqsHdrSize]byte\n\tvar buf []byte\n\n\tmaxSize := seqsHdrSize\n\tif lp := len(state.Pending); lp > 0 {\n\t\tmaxSize += lp*(3*binary.MaxVarintLen64) + binary.MaxVarintLen64\n\t}\n\tif lr := len(state.Redelivered); lr > 0 {\n\t\tmaxSize += lr*(2*binary.MaxVarintLen64) + binary.MaxVarintLen64\n\t}\n\tif maxSize == seqsHdrSize {\n\t\tbuf = hdr[:seqsHdrSize]\n\t} else {\n\t\tbuf = make([]byte, maxSize)\n\t}\n\n\t// Write header\n\tbuf[0] = magic\n\tbuf[1] = 2\n\n\tn := hdrLen\n\tn += binary.PutUvarint(buf[n:], state.AckFloor.Consumer)\n\tn += binary.PutUvarint(buf[n:], state.AckFloor.Stream)\n\tn += binary.PutUvarint(buf[n:], state.Delivered.Consumer)\n\tn += binary.PutUvarint(buf[n:], state.Delivered.Stream)\n\tn += binary.PutUvarint(buf[n:], uint64(len(state.Pending)))\n\n\tasflr := state.AckFloor.Stream\n\tadflr := state.AckFloor.Consumer\n\n\t// These are optional, but always write len. This is to avoid a truncate inline.\n\tif len(state.Pending) > 0 {\n\t\t// To save space we will use now rounded to seconds to be base timestamp.\n\t\tmints := time.Now().Round(time.Second).Unix()\n\t\t// Write minimum timestamp we found from above.\n\t\tn += binary.PutVarint(buf[n:], mints)\n\n\t\tfor k, v := range state.Pending {\n\t\t\tn += binary.PutUvarint(buf[n:], k-asflr)\n\t\t\tn += binary.PutUvarint(buf[n:], v.Sequence-adflr)\n\t\t\t// Downsample to seconds to save on space.\n\t\t\t// Subsecond resolution not needed for recovery etc.\n\t\t\tts := v.Timestamp / 1_000_000_000\n\t\t\tn += binary.PutVarint(buf[n:], mints-ts)\n\t\t}\n\t}\n\n\t// We always write the redelivered len.\n\tn += binary.PutUvarint(buf[n:], uint64(len(state.Redelivered)))\n\n\t// We expect these to be small.\n\tif len(state.Redelivered) > 0 {\n\t\tfor k, v := range state.Redelivered {\n\t\t\tn += binary.PutUvarint(buf[n:], k-asflr)\n\t\t\tn += binary.PutUvarint(buf[n:], v)\n\t\t}\n\t}\n\n\treturn buf[:n]\n}\n\nfunc (o *consumerFileStore) Update(state *ConsumerState) error {\n\t// Sanity checks.\n\tif state.AckFloor.Consumer > state.Delivered.Consumer {\n\t\treturn fmt.Errorf(\"bad ack floor for consumer\")\n\t}\n\tif state.AckFloor.Stream > state.Delivered.Stream {\n\t\treturn fmt.Errorf(\"bad ack floor for stream\")\n\t}\n\n\t// Copy to our state.\n\tvar pending map[uint64]*Pending\n\tvar redelivered map[uint64]uint64\n\tif len(state.Pending) > 0 {\n\t\tpending = make(map[uint64]*Pending, len(state.Pending))\n\t\tfor seq, p := range state.Pending {\n\t\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t\t}\n\t\tfor seq := range pending {\n\t\t\tif seq <= state.AckFloor.Stream || seq > state.Delivered.Stream {\n\t\t\t\treturn fmt.Errorf(\"bad pending entry, sequence [%d] out of range\", seq)\n\t\t\t}\n\t\t}\n\t}\n\tif len(state.Redelivered) > 0 {\n\t\tredelivered = make(map[uint64]uint64, len(state.Redelivered))\n\t\tfor seq, dc := range state.Redelivered {\n\t\t\tredelivered[seq] = dc\n\t\t}\n\t}\n\n\t// Replace our state.\n\to.mu.Lock()\n\n\t// Check to see if this is an outdated update.\n\tif state.Delivered.Consumer < o.state.Delivered.Consumer {\n\t\to.mu.Unlock()\n\t\treturn fmt.Errorf(\"old update ignored\")\n\t}\n\n\to.state.Delivered = state.Delivered\n\to.state.AckFloor = state.AckFloor\n\to.state.Pending = pending\n\to.state.Redelivered = redelivered\n\to.kickFlusher()\n\to.mu.Unlock()\n\n\treturn nil\n}\n\n// Will encrypt the state with our asset key. Will be a no-op if encryption not enabled.\n// Lock should be held.\nfunc (o *consumerFileStore) encryptState(buf []byte) []byte {\n\tif o.aek == nil {\n\t\treturn buf\n\t}\n\t// TODO(dlc) - Optimize on space usage a bit?\n\tnonce := make([]byte, o.aek.NonceSize(), o.aek.NonceSize()+len(buf)+o.aek.Overhead())\n\tmrand.Read(nonce)\n\treturn o.aek.Seal(nonce, nonce, buf, nil)\n}\n\nfunc (o *consumerFileStore) writeState(buf []byte) error {\n\t// Check if we have the index file open.\n\to.mu.Lock()\n\tif o.writing || len(buf) == 0 {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\n\t// Check on encryption.\n\tif o.aek != nil {\n\t\tbuf = o.encryptState(buf)\n\t}\n\n\to.writing = true\n\to.dirty = false\n\tifn := o.ifn\n\to.mu.Unlock()\n\n\t// Lock not held here.\n\terr := ioutil.WriteFile(ifn, buf, defaultFilePerms)\n\n\to.mu.Lock()\n\tif err != nil {\n\t\to.dirty = true\n\t}\n\to.writing = false\n\to.mu.Unlock()\n\n\treturn err\n}\n\n// Will upodate the config. Only used when recovering ephemerals.\nfunc (o *consumerFileStore) updateConfig(cfg ConsumerConfig) error {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\to.cfg = &FileConsumerInfo{ConsumerConfig: cfg}\n\treturn o.writeConsumerMeta()\n}\n\n// Write out the consumer meta data, i.e. state.\n// Lock should be held.\nfunc (cfs *consumerFileStore) writeConsumerMeta() error {\n\tmeta := path.Join(cfs.odir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); err != nil && !os.IsNotExist(err) {\n\t\treturn err\n\t}\n\n\tif cfs.prf != nil && cfs.aek == nil {\n\t\tfs := cfs.fs\n\t\tkey, _, _, encrypted, err := fs.genEncryptionKeys(fs.cfg.Name + tsep + cfs.name)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcfs.aek = key\n\t\tkeyFile := path.Join(cfs.odir, JetStreamMetaFileKey)\n\t\tif _, err := os.Stat(keyFile); err != nil && !os.IsNotExist(err) {\n\t\t\treturn err\n\t\t}\n\t\tif err := ioutil.WriteFile(keyFile, encrypted, defaultFilePerms); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tb, err := json.Marshal(cfs.cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Encrypt if needed.\n\tif cfs.aek != nil {\n\t\tnonce := make([]byte, cfs.aek.NonceSize(), cfs.aek.NonceSize()+len(b)+cfs.aek.Overhead())\n\t\tmrand.Read(nonce)\n\t\tb = cfs.aek.Seal(nonce, nonce, b, nil)\n\t}\n\n\tif err := ioutil.WriteFile(meta, b, defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\tcfs.hh.Reset()\n\tcfs.hh.Write(b)\n\tchecksum := hex.EncodeToString(cfs.hh.Sum(nil))\n\tsum := path.Join(cfs.odir, JetStreamMetaFileSum)\n\tif err := ioutil.WriteFile(sum, []byte(checksum), defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Make sure the header is correct.\nfunc checkHeader(hdr []byte) error {\n\tif hdr == nil || len(hdr) < 2 || hdr[0] != magic || hdr[1] != version {\n\t\treturn errCorruptState\n\t}\n\treturn nil\n}\n\n// Consumer version.\nfunc checkConsumerHeader(hdr []byte) (uint8, error) {\n\tif hdr == nil || len(hdr) < 2 || hdr[0] != magic {\n\t\treturn 0, errCorruptState\n\t}\n\tversion := hdr[1]\n\tswitch version {\n\tcase 1, 2:\n\t\treturn version, nil\n\t}\n\treturn 0, fmt.Errorf(\"unsupported version: %d\", version)\n}\n\nfunc (o *consumerFileStore) copyPending() map[uint64]*Pending {\n\tpending := make(map[uint64]*Pending, len(o.state.Pending))\n\tfor seq, p := range o.state.Pending {\n\t\tpending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t}\n\treturn pending\n}\n\nfunc (o *consumerFileStore) copyRedelivered() map[uint64]uint64 {\n\tredelivered := make(map[uint64]uint64, len(o.state.Redelivered))\n\tfor seq, dc := range o.state.Redelivered {\n\t\tredelivered[seq] = dc\n\t}\n\treturn redelivered\n}\n\n// State retrieves the state from the state file.\n// This is not expected to be called in high performance code, only on startup.\nfunc (o *consumerFileStore) State() (*ConsumerState, error) {\n\to.mu.Lock()\n\tdefer o.mu.Unlock()\n\n\tvar state *ConsumerState\n\n\t// See if we have a running state or if we need to read in from disk.\n\tif o.state.Delivered.Consumer != 0 {\n\t\tstate = &ConsumerState{}\n\t\tstate.Delivered = o.state.Delivered\n\t\tstate.AckFloor = o.state.AckFloor\n\t\tif len(o.state.Pending) > 0 {\n\t\t\tstate.Pending = o.copyPending()\n\t\t}\n\t\tif len(o.state.Redelivered) > 0 {\n\t\t\tstate.Redelivered = o.copyRedelivered()\n\t\t}\n\t\treturn state, nil\n\t}\n\n\t// Read the state in here from disk..\n\tbuf, err := ioutil.ReadFile(o.ifn)\n\tif err != nil && !os.IsNotExist(err) {\n\t\treturn nil, err\n\t}\n\n\tif len(buf) == 0 {\n\t\treturn state, nil\n\t}\n\n\t// Check on encryption.\n\tif o.aek != nil {\n\t\tns := o.aek.NonceSize()\n\t\tbuf, err = o.aek.Open(nil, buf[:ns], buf[ns:], nil)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tstate, err = decodeConsumerState(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy this state into our own.\n\to.state.Delivered = state.Delivered\n\to.state.AckFloor = state.AckFloor\n\tif len(state.Pending) > 0 {\n\t\to.state.Pending = make(map[uint64]*Pending, len(state.Pending))\n\t\tfor seq, p := range state.Pending {\n\t\t\to.state.Pending[seq] = &Pending{p.Sequence, p.Timestamp}\n\t\t}\n\t}\n\tif len(state.Redelivered) > 0 {\n\t\to.state.Redelivered = make(map[uint64]uint64, len(state.Redelivered))\n\t\tfor seq, dc := range state.Redelivered {\n\t\t\to.state.Redelivered[seq] = dc\n\t\t}\n\t}\n\n\treturn state, nil\n}\n\nfunc decodeConsumerState(buf []byte) (*ConsumerState, error) {\n\tversion, err := checkConsumerHeader(buf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tbi := hdrLen\n\t// Helpers, will set i to -1 on error.\n\treadSeq := func() uint64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tseq, n := binary.Uvarint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn 0\n\t\t}\n\t\tbi += n\n\t\treturn seq\n\t}\n\treadTimeStamp := func() int64 {\n\t\tif bi < 0 {\n\t\t\treturn 0\n\t\t}\n\t\tts, n := binary.Varint(buf[bi:])\n\t\tif n <= 0 {\n\t\t\tbi = -1\n\t\t\treturn -1\n\t\t}\n\t\tbi += n\n\t\treturn ts\n\t}\n\t// Just for clarity below.\n\treadLen := readSeq\n\treadCount := readSeq\n\n\tstate := &ConsumerState{}\n\tstate.AckFloor.Consumer = readSeq()\n\tstate.AckFloor.Stream = readSeq()\n\tstate.Delivered.Consumer = readSeq()\n\tstate.Delivered.Stream = readSeq()\n\n\tif bi == -1 {\n\t\treturn nil, errCorruptState\n\t}\n\tif version == 1 {\n\t\t// Adjust back. Version 1 also stored delivered as next to be delivered,\n\t\t// so adjust that back down here.\n\t\tif state.AckFloor.Consumer > 1 {\n\t\t\tstate.Delivered.Consumer += state.AckFloor.Consumer - 1\n\t\t}\n\t\tif state.AckFloor.Stream > 1 {\n\t\t\tstate.Delivered.Stream += state.AckFloor.Stream - 1\n\t\t}\n\t}\n\n\t// We have additional stuff.\n\tif numPending := readLen(); numPending > 0 {\n\t\tmints := readTimeStamp()\n\t\tstate.Pending = make(map[uint64]*Pending, numPending)\n\t\tfor i := 0; i < int(numPending); i++ {\n\t\t\tsseq := readSeq()\n\t\t\tvar dseq uint64\n\t\t\tif version == 2 {\n\t\t\t\tdseq = readSeq()\n\t\t\t}\n\t\t\tts := readTimeStamp()\n\t\t\tif sseq == 0 || ts == -1 {\n\t\t\t\treturn nil, errCorruptState\n\t\t\t}\n\t\t\t// Adjust seq back.\n\t\t\tsseq += state.AckFloor.Stream\n\t\t\tif version == 2 {\n\t\t\t\tdseq += state.AckFloor.Consumer\n\t\t\t}\n\t\t\t// Adjust the timestamp back.\n\t\t\tif version == 1 {\n\t\t\t\tts = (ts + mints) * int64(time.Second)\n\t\t\t} else {\n\t\t\t\tts = (mints - ts) * int64(time.Second)\n\t\t\t}\n\t\t\t// Store in pending.\n\t\t\tstate.Pending[sseq] = &Pending{dseq, ts}\n\t\t}\n\t}\n\n\t// We have redelivered entries here.\n\tif numRedelivered := readLen(); numRedelivered > 0 {\n\t\tstate.Redelivered = make(map[uint64]uint64, numRedelivered)\n\t\tfor i := 0; i < int(numRedelivered); i++ {\n\t\t\tif seq, n := readSeq(), readCount(); seq > 0 && n > 0 {\n\t\t\t\tstate.Redelivered[seq] = n\n\t\t\t}\n\t\t}\n\t}\n\n\treturn state, nil\n}\n\n// Stop the processing of the consumers's state.\nfunc (o *consumerFileStore) Stop() error {\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\tif o.qch != nil {\n\t\tclose(o.qch)\n\t\to.qch = nil\n\t}\n\n\tvar err error\n\tvar buf []byte\n\n\tif o.dirty {\n\t\t// Make sure to write this out..\n\t\tif buf, err = o.encodeState(); err == nil && len(buf) > 0 {\n\t\t\tif o.aek != nil {\n\t\t\t\tbuf = o.encryptState(buf)\n\t\t\t}\n\t\t}\n\t}\n\n\to.odir = _EMPTY_\n\to.closed = true\n\tifn, fs := o.ifn, o.fs\n\to.mu.Unlock()\n\n\tfs.removeConsumer(o)\n\n\tif len(buf) > 0 {\n\t\to.waitOnFlusher()\n\t\terr = ioutil.WriteFile(ifn, buf, defaultFilePerms)\n\t}\n\treturn err\n}\n\nfunc (o *consumerFileStore) waitOnFlusher() {\n\tif !o.inFlusher() {\n\t\treturn\n\t}\n\n\ttimeout := time.Now().Add(100 * time.Millisecond)\n\tfor time.Now().Before(timeout) {\n\t\tif !o.inFlusher() {\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(10 * time.Millisecond)\n\t}\n}\n\n// Delete the consumer.\nfunc (o *consumerFileStore) Delete() error {\n\treturn o.delete(false)\n}\n\nfunc (o *consumerFileStore) StreamDelete() error {\n\treturn o.delete(true)\n}\n\nfunc (o *consumerFileStore) delete(streamDeleted bool) error {\n\to.mu.Lock()\n\tif o.closed {\n\t\to.mu.Unlock()\n\t\treturn nil\n\t}\n\tif o.qch != nil {\n\t\tclose(o.qch)\n\t\to.qch = nil\n\t}\n\n\tvar err error\n\t// If our stream was deleted it will remove the directories.\n\tif o.odir != _EMPTY_ && !streamDeleted {\n\t\terr = os.RemoveAll(o.odir)\n\t}\n\to.odir = _EMPTY_\n\to.closed = true\n\tfs := o.fs\n\to.mu.Unlock()\n\n\tif !streamDeleted {\n\t\tfs.removeConsumer(o)\n\t}\n\n\treturn err\n}\n\nfunc (fs *fileStore) removeConsumer(cfs *consumerFileStore) {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\tfor i, o := range fs.cfs {\n\t\tif o == cfs {\n\t\t\tfs.cfs = append(fs.cfs[:i], fs.cfs[i+1:]...)\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Templates\n////////////////////////////////////////////////////////////////////////////////\n\ntype templateFileStore struct {\n\tdir string\n\thh  hash.Hash64\n}\n\nfunc newTemplateFileStore(storeDir string) *templateFileStore {\n\ttdir := path.Join(storeDir, tmplsDir)\n\tkey := sha256.Sum256([]byte(\"templates\"))\n\thh, err := highwayhash.New64(key[:])\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn &templateFileStore{dir: tdir, hh: hh}\n}\n\nfunc (ts *templateFileStore) Store(t *streamTemplate) error {\n\tdir := path.Join(ts.dir, t.Name)\n\tif err := os.MkdirAll(dir, defaultDirPerms); err != nil {\n\t\treturn fmt.Errorf(\"could not create templates storage directory for %q- %v\", t.Name, err)\n\t}\n\tmeta := path.Join(dir, JetStreamMetaFile)\n\tif _, err := os.Stat(meta); (err != nil && !os.IsNotExist(err)) || err == nil {\n\t\treturn err\n\t}\n\tt.mu.Lock()\n\tb, err := json.Marshal(t)\n\tt.mu.Unlock()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := ioutil.WriteFile(meta, b, defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\t// FIXME(dlc) - Do checksum\n\tts.hh.Reset()\n\tts.hh.Write(b)\n\tchecksum := hex.EncodeToString(ts.hh.Sum(nil))\n\tsum := path.Join(dir, JetStreamMetaFileSum)\n\tif err := ioutil.WriteFile(sum, []byte(checksum), defaultFilePerms); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (ts *templateFileStore) Delete(t *streamTemplate) error {\n\treturn os.RemoveAll(path.Join(ts.dir, t.Name))\n}\n", "idx": 35, "id": 13914, "msg": "", "proj": "nats-io-nats-server", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -941,9 +941,9 @@ void iterateCIPRanks(const ROMol &mol, DOUBLE_VECT &invars, UINT_VECT &ranks,\n     //\n     // for each atom, get a sorted list of its neighbors' ranks:\n     //\n-    for (int &index : allIndices) {\n-      CIP_ENTRY localEntry;\n-      localEntry.reserve(16);\n+    for (unsigned int index = 0; index < numAtoms; ++index) {\n+      // Note: counts is cleaned up when we drain into cipEntries.\n+      updatedNbrIdxs.clear();\n \n       // start by pushing on our neighbors' ranks:\n       ROMol::OEDGE_ITER beg, end;", "y": 0, "oldf": "//\n//  Copyright (C) 2004-2018 Greg Landrum and Rational Discovery LLC\n//\n//   @@ All Rights Reserved @@\n//  This file is part of the RDKit.\n//  The contents are covered by the terms of the BSD license\n//  which is included in the file license.txt, found at the root\n//  of the RDKit source tree.\n//\n#include <GraphMol/RDKitBase.h>\n#include <RDGeneral/Ranking.h>\n#include <GraphMol/new_canon.h>\n#include <RDGeneral/types.h>\n#include <sstream>\n#include <set>\n#include <algorithm>\n#include <RDGeneral/utils.h>\n#include <RDGeneral/Invariant.h>\n#include <RDGeneral/RDLog.h>\n\n#include <boost/dynamic_bitset.hpp>\n#include <Geometry/point.h>\n#include \"Chirality.h\"\n\n// #define VERBOSE_CANON 1\n\nnamespace RDKit {\n\nnamespace {\nbool shouldDetectDoubleBondStereo(const Bond *bond) {\n  const RingInfo *ri = bond->getOwningMol().getRingInfo();\n  return (!ri->numBondRings(bond->getIdx()) ||\n          ri->minBondRingSize(bond->getIdx()) > 7);\n}\n\n// ----------------------------------- -----------------------------------\n// This algorithm is identical to that used in the CombiCode Mol file\n//  parser (also developed by RD).\n//\n//\n// SUMMARY:\n//   Derive a chiral code for an atom that has a wedged (or dashed) bond\n//   drawn to it.\n//\n// RETURNS:\n//   The chiral type\n//\n// CAVEATS:\n//   This is careful to ensure that the central atom has 4 neighbors and\n//   only single bonds to it, but that's about it.\n//\n// NOTE: this isn't careful at all about checking to make sure that\n// things actually *should* be chiral. e.g. if the file has a\n// 3-coordinate N with a wedged bond, it will make some erroneous\n// assumptions about the chirality.\n//\n// ----------------------------------- -----------------------------------\n\nAtom::ChiralType atomChiralTypeFromBondDir(const ROMol &mol, const Bond *bond,\n                                           const Conformer *conf) {\n  PRECONDITION(bond, \"no bond\");\n  PRECONDITION(conf, \"no conformer\");\n  Bond::BondDir bondDir = bond->getBondDir();\n  PRECONDITION(bondDir == Bond::BEGINWEDGE || bondDir == Bond::BEGINDASH,\n               \"bad bond direction\");\n\n  // NOTE that according to the CT file spec, wedging assigns chirality\n  // to the atom at the point of the wedge, (atom 1 in the bond).\n  const Atom *atom = bond->getBeginAtom();\n  PRECONDITION(atom, \"no atom\");\n\n  // we can't do anything with atoms that have more than 4 neighbors:\n  if (atom->getDegree() > 4) {\n    return Atom::CHI_UNSPECIFIED;\n  }\n  const Atom *bondAtom = bond->getEndAtom();\n\n  Atom::ChiralType res = Atom::CHI_UNSPECIFIED;\n\n  INT_LIST neighborBondIndices;\n  RDGeom::Point3D centerLoc, tmpPt;\n  centerLoc = conf->getAtomPos(atom->getIdx());\n  tmpPt = conf->getAtomPos(bondAtom->getIdx());\n  centerLoc.z = 0.0;\n  tmpPt.z = 0.0;\n\n  RDGeom::Point3D refVect = centerLoc.directionVector(tmpPt);\n\n  //----------------------------------------------------------\n  //\n  //  start by ensuring that all the bonds to neighboring atoms\n  //  are single bonds and collecting a list of neighbor indices:\n  //\n  //----------------------------------------------------------\n  bool hSeen = false;\n\n  neighborBondIndices.push_back(bond->getIdx());\n  if (bondAtom->getAtomicNum() == 1 && bondAtom->getIsotope() == 0) {\n    hSeen = true;\n  }\n\n  bool allSingle = true;\n  ROMol::OEDGE_ITER beg, end;\n  boost::tie(beg, end) = mol.getAtomBonds(atom);\n  while (beg != end) {\n    const Bond *nbrBond = mol[*beg];\n    if (nbrBond->getBondType() != Bond::SINGLE) {\n      allSingle = false;\n      // break;\n    }\n    if (nbrBond != bond) {\n      if ((nbrBond->getOtherAtom(atom)->getAtomicNum() == 1 &&\n           nbrBond->getOtherAtom(atom)->getIsotope() == 0)) {\n        hSeen = true;\n      }\n      neighborBondIndices.push_back(nbrBond->getIdx());\n    }\n    ++beg;\n  }\n  size_t nNbrs = neighborBondIndices.size();\n\n  //----------------------------------------------------------\n  //\n  //  Return now if there aren't at least 3 non-H bonds to the atom.\n  //  (we can implicitly add a single H to 3 coordinate atoms, but\n  //  we're horked otherwise).\n  //\n  //----------------------------------------------------------\n  if (nNbrs < 3 || (hSeen && nNbrs < 4)) {\n    return Atom::CHI_UNSPECIFIED;\n  }\n\n  //----------------------------------------------------------\n  //\n  //  Continue if there are all single bonds or if we're considering\n  //  4-coordinate P or S\n  //\n  //----------------------------------------------------------\n  if (allSingle || atom->getAtomicNum() == 15 || atom->getAtomicNum() == 16) {\n    //------------------------------------------------------------\n    //\n    //  Here we need to figure out the rotation direction between\n    //  the neighbor bonds and the wedged bond:\n    //\n    //------------------------------------------------------------\n    bool isCCW = true;\n    INT_LIST::const_iterator bondIter = neighborBondIndices.begin();\n    ++bondIter;\n    auto bond1 = mol.getBondWithIdx(*bondIter);\n    int oaid = bond1->getOtherAtom(atom)->getIdx();\n    tmpPt = conf->getAtomPos(oaid);\n    tmpPt.z = 0;\n    auto atomVect0 = centerLoc.directionVector(tmpPt);\n    auto angle01 = refVect.signedAngleTo(atomVect0);\n\n    ++bondIter;\n    auto bond2 = mol.getBondWithIdx(*bondIter);\n    oaid = bond2->getOtherAtom(atom)->getIdx();\n    tmpPt = conf->getAtomPos(oaid);\n    tmpPt.z = 0;\n    auto atomVect1 = centerLoc.directionVector(tmpPt);\n    auto angle02 = refVect.signedAngleTo(atomVect1);\n\n    // order everything so that looking from the top in a CCW direction we\n    // have 0, 1, 2\n    bool swappedIt = false;\n    if (angle01 > angle02) {\n      // std::cerr << \" swap because \" << angle01 << \" \" << angle02 << \" \"\n      //           << bond1->getIdx() << \"->\" << bond2->getIdx() << std::endl;\n      std::swap(angle01, angle02);\n      std::swap(bond1, bond2);\n      std::swap(atomVect0, atomVect1);\n      swappedIt = true;\n    }\n\n    double firstAngle, secondAngle;\n    // We proceed differently for 3 and 4 coordinate atoms:\n    double angle12;\n    if (nNbrs == 4) {\n      bool flipIt = false;\n      // grab the angle to the last neighbor:\n      ++bondIter;\n      auto bond3 = mol.getBondWithIdx(*bondIter);\n      oaid = bond3->getOtherAtom(atom)->getIdx();\n      tmpPt = conf->getAtomPos(oaid);\n      tmpPt.z = 0;\n      auto atomVect2 = centerLoc.directionVector(tmpPt);\n      angle12 = refVect.signedAngleTo(atomVect2);\n\n      // find the lowest and second-lowest angle and keep track of\n      // whether or not we have to do a non-cyclic permutation to\n      // get there:\n      if (angle01 < angle02) {\n        if (angle02 < angle12) {\n          // order is angle01 -> angle02 -> angle12\n          firstAngle = angle01;\n          secondAngle = angle02;\n        } else if (angle01 < angle12) {\n          // order is angle01 -> angle12 -> angle02\n          firstAngle = angle01;\n          secondAngle = angle12;\n          flipIt = true;\n        } else {\n          // order is angle12 -> angle01 -> angle02\n          firstAngle = angle12;\n          secondAngle = angle01;\n        }\n      } else if (angle01 < angle12) {\n        // order is angle02 -> angle01 -> angle12\n        firstAngle = angle02;\n        secondAngle = angle01;\n        flipIt = true;\n      } else {\n        if (angle02 < angle12) {\n          // order is angle02 -> angle12 -> angle01\n          firstAngle = angle02;\n          secondAngle = angle12;\n        } else {\n          // order is angle12 -> angle02 -> angle01\n          firstAngle = angle12;\n          secondAngle = angle02;\n          flipIt = true;\n        }\n      }\n      if (flipIt) {\n        isCCW = !isCCW;\n      }\n    } else {\n      // it's three coordinate.  Things are a bit different here\n      // because we have to at least kind of figure out where the\n      // hydrogen might be.\n\n      // before getting started with that, use some of the inchi rules\n      // for contradictory stereochemistry\n      // (Table 10 in the InChi v1 technical manual)\n\n      angle12 = atomVect0.signedAngleTo(atomVect1);\n      double angle20 = atomVect1.signedAngleTo(refVect);\n\n      // to simplify the code below, pick out the directions of the bonds\n      // if and only if they start at our atom:\n      auto dir0 = bondDir;\n      auto dir1 = (bond1->getBeginAtomIdx() == bond->getBeginAtomIdx())\n                      ? bond1->getBondDir()\n                      : Bond::NONE;\n      auto dir2 = (bond2->getBeginAtomIdx() == bond->getBeginAtomIdx())\n                      ? bond2->getBondDir()\n                      : Bond::NONE;\n\n      // we know bond 0 has the direction set\n\n      //  this one is never allowed with different directions:\n      //     0   2\n      //      \\ /\n      //       C\n      //       *\n      //       1\n      if (angle01 < (M_PI - 1e-3) && angle12 < (M_PI - 1e-3) &&\n          angle20 < (M_PI - 1e-3)) {\n        if ((dir1 != Bond::NONE && dir1 != dir0) ||\n            (dir2 != Bond::NONE && dir2 != dir0)) {\n          BOOST_LOG(rdWarningLog)\n              << \"Warning: conflicting stereochemistry at atom \"\n              << bond->getBeginAtomIdx() << \" ignored.\"\n              << \" by rule 1a.\" << std::endl;\n          return Atom::CHI_UNSPECIFIED;\n        }\n      } else {\n        // otherwise they cannot all be the same\n        if (dir1 == dir0 && dir1 == dir2) {\n          BOOST_LOG(rdWarningLog)\n              << \"Warning: conflicting stereochemistry at atom \"\n              << bond->getBeginAtomIdx() << \" ignored.\"\n              << \" by rule 1b.\" << std::endl;\n          return Atom::CHI_UNSPECIFIED;\n        }\n\n        // the remaining cases where stereo is allowed are:\n        //      0        1 0 2\n        //      *         \\*/\n        //  1 - C - 2      C      for these two bond1 and bond2 must match\n        //    and\n        //      1        2 1 0\n        //      *         \\*/\n        //  2 - C - 0      C      for these two bond0 and bond2 must match\n        //    and\n        //      2        0 2 1\n        //      *         \\*/\n        //  0 - C - 1      C      for these two bond0 and bond1 must match\n        //\n\n        if (dir1 != Bond::NONE && dir1 != dir0) {\n          if ((angle01 >= M_PI) ||  // last two examples\n              (angle02 > M_PI && dir2 != Bond::NONE &&\n               dir1 != dir2) ||  // top two examples\n              (angle02 <= M_PI && dir2 != Bond::NONE &&\n               dir0 != dir2)) {  // middle two examples\n            BOOST_LOG(rdWarningLog)\n                << \"Warning: conflicting stereochemistry at atom \"\n                << bond->getBeginAtomIdx() << \" ignored.\"\n                << \" by rule 2a.\" << std::endl;\n            return Atom::CHI_UNSPECIFIED;\n          }\n        } else if (dir0 == dir2) {\n          // all bonds are the same and we already removed the \"all\n          // angles less than 180\" case above\n          BOOST_LOG(rdWarningLog)\n              << \"Warning: conflicting stereochemistry at atom \"\n              << bond->getBeginAtomIdx() << \" ignored.\"\n              << \" by rule 2b.\" << std::endl;\n          return Atom::CHI_UNSPECIFIED;\n        }\n\n        if (angle01 < angle02) {\n          firstAngle = angle01;\n          secondAngle = angle02;\n          isCCW = true;\n        } else {\n          firstAngle = angle02;\n          secondAngle = angle01;\n          isCCW = false;\n        }\n        if (secondAngle - firstAngle >= (M_PI - 1e-4)) {\n          // it's a situation like one of these:\n          //\n          //      0        1 0 2\n          //      *         \\*/\n          //  1 - C - 2      C\n          //\n          // In each of these cases, the implicit H is between atoms 1\n          // and 2, so we need to flip the rotation direction (go\n          // around the back).\n          isCCW = !isCCW;\n        }\n      }\n    }\n    // reverse the rotation direction if the reference is wedged down:\n    if (bondDir == Bond::BEGINDASH) {\n      isCCW = !isCCW;\n    }\n    if (swappedIt) {\n      // we swapped the order of the bonds to simplify the analysis above:\n      isCCW = !isCCW;\n    }\n\n    // ----------------\n    //\n    // We now have the rotation direction using mol-file order.\n    // We need to convert that into the appropriate label for the\n    // central atom\n    //\n    // ----------------\n    int nSwaps = atom->getPerturbationOrder(neighborBondIndices);\n    if (nSwaps % 2) {\n      isCCW = !isCCW;\n    }\n    if (isCCW) {\n      res = Atom::CHI_TETRAHEDRAL_CCW;\n    } else {\n      res = Atom::CHI_TETRAHEDRAL_CW;\n    }\n  }\n\n  return res;\n}  // namespace RDKit\n\nBond::BondDir getOppositeBondDir(Bond::BondDir dir) {\n  PRECONDITION(dir == Bond::ENDDOWNRIGHT || dir == Bond::ENDUPRIGHT,\n               \"bad bond direction\");\n  switch (dir) {\n    case Bond::ENDDOWNRIGHT:\n      return Bond::ENDUPRIGHT;\n    case Bond::ENDUPRIGHT:\n      return Bond::ENDDOWNRIGHT;\n    default:\n      return Bond::NONE;\n  }\n}\n\nvoid setBondDirRelativeToAtom(Bond *bond, Atom *atom, Bond::BondDir dir,\n                              bool reverse, boost::dynamic_bitset<> &needsDir) {\n  PRECONDITION(bond, \"bad bond\");\n  PRECONDITION(atom, \"bad atom\");\n  PRECONDITION(dir == Bond::ENDUPRIGHT || dir == Bond::ENDDOWNRIGHT, \"bad dir\");\n  PRECONDITION(atom == bond->getBeginAtom() || atom == bond->getEndAtom(),\n               \"atom doesn't belong to bond\");\n  RDUNUSED_PARAM(needsDir);\n\n  if (bond->getBeginAtom() != atom) {\n    reverse = !reverse;\n  }\n\n  if (reverse) {\n    dir = (dir == Bond::ENDUPRIGHT ? Bond::ENDDOWNRIGHT : Bond::ENDUPRIGHT);\n  }\n  // to ensure maximum compatibility, even when a bond has unknown stereo (set\n  // explicitly and recorded in _UnknownStereo property), I will still let a\n  // direction to be computed. You must check the _UnknownStereo property to\n  // make sure whether this bond is explicitly set to have no direction info.\n  // This makes sense because the direction info are all derived from\n  // coordinates, the _UnknownStereo property is like extra metadata to be\n  // used with the direction info.\n  bond->setBondDir(dir);\n}\n\nbool isLinearArrangement(const RDGeom::Point3D &v1, const RDGeom::Point3D &v2,\n                         double tol = 0.035) {  // tolerance of 2 degrees\n  return fabs(v2.angleTo(v1) - M_PI) < tol;\n}\nvoid updateDoubleBondNeighbors(ROMol &mol, Bond *dblBond, const Conformer *conf,\n                               boost::dynamic_bitset<> &needsDir,\n                               std::vector<unsigned int> &singleBondCounts,\n                               const VECT_INT_VECT &singleBondNbrs) {\n  // we want to deal only with double bonds:\n  PRECONDITION(dblBond, \"bad bond\");\n  PRECONDITION(dblBond->getBondType() == Bond::DOUBLE, \"not a double bond\");\n  if (!needsDir[dblBond->getIdx()]) {\n    return;\n  }\n  needsDir.set(dblBond->getIdx(), 0);\n#if 0\n  std::cerr << \"**********************\\n\";\n  std::cerr << \"**********************\\n\";\n  std::cerr << \"**********************\\n\";\n  std::cerr << \"UDBN: \" << dblBond->getIdx() << \" \"\n            << dblBond->getBeginAtomIdx() << \"=\" << dblBond->getEndAtomIdx()\n            << \"\\n\";\n#endif\n\n  ROMol::OEDGE_ITER beg, end;\n  std::vector<Bond *> followupBonds;\n\n  Bond *bond1 = nullptr, *obond1 = nullptr;\n  bool squiggleBondSeen = false;\n  bool doubleBondSeen = false;\n  boost::tie(beg, end) = mol.getAtomBonds(dblBond->getBeginAtom());\n  while (beg != end) {\n    Bond *tBond = mol[*beg];\n    if (tBond == dblBond) {\n      ++beg;\n      continue;\n    }\n    if (tBond->getBondType() == Bond::SINGLE ||\n        tBond->getBondType() == Bond::AROMATIC) {\n      // prefer bonds that already have their directionality set\n      // or that are adjacent to more double bonds:\n      if (!bond1) {\n        bond1 = tBond;\n      } else if (needsDir[tBond->getIdx()]) {\n        if (singleBondCounts[tBond->getIdx()] >\n            singleBondCounts[bond1->getIdx()]) {\n          obond1 = bond1;\n          bond1 = tBond;\n        } else {\n          obond1 = tBond;\n        }\n      } else {\n        obond1 = bond1;\n        bond1 = tBond;\n      }\n    } else if (tBond->getBondType() == Bond::DOUBLE) {\n      doubleBondSeen = true;\n    }\n    int explicit_unknown_stereo;\n    if ((tBond->getBondType() == Bond::SINGLE ||\n         tBond->getBondType() == Bond::AROMATIC) &&\n        (tBond->getBondDir() == Bond::UNKNOWN ||\n         ((tBond->getPropIfPresent<int>(common_properties::_UnknownStereo,\n                                        explicit_unknown_stereo) &&\n           explicit_unknown_stereo)))) {\n      squiggleBondSeen = true;\n      break;\n    }\n\n    ++beg;\n  }\n  // Don't do any direction setting if we've seen a squiggle bond, but do mark\n  // the double bond as a crossed bond and return\n  if (!bond1 || squiggleBondSeen || doubleBondSeen) {\n    if (!doubleBondSeen) {\n      // FIX: This is the fix for #2649, but it will need to be modified once we\n      // decide to properly handle allenese\n      dblBond->setBondDir(Bond::EITHERDOUBLE);\n    }\n    return;\n  }\n\n  Bond *bond2 = nullptr, *obond2 = nullptr;\n  boost::tie(beg, end) = mol.getAtomBonds(dblBond->getEndAtom());\n  while (beg != end) {\n    Bond *tBond = mol[*beg];\n    if (tBond == dblBond) {\n      ++beg;\n      continue;\n    }\n    if (tBond->getBondType() == Bond::SINGLE ||\n        tBond->getBondType() == Bond::AROMATIC) {\n      if (!bond2) {\n        bond2 = tBond;\n      } else if (needsDir[tBond->getIdx()]) {\n        if (singleBondCounts[tBond->getIdx()] >\n            singleBondCounts[bond2->getIdx()]) {\n          obond2 = bond2;\n          bond2 = tBond;\n        } else {\n          obond2 = tBond;\n        }\n      } else {\n        // we already had a bond2 and we don't need to set the direction\n        // on the new one, so swap.\n        obond2 = bond2;\n        bond2 = tBond;\n      }\n    } else if (tBond->getBondType() == Bond::DOUBLE) {\n      doubleBondSeen = true;\n    }\n    int explicit_unknown_stereo;\n    if (tBond->getBondType() == Bond::SINGLE &&\n        (tBond->getBondDir() == Bond::UNKNOWN ||\n         ((tBond->getPropIfPresent<int>(common_properties::_UnknownStereo,\n                                        explicit_unknown_stereo) &&\n           explicit_unknown_stereo)))) {\n      squiggleBondSeen = true;\n      break;\n    }\n    ++beg;\n  }\n  // Don't do any direction setting if we've seen a squiggle bond, but do mark\n  // the double bond as a crossed bond and return\n  if (!bond2 || squiggleBondSeen || doubleBondSeen) {\n    if (!doubleBondSeen) {\n      // FIX: This is the fix for #2649, but it will need to be modified once we\n      // decide to properly handle allenese\n      dblBond->setBondDir(Bond::EITHERDOUBLE);\n    }\n    return;\n  }\n\n  CHECK_INVARIANT(bond1 && bond2, \"no bonds found\");\n\n  bool sameTorsionDir = false;\n  if (conf) {\n    RDGeom::Point3D beginP = conf->getAtomPos(dblBond->getBeginAtomIdx());\n    RDGeom::Point3D endP = conf->getAtomPos(dblBond->getEndAtomIdx());\n    RDGeom::Point3D bond1P =\n        conf->getAtomPos(bond1->getOtherAtomIdx(dblBond->getBeginAtomIdx()));\n    RDGeom::Point3D bond2P =\n        conf->getAtomPos(bond2->getOtherAtomIdx(dblBond->getEndAtomIdx()));\n    // check for a linear arrangement of atoms on either end:\n    bool linear = false;\n    RDGeom::Point3D p1;\n    RDGeom::Point3D p2;\n    p1 = bond1P - beginP;\n    p2 = endP - beginP;\n    if (isLinearArrangement(p1, p2)) {\n      if (!obond1) {\n        linear = true;\n      } else {\n        // one of the bonds was linear; what about the other one?\n        Bond *tBond = bond1;\n        bond1 = obond1;\n        obond1 = tBond;\n        bond1P = conf->getAtomPos(\n            bond1->getOtherAtomIdx(dblBond->getBeginAtomIdx()));\n        p1 = bond1P - beginP;\n        if (isLinearArrangement(p1, p2)) {\n          linear = true;\n        }\n      }\n    }\n    if (!linear) {\n      p1 = bond2P - endP;\n      p2 = beginP - endP;\n      if (isLinearArrangement(p1, p2)) {\n        if (!obond2) {\n          linear = true;\n        } else {\n          Bond *tBond = bond2;\n          bond2 = obond2;\n          obond2 = tBond;\n          bond2P = conf->getAtomPos(\n              bond2->getOtherAtomIdx(dblBond->getEndAtomIdx()));\n          p1 = bond2P - beginP;\n          if (isLinearArrangement(p1, p2)) {\n            linear = true;\n          }\n        }\n      }\n    }\n    if (linear) {\n      dblBond->setBondDir(Bond::EITHERDOUBLE);\n      return;\n    }\n\n    double ang = RDGeom::computeDihedralAngle(bond1P, beginP, endP, bond2P);\n    if (ang < M_PI / 2) {\n      sameTorsionDir = false;\n    } else {\n      sameTorsionDir = true;\n    }\n    // std::cerr << \"   angle: \" << ang << \" sameTorsionDir: \" << sameTorsionDir\n    // << \"\\n\";\n  } else {\n    if (dblBond->getStereo() == Bond::STEREOCIS ||\n        dblBond->getStereo() == Bond::STEREOZ) {\n      sameTorsionDir = false;\n    } else if (dblBond->getStereo() == Bond::STEREOTRANS ||\n               dblBond->getStereo() == Bond::STEREOE) {\n      sameTorsionDir = true;\n    } else {\n      return;\n    }\n    // if bond1 or bond2 are not to the stereo-controlling atoms, flip\n    // our expections of the torsion dir\n    int bond1AtomIdx = bond1->getOtherAtomIdx(dblBond->getBeginAtomIdx());\n    if (bond1AtomIdx != dblBond->getStereoAtoms()[0] &&\n        bond1AtomIdx != dblBond->getStereoAtoms()[1]) {\n      sameTorsionDir = !sameTorsionDir;\n    }\n    int bond2AtomIdx = bond2->getOtherAtomIdx(dblBond->getEndAtomIdx());\n    if (bond2AtomIdx != dblBond->getStereoAtoms()[0] &&\n        bond2AtomIdx != dblBond->getStereoAtoms()[1]) {\n      sameTorsionDir = !sameTorsionDir;\n    }\n  }\n\n  /*\n     Time for some clarificatory text, because this gets really\n     confusing really fast.\n\n     The dihedral angle analysis above is based on viewing things\n     with an atom order as follows:\n\n     1\n      \\\n       2 = 3\n            \\\n             4\n\n     so dihedrals > 90 correspond to sameDir=true\n\n     however, the stereochemistry representation is\n     based on something more like this:\n\n     2\n      \\\n       1 = 3\n            \\\n             4\n     (i.e. we consider the direction-setting single bonds to be\n      starting at the double-bonded atom)\n\n  */\n  bool reverseBondDir = sameTorsionDir;\n\n  Atom *atom1 = dblBond->getBeginAtom(), *atom2 = dblBond->getEndAtom();\n  if (needsDir[bond1->getIdx()]) {\n    BOOST_FOREACH (int bidx, singleBondNbrs[bond1->getIdx()]) {\n      // std::cerr << \"       neighbor from: \" << bond1->getIdx() << \" \" << bidx\n      //           << \": \" << needsDir[bidx] << std::endl;\n      if (needsDir[bidx]) {\n        followupBonds.push_back(mol.getBondWithIdx(bidx));\n      }\n    }\n  }\n  if (needsDir[bond2->getIdx()]) {\n    BOOST_FOREACH (int bidx, singleBondNbrs[bond2->getIdx()]) {\n      // std::cerr << \"       neighbor from: \" << bond2->getIdx() << \" \" << bidx\n      //           << \": \" << needsDir[bidx] << std::endl;\n      if (needsDir[bidx]) {\n        followupBonds.push_back(mol.getBondWithIdx(bidx));\n      }\n    }\n  }\n  if (!needsDir[bond1->getIdx()]) {\n    if (!needsDir[bond2->getIdx()]) {\n      // check that we agree\n    } else {\n      if (bond1->getBeginAtom() != atom1) {\n        reverseBondDir = !reverseBondDir;\n      }\n      setBondDirRelativeToAtom(bond2, atom2, bond1->getBondDir(),\n                               reverseBondDir, needsDir);\n    }\n  } else if (!needsDir[bond2->getIdx()]) {\n    if (bond2->getBeginAtom() != atom2) {\n      reverseBondDir = !reverseBondDir;\n    }\n    setBondDirRelativeToAtom(bond1, atom1, bond2->getBondDir(), reverseBondDir,\n                             needsDir);\n  } else {\n    setBondDirRelativeToAtom(bond1, atom1, Bond::ENDDOWNRIGHT, false, needsDir);\n    setBondDirRelativeToAtom(bond2, atom2, Bond::ENDDOWNRIGHT, reverseBondDir,\n                             needsDir);\n  }\n  needsDir[bond1->getIdx()] = 0;\n  needsDir[bond2->getIdx()] = 0;\n  if (obond1 && needsDir[obond1->getIdx()]) {\n    setBondDirRelativeToAtom(obond1, atom1, bond1->getBondDir(),\n                             bond1->getBeginAtom() == atom1, needsDir);\n    needsDir[obond1->getIdx()] = 0;\n  }\n  if (obond2 && needsDir[obond2->getIdx()]) {\n    setBondDirRelativeToAtom(obond2, atom2, bond2->getBondDir(),\n                             bond2->getBeginAtom() == atom2, needsDir);\n    needsDir[obond2->getIdx()] = 0;\n  }\n#if 0\n  std::cerr << \"  1:\" << bond1->getIdx() << \" \";\n  if (obond1)\n    std::cerr << obond1->getIdx() << std::endl;\n  else\n    std::cerr << \"N/A\" << std::endl;\n  std::cerr << \"  2:\" << bond2->getIdx() << \" \";\n  if (obond2)\n    std::cerr << obond2->getIdx() << std::endl;\n  else\n    std::cerr << \"N/A\" << std::endl;\n  std::cerr << \"**********************\\n\";\n  std::cerr << \"**********************\\n\";\n  std::cerr << \"**********************\\n\";\n#endif\n  BOOST_FOREACH (Bond *oDblBond, followupBonds) {\n    // std::cerr << \"FOLLOWUP: \" << oDblBond->getIdx() << \" \"\n    //           << needsDir[oDblBond->getIdx()] << std::endl;\n    updateDoubleBondNeighbors(mol, oDblBond, conf, needsDir, singleBondCounts,\n                              singleBondNbrs);\n  }\n}\n\nbool isBondCandidateForStereo(const Bond *bond) {\n  PRECONDITION(bond, \"no bond\");\n  if (bond->getBondType() == Bond::DOUBLE &&\n      bond->getStereo() != Bond::STEREOANY &&\n      bond->getBondDir() != Bond::EITHERDOUBLE &&\n      bond->getBeginAtom()->getDegree() > 1 &&\n      bond->getEndAtom()->getDegree() > 1 &&\n      shouldDetectDoubleBondStereo(bond)) {\n    return true;\n  }\n  return false;\n}\n\nconst Atom *findHighestCIPNeighbor(const Atom *atom, const Atom *skipAtom) {\n  PRECONDITION(atom, \"bad atom\");\n\n  unsigned bestCipRank = 0;\n  const Atom *bestCipRankedAtom = nullptr;\n  const auto &mol = atom->getOwningMol();\n\n  for (const auto &index :\n       boost::make_iterator_range(mol.getAtomNeighbors(atom))) {\n    const auto neighbor = mol[index];\n    if (neighbor == skipAtom) {\n      continue;\n    }\n    unsigned cip = 0;\n    if (!neighbor->getPropIfPresent(common_properties::_CIPRank, cip)) {\n      // If at least one of the atoms doesn't have a CIP rank, the highest rank\n      // does not make sense, so return a nullptr.\n      return nullptr;\n    } else if (cip > bestCipRank || bestCipRankedAtom == nullptr) {\n      bestCipRank = cip;\n      bestCipRankedAtom = neighbor;\n    } else if (cip == bestCipRank) {\n      // This also doesn't make sense if there is a tie (if that's possible).\n      // We still keep the best CIP rank in case something better comes around\n      // (also not sure if that's possible).\n      BOOST_LOG(rdWarningLog)\n          << \"Warning: duplicate CIP ranks found in findHighestCIPNeighbor()\"\n          << std::endl;\n      bestCipRankedAtom = nullptr;\n    }\n  }\n  return bestCipRankedAtom;\n}\n\n}  // namespace\n\nnamespace Chirality {\ntypedef std::pair<int, int> INT_PAIR;\ntypedef std::vector<INT_PAIR> INT_PAIR_VECT;\ntypedef std::vector<INT_PAIR>::iterator INT_PAIR_VECT_I;\ntypedef std::vector<INT_PAIR>::const_iterator INT_PAIR_VECT_CI;\n\ntypedef INT_VECT CIP_ENTRY;\ntypedef std::vector<CIP_ENTRY> CIP_ENTRY_VECT;\n\ntemplate <typename T>\nvoid debugVect(const std::vector<T> arg) {\n  typename std::vector<T>::const_iterator viIt;\n  std::stringstream outS;\n  for (viIt = arg.begin(); viIt != arg.end(); viIt++) {\n    outS << *viIt << \" \";\n  }\n  BOOST_LOG(rdDebugLog) << outS.str() << std::endl;\n}\n\n// --------------------------------------------------\n//\n// Calculates chiral invariants for the atoms of a molecule\n//  These are based on Labute's proposal in:\n//  \"An Efficient Algorithm for the Determination of Topological\n//   RS Chirality\" Journal of the CCG (1996)\n//\n// --------------------------------------------------\nvoid buildCIPInvariants(const ROMol &mol, DOUBLE_VECT &res) {\n  PRECONDITION(res.size() >= mol.getNumAtoms(), \"res vect too small\");\n  int atsSoFar = 0;\n  //\n  // NOTE:\n  // If you make modifications to this, keep in mind that it is\n  // essential that the initial comparison of ranks behave properly.\n  // So, though it seems like it would makes sense to include\n  // information about the number of Hs (or charge, etc) in the CIP\n  // invariants, this will result in bad rankings.  For example, in\n  // this molecule: OC[C@H](C)O, including the number of Hs would\n  // cause the methyl group (atom 3) to be ranked higher than the CH2\n  // connected to O (atom 1).  This is totally wrong.\n  //\n  // We also don't include any pre-existing stereochemistry information.\n  // Though R and S assignments do factor in to the priorities of atoms,\n  // we're starting here from scratch and we'll let the R and S stuff\n  // be taken into account during the iterations.\n  //\n  for (ROMol::ConstAtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n       ++atIt) {\n    const unsigned short nMassBits = 10;\n    const unsigned short maxMass = 1 << nMassBits;\n    Atom const *atom = *atIt;\n    unsigned long invariant = 0;\n    int num = atom->getAtomicNum() % 128;\n    // get an int with the deviation in the mass from the default:\n    int mass = 0;\n    if (atom->getIsotope()) {\n      mass =\n          atom->getIsotope() -\n          PeriodicTable::getTable()->getMostCommonIsotope(atom->getAtomicNum());\n      if (mass >= 0) {\n        mass += 1;\n      }\n    }\n    mass += maxMass / 2;\n    if (mass < 0) {\n      mass = 0;\n    } else {\n      mass = mass % maxMass;\n    }\n\n#if 0\n        // NOTE: the inclusion of hybridization in the invariant (as\n        // suggested in the original paper), leads to the situation\n        // that\n        //   C[C@@](O)(C=C)C(C)CC\n        // and\n        //   C[C@@](O)(C=C)C(C)CO\n        // are assigned S chirality even though the rest of the world\n        // seems to agree that they ought to be R (atom 3, sp2, is ranked\n        // higher than atom 5, sp3, no matter what their environments)\n        int hyb=0;\n        switch(atom->getHybridization()) {\n        case Atom::SP: hyb=6;break;\n        case Atom::SP2: hyb=5;break;\n        case Atom::SP3: hyb=1;break;\n        case Atom::SP3D: hyb=3;break;\n        case Atom::SP3D2: hyb=2;break;\n        default: break;\n        }\n#endif\n\n    invariant = num;  // 7 bits here\n    invariant = (invariant << nMassBits) | mass;\n\n    int mapnum = -1;\n    atom->getPropIfPresent(common_properties::molAtomMapNumber, mapnum);\n    mapnum = (mapnum + 1) % 1024;  // increment to allow map numbers of zero\n                                   // (though that would be stupid)\n    invariant = (invariant << 10) | mapnum;\n\n    res[atsSoFar++] = invariant;\n  }\n}\n\nvoid iterateCIPRanks(const ROMol &mol, DOUBLE_VECT &invars, UINT_VECT &ranks,\n                     bool seedWithInvars) {\n  PRECONDITION(invars.size() == mol.getNumAtoms(), \"bad invars size\");\n  PRECONDITION(ranks.size() >= mol.getNumAtoms(), \"bad ranks size\");\n\n  unsigned int numAtoms = mol.getNumAtoms();\n  CIP_ENTRY_VECT cipEntries(numAtoms);\n  INT_LIST allIndices;\n  for (unsigned int i = 0; i < numAtoms; ++i) {\n    allIndices.push_back(i);\n  }\n#ifdef VERBOSE_CANON\n  BOOST_LOG(rdDebugLog) << \"invariants:\" << std::endl;\n  for (unsigned int i = 0; i < numAtoms; i++) {\n    BOOST_LOG(rdDebugLog) << i << \": \" << invars[i] << std::endl;\n  }\n#endif\n\n  // rank those:\n  Rankers::rankVect(invars, ranks);\n#ifdef VERBOSE_CANON\n  BOOST_LOG(rdDebugLog) << \"initial ranks:\" << std::endl;\n  for (unsigned int i = 0; i < numAtoms; ++i) {\n    BOOST_LOG(rdDebugLog) << i << \": \" << ranks[i] << std::endl;\n  }\n#endif\n  // Start each atom's rank vector with its atomic number:\n  //  Note: in general one should avoid the temptation to\n  //  use invariants here, those lead to incorrect answers\n  for (unsigned int i = 0; i < numAtoms; i++) {\n    if (!seedWithInvars) {\n      cipEntries[i].push_back(mol[i]->getAtomicNum());\n      cipEntries[i].push_back(static_cast<int>(ranks[i]));\n    } else {\n      cipEntries[i].push_back(static_cast<int>(invars[i]));\n    }\n  }\n\n  // Loop until either:\n  //   1) all classes are uniquified\n  //   2) the number of ranks doesn't change from one iteration to\n  //      the next\n  //   3) we've gone through maxIts times\n  //      maxIts is calculated by dividing the number of atoms\n  //      by 2. That's a pessimal version of the\n  //      maximum number of steps required for two atoms to\n  //      \"feel\" each other (each influences one additional\n  //      neighbor shell per iteration).\n  unsigned int maxIts = numAtoms / 2 + 1;\n  unsigned int numIts = 0;\n  int lastNumRanks = -1;\n  unsigned int numRanks = *std::max_element(ranks.begin(), ranks.end()) + 1;\n  while (numRanks < numAtoms && numIts < maxIts &&\n         (lastNumRanks < 0 ||\n          static_cast<unsigned int>(lastNumRanks) < numRanks)) {\n    unsigned int longestEntry = 0;\n    // ----------------------------------------------------\n    //\n    // for each atom, get a sorted list of its neighbors' ranks:\n    //\n    for (int &index : allIndices) {\n      CIP_ENTRY localEntry;\n      localEntry.reserve(16);\n\n      // start by pushing on our neighbors' ranks:\n      ROMol::OEDGE_ITER beg, end;\n      boost::tie(beg, end) = mol.getAtomBonds(mol[index]);\n      while (beg != end) {\n        const Bond *bond = mol[*beg];\n        ++beg;\n        unsigned int nbrIdx = bond->getOtherAtomIdx(index);\n        const Atom *nbr = mol[nbrIdx];\n\n        int rank = ranks[nbrIdx] + 1;\n        // put the neighbor in 2N times where N is the bond order as a double.\n        // this is to treat aromatic linkages on fair footing. i.e. at least in\n        // the\n        // first iteration --c(:c):c and --C(=C)-C should look the same.\n        // this was part of issue 3009911\n\n        unsigned int count;\n        if (bond->getBondType() == Bond::DOUBLE && nbr->getAtomicNum() == 15 &&\n            (nbr->getDegree() == 4 || nbr->getDegree() == 3)) {\n          // a special case for chiral phosphorous compounds\n          // (this was leading to incorrect assignment of\n          // R/S labels ):\n          count = 1;\n\n          // general justification of this is:\n          // Paragraph 2.2. in the 1966 article is \"Valence-Bond Conventions:\n          // Multiple-Bond Unsaturation and Aromaticity\". It contains several\n          // conventions of which convention (b) is the one applying here:\n          // \"(b) Contributions by d orbitals to bonds of quadriligant atoms are\n          // neglected.\"\n          // FIX: this applies to more than just P\n        } else {\n          count = static_cast<unsigned int>(\n              floor(2. * bond->getBondTypeAsDouble() + .1));\n        }\n        auto ePos =\n            std::lower_bound(localEntry.begin(), localEntry.end(), rank);\n        localEntry.insert(ePos, count, rank);\n        ++nbr;\n      }\n      // add a zero for each coordinated H:\n      // (as long as we're not a query atom)\n      if (!mol[index]->hasQuery()) {\n        localEntry.insert(localEntry.begin(), mol[index]->getTotalNumHs(), 0);\n      }\n\n      // we now have a sorted list of our neighbors' ranks,\n      // copy it on in reversed order:\n      cipEntries[index].insert(cipEntries[index].end(), localEntry.rbegin(),\n                               localEntry.rend());\n      if (cipEntries[index].size() > longestEntry) {\n        longestEntry = rdcast<unsigned int>(cipEntries[index].size());\n      }\n    }\n    // ----------------------------------------------------\n    //\n    // pad the entries so that we compare rounds to themselves:\n    //\n    for (int &index : allIndices) {\n      auto sz = rdcast<unsigned int>(cipEntries[index].size());\n      if (sz < longestEntry) {\n        cipEntries[index].insert(cipEntries[index].end(), longestEntry - sz,\n                                 -1);\n      }\n    }\n    // ----------------------------------------------------\n    //\n    // sort the new ranks and update the list of active indices:\n    //\n    lastNumRanks = numRanks;\n\n    Rankers::rankVect(cipEntries, ranks);\n    numRanks = *std::max_element(ranks.begin(), ranks.end()) + 1;\n\n    // now truncate each vector and stick the rank at the end\n    for (unsigned int i = 0; i < numAtoms; ++i) {\n      cipEntries[i][numIts + 1] = ranks[i];\n      cipEntries[i].erase(cipEntries[i].begin() + numIts + 2,\n                          cipEntries[i].end());\n    }\n\n    ++numIts;\n#ifdef VERBOSE_CANON\n    BOOST_LOG(rdDebugLog) << \"strings and ranks:\" << std::endl;\n    for (unsigned int i = 0; i < numAtoms; i++) {\n      BOOST_LOG(rdDebugLog) << i << \": \" << ranks[i] << \" > \";\n      debugVect(cipEntries[i]);\n    }\n#endif\n  }\n}\n// Figure out the CIP ranks for the atoms of a molecule\nvoid assignAtomCIPRanks(const ROMol &mol, UINT_VECT &ranks) {\n  PRECONDITION((!ranks.size() || ranks.size() >= mol.getNumAtoms()),\n               \"bad ranks size\");\n  if (!ranks.size()) {\n    ranks.resize(mol.getNumAtoms());\n  }\n  unsigned int numAtoms = mol.getNumAtoms();\n#ifndef USE_NEW_STEREOCHEMISTRY\n  // get the initial invariants:\n  DOUBLE_VECT invars(numAtoms, 0);\n  buildCIPInvariants(mol, invars);\n  iterateCIPRanks(mol, invars, ranks, false);\n#else\n  Canon::chiralRankMolAtoms(mol, ranks);\n#endif\n\n  // copy the ranks onto the atoms:\n  for (unsigned int i = 0; i < numAtoms; ++i) {\n    mol[i]->setProp(common_properties::_CIPRank, ranks[i], 1);\n  }\n}\n\n// construct a vector with <atomIdx,direction> pairs for\n// neighbors of a given atom.  This list will only be\n// non-empty if at least one of the bonds has its direction\n// set.\nvoid findAtomNeighborDirHelper(const ROMol &mol, const Atom *atom,\n                               const Bond *refBond, UINT_VECT &ranks,\n                               INT_PAIR_VECT &neighbors,\n                               bool &hasExplicitUnknownStereo) {\n  PRECONDITION(atom, \"bad atom\");\n  PRECONDITION(refBond, \"bad bond\");\n\n  bool seenDir = false;\n  ROMol::OEDGE_ITER beg, end;\n  boost::tie(beg, end) = mol.getAtomBonds(atom);\n  while (beg != end) {\n    const Bond *bond = mol[*beg];\n    // check whether this bond is explicitly set to have unknown stereo\n    if (!hasExplicitUnknownStereo) {\n      int explicit_unknown_stereo;\n      if (bond->getBondDir() == Bond::UNKNOWN  // there's a squiggle bond\n          || (bond->getPropIfPresent<int>(common_properties::_UnknownStereo,\n                                          explicit_unknown_stereo) &&\n              explicit_unknown_stereo)) {\n        hasExplicitUnknownStereo = true;\n      }\n    }\n\n    Bond::BondDir dir = bond->getBondDir();\n    if (bond->getIdx() != refBond->getIdx()) {\n      if (dir == Bond::ENDDOWNRIGHT || dir == Bond::ENDUPRIGHT) {\n        seenDir = true;\n        // If we're considering the bond \"backwards\", (i.e. from end\n        // to beginning, reverse the effective direction:\n        if (atom != bond->getBeginAtom()) {\n          if (dir == Bond::ENDDOWNRIGHT) {\n            dir = Bond::ENDUPRIGHT;\n          } else {\n            dir = Bond::ENDDOWNRIGHT;\n          }\n        }\n      }\n      Atom *nbrAtom = bond->getOtherAtom(atom);\n      neighbors.push_back(std::make_pair(nbrAtom->getIdx(), dir));\n    }\n    ++beg;\n  }\n  if (!seenDir) {\n    neighbors.clear();\n  } else {\n    if (neighbors.size() == 2 &&\n        ranks[neighbors[0].first] == ranks[neighbors[1].first]) {\n      // the two substituents are identical, no stereochemistry here:\n      neighbors.clear();\n    } else {\n      // it's possible that direction was set only one of the bonds, set the\n      // other\n      // bond's direction to be reversed:\n      if (neighbors[0].second != Bond::ENDDOWNRIGHT &&\n          neighbors[0].second != Bond::ENDUPRIGHT) {\n        CHECK_INVARIANT(neighbors.size() > 1, \"too few neighbors\");\n        neighbors[0].second = neighbors[1].second == Bond::ENDDOWNRIGHT\n                                  ? Bond::ENDUPRIGHT\n                                  : Bond::ENDDOWNRIGHT;\n      } else if (neighbors.size() > 1 &&\n                 neighbors[1].second != Bond::ENDDOWNRIGHT &&\n                 neighbors[1].second != Bond::ENDUPRIGHT) {\n        neighbors[1].second = neighbors[0].second == Bond::ENDDOWNRIGHT\n                                  ? Bond::ENDUPRIGHT\n                                  : Bond::ENDDOWNRIGHT;\n      }\n    }\n  }\n}\n\n// find the neighbors for an atoms that are not connected by single bond that is\n// not refBond\n// if checkDir is true only neighbor atoms with bonds marked with a direction\n// will be returned\nvoid findAtomNeighborsHelper(const ROMol &mol, const Atom *atom,\n                             const Bond *refBond, UINT_VECT &neighbors,\n                             bool checkDir = false,\n                             bool includeAromatic = false) {\n  PRECONDITION(atom, \"bad atom\");\n  PRECONDITION(refBond, \"bad bond\");\n  neighbors.clear();\n  ROMol::OEDGE_ITER beg, end;\n  boost::tie(beg, end) = mol.getAtomBonds(atom);\n  while (beg != end) {\n    const Bond *bond = mol[*beg];\n    Bond::BondDir dir = bond->getBondDir();\n    if ((bond->getBondType() == Bond::SINGLE ||\n         (includeAromatic && bond->getBondType() == Bond::AROMATIC)) &&\n        bond->getIdx() != refBond->getIdx()) {\n      if (checkDir) {\n        if ((dir != Bond::ENDDOWNRIGHT) && (dir != Bond::ENDUPRIGHT)) {\n          ++beg;\n          continue;\n        }\n      }\n      Atom *nbrAtom = bond->getOtherAtom(atom);\n      neighbors.push_back(nbrAtom->getIdx());\n    }\n    ++beg;\n  }\n}\n\n// conditions for an atom to be a candidate for ring stereochem:\n//   1) two non-ring neighbors that have different ranks\n//   2) one non-ring neighbor and two ring neighbors (the ring neighbors will\n//      have the same rank)\n//   3) four ring neighbors with three different ranks\n//   4) three ring neighbors with two different ranks\n//     example for this last one: C[C@H]1CC2CCCC3CCCC(C1)[C@@H]23\nbool atomIsCandidateForRingStereochem(const ROMol &mol, const Atom *atom) {\n  PRECONDITION(atom, \"bad atom\");\n  bool res = false;\n  std::set<unsigned int> nbrRanks;\n  if (!atom->getPropIfPresent(common_properties::_ringStereochemCand, res)) {\n    const RingInfo *ringInfo = mol.getRingInfo();\n    if (ringInfo->isInitialized() && ringInfo->numAtomRings(atom->getIdx())) {\n      ROMol::OEDGE_ITER beg, end;\n      boost::tie(beg, end) = mol.getAtomBonds(atom);\n      std::vector<const Atom *> nonRingNbrs;\n      std::vector<const Atom *> ringNbrs;\n      while (beg != end) {\n        const Bond *bond = mol[*beg];\n        if (!ringInfo->numBondRings(bond->getIdx())) {\n          nonRingNbrs.push_back(bond->getOtherAtom(atom));\n        } else {\n          const Atom *nbr = bond->getOtherAtom(atom);\n          ringNbrs.push_back(nbr);\n          unsigned int rnk = 0;\n          nbr->getPropIfPresent(common_properties::_CIPRank, rnk);\n          nbrRanks.insert(rnk);\n        }\n        ++beg;\n      }\n      unsigned int rank1 = 0, rank2 = 0;\n      switch (nonRingNbrs.size()) {\n        case 2:\n          if (nonRingNbrs[0]->getPropIfPresent(common_properties::_CIPRank,\n                                               rank1) &&\n              nonRingNbrs[1]->getPropIfPresent(common_properties::_CIPRank,\n                                               rank2)) {\n            if (rank1 == rank2) {\n              res = false;\n            } else {\n              res = true;\n            }\n          }\n          break;\n        case 1:\n          if (ringNbrs.size() >= 2) {\n            res = true;\n          }\n          break;\n        case 0:\n          if (ringNbrs.size() == 4 && nbrRanks.size() == 3) {\n            res = true;\n          } else if (ringNbrs.size() == 3 && nbrRanks.size() == 2) {\n            res = true;\n          } else {\n            res = false;\n          }\n          break;\n        default:\n          res = false;\n      }\n    }\n    atom->setProp(common_properties::_ringStereochemCand, res, 1);\n  }\n  return res;\n}\n\n// finds all possible chiral special cases.\n// at the moment this is just candidates for ring stereochemistry\nvoid findChiralAtomSpecialCases(ROMol &mol,\n                                boost::dynamic_bitset<> &possibleSpecialCases) {\n  PRECONDITION(possibleSpecialCases.size() >= mol.getNumAtoms(),\n               \"bit vector too small\");\n  possibleSpecialCases.reset();\n  if (!mol.getRingInfo()->isInitialized()) {\n    VECT_INT_VECT sssrs;\n    MolOps::symmetrizeSSSR(mol, sssrs);\n  }\n  boost::dynamic_bitset<> atomsSeen(mol.getNumAtoms());\n  boost::dynamic_bitset<> atomsUsed(mol.getNumAtoms());\n  boost::dynamic_bitset<> bondsSeen(mol.getNumBonds());\n\n  for (ROMol::AtomIterator ait = mol.beginAtoms(); ait != mol.endAtoms();\n       ++ait) {\n    const Atom *atom = *ait;\n    if (atomsSeen[atom->getIdx()]) {\n      continue;\n    }\n    if (atom->getChiralTag() == Atom::CHI_UNSPECIFIED ||\n        atom->hasProp(common_properties::_CIPCode) ||\n        !mol.getRingInfo()->numAtomRings(atom->getIdx()) ||\n        !atomIsCandidateForRingStereochem(mol, atom)) {\n      continue;\n    }\n    // do a BFS from this ring atom along ring bonds and find other\n    // stereochemistry candidates.\n    std::list<const Atom *> nextAtoms;\n    // start with finding viable neighbors\n    ROMol::OEDGE_ITER beg, end;\n    boost::tie(beg, end) = mol.getAtomBonds(atom);\n    while (beg != end) {\n      unsigned int bidx = mol[*beg]->getIdx();\n      if (!bondsSeen[bidx]) {\n        bondsSeen.set(bidx);\n        if (mol.getRingInfo()->numBondRings(bidx)) {\n          const Atom *oatom = mol[*beg]->getOtherAtom(atom);\n          if (!atomsSeen[oatom->getIdx()]) {\n            nextAtoms.push_back(oatom);\n            atomsUsed.set(oatom->getIdx());\n          }\n        }\n      }\n      ++beg;\n    }\n    INT_VECT ringStereoAtoms(0);\n    if (!nextAtoms.empty()) {\n      atom->getPropIfPresent(common_properties::_ringStereoAtoms,\n                             ringStereoAtoms);\n    }\n\n    while (!nextAtoms.empty()) {\n      const Atom *ratom = nextAtoms.front();\n      nextAtoms.pop_front();\n      atomsSeen.set(ratom->getIdx());\n      if (ratom->getChiralTag() != Atom::CHI_UNSPECIFIED &&\n          !ratom->hasProp(common_properties::_CIPCode) &&\n          atomIsCandidateForRingStereochem(mol, ratom)) {\n        int same = (ratom->getChiralTag() == atom->getChiralTag()) ? 1 : -1;\n        ringStereoAtoms.push_back(same * (ratom->getIdx() + 1));\n        INT_VECT oringatoms(0);\n        ratom->getPropIfPresent(common_properties::_ringStereoAtoms,\n                                oringatoms);\n        oringatoms.push_back(same * (atom->getIdx() + 1));\n        ratom->setProp(common_properties::_ringStereoAtoms, oringatoms, true);\n        possibleSpecialCases.set(ratom->getIdx());\n        possibleSpecialCases.set(atom->getIdx());\n      }\n      // now push this atom's neighbors\n      boost::tie(beg, end) = mol.getAtomBonds(ratom);\n      while (beg != end) {\n        unsigned int bidx = mol[*beg]->getIdx();\n        if (!bondsSeen[bidx]) {\n          bondsSeen.set(bidx);\n          if (mol.getRingInfo()->numBondRings(bidx)) {\n            const Atom *oatom = mol[*beg]->getOtherAtom(ratom);\n            if (!atomsSeen[oatom->getIdx()] && !atomsUsed[oatom->getIdx()]) {\n              nextAtoms.push_back(oatom);\n              atomsUsed.set(oatom->getIdx());\n            }\n          }\n        }\n        ++beg;\n      }\n    }  // end of BFS\n    if (ringStereoAtoms.size() != 0) {\n      atom->setProp(common_properties::_ringStereoAtoms, ringStereoAtoms, true);\n      // because we're only going to hit each ring atom once, the first atom we\n      // encounter in a ring is going to end up with all the other atoms set as\n      // stereoAtoms, but each of them will only have the first atom present. We\n      // need to fix that. because the traverse from the first atom only\n      // followed ring bonds, these things are all by definition in one ring\n      // system. (Q: is this true if there's a spiro center in there?)\n      INT_VECT same(mol.getNumAtoms(), 0);\n      BOOST_FOREACH (int ringAtomEntry, ringStereoAtoms) {\n        int ringAtomIdx =\n            ringAtomEntry < 0 ? -ringAtomEntry - 1 : ringAtomEntry - 1;\n        same[ringAtomIdx] = ringAtomEntry;\n      }\n      for (INT_VECT_CI rae = ringStereoAtoms.begin();\n           rae != ringStereoAtoms.end(); ++rae) {\n        int ringAtomEntry = *rae;\n        int ringAtomIdx =\n            ringAtomEntry < 0 ? -ringAtomEntry - 1 : ringAtomEntry - 1;\n        INT_VECT lringatoms(0);\n        mol.getAtomWithIdx(ringAtomIdx)\n            ->getPropIfPresent(common_properties::_ringStereoAtoms, lringatoms);\n        CHECK_INVARIANT(lringatoms.size() > 0, \"no other ring atoms found.\");\n        for (auto orae = rae + 1; orae != ringStereoAtoms.end(); ++orae) {\n          int oringAtomEntry = *orae;\n          int oringAtomIdx =\n              oringAtomEntry < 0 ? -oringAtomEntry - 1 : oringAtomEntry - 1;\n          int theseDifferent = (ringAtomEntry < 0) ^ (oringAtomEntry < 0);\n          lringatoms.push_back(theseDifferent ? -(oringAtomIdx + 1)\n                                              : (oringAtomIdx + 1));\n          INT_VECT olringatoms(0);\n          mol.getAtomWithIdx(oringAtomIdx)\n              ->getPropIfPresent(common_properties::_ringStereoAtoms,\n                                 olringatoms);\n          CHECK_INVARIANT(olringatoms.size() > 0, \"no other ring atoms found.\");\n          olringatoms.push_back(theseDifferent ? -(ringAtomIdx + 1)\n                                               : (ringAtomIdx + 1));\n          mol.getAtomWithIdx(oringAtomIdx)\n              ->setProp(common_properties::_ringStereoAtoms, olringatoms);\n        }\n        mol.getAtomWithIdx(ringAtomIdx)\n            ->setProp(common_properties::_ringStereoAtoms, lringatoms);\n      }\n\n    } else {\n      possibleSpecialCases.reset(atom->getIdx());\n    }\n    atomsSeen.set(atom->getIdx());\n  }\n}\n\nstd::pair<bool, bool> isAtomPotentialChiralCenter(\n    const Atom *atom, const ROMol &mol, const UINT_VECT &ranks,\n    Chirality::INT_PAIR_VECT &nbrs) {\n  // loop over all neighbors and form a decorated list of their\n  // ranks:\n  bool legalCenter = true;\n  bool hasDupes = false;\n\n  if (atom->getTotalDegree() > 4) {\n    // we only know tetrahedral chirality\n    legalCenter = false;\n  } else {\n    boost::dynamic_bitset<> codesSeen(mol.getNumAtoms());\n    ROMol::OEDGE_ITER beg, end;\n    boost::tie(beg, end) = mol.getAtomBonds(atom);\n    while (beg != end) {\n      unsigned int otherIdx = mol[*beg]->getOtherAtom(atom)->getIdx();\n      CHECK_INVARIANT(ranks[otherIdx] < mol.getNumAtoms(),\n                      \"CIP rank higher than the number of atoms.\");\n      // watch for neighbors with duplicate ranks, which would mean\n      // that we cannot be chiral:\n      if (codesSeen[ranks[otherIdx]]) {\n        // we've already seen this code, it's a dupe\n        hasDupes = true;\n        break;\n      }\n      codesSeen[ranks[otherIdx]] = 1;\n      nbrs.push_back(std::make_pair(ranks[otherIdx], mol[*beg]->getIdx()));\n      ++beg;\n    }\n\n    // figure out if this is a legal chiral center or not:\n    if (!hasDupes) {\n      if (nbrs.size() < 3 &&\n          (atom->getAtomicNum() != 15 && atom->getAtomicNum() != 33)) {\n        // less than three neighbors is never stereogenic\n        // unless it is a phosphine/arsine with implicit H\n        legalCenter = false;\n      } else if (atom->getAtomicNum() == 15 || atom->getAtomicNum() == 33) {\n        // from logical flow: nbrs.size is 3 or 4, or 2 (implicit H)\n        // Since InChI Software v. 1.02-standard (2009), phosphines and arsines\n        // are always treated as stereogenic even with H atom neighbors.\n        // Accept automatically.\n        legalCenter = true;\n      } else if (nbrs.size() == 3) {\n        // three-coordinate with a single H we'll accept automatically:\n        if (atom->getTotalNumHs() != 1) {\n          // otherwise we default to not being a legal center\n          legalCenter = false;\n          // but there are a few special cases we'll accept\n          // sulfur or selenium with either a positive charge or a double\n          // bond:\n          if ((atom->getAtomicNum() == 16 || atom->getAtomicNum() == 34) &&\n              (atom->getExplicitValence() == 4 ||\n               (atom->getExplicitValence() == 3 &&\n                atom->getFormalCharge() == 1))) {\n            legalCenter = true;\n          } else if (atom->getAtomicNum() == 7 &&\n                     mol.getRingInfo()->isAtomInRingOfSize(atom->getIdx(), 3)) {\n            // N in a three-membered ring is another one of the InChI special\n            // cases\n            legalCenter = true;\n          }\n        }\n      }\n    }\n  }\n  return std::make_pair(legalCenter, hasDupes);\n}\n\n// returns a pair:\n//   1) are there unassigned stereoatoms\n//   2) did we assign any?\nstd::pair<bool, bool> assignAtomChiralCodes(ROMol &mol, UINT_VECT &ranks,\n                                            bool flagPossibleStereoCenters) {\n  PRECONDITION((!ranks.size() || ranks.size() == mol.getNumAtoms()),\n               \"bad rank vector size\");\n  bool atomChanged = false;\n  unsigned int unassignedAtoms = 0;\n\n  // ------------------\n  // now loop over each atom and, if it's marked as chiral,\n  //  figure out the appropriate CIP label:\n  for (ROMol::AtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n       ++atIt) {\n    Atom *atom = *atIt;\n    Atom::ChiralType tag = atom->getChiralTag();\n\n    // only worry about this atom if it has a marked chirality\n    // we understand:\n    if (flagPossibleStereoCenters ||\n        (tag != Atom::CHI_UNSPECIFIED && tag != Atom::CHI_OTHER)) {\n      if (atom->hasProp(common_properties::_CIPCode)) {\n        continue;\n      }\n\n      if (!ranks.size()) {\n        //  if we need to, get the \"CIP\" ranking of each atom:\n        assignAtomCIPRanks(mol, ranks);\n      }\n      Chirality::INT_PAIR_VECT nbrs;\n      bool legalCenter, hasDupes;\n      boost::tie(legalCenter, hasDupes) =\n          isAtomPotentialChiralCenter(atom, mol, ranks, nbrs);\n      if (legalCenter) {\n        ++unassignedAtoms;\n      }\n      if (legalCenter && !hasDupes && flagPossibleStereoCenters) {\n        atom->setProp(common_properties::_ChiralityPossible, 1);\n      }\n\n      if (legalCenter && !hasDupes && tag != Atom::CHI_UNSPECIFIED &&\n          tag != Atom::CHI_OTHER) {\n        // stereochem is possible and we have no duplicate neighbors, assign\n        // a CIP code:\n        atomChanged = true;\n        --unassignedAtoms;\n\n        // sort the list of neighbors by their CIP ranks:\n        std::sort(nbrs.begin(), nbrs.end(), Rankers::pairLess<int, int>());\n\n        // collect the list of neighbor indices:\n        std::list<int> nbrIndices;\n        for (Chirality::INT_PAIR_VECT_CI nbrIt = nbrs.begin();\n             nbrIt != nbrs.end(); ++nbrIt) {\n          nbrIndices.push_back((*nbrIt).second);\n        }\n        // ask the atom how many swaps we have to make:\n        int nSwaps = atom->getPerturbationOrder(nbrIndices);\n\n        // if the atom has 3 neighbors and a hydrogen, add a swap:\n        if (nbrIndices.size() == 3 && atom->getTotalNumHs() == 1) {\n          ++nSwaps;\n        }\n\n        // if that number is odd, we'll change our chirality:\n        if (nSwaps % 2) {\n          if (tag == Atom::CHI_TETRAHEDRAL_CCW) {\n            tag = Atom::CHI_TETRAHEDRAL_CW;\n          } else {\n            tag = Atom::CHI_TETRAHEDRAL_CCW;\n          }\n        }\n        // now assign the CIP code:\n        std::string cipCode;\n        if (tag == Atom::CHI_TETRAHEDRAL_CCW) {\n          cipCode = \"S\";\n        } else {\n          cipCode = \"R\";\n        }\n        atom->setProp(common_properties::_CIPCode, cipCode);\n      }\n    }\n  }\n  return std::make_pair((unassignedAtoms > 0), atomChanged);\n}\n\n// returns a pair:\n//   1) are there unassigned stereo bonds?\n//   2) did we assign any?\nstd::pair<bool, bool> assignBondStereoCodes(ROMol &mol, UINT_VECT &ranks) {\n  PRECONDITION((!ranks.size() || ranks.size() == mol.getNumAtoms()),\n               \"bad rank vector size\");\n  bool assignedABond = false;\n  unsigned int unassignedBonds = 0;\n  boost::dynamic_bitset<> bondsToClear(mol.getNumBonds());\n  // find the double bonds:\n  for (ROMol::BondIterator bondIt = mol.beginBonds(); bondIt != mol.endBonds();\n       ++bondIt) {\n    if ((*bondIt)->getBondType() == Bond::DOUBLE) {\n      Bond *dblBond = *bondIt;\n      if (dblBond->getStereo() != Bond::STEREONONE) {\n        continue;\n      }\n      if (!ranks.size()) {\n        assignAtomCIPRanks(mol, ranks);\n      }\n      dblBond->getStereoAtoms().clear();\n\n      // at the moment we are ignoring stereochem on ring bonds with less than\n      // 8\n      // members.\n      if (shouldDetectDoubleBondStereo(dblBond)) {\n        const Atom *begAtom = dblBond->getBeginAtom();\n        const Atom *endAtom = dblBond->getEndAtom();\n        // we're only going to handle 2 or three coordinate atoms:\n        if ((begAtom->getDegree() == 2 || begAtom->getDegree() == 3) &&\n            (endAtom->getDegree() == 2 || endAtom->getDegree() == 3)) {\n          ++unassignedBonds;\n\n          // look around each atom and see if it has at least one bond with\n          // direction marked:\n\n          // the pairs here are: atomrank,bonddir\n          Chirality::INT_PAIR_VECT begAtomNeighbors, endAtomNeighbors;\n          bool hasExplicitUnknownStereo = false;\n          int bgn_stereo = false, end_stereo = false;\n          if ((dblBond->getBeginAtom()->getPropIfPresent(\n                   common_properties::_UnknownStereo, bgn_stereo) &&\n               bgn_stereo) ||\n              (dblBond->getEndAtom()->getPropIfPresent(\n                   common_properties::_UnknownStereo, end_stereo) &&\n               end_stereo)) {\n            hasExplicitUnknownStereo = true;\n          }\n          Chirality::findAtomNeighborDirHelper(mol, begAtom, dblBond, ranks,\n                                               begAtomNeighbors,\n                                               hasExplicitUnknownStereo);\n          Chirality::findAtomNeighborDirHelper(mol, endAtom, dblBond, ranks,\n                                               endAtomNeighbors,\n                                               hasExplicitUnknownStereo);\n\n          if (begAtomNeighbors.size() && endAtomNeighbors.size()) {\n            // Each atom has at least one neighboring bond with marked\n            // directionality.  Find the highest-ranked directionality\n            // on each side:\n\n            int begDir, endDir, endNbrAid, begNbrAid;\n            if (begAtomNeighbors.size() == 1 ||\n                ranks[begAtomNeighbors[0].first] >\n                    ranks[begAtomNeighbors[1].first]) {\n              begDir = begAtomNeighbors[0].second;\n              begNbrAid = begAtomNeighbors[0].first;\n            } else {\n              begDir = begAtomNeighbors[1].second;\n              begNbrAid = begAtomNeighbors[1].first;\n            }\n            if (endAtomNeighbors.size() == 1 ||\n                ranks[endAtomNeighbors[0].first] >\n                    ranks[endAtomNeighbors[1].first]) {\n              endDir = endAtomNeighbors[0].second;\n              endNbrAid = endAtomNeighbors[0].first;\n            } else {\n              endDir = endAtomNeighbors[1].second;\n              endNbrAid = endAtomNeighbors[1].first;\n            }\n\n            bool conflictingBegin =\n                (begAtomNeighbors.size() == 2 &&\n                 begAtomNeighbors[0].second == begAtomNeighbors[1].second);\n            bool conflictingEnd =\n                (endAtomNeighbors.size() == 2 &&\n                 endAtomNeighbors[0].second == endAtomNeighbors[1].second);\n            if (conflictingBegin || conflictingEnd) {\n              dblBond->setStereo(Bond::STEREONONE);\n              BOOST_LOG(rdWarningLog) << \"Conflicting single bond directions \"\n                                         \"around double bond at index \"\n                                      << dblBond->getIdx() << \".\" << std::endl;\n              BOOST_LOG(rdWarningLog) << \"  BondStereo set to STEREONONE and \"\n                                         \"single bond directions set to NONE.\"\n                                      << std::endl;\n              assignedABond = true;\n              if (conflictingBegin) {\n                bondsToClear[mol.getBondBetweenAtoms(begAtomNeighbors[0].first,\n                                                     begAtom->getIdx())\n                                 ->getIdx()] = 1;\n                bondsToClear[mol.getBondBetweenAtoms(begAtomNeighbors[1].first,\n                                                     begAtom->getIdx())\n                                 ->getIdx()] = 1;\n              }\n              if (conflictingEnd) {\n                bondsToClear[mol.getBondBetweenAtoms(endAtomNeighbors[0].first,\n                                                     endAtom->getIdx())\n                                 ->getIdx()] = 1;\n                bondsToClear[mol.getBondBetweenAtoms(endAtomNeighbors[1].first,\n                                                     endAtom->getIdx())\n                                 ->getIdx()] = 1;\n              }\n            } else {\n              dblBond->getStereoAtoms().push_back(begNbrAid);\n              dblBond->getStereoAtoms().push_back(endNbrAid);\n              if (hasExplicitUnknownStereo) {\n                dblBond->setStereo(Bond::STEREOANY);\n                assignedABond = true;\n              } else if (begDir == endDir) {\n                // In findAtomNeighborDirHelper, we've set up the\n                // bond directions here so that they correspond to\n                // having both single bonds START at the double bond.\n                // This means that if the single bonds point in the same\n                // direction, the bond is cis, \"Z\"\n                dblBond->setStereo(Bond::STEREOZ);\n                assignedABond = true;\n              } else {\n                dblBond->setStereo(Bond::STEREOE);\n                assignedABond = true;\n              }\n            }\n            --unassignedBonds;\n          }\n        }\n      }\n    }\n  }\n\n  for (unsigned int i = 0; i < mol.getNumBonds(); ++i) {\n    if (bondsToClear[i]) {\n      mol.getBondWithIdx(i)->setBondDir(Bond::NONE);\n    }\n  }\n\n  return std::make_pair(unassignedBonds > 0, assignedABond);\n}\n\n// reassign atom ranks by supplementing the current ranks\n// with information about known chirality\nvoid rerankAtoms(const ROMol &mol, UINT_VECT &ranks) {\n  PRECONDITION(ranks.size() == mol.getNumAtoms(), \"bad rank vector size\");\n  unsigned int factor = 100;\n  while (factor < mol.getNumAtoms()) {\n    factor *= 10;\n  }\n\n#ifdef VERBOSE_CANON\n  BOOST_LOG(rdDebugLog) << \"rerank PRE: \" << std::endl;\n  for (int i = 0; i < mol.getNumAtoms(); i++) {\n    BOOST_LOG(rdDebugLog) << \"  \" << i << \": \" << ranks[i] << std::endl;\n  }\n#endif\n\n  DOUBLE_VECT invars(mol.getNumAtoms());\n  // and now supplement them:\n  for (unsigned int i = 0; i < mol.getNumAtoms(); ++i) {\n    invars[i] = ranks[i] * factor;\n    const Atom *atom = mol.getAtomWithIdx(i);\n    // Priority order: R > S > nothing\n    std::string cipCode;\n    if (atom->getPropIfPresent(common_properties::_CIPCode, cipCode)) {\n      if (cipCode == \"S\") {\n        invars[i] += 10;\n      } else if (cipCode == \"R\") {\n        invars[i] += 20;\n      }\n    }\n    ROMol::OEDGE_ITER beg, end;\n    boost::tie(beg, end) = mol.getAtomBonds(atom);\n    while (beg != end) {\n      const Bond *oBond = mol[*beg];\n      if (oBond->getBondType() == Bond::DOUBLE) {\n        if (oBond->getStereo() == Bond::STEREOE) {\n          invars[i] += 1;\n        } else if (oBond->getStereo() == Bond::STEREOZ) {\n          invars[i] += 2;\n        }\n      }\n      ++beg;\n    }\n  }\n  iterateCIPRanks(mol, invars, ranks, true);\n  // copy the ranks onto the atoms:\n  for (unsigned int i = 0; i < mol.getNumAtoms(); i++) {\n    mol.getAtomWithIdx(i)->setProp(common_properties::_CIPRank, ranks[i]);\n  }\n\n#ifdef VERBOSE_CANON\n  BOOST_LOG(rdDebugLog) << \"   post: \" << std::endl;\n  for (int i = 0; i < mol.getNumAtoms(); i++) {\n    BOOST_LOG(rdDebugLog) << \"  \" << i << \": \" << ranks[i] << std::endl;\n  }\n#endif\n}\n\nbool hasStereoBondDir(const Bond *bond) {\n  PRECONDITION(bond, \"no bond\");\n  return bond->getBondDir() == Bond::BondDir::ENDDOWNRIGHT ||\n         bond->getBondDir() == Bond::BondDir::ENDUPRIGHT;\n}\n\nconst Bond *getNeighboringDirectedBond(const ROMol &mol, const Atom *atom) {\n  PRECONDITION(atom, \"no atom\");\n  for (const auto &bondIdx :\n       boost::make_iterator_range(mol.getAtomBonds(atom))) {\n    const Bond *bond = mol[bondIdx];\n\n    if (bond->getBondType() != Bond::BondType::DOUBLE &&\n        hasStereoBondDir(bond)) {\n      return bond;\n    }\n  }\n  return nullptr;\n}\n\nBond::BondStereo translateEZLabelToCisTrans(Bond::BondStereo label) {\n  switch (label) {\n    case Bond::STEREOE:\n      return Bond::STEREOTRANS;\n    case Bond::STEREOZ:\n      return Bond::STEREOCIS;\n    default:\n      return label;\n  }\n}\n\nINT_VECT findStereoAtoms(const Bond *bond) {\n  PRECONDITION(bond, \"bad bond\");\n  PRECONDITION(bond->hasOwningMol(), \"no mol\");\n  PRECONDITION(bond->getBondType() == Bond::DOUBLE, \"not double bond\");\n  PRECONDITION(bond->getStereo() > Bond::BondStereo::STEREOANY,\n               \"no defined stereo\");\n\n  if (!bond->getStereoAtoms().empty()) {\n    return bond->getStereoAtoms();\n  }\n  if (bond->getStereo() == Bond::BondStereo::STEREOE ||\n      bond->getStereo() == Bond::BondStereo::STEREOZ) {\n    const Atom *startStereoAtom =\n        findHighestCIPNeighbor(bond->getBeginAtom(), bond->getEndAtom());\n    const Atom *endStereoAtom =\n        findHighestCIPNeighbor(bond->getEndAtom(), bond->getBeginAtom());\n\n    if (startStereoAtom == nullptr || endStereoAtom == nullptr) {\n      return {};\n    }\n\n    int startStereoAtomIdx = static_cast<int>(startStereoAtom->getIdx());\n    int endStereoAtomIdx = static_cast<int>(endStereoAtom->getIdx());\n\n    return {startStereoAtomIdx, endStereoAtomIdx};\n  } else {\n    BOOST_LOG(rdWarningLog) << \"Unable to assign stereo atoms for bond \"\n                            << bond->getIdx() << std::endl;\n    return {};\n  }\n}\n\n}  // namespace Chirality\n\nnamespace MolOps {\n\n/*\n    We're going to do this iteratively:\n      1) assign atom stereochemistry\n      2) assign bond stereochemistry\n      3) if there are still unresolved atoms or bonds\n         repeat the above steps as necessary\n */\nvoid assignStereochemistry(ROMol &mol, bool cleanIt, bool force,\n                           bool flagPossibleStereoCenters) {\n  if (!force && mol.hasProp(common_properties::_StereochemDone)) {\n    return;\n  }\n\n  // later we're going to need ring information, get it now if we don't\n  // have it already:\n  if (!mol.getRingInfo()->isInitialized()) {\n    MolOps::fastFindRings(mol);\n  }\n\n#if 0\n  std::cerr << \">>>>>>>>>>>>>\\n\";\n  std::cerr << \"assign stereochem\\n\";\n  mol.debugMol(std::cerr);\n#endif\n\n  // as part of the preparation, we'll loop over the atoms and\n  // bonds to see if anything has stereochemistry\n  // indicated. There's no point in doing the work here if there\n  // are neither stereocenters nor bonds that we need to consider.\n  // The exception to this is when flagPossibleStereoCenters is\n  // true; then we always need to do the work\n  bool hasStereoAtoms = flagPossibleStereoCenters;\n  for (ROMol::AtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n       ++atIt) {\n    if (cleanIt) {\n      if ((*atIt)->hasProp(common_properties::_CIPCode)) {\n        (*atIt)->clearProp(common_properties::_CIPCode);\n      }\n      if ((*atIt)->hasProp(common_properties::_ChiralityPossible)) {\n        (*atIt)->clearProp(common_properties::_ChiralityPossible);\n      }\n    }\n    if (!hasStereoAtoms && (*atIt)->getChiralTag() != Atom::CHI_UNSPECIFIED &&\n        (*atIt)->getChiralTag() != Atom::CHI_OTHER) {\n      hasStereoAtoms = true;\n    }\n  }\n  bool hasStereoBonds = false;\n  for (ROMol::BondIterator bondIt = mol.beginBonds(); bondIt != mol.endBonds();\n       ++bondIt) {\n    if (cleanIt) {\n      if ((*bondIt)->getBondType() == Bond::DOUBLE) {\n        if ((*bondIt)->getBondDir() == Bond::EITHERDOUBLE) {\n          (*bondIt)->setStereo(Bond::STEREOANY);\n        } else if ((*bondIt)->getStereo() != Bond::STEREOANY) {\n          (*bondIt)->setStereo(Bond::STEREONONE);\n          (*bondIt)->getStereoAtoms().clear();\n        }\n      }\n    }\n    if (!hasStereoBonds && (*bondIt)->getBondType() == Bond::DOUBLE) {\n      ROMol::OEDGE_ITER beg, end;\n      boost::tie(beg, end) = mol.getAtomBonds((*bondIt)->getBeginAtom());\n      while (!hasStereoBonds && beg != end) {\n        const Bond *nbond = mol[*beg];\n        ++beg;\n        if (nbond->getBondDir() == Bond::ENDDOWNRIGHT ||\n            nbond->getBondDir() == Bond::ENDUPRIGHT) {\n          hasStereoBonds = true;\n        }\n      }\n      boost::tie(beg, end) = mol.getAtomBonds((*bondIt)->getEndAtom());\n      while (!hasStereoBonds && beg != end) {\n        const Bond *nbond = mol[*beg];\n        ++beg;\n        if (nbond->getBondDir() == Bond::ENDDOWNRIGHT ||\n            nbond->getBondDir() == Bond::ENDUPRIGHT) {\n          hasStereoBonds = true;\n        }\n      }\n    }\n    if (!cleanIt && hasStereoBonds) {\n      break;  // no reason to keep iterating if we've already\n              // determined there are stereo bonds to consider\n    }\n  }\n  UINT_VECT atomRanks;\n  bool keepGoing = hasStereoAtoms | hasStereoBonds;\n  bool changedStereoAtoms, changedStereoBonds;\n  while (keepGoing) {\n    if (hasStereoAtoms) {\n      boost::tie(hasStereoAtoms, changedStereoAtoms) =\n          Chirality::assignAtomChiralCodes(mol, atomRanks,\n                                           flagPossibleStereoCenters);\n    } else {\n      changedStereoAtoms = false;\n    }\n    if (hasStereoBonds) {\n      boost::tie(hasStereoBonds, changedStereoBonds) =\n          Chirality::assignBondStereoCodes(mol, atomRanks);\n    } else {\n      changedStereoBonds = false;\n    }\n    keepGoing = (hasStereoAtoms || hasStereoBonds) &&\n                (changedStereoAtoms || changedStereoBonds);\n\n    if (keepGoing) {\n      // update the atom ranks based on the new information we have:\n      Chirality::rerankAtoms(mol, atomRanks);\n    }\n#if 0\n    std::cout << \"*************** done iteration \" << keepGoing\n              << \" ***********\" << std::endl;\n    mol.debugMol(std::cout);\n    std::cout << \"*************** done iteration \" << keepGoing\n              << \" ***********\" << std::endl;\n#endif\n  }\n\n  if (cleanIt) {\n    // if the ranks are needed again, this will force them to be\n    // re-calculated based on the stereo calculated above.\n    // atomRanks.clear();\n\n    for (ROMol::AtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n         ++atIt) {\n      if ((*atIt)->hasProp(common_properties::_ringStereochemCand)) {\n        (*atIt)->clearProp(common_properties::_ringStereochemCand);\n      }\n      if ((*atIt)->hasProp(common_properties::_ringStereoAtoms)) {\n        (*atIt)->clearProp(common_properties::_ringStereoAtoms);\n      }\n    }\n    boost::dynamic_bitset<> possibleSpecialCases(mol.getNumAtoms());\n    Chirality::findChiralAtomSpecialCases(mol, possibleSpecialCases);\n\n    for (auto atom : mol.atoms()) {\n      if (atom->getChiralTag() != Atom::CHI_UNSPECIFIED &&\n          !atom->hasProp(common_properties::_CIPCode) &&\n          (!possibleSpecialCases[atom->getIdx()] ||\n           !atom->hasProp(common_properties::_ringStereoAtoms))) {\n        atom->setChiralTag(Atom::CHI_UNSPECIFIED);\n\n        // If the atom has an explicit hydrogen and no charge, that H\n        // was probably put there solely because of the chirality.\n        // So we'll go ahead and remove it.\n        // This was Issue 194\n        if (atom->getNumExplicitHs() == 1 && atom->getFormalCharge() == 0 &&\n            !atom->getIsAromatic()) {\n          atom->setNumExplicitHs(0);\n          atom->setNoImplicit(false);\n          atom->calcExplicitValence(false);\n          atom->calcImplicitValence(false);\n        }\n      }\n    }\n    for (auto bond : mol.bonds()) {\n      // wedged bonds to atoms that have no stereochem\n      // should be removed. (github issue 87)\n      if ((bond->getBondDir() == Bond::BEGINWEDGE ||\n           bond->getBondDir() == Bond::BEGINDASH) &&\n          bond->getBeginAtom()->getChiralTag() == Atom::CHI_UNSPECIFIED &&\n          bond->getEndAtom()->getChiralTag() == Atom::CHI_UNSPECIFIED) {\n        bond->setBondDir(Bond::NONE);\n      }\n\n      // check for directionality on single bonds around\n      // double bonds without stereo. This was github #2422\n      if (bond->getBondType() == Bond::DOUBLE &&\n          (bond->getStereo() == Bond::STEREOANY ||\n           bond->getStereo() == Bond::STEREONONE)) {\n        std::vector<Atom *> batoms = {bond->getBeginAtom(), bond->getEndAtom()};\n        for (auto batom : batoms) {\n          for (const auto &nbri :\n               boost::make_iterator_range(mol.getAtomBonds(batom))) {\n            auto nbrBndI = mol[nbri];\n            if ((nbrBndI->getBondDir() == Bond::ENDDOWNRIGHT ||\n                 nbrBndI->getBondDir() == Bond::ENDUPRIGHT) &&\n                (nbrBndI->getBondType() == Bond::SINGLE ||\n                 nbrBndI->getBondType() == Bond::AROMATIC)) {\n              // direction is set, and we know it's not because of our\n              // bond. What about other neighbors?\n              bool okToClear = true;\n              for (const auto &nbrj : boost::make_iterator_range(\n                       mol.getAtomBonds(nbrBndI->getOtherAtom(batom)))) {\n                auto nbrBndJ = mol[nbrj];\n                if (nbrBndJ->getBondType() == Bond::DOUBLE &&\n                    nbrBndJ->getStereo() != Bond::STEREOANY &&\n                    nbrBndJ->getStereo() != Bond::STEREONONE) {\n                  okToClear = false;\n                  break;\n                }\n              }\n              if (okToClear) {\n                nbrBndI->setBondDir(Bond::NONE);\n              }\n            }\n          }\n        }\n      }\n#if 0\n      // make sure CIS/TRANS assignments are actually stereo bonds\n      if ((*bondIt)->getBondType() == Bond::DOUBLE) {\n        if ((*bondIt)->getStereo() == Bond::STEREOCIS ||\n            (*bondIt)->getStereo() == Bond::STEREOTRANS) {\n          if (!atomRanks.size()) {\n            Chirality::assignAtomCIPRanks(mol, atomRanks);\n          }\n\n          const Atom *begAtom = (*bondIt)->getBeginAtom(),\n                     *endAtom = (*bondIt)->getEndAtom();\n          UINT_VECT begAtomNeighbors, endAtomNeighbors;\n          Chirality::findAtomNeighborsHelper(mol, begAtom, *bondIt,\n                                             begAtomNeighbors);\n          Chirality::findAtomNeighborsHelper(mol, endAtom, *bondIt,\n                                             endAtomNeighbors);\n\n          // Note, this relies on this being a hydrogen-suppressed\n          // graph as the 'Note' in the doc string of this function\n          // indicates is a pre-condition.\n          if ((begAtomNeighbors.size() == 2 &&\n               atomRanks[begAtomNeighbors[0]] ==\n                   atomRanks[begAtomNeighbors[1]]) ||\n              (endAtomNeighbors.size() == 2 &&\n               atomRanks[endAtomNeighbors[0]] ==\n                   atomRanks[endAtomNeighbors[1]])) {\n            (*bondIt)->setStereo(Bond::STEREONONE);\n            (*bondIt)->getStereoAtoms().clear();\n          }\n        }\n      }\n#endif\n    }\n  }\n  mol.setProp(common_properties::_StereochemDone, 1, true);\n\n#if 0\n  std::cerr << \"---\\n\";\n  mol.debugMol(std::cerr);\n  std::cerr << \"<<<<<<<<<<<<<<<<\\n\";\n#endif\n}\n\n// Find bonds than can be cis/trans in a molecule and mark them as\n// Bond::STEREOANY.\nvoid findPotentialStereoBonds(ROMol &mol, bool cleanIt) {\n  // FIX: The earlier thought was to provide an optional argument to ignore or\n  // consider\n  //  double bonds in a ring. But I am removing this optional argument and\n  //  ignoring ring bonds\n  //  completely for now. This is because finding a potential stereo bond in a\n  //  ring involves\n  //  more than just checking the CIPranks for the neighbors - SP 05/04/04\n\n  // make this function callable multiple times\n  if ((mol.hasProp(common_properties::_BondsPotentialStereo)) && (!cleanIt)) {\n    return;\n  } else {\n    UINT_VECT ranks;\n    ranks.resize(mol.getNumAtoms());\n    bool cipDone = false;\n\n    ROMol::BondIterator bondIt;\n    for (bondIt = mol.beginBonds(); bondIt != mol.endBonds(); ++bondIt) {\n      if ((*bondIt)->getBondType() == Bond::DOUBLE &&\n          !(mol.getRingInfo()->numBondRings((*bondIt)->getIdx()))) {\n        // we are ignoring ring bonds here - read the FIX above\n        Bond *dblBond = *bondIt;\n        // We ignore bonds flagged as EITHERDOUBLE or STEREOANY which have\n        // stereo atoms set.\n        if (dblBond->getBondDir() == Bond::EITHERDOUBLE ||\n            (dblBond->getStereo() == Bond::STEREOANY &&\n             dblBond->getStereoAtoms().size() == 2)) {\n          continue;\n        }\n        // proceed only if we either want to clean the stereocode on this bond,\n        // if none is set on it yet, or it is STEREOANY and we need to find\n        // stereoatoms\n        if (cleanIt || dblBond->getStereo() == Bond::STEREONONE ||\n            dblBond->getStereo() == Bond::STEREOANY) {\n          dblBond->setStereo(Bond::STEREONONE);\n          const Atom *begAtom = dblBond->getBeginAtom(),\n                     *endAtom = dblBond->getEndAtom();\n          // we're only going to handle 2 or three coordinate atoms:\n          if ((begAtom->getDegree() == 2 || begAtom->getDegree() == 3) &&\n              (endAtom->getDegree() == 2 || endAtom->getDegree() == 3)) {\n            // ------------------\n            // get the CIP ranking of each atom if we need it:\n            if (!cipDone) {\n              if (!begAtom->hasProp(common_properties::_CIPRank)) {\n                Chirality::assignAtomCIPRanks(mol, ranks);\n              } else {\n                // no need to recompute if we don't need to recompute. :-)\n                for (unsigned int ai = 0; ai < mol.getNumAtoms(); ++ai) {\n                  ranks[ai] = mol.getAtomWithIdx(ai)->getProp<unsigned int>(\n                      common_properties::_CIPRank);\n                }\n              }\n              cipDone = true;\n            }\n            // find the neighbors for the begin atom and the endAtom\n            UINT_VECT begAtomNeighbors, endAtomNeighbors;\n            bool checkDir = false;\n            bool includeAromatic = true;\n            Chirality::findAtomNeighborsHelper(mol, begAtom, dblBond,\n                                               begAtomNeighbors, checkDir,\n                                               includeAromatic);\n            Chirality::findAtomNeighborsHelper(mol, endAtom, dblBond,\n                                               endAtomNeighbors, checkDir,\n                                               includeAromatic);\n            if (begAtomNeighbors.size() > 0 && endAtomNeighbors.size() > 0) {\n              if ((begAtomNeighbors.size() == 2) &&\n                  (endAtomNeighbors.size() == 2)) {\n// if both of the atoms have 2 neighbors (other than the one\n// connected\n// by the double bond) and ....\n#if 0\n                std::cerr << \"Bond: \" << dblBond->getIdx() << \" \"\n                          << begAtom->getIdx() << \"=\" << endAtom->getIdx()\n                          << std::endl;\n                std::cerr << \"   \" << begAtomNeighbors[0] << \"=\"\n                          << ranks[begAtomNeighbors[0]] << \":\";\n                std::cerr << \"   \" << begAtomNeighbors[1] << \"=\"\n                          << ranks[begAtomNeighbors[1]] << std::endl;\n                std::cerr << \"   \" << endAtomNeighbors[0] << \"=\"\n                          << ranks[endAtomNeighbors[0]] << \":\";\n                std::cerr << \"   \" << endAtomNeighbors[1] << \"=\"\n                          << ranks[endAtomNeighbors[1]] << std::endl;\n#endif\n                if ((ranks[begAtomNeighbors[0]] !=\n                     ranks[begAtomNeighbors[1]]) &&\n                    (ranks[endAtomNeighbors[0]] !=\n                     ranks[endAtomNeighbors[1]])) {\n                  // the neighbors ranks are different at both the ends,\n                  // this bond can be part of a cis/trans system\n                  if (ranks[begAtomNeighbors[0]] > ranks[begAtomNeighbors[1]]) {\n                    dblBond->getStereoAtoms().push_back(begAtomNeighbors[0]);\n                  } else {\n                    dblBond->getStereoAtoms().push_back(begAtomNeighbors[1]);\n                  }\n                  if (ranks[endAtomNeighbors[0]] > ranks[endAtomNeighbors[1]]) {\n                    dblBond->getStereoAtoms().push_back(endAtomNeighbors[0]);\n                  } else {\n                    dblBond->getStereoAtoms().push_back(endAtomNeighbors[1]);\n                  }\n                }\n              } else if (begAtomNeighbors.size() == 2) {\n                // if the begAtom has two neighbors and ....\n                if (ranks[begAtomNeighbors[0]] != ranks[begAtomNeighbors[1]]) {\n                  // their ranks are different\n                  if (ranks[begAtomNeighbors[0]] > ranks[begAtomNeighbors[1]]) {\n                    dblBond->getStereoAtoms().push_back(begAtomNeighbors[0]);\n                  } else {\n                    dblBond->getStereoAtoms().push_back(begAtomNeighbors[1]);\n                  }\n                  dblBond->getStereoAtoms().push_back(endAtomNeighbors[0]);\n                }\n              } else if (endAtomNeighbors.size() == 2) {\n                // if the endAtom has two neighbors and ...\n                if (ranks[endAtomNeighbors[0]] != ranks[endAtomNeighbors[1]]) {\n                  // their ranks are different\n                  dblBond->getStereoAtoms().push_back(begAtomNeighbors[0]);\n                  if (ranks[endAtomNeighbors[0]] > ranks[endAtomNeighbors[1]]) {\n                    dblBond->getStereoAtoms().push_back(endAtomNeighbors[0]);\n                  } else {\n                    dblBond->getStereoAtoms().push_back(endAtomNeighbors[1]);\n                  }\n                }\n              } else {\n                // end and beg atoms has only one neighbor each, it doesn't\n                // matter what the ranks are:\n                dblBond->getStereoAtoms().push_back(begAtomNeighbors[0]);\n                dblBond->getStereoAtoms().push_back(endAtomNeighbors[0]);\n              }  // end of different number of neighbors on beg and end atoms\n\n              // mark this double bond as a potential stereo bond\n              if (!dblBond->getStereoAtoms().empty()) {\n                dblBond->setStereo(Bond::STEREOANY);\n              }\n            }  // end of check that beg and end atoms have at least 1\n               // neighbor:\n          }    // end of 2 and 3 coordinated atoms only\n        }      // end of we want it or CIP code is not set\n      }        // end of double bond\n    }          // end of for loop over all bonds\n    mol.setProp(common_properties::_BondsPotentialStereo, 1, true);\n  }\n}\n\n// removes chirality markers from sp and sp2 hybridized centers:\nvoid cleanupChirality(RWMol &mol) {\n  for (ROMol::AtomIterator atomIt = mol.beginAtoms(); atomIt != mol.endAtoms();\n       ++atomIt) {\n    if ((*atomIt)->getChiralTag() != Atom::CHI_UNSPECIFIED &&\n        (*atomIt)->getHybridization() < Atom::SP3) {\n      (*atomIt)->setChiralTag(Atom::CHI_UNSPECIFIED);\n    }\n  }\n}\n\nvoid assignChiralTypesFrom3D(ROMol &mol, int confId, bool replaceExistingTags) {\n  const double ZERO_VOLUME_TOL = 0.1;\n  if (!mol.getNumConformers()) {\n    return;\n  }\n  const Conformer &conf = mol.getConformer(confId);\n  if (!conf.is3D()) {\n    return;\n  }\n\n  // if the molecule already has stereochemistry\n  // perceived, remove the flags that indicate\n  // this... what we're about to do will require\n  // that we go again.\n  if (mol.hasProp(common_properties::_StereochemDone)) {\n    mol.clearProp(common_properties::_StereochemDone);\n  }\n\n  for (ROMol::AtomIterator atomIt = mol.beginAtoms(); atomIt != mol.endAtoms();\n       ++atomIt) {\n    Atom *atom = *atomIt;\n    // if we aren't replacing existing tags and the atom is already tagged,\n    // punt:\n    if (!replaceExistingTags && atom->getChiralTag() != Atom::CHI_UNSPECIFIED) {\n      continue;\n    }\n    atom->setChiralTag(Atom::CHI_UNSPECIFIED);\n    // additional reasons to skip the atom:\n    if (atom->getDegree() < 3 || atom->getTotalDegree() > 4) {\n      // not enough explicit neighbors or too many total neighbors\n      continue;\n    } else {\n      int anum = atom->getAtomicNum();\n      if (anum != 16 && anum != 34 &&  // S or Se are special\n                                       // (just using the InChI list for now)\n          (atom->getTotalDegree() != 4 ||  // not enough total neighbors\n           atom->getTotalNumHs(true) > 1)) {\n        continue;\n      }\n    }\n    const RDGeom::Point3D &p0 = conf.getAtomPos(atom->getIdx());\n    ROMol::ADJ_ITER nbrIdx, endNbrs;\n    boost::tie(nbrIdx, endNbrs) = mol.getAtomNeighbors(atom);\n    const RDGeom::Point3D &p1 = conf.getAtomPos(*nbrIdx);\n    ++nbrIdx;\n    const RDGeom::Point3D &p2 = conf.getAtomPos(*nbrIdx);\n    ++nbrIdx;\n    const RDGeom::Point3D &p3 = conf.getAtomPos(*nbrIdx);\n\n    RDGeom::Point3D v1 = p1 - p0;\n    RDGeom::Point3D v2 = p2 - p0;\n    RDGeom::Point3D v3 = p3 - p0;\n\n    double chiralVol = v1.dotProduct(v2.crossProduct(v3));\n    if (chiralVol < -ZERO_VOLUME_TOL) {\n      atom->setChiralTag(Atom::CHI_TETRAHEDRAL_CW);\n    } else if (chiralVol > ZERO_VOLUME_TOL) {\n      atom->setChiralTag(Atom::CHI_TETRAHEDRAL_CCW);\n    } else {\n      atom->setChiralTag(Atom::CHI_UNSPECIFIED);\n    }\n  }\n}\n\nvoid assignChiralTypesFromMolParity(ROMol &mol, bool replaceExistingTags) {\n  static const std::vector<Atom::ChiralType> chiralTypeVect{\n      Atom::CHI_TETRAHEDRAL_CW, Atom::CHI_TETRAHEDRAL_CCW};\n  // if the molecule already has stereochemistry\n  // perceived, remove the flags that indicate\n  // this... what we're about to do will require\n  // that we go again.\n  if (mol.hasProp(common_properties::_StereochemDone)) {\n    mol.clearProp(common_properties::_StereochemDone);\n  }\n  // Atom-based parity\n  // Number the atoms surrounding the stereo center with 1, 2, 3, and 4\n  // in order of increasing atom number (position in the atom block)\n  // (an implicit hydrogen should be considered the highest numbered atom).\n  // View the center from a position such that the bond connecting the\n  // highest-numbered atom (4) projects behind the plane formed by\n  // atoms 1, 2, and 3.\n  //\n  // Parity 1 (CW)        Parity 2 (CCW)\n  //     3   1                3   2\n  //      \\ /                  \\ /\n  //       |                    |\n  //       2                    1\n  //\n  for (auto atom : mol.atoms()) {\n    // if we aren't replacing existing tags and the atom is already tagged,\n    // punt:\n    if (!replaceExistingTags && atom->getChiralTag() != Atom::CHI_UNSPECIFIED) {\n      continue;\n    }\n    int parity = 0;\n    atom->getPropIfPresent(common_properties::molParity, parity);\n    if (parity <= 0 || parity > 2 || atom->getDegree() < 3) {\n      atom->setChiralTag(Atom::CHI_UNSPECIFIED);\n      continue;\n    }\n    // if we are here, parity was 1 (CW) or 2 (CCW)\n    // now we set parity 0 to be CW and 1 to be CCW\n    --parity;\n    RDKit::ROMol::OBOND_ITER_PAIR nbrBonds = mol.getAtomBonds(atom);\n    INT_LIST nbrBondIdxList;\n    std::transform(\n        nbrBonds.first, nbrBonds.second, std::back_inserter(nbrBondIdxList),\n        [mol](const ROMol::edge_descriptor &e) { return mol[e]->getIdx(); });\n    unsigned int atomIdx = atom->getIdx();\n    nbrBondIdxList.sort([mol, atomIdx](const int ai, const int bi) {\n      return (mol.getBondWithIdx(ai)->getOtherAtomIdx(atomIdx) <\n              mol.getBondWithIdx(bi)->getOtherAtomIdx(atomIdx));\n    });\n    int nSwaps = atom->getPerturbationOrder(nbrBondIdxList);\n    if (nSwaps % 2) {\n      parity = 1 - parity;\n    }\n    atom->setChiralTag(chiralTypeVect[parity]);\n    if (atom->getImplicitValence() == -1) {\n      atom->calcExplicitValence(false);\n      atom->calcImplicitValence(false);\n    }\n    // within the RD representation, if a three-coordinate atom\n    // is chiral and has an implicit H, that H needs to be made explicit:\n    if (atom->getDegree() == 3 && !atom->getNumExplicitHs() &&\n        atom->getNumImplicitHs() == 1) {\n      atom->setNumExplicitHs(1);\n      // recalculated number of implicit Hs:\n      atom->updatePropertyCache();\n    }\n  }\n}\n\nvoid setDoubleBondNeighborDirections(ROMol &mol, const Conformer *conf) {\n  // used to store the number of single bonds a given\n  // single bond is adjacent to\n  std::vector<unsigned int> singleBondCounts(mol.getNumBonds(), 0);\n  std::vector<Bond *> bondsInPlay;\n  // keeps track of which single bonds are adjacent to each double bond:\n  VECT_INT_VECT dblBondNbrs(mol.getNumBonds());\n  // keeps track of which double bonds are adjacent to each single bond:\n  VECT_INT_VECT singleBondNbrs(mol.getNumBonds());\n  // keeps track of which single bonds need a dir set and which double bonds\n  // need to have their neighbors' dirs set\n  boost::dynamic_bitset<> needsDir(mol.getNumBonds());\n\n  // find double bonds that should be considered for\n  // stereochemistry\n  // NOTE that we are explicitly excluding double bonds in rings\n  // with this test.\n  bool resetRings = false;\n  if (!mol.getRingInfo()->isInitialized()) {\n    resetRings = true;\n    MolOps::fastFindRings(mol);\n  }\n\n  for (RWMol::BondIterator bondIt = mol.beginBonds(); bondIt != mol.endBonds();\n       ++bondIt) {\n    if (isBondCandidateForStereo(*bondIt)) {\n      const Atom *a1 = (*bondIt)->getBeginAtom();\n      const Atom *a2 = (*bondIt)->getEndAtom();\n\n      ROMol::OEDGE_ITER beg, end;\n      boost::tie(beg, end) = mol.getAtomBonds(a1);\n      while (beg != end) {\n        const Bond *nbrBond = mol[*beg];\n        if (nbrBond->getBondType() == Bond::SINGLE ||\n            nbrBond->getBondType() == Bond::AROMATIC) {\n          singleBondCounts[nbrBond->getIdx()] += 1;\n          auto nbrDir = nbrBond->getBondDir();\n          if (nbrDir == Bond::BondDir::NONE ||\n              nbrDir == Bond::BondDir::ENDDOWNRIGHT ||\n              nbrDir == Bond::BondDir::ENDUPRIGHT) {\n            needsDir[nbrBond->getIdx()] = 1;\n          }\n          needsDir[(*bondIt)->getIdx()] = 1;\n          dblBondNbrs[(*bondIt)->getIdx()].push_back(nbrBond->getIdx());\n          // the search may seem inefficient, but these vectors are going to\n          // be at most 2 long (with very few exceptions). It's just not worth\n          // using a different data structure\n          if (std::find(singleBondNbrs[nbrBond->getIdx()].begin(),\n                        singleBondNbrs[nbrBond->getIdx()].end(),\n                        (*bondIt)->getIdx()) ==\n              singleBondNbrs[nbrBond->getIdx()].end()) {\n            singleBondNbrs[nbrBond->getIdx()].push_back((*bondIt)->getIdx());\n          }\n        }\n        ++beg;\n      }\n      boost::tie(beg, end) = mol.getAtomBonds(a2);\n      while (beg != end) {\n        const Bond *nbrBond = mol[*beg];\n        if (nbrBond->getBondType() == Bond::SINGLE ||\n            nbrBond->getBondType() == Bond::AROMATIC) {\n          singleBondCounts[nbrBond->getIdx()] += 1;\n          auto nbrDir = nbrBond->getBondDir();\n          if (nbrDir == Bond::BondDir::NONE ||\n              nbrDir == Bond::BondDir::ENDDOWNRIGHT ||\n              nbrDir == Bond::BondDir::ENDUPRIGHT) {\n            needsDir[nbrBond->getIdx()] = 1;\n          }\n          needsDir[(*bondIt)->getIdx()] = 1;\n          dblBondNbrs[(*bondIt)->getIdx()].push_back(nbrBond->getIdx());\n\n          // the search may seem inefficient, but these vectors are going to\n          // be at most 2 long (with very few exceptions). It's just not worth\n          // using a different data structure\n          if (std::find(singleBondNbrs[nbrBond->getIdx()].begin(),\n                        singleBondNbrs[nbrBond->getIdx()].end(),\n                        (*bondIt)->getIdx()) ==\n              singleBondNbrs[nbrBond->getIdx()].end()) {\n            singleBondNbrs[nbrBond->getIdx()].push_back((*bondIt)->getIdx());\n          }\n        }\n        ++beg;\n      }\n      bondsInPlay.push_back(*bondIt);\n    }\n  }\n\n  if (!bondsInPlay.size()) {\n    if (resetRings) {\n      mol.getRingInfo()->reset();\n    }\n    return;\n  }\n\n  // order the double bonds based on the singleBondCounts of their neighbors:\n  std::vector<std::pair<unsigned int, Bond *>> orderedBondsInPlay;\n  for (auto dblBond : bondsInPlay) {\n    unsigned int countHere =\n        std::accumulate(dblBondNbrs[dblBond->getIdx()].begin(),\n                        dblBondNbrs[dblBond->getIdx()].end(), 0);\n    // and favor double bonds that are *not* in rings. The combination of\n    // using the sum above (instead of the max) and this ring-membershipt test\n    // seem to fix sf.net issue 3009836\n    if (!(mol.getRingInfo()->numBondRings(dblBond->getIdx()))) {\n      countHere *= 10;\n    }\n    orderedBondsInPlay.push_back(std::make_pair(countHere, dblBond));\n  }\n  std::sort(orderedBondsInPlay.begin(), orderedBondsInPlay.end());\n\n  // oof, now loop over the double bonds in that order and\n  // update their neighbor directionalities:\n  std::vector<std::pair<unsigned int, Bond *>>::reverse_iterator pairIter;\n  for (pairIter = orderedBondsInPlay.rbegin();\n       pairIter != orderedBondsInPlay.rend(); ++pairIter) {\n    // std::cerr << \"RESET?: \" << pairIter->second->getIdx() << \" \"\n    //           << pairIter->second->getStereo() << std::endl;\n    updateDoubleBondNeighbors(mol, pairIter->second, conf, needsDir,\n                              singleBondCounts, singleBondNbrs);\n  }\n  if (resetRings) {\n    mol.getRingInfo()->reset();\n  }\n}\n\nvoid detectBondStereochemistry(ROMol &mol, int confId) {\n  if (!mol.getNumConformers()) {\n    return;\n  }\n  const Conformer &conf = mol.getConformer(confId);\n  setDoubleBondNeighborDirections(mol, &conf);\n}\n\nvoid setBondStereoFromDirections(ROMol &mol) {\n  for (Bond *bond : mol.bonds()) {\n    if (bond->getBondType() == Bond::DOUBLE) {\n      const Atom *stereoBondBeginAtom = bond->getBeginAtom();\n      const Atom *stereoBondEndAtom = bond->getEndAtom();\n\n      const Bond *directedBondAtBegin =\n          Chirality::getNeighboringDirectedBond(mol, stereoBondBeginAtom);\n      const Bond *directedBondAtEnd =\n          Chirality::getNeighboringDirectedBond(mol, stereoBondEndAtom);\n\n      if (directedBondAtBegin != nullptr && directedBondAtEnd != nullptr) {\n        unsigned beginSideStereoAtom =\n            directedBondAtBegin->getOtherAtomIdx(stereoBondBeginAtom->getIdx());\n        unsigned endSideStereoAtom =\n            directedBondAtEnd->getOtherAtomIdx(stereoBondEndAtom->getIdx());\n\n        bond->setStereoAtoms(beginSideStereoAtom, endSideStereoAtom);\n\n        auto beginSideBondDirection = directedBondAtBegin->getBondDir();\n        if (directedBondAtBegin->getBeginAtom() == stereoBondBeginAtom) {\n          beginSideBondDirection = getOppositeBondDir(beginSideBondDirection);\n        }\n\n        auto endSideBondDirection = directedBondAtEnd->getBondDir();\n        if (directedBondAtEnd->getEndAtom() == stereoBondEndAtom) {\n          endSideBondDirection = getOppositeBondDir(endSideBondDirection);\n        }\n\n        if (beginSideBondDirection == endSideBondDirection) {\n          bond->setStereo(Bond::STEREOTRANS);\n        } else {\n          bond->setStereo(Bond::STEREOCIS);\n        }\n      }\n    }\n  }\n}\n\nvoid assignStereochemistryFrom3D(ROMol &mol, int confId,\n                                 bool replaceExistingTags) {\n  if (!mol.getNumConformers() || !mol.getConformer(confId).is3D()) {\n    return;\n  }\n\n  detectBondStereochemistry(mol, confId);\n  assignChiralTypesFrom3D(mol, confId, replaceExistingTags);\n  bool force = true;\n  bool flagPossibleStereoCenters = true;\n  assignStereochemistry(mol, replaceExistingTags, force,\n                        flagPossibleStereoCenters);\n}\n\nvoid assignChiralTypesFromBondDirs(ROMol &mol, const int confId,\n                                   const bool replaceExistingTags) {\n  if (!mol.getNumConformers()) {\n    return;\n  }\n  auto conf = mol.getConformer(confId);\n  boost::dynamic_bitset<> atomsSet(mol.getNumAtoms(), 0);\n  for (auto &bond : mol.bonds()) {\n    const Bond::BondDir dir = bond->getBondDir();\n    if (dir != Bond::UNKNOWN) {\n      // the bond is marked as chiral:\n      if (dir == Bond::BEGINWEDGE || dir == Bond::BEGINDASH) {\n        Atom *atom = bond->getBeginAtom();\n        if (atomsSet[atom->getIdx()] ||\n            (!replaceExistingTags &&\n             atom->getChiralTag() != Atom::CHI_UNSPECIFIED)) {\n          continue;\n        }\n        if (atom->getImplicitValence() == -1) {\n          atom->calcExplicitValence(false);\n          atom->calcImplicitValence(false);\n        }\n        Atom::ChiralType code = atomChiralTypeFromBondDir(mol, bond, &conf);\n        if (code != Atom::ChiralType::CHI_UNSPECIFIED) {\n          atomsSet.set(atom->getIdx());\n          //   std::cerr << \"atom \" << atom->getIdx() << \" code \" << code\n          //             << \" from bond \" << bond->getIdx() << std::endl;\n        }\n        atom->setChiralTag(code);\n\n        // within the RD representation, if a three-coordinate atom\n        // is chiral and has an implicit H, that H needs to be made explicit:\n        if (atom->getDegree() == 3 && !atom->getNumExplicitHs() &&\n            atom->getNumImplicitHs() == 1) {\n          atom->setNumExplicitHs(1);\n          // recalculated number of implicit Hs:\n          atom->updatePropertyCache();\n        }\n      }\n    }\n  }\n}\n\nvoid removeStereochemistry(ROMol &mol) {\n  if (mol.hasProp(common_properties::_StereochemDone)) {\n    mol.clearProp(common_properties::_StereochemDone);\n  }\n  for (ROMol::AtomIterator atIt = mol.beginAtoms(); atIt != mol.endAtoms();\n       ++atIt) {\n    (*atIt)->setChiralTag(Atom::CHI_UNSPECIFIED);\n    if ((*atIt)->hasProp(common_properties::_CIPCode)) {\n      (*atIt)->clearProp(common_properties::_CIPCode);\n    }\n    if ((*atIt)->hasProp(common_properties::_CIPRank)) {\n      (*atIt)->clearProp(common_properties::_CIPRank);\n    }\n  }\n  for (ROMol::BondIterator bondIt = mol.beginBonds(); bondIt != mol.endBonds();\n       ++bondIt) {\n    if ((*bondIt)->getBondType() == Bond::DOUBLE) {\n      (*bondIt)->setStereo(Bond::STEREONONE);\n      (*bondIt)->getStereoAtoms().clear();\n    } else if ((*bondIt)->getBondType() == Bond::SINGLE) {\n      (*bondIt)->setBondDir(Bond::NONE);\n    }\n  }\n}\n\n}  // end of namespace MolOps\n}  // end of namespace RDKit\n", "idx": 4, "id": 21818, "msg": "", "proj": "rdkit-rdkit", "lang": "cpp", "sampling_weight": 0.12341796925967485}
{"patch": "@@ -530,6 +530,7 @@ public class K9 extends Application {\n \n         super.onCreate();\n         app = this;\n+        DI.start(this);\n         Globals.setContext(this);\n \n         K9MailLib.setDebugStatus(new K9MailLib.DebugStatus() {", "y": 1, "oldf": "\npackage com.fsck.k9;\n\n\nimport java.io.File;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.SynchronousQueue;\n\nimport android.app.Application;\nimport android.content.ComponentName;\nimport android.content.Context;\nimport android.content.Intent;\nimport android.content.IntentFilter;\nimport android.content.SharedPreferences;\nimport android.content.SharedPreferences.Editor;\nimport android.content.pm.PackageManager;\nimport android.net.Uri;\nimport android.os.AsyncTask;\nimport android.os.Environment;\nimport android.os.Handler;\nimport android.os.Looper;\nimport android.os.StrictMode;\n\nimport com.fsck.k9.Account.SortType;\nimport com.fsck.k9.activity.MessageCompose;\nimport com.fsck.k9.activity.UpgradeDatabases;\nimport com.fsck.k9.controller.MessagingController;\nimport com.fsck.k9.controller.SimpleMessagingListener;\nimport com.fsck.k9.mail.Address;\nimport com.fsck.k9.mail.K9MailLib;\nimport com.fsck.k9.mail.Message;\nimport com.fsck.k9.mail.internet.BinaryTempFileBody;\nimport com.fsck.k9.mail.ssl.LocalKeyStore;\nimport com.fsck.k9.mailstore.LocalStore;\nimport com.fsck.k9.power.DeviceIdleManager;\nimport com.fsck.k9.preferences.Storage;\nimport com.fsck.k9.preferences.StorageEditor;\nimport com.fsck.k9.provider.UnreadWidgetProvider;\nimport com.fsck.k9.service.BootReceiver;\nimport com.fsck.k9.service.MailService;\nimport com.fsck.k9.service.ShutdownReceiver;\nimport com.fsck.k9.service.StorageGoneReceiver;\nimport com.fsck.k9.widget.list.MessageListWidgetProvider;\nimport timber.log.Timber;\nimport timber.log.Timber.DebugTree;\n\n\npublic class K9 extends Application {\n    /**\n     * Components that are interested in knowing when the K9 instance is\n     * available and ready (Android invokes Application.onCreate() after other\n     * components') should implement this interface and register using\n     * {@link K9#registerApplicationAware(ApplicationAware)}.\n     */\n    public static interface ApplicationAware {\n        /**\n         * Called when the Application instance is available and ready.\n         *\n         * @param application\n         *            The application instance. Never <code>null</code>.\n         * @throws Exception\n         */\n        void initializeComponent(Application application);\n    }\n\n    public static Application app = null;\n    public static File tempDirectory;\n    public static final String LOG_TAG = \"k9\";\n\n    /**\n     * Name of the {@link SharedPreferences} file used to store the last known version of the\n     * accounts' databases.\n     *\n     * <p>\n     * See {@link UpgradeDatabases} for a detailed explanation of the database upgrade process.\n     * </p>\n     */\n    private static final String DATABASE_VERSION_CACHE = \"database_version_cache\";\n\n    /**\n     * Key used to store the last known database version of the accounts' databases.\n     *\n     * @see #DATABASE_VERSION_CACHE\n     */\n    private static final String KEY_LAST_ACCOUNT_DATABASE_VERSION = \"last_account_database_version\";\n\n    /**\n     * Components that are interested in knowing when the K9 instance is\n     * available and ready.\n     *\n     * @see ApplicationAware\n     */\n    private static final List<ApplicationAware> observers = new ArrayList<ApplicationAware>();\n\n    /**\n     * This will be {@code true} once the initialization is complete and {@link #notifyObservers()}\n     * was called.\n     * Afterwards calls to {@link #registerApplicationAware(com.fsck.k9.K9.ApplicationAware)} will\n     * immediately call {@link com.fsck.k9.K9.ApplicationAware#initializeComponent(Application)} for the\n     * supplied argument.\n     */\n    private static boolean initialized = false;\n\n    public enum BACKGROUND_OPS {\n        ALWAYS, NEVER, WHEN_CHECKED_AUTO_SYNC\n    }\n\n    private static String language = \"\";\n    private static Theme theme = Theme.LIGHT;\n    private static Theme messageViewTheme = Theme.USE_GLOBAL;\n    private static Theme composerTheme = Theme.USE_GLOBAL;\n    private static boolean useFixedMessageTheme = true;\n\n    private static final FontSizes fontSizes = new FontSizes();\n\n    private static BACKGROUND_OPS backgroundOps = BACKGROUND_OPS.WHEN_CHECKED_AUTO_SYNC;\n    /**\n     * Some log messages can be sent to a file, so that the logs\n     * can be read using unprivileged access (eg. Terminal Emulator)\n     * on the phone, without adb.  Set to null to disable\n     */\n    public static final String logFile = null;\n    //public static final String logFile = Environment.getExternalStorageDirectory() + \"/k9mail/debug.log\";\n\n    /**\n     * If this is enabled, various development settings will be enabled\n     * It should NEVER be on for Market builds\n     * Right now, it just governs strictmode\n     **/\n    public static boolean DEVELOPER_MODE = BuildConfig.DEVELOPER_MODE;\n\n\n    /**\n     * If this is enabled there will be additional logging information sent to\n     * Log.d, including protocol dumps.\n     * Controlled by Preferences at run-time\n     */\n    private static boolean DEBUG = false;\n\n    /**\n     * If this is enabled than logging that normally hides sensitive information\n     * like passwords will show that information.\n     */\n    public static boolean DEBUG_SENSITIVE = false;\n\n    /**\n     * A reference to the {@link SharedPreferences} used for caching the last known database\n     * version.\n     *\n     * @see #checkCachedDatabaseVersion()\n     * @see #setDatabasesUpToDate(boolean)\n     */\n    private static SharedPreferences databaseVersionCache;\n\n    private static boolean animations = true;\n\n    private static boolean confirmDelete = false;\n    private static boolean confirmDiscardMessage = true;\n    private static boolean confirmDeleteStarred = false;\n    private static boolean confirmSpam = false;\n    private static boolean confirmDeleteFromNotification = true;\n    private static boolean confirmMarkAllRead = true;\n\n    private static NotificationHideSubject notificationHideSubject = NotificationHideSubject.NEVER;\n\n    /**\n     * Controls when to hide the subject in the notification area.\n     */\n    public enum NotificationHideSubject {\n        ALWAYS,\n        WHEN_LOCKED,\n        NEVER\n    }\n\n    private static NotificationQuickDelete notificationQuickDelete = NotificationQuickDelete.NEVER;\n\n    /**\n     * Controls behaviour of delete button in notifications.\n     */\n    public enum NotificationQuickDelete {\n        ALWAYS,\n        FOR_SINGLE_MSG,\n        NEVER\n    }\n\n    private static LockScreenNotificationVisibility sLockScreenNotificationVisibility =\n        LockScreenNotificationVisibility.MESSAGE_COUNT;\n\n    public enum LockScreenNotificationVisibility {\n        EVERYTHING,\n        SENDERS,\n        MESSAGE_COUNT,\n        APP_NAME,\n        NOTHING\n    }\n\n    /**\n     * Controls when to use the message list split view.\n     */\n    public enum SplitViewMode {\n        ALWAYS,\n        NEVER,\n        WHEN_IN_LANDSCAPE\n    }\n\n    private static boolean messageListCheckboxes = true;\n    private static boolean messageListStars = true;\n    private static int messageListPreviewLines = 2;\n\n    private static boolean showCorrespondentNames = true;\n    private static boolean messageListSenderAboveSubject = false;\n    private static boolean showContactName = false;\n    private static boolean changeContactNameColor = false;\n    private static int contactNameColor = 0xff00008f;\n    private static boolean showContactPicture = true;\n    private static boolean messageViewFixedWidthFont = false;\n    private static boolean messageViewReturnToList = false;\n    private static boolean messageViewShowNext = false;\n\n    private static boolean gesturesEnabled = true;\n    private static boolean useVolumeKeysForNavigation = false;\n    private static boolean useVolumeKeysForListNavigation = false;\n    private static boolean startIntegratedInbox = false;\n    private static boolean measureAccounts = true;\n    private static boolean countSearchMessages = true;\n    private static boolean hideSpecialAccounts = false;\n    private static boolean autofitWidth;\n    private static boolean quietTimeEnabled = false;\n    private static boolean notificationDuringQuietTimeEnabled = true;\n    private static String quietTimeStarts = null;\n    private static String quietTimeEnds = null;\n    private static String attachmentDefaultPath = \"\";\n    private static boolean wrapFolderNames = false;\n    private static boolean hideUserAgent = false;\n    private static boolean hideTimeZone = false;\n    private static boolean hideHostnameWhenConnecting = false;\n\n    private static String openPgpProvider = \"\";\n    private static boolean openPgpSupportSignOnly = false;\n\n    private static SortType sortType;\n    private static Map<SortType, Boolean> sortAscending = new HashMap<SortType, Boolean>();\n\n    private static boolean useBackgroundAsUnreadIndicator = true;\n    private static boolean threadedViewEnabled = true;\n    private static SplitViewMode splitViewMode = SplitViewMode.NEVER;\n    private static boolean colorizeMissingContactPictures = true;\n\n    private static boolean messageViewArchiveActionVisible = false;\n    private static boolean messageViewDeleteActionVisible = true;\n    private static boolean messageViewMoveActionVisible = false;\n    private static boolean messageViewCopyActionVisible = false;\n    private static boolean messageViewSpamActionVisible = false;\n\n    private static int pgpInlineDialogCounter;\n    private static int pgpSignOnlyDialogCounter;\n\n\n    /**\n     * @see #areDatabasesUpToDate()\n     */\n    private static boolean databasesUpToDate = false;\n\n    /**\n     * For use when displaying that no folder is selected\n     */\n    public static final String FOLDER_NONE = \"-NONE-\";\n\n    public static final String LOCAL_UID_PREFIX = \"K9LOCAL:\";\n\n    public static final String REMOTE_UID_PREFIX = \"K9REMOTE:\";\n\n    public static final String IDENTITY_HEADER = K9MailLib.IDENTITY_HEADER;\n\n    /**\n     * Specifies how many messages will be shown in a folder by default. This number is set\n     * on each new folder and can be incremented with \"Load more messages...\" by the\n     * VISIBLE_LIMIT_INCREMENT\n     */\n    public static final int DEFAULT_VISIBLE_LIMIT = 25;\n\n    /**\n     * The maximum size of an attachment we're willing to download (either View or Save)\n     * Attachments that are base64 encoded (most) will be about 1.375x their actual size\n     * so we should probably factor that in. A 5MB attachment will generally be around\n     * 6.8MB downloaded but only 5MB saved.\n     */\n    public static final int MAX_ATTACHMENT_DOWNLOAD_SIZE = (128 * 1024 * 1024);\n\n\n    /* How many times should K-9 try to deliver a message before giving up\n     * until the app is killed and restarted\n     */\n\n    public static final int MAX_SEND_ATTEMPTS = 5;\n\n    /**\n     * Max time (in millis) the wake lock will be held for when background sync is happening\n     */\n    public static final int WAKE_LOCK_TIMEOUT = 600000;\n\n    public static final int MANUAL_WAKE_LOCK_TIMEOUT = 120000;\n\n    public static final int PUSH_WAKE_LOCK_TIMEOUT = K9MailLib.PUSH_WAKE_LOCK_TIMEOUT;\n\n    public static final int MAIL_SERVICE_WAKE_LOCK_TIMEOUT = 60000;\n\n    public static final int BOOT_RECEIVER_WAKE_LOCK_TIMEOUT = 60000;\n\n    public static final String NO_OPENPGP_PROVIDER = \"\";\n\n    public static class Intents {\n\n        public static class EmailReceived {\n            public static final String ACTION_EMAIL_RECEIVED = BuildConfig.APPLICATION_ID + \".intent.action.EMAIL_RECEIVED\";\n            public static final String ACTION_EMAIL_DELETED = BuildConfig.APPLICATION_ID + \".intent.action.EMAIL_DELETED\";\n            public static final String ACTION_REFRESH_OBSERVER = BuildConfig.APPLICATION_ID + \".intent.action.REFRESH_OBSERVER\";\n            public static final String EXTRA_ACCOUNT = BuildConfig.APPLICATION_ID + \".intent.extra.ACCOUNT\";\n            public static final String EXTRA_FOLDER = BuildConfig.APPLICATION_ID + \".intent.extra.FOLDER\";\n            public static final String EXTRA_SENT_DATE = BuildConfig.APPLICATION_ID + \".intent.extra.SENT_DATE\";\n            public static final String EXTRA_FROM = BuildConfig.APPLICATION_ID + \".intent.extra.FROM\";\n            public static final String EXTRA_TO = BuildConfig.APPLICATION_ID + \".intent.extra.TO\";\n            public static final String EXTRA_CC = BuildConfig.APPLICATION_ID + \".intent.extra.CC\";\n            public static final String EXTRA_BCC = BuildConfig.APPLICATION_ID + \".intent.extra.BCC\";\n            public static final String EXTRA_SUBJECT = BuildConfig.APPLICATION_ID + \".intent.extra.SUBJECT\";\n            public static final String EXTRA_FROM_SELF = BuildConfig.APPLICATION_ID + \".intent.extra.FROM_SELF\";\n        }\n\n        public static class Share {\n            /*\n             * We don't want to use EmailReceived.EXTRA_FROM (\"com.fsck.k9.intent.extra.FROM\")\n             * because of different semantics (String array vs. string with comma separated\n             * email addresses)\n             */\n            public static final String EXTRA_FROM = BuildConfig.APPLICATION_ID + \".intent.extra.SENDER\";\n        }\n    }\n\n    /**\n     * Called throughout the application when the number of accounts has changed. This method\n     * enables or disables the Compose activity, the boot receiver and the service based on\n     * whether any accounts are configured.\n     */\n    public static void setServicesEnabled(Context context) {\n        Context appContext = context.getApplicationContext();\n        int acctLength = Preferences.getPreferences(appContext).getAvailableAccounts().size();\n        boolean enable = acctLength > 0;\n\n        setServicesEnabled(appContext, enable, null);\n\n        updateDeviceIdleReceiver(appContext, enable);\n    }\n\n    private static void updateDeviceIdleReceiver(Context context, boolean enable) {\n        DeviceIdleManager deviceIdleManager = DeviceIdleManager.getInstance(context);\n        if (enable) {\n            deviceIdleManager.registerReceiver();\n        } else {\n            deviceIdleManager.unregisterReceiver();\n        }\n    }\n\n    private static void setServicesEnabled(Context context, boolean enabled, Integer wakeLockId) {\n\n        PackageManager pm = context.getPackageManager();\n\n        if (!enabled && pm.getComponentEnabledSetting(new ComponentName(context, MailService.class)) ==\n                PackageManager.COMPONENT_ENABLED_STATE_ENABLED) {\n            /*\n             * If no accounts now exist but the service is still enabled we're about to disable it\n             * so we'll reschedule to kill off any existing alarms.\n             */\n            MailService.actionReset(context, wakeLockId);\n        }\n        Class<?>[] classes = { MessageCompose.class, BootReceiver.class, MailService.class };\n\n        for (Class<?> clazz : classes) {\n\n            boolean alreadyEnabled = pm.getComponentEnabledSetting(new ComponentName(context, clazz)) ==\n                                     PackageManager.COMPONENT_ENABLED_STATE_ENABLED;\n\n            if (enabled != alreadyEnabled) {\n                pm.setComponentEnabledSetting(\n                    new ComponentName(context, clazz),\n                    enabled ? PackageManager.COMPONENT_ENABLED_STATE_ENABLED :\n                    PackageManager.COMPONENT_ENABLED_STATE_DISABLED,\n                    PackageManager.DONT_KILL_APP);\n            }\n        }\n\n        if (enabled && pm.getComponentEnabledSetting(new ComponentName(context, MailService.class)) ==\n                PackageManager.COMPONENT_ENABLED_STATE_ENABLED) {\n            /*\n             * And now if accounts do exist then we've just enabled the service and we want to\n             * schedule alarms for the new accounts.\n             */\n            MailService.actionReset(context, wakeLockId);\n        }\n\n    }\n\n    /**\n     * Register BroadcastReceivers programmatically because doing it from manifest\n     * would make K-9 auto-start. We don't want auto-start because the initialization\n     * sequence isn't safe while some events occur (SD card unmount).\n     */\n    protected void registerReceivers() {\n        final StorageGoneReceiver receiver = new StorageGoneReceiver();\n        final IntentFilter filter = new IntentFilter();\n        filter.addAction(Intent.ACTION_MEDIA_EJECT);\n        filter.addAction(Intent.ACTION_MEDIA_UNMOUNTED);\n        filter.addDataScheme(\"file\");\n\n        final BlockingQueue<Handler> queue = new SynchronousQueue<Handler>();\n\n        // starting a new thread to handle unmount events\n        new Thread(new Runnable() {\n            @Override\n            public void run() {\n                Looper.prepare();\n                try {\n                    queue.put(new Handler());\n                } catch (InterruptedException e) {\n                    Timber.e(e);\n                }\n                Looper.loop();\n            }\n\n        }, \"Unmount-thread\").start();\n\n        try {\n            final Handler storageGoneHandler = queue.take();\n            registerReceiver(receiver, filter, null, storageGoneHandler);\n            Timber.i(\"Registered: unmount receiver\");\n        } catch (InterruptedException e) {\n            Timber.e(e, \"Unable to register unmount receiver\");\n        }\n\n        registerReceiver(new ShutdownReceiver(), new IntentFilter(Intent.ACTION_SHUTDOWN));\n        Timber.i(\"Registered: shutdown receiver\");\n    }\n\n    public static void save(StorageEditor editor) {\n        editor.putBoolean(\"enableDebugLogging\", K9.DEBUG);\n        editor.putBoolean(\"enableSensitiveLogging\", K9.DEBUG_SENSITIVE);\n        editor.putString(\"backgroundOperations\", K9.backgroundOps.name());\n        editor.putBoolean(\"animations\", animations);\n        editor.putBoolean(\"gesturesEnabled\", gesturesEnabled);\n        editor.putBoolean(\"useVolumeKeysForNavigation\", useVolumeKeysForNavigation);\n        editor.putBoolean(\"useVolumeKeysForListNavigation\", useVolumeKeysForListNavigation);\n        editor.putBoolean(\"autofitWidth\", autofitWidth);\n        editor.putBoolean(\"quietTimeEnabled\", quietTimeEnabled);\n        editor.putBoolean(\"notificationDuringQuietTimeEnabled\", notificationDuringQuietTimeEnabled);\n        editor.putString(\"quietTimeStarts\", quietTimeStarts);\n        editor.putString(\"quietTimeEnds\", quietTimeEnds);\n\n        editor.putBoolean(\"startIntegratedInbox\", startIntegratedInbox);\n        editor.putBoolean(\"measureAccounts\", measureAccounts);\n        editor.putBoolean(\"countSearchMessages\", countSearchMessages);\n        editor.putBoolean(\"messageListSenderAboveSubject\", messageListSenderAboveSubject);\n        editor.putBoolean(\"hideSpecialAccounts\", hideSpecialAccounts);\n        editor.putBoolean(\"messageListStars\", messageListStars);\n        editor.putInt(\"messageListPreviewLines\", messageListPreviewLines);\n        editor.putBoolean(\"messageListCheckboxes\", messageListCheckboxes);\n        editor.putBoolean(\"showCorrespondentNames\", showCorrespondentNames);\n        editor.putBoolean(\"showContactName\", showContactName);\n        editor.putBoolean(\"showContactPicture\", showContactPicture);\n        editor.putBoolean(\"changeRegisteredNameColor\", changeContactNameColor);\n        editor.putInt(\"registeredNameColor\", contactNameColor);\n        editor.putBoolean(\"messageViewFixedWidthFont\", messageViewFixedWidthFont);\n        editor.putBoolean(\"messageViewReturnToList\", messageViewReturnToList);\n        editor.putBoolean(\"messageViewShowNext\", messageViewShowNext);\n        editor.putBoolean(\"wrapFolderNames\", wrapFolderNames);\n        editor.putBoolean(\"hideUserAgent\", hideUserAgent);\n        editor.putBoolean(\"hideTimeZone\", hideTimeZone);\n        editor.putBoolean(\"hideHostnameWhenConnecting\", hideHostnameWhenConnecting);\n\n        editor.putString(\"openPgpProvider\", openPgpProvider);\n        editor.putBoolean(\"openPgpSupportSignOnly\", openPgpSupportSignOnly);\n\n        editor.putString(\"language\", language);\n        editor.putInt(\"theme\", theme.ordinal());\n        editor.putInt(\"messageViewTheme\", messageViewTheme.ordinal());\n        editor.putInt(\"messageComposeTheme\", composerTheme.ordinal());\n        editor.putBoolean(\"fixedMessageViewTheme\", useFixedMessageTheme);\n\n        editor.putBoolean(\"confirmDelete\", confirmDelete);\n        editor.putBoolean(\"confirmDiscardMessage\", confirmDiscardMessage);\n        editor.putBoolean(\"confirmDeleteStarred\", confirmDeleteStarred);\n        editor.putBoolean(\"confirmSpam\", confirmSpam);\n        editor.putBoolean(\"confirmDeleteFromNotification\", confirmDeleteFromNotification);\n        editor.putBoolean(\"confirmMarkAllRead\", confirmMarkAllRead);\n\n        editor.putString(\"sortTypeEnum\", sortType.name());\n        editor.putBoolean(\"sortAscending\", sortAscending.get(sortType));\n\n        editor.putString(\"notificationHideSubject\", notificationHideSubject.toString());\n        editor.putString(\"notificationQuickDelete\", notificationQuickDelete.toString());\n        editor.putString(\"lockScreenNotificationVisibility\", sLockScreenNotificationVisibility.toString());\n\n        editor.putString(\"attachmentdefaultpath\", attachmentDefaultPath);\n        editor.putBoolean(\"useBackgroundAsUnreadIndicator\", useBackgroundAsUnreadIndicator);\n        editor.putBoolean(\"threadedView\", threadedViewEnabled);\n        editor.putString(\"splitViewMode\", splitViewMode.name());\n        editor.putBoolean(\"colorizeMissingContactPictures\", colorizeMissingContactPictures);\n\n        editor.putBoolean(\"messageViewArchiveActionVisible\", messageViewArchiveActionVisible);\n        editor.putBoolean(\"messageViewDeleteActionVisible\", messageViewDeleteActionVisible);\n        editor.putBoolean(\"messageViewMoveActionVisible\", messageViewMoveActionVisible);\n        editor.putBoolean(\"messageViewCopyActionVisible\", messageViewCopyActionVisible);\n        editor.putBoolean(\"messageViewSpamActionVisible\", messageViewSpamActionVisible);\n\n        editor.putInt(\"pgpInlineDialogCounter\", pgpInlineDialogCounter);\n        editor.putInt(\"pgpSignOnlyDialogCounter\", pgpSignOnlyDialogCounter);\n\n        fontSizes.save(editor);\n    }\n\n    @Override\n    public void onCreate() {\n        if (K9.DEVELOPER_MODE) {\n            StrictMode.enableDefaults();\n        }\n\n        PRNGFixes.apply();\n\n        super.onCreate();\n        app = this;\n        Globals.setContext(this);\n\n        K9MailLib.setDebugStatus(new K9MailLib.DebugStatus() {\n            @Override public boolean enabled() {\n                return DEBUG;\n            }\n\n            @Override public boolean debugSensitive() {\n                return DEBUG_SENSITIVE;\n            }\n        });\n\n        checkCachedDatabaseVersion();\n\n        Preferences prefs = Preferences.getPreferences(this);\n        loadPrefs(prefs);\n\n        /*\n         * We have to give MimeMessage a temp directory because File.createTempFile(String, String)\n         * doesn't work in Android and MimeMessage does not have access to a Context.\n         */\n        BinaryTempFileBody.setTempDirectory(getCacheDir());\n\n        LocalKeyStore.setKeyStoreLocation(getDir(\"KeyStore\", MODE_PRIVATE).toString());\n\n        /*\n         * Enable background sync of messages\n         */\n\n        setServicesEnabled(this);\n        registerReceivers();\n\n        MessagingController.getInstance(this).addListener(new SimpleMessagingListener() {\n            private void broadcastIntent(String action, Account account, String folder, Message message) {\n                Uri uri = Uri.parse(\"email://messages/\" + account.getAccountNumber() + \"/\" + Uri.encode(folder) + \"/\" + Uri.encode(message.getUid()));\n                Intent intent = new Intent(action, uri);\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_ACCOUNT, account.getDescription());\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_FOLDER, folder);\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_SENT_DATE, message.getSentDate());\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_FROM, Address.toString(message.getFrom()));\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_TO, Address.toString(message.getRecipients(Message.RecipientType.TO)));\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_CC, Address.toString(message.getRecipients(Message.RecipientType.CC)));\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_BCC, Address.toString(message.getRecipients(Message.RecipientType.BCC)));\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_SUBJECT, message.getSubject());\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_FROM_SELF, account.isAnIdentity(message.getFrom()));\n                K9.this.sendBroadcast(intent);\n\n                Timber.d(\"Broadcasted: action=%s account=%s folder=%s message uid=%s\",\n                        action,\n                        account.getDescription(),\n                        folder,\n                        message.getUid());\n            }\n\n            private void updateUnreadWidget() {\n                try {\n                    UnreadWidgetProvider.updateUnreadCount(K9.this);\n                } catch (Exception e) {\n                    Timber.e(e, \"Error while updating unread widget(s)\");\n                }\n            }\n\n            private void updateMailListWidget() {\n                try {\n                    MessageListWidgetProvider.triggerMessageListWidgetUpdate(K9.this);\n                } catch (RuntimeException e) {\n                    if (BuildConfig.DEBUG) {\n                        throw e;\n                    } else {\n                        Timber.e(e, \"Error while updating message list widget\");\n                    }\n                }\n            }\n\n            @Override\n            public void synchronizeMailboxRemovedMessage(Account account, String folderServerId, Message message) {\n                broadcastIntent(K9.Intents.EmailReceived.ACTION_EMAIL_DELETED, account, folderServerId, message);\n                updateUnreadWidget();\n                updateMailListWidget();\n            }\n\n            @Override\n            public void messageDeleted(Account account, String folderServerId, Message message) {\n                broadcastIntent(K9.Intents.EmailReceived.ACTION_EMAIL_DELETED, account, folderServerId, message);\n                updateUnreadWidget();\n                updateMailListWidget();\n            }\n\n            @Override\n            public void synchronizeMailboxNewMessage(Account account, String folderServerId, Message message) {\n                broadcastIntent(K9.Intents.EmailReceived.ACTION_EMAIL_RECEIVED, account, folderServerId, message);\n                updateUnreadWidget();\n                updateMailListWidget();\n            }\n\n            @Override\n            public void folderStatusChanged(Account account, String folderServerId,\n                    int unreadMessageCount) {\n\n                updateUnreadWidget();\n                updateMailListWidget();\n\n                // let observers know a change occurred\n                Intent intent = new Intent(K9.Intents.EmailReceived.ACTION_REFRESH_OBSERVER, null);\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_ACCOUNT, account.getDescription());\n                intent.putExtra(K9.Intents.EmailReceived.EXTRA_FOLDER, folderServerId);\n                K9.this.sendBroadcast(intent);\n\n            }\n\n        });\n\n        notifyObservers();\n    }\n\n    /**\n     * Loads the last known database version of the accounts' databases from a\n     * {@code SharedPreference}.\n     *\n     * <p>\n     * If the stored version matches {@link LocalStore#DB_VERSION} we know that the databases are\n     * up to date.<br>\n     * Using {@code SharedPreferences} should be a lot faster than opening all SQLite databases to\n     * get the current database version.\n     * </p><p>\n     * See {@link UpgradeDatabases} for a detailed explanation of the database upgrade process.\n     * </p>\n     *\n     * @see #areDatabasesUpToDate()\n     */\n    public void checkCachedDatabaseVersion() {\n        databaseVersionCache = getSharedPreferences(DATABASE_VERSION_CACHE, MODE_PRIVATE);\n\n        int cachedVersion = databaseVersionCache.getInt(KEY_LAST_ACCOUNT_DATABASE_VERSION, 0);\n\n        if (cachedVersion >= LocalStore.DB_VERSION) {\n            K9.setDatabasesUpToDate(false);\n        }\n    }\n\n    /**\n     * Load preferences into our statics.\n     *\n     * If you're adding a preference here, odds are you'll need to add it to\n     * {@link com.fsck.k9.preferences.GlobalSettings}, too.\n     *\n     * @param prefs Preferences to load\n     */\n    public static void loadPrefs(Preferences prefs) {\n        Storage storage = prefs.getStorage();\n        setDebug(storage.getBoolean(\"enableDebugLogging\", BuildConfig.DEVELOPER_MODE));\n        DEBUG_SENSITIVE = storage.getBoolean(\"enableSensitiveLogging\", false);\n        animations = storage.getBoolean(\"animations\", true);\n        gesturesEnabled = storage.getBoolean(\"gesturesEnabled\", false);\n        useVolumeKeysForNavigation = storage.getBoolean(\"useVolumeKeysForNavigation\", false);\n        useVolumeKeysForListNavigation = storage.getBoolean(\"useVolumeKeysForListNavigation\", false);\n        startIntegratedInbox = storage.getBoolean(\"startIntegratedInbox\", false);\n        measureAccounts = storage.getBoolean(\"measureAccounts\", true);\n        countSearchMessages = storage.getBoolean(\"countSearchMessages\", true);\n        hideSpecialAccounts = storage.getBoolean(\"hideSpecialAccounts\", false);\n        messageListSenderAboveSubject = storage.getBoolean(\"messageListSenderAboveSubject\", false);\n        messageListCheckboxes = storage.getBoolean(\"messageListCheckboxes\", false);\n        messageListStars = storage.getBoolean(\"messageListStars\", true);\n        messageListPreviewLines = storage.getInt(\"messageListPreviewLines\", 2);\n\n        autofitWidth = storage.getBoolean(\"autofitWidth\", true);\n\n        quietTimeEnabled = storage.getBoolean(\"quietTimeEnabled\", false);\n        notificationDuringQuietTimeEnabled = storage.getBoolean(\"notificationDuringQuietTimeEnabled\", true);\n        quietTimeStarts = storage.getString(\"quietTimeStarts\", \"21:00\");\n        quietTimeEnds = storage.getString(\"quietTimeEnds\", \"7:00\");\n\n        showCorrespondentNames = storage.getBoolean(\"showCorrespondentNames\", true);\n        showContactName = storage.getBoolean(\"showContactName\", false);\n        showContactPicture = storage.getBoolean(\"showContactPicture\", true);\n        changeContactNameColor = storage.getBoolean(\"changeRegisteredNameColor\", false);\n        contactNameColor = storage.getInt(\"registeredNameColor\", 0xff00008f);\n        messageViewFixedWidthFont = storage.getBoolean(\"messageViewFixedWidthFont\", false);\n        messageViewReturnToList = storage.getBoolean(\"messageViewReturnToList\", false);\n        messageViewShowNext = storage.getBoolean(\"messageViewShowNext\", false);\n        wrapFolderNames = storage.getBoolean(\"wrapFolderNames\", false);\n        hideUserAgent = storage.getBoolean(\"hideUserAgent\", false);\n        hideTimeZone = storage.getBoolean(\"hideTimeZone\", false);\n        hideHostnameWhenConnecting = storage.getBoolean(\"hideHostnameWhenConnecting\", false);\n\n        openPgpProvider = storage.getString(\"openPgpProvider\", NO_OPENPGP_PROVIDER);\n        openPgpSupportSignOnly = storage.getBoolean(\"openPgpSupportSignOnly\", false);\n\n        confirmDelete = storage.getBoolean(\"confirmDelete\", false);\n        confirmDiscardMessage = storage.getBoolean(\"confirmDiscardMessage\", true);\n        confirmDeleteStarred = storage.getBoolean(\"confirmDeleteStarred\", false);\n        confirmSpam = storage.getBoolean(\"confirmSpam\", false);\n        confirmDeleteFromNotification = storage.getBoolean(\"confirmDeleteFromNotification\", true);\n        confirmMarkAllRead = storage.getBoolean(\"confirmMarkAllRead\", true);\n\n        try {\n            String value = storage.getString(\"sortTypeEnum\", Account.DEFAULT_SORT_TYPE.name());\n            sortType = SortType.valueOf(value);\n        } catch (Exception e) {\n            sortType = Account.DEFAULT_SORT_TYPE;\n        }\n\n        boolean sortAscending = storage.getBoolean(\"sortAscending\", Account.DEFAULT_SORT_ASCENDING);\n        K9.sortAscending.put(sortType, sortAscending);\n\n        String notificationHideSubject = storage.getString(\"notificationHideSubject\", null);\n        if (notificationHideSubject == null) {\n            // If the \"notificationHideSubject\" setting couldn't be found, the app was probably\n            // updated. Look for the old \"keyguardPrivacy\" setting and map it to the new enum.\n            K9.notificationHideSubject = (storage.getBoolean(\"keyguardPrivacy\", false)) ?\n                    NotificationHideSubject.WHEN_LOCKED : NotificationHideSubject.NEVER;\n        } else {\n            K9.notificationHideSubject = NotificationHideSubject.valueOf(notificationHideSubject);\n        }\n\n        String notificationQuickDelete = storage.getString(\"notificationQuickDelete\", null);\n        if (notificationQuickDelete != null) {\n            K9.notificationQuickDelete = NotificationQuickDelete.valueOf(notificationQuickDelete);\n        }\n\n        String lockScreenNotificationVisibility = storage.getString(\"lockScreenNotificationVisibility\", null);\n        if(lockScreenNotificationVisibility != null) {\n            sLockScreenNotificationVisibility = LockScreenNotificationVisibility.valueOf(lockScreenNotificationVisibility);\n        }\n\n        String splitViewMode = storage.getString(\"splitViewMode\", null);\n        if (splitViewMode != null) {\n            K9.splitViewMode = SplitViewMode.valueOf(splitViewMode);\n        }\n\n        attachmentDefaultPath = storage.getString(\"attachmentdefaultpath\",\n                Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS).toString());\n        useBackgroundAsUnreadIndicator = storage.getBoolean(\"useBackgroundAsUnreadIndicator\", true);\n        threadedViewEnabled = storage.getBoolean(\"threadedView\", true);\n        fontSizes.load(storage);\n\n        try {\n            setBackgroundOps(BACKGROUND_OPS.valueOf(storage.getString(\n                    \"backgroundOperations\",\n                    BACKGROUND_OPS.WHEN_CHECKED_AUTO_SYNC.name())));\n        } catch (Exception e) {\n            setBackgroundOps(BACKGROUND_OPS.WHEN_CHECKED_AUTO_SYNC);\n        }\n\n        colorizeMissingContactPictures = storage.getBoolean(\"colorizeMissingContactPictures\", true);\n\n        messageViewArchiveActionVisible = storage.getBoolean(\"messageViewArchiveActionVisible\", false);\n        messageViewDeleteActionVisible = storage.getBoolean(\"messageViewDeleteActionVisible\", true);\n        messageViewMoveActionVisible = storage.getBoolean(\"messageViewMoveActionVisible\", false);\n        messageViewCopyActionVisible = storage.getBoolean(\"messageViewCopyActionVisible\", false);\n        messageViewSpamActionVisible = storage.getBoolean(\"messageViewSpamActionVisible\", false);\n\n        pgpInlineDialogCounter = storage.getInt(\"pgpInlineDialogCounter\", 0);\n        pgpSignOnlyDialogCounter = storage.getInt(\"pgpSignOnlyDialogCounter\", 0);\n\n        K9.setK9Language(storage.getString(\"language\", \"\"));\n\n        int themeValue = storage.getInt(\"theme\", Theme.LIGHT.ordinal());\n        // We used to save the resource ID of the theme. So convert that to the new format if\n        // necessary.\n        if (themeValue == Theme.DARK.ordinal() || themeValue == android.R.style.Theme) {\n            K9.setK9Theme(Theme.DARK);\n        } else {\n            K9.setK9Theme(Theme.LIGHT);\n        }\n\n        themeValue = storage.getInt(\"messageViewTheme\", Theme.USE_GLOBAL.ordinal());\n        K9.setK9MessageViewThemeSetting(Theme.values()[themeValue]);\n        themeValue = storage.getInt(\"messageComposeTheme\", Theme.USE_GLOBAL.ordinal());\n        K9.setK9ComposerThemeSetting(Theme.values()[themeValue]);\n        K9.setUseFixedMessageViewTheme(storage.getBoolean(\"fixedMessageViewTheme\", true));\n    }\n\n    /**\n     * since Android invokes Application.onCreate() only after invoking all\n     * other components' onCreate(), here is a way to notify interested\n     * component that the application is available and ready\n     */\n    protected void notifyObservers() {\n        synchronized (observers) {\n            for (final ApplicationAware aware : observers) {\n                Timber.v(\"Initializing observer: %s\", aware);\n\n                try {\n                    aware.initializeComponent(this);\n                } catch (Exception e) {\n                    Timber.w(e, \"Failure when notifying %s\", aware);\n                }\n            }\n\n            initialized = true;\n            observers.clear();\n        }\n    }\n\n    /**\n     * Register a component to be notified when the {@link K9} instance is ready.\n     *\n     * @param component\n     *            Never <code>null</code>.\n     */\n    public static void registerApplicationAware(final ApplicationAware component) {\n        synchronized (observers) {\n            if (initialized) {\n                component.initializeComponent(K9.app);\n            } else if (!observers.contains(component)) {\n                observers.add(component);\n            }\n        }\n    }\n\n    public static String getK9Language() {\n        return language;\n    }\n\n    public static void setK9Language(String nlanguage) {\n        language = nlanguage;\n    }\n\n    /**\n     * Possible values for the different theme settings.\n     *\n     * <p><strong>Important:</strong>\n     * Do not change the order of the items! The ordinal value (position) is used when saving the\n     * settings.</p>\n     */\n    public enum Theme {\n        LIGHT,\n        DARK,\n        USE_GLOBAL\n    }\n\n    public static int getK9ThemeResourceId(Theme themeId) {\n        return (themeId == Theme.LIGHT) ? R.style.Theme_K9_Light : R.style.Theme_K9_Dark;\n    }\n\n    public static int getK9ThemeResourceId() {\n        return getK9ThemeResourceId(theme);\n    }\n\n    public static Theme getK9MessageViewTheme() {\n        return messageViewTheme == Theme.USE_GLOBAL ? theme : messageViewTheme;\n    }\n\n    public static Theme getK9MessageViewThemeSetting() {\n        return messageViewTheme;\n    }\n\n    public static Theme getK9ComposerTheme() {\n        return composerTheme == Theme.USE_GLOBAL ? theme : composerTheme;\n    }\n\n    public static Theme getK9ComposerThemeSetting() {\n        return composerTheme;\n    }\n\n    public static Theme getK9Theme() {\n        return theme;\n    }\n\n    public static void setK9Theme(Theme ntheme) {\n        if (ntheme != Theme.USE_GLOBAL) {\n            theme = ntheme;\n        }\n    }\n\n    public static void setK9MessageViewThemeSetting(Theme nMessageViewTheme) {\n        messageViewTheme = nMessageViewTheme;\n    }\n\n    public static boolean useFixedMessageViewTheme() {\n        return useFixedMessageTheme;\n    }\n\n    public static void setK9ComposerThemeSetting(Theme compTheme) {\n        composerTheme = compTheme;\n    }\n\n    public static void setUseFixedMessageViewTheme(boolean useFixed) {\n        useFixedMessageTheme = useFixed;\n        if (!useFixedMessageTheme && messageViewTheme == Theme.USE_GLOBAL) {\n            messageViewTheme = theme;\n        }\n    }\n\n    public static BACKGROUND_OPS getBackgroundOps() {\n        return backgroundOps;\n    }\n\n    public static boolean setBackgroundOps(BACKGROUND_OPS backgroundOps) {\n        BACKGROUND_OPS oldBackgroundOps = K9.backgroundOps;\n        K9.backgroundOps = backgroundOps;\n        return backgroundOps != oldBackgroundOps;\n    }\n\n    public static boolean setBackgroundOps(String nbackgroundOps) {\n        return setBackgroundOps(BACKGROUND_OPS.valueOf(nbackgroundOps));\n    }\n\n    public static boolean gesturesEnabled() {\n        return gesturesEnabled;\n    }\n\n    public static void setGesturesEnabled(boolean gestures) {\n        gesturesEnabled = gestures;\n    }\n\n    public static boolean useVolumeKeysForNavigationEnabled() {\n        return useVolumeKeysForNavigation;\n    }\n\n    public static void setUseVolumeKeysForNavigation(boolean volume) {\n        useVolumeKeysForNavigation = volume;\n    }\n\n    public static boolean useVolumeKeysForListNavigationEnabled() {\n        return useVolumeKeysForListNavigation;\n    }\n\n    public static void setUseVolumeKeysForListNavigation(boolean enabled) {\n        useVolumeKeysForListNavigation = enabled;\n    }\n\n    public static boolean autofitWidth() {\n        return autofitWidth;\n    }\n\n    public static void setAutofitWidth(boolean autofitWidth) {\n        K9.autofitWidth = autofitWidth;\n    }\n\n    public static boolean getQuietTimeEnabled() {\n        return quietTimeEnabled;\n    }\n\n    public static void setQuietTimeEnabled(boolean quietTimeEnabled) {\n        K9.quietTimeEnabled = quietTimeEnabled;\n    }\n\n    public static boolean isNotificationDuringQuietTimeEnabled() {\n        return notificationDuringQuietTimeEnabled;\n    }\n\n    public static void setNotificationDuringQuietTimeEnabled(boolean notificationDuringQuietTimeEnabled) {\n        K9.notificationDuringQuietTimeEnabled = notificationDuringQuietTimeEnabled;\n    }\n\n    public static String getQuietTimeStarts() {\n        return quietTimeStarts;\n    }\n\n    public static void setQuietTimeStarts(String quietTimeStarts) {\n        K9.quietTimeStarts = quietTimeStarts;\n    }\n\n    public static String getQuietTimeEnds() {\n        return quietTimeEnds;\n    }\n\n    public static void setQuietTimeEnds(String quietTimeEnds) {\n        K9.quietTimeEnds = quietTimeEnds;\n    }\n\n\n    public static boolean isQuietTime() {\n        if (!quietTimeEnabled) {\n            return false;\n        }\n\n        QuietTimeChecker quietTimeChecker = new QuietTimeChecker(Clock.INSTANCE, quietTimeStarts, quietTimeEnds);\n        return quietTimeChecker.isQuietTime();\n    }\n\n    public static void setDebug(boolean debug) {\n        K9.DEBUG = debug;\n        updateLoggingStatus();\n    }\n\n    public static boolean isDebug() {\n        return DEBUG;\n    }\n\n    public static boolean startIntegratedInbox() {\n        return startIntegratedInbox;\n    }\n\n    public static void setStartIntegratedInbox(boolean startIntegratedInbox) {\n        K9.startIntegratedInbox = startIntegratedInbox;\n    }\n\n    public static boolean showAnimations() {\n        return animations;\n    }\n\n    public static void setAnimations(boolean animations) {\n        K9.animations = animations;\n    }\n\n    public static int messageListPreviewLines() {\n        return messageListPreviewLines;\n    }\n\n    public static void setMessageListPreviewLines(int lines) {\n        messageListPreviewLines = lines;\n    }\n\n    public static boolean messageListCheckboxes() {\n        return messageListCheckboxes;\n    }\n\n    public static void setMessageListCheckboxes(boolean checkboxes) {\n        messageListCheckboxes = checkboxes;\n    }\n\n    public static boolean messageListStars() {\n        return messageListStars;\n    }\n\n    public static void setMessageListStars(boolean stars) {\n        messageListStars = stars;\n    }\n\n    public static boolean showCorrespondentNames() {\n        return showCorrespondentNames;\n    }\n\n     public static boolean messageListSenderAboveSubject() {\n         return messageListSenderAboveSubject;\n     }\n\n    public static void setMessageListSenderAboveSubject(boolean sender) {\n         messageListSenderAboveSubject = sender;\n    }\n    public static void setShowCorrespondentNames(boolean showCorrespondentNames) {\n        K9.showCorrespondentNames = showCorrespondentNames;\n    }\n\n    public static boolean showContactName() {\n        return showContactName;\n    }\n\n    public static void setShowContactName(boolean showContactName) {\n        K9.showContactName = showContactName;\n    }\n\n    public static boolean changeContactNameColor() {\n        return changeContactNameColor;\n    }\n\n    public static void setChangeContactNameColor(boolean changeContactNameColor) {\n        K9.changeContactNameColor = changeContactNameColor;\n    }\n\n    public static int getContactNameColor() {\n        return contactNameColor;\n    }\n\n    public static void setContactNameColor(int contactNameColor) {\n        K9.contactNameColor = contactNameColor;\n    }\n\n    public static boolean messageViewFixedWidthFont() {\n        return messageViewFixedWidthFont;\n    }\n\n    public static void setMessageViewFixedWidthFont(boolean fixed) {\n        messageViewFixedWidthFont = fixed;\n    }\n\n    public static boolean messageViewReturnToList() {\n        return messageViewReturnToList;\n    }\n\n    public static void setMessageViewReturnToList(boolean messageViewReturnToList) {\n        K9.messageViewReturnToList = messageViewReturnToList;\n    }\n\n    public static boolean messageViewShowNext() {\n        return messageViewShowNext;\n    }\n\n    public static void setMessageViewShowNext(boolean messageViewShowNext) {\n        K9.messageViewShowNext = messageViewShowNext;\n    }\n\n    public static FontSizes getFontSizes() {\n        return fontSizes;\n    }\n\n    public static boolean measureAccounts() {\n        return measureAccounts;\n    }\n\n    public static void setMeasureAccounts(boolean measureAccounts) {\n        K9.measureAccounts = measureAccounts;\n    }\n\n    public static boolean countSearchMessages() {\n        return countSearchMessages;\n    }\n\n    public static void setCountSearchMessages(boolean countSearchMessages) {\n        K9.countSearchMessages = countSearchMessages;\n    }\n\n    public static boolean isHideSpecialAccounts() {\n        return hideSpecialAccounts;\n    }\n\n    public static void setHideSpecialAccounts(boolean hideSpecialAccounts) {\n        K9.hideSpecialAccounts = hideSpecialAccounts;\n    }\n\n    public static boolean confirmDelete() {\n        return confirmDelete;\n    }\n\n    public static void setConfirmDelete(final boolean confirm) {\n        confirmDelete = confirm;\n    }\n\n    public static boolean confirmDeleteStarred() {\n        return confirmDeleteStarred;\n    }\n\n    public static void setConfirmDeleteStarred(final boolean confirm) {\n        confirmDeleteStarred = confirm;\n    }\n\n    public static boolean confirmSpam() {\n        return confirmSpam;\n    }\n\n    public static boolean confirmDiscardMessage() {\n        return confirmDiscardMessage;\n    }\n\n    public static void setConfirmSpam(final boolean confirm) {\n        confirmSpam = confirm;\n    }\n\n    public static void setConfirmDiscardMessage(final boolean confirm) {\n        confirmDiscardMessage = confirm;\n    }\n\n    public static boolean confirmDeleteFromNotification() {\n        return confirmDeleteFromNotification;\n    }\n\n    public static void setConfirmDeleteFromNotification(final boolean confirm) {\n        confirmDeleteFromNotification = confirm;\n    }\n\n    public static boolean confirmMarkAllRead() {\n        return confirmMarkAllRead;\n    }\n\n    public static void setConfirmMarkAllRead(final boolean confirm) {\n        confirmMarkAllRead = confirm;\n    }\n\n    public static NotificationHideSubject getNotificationHideSubject() {\n        return notificationHideSubject;\n    }\n\n    public static void setNotificationHideSubject(final NotificationHideSubject mode) {\n        notificationHideSubject = mode;\n    }\n\n    public static NotificationQuickDelete getNotificationQuickDeleteBehaviour() {\n        return notificationQuickDelete;\n    }\n\n    public static void setNotificationQuickDeleteBehaviour(final NotificationQuickDelete mode) {\n        notificationQuickDelete = mode;\n    }\n\n    public static LockScreenNotificationVisibility getLockScreenNotificationVisibility() {\n        return sLockScreenNotificationVisibility;\n    }\n\n    public static void setLockScreenNotificationVisibility(final LockScreenNotificationVisibility visibility) {\n        sLockScreenNotificationVisibility = visibility;\n    }\n\n    public static boolean wrapFolderNames() {\n        return wrapFolderNames;\n    }\n    public static void setWrapFolderNames(final boolean state) {\n        wrapFolderNames = state;\n    }\n\n    public static boolean hideUserAgent() {\n        return hideUserAgent;\n    }\n    public static void setHideUserAgent(final boolean state) {\n        hideUserAgent = state;\n    }\n\n    public static boolean hideTimeZone() {\n        return hideTimeZone;\n    }\n    public static void setHideTimeZone(final boolean state) {\n        hideTimeZone = state;\n    }\n\n    public static boolean hideHostnameWhenConnecting() {\n        return hideHostnameWhenConnecting;\n    }\n\n    public static void setHideHostnameWhenConnecting(final boolean state) {\n        hideHostnameWhenConnecting = state;\n    }\n\n    public static boolean isOpenPgpProviderConfigured() {\n        return !NO_OPENPGP_PROVIDER.equals(openPgpProvider);\n    }\n\n    public static String getOpenPgpProvider() {\n        return openPgpProvider;\n    }\n\n    public static void setOpenPgpProvider(String openPgpProvider) {\n        K9.openPgpProvider = openPgpProvider;\n    }\n\n    public static boolean getOpenPgpSupportSignOnly() {\n        return openPgpSupportSignOnly;\n    }\n\n    public static void setOpenPgpSupportSignOnly(boolean supportSignOnly) {\n        openPgpSupportSignOnly = supportSignOnly;\n    }\n\n    public static String getAttachmentDefaultPath() {\n        return attachmentDefaultPath;\n    }\n\n    public static void setAttachmentDefaultPath(String attachmentDefaultPath) {\n        K9.attachmentDefaultPath = attachmentDefaultPath;\n    }\n\n    public static synchronized SortType getSortType() {\n        return sortType;\n    }\n\n    public static synchronized void setSortType(SortType sortType) {\n        K9.sortType = sortType;\n    }\n\n    public static synchronized boolean isSortAscending(SortType sortType) {\n        if (sortAscending.get(sortType) == null) {\n            sortAscending.put(sortType, sortType.isDefaultAscending());\n        }\n        return sortAscending.get(sortType);\n    }\n\n    public static synchronized void setSortAscending(SortType sortType, boolean sortAscending) {\n        K9.sortAscending.put(sortType, sortAscending);\n    }\n\n    public static synchronized boolean useBackgroundAsUnreadIndicator() {\n        return useBackgroundAsUnreadIndicator;\n    }\n\n    public static synchronized void setUseBackgroundAsUnreadIndicator(boolean enabled) {\n        useBackgroundAsUnreadIndicator = enabled;\n    }\n\n    public static synchronized boolean isThreadedViewEnabled() {\n        return threadedViewEnabled;\n    }\n\n    public static synchronized void setThreadedViewEnabled(boolean enable) {\n        threadedViewEnabled = enable;\n    }\n\n    public static synchronized SplitViewMode getSplitViewMode() {\n        return splitViewMode;\n    }\n\n    public static synchronized void setSplitViewMode(SplitViewMode mode) {\n        splitViewMode = mode;\n    }\n\n    public static boolean showContactPicture() {\n        return showContactPicture;\n    }\n\n    public static void setShowContactPicture(boolean show) {\n        showContactPicture = show;\n    }\n\n    public static boolean isColorizeMissingContactPictures() {\n        return colorizeMissingContactPictures;\n    }\n\n    public static void setColorizeMissingContactPictures(boolean enabled) {\n        colorizeMissingContactPictures = enabled;\n    }\n\n    public static boolean isMessageViewArchiveActionVisible() {\n        return messageViewArchiveActionVisible;\n    }\n\n    public static void setMessageViewArchiveActionVisible(boolean visible) {\n        messageViewArchiveActionVisible = visible;\n    }\n\n    public static boolean isMessageViewDeleteActionVisible() {\n        return messageViewDeleteActionVisible;\n    }\n\n    public static void setMessageViewDeleteActionVisible(boolean visible) {\n        messageViewDeleteActionVisible = visible;\n    }\n\n    public static boolean isMessageViewMoveActionVisible() {\n        return messageViewMoveActionVisible;\n    }\n\n    public static void setMessageViewMoveActionVisible(boolean visible) {\n        messageViewMoveActionVisible = visible;\n    }\n\n    public static boolean isMessageViewCopyActionVisible() {\n        return messageViewCopyActionVisible;\n    }\n\n    public static void setMessageViewCopyActionVisible(boolean visible) {\n        messageViewCopyActionVisible = visible;\n    }\n\n    public static boolean isMessageViewSpamActionVisible() {\n        return messageViewSpamActionVisible;\n    }\n\n    public static void setMessageViewSpamActionVisible(boolean visible) {\n        messageViewSpamActionVisible = visible;\n    }\n\n    public static int getPgpInlineDialogCounter() {\n        return pgpInlineDialogCounter;\n    }\n\n    public static void setPgpInlineDialogCounter(int pgpInlineDialogCounter) {\n        K9.pgpInlineDialogCounter = pgpInlineDialogCounter;\n    }\n\n    public static int getPgpSignOnlyDialogCounter() {\n        return pgpSignOnlyDialogCounter;\n    }\n\n    public static void setPgpSignOnlyDialogCounter(int pgpSignOnlyDialogCounter) {\n        K9.pgpSignOnlyDialogCounter = pgpSignOnlyDialogCounter;\n    }\n\n    /**\n     * Check if we already know whether all databases are using the current database schema.\n     *\n     * <p>\n     * This method is only used for optimizations. If it returns {@code true} we can be certain that\n     * getting a {@link LocalStore} instance won't trigger a schema upgrade.\n     * </p>\n     *\n     * @return {@code true}, if we know that all databases are using the current database schema.\n     *         {@code false}, otherwise.\n     */\n    public static synchronized boolean areDatabasesUpToDate() {\n        return databasesUpToDate;\n    }\n\n    /**\n     * Remember that all account databases are using the most recent database schema.\n     *\n     * @param save\n     *         Whether or not to write the current database version to the\n     *         {@code SharedPreferences} {@link #DATABASE_VERSION_CACHE}.\n     *\n     * @see #areDatabasesUpToDate()\n     */\n    public static synchronized void setDatabasesUpToDate(boolean save) {\n        databasesUpToDate = true;\n\n        if (save) {\n            Editor editor = databaseVersionCache.edit();\n            editor.putInt(KEY_LAST_ACCOUNT_DATABASE_VERSION, LocalStore.DB_VERSION);\n            editor.apply();\n        }\n    }\n\n    private static void updateLoggingStatus() {\n        Timber.uprootAll();\n        boolean enableDebugLogging = BuildConfig.DEBUG || DEBUG;\n        if (enableDebugLogging) {\n            Timber.plant(new DebugTree());\n        }\n    }\n\n    public static void saveSettingsAsync() {\n        new AsyncTask<Void,Void,Void>() {\n            @Override\n            protected Void doInBackground(Void... voids) {\n                Preferences prefs = Preferences.getPreferences(app);\n                StorageEditor editor = prefs.getStorage().edit();\n                save(editor);\n                editor.commit();\n\n                return null;\n            }\n        }.execute();\n    }\n\n}\n", "idx": 1, "id": 16694, "msg": "is `DI` an established name for this class? seems unnecessarily nondescriptive", "proj": "k9mail-k-9", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -200,7 +200,7 @@ public class JsonHttpRemoteConfig {\n     }\n   }\n \n-  private UrlMapper getUrlMapper(String method) {\n+  protected UrlMapper getUrlMapper(String method) {\n     if (\"DELETE\".equals(method)) {\n       return deleteMapper;\n     } else if (\"GET\".equals(method)) {", "y": 1, "oldf": "/*\nCopyright 2012 Selenium committers\nCopyright 2012 Software Freedom Conservancy\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage org.openqa.selenium.remote.server;\n\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.remote.SessionNotFoundException;\nimport org.openqa.selenium.remote.server.handler.AcceptAlert;\nimport org.openqa.selenium.remote.server.handler.AddConfig;\nimport org.openqa.selenium.remote.server.handler.AddCookie;\nimport org.openqa.selenium.remote.server.handler.CaptureScreenshot;\nimport org.openqa.selenium.remote.server.handler.ChangeUrl;\nimport org.openqa.selenium.remote.server.handler.ClearElement;\nimport org.openqa.selenium.remote.server.handler.ClickElement;\nimport org.openqa.selenium.remote.server.handler.CloseWindow;\nimport org.openqa.selenium.remote.server.handler.ConfigureTimeout;\nimport org.openqa.selenium.remote.server.handler.DeleteCookie;\nimport org.openqa.selenium.remote.server.handler.DeleteNamedCookie;\nimport org.openqa.selenium.remote.server.handler.DeleteSession;\nimport org.openqa.selenium.remote.server.handler.DescribeElement;\nimport org.openqa.selenium.remote.server.handler.DismissAlert;\nimport org.openqa.selenium.remote.server.handler.ElementEquality;\nimport org.openqa.selenium.remote.server.handler.ExecuteAsyncScript;\nimport org.openqa.selenium.remote.server.handler.ExecuteScript;\nimport org.openqa.selenium.remote.server.handler.FindActiveElement;\nimport org.openqa.selenium.remote.server.handler.FindChildElement;\nimport org.openqa.selenium.remote.server.handler.FindChildElements;\nimport org.openqa.selenium.remote.server.handler.FindElement;\nimport org.openqa.selenium.remote.server.handler.FindElements;\nimport org.openqa.selenium.remote.server.handler.GetAlertText;\nimport org.openqa.selenium.remote.server.handler.GetAllCookies;\nimport org.openqa.selenium.remote.server.handler.GetAllSessions;\nimport org.openqa.selenium.remote.server.handler.GetAllWindowHandles;\nimport org.openqa.selenium.remote.server.handler.GetAvailableLogTypesHandler;\nimport org.openqa.selenium.remote.server.handler.GetCssProperty;\nimport org.openqa.selenium.remote.server.handler.GetCurrentUrl;\nimport org.openqa.selenium.remote.server.handler.GetCurrentWindowHandle;\nimport org.openqa.selenium.remote.server.handler.GetElementAttribute;\nimport org.openqa.selenium.remote.server.handler.GetElementDisplayed;\nimport org.openqa.selenium.remote.server.handler.GetElementEnabled;\nimport org.openqa.selenium.remote.server.handler.GetElementLocation;\nimport org.openqa.selenium.remote.server.handler.GetElementLocationInView;\nimport org.openqa.selenium.remote.server.handler.GetElementSelected;\nimport org.openqa.selenium.remote.server.handler.GetElementSize;\nimport org.openqa.selenium.remote.server.handler.GetElementText;\nimport org.openqa.selenium.remote.server.handler.GetElementValue;\nimport org.openqa.selenium.remote.server.handler.GetPageSource;\nimport org.openqa.selenium.remote.server.handler.GetScreenOrientation;\nimport org.openqa.selenium.remote.server.handler.GetSessionLogsHandler;\nimport org.openqa.selenium.remote.server.handler.GetSessionCapabilities;\nimport org.openqa.selenium.remote.server.handler.GetTagName;\nimport org.openqa.selenium.remote.server.handler.GetTitle;\nimport org.openqa.selenium.remote.server.handler.GetWindowPosition;\nimport org.openqa.selenium.remote.server.handler.GetWindowSize;\nimport org.openqa.selenium.remote.server.handler.GoBack;\nimport org.openqa.selenium.remote.server.handler.GoForward;\nimport org.openqa.selenium.remote.server.handler.ImeActivateEngine;\nimport org.openqa.selenium.remote.server.handler.ImeDeactivate;\nimport org.openqa.selenium.remote.server.handler.ImeGetActiveEngine;\nimport org.openqa.selenium.remote.server.handler.ImeGetAvailableEngines;\nimport org.openqa.selenium.remote.server.handler.ImeIsActivated;\nimport org.openqa.selenium.remote.server.handler.ImplicitlyWait;\nimport org.openqa.selenium.remote.server.handler.GetLogHandler;\nimport org.openqa.selenium.remote.server.handler.MaximizeWindow;\nimport org.openqa.selenium.remote.server.handler.NewSession;\nimport org.openqa.selenium.remote.server.handler.RefreshPage;\nimport org.openqa.selenium.remote.server.handler.Rotate;\nimport org.openqa.selenium.remote.server.handler.SendKeys;\nimport org.openqa.selenium.remote.server.handler.SetAlertText;\nimport org.openqa.selenium.remote.server.handler.SetScriptTimeout;\nimport org.openqa.selenium.remote.server.handler.SetWindowPosition;\nimport org.openqa.selenium.remote.server.handler.SetWindowSize;\nimport org.openqa.selenium.remote.server.handler.Status;\nimport org.openqa.selenium.remote.server.handler.SubmitElement;\nimport org.openqa.selenium.remote.server.handler.SwitchToFrame;\nimport org.openqa.selenium.remote.server.handler.SwitchToWindow;\nimport org.openqa.selenium.remote.server.handler.UploadFile;\nimport org.openqa.selenium.remote.server.handler.html5.ClearLocalStorage;\nimport org.openqa.selenium.remote.server.handler.html5.ClearSessionStorage;\nimport org.openqa.selenium.remote.server.handler.html5.ExecuteSQL;\nimport org.openqa.selenium.remote.server.handler.html5.GetAppCacheStatus;\nimport org.openqa.selenium.remote.server.handler.html5.GetLocalStorageItem;\nimport org.openqa.selenium.remote.server.handler.html5.GetLocalStorageKeys;\nimport org.openqa.selenium.remote.server.handler.html5.GetLocalStorageSize;\nimport org.openqa.selenium.remote.server.handler.html5.GetLocationContext;\nimport org.openqa.selenium.remote.server.handler.html5.GetSessionStorageItem;\nimport org.openqa.selenium.remote.server.handler.html5.GetSessionStorageKeys;\nimport org.openqa.selenium.remote.server.handler.html5.GetSessionStorageSize;\nimport org.openqa.selenium.remote.server.handler.html5.IsBrowserOnline;\nimport org.openqa.selenium.remote.server.handler.html5.RemoveLocalStorageItem;\nimport org.openqa.selenium.remote.server.handler.html5.RemoveSessionStorageItem;\nimport org.openqa.selenium.remote.server.handler.html5.SetBrowserConnection;\nimport org.openqa.selenium.remote.server.handler.html5.SetLocalStorageItem;\nimport org.openqa.selenium.remote.server.handler.html5.SetLocationContext;\nimport org.openqa.selenium.remote.server.handler.html5.SetSessionStorageItem;\nimport org.openqa.selenium.remote.server.handler.interactions.ClickInSession;\nimport org.openqa.selenium.remote.server.handler.interactions.DoubleClickInSession;\nimport org.openqa.selenium.remote.server.handler.interactions.MouseDown;\nimport org.openqa.selenium.remote.server.handler.interactions.MouseMoveToLocation;\nimport org.openqa.selenium.remote.server.handler.interactions.MouseUp;\nimport org.openqa.selenium.remote.server.handler.interactions.SendKeyToActiveElement;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.DoubleTapOnElement;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.Down;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.Flick;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.LongPressOnElement;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.Move;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.Scroll;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.SingleTapOnElement;\nimport org.openqa.selenium.remote.server.handler.interactions.touch.Up;\nimport org.openqa.selenium.remote.server.renderer.EmptyResult;\nimport org.openqa.selenium.remote.server.renderer.ForwardResult;\nimport org.openqa.selenium.remote.server.renderer.JsonErrorExceptionResult;\nimport org.openqa.selenium.remote.server.renderer.JsonResult;\nimport org.openqa.selenium.remote.server.renderer.RedirectResult;\nimport org.openqa.selenium.remote.server.renderer.ResourceCopyResult;\nimport org.openqa.selenium.remote.server.resource.StaticResource;\nimport org.openqa.selenium.remote.server.rest.RestishHandler;\nimport org.openqa.selenium.remote.server.rest.Result;\nimport org.openqa.selenium.remote.server.rest.ResultConfig;\nimport org.openqa.selenium.remote.server.rest.ResultType;\nimport org.openqa.selenium.remote.server.rest.UrlMapper;\nimport org.openqa.selenium.remote.server.xdrpc.CrossDomainRpcRenderer;\n\nimport java.util.EnumSet;\nimport java.util.logging.Logger;\n\nimport static org.openqa.selenium.remote.server.HttpStatusCodes.INTERNAL_SERVER_ERROR;\nimport static org.openqa.selenium.remote.server.HttpStatusCodes.NOT_FOUND;\n\npublic class JsonHttpRemoteConfig {\n  private static final String EXCEPTION = \":exception\";\n  private static final String RESPONSE = \":response\";\n\n  private UrlMapper getMapper;\n  private UrlMapper postMapper;\n  private UrlMapper deleteMapper;\n  private final Logger log;\n\n  public JsonHttpRemoteConfig(DriverSessions sessions, Logger log) {\n    this.log = log;\n    setUpMappings(sessions, log);\n  }\n\n  public void addGlobalHandler(ResultType type, Result result) {\n    getMapper.addGlobalHandler(type, result);\n    postMapper.addGlobalHandler(type, result);\n    deleteMapper.addGlobalHandler(type, result);\n  }\n\n  public ResultConfig addNewGetMapping(String path, Class<? extends RestishHandler> implementationClass) {\n    return getMapper.bind(path, implementationClass);\n  }\n\n  public ResultConfig addNewPostMapping(String path, Class<? extends RestishHandler> implementationClass) {\n    return postMapper.bind(path, implementationClass);\n  }\n\n  public ResultConfig addNewDeleteMapping(String path,\n                                             Class<? extends RestishHandler> implementationClass) {\n    return deleteMapper.bind(path, implementationClass);\n  }\n\n  public void handleRequest(HttpRequest request, HttpResponse response)\n      throws WebDriverException {\n    try {\n      UrlMapper mapper = getUrlMapper(request.getMethod());\n      if (mapper == null) {\n        response.setStatus(INTERNAL_SERVER_ERROR);\n        response.end();\n        return;\n      }\n\n      ResultConfig config = mapper.getConfig(request.getPath());\n      if (config == null) {\n        response.setStatus(NOT_FOUND);\n        response.end();\n      } else {\n        config.handle(request.getPath(), request, response);\n      }\n    } catch (SessionNotFoundException e){\n      response.setStatus(NOT_FOUND);\n      response.end();\n    } catch (Exception e) {\n      log.warning(\"Fatal, unhandled exception: \" + request.getPath() + \": \" + e);\n      throw new WebDriverException(e);\n    }\n  }\n\n  private UrlMapper getUrlMapper(String method) {\n    if (\"DELETE\".equals(method)) {\n      return deleteMapper;\n    } else if (\"GET\".equals(method)) {\n      return getMapper;\n    } else if (\"POST\".equals(method)) {\n      return postMapper;\n    } else {\n      throw new IllegalArgumentException(\"Unknown method: \" + method);\n    }\n  }\n\n  private void setUpMappings(DriverSessions driverSessions, Logger logger) {\n    final EmptyResult emptyResponse = new EmptyResult();\n    final JsonResult jsonResponse = new JsonResult(RESPONSE);\n\n    getMapper = new UrlMapper(driverSessions, logger);\n    postMapper = new UrlMapper(driverSessions, logger);\n    deleteMapper = new UrlMapper(driverSessions, logger);\n\n    Result jsonErrorResult = new Result(MimeType.EMPTY,\n                                        new JsonErrorExceptionResult(EXCEPTION, RESPONSE));\n    addGlobalHandler(ResultType.EXCEPTION, jsonErrorResult);\n    addGlobalHandler(ResultType.ERROR, jsonErrorResult);\n\n    Result xdrpcResult = new Result(MimeType.CROSS_DOMAIN_RPC,\n                                    new CrossDomainRpcRenderer(RESPONSE, EXCEPTION), true);\n    for (ResultType resultType : EnumSet.allOf(ResultType.class)) {\n      addGlobalHandler(resultType, xdrpcResult);\n    }\n\n    // When requesting the command root from a JSON-client, just return the server\n    // status. For everyone else, redirect to the web front end.\n    getMapper.bind(\"/\", Status.class)\n        .on(ResultType.SUCCESS, new RedirectResult(\"/static/resource/hub.html\"))\n        .on(ResultType.SUCCESS, jsonResponse, \"application/json\");\n\n    getMapper.bind(\"/static/resource/:file\", StaticResource.class)\n        .on(ResultType.SUCCESS, new ResourceCopyResult(RESPONSE))\n            // Nope, JSON clients don't get access to static resources.\n        .on(ResultType.SUCCESS, emptyResponse, \"application/json\");\n\n    postMapper.bind(\"/config/drivers\", AddConfig.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/status\", Status.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/sessions\", GetAllSessions.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session\", NewSession.class)\n        .on(ResultType.SUCCESS, new RedirectResult(\"/session/:sessionId\"));\n    getMapper.bind(\"/session/:sessionId\", GetSessionCapabilities.class)\n        .on(ResultType.SUCCESS, new ForwardResult(\"/WEB-INF/views/sessionCapabilities.jsp\"))\n        .on(ResultType.SUCCESS, jsonResponse, \"application/json\");\n\n    deleteMapper.bind(\"/session/:sessionId\", DeleteSession.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/session/:sessionId/window_handle\", GetCurrentWindowHandle.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/window_handles\", GetAllWindowHandles.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/dismiss_alert\", DismissAlert.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/accept_alert\", AcceptAlert.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/alert_text\", GetAlertText.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/alert_text\", SetAlertText.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/url\", ChangeUrl.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/url\", GetCurrentUrl.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/forward\", GoForward.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/back\", GoBack.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/refresh\", RefreshPage.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/execute\", ExecuteScript.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/execute_async\", ExecuteAsyncScript.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/source\", GetPageSource.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/screenshot\", CaptureScreenshot.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/title\", GetTitle.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/element\", FindElement.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id\", DescribeElement.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/elements\", FindElements.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/element/active\", FindActiveElement.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/element/:id/element\", FindChildElement.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/element/:id/elements\", FindChildElements.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n\n    postMapper.bind(\"/session/:sessionId/element/:id/click\", ClickElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/text\", GetElementText.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/element/:id/submit\", SubmitElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/file\", UploadFile.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/element/:id/value\", SendKeys.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/value\", GetElementValue.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/name\", GetTagName.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    postMapper.bind(\"/session/:sessionId/element/:id/clear\", ClearElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/selected\", GetElementSelected.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/enabled\", GetElementEnabled.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/displayed\", GetElementDisplayed.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/location\", GetElementLocation.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/location_in_view\",\n                   GetElementLocationInView.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/size\", GetElementSize.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/css/:propertyName\", GetCssProperty.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/element/:id/attribute/:name\", GetElementAttribute.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/element/:id/equals/:other\", ElementEquality.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/cookie\", GetAllCookies.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/cookie\", AddCookie.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    deleteMapper.bind(\"/session/:sessionId/cookie\", DeleteCookie.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    deleteMapper.bind(\"/session/:sessionId/cookie/:name\", DeleteNamedCookie.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/frame\", SwitchToFrame.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/window\", SwitchToWindow.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    deleteMapper.bind(\"/session/:sessionId/window\", CloseWindow.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/session/:sessionId/window/:windowHandle/size\", GetWindowSize.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/window/:windowHandle/size\", SetWindowSize.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/window/:windowHandle/position\", GetWindowPosition.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/window/:windowHandle/position\", SetWindowPosition.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/window/:windowHandle/maximize\", MaximizeWindow.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/timeouts\", ConfigureTimeout.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/timeouts/implicit_wait\", ImplicitlyWait.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/timeouts/async_script\", SetScriptTimeout.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/execute_sql\", ExecuteSQL.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/location\", GetLocationContext.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/location\", SetLocationContext.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/session/:sessionId/application_cache/status\", GetAppCacheStatus.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/browser_connection\", SetBrowserConnection.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/browser_connection\", IsBrowserOnline.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/local_storage/key/:key\", GetLocalStorageItem.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    deleteMapper.bind(\"/session/:sessionId/local_storage/key/:key\", RemoveLocalStorageItem.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/local_storage\", GetLocalStorageKeys.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/local_storage\", SetLocalStorageItem.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    deleteMapper.bind(\"/session/:sessionId/local_storage\", ClearLocalStorage.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/local_storage/size\", GetLocalStorageSize.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/session_storage/key/:key\", GetSessionStorageItem.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    deleteMapper.bind(\"/session/:sessionId/session_storage/key/:key\", RemoveSessionStorageItem.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/session_storage\", GetSessionStorageKeys.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/session_storage\", SetSessionStorageItem.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    deleteMapper.bind(\"/session/:sessionId/session_storage\", ClearSessionStorage.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    getMapper.bind(\"/session/:sessionId/session_storage/size\", GetSessionStorageSize.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    getMapper.bind(\"/session/:sessionId/orientation\", GetScreenOrientation.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/orientation\", Rotate.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    postMapper.bind(\"/session/:sessionId/moveto\", MouseMoveToLocation.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/click\", ClickInSession.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/doubleclick\", DoubleClickInSession.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/buttondown\", MouseDown.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/buttonup\", MouseUp.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/keys\", SendKeyToActiveElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/session/:sessionId/ime/available_engines\", ImeGetAvailableEngines.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/ime/active_engine\", ImeGetActiveEngine.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    getMapper.bind(\"/session/:sessionId/ime/activated\", ImeIsActivated.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/ime/deactivate\", ImeDeactivate.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/ime/activate\", ImeActivateEngine.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n\n    // Advanced Touch API\n    postMapper.bind(\"/session/:sessionId/touch/click\", SingleTapOnElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/down\", Down.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/up\", Up.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/move\", Move.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/scroll\", Scroll.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/doubleclick\", DoubleTapOnElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/longclick\", LongPressOnElement.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n    postMapper.bind(\"/session/:sessionId/touch/flick\", Flick.class)\n        .on(ResultType.SUCCESS, emptyResponse);\n\n    getMapper.bind(\"/session/:sessionId/log/types\", GetAvailableLogTypesHandler.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/session/:sessionId/log\", GetLogHandler.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n    postMapper.bind(\"/logs\", GetSessionLogsHandler.class)\n        .on(ResultType.SUCCESS, jsonResponse);\n  }\n}\n", "idx": 1, "id": 10333, "msg": "You don't need to expose this method to do what you want. There are already public addNewGetMapping, addNewPostMapping, and addNewDeleteMapping methods.", "proj": "SeleniumHQ-selenium", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -364,7 +364,7 @@ class DictSet(VaspInputSet):\n             incar['NUPDOWN'] = nupdown\n \n         if self.structure._charge:\n-            incar[\"NELECT\"] = self.nelect + self.structure._charge\n+            incar[\"NELECT\"] = self.nelect\n \n         return incar\n ", "y": 1, "oldf": "# coding: utf-8\n# Copyright (c) Pymatgen Development Team.\n# Distributed under the terms of the MIT License.\n\nfrom __future__ import division, unicode_literals, print_function\n\nimport abc\nimport re\nimport os\nimport glob\nimport shutil\nimport warnings\nfrom itertools import chain\nfrom copy import deepcopy\n\nimport six\nimport numpy as np\n\nfrom monty.serialization import loadfn\nfrom monty.io import zopen\n\nfrom pymatgen.core.periodic_table import Specie\nfrom pymatgen.core.structure import Structure\nfrom pymatgen.io.vasp.inputs import Incar, Poscar, Potcar, Kpoints\nfrom pymatgen.io.vasp.outputs import Vasprun, Outcar\nfrom monty.json import MSONable\nfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzer\nfrom pymatgen.symmetry.bandstructure import HighSymmKpath\nfrom pymatgen.analysis.structure_matcher import StructureMatcher\nfrom pymatgen.core.sites import PeriodicSite\n\n\"\"\"\nThis module defines the VaspInputSet abstract base class and a concrete\nimplementation for the parameters developed and tested by the core team\nof pymatgen, including the Materials Virtual Lab, Materials Project and the MIT\nhigh throughput project.  The basic concept behind an input set is to specify\na scheme to generate a consistent set of VASP inputs from a structure\nwithout further user intervention. This ensures comparability across\nruns.\n\nRead the following carefully before implementing new input sets:\n\n1. 99% of what needs to be done can be done by specifying user_incar_settings\n   to override some of the defaults of various input sets. Unless there is an\n   extremely good reason to add a new set, DO NOT add one. E.g., if you want\n   to turn the hubbard U off, just set \"LDAU\": False as a user_incar_setting.\n2. All derivative input sets should inherit from one of the usual MPRelaxSet or\n   MITRelaxSet, and proper superclass delegation should be used where possible.\n   In particular, you are not supposed to implement your own as_dict or\n   from_dict for derivative sets unless you know what you are doing.\n   Improper overriding the as_dict and from_dict protocols is the major\n   cause of implementation headaches. If you need an example, look at how the\n   MPStaticSet or MPNonSCFSets are constructed.\n\nThe above are recommendations. The following are UNBREAKABLE rules:\n1. All input sets must take in a structure or list of structures as the first\n   argument.\n2. user_incar_settings and user_kpoints_settings are absolute. Any new sets you\n   implement must obey this. If a user wants to override your settings,\n   you assume he knows what he is doing. Do not magically override user\n   supplied settings. You can issue a warning if you think the user is wrong.\n3. All input sets must save all supplied args and kwargs as instance variables.\n   E.g., self.my_arg = my_arg and self.kwargs = kwargs in the __init__. This\n   ensures the as_dict and from_dict work correctly.\n\"\"\"\n\n__author__ = \"Shyue Ping Ong, Wei Chen, Will Richards, Geoffroy Hautier, Anubhav Jain\"\n__copyright__ = \"Copyright 2011, The Materials Project\"\n__version__ = \"1.0\"\n__maintainer__ = \"Shyue Ping Ong\"\n__email__ = \"shyuep@gmail.com\"\n__date__ = \"May 28 2016\"\n\nMODULE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass VaspInputSet(six.with_metaclass(abc.ABCMeta, MSONable)):\n    \"\"\"\n    Base class representing a set of Vasp input parameters with a structure\n    supplied as init parameters. Typically, you should not inherit from this\n    class. Start from DictSet or MPRelaxSet or MITRelaxSet.\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def incar(self):\n        \"\"\"Incar object\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def kpoints(self):\n        \"\"\"Kpoints object\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def poscar(self):\n        \"\"\"Poscar object\"\"\"\n        pass\n\n    @property\n    def potcar_symbols(self):\n        \"\"\"\n        List of POTCAR symbols.\n        \"\"\"\n        elements = self.poscar.site_symbols\n        potcar_symbols = []\n        settings = self._config_dict[\"POTCAR\"]\n\n        if isinstance(settings[elements[-1]], dict):\n            for el in elements:\n                potcar_symbols.append(settings[el]['symbol']\n                                      if el in settings else el)\n        else:\n            for el in elements:\n                potcar_symbols.append(settings.get(el, el))\n\n        return potcar_symbols\n\n    @property\n    def potcar(self):\n        \"\"\"\n        Potcar object.\n        \"\"\"\n        return Potcar(self.potcar_symbols, functional=self.potcar_functional)\n\n    @property\n    def all_input(self):\n        \"\"\"\n        Returns all input files as a dict of {filename: vasp object}\n\n        Returns:\n            dict of {filename: object}, e.g., {'INCAR': Incar object, ...}\n        \"\"\"\n        kpoints = self.kpoints\n        incar = self.incar\n        if np.product(kpoints.kpts) < 4 and incar.get(\"ISMEAR\", 0) == -5:\n            incar[\"ISMEAR\"] = 0\n\n        return {'INCAR': incar,\n                'KPOINTS': kpoints,\n                'POSCAR': self.poscar,\n                'POTCAR': self.potcar}\n\n    def write_input(self, output_dir,\n                    make_dir_if_not_present=True, include_cif=False):\n        \"\"\"\n        Writes a set of VASP input to a directory.\n\n        Args:\n            output_dir (str): Directory to output the VASP input files\n            make_dir_if_not_present (bool): Set to True if you want the\n                directory (and the whole path) to be created if it is not\n                present.\n            include_cif (bool): Whether to write a CIF file in the output\n                directory for easier opening by VESTA.\n        \"\"\"\n        if make_dir_if_not_present and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        for k, v in self.all_input.items():\n            v.write_file(os.path.join(output_dir, k))\n        if include_cif:\n            s = self.all_input[\"POSCAR\"].structure\n            fname = os.path.join(output_dir, \"%s.cif\" % re.sub(r'\\s', \"\",\n                                                               s.formula))\n            s.to(filename=fname)\n\n    def as_dict(self, verbosity=2):\n        d = MSONable.as_dict(self)\n        if verbosity == 1:\n            d.pop(\"structure\", None)\n        return d\n\n\ndef _load_yaml_config(fname):\n    config = loadfn(os.path.join(MODULE_DIR, \"%s.yaml\" % fname))\n    config[\"INCAR\"].update(loadfn(os.path.join(MODULE_DIR,\n                                               \"VASPIncarBase.yaml\")))\n    return config\n\n\nclass DictSet(VaspInputSet):\n    \"\"\"\n    Concrete implementation of VaspInputSet that is initialized from a dict\n    settings. This allows arbitrary settings to be input. In general,\n    this is rarely used directly unless there is a source of settings in yaml\n    format (e.g., from a REST interface). It is typically used by other\n    VaspInputSets for initialization.\n\n    Special consideration should be paid to the way the MAGMOM initialization\n    for the INCAR is done. The initialization differs depending on the type of\n    structure and the configuration settings. The order in which the magmom is\n    determined is as follows:\n\n    1. If the site itself has a magmom setting, that is used.\n    2. If the species on the site has a spin setting, that is used.\n    3. If the species itself has a particular setting in the config file, that\n       is used, e.g., Mn3+ may have a different magmom than Mn4+.\n    4. Lastly, the element symbol itself is checked in the config file. If\n       there are no settings, VASP's default of 0.6 is used.\n\n    Args:\n        structure (Structure): The Structure to create inputs for.\n        config_dict (dict): The config dictionary to use.\n        files_to_transfer (dict): A dictionary of {filename: filepath}. This\n            allows the transfer of files from a previous calculation.\n        user_incar_settings (dict): User INCAR settings. This allows a user\n            to override INCAR settings, e.g., setting a different MAGMOM for\n            various elements or species. Note that in the new scheme,\n            ediff_per_atom and hubbard_u are no longer args. Instead, the\n            config_dict supports EDIFF_PER_ATOM and EDIFF keys. The former\n            scales with # of atoms, the latter does not. If both are\n            present, EDIFF is preferred. To force such settings, just supply\n            user_incar_settings={\"EDIFF\": 1e-5, \"LDAU\": False} for example.\n            The keys 'LDAUU', 'LDAUJ', 'LDAUL' are special cases since\n            pymatgen defines different values depending on what anions are\n            present in the structure, so these keys can be defined in one\n            of two ways, e.g. either {\"LDAUU\":{\"O\":{\"Fe\":5}}} to set LDAUU\n            for Fe to 5 in an oxide, or {\"LDAUU\":{\"Fe\":5}} to set LDAUU to\n            5 regardless of the input structure.\n        user_kpoints_settings (dict or Kpoints): Allow user to override kpoints \n            setting by supplying a dict E.g., {\"reciprocal_density\": 1000}. \n            User can also supply Kpoints object. Default is None.\n        user_potcar_settings (dict: Allow user to override POTCARs. E.g.,\n            {\"Gd\": \"Gd_3\"}. This is generally not recommended. Default is None.\n        constrain_total_magmom (bool): Whether to constrain the total magmom\n            (NUPDOWN in INCAR) to be the sum of the expected MAGMOM for all\n            species. Defaults to False.\n        sort_structure (bool): Whether to sort the structure (using the\n            default sort order of electronegativity) before generating input\n            files. Defaults to True, the behavior you would want most of the\n            time. This ensures that similar atomic species are grouped\n            together.\n        potcar_functional (str): Functional to use. Default (None) is to use\n            the functional in Potcar.DEFAULT_FUNCTIONAL. Valid values:\n            \"PBE\", \"LDA\", \"PW91\", \"LDA_US\"\n        force_gamma (bool): Force gamma centered kpoint generation. Default\n            (False) is to use the Automatic Density kpoint scheme, which\n            will use the Gamma centered generation scheme for hexagonal\n            cells, and Monkhorst-Pack otherwise.\n        reduce_structure (None/str): Before generating the input files,\n            generate the reduced structure. Default (None), does not\n            alter the structure. Valid values: None, \"niggli\", \"LLL\".\n        vdw: Adds default parameters for van-der-Waals functionals supported\n            by VASP to INCAR. Supported functionals are: DFT-D2, undamped\n            DFT-D3, DFT-D3 with Becke-Jonson damping, Tkatchenko-Scheffler,\n            Tkatchenko-Scheffler with iterative Hirshfeld partitioning,\n            MBD@rSC, dDsC, Dion's vdW-DF, DF2, optPBE, optB88, optB86b and\n            rVV10.\n    \"\"\"\n\n    def __init__(self, structure, config_dict,\n                 files_to_transfer=None, user_incar_settings=None,\n                 user_kpoints_settings=None, user_potcar_settings=None,\n                 constrain_total_magmom=False, sort_structure=True,\n                 potcar_functional=\"PBE\", force_gamma=False,\n                 reduce_structure=None, vdw=None):\n        if reduce_structure:\n            structure = structure.get_reduced_structure(reduce_structure)\n        if sort_structure:\n            structure = structure.get_sorted_structure()\n        self.structure = structure\n        self._config_dict = deepcopy(config_dict)\n        self.files_to_transfer = files_to_transfer or {}\n        self.constrain_total_magmom = constrain_total_magmom\n        self.sort_structure = sort_structure\n        self.potcar_functional = potcar_functional\n        self.force_gamma = force_gamma\n        self.reduce_structure = reduce_structure\n        self.user_incar_settings = user_incar_settings or {}\n        self.user_kpoints_settings = user_kpoints_settings\n        self.user_potcar_settings = user_potcar_settings\n        self.vdw = vdw.lower() if vdw is not None else None\n        if self.vdw:\n            vdw_par = loadfn(os.path.join(MODULE_DIR, \"vdW_parameters.yaml\"))\n            try:\n                self._config_dict[\"INCAR\"].update(vdw_par[self.vdw])\n            except KeyError:\n                raise KeyError(\"Invalid or unsupported van-der-Waals \"\n                               \"functional. Supported functionals are \"\n                               \"%s.\" % vdw_par.keys())\n        if self.user_potcar_settings:\n            warnings.warn(\n                \"Overriding POTCARs is generally not recommended as it \"\n                \"significantly affect the results of calculations and \"\n                \"compatibility with other calculations done with the same \"\n                \"input set. In many instances, it is better to write a \"\n                \"subclass of a desired input set and override the POTCAR in \"\n                \"the subclass to be explicit on the differences.\")\n            for k, v in self.user_potcar_settings.items():\n                self._config_dict[\"POTCAR\"][k] = v\n\n    @property\n    def incar(self):\n        settings = dict(self._config_dict[\"INCAR\"])\n        settings.update(self.user_incar_settings)\n        structure = self.structure\n        incar = Incar()\n        comp = structure.composition\n        elements = sorted([el for el in comp.elements if comp[el] > 0],\n                          key=lambda e: e.X)\n        most_electroneg = elements[-1].symbol\n        poscar = Poscar(structure)\n        hubbard_u = settings.get(\"LDAU\", False)\n\n        for k, v in settings.items():\n            if k == \"MAGMOM\":\n                mag = []\n                for site in structure:\n                    if hasattr(site, 'magmom'):\n                        mag.append(site.magmom)\n                    elif hasattr(site.specie, 'spin'):\n                        mag.append(site.specie.spin)\n                    elif str(site.specie) in v:\n                        mag.append(v.get(str(site.specie)))\n                    else:\n                        mag.append(v.get(site.specie.symbol, 0.6))\n                incar[k] = mag\n            elif k in ('LDAUU', 'LDAUJ', 'LDAUL'):\n                if hubbard_u:\n                    if hasattr(structure[0], k.lower()):\n                        m = dict([(site.specie.symbol, getattr(site, k.lower()))\n                                  for site in structure])\n                        incar[k] = [m[sym] for sym in poscar.site_symbols]\n                    # lookup specific LDAU if specified for most_electroneg atom\n                    elif most_electroneg in v.keys() and \\\n                            isinstance(v[most_electroneg], dict):\n                        incar[k] = [v[most_electroneg].get(sym, 0)\n                                    for sym in poscar.site_symbols]\n                    # else, use fallback LDAU value if it exists\n                    else:\n                        incar[k] = [v.get(sym, 0)\n                                    if isinstance(v.get(sym, 0), (float, int))\n                                    else 0 for sym in poscar.site_symbols]\n            elif k.startswith(\"EDIFF\") and k != \"EDIFFG\":\n                if \"EDIFF\" not in settings and k == \"EDIFF_PER_ATOM\":\n                    incar[\"EDIFF\"] = float(v) * structure.num_sites\n                else:\n                    incar[\"EDIFF\"] = float(settings[\"EDIFF\"])\n            else:\n                incar[k] = v\n\n        has_u = hubbard_u and sum(incar['LDAUU']) > 0\n        if has_u:\n            # modify LMAXMIX if LSDA+U and you have d or f electrons\n            # note that if the user explicitly sets LMAXMIX in settings it will\n            # override this logic.\n            if 'LMAXMIX' not in settings.keys():\n                # contains f-electrons\n                if any([el.Z > 56 for el in structure.composition]):\n                    incar['LMAXMIX'] = 6\n                # contains d-electrons\n                elif any([el.Z > 20 for el in structure.composition]):\n                    incar['LMAXMIX'] = 4\n        else:\n            for key in list(incar.keys()):\n                if key.startswith('LDAU'):\n                    del incar[key]\n\n        if self.constrain_total_magmom:\n            nupdown = sum([mag if abs(mag) > 0.6 else 0\n                           for mag in incar['MAGMOM']])\n            incar['NUPDOWN'] = nupdown\n\n        if self.structure._charge:\n            incar[\"NELECT\"] = self.nelect + self.structure._charge\n\n        return incar\n\n    @property\n    def poscar(self):\n        return Poscar(self.structure)\n\n    @property\n    def nelect(self):\n        \"\"\"\n        Gets the default number of electrons for a given structure.\n        \"\"\"\n        return int(round(\n            sum([self.structure.composition.element_composition[ps.element]\n                 * ps.ZVAL\n                 for ps in self.potcar])))\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        Writes out a KPOINTS file using the fully automated grid method. Uses\n        Gamma centered meshes for hexagonal cells and Monk grids otherwise.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n        \"\"\"\n        settings = self.user_kpoints_settings or self._config_dict[\"KPOINTS\"]\n\n        if isinstance(settings, Kpoints):\n            return settings\n\n        # If grid_density is in the kpoints_settings use\n        # Kpoints.automatic_density\n        if settings.get('grid_density'):\n            return Kpoints.automatic_density(\n                self.structure, int(settings['grid_density']),\n                self.force_gamma)\n\n        # If reciprocal_density is in the kpoints_settings use\n        # Kpoints.automatic_density_by_vol\n        elif settings.get('reciprocal_density'):\n            return Kpoints.automatic_density_by_vol(\n                self.structure, int(settings['reciprocal_density']),\n                self.force_gamma)\n\n        # If length is in the kpoints_settings use Kpoints.automatic\n        elif settings.get('length'):\n            return Kpoints.automatic(settings['length'])\n\n        # Raise error. Unsure of which kpoint generation to use\n        else:\n            raise ValueError(\n                \"Invalid KPoint Generation algo : Supported Keys are \"\n                \"grid_density: for Kpoints.automatic_density generation, \"\n                \"reciprocal_density: for KPoints.automatic_density_by_vol \"\n                \"generation, and length  : for Kpoints.automatic generation\")\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    def write_input(self, output_dir,\n                    make_dir_if_not_present=True, include_cif=False):\n        super(DictSet, self).write_input(\n            output_dir=output_dir,\n            make_dir_if_not_present=make_dir_if_not_present,\n            include_cif=include_cif)\n        for k, v in self.files_to_transfer.items():\n            with zopen(v, \"rb\") as fin, \\\n                    zopen(os.path.join(output_dir, k), \"wb\") as fout:\n                shutil.copyfileobj(fin, fout)\n\n\nclass MITRelaxSet(DictSet):\n    \"\"\"\n    Standard implementation of VaspInputSet utilizing parameters in the MIT\n    High-throughput project.\n    The parameters are chosen specifically for a high-throughput project,\n    which means in general pseudopotentials with fewer electrons were chosen.\n\n    Please refer::\n\n        A Jain, G. Hautier, C. Moore, S. P. Ong, C. Fischer, T. Mueller,\n        K. A. Persson, G. Ceder. A high-throughput infrastructure for density\n        functional theory calculations. Computational Materials Science,\n        2011, 50(8), 2295-2310. doi:10.1016/j.commatsci.2011.02.023\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MITRelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        super(MITRelaxSet, self).__init__(\n            structure, MITRelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPRelaxSet(DictSet):\n    \"\"\"\n    Implementation of VaspInputSet utilizing parameters in the public\n    Materials Project. Typically, the pseudopotentials chosen contain more\n    electrons than the MIT parameters, and the k-point grid is ~50% more dense.\n    The LDAUU parameters are also different due to the different psps used,\n    which result in different fitted values.\n    \"\"\"\n    CONFIG = CONFIG = _load_yaml_config(\"MPRelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        super(MPRelaxSet, self).__init__(\n            structure, MPRelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPHSERelaxSet(DictSet):\n    \"\"\"\n    Same as the MPRelaxSet, but with HSE parameters.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MPHSERelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        super(MPHSERelaxSet, self).__init__(\n            structure, MPHSERelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPStaticSet(MPRelaxSet):\n    def __init__(self, structure, prev_incar=None, prev_kpoints=None,\n                 lepsilon=False, lcalcpol=False, reciprocal_density=100,\n                 **kwargs):\n        \"\"\"\n        Run a static calculation.\n\n        Args:\n            structure (Structure): Structure from previous run.\n            prev_incar (Incar): Incar file from previous run.\n            prev_kpoints (Kpoints): Kpoints from previous run.\n            lepsilon (bool): Whether to add static dielectric calculation\n            reciprocal_density (int): For static calculations,\n                we usually set the reciprocal density by volume. This is a\n                convenience arg to change that, rather than using\n                user_kpoints_settings. Defaults to 100, which is ~50% more than\n                that of standard relaxation calculations.\n            \\\\*\\\\*kwargs: kwargs supported by MPRelaxSet.\n        \"\"\"\n        super(MPStaticSet, self).__init__(structure, **kwargs)\n        if isinstance(prev_incar, six.string_types):\n            prev_incar = Incar.from_file(prev_incar)\n        if isinstance(prev_kpoints, six.string_types):\n            prev_kpoints = Kpoints.from_file(prev_kpoints)\n\n        self.prev_incar = prev_incar\n        self.prev_kpoints = prev_kpoints\n        self.reciprocal_density = reciprocal_density\n        self.structure = structure\n        self.kwargs = kwargs\n        self.lepsilon = lepsilon\n        self.lcalcpol = lcalcpol\n\n    @property\n    def incar(self):\n        parent_incar = super(MPStaticSet, self).incar\n        incar = Incar(self.prev_incar) if self.prev_incar is not None else \\\n            Incar(parent_incar)\n\n        incar.update(\n            {\"IBRION\": -1, \"ISMEAR\": -5, \"LAECHG\": True, \"LCHARG\": True,\n             \"LORBIT\": 11, \"LVHAR\": True, \"LWAVE\": False, \"NSW\": 0,\n             \"ICHARG\": 0, \"ALGO\": \"Normal\"})\n\n        if self.lepsilon:\n            incar[\"IBRION\"] = 8\n            incar[\"LEPSILON\"] = True\n\n            # LPEAD=T: numerical evaluation of overlap integral prevents\n            # LRF_COMMUTATOR errors and can lead to better expt. agreement\n            # but produces slightly different results\n            incar[\"LPEAD\"] = True\n\n            # Note that DFPT calculations MUST unset NSW. NSW = 0 will fail\n            # to output ionic.\n            incar.pop(\"NSW\", None)\n            incar.pop(\"NPAR\", None)\n\n        if self.lcalcpol:\n            incar[\"LCALCPOL\"] = True\n\n        for k in [\"MAGMOM\", \"NUPDOWN\"] + list(self.kwargs.get(\n                \"user_incar_settings\", {}).keys()):\n            # For these parameters as well as user specified settings, override\n            # the incar settings.\n            if parent_incar.get(k, None) is not None:\n                incar[k] = parent_incar[k]\n            else:\n                incar.pop(k, None)\n\n        # use new LDAUU when possible b/c the Poscar might have changed\n        # representation\n        if incar.get('LDAU'):\n            u = incar.get('LDAUU', [])\n            j = incar.get('LDAUJ', [])\n            if sum([u[x] - j[x] for x, y in enumerate(u)]) > 0:\n                for tag in ('LDAUU', 'LDAUL', 'LDAUJ'):\n                    incar.update({tag: parent_incar[tag]})\n            # ensure to have LMAXMIX for GGA+U static run\n            if \"LMAXMIX\" not in incar:\n                incar.update({\"LMAXMIX\": parent_incar[\"LMAXMIX\"]})\n\n        # Compare ediff between previous and staticinputset values,\n        # choose the tighter ediff\n        incar[\"EDIFF\"] = min(incar.get(\"EDIFF\", 1), parent_incar[\"EDIFF\"])\n        return incar\n\n    @property\n    def kpoints(self):\n        self._config_dict[\"KPOINTS\"][\"reciprocal_density\"] = \\\n            self.reciprocal_density\n        kpoints = super(MPStaticSet, self).kpoints\n        # Prefer to use k-point scheme from previous run\n        if self.prev_kpoints and self.prev_kpoints.style != kpoints.style:\n            if self.prev_kpoints.style == Kpoints.supported_modes.Monkhorst:\n                k_div = [kp + 1 if kp % 2 == 1 else kp\n                         for kp in kpoints.kpts[0]]\n                kpoints = Kpoints.monkhorst_automatic(k_div)\n            else:\n                kpoints = Kpoints.gamma_automatic(kpoints.kpts[0])\n        return kpoints\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, standardize=False, sym_prec=0.1,\n                       international_monoclinic=True, reciprocal_density=100,\n                       small_gap_multiply=None, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for static calculations from a\n        directory of previous Vasp run.\n\n        Args:\n            prev_calc_dir (str): Directory containing the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            standardize (float): Whether to standardize to a primitive\n                standard cell. Defaults to False.\n            sym_prec (float): Tolerance for symmetry finding. If not 0,\n                the final structure from the previous run will be symmetrized\n                to get a primitive standard cell. Set to 0 if you don't want\n                that.\n            international_monoclinic (bool): Whether to use international\n                    convention (vs Curtarolo) for monoclinic. Defaults True.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                                    volume (defaults to 100)\n            small_gap_multiply ([float, float]): If the gap is less than\n                1st index, multiply the default reciprocal_density by the 2nd\n                index.\n            \\\\*\\\\*kwargs: All kwargs supported by MPStaticSet,\n                other than prev_incar and prev_structure and prev_kpoints which\n                are determined from the prev_calc_dir.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        prev_incar = vasprun.incar\n        prev_kpoints = vasprun.kpoints\n\n        # We will make a standard structure for the given symprec.\n        prev_structure = get_structure_from_prev_run(\n            vasprun, outcar, sym_prec=standardize and sym_prec,\n            international_monoclinic=international_monoclinic)\n\n        # multiply the reciprocal density if needed:\n        if small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= small_gap_multiply[0]:\n                reciprocal_density = reciprocal_density * small_gap_multiply[1]\n\n        return cls(\n            structure=prev_structure, prev_incar=prev_incar,\n            prev_kpoints=prev_kpoints,\n            reciprocal_density=reciprocal_density, **kwargs)\n\n\nclass MPHSEBSSet(MPHSERelaxSet):\n    def __init__(self, structure, user_incar_settings=None, added_kpoints=None,\n                 mode=\"Uniform\", reciprocal_density=None,\n                 kpoints_line_density=20, **kwargs):\n        \"\"\"\n        Implementation of a VaspInputSet for HSE band structure computations.\n        Remember that HSE band structures must be self-consistent in VASP. A\n        band structure along symmetry lines for instance needs BOTH a uniform\n        grid with appropriate weights AND a path along the lines with weight 0.\n\n        Thus, the \"Uniform\" mode is just like regular static SCF but allows\n        adding custom kpoints (e.g., corresponding to known VBM/CBM) to the\n        uniform grid that have zero weight (e.g., for better gap estimate).\n\n        The \"Line\" mode is just like Uniform mode, but additionally adds\n        k-points along symmetry lines with zero weight.\n\n        Args:\n            structure (Structure): Structure to compute\n            user_incar_settings (dict): A dict specifying additional incar\n                settings\n            added_kpoints (list): a list of kpoints (list of 3 number list)\n                added to the run. The k-points are in fractional coordinates\n            mode (str): \"Line\" - generate k-points along symmetry lines for\n                bandstructure. \"Uniform\" - generate uniform k-points grid\n            reciprocal_density (int): k-point density to use for uniform mesh\n            kpoints_line_density (int): k-point density for high symmetry lines\n            **kwargs (dict): Any other parameters to pass into DictVaspInputSet\n\n        \"\"\"\n        super(MPHSEBSSet, self).__init__(structure, **kwargs)\n        self.structure = structure\n        self.user_incar_settings = user_incar_settings or {}\n        self._config_dict[\"INCAR\"].update({\n            \"NSW\": 0,\n            \"ISMEAR\": 0,\n            \"SIGMA\": 0.05,\n            \"ISYM\": 3,\n            \"LCHARG\": False,\n            \"NELMIN\": 5\n        })\n        self.added_kpoints = added_kpoints if added_kpoints is not None else []\n        self.mode = mode\n        self.reciprocal_density = reciprocal_density or \\\n                                  self.kpoints_settings['reciprocal_density']\n        self.kpoints_line_density = kpoints_line_density\n\n    @property\n    def kpoints(self):\n        kpts = []\n        weights = []\n        all_labels = []\n\n        # for both modes, include the Uniform mesh w/standard weights\n        grid = Kpoints.automatic_density_by_vol(self.structure,\n                                                self.reciprocal_density).kpts\n        ir_kpts = SpacegroupAnalyzer(self.structure, symprec=0.1) \\\n            .get_ir_reciprocal_mesh(grid[0])\n        for k in ir_kpts:\n            kpts.append(k[0])\n            weights.append(int(k[1]))\n            all_labels.append(None)\n\n        # for both modes, include any user-added kpoints w/zero weight\n        for k in self.added_kpoints:\n            kpts.append(k)\n            weights.append(0.0)\n            all_labels.append(\"user-defined\")\n\n        # for line mode only, add the symmetry lines w/zero weight\n        if self.mode.lower() == \"line\":\n            kpath = HighSymmKpath(self.structure)\n            frac_k_points, labels = kpath.get_kpoints(\n                line_density=self.kpoints_line_density,\n                coords_are_cartesian=False)\n\n            for k in range(len(frac_k_points)):\n                kpts.append(frac_k_points[k])\n                weights.append(0.0)\n                all_labels.append(labels[k])\n\n        comment = (\"HSE run along symmetry lines\"\n                   if self.mode.lower() == \"line\"\n                   else \"HSE run on uniform grid\")\n\n        return Kpoints(comment=comment,\n                       style=Kpoints.supported_modes.Reciprocal,\n                       num_kpts=len(kpts), kpts=kpts, kpts_weights=weights,\n                       labels=all_labels)\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, mode=\"gap\",\n                       reciprocal_density=50, copy_chgcar=True, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for HSE calculations from a\n        directory of previous Vasp run. if mode==\"gap\", it explicitly adds VBM\n        and CBM of the prev run to the k-point list of this run.\n\n        Args:\n            prev_calc_dir (str): Directory containing the outputs\n                (vasprun.xml and OUTCAR) of previous vasp run.\n            mode (str): Either \"uniform\", \"gap\" or \"line\"\n            reciprocal_density (int): density of k-mesh\n            copy_chgcar (bool): whether to copy CHGCAR of previous run\n            \\\\*\\\\*kwargs: All kwargs supported by MPHSEBSStaticSet,\n                other than prev_structure which is determined from the previous\n                calc dir.\n        \"\"\"\n\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        # note: don't standardize the cell because we want to retain k-points\n        prev_structure = get_structure_from_prev_run(vasprun, outcar,\n                                                     sym_prec=0)\n\n        added_kpoints = []\n\n        if mode.lower() == \"gap\":\n            bs = vasprun.get_band_structure()\n            vbm, cbm = bs.get_vbm()[\"kpoint\"], bs.get_cbm()[\"kpoint\"]\n            if vbm:\n                added_kpoints.append(vbm.frac_coords)\n            if cbm:\n                added_kpoints.append(cbm.frac_coords)\n\n        files_to_transfer = {}\n        if copy_chgcar:\n            chgcars = sorted(glob.glob(os.path.join(prev_calc_dir, \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        return cls(\n            structure=prev_structure,\n            added_kpoints=added_kpoints, reciprocal_density=reciprocal_density,\n            mode=mode, files_to_transfer=files_to_transfer, **kwargs)\n\n\nclass MPNonSCFSet(MPRelaxSet):\n    def __init__(self, structure, prev_incar=None,\n                 mode=\"line\", nedos=601, reciprocal_density=100, sym_prec=0.1,\n                 kpoints_line_density=20, optics=False, **kwargs):\n        \"\"\"\n        Init a MPNonSCFSet. Typically, you would use the classmethod\n        from_prev_calc to initialize from a previous SCF run.\n\n        Args:\n            structure (Structure): Structure to compute\n            prev_incar (Incar/string): Incar file from previous run.\n            mode (str): Line or Uniform mode supported.\n            nedos (int): nedos parameter. Default to 601.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                                    volume (defaults to 100)\n            sym_prec (float): Symmetry precision (for Uniform mode).\n            kpoints_line_density (int): Line density for Line mode.\n            optics (bool): whether to add dielectric function\n            \\\\*\\\\*kwargs: kwargs supported by MPVaspInputSet.\n        \"\"\"\n        super(MPNonSCFSet, self).__init__(structure, **kwargs)\n        if isinstance(prev_incar, six.string_types):\n            prev_incar = Incar.from_file(prev_incar)\n        self.prev_incar = prev_incar\n        self.kwargs = kwargs\n        self.nedos = nedos\n        self.reciprocal_density = reciprocal_density\n        self.sym_prec = sym_prec\n        self.kpoints_line_density = kpoints_line_density\n        self.optics = optics\n        self.mode = mode.lower()\n\n        if self.mode.lower() not in [\"line\", \"uniform\"]:\n            raise ValueError(\"Supported modes for NonSCF runs are 'Line' and \"\n                             \"'Uniform'!\")\n        if (self.mode.lower() != \"uniform\" or nedos < 2000) and optics:\n            warnings.warn(\"It is recommended to use Uniform mode with a high \"\n                          \"NEDOS for optics calculations.\")\n\n    @property\n    def incar(self):\n        incar = super(MPNonSCFSet, self).incar\n        if self.prev_incar is not None:\n            incar.update({k: v for k, v in self.prev_incar.items()})\n\n        # Overwrite necessary INCAR parameters from previous runs\n        incar.update({\"IBRION\": -1, \"ISMEAR\": 0, \"SIGMA\": 0.001,\n                      \"LCHARG\": False, \"LORBIT\": 11, \"LWAVE\": False,\n                      \"NSW\": 0, \"ISYM\": 0, \"ICHARG\": 11})\n\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        if self.mode.lower() == \"uniform\":\n            # Set smaller steps for DOS output\n            incar[\"NEDOS\"] = self.nedos\n\n        if self.optics:\n            incar[\"LOPTICS\"] = True\n\n        incar.pop(\"MAGMOM\", None)\n\n        return incar\n\n    @property\n    def kpoints(self):\n        if self.mode == \"line\":\n            kpath = HighSymmKpath(self.structure)\n            frac_k_points, k_points_labels = kpath.get_kpoints(\n                line_density=self.kpoints_line_density,\n                coords_are_cartesian=False)\n            kpoints = Kpoints(\n                comment=\"Non SCF run along symmetry lines\",\n                style=Kpoints.supported_modes.Reciprocal,\n                num_kpts=len(frac_k_points),\n                kpts=frac_k_points, labels=k_points_labels,\n                kpts_weights=[1] * len(frac_k_points))\n        else:\n            kpoints = Kpoints.automatic_density_by_vol(self.structure,\n                                                       self.reciprocal_density)\n            mesh = kpoints.kpts[0]\n            ir_kpts = SpacegroupAnalyzer(\n                self.structure,\n                symprec=self.sym_prec).get_ir_reciprocal_mesh(mesh)\n            kpts = []\n            weights = []\n            for k in ir_kpts:\n                kpts.append(k[0])\n                weights.append(int(k[1]))\n            kpoints = Kpoints(comment=\"Non SCF run on uniform grid\",\n                              style=Kpoints.supported_modes.Reciprocal,\n                              num_kpts=len(ir_kpts),\n                              kpts=kpts, kpts_weights=weights)\n        return kpoints\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, copy_chgcar=True,\n                       nbands_factor=1.2, standardize=False, sym_prec=0.1,\n                       international_monoclinic=True, reciprocal_density=100,\n                       kpoints_line_density=20, small_gap_multiply=None,\n                       **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for NonSCF calculations from a\n        directory of previous static Vasp run.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            copy_chgcar: Whether to copy the old CHGCAR. Defaults to True.\n            nbands_factor (float): Multiplicative factor for NBANDS. Choose a\n                higher number if you are doing an LOPTICS calculation.\n            standardize (float): Whether to standardize to a primitive\n                standard cell. Defaults to False.\n            sym_prec (float): Tolerance for symmetry finding. If not 0,\n                the final structure from the previous run will be symmetrized\n                to get a primitive standard cell. Set to 0 if you don't want\n                that.\n            international_monoclinic (bool): Whether to use international\n                convention (vs Curtarolo) for monoclinic. Defaults True.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                volume in uniform mode (defaults to 100)\n            kpoints_line_density (int): density of k-mesh in line mode\n                (defaults to 20)\n            small_gap_multiply ([float, float]): If the gap is less than\n                1st index, multiply the default reciprocal_density by the 2nd\n                index.\n            \\\\*\\\\*kwargs: All kwargs supported by MPNonSCFSet,\n                other than structure, prev_incar and prev_chgcar which\n                are determined from the prev_calc_dir.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        incar = vasprun.incar\n        # Get a Magmom-decorated structure\n        structure = get_structure_from_prev_run(\n            vasprun, outcar, sym_prec=standardize and sym_prec,\n            international_monoclinic=international_monoclinic)\n        # Turn off spin when magmom for every site is smaller than 0.02.\n        if outcar and outcar.magnetization:\n            site_magmom = np.array([i['tot'] for i in outcar.magnetization])\n            ispin = 2 if np.any(site_magmom[np.abs(site_magmom) > 0.02]) else 1\n        elif vasprun.is_spin:\n            ispin = 2\n        else:\n            ispin = 1\n        nbands = int(np.ceil(vasprun.parameters[\"NBANDS\"] * nbands_factor))\n        incar.update({\"ISPIN\": ispin, \"NBANDS\": nbands})\n\n        files_to_transfer = {}\n        if copy_chgcar:\n            chgcars = sorted(glob.glob(os.path.join(prev_calc_dir, \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        # multiply the reciprocal density if needed:\n        if small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= small_gap_multiply[0]:\n                reciprocal_density = reciprocal_density * small_gap_multiply[1]\n                kpoints_line_density = kpoints_line_density * \\\n                                       small_gap_multiply[1]\n\n        return cls(structure=structure, prev_incar=incar,\n                           reciprocal_density=reciprocal_density,\n                           kpoints_line_density=kpoints_line_density,\n                           files_to_transfer=files_to_transfer, **kwargs)\n\n\nclass MPSOCSet(MPStaticSet):\n    def __init__(self, structure, saxis=(0, 0, 1), prev_incar=None,\n                 reciprocal_density=100, **kwargs):\n        \"\"\"\n        Init a MPSOCSet.\n\n        Args:\n            structure (Structure): the structure must have the 'magmom' site\n                property and each magnetic moment value must have 3\n                components. eg:- magmom = [[0,0,2], ...]\n            saxis (tuple): magnetic moment orientation\n            prev_incar (Incar): Incar file from previous run.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                                    volume (defaults to 100)\n            \\\\*\\\\*kwargs: kwargs supported by MPVaspInputSet.\n        \"\"\"\n        if not hasattr(structure[0], \"magmom\") and \\\n                not isinstance(structure[0].magmom, list):\n            raise ValueError(\n                \"The structure must have the 'magmom' site \"\n                \"property and each magnetic moment value must have 3 \"\n                \"components. eg:- magmom = [0,0,2]\")\n        self.saxis = saxis\n        super(MPSOCSet, self).__init__(\n            structure, prev_incar=prev_incar,\n            reciprocal_density=reciprocal_density, **kwargs)\n\n    @property\n    def incar(self):\n        incar = super(MPSOCSet, self).incar\n        if self.prev_incar is not None:\n            incar.update({k: v for k, v in self.prev_incar.items()})\n\n        # Overwrite necessary INCAR parameters from previous runs\n        incar.update({\"ISYM\": -1, \"LSORBIT\": \"T\", \"ICHARG\": 11,\n                      \"SAXIS\": list(self.saxis)})\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, copy_chgcar=True,\n                       nbands_factor=1.2, standardize=False, sym_prec=0.1,\n                       international_monoclinic=True, reciprocal_density=100,\n                       small_gap_multiply=None, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for SOC calculations from a\n        directory of previous static Vasp run. SOC calc requires all 3\n        components for MAGMOM for each atom in the structure.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            copy_chgcar: Whether to copy the old CHGCAR. Defaults to True.\n            nbands_factor (float): Multiplicative factor for NBANDS. Choose a\n                higher number if you are doing an LOPTICS calculation.\n            standardize (float): Whether to standardize to a primitive\n                standard cell. Defaults to False.\n            sym_prec (float): Tolerance for symmetry finding. If not 0,\n                the final structure from the previous run will be symmetrized\n                to get a primitive standard cell. Set to 0 if you don't want\n                that.\n            international_monoclinic (bool): Whether to use international\n                convention (vs Curtarolo) for monoclinic. Defaults True.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                volume (defaults to 100)\n            small_gap_multiply ([float, float]): If the gap is less than\n                1st index, multiply the default reciprocal_density by the 2nd\n                index.\n            \\\\*\\\\*kwargs: All kwargs supported by MPSOCSet,\n                other than structure, prev_incar and prev_chgcar which\n                are determined from the prev_calc_dir.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        incar = vasprun.incar\n        # Get a magmom-decorated structure\n        structure = get_structure_from_prev_run(\n            vasprun, outcar, sym_prec=standardize and sym_prec,\n            international_monoclinic=international_monoclinic)\n        # override magmom if provided\n        if kwargs.get(\"magmom\", None):\n            structure = structure.copy(\n                site_properties={\"magmom\": kwargs[\"magmom\"]})\n            kwargs.pop(\"magmom\", None)\n        # magmom has to be 3D for SOC calculation.\n        if hasattr(structure[0], \"magmom\"):\n            if not isinstance(structure[0].magmom, list):\n                structure = structure.copy(site_properties={\n                    \"magmom\": [[0, 0, site.magmom] for site in structure]})\n        else:\n            raise ValueError(\"Neither the previous structure has mamgom \"\n                             \"property nor magmom provided\")\n\n        nbands = int(np.ceil(vasprun.parameters[\"NBANDS\"] * nbands_factor))\n        incar.update({\"NBANDS\": nbands})\n\n        files_to_transfer = {}\n        if copy_chgcar:\n            chgcars = sorted(glob.glob(os.path.join(prev_calc_dir, \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        # multiply the reciprocal density if needed:\n        if small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= small_gap_multiply[0]:\n                reciprocal_density = reciprocal_density * small_gap_multiply[1]\n\n        return cls(structure, prev_incar=incar,\n                        files_to_transfer=files_to_transfer,\n                        reciprocal_density=reciprocal_density, **kwargs)\n\nclass MPNMRSet(MPStaticSet):\n\n    def __init__(self, structure, mode=\"cs\", isotopes=None,\n                 prev_incar=None, reciprocal_density=100, **kwargs):\n        \"\"\"\n        Init a MPNMRSet.\n\n        Args:\n            structure (Structure): Structure to compute\n            mode (str): The NMR calculation to run\n                            \"cs\": for Chemical Shift\n                            \"efg\" for Electric Field Gradient\n            isotopes (list): list of Isotopes for quadrupole moments\n            prev_incar (Incar): Incar file from previous run.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                                    volume (defaults to 100)\n            \\\\*\\\\*kwargs: kwargs supported by MPStaticSet.\n        \"\"\"\n        self.mode = mode\n        self.isotopes = isotopes if isotopes else []\n        super(MPNMRSet, self).__init__(\n            structure, prev_incar=prev_incar,\n            reciprocal_density=reciprocal_density, **kwargs)\n\n    @property\n    def incar(self):\n        incar = super(MPNMRSet, self).incar\n\n        if self.mode.lower() == \"cs\":\n            incar.update({\"LCHIMAG\": True,\n                          \"EDIFF\": -1.0e-10,\n                          \"ISYM\": 0,\n                          \"LCHARG\": False,\n                          \"LNMR_SYM_RED\": True,\n                          \"NELMIN\": 10,\n                          \"NSLPLINE\": True,\n                          \"PREC\": \"ACCURATE\",\n                          \"SIGMA\": 0.01})\n        elif self.mode.lower() == \"efg\":\n\n            isotopes = {ist.split(\"-\")[0]: ist for ist in self.isotopes}\n\n            quad_efg = [Specie(p).get_nmr_quadrupole_moment(isotopes.get(p, None)) for p in self.poscar.site_symbols]\n\n            incar.update({\"ALGO\": \"FAST\",\n                          \"EDIFF\": -1.0e-10,\n                          \"ISYM\": 0,\n                          \"LCHARG\": False,\n                          \"LEFG\": True,\n                          \"QUAD_EFG\": quad_efg,\n                          \"NELMIN\": 10,\n                          \"PREC\": \"ACCURATE\",\n                          \"SIGMA\": 0.01})\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n\nclass MVLElasticSet(MPRelaxSet):\n    \"\"\"\n    MVL denotes VASP input sets that are implemented by the Materials Virtual\n    Lab (http://www.materialsvirtuallab.org) for various research.\n\n    This input set is used to calculate elastic constants in VASP. It is used\n    in the following work::\n\n        Z. Deng, Z. Wang, I.-H. Chu, J. Luo, S. P. Ong.\n        \u201cElastic Properties of Alkali Superionic Conductor Electrolytes\n        from First Principles Calculations\u201d, J. Electrochem. Soc.\n        2016, 163(2), A67-A74. doi: 10.1149/2.0061602jes\n\n    To read the elastic constants, you may use the Outcar class which parses the\n    elastic constants.\n\n    Args:\n        scale (float): POTIM parameter. The default of 0.015 is usually fine,\n            but some structures may require a smaller step.\n        user_incar_settings (dict): A dict specifying additional incar\n            settings.\n    \"\"\"\n\n    def __init__(self, structure, potim=0.015, **kwargs):\n        super(MVLElasticSet, self).__init__(structure, **kwargs)\n        self._config_dict[\"INCAR\"].update({\"IBRION\": 6, \"NFREE\": 2,\n                                           \"POTIM\": potim})\n        self._config_dict[\"INCAR\"].pop(\"NPAR\", None)\n\n\nclass MVLGWSet(DictSet):\n    \"\"\"\n    MVL denotes VASP input sets that are implemented by the Materials Virtual\n    Lab (http://www.materialsvirtuallab.org) for various research. This is a\n    flexible input set for GW calculations.\n\n    Note that unlike all other input sets in this module, the PBE_54 series of\n    functional is set as the default. These have much improved performance for\n    GW calculations.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MVLGWSet\")\n\n    SUPPORTED_MODES = (\"DIAG\", \"GW\", \"STATIC\", \"BSE\")\n\n    def __init__(self, structure, prev_incar=None, nbands=None,\n                 potcar_functional=\"PBE_54\",\n                 reciprocal_density=100, mode=\"STATIC\", **kwargs):\n        \"\"\"\n        A typical sequence is mode=\"STATIC\" -> mode=\"DIAG\" -> mode=\"GW\" ->\n        mode=\"BSE\". For all steps other than the first one (static), the\n        recommendation is to use from_prev_calculation on the preceding run in\n        the series.\n\n        Args:\n            structure (Structure): Input structure.\n            prev_incar (Incar/string): Incar file from previous run.\n            mode (str): Supported modes are \"STATIC\" (default), \"DIAG\", \"GW\",\n                and \"BSE\".\n            nbands (int): For subsequent calculations, it is generally\n                recommended to perform NBANDS convergence starting from the\n                NBANDS of the previous run for DIAG, and to use the exact same\n                NBANDS for GW and BSE. This parameter is used by\n                from_previous_calculation to set nband.\n            potcar_functional (str): Defaults to \"PBE_54\".\n            \\\\*\\\\*kwargs: All kwargs supported by DictSet. Typically,\n                user_incar_settings is a commonly used option.\n        \"\"\"\n        super(MVLGWSet, self).__init__(\n            structure, MVLGWSet.CONFIG, **kwargs)\n        self.prev_incar = prev_incar\n        self.nbands = nbands\n        self.potcar_functional = potcar_functional\n        self.reciprocal_density = reciprocal_density\n        self.mode = mode.upper()\n        if self.mode not in MVLGWSet.SUPPORTED_MODES:\n            raise ValueError(\"%s not one of the support modes : %s\" %\n                             (self.mode, MVLGWSet.SUPPORTED_MODES))\n        self.kwargs = kwargs\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        Generate gamma center k-points mesh grid for GW calc,\n        which is requested by GW calculation.\n        \"\"\"\n        return Kpoints.automatic_density_by_vol(self.structure,\n                                                self.reciprocal_density,\n                                                force_gamma=True)\n\n    @property\n    def incar(self):\n        parent_incar = super(MVLGWSet, self).incar\n        incar = Incar(self.prev_incar) if self.prev_incar is not None else \\\n            Incar(parent_incar)\n\n        if self.mode == \"DIAG\":\n            # Default parameters for diagonalization calculation.\n            incar.update({\n                \"ALGO\": \"Exact\",\n                \"NELM\": 1,\n                \"LOPTICS\": True,\n                \"LPEAD\": True\n            })\n        elif self.mode == \"GW\":\n            # Default parameters for GW calculation.\n            incar.update({\n                \"ALGO\": \"GW0\",\n                \"NELM\": 1,\n                \"NOMEGA\": 80,\n                \"ENCUTGW\": 250\n            })\n            incar.pop(\"EDIFF\", None)\n            incar.pop(\"LOPTICS\", None)\n            incar.pop(\"LPEAD\", None)\n        elif self.mode == \"BSE\":\n            # Default parameters for BSE calculation.\n            incar.update({\n                \"ALGO\": \"BSE\",\n                \"ANTIRES\": 0,\n                \"NBANDSO\": 20,\n                \"NBANDSV\": 20\n            })\n\n        if self.nbands:\n            incar[\"NBANDS\"] = self.nbands\n\n        # Respect user set INCAR.\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, copy_wavecar=True, mode=\"DIAG\",\n                       nbands_factor=5, ncores=16, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for GW or BSE calculations from a\n        directory of previous Exact Diag Vasp run.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml of previous vasp run.\n            copy_wavecar: Whether to copy the old WAVECAR, WAVEDER and\n                associated files. Defaults to True.\n            mode (str): Supported modes are \"STATIC\", \"DIAG\" (default), \"GW\",\n                and \"BSE\".\n            nbands_factor (int): Multiplicative factor for NBANDS. Only applies\n                if mode==\"DIAG\". Need to be tested for convergence.\n            ncores (int): numbers of cores you do calculations. VASP will alter\n                NBANDS if it was not dividable by ncores. Only applies\n                if mode==\"DIAG\".\n            \\\\*\\\\*kwargs: All kwargs supported by MVLGWSet,\n                other than structure, prev_incar and mode, which\n                are determined from the prev_calc_dir.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n        prev_incar = vasprun.incar\n        structure = vasprun.final_structure\n\n        nbands = int(vasprun.parameters[\"NBANDS\"])\n        if mode.upper() == \"DIAG\":\n            nbands = int(np.ceil(nbands * nbands_factor / ncores) * ncores)\n\n        # copy WAVECAR, WAVEDER (derivatives)\n        files_to_transfer = {}\n        if copy_wavecar:\n            for fname in (\"WAVECAR\", \"WAVEDER\", \"WFULL\"):\n                w = sorted(glob.glob(os.path.join(prev_calc_dir, fname + \"*\")))\n                if w:\n                    if fname == \"WFULL\":\n                        for f in w:\n                            fname = os.path.basename(f)\n                            fname = fname.split(\".\")[0]\n                            files_to_transfer[fname] = f\n                    else:\n                        files_to_transfer[fname] = str(w[-1])\n\n        return cls(structure=structure, prev_incar=prev_incar,\n                        nbands=nbands, mode=mode,\n                        files_to_transfer=files_to_transfer, **kwargs)\n\n\nclass MVLSlabSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a set of slab vasp runs,\n    including both slabs (along the c direction) and orient unit cells (bulk),\n    to ensure the same KPOINTS, POTCAR and INCAR criterion.\n\n    Args:\n        k_product: default to 50, kpoint number * length for a & b directions,\n            also for c direction in bulk calculations\n        bulk (bool): Set to True for bulk calculation. Defaults to False.\n        get_locpot (bool): For calculating the electrostatic potential across the\n            slab. Use this if you want to calculate the work function. Will\n            set if slab calculation only.\n        **kwargs:\n            Other kwargs supported by :class:`DictSet`.\n    \"\"\"\n\n    def __init__(self, structure, k_product=50, bulk=False,\n                 auto_dipole=False, get_locpot=False, set_mix=True, **kwargs):\n        super(MVLSlabSet, self).__init__(structure, **kwargs)\n        self.structure = structure\n        self.k_product = k_product\n        self.bulk = bulk\n        self.auto_dipole = auto_dipole\n        self.get_locpot = get_locpot\n        self.kwargs = kwargs\n        self.set_mix = set_mix\n\n        slab_incar = {\"EDIFF\": 1e-4, \"EDIFFG\": -0.02, \"ENCUT\": 400,\n                      \"ISMEAR\": 0, \"SIGMA\": 0.05, \"ISIF\": 3}\n        if not self.bulk:\n            slab_incar[\"ISIF\"] = 2\n            if self.set_mix:\n                slab_incar[\"AMIN\"] = 0.01\n                slab_incar[\"AMIX\"] = 0.2\n                slab_incar[\"BMIX\"] = 0.001\n            slab_incar[\"NELMIN\"] = 8\n            if self.auto_dipole:\n                slab_incar[\"IDIPOL\"] = 3\n                slab_incar[\"LDIPOL\"] = True\n                slab_incar[\"DIPOL\"] = structure.center_of_mass\n            if self.get_locpot:\n                slab_incar[\"LVTOT\"] = True\n        self._config_dict[\"INCAR\"].update(slab_incar)\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        k_product, default to 50, is kpoint number * length for a & b\n            directions, also for c direction in bulk calculations\n        Automatic mesh & Gamma is the default setting.\n        \"\"\"\n\n        # To get input sets, the input structure has to has the same number\n        # of required parameters as a Structure object (ie. 4). Slab\n        # attributes aren't going to affect the VASP inputs anyways so\n        # converting the slab into a structure should not matter\n\n        kpt = super(MVLSlabSet, self).kpoints\n        kpt.comment = \"Automatic mesh\"\n        kpt.style = 'Gamma'\n\n        # use k_product to calculate kpoints, k_product = kpts[0][0] * a\n        abc = self.structure.lattice.abc\n        kpt_calc = [int(self.k_product / abc[0] + 0.5),\n                    int(self.k_product / abc[1] + 0.5), 1]\n        self.kpt_calc = kpt_calc\n        # calculate kpts (c direction) for bulk. (for slab, set to 1)\n        if self.bulk:\n            kpt_calc[2] = int(self.k_product / abc[2] + 0.5)\n\n        kpt.kpts[0] = kpt_calc\n\n        return kpt\n\n    def as_dict(self, verbosity=2):\n        d = MSONable.as_dict(self)\n        if verbosity == 1:\n            d.pop(\"structure\", None)\n        return d\n\n\n\nclass MVLGBSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a vasp input files for grain boundary calculations, slab\n    or bulk.\n\n    Args:\n        structure(Structure): provide the structure\n        k_product: Kpoint number * length for a & b directions, also for c\n            direction in bulk calculations. Default to 40.\n        slab_mode (bool): Defaults to False. Use default (False) for a\n            bulk supercell. Use True if you are performing calculations on a\n            slab-like (i.e., surface) of the GB, for example, when you are\n            calculating the work of separation.\n        is_metal (bool): Defaults to True. This determines whether an ISMEAR of\n            1 is used (for metals) or not (for insulators and semiconductors)\n            by default. Note that it does *not* override user_incar_settings,\n            which can be set by the user to be anything desired.\n        **kwargs:\n            Other kwargs supported by :class:`DictVaspInputSet`.\n    \"\"\"\n\n    def __init__(self, structure, k_product=40, slab_mode=False, is_metal=True,\n                 **kwargs):\n        super(MVLGBSet, self).__init__(structure, **kwargs)\n        self.structure = structure\n        self.k_product = k_product\n        self.slab_mode = slab_mode\n        self.is_metal = is_metal\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        k_product, default to 40, is kpoint number * length for a & b\n        directions, also for c direction in bulk calculations\n        Automatic mesh & Gamma is the default setting.\n        \"\"\"\n\n        # To get input sets, the input structure has to has the same number\n        # of required parameters as a Structure object.\n\n        kpt = super(MVLGBSet, self).kpoints\n        kpt.comment = \"Generated by pymatgen's MVLGBSet\"\n        kpt.style = 'Gamma'\n\n        # use k_product to calculate kpoints, k_product = kpts[0][0] * a\n        lengths = self.structure.lattice.abc\n        kpt_calc = [int(self.k_product / lengths[0] + 0.5),\n                    int(self.k_product / lengths[1] + 0.5),\n                    int(self.k_product / lengths[2] + 0.5)]\n\n        if self.slab_mode:\n            kpt_calc[2] = 1\n\n        kpt.kpts[0] = kpt_calc\n\n        return kpt\n\n    @property\n    def incar(self):\n\n        incar = super(MVLGBSet, self).incar\n\n        # The default incar setting is used for metallic system, for\n        # insulator or semiconductor, ISMEAR need to be changed.\n        incar.update({\n            \"LCHARG\": False,\n            \"NELM\": 60,\n            \"PREC\": \"Normal\",\n            \"EDIFFG\": -0.02,\n            \"ICHARG\": 0,\n            \"NSW\": 200,\n            \"EDIFF\": 0.0001\n        })\n\n        if self.is_metal:\n            incar[\"ISMEAR\"] = 1\n            incar[\"LDAU\"] = False\n\n        if self.slab_mode:\n            # for clean grain boundary and bulk relaxation, full optimization\n            # relaxation (ISIF=3) is used. For slab relaxation (ISIF=2) is used.\n            incar[\"ISIF\"] = 2\n            incar[\"NELMIN\"] = 8\n\n        incar.update(self.user_incar_settings)\n\n        return incar\n\n\nclass MITNEBSet(MITRelaxSet):\n    \"\"\"\n    Class for writing NEB inputs. Note that EDIFF is not on a per atom\n    basis for this input set.\n\n    Args:\n        unset_encut (bool): Whether to unset ENCUT.\n        \\\\*\\\\*kwargs: Other kwargs supported by :class:`DictSet`.\n    \"\"\"\n\n    def __init__(self, structures, unset_encut=False, **kwargs):\n        if len(structures) < 3:\n            raise ValueError(\"You need at least 3 structures for an NEB.\")\n        kwargs[\"sort_structure\"] = False\n        super(MITNEBSet, self).__init__(structures[0], **kwargs)\n        self.structures = self._process_structures(structures)\n        self.unset_encut = False\n        if unset_encut:\n            self._config_dict[\"INCAR\"].pop(\"ENCUT\", None)\n\n        if \"EDIFF\" not in self._config_dict[\"INCAR\"]:\n            self._config_dict[\"INCAR\"][\"EDIFF\"] = self._config_dict[\n                \"INCAR\"].pop(\"EDIFF_PER_ATOM\")\n\n        # NEB specific defaults\n        defaults = {'IMAGES': len(structures) - 2, 'IBRION': 1, 'ISYM': 0,\n                    'LCHARG': False, \"LDAU\": False}\n        self._config_dict[\"INCAR\"].update(defaults)\n\n    @property\n    def poscar(self):\n        return Poscar(self.structures[0])\n\n    @property\n    def poscars(self):\n        return [Poscar(s) for s in self.structures]\n\n    def _process_structures(self, structures):\n        \"\"\"\n        Remove any atom jumps across the cell\n        \"\"\"\n        input_structures = structures\n        structures = [input_structures[0]]\n        for s in input_structures[1:]:\n            prev = structures[-1]\n            for i in range(len(s)):\n                t = np.round(prev[i].frac_coords - s[i].frac_coords)\n                if np.any(np.abs(t) > 0.5):\n                    s.translate_sites([i], t, to_unit_cell=False)\n            structures.append(s)\n        return structures\n\n    def write_input(self, output_dir, make_dir_if_not_present=True,\n                    write_cif=False, write_path_cif=False,\n                    write_endpoint_inputs=False):\n        \"\"\"\n        NEB inputs has a special directory structure where inputs are in 00,\n        01, 02, ....\n\n        Args:\n            output_dir (str): Directory to output the VASP input files\n            make_dir_if_not_present (bool): Set to True if you want the\n                directory (and the whole path) to be created if it is not\n                present.\n            write_cif (bool): If true, writes a cif along with each POSCAR.\n            write_path_cif (bool): If true, writes a cif for each image.\n            write_endpoint_inputs (bool): If true, writes input files for\n                running endpoint calculations.\n        \"\"\"\n\n        if make_dir_if_not_present and not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        self.incar.write_file(os.path.join(output_dir, 'INCAR'))\n        self.kpoints.write_file(os.path.join(output_dir, 'KPOINTS'))\n        self.potcar.write_file(os.path.join(output_dir, 'POTCAR'))\n\n        for i, p in enumerate(self.poscars):\n            d = os.path.join(output_dir, str(i).zfill(2))\n            if not os.path.exists(d):\n                os.makedirs(d)\n            p.write_file(os.path.join(d, 'POSCAR'))\n            if write_cif:\n                p.structure.to(filename=os.path.join(d, '{}.cif'.format(i)))\n        if write_endpoint_inputs:\n            end_point_param = MITRelaxSet(\n                self.structures[0],\n                user_incar_settings=self.user_incar_settings)\n\n            for image in ['00', str(len(self.structures) - 1).zfill(2)]:\n                end_point_param.incar.write_file(os.path.join(output_dir, image, 'INCAR'))\n                end_point_param.kpoints.write_file(os.path.join(output_dir, image, 'KPOINTS'))\n                end_point_param.potcar.write_file(os.path.join(output_dir, image, 'POTCAR'))\n        if write_path_cif:\n            sites = set()\n            l = self.structures[0].lattice\n            for site in chain(*(s.sites for s in self.structures)):\n                sites.add(PeriodicSite(site.species_and_occu, site.frac_coords, l))\n            nebpath = Structure.from_sites(sorted(sites))\n            nebpath.to(filename=os.path.join(output_dir, 'path.cif'))\n\n\nclass MITMDSet(MITRelaxSet):\n    \"\"\"\n    Class for writing a vasp md run. This DOES NOT do multiple stage\n    runs.\n    \"\"\"\n\n    def __init__(self, structure, start_temp, end_temp, nsteps, time_step=2,\n                 spin_polarized=False, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): Input structure.\n            start_temp (int): Starting temperature.\n            end_temp (int): Final temperature.\n            nsteps (int): Number of time steps for simulations. The NSW parameter.\n            time_step (int): The time step for the simulation. The POTIM\n                parameter. Defaults to 2fs.\n            spin_polarized (bool): Whether to do spin polarized calculations.\n                The ISPIN parameter. Defaults to False.\n            \\\\*\\\\*kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n\n        # MD default settings\n        defaults = {'TEBEG': start_temp, 'TEEND': end_temp, 'NSW': nsteps,\n                    'EDIFF_PER_ATOM': 0.000001, 'LSCALU': False,\n                    'LCHARG': False,\n                    'LPLANE': False, 'LWAVE': True, 'ISMEAR': 0,\n                    'NELMIN': 4, 'LREAL': True, 'BMIX': 1,\n                    'MAXMIX': 20, 'NELM': 500, 'NSIM': 4, 'ISYM': 0,\n                    'ISIF': 0, 'IBRION': 0, 'NBLOCK': 1, 'KBLOCK': 100,\n                    'SMASS': 0, 'POTIM': time_step, 'PREC': 'Low',\n                    'ISPIN': 2 if spin_polarized else 1,\n                    \"LDAU\": False}\n\n        super(MITMDSet, self).__init__(structure, **kwargs)\n\n        self.start_temp = start_temp\n        self.end_temp = end_temp\n        self.nsteps = nsteps\n        self.time_step = time_step\n        self.spin_polarized = spin_polarized\n        self.kwargs = kwargs\n\n        # use VASP default ENCUT\n        self._config_dict[\"INCAR\"].pop('ENCUT', None)\n\n        if defaults['ISPIN'] == 1:\n            self._config_dict[\"INCAR\"].pop('MAGMOM', None)\n        self._config_dict[\"INCAR\"].update(defaults)\n\n    @property\n    def kpoints(self):\n        return Kpoints.gamma_automatic()\n\n\nclass MVLNPTMDSet(MITMDSet):\n    \"\"\"\n    Class for writing a vasp md run in NPT ensemble.\n    \"\"\"\n\n    def __init__(self, structure, start_temp, end_temp, nsteps, time_step=2,\n                 spin_polarized=False, **kwargs):\n        \"\"\"\n        Notes:\n            To eliminate Pulay stress, the default ENCUT is set to a rather\n            large value of ENCUT, which is 1.5 * ENMAX.\n        Args:\n            structure (Structure): input structure.\n            start_temp (int): Starting temperature.\n            end_temp (int): Final temperature.\n            nsteps(int): Number of time steps for simulations. The NSW parameter.\n            time_step (int): The time step for the simulation. The POTIM\n                parameter. Defaults to 2fs.\n            spin_polarized (bool): Whether to do spin polarized calculations.\n                The ISPIN parameter. Defaults to False.\n            \\\\*\\\\*kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        user_incar_settings = kwargs.get(\"user_incar_settings\", {})\n\n        # NPT-AIMD default settings\n        defaults = {\"IALGO\": 48,\n                    \"ISIF\": 3,\n                    \"LANGEVIN_GAMMA\": [10] * structure.ntypesp,\n                    \"LANGEVIN_GAMMA_L\": 1,\n                    \"MDALGO\": 3,\n                    \"PMASS\": 10,\n                    \"PSTRESS\": 0,\n                    \"SMASS\": 0}\n\n        defaults.update(user_incar_settings)\n        kwargs[\"user_incar_settings\"] = defaults\n\n        super(MVLNPTMDSet, self).__init__(structure, start_temp, end_temp,\n                                          nsteps, time_step, spin_polarized,\n                                          **kwargs)\n\n        # Set NPT-AIMD ENCUT = 1.5 * VASP_default\n        enmax = [self.potcar[i].keywords['ENMAX']\n                 for i in range(structure.ntypesp)]\n        encut = max(enmax) * 1.5\n        self._config_dict[\"INCAR\"][\"ENCUT\"] = encut\n\n\nclass MVLScanRelaxSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a relax input set using Strongly Constrained and\n    Appropriately Normed (SCAN) semilocal density functional.\n    \"\"\"\n\n    def __init__(self, structure, potcar_functional=\"PBE_52\", **kwargs):\n        \"\"\"\n        Notes:\n            1. This functional is only available from VASP.5.4.3 upwards.\n\n            2. Meta-GGA calculations require POTCAR files that include\n            information on the kinetic energy density of the core-electrons,\n            i.e. \"PBE_52\" or \"PBE_54\". Make sure the POTCAR including the\n            following lines (see VASP wiki for more details):\n\n                $ grep kinetic POTCAR\n                kinetic energy-density\n                mkinetic energy-density pseudized\n                kinetic energy density (partial)\n\n        Args:\n            structure (Structure): input structure.\n            potcar_functional (str): choose from \"PBE_52\" and \"PBE_54\".\n            vdw (str): set \"rVV10\" to enable SCAN+rVV10, which is a versatile\n                van der Waals density functional by combing the SCAN functional\n                with the rVV10 non-local correlation functional.\n            \\\\*\\\\*kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        if potcar_functional not in [\"PBE_52\", \"PBE_54\"]:\n            raise ValueError(\"SCAN calculations required PBE_52 or PBE_54!\")\n\n        super(MVLScanRelaxSet, self).__init__(\n            structure, potcar_functional=potcar_functional,\n            **kwargs)\n\n        self._config_dict[\"INCAR\"].update({\"ADDGRID\": True,\n                                           \"EDIFF\": 1e-05,\n                                           \"EDIFFG\": -0.05,\n                                           \"LASPH\": True,\n                                           \"LDAU\": False,\n                                           \"METAGGA\": \"SCAN\",\n                                           \"NELM\": 200})\n\n\ndef get_vasprun_outcar(path, parse_dos=True, parse_eigen=True):\n    vruns = list(glob.glob(os.path.join(path, \"vasprun.xml*\")))\n    outcars = list(glob.glob(os.path.join(path, \"OUTCAR*\")))\n\n    if len(vruns) == 0 or len(outcars) == 0:\n        raise ValueError(\n            \"Unable to get vasprun.xml/OUTCAR from prev calculation in %s\" %\n            path)\n    vsfile_fullpath = os.path.join(path, \"vasprun.xml\")\n    outcarfile_fullpath = os.path.join(path, \"OUTCAR\")\n    vsfile = vsfile_fullpath if vsfile_fullpath in vruns else sorted(vruns)[-1]\n    outcarfile = outcarfile_fullpath if outcarfile_fullpath in outcars else sorted(outcars)[-1]\n    return Vasprun(str(vsfile), parse_dos=parse_dos, parse_eigen=parse_eigen), \\\n           Outcar(str(outcarfile))\n\n\ndef get_structure_from_prev_run(vasprun, outcar=None, sym_prec=0.1,\n                                international_monoclinic=True):\n    \"\"\"\n    Process structure from previous run.\n\n    Args:\n        vasprun (Vasprun): Vasprun that contains the final structure\n            from previous run.\n        outcar (Outcar): Outcar that contains the magnetization info from\n            previous run.\n        sym_prec (float): Tolerance for symmetry finding for standardization. If\n            no standardization is desired, set to 0 or a False.\n        international_monoclinic (bool): Whether to use international\n            convention (vs Curtarolo) for monoclinic. Defaults True.\n\n    Returns:\n        Returns the magmom-decorated structure that can be passed to get\n        Vasp input files, e.g. get_kpoints.\n    \"\"\"\n    structure = vasprun.final_structure\n\n    site_properties = {}\n    # magmom\n    if vasprun.is_spin:\n        if outcar and outcar.magnetization:\n            site_properties.update({\"magmom\": [i['tot']\n                                               for i in outcar.magnetization]})\n        else:\n            site_properties.update({\"magmom\": vasprun.parameters['MAGMOM']})\n    # ldau\n    if vasprun.parameters.get(\"LDAU\", False):\n        for k in (\"LDAUU\", \"LDAUJ\", \"LDAUL\"):\n            vals = vasprun.incar[k]\n            m = {}\n            l = []\n            s = 0\n            for site in structure:\n                if site.specie.symbol not in m:\n                    m[site.specie.symbol] = vals[s]\n                    s += 1\n                l.append(m[site.specie.symbol])\n            if len(l) == len(structure):\n                site_properties.update({k.lower(): l})\n            else:\n                raise ValueError(\"length of list {} not the same as\"\n                                 \"structure\".format(l))\n\n    structure = structure.copy(site_properties=site_properties)\n\n    if sym_prec:\n        sym_finder = SpacegroupAnalyzer(structure, symprec=sym_prec)\n        new_structure = sym_finder.get_primitive_standard_structure(\n            international_monoclinic=international_monoclinic)\n        # the primitive structure finding has had several bugs in the past\n        # defend through validation\n        vpa_old = structure.volume / structure.num_sites\n        vpa_new = new_structure.volume / new_structure.num_sites\n        if abs(vpa_old - vpa_new) / vpa_old > 0.02:\n            raise ValueError(\n                \"Standardizing cell failed! VPA old: {}, VPA new: {}\".format(\n                    vpa_old, vpa_new))\n        sm = StructureMatcher()\n        if not sm.fit(structure, new_structure):\n            raise ValueError(\n                \"Standardizing cell failed! Old structure doesn't match new.\")\n        structure = new_structure\n\n    return structure\n\n\ndef batch_write_input(structures, vasp_input_set=MPRelaxSet, output_dir=\".\",\n                      make_dir_if_not_present=True, subfolder=None,\n                      sanitize=False, include_cif=False, **kwargs):\n    \"\"\"\n    Batch write vasp input for a sequence of structures to\n    output_dir, following the format output_dir/{group}/{formula}_{number}.\n\n    Args:\n        structures ([Structure]): Sequence of Structures.\n        vasp_input_set (VaspInputSet): VaspInputSet class that creates\n            vasp input files from structures. Note that a class should be\n            supplied. Defaults to MPRelaxSet.\n        output_dir (str): Directory to output files. Defaults to current\n            directory \".\".\n        make_dir_if_not_present (bool): Create the directory if not present.\n            Defaults to True.\n        subfolder (callable): Function to create subdirectory name from\n            structure. Defaults to simply \"formula_count\".\n        sanitize (bool): Boolean indicating whether to sanitize the\n            structure before writing the VASP input files. Sanitized output\n            are generally easier for viewing and certain forms of analysis.\n            Defaults to False.\n        include_cif (bool): Whether to output a CIF as well. CIF files are\n            generally better supported in visualization programs.\n        \\\\*\\\\*kwargs: Additional kwargs are passed to the vasp_input_set class\n            in addition to structure.\n    \"\"\"\n    for i, s in enumerate(structures):\n        formula = re.sub(r'\\s+', \"\", s.formula)\n        if subfolder is not None:\n            subdir = subfolder(s)\n            d = os.path.join(output_dir, subdir)\n        else:\n            d = os.path.join(output_dir, '{}_{}'.format(formula, i))\n        if sanitize:\n            s = s.copy(sanitize=True)\n        v = vasp_input_set(s, **kwargs)\n        v.write_input(d, make_dir_if_not_present=make_dir_if_not_present,\n                      include_cif=include_cif)\n", "idx": 1, "id": 17410, "msg": "Why is a private variable being accessed?", "proj": "materialsproject-pymatgen", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -485,7 +485,7 @@ func cleanupAllNamespaces(clientset *kubernetes.Clientset) {\n \t}\n \tlog.WithField(\"count\", len(nsList.Items)).Info(\"Namespaces present\")\n \tfor _, ns := range nsList.Items {\n-\t\tif ns.Status.Phase != v1.NamespaceTerminating {\n+\t\tif ns.Status.Phase != v1.NamespaceTerminating && !isSystemNamespace(ns.ObjectMeta.Name) {\n \t\t\terr = clientset.CoreV1().Namespaces().Delete(ns.ObjectMeta.Name, DeleteImmediately)\n \t\t\tif err != nil {\n \t\t\t\tpanic(err)", "y": 0, "oldf": "// Copyright (c) 2018 Tigera, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage infrastructure\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\t\"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/client-go/kubernetes\"\n\t\"k8s.io/client-go/rest\"\n\n\t\"github.com/projectcalico/felix/fv/containers\"\n\t\"github.com/projectcalico/felix/fv/utils\"\n\t\"github.com/projectcalico/libcalico-go/lib/apiconfig\"\n\tapi \"github.com/projectcalico/libcalico-go/lib/apis/v3\"\n\tclient \"github.com/projectcalico/libcalico-go/lib/clientv3\"\n\t\"github.com/projectcalico/libcalico-go/lib/names\"\n\t\"github.com/projectcalico/libcalico-go/lib/options\"\n)\n\ntype K8sDatastoreInfra struct {\n\tetcdContainer   *containers.Container\n\tk8sApiContainer *containers.Container\n\n\tcalicoClient client.Interface\n\tK8sClient    *kubernetes.Clientset\n\n\tEndpoint    string\n\tBadEndpoint string\n\n\tCertFileName string\n}\n\nvar (\n\t// This transport is based on  http.DefaultTransport, with InsecureSkipVerify set.\n\tinsecureTransport = &http.Transport{\n\t\tProxy: http.ProxyFromEnvironment,\n\t\tDialContext: (&net.Dialer{\n\t\t\tTimeout:   30 * time.Second,\n\t\t\tKeepAlive: 30 * time.Second,\n\t\t\tDualStack: true,\n\t\t}).DialContext,\n\t\tMaxIdleConns:        100,\n\t\tIdleConnTimeout:     90 * time.Second,\n\t\tTLSHandshakeTimeout: 10 * time.Second,\n\t\tTLSClientConfig: &tls.Config{\n\t\t\tInsecureSkipVerify: true,\n\t\t},\n\t\tExpectContinueTimeout: 1 * time.Second,\n\t}\n\tinsecureHTTPClient = http.Client{\n\t\tTransport: insecureTransport,\n\t}\n\n\tK8sInfra *K8sDatastoreInfra\n)\n\nfunc TearDownK8sInfra(kds *K8sDatastoreInfra) {\n\tkds.etcdContainer.Stop()\n\tkds.k8sApiContainer.Stop()\n}\n\nfunc createK8sDatastoreInfra() (DatastoreInfra, error) {\n\treturn GetK8sDatastoreInfra()\n}\n\nfunc GetK8sDatastoreInfra() (*K8sDatastoreInfra, error) {\n\tif K8sInfra != nil {\n\t\tK8sInfra.EnsureReady()\n\t\treturn K8sInfra, nil\n\t}\n\n\tvar err error\n\tK8sInfra, err = setupK8sDatastoreInfra()\n\treturn K8sInfra, err\n}\n\nfunc setupK8sDatastoreInfra() (*K8sDatastoreInfra, error) {\n\tkds := &K8sDatastoreInfra{}\n\n\t// Start etcd, which will back the k8s API server.\n\tkds.etcdContainer = RunEtcd()\n\tif kds.etcdContainer == nil {\n\t\treturn nil, errors.New(\"failed to create etcd container\")\n\t}\n\n\t// Start the k8s API server.\n\t//\n\t// The clients in this test - Felix, Typha and the test code itself - all connect\n\t// anonymously to the API server, because (a) they aren't running in pods in a proper\n\t// Kubernetes cluster, and (b) they don't provide client TLS certificates, and (c) they\n\t// don't use any of the other non-anonymous mechanisms that Kubernetes supports.  But, as of\n\t// 1.6, the API server doesn't allow anonymous users with the default \"AlwaysAllow\"\n\t// authorization mode.  So we specify the \"RBAC\" authorization mode instead, and create a\n\t// ClusterRoleBinding that gives the \"system:anonymous\" user unlimited power (aka the\n\t// \"cluster-admin\" role).\n\tkds.k8sApiContainer = containers.Run(\"apiserver\",\n\t\tcontainers.RunOpts{AutoRemove: true},\n\t\tutils.Config.K8sImage,\n\t\t\"/hyperkube\", \"apiserver\",\n\t\tfmt.Sprintf(\"--etcd-servers=http://%s:2379\", kds.etcdContainer.IP),\n\t\t\"--service-cluster-ip-range=10.101.0.0/16\",\n\t\t//\"-v=10\",\n\t\t\"--authorization-mode=RBAC\",\n\t)\n\tif kds.k8sApiContainer == nil {\n\t\tTearDownK8sInfra(kds)\n\t\treturn nil, errors.New(\"failed to create k8s API server container\")\n\t}\n\n\t// Allow anonymous connections to the API server.  We also use this command to wait\n\t// for the API server to be up.\n\tstart := time.Now()\n\tfor {\n\t\terr := kds.k8sApiContainer.ExecMayFail(\n\t\t\t\"kubectl\", \"create\", \"clusterrolebinding\",\n\t\t\t\"anonymous-admin\",\n\t\t\t\"--clusterrole=cluster-admin\",\n\t\t\t\"--user=system:anonymous\",\n\t\t)\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif time.Since(start) > 90*time.Second && err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to install role binding\")\n\t\t\tTearDownK8sInfra(kds)\n\t\t\treturn nil, err\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\t// Copy CRD registration manifest into the API server container, and apply it.\n\terr := kds.k8sApiContainer.CopyFileIntoContainer(\"../vendor/github.com/projectcalico/libcalico-go/test/crds.yaml\", \"/crds.yaml\")\n\tif err != nil {\n\t\tTearDownK8sInfra(kds)\n\t\treturn nil, err\n\t}\n\terr = kds.k8sApiContainer.ExecMayFail(\"kubectl\", \"apply\", \"-f\", \"/crds.yaml\")\n\tif err != nil {\n\t\tTearDownK8sInfra(kds)\n\t\treturn nil, err\n\t}\n\n\tkds.Endpoint = fmt.Sprintf(\"https://%s:6443\", kds.k8sApiContainer.IP)\n\tkds.BadEndpoint = fmt.Sprintf(\"https://%s:1234\", kds.k8sApiContainer.IP)\n\n\tstart = time.Now()\n\tfor {\n\t\tvar resp *http.Response\n\t\tresp, err = insecureHTTPClient.Get(kds.Endpoint + \"/apis/crd.projectcalico.org/v1/globalfelixconfigs\")\n\t\tif resp.StatusCode != 200 {\n\t\t\terr = errors.New(fmt.Sprintf(\"Bad status (%v) for CRD GET request\", resp.StatusCode))\n\t\t}\n\t\tif err != nil || resp.StatusCode != 200 {\n\t\t\tlog.WithError(err).WithField(\"status\", resp.StatusCode).Warn(\"Waiting for API server to respond to requests\")\n\t\t}\n\t\tresp.Body.Close()\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif time.Since(start) > 120*time.Second && err != nil {\n\t\t\tlog.WithError(err).Error(\"API server is not responding to requests\")\n\t\t\tTearDownK8sInfra(kds)\n\t\t\treturn nil, err\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tlog.Info(\"API server is up.\")\n\n\tkds.CertFileName = \"/tmp/\" + kds.k8sApiContainer.Name + \".crt\"\n\tstart = time.Now()\n\tfor {\n\t\tcmd := utils.Command(\"docker\", \"cp\",\n\t\t\tkds.k8sApiContainer.Name+\":/var/run/kubernetes/apiserver.crt\",\n\t\t\tkds.CertFileName,\n\t\t)\n\t\terr = cmd.Run()\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif time.Since(start) > 120*time.Second && err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to get API server cert\")\n\t\t\tTearDownK8sInfra(kds)\n\t\t\treturn nil, err\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tstart = time.Now()\n\tfor {\n\t\tkds.calicoClient, err = client.New(apiconfig.CalicoAPIConfig{\n\t\t\tSpec: apiconfig.CalicoAPIConfigSpec{\n\t\t\t\tDatastoreType: apiconfig.Kubernetes,\n\t\t\t\tKubeConfig: apiconfig.KubeConfig{\n\t\t\t\t\tK8sAPIEndpoint:           kds.Endpoint,\n\t\t\t\t\tK8sInsecureSkipTLSVerify: true,\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err == nil {\n\t\t\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\t\t\terr = kds.calicoClient.EnsureInitialized(\n\t\t\t\tctx,\n\t\t\t\t\"v3.0.0-test\",\n\t\t\t\t\"felix-fv,typha\", // Including typha in clusterType to prevent config churn\n\t\t\t)\n\t\t\tcancel()\n\t\t\tif err == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif time.Since(start) > 120*time.Second && err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to initialise calico client\")\n\t\t\tTearDownK8sInfra(kds)\n\t\t\treturn nil, err\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\tstart = time.Now()\n\tfor {\n\t\tkds.K8sClient, err = kubernetes.NewForConfig(&rest.Config{\n\t\t\tTransport: insecureTransport,\n\t\t\tHost:      \"https://\" + kds.k8sApiContainer.IP + \":6443\",\n\t\t})\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t\tif time.Since(start) > 120*time.Second && err != nil {\n\t\t\tlog.WithError(err).Error(\"Failed to create k8s client.\")\n\t\t\tTearDownK8sInfra(kds)\n\t\t\treturn nil, err\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\treturn kds, nil\n}\n\nfunc (kds *K8sDatastoreInfra) EnsureReady() {\n\tinfo, err := kds.GetCalicoClient().ClusterInformation().Get(\n\t\tcontext.Background(),\n\t\t\"default\",\n\t\toptions.GetOptions{},\n\t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tready := true\n\tinfo.Spec.DatastoreReady = &ready\n\t_, err = kds.GetCalicoClient().ClusterInformation().Update(\n\t\tcontext.Background(),\n\t\tinfo,\n\t\toptions.SetOptions{},\n\t)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (kds *K8sDatastoreInfra) Stop() {\n\tcleanupAllPods(kds.K8sClient)\n\tcleanupAllNodes(kds.K8sClient)\n\tcleanupAllNamespaces(kds.K8sClient)\n\tcleanupAllPools(kds.calicoClient)\n\tcleanupAllGlobalNetworkPolicies(kds.calicoClient)\n\tcleanupAllNetworkPolicies(kds.calicoClient)\n\tcleanupAllHostEndpoints(kds.calicoClient)\n}\n\nfunc (kds *K8sDatastoreInfra) GetDockerArgs() []string {\n\treturn []string{\n\t\t\"-e\", \"CALICO_DATASTORE_TYPE=kubernetes\",\n\t\t\"-e\", \"FELIX_DATASTORETYPE=kubernetes\",\n\t\t\"-e\", \"TYPHA_DATASTORETYPE=kubernetes\",\n\t\t\"-e\", \"K8S_API_ENDPOINT=\" + kds.Endpoint,\n\t\t\"-e\", \"K8S_INSECURE_SKIP_TLS_VERIFY=true\",\n\t\t\"-v\", kds.CertFileName + \":/tmp/apiserver.crt\",\n\t}\n}\n\nfunc (kds *K8sDatastoreInfra) GetBadEndpointDockerArgs() []string {\n\treturn []string{\n\t\t\"-e\", \"CALICO_DATASTORE_TYPE=kubernetes\",\n\t\t\"-e\", \"FELIX_DATASTORETYPE=kubernetes\",\n\t\t\"-e\", \"TYPHA_DATASTORETYPE=kubernetes\",\n\t\t\"-e\", \"K8S_API_ENDPOINT=\" + kds.BadEndpoint,\n\t\t\"-e\", \"K8S_INSECURE_SKIP_TLS_VERIFY=true\",\n\t\t\"-v\", kds.CertFileName + \":/tmp/apiserver.crt\",\n\t}\n}\n\nfunc (kds *K8sDatastoreInfra) GetCalicoClient() client.Interface {\n\treturn kds.calicoClient\n}\n\nfunc (kds *K8sDatastoreInfra) SetExpectedIPIPTunnelAddr(felix *Felix, idx int, needBGP bool) {\n\tfelix.ExpectedIPIPTunnelAddr = fmt.Sprintf(\"10.65.%d.1\", idx)\n}\n\nfunc (kds *K8sDatastoreInfra) AddNode(felix *Felix, idx int, needBGP bool) {\n\tnode_in := &v1.Node{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName: felix.Hostname,\n\t\t\tAnnotations: map[string]string{\n\t\t\t\t\"projectcalico.org/IPv4Address\": felix.IP,\n\t\t\t},\n\t\t},\n\t\tSpec: v1.NodeSpec{PodCIDR: fmt.Sprintf(\"10.65.%d.0/24\", idx)},\n\t}\n\tlog.WithField(\"node_in\", node_in).Debug(\"Node defined\")\n\tnode_out, err := kds.K8sClient.CoreV1().Nodes().Create(node_in)\n\tlog.WithField(\"node_out\", node_out).Debug(\"Created node\")\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (kds *K8sDatastoreInfra) AddWorkload(wep *api.WorkloadEndpoint) (*api.WorkloadEndpoint, error) {\n\tpod_in := &v1.Pod{\n\t\tObjectMeta: metav1.ObjectMeta{Name: wep.Spec.Workload, Namespace: wep.Namespace},\n\t\tSpec: v1.PodSpec{Containers: []v1.Container{{\n\t\t\tName:  wep.Spec.Endpoint,\n\t\t\tImage: \"ignore\",\n\t\t}},\n\t\t\tNodeName: wep.Spec.Node,\n\t\t},\n\t\tStatus: v1.PodStatus{\n\t\t\tPhase: v1.PodRunning,\n\t\t\tConditions: []v1.PodCondition{{\n\t\t\t\tType:   v1.PodScheduled,\n\t\t\t\tStatus: v1.ConditionTrue,\n\t\t\t}},\n\t\t\tPodIP: wep.Spec.IPNetworks[0],\n\t\t},\n\t}\n\tif wep.Labels != nil {\n\t\tpod_in.ObjectMeta.Labels = wep.Labels\n\t}\n\tlog.WithField(\"pod_in\", pod_in).Debug(\"Pod defined\")\n\tpod_out, err := kds.K8sClient.CoreV1().Pods(wep.Namespace).Create(pod_in)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"pod_out\", pod_out).Debug(\"Created pod\")\n\tpod_in = pod_out\n\tpod_in.Status.PodIP = wep.Spec.IPNetworks[0]\n\tpod_out, err = kds.K8sClient.CoreV1().Pods(wep.Namespace).UpdateStatus(pod_in)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"pod_out\", pod_out).Debug(\"Updated pod\")\n\n\twepid := names.WorkloadEndpointIdentifiers{\n\t\tNode:         wep.Spec.Node,\n\t\tOrchestrator: \"k8s\",\n\t\tEndpoint:     wep.Spec.Endpoint,\n\t\tPod:          wep.Spec.Workload,\n\t}\n\n\tname, err := wepid.CalculateWorkloadEndpointName(false)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"name\", name).Debug(\"Getting WorkloadEndpoint\")\n\treturn kds.calicoClient.WorkloadEndpoints().Get(context.Background(), wep.Namespace, name, options.GetOptions{})\n}\n\nfunc (kds *K8sDatastoreInfra) AddAllowToDatastore(selector string) error {\n\t// Create a policy to allow egress from the host so that we don't cut off Felix's datastore connection\n\t// when we enable the host endpoint.\n\tpolicy := api.NewGlobalNetworkPolicy()\n\tpolicy.Name = \"allow-egress\"\n\tpolicy.Spec.Selector = selector\n\tpolicy.Spec.Egress = []api.Rule{{\n\t\tAction: api.Allow,\n\t\tDestination: api.EntityRule{\n\t\t\tNets: []string{kds.k8sApiContainer.IP + \"/32\"},\n\t\t},\n\t}}\n\t_, err := kds.calicoClient.GlobalNetworkPolicies().Create(utils.Ctx, policy, utils.NoOptions)\n\treturn err\n}\n\nfunc (kds *K8sDatastoreInfra) AddDefaultAllow() error {\n\treturn nil\n}\n\nfunc (kds *K8sDatastoreInfra) AddDefaultDeny() error {\n\tpolicy := api.NewNetworkPolicy()\n\tpolicy.Name = \"deny-all\"\n\tpolicy.Namespace = \"default\"\n\tpolicy.Spec.Ingress = []api.Rule{{Action: api.Deny}}\n\tpolicy.Spec.Egress = []api.Rule{{Action: api.Deny}}\n\tpolicy.Spec.Selector = \"all()\"\n\t_, err := kds.calicoClient.NetworkPolicies().Create(utils.Ctx, policy, utils.NoOptions)\n\treturn err\n}\n\nfunc (kds *K8sDatastoreInfra) DumpErrorData() {\n\tnsList, err := kds.K8sClient.CoreV1().Namespaces().List(metav1.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Kubernetes Namespaces\\n\")\n\t\tfor _, ns := range nsList.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", ns))\n\t\t}\n\t}\n\n\tprofiles, err := kds.calicoClient.Profiles().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico Profiles\\n\")\n\t\tfor _, profile := range profiles.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", profile))\n\t\t}\n\t}\n\tpolicies, err := kds.calicoClient.NetworkPolicies().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico NetworkPolicies\\n\")\n\t\tfor _, policy := range policies.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", policy))\n\t\t}\n\t}\n\tgnps, err := kds.calicoClient.GlobalNetworkPolicies().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico GlobalNetworkPolicies\\n\")\n\t\tfor _, gnp := range gnps.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", gnp))\n\t\t}\n\t}\n\tworkloads, err := kds.calicoClient.WorkloadEndpoints().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico WorkloadEndpoints\\n\")\n\t\tfor _, w := range workloads.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", w))\n\t\t}\n\t}\n\tnodes, err := kds.calicoClient.Nodes().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico Nodes\\n\")\n\t\tfor _, n := range nodes.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", n))\n\t\t}\n\t}\n\theps, err := kds.calicoClient.HostEndpoints().List(context.Background(), options.ListOptions{})\n\tif err == nil {\n\t\tutils.AddToTestOutput(\"Calico Host Endpoints\\n\")\n\t\tfor _, hep := range heps.Items {\n\t\t\tutils.AddToTestOutput(fmt.Sprintf(\"%v\\n\", hep))\n\t\t}\n\t}\n}\n\nvar zeroGracePeriod int64 = 0\nvar DeleteImmediately = &metav1.DeleteOptions{\n\tGracePeriodSeconds: &zeroGracePeriod,\n}\n\nfunc cleanupAllNamespaces(clientset *kubernetes.Clientset) {\n\tlog.Info(\"Cleaning up all namespaces...\")\n\tnsList, err := clientset.CoreV1().Namespaces().List(metav1.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(nsList.Items)).Info(\"Namespaces present\")\n\tfor _, ns := range nsList.Items {\n\t\tif ns.Status.Phase != v1.NamespaceTerminating {\n\t\t\terr = clientset.CoreV1().Namespaces().Delete(ns.ObjectMeta.Name, DeleteImmediately)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n\tlog.Info(\"Cleaned up all namespaces\")\n}\n\nfunc cleanupAllNodes(clientset *kubernetes.Clientset) {\n\tlog.Info(\"Cleaning up all nodes...\")\n\tnodeList, err := clientset.CoreV1().Nodes().List(metav1.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(nodeList.Items)).Info(\"Nodes present\")\n\tfor _, node := range nodeList.Items {\n\t\terr = clientset.CoreV1().Nodes().Delete(node.ObjectMeta.Name, DeleteImmediately)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\tlog.Info(\"Cleaned up all nodes\")\n}\nfunc cleanupAllPods(clientset *kubernetes.Clientset) {\n\tnsList, err := clientset.CoreV1().Namespaces().List(metav1.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(nsList.Items)).Info(\"Namespaces present\")\n\tpodsDeleted := 0\n\tadmission := make(chan int, 10)\n\twaiter := sync.WaitGroup{}\n\twaiter.Add(len(nsList.Items))\n\tfor _, ns := range nsList.Items {\n\t\tnsName := ns.ObjectMeta.Name\n\t\tgo func() {\n\t\t\tadmission <- 1\n\t\t\tpodList, err := clientset.CoreV1().Pods(nsName).List(metav1.ListOptions{})\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tlog.WithField(\"count\", len(podList.Items)).WithField(\"namespace\", nsName).Debug(\n\t\t\t\t\"Pods present\")\n\t\t\tfor _, pod := range podList.Items {\n\t\t\t\terr = clientset.CoreV1().Pods(nsName).Delete(pod.ObjectMeta.Name, DeleteImmediately)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tpodsDeleted += len(podList.Items)\n\t\t\t<-admission\n\t\t\twaiter.Done()\n\t\t}()\n\t}\n\twaiter.Wait()\n\n\tlog.WithField(\"podsDeleted\", podsDeleted).Info(\"Cleaned up all pods\")\n}\n\nfunc cleanupAllPools(client client.Interface) {\n\tctx := context.Background()\n\tpools, err := client.IPPools().List(ctx, options.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(pools.Items)).Info(\"IP Pools present\")\n\tfor _, pool := range pools.Items {\n\t\t_, err = client.IPPools().Delete(ctx, pool.Name, options.DeleteOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc cleanupAllGlobalNetworkPolicies(client client.Interface) {\n\tctx := context.Background()\n\tgnps, err := client.GlobalNetworkPolicies().List(ctx, options.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(gnps.Items)).Info(\"Global Network Policies present\")\n\tfor _, gnp := range gnps.Items {\n\t\t_, err = client.GlobalNetworkPolicies().Delete(ctx, gnp.Name, options.DeleteOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc cleanupAllNetworkPolicies(client client.Interface) {\n\tctx := context.Background()\n\tnps, err := client.NetworkPolicies().List(ctx, options.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(nps.Items)).Info(\"Global Network Policies present\")\n\tfor _, np := range nps.Items {\n\t\t_, err = client.NetworkPolicies().Delete(ctx, np.Namespace, np.Name, options.DeleteOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n\nfunc cleanupAllHostEndpoints(client client.Interface) {\n\tctx := context.Background()\n\theps, err := client.HostEndpoints().List(ctx, options.ListOptions{})\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tlog.WithField(\"count\", len(heps.Items)).Info(\"HostEndpoints present\")\n\tfor _, hep := range heps.Items {\n\t\t_, err = client.HostEndpoints().Delete(ctx, hep.Name, options.DeleteOptions{})\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n}\n", "idx": 11, "id": 16587, "msg": "", "proj": "projectcalico-felix", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -32,12 +32,6 @@ if (typeof AggregateError === 'undefined') {\n     }\n   }\n }\n-class AggregateRestrictionError extends AggregateError {\n-  constructor (...args) {\n-    super(...args)\n-    this.isRestriction = true\n-  }\n-}\n \n /**\n  * Uppy Core module.", "y": 1, "oldf": "/* global AggregateError */\n\n'use strict'\n\nconst Translator = require('@uppy/utils/lib/Translator')\nconst ee = require('namespace-emitter')\nconst { nanoid } = require('nanoid')\nconst throttle = require('lodash.throttle')\nconst prettierBytes = require('@transloadit/prettier-bytes')\nconst match = require('mime-match')\nconst DefaultStore = require('@uppy/store-default')\nconst getFileType = require('@uppy/utils/lib/getFileType')\nconst getFileNameAndExtension = require('@uppy/utils/lib/getFileNameAndExtension')\nconst generateFileID = require('@uppy/utils/lib/generateFileID')\nconst supportsUploadProgress = require('./supportsUploadProgress')\nconst getFileName = require('./getFileName')\nconst { justErrorsLogger, debugLogger } = require('./loggers')\n\n// Exported from here.\nclass RestrictionError extends Error {\n  constructor (...args) {\n    super(...args)\n    this.isRestriction = true\n  }\n}\nif (typeof AggregateError === 'undefined') {\n  // eslint-disable-next-line no-global-assign\n  globalThis.AggregateError = class AggregateError extends Error {\n    constructor (message, errors) {\n      super(message)\n      this.errors = errors\n    }\n  }\n}\nclass AggregateRestrictionError extends AggregateError {\n  constructor (...args) {\n    super(...args)\n    this.isRestriction = true\n  }\n}\n\n/**\n * Uppy Core module.\n * Manages plugins, state updates, acts as an event bus,\n * adds/removes files and metadata.\n */\nclass Uppy {\n  // eslint-disable-next-line global-require\n  static VERSION = require('../package.json').version\n\n  /** @type {Record<string, BasePlugin[]>} */\n  #plugins = Object.create(null)\n\n  #storeUnsubscribe\n\n  #emitter = ee()\n\n  #preProcessors = new Set()\n\n  #uploaders = new Set()\n\n  #postProcessors = new Set()\n\n  /**\n   * Instantiate Uppy\n   *\n   * @param {object} opts \u2014 Uppy options\n   */\n  constructor (opts) {\n    this.defaultLocale = {\n      strings: {\n        addBulkFilesFailed: {\n          0: 'Failed to add %{smart_count} file due to an internal error',\n          1: 'Failed to add %{smart_count} files due to internal errors',\n        },\n        youCanOnlyUploadX: {\n          0: 'You can only upload %{smart_count} file',\n          1: 'You can only upload %{smart_count} files',\n        },\n        youHaveToAtLeastSelectX: {\n          0: 'You have to select at least %{smart_count} file',\n          1: 'You have to select at least %{smart_count} files',\n        },\n        exceedsSize: '%{file} exceeds maximum allowed size of %{size}',\n        missingRequiredMetaField: 'Missing required meta fields',\n        missingRequiredMetaFieldOnFile: 'Missing required meta fields in %{fileName}',\n        inferiorSize: 'This file is smaller than the allowed size of %{size}',\n        youCanOnlyUploadFileTypes: 'You can only upload: %{types}',\n        noMoreFilesAllowed: 'Cannot add more files',\n        noDuplicates: 'Cannot add the duplicate file \\'%{fileName}\\', it already exists',\n        companionError: 'Connection with Companion failed',\n        authAborted: 'Authentication aborted',\n        companionUnauthorizeHint: 'To unauthorize to your %{provider} account, please go to %{url}',\n        failedToUpload: 'Failed to upload %{file}',\n        noInternetConnection: 'No Internet connection',\n        connectedToInternet: 'Connected to the Internet',\n        // Strings for remote providers\n        noFilesFound: 'You have no files or folders here',\n        selectX: {\n          0: 'Select %{smart_count}',\n          1: 'Select %{smart_count}',\n        },\n        allFilesFromFolderNamed: 'All files from folder %{name}',\n        openFolderNamed: 'Open folder %{name}',\n        cancel: 'Cancel',\n        logOut: 'Log out',\n        filter: 'Filter',\n        resetFilter: 'Reset filter',\n        loading: 'Loading...',\n        authenticateWithTitle: 'Please authenticate with %{pluginName} to select files',\n        authenticateWith: 'Connect to %{pluginName}',\n        signInWithGoogle: 'Sign in with Google',\n        searchImages: 'Search for images',\n        enterTextToSearch: 'Enter text to search for images',\n        backToSearch: 'Back to Search',\n        emptyFolderAdded: 'No files were added from empty folder',\n        folderAlreadyAdded: 'The folder \"%{folder}\" was already added',\n        folderAdded: {\n          0: 'Added %{smart_count} file from %{folder}',\n          1: 'Added %{smart_count} files from %{folder}',\n        },\n      },\n    }\n\n    const defaultOptions = {\n      id: 'uppy',\n      autoProceed: false,\n      /**\n       * @deprecated The method should not be used\n       */\n      allowMultipleUploads: true,\n      allowMultipleUploadBatches: true,\n      debug: false,\n      restrictions: {\n        maxFileSize: null,\n        minFileSize: null,\n        maxTotalFileSize: null,\n        maxNumberOfFiles: null,\n        minNumberOfFiles: null,\n        allowedFileTypes: null,\n        requiredMetaFields: [],\n      },\n      meta: {},\n      onBeforeFileAdded: (currentFile) => currentFile,\n      onBeforeUpload: (files) => files,\n      store: DefaultStore(),\n      logger: justErrorsLogger,\n      infoTimeout: 5000,\n    }\n\n    // Merge default options with the ones set by user,\n    // making sure to merge restrictions too\n    this.opts = {\n      ...defaultOptions,\n      ...opts,\n      restrictions: {\n        ...defaultOptions.restrictions,\n        ...(opts && opts.restrictions),\n      },\n    }\n\n    // Support debug: true for backwards-compatability, unless logger is set in opts\n    // opts instead of this.opts to avoid comparing objects \u2014 we set logger: justErrorsLogger in defaultOptions\n    if (opts && opts.logger && opts.debug) {\n      this.log('You are using a custom `logger`, but also set `debug: true`, which uses built-in logger to output logs to console. Ignoring `debug: true` and using your custom `logger`.', 'warning')\n    } else if (opts && opts.debug) {\n      this.opts.logger = debugLogger\n    }\n\n    this.log(`Using Core v${this.constructor.VERSION}`)\n\n    if (this.opts.restrictions.allowedFileTypes\n        && this.opts.restrictions.allowedFileTypes !== null\n        && !Array.isArray(this.opts.restrictions.allowedFileTypes)) {\n      throw new TypeError('`restrictions.allowedFileTypes` must be an array')\n    }\n\n    this.i18nInit()\n\n    // ___Why throttle at 500ms?\n    //    - We must throttle at >250ms for superfocus in Dashboard to work well\n    //    (because animation takes 0.25s, and we want to wait for all animations to be over before refocusing).\n    //    [Practical Check]: if thottle is at 100ms, then if you are uploading a file,\n    //    and click 'ADD MORE FILES', - focus won't activate in Firefox.\n    //    - We must throttle at around >500ms to avoid performance lags.\n    //    [Practical Check] Firefox, try to upload a big file for a prolonged period of time. Laptop will start to heat up.\n    this.calculateProgress = throttle(this.calculateProgress.bind(this), 500, { leading: true, trailing: true })\n\n    this.store = this.opts.store\n    this.setState({\n      plugins: {},\n      files: {},\n      currentUploads: {},\n      allowNewUpload: true,\n      capabilities: {\n        uploadProgress: supportsUploadProgress(),\n        individualCancellation: true,\n        resumableUploads: false,\n      },\n      totalProgress: 0,\n      meta: { ...this.opts.meta },\n      info: [],\n      recoveredState: null,\n    })\n\n    this.#storeUnsubscribe = this.store.subscribe((prevState, nextState, patch) => {\n      this.emit('state-update', prevState, nextState, patch)\n      this.updateAll(nextState)\n    })\n\n    // Exposing uppy object on window for debugging and testing\n    if (this.opts.debug && typeof window !== 'undefined') {\n      window[this.opts.id] = this\n    }\n\n    this.#addListeners()\n  }\n\n  emit (event, ...args) {\n    this.#emitter.emit(event, ...args)\n  }\n\n  on (event, callback) {\n    this.#emitter.on(event, callback)\n    return this\n  }\n\n  once (event, callback) {\n    this.#emitter.once(event, callback)\n    return this\n  }\n\n  off (event, callback) {\n    this.#emitter.off(event, callback)\n    return this\n  }\n\n  /**\n   * Iterate on all plugins and run `update` on them.\n   * Called each time state changes.\n   *\n   */\n  updateAll (state) {\n    this.iteratePlugins(plugin => {\n      plugin.update(state)\n    })\n  }\n\n  /**\n   * Updates state with a patch\n   *\n   * @param {object} patch {foo: 'bar'}\n   */\n  setState (patch) {\n    this.store.setState(patch)\n  }\n\n  /**\n   * Returns current state.\n   *\n   * @returns {object}\n   */\n  getState () {\n    return this.store.getState()\n  }\n\n  /**\n   * Back compat for when uppy.state is used instead of uppy.getState().\n   *\n   * @deprecated\n   */\n  get state () {\n    // Here, state is a non-enumerable property.\n    return this.getState()\n  }\n\n  /**\n   * Shorthand to set state for a specific file.\n   */\n  setFileState (fileID, state) {\n    if (!this.getState().files[fileID]) {\n      throw new Error(`Can\u2019t set state for ${fileID} (the file could have been removed)`)\n    }\n\n    this.setState({\n      files: { ...this.getState().files, [fileID]: { ...this.getState().files[fileID], ...state } },\n    })\n  }\n\n  i18nInit () {\n    const translator = new Translator([this.defaultLocale, this.opts.locale])\n    this.i18n = translator.translate.bind(translator)\n    this.i18nArray = translator.translateArray.bind(translator)\n    this.locale = translator.locale\n  }\n\n  setOptions (newOpts) {\n    this.opts = {\n      ...this.opts,\n      ...newOpts,\n      restrictions: {\n        ...this.opts.restrictions,\n        ...(newOpts && newOpts.restrictions),\n      },\n    }\n\n    if (newOpts.meta) {\n      this.setMeta(newOpts.meta)\n    }\n\n    this.i18nInit()\n\n    if (newOpts.locale) {\n      this.iteratePlugins((plugin) => {\n        plugin.setOptions()\n      })\n    }\n\n    // Note: this is not the preact `setState`, it's an internal function that has the same name.\n    this.setState() // so that UI re-renders with new options\n  }\n\n  resetProgress () {\n    const defaultProgress = {\n      percentage: 0,\n      bytesUploaded: 0,\n      uploadComplete: false,\n      uploadStarted: null,\n    }\n    const files = { ...this.getState().files }\n    const updatedFiles = {}\n    Object.keys(files).forEach(fileID => {\n      const updatedFile = { ...files[fileID] }\n      updatedFile.progress = { ...updatedFile.progress, ...defaultProgress }\n      updatedFiles[fileID] = updatedFile\n    })\n\n    this.setState({\n      files: updatedFiles,\n      totalProgress: 0,\n    })\n\n    this.emit('reset-progress')\n  }\n\n  addPreProcessor (fn) {\n    this.#preProcessors.add(fn)\n  }\n\n  removePreProcessor (fn) {\n    return this.#preProcessors.delete(fn)\n  }\n\n  addPostProcessor (fn) {\n    this.#postProcessors.add(fn)\n  }\n\n  removePostProcessor (fn) {\n    return this.#postProcessors.delete(fn)\n  }\n\n  addUploader (fn) {\n    this.#uploaders.add(fn)\n  }\n\n  removeUploader (fn) {\n    return this.#uploaders.delete(fn)\n  }\n\n  setMeta (data) {\n    const updatedMeta = { ...this.getState().meta, ...data }\n    const updatedFiles = { ...this.getState().files }\n\n    Object.keys(updatedFiles).forEach((fileID) => {\n      updatedFiles[fileID] = { ...updatedFiles[fileID], meta: { ...updatedFiles[fileID].meta, ...data } }\n    })\n\n    this.log('Adding metadata:')\n    this.log(data)\n\n    this.setState({\n      meta: updatedMeta,\n      files: updatedFiles,\n    })\n  }\n\n  setFileMeta (fileID, data) {\n    const updatedFiles = { ...this.getState().files }\n    if (!updatedFiles[fileID]) {\n      this.log('Was trying to set metadata for a file that has been removed: ', fileID)\n      return\n    }\n    const newMeta = { ...updatedFiles[fileID].meta, ...data }\n    updatedFiles[fileID] = { ...updatedFiles[fileID], meta: newMeta }\n    this.setState({ files: updatedFiles })\n  }\n\n  /**\n   * Get a file object.\n   *\n   * @param {string} fileID The ID of the file object to return.\n   */\n  getFile (fileID) {\n    return this.getState().files[fileID]\n  }\n\n  /**\n   * Get all files in an array.\n   */\n  getFiles () {\n    const { files } = this.getState()\n    return Object.values(files)\n  }\n\n  getObjectOfFilesPerState () {\n    const { files: filesObject, totalProgress, error } = this.getState()\n    const files = Object.values(filesObject)\n    const inProgressFiles = files.filter(({ progress }) => !progress.uploadComplete && progress.uploadStarted)\n    const newFiles =  files.filter((file) => !file.progress.uploadStarted)\n    const startedFiles = files.filter(\n      file => file.progress.uploadStarted || file.progress.preprocess || file.progress.postprocess\n    )\n    const uploadStartedFiles = files.filter((file) => file.progress.uploadStarted)\n    const pausedFiles = files.filter((file) => file.isPaused)\n    const completeFiles = files.filter((file) => file.progress.uploadComplete)\n    const erroredFiles = files.filter((file) => file.error)\n    const inProgressNotPausedFiles = inProgressFiles.filter((file) => !file.isPaused)\n    const processingFiles = files.filter((file) => file.progress.preprocess || file.progress.postprocess)\n\n    return {\n      newFiles,\n      startedFiles,\n      uploadStartedFiles,\n      pausedFiles,\n      completeFiles,\n      erroredFiles,\n      inProgressFiles,\n      inProgressNotPausedFiles,\n      processingFiles,\n\n      isUploadStarted: uploadStartedFiles.length > 0,\n      isAllComplete: totalProgress === 100\n        && completeFiles.length === files.length\n        && processingFiles.length === 0,\n      isAllErrored: !!error && erroredFiles.length === files.length,\n      isAllPaused: inProgressFiles.length !== 0 && pausedFiles.length === inProgressFiles.length,\n      isUploadInProgress: inProgressFiles.length > 0,\n      isSomeGhost: files.some(file => file.isGhost),\n    }\n  }\n\n  /**\n   * A public wrapper for _checkRestrictions \u2014 checks if a file passes a set of restrictions.\n   * For use in UI pluigins (like Providers), to disallow selecting files that won\u2019t pass restrictions.\n   *\n   * @param {object} file object to check\n   * @param {Array} [files] array to check maxNumberOfFiles and maxTotalFileSize\n   * @returns {object} { result: true/false, reason: why file didn\u2019t pass restrictions }\n   */\n  validateRestrictions (file, files) {\n    try {\n      this.#checkRestrictions(file, files)\n      return {\n        result: true,\n      }\n    } catch (err) {\n      return {\n        result: false,\n        reason: err.message,\n      }\n    }\n  }\n\n  /**\n   * Check if file passes a set of restrictions set in options: maxFileSize, minFileSize,\n   * maxNumberOfFiles and allowedFileTypes.\n   *\n   * @param {object} file object to check\n   * @param {Array} [files] array to check maxNumberOfFiles and maxTotalFileSize\n   * @private\n   */\n  #checkRestrictions (file, files = this.getFiles()) {\n    const { maxFileSize, minFileSize, maxTotalFileSize, maxNumberOfFiles, allowedFileTypes } = this.opts.restrictions\n\n    if (maxNumberOfFiles) {\n      if (files.length + 1 > maxNumberOfFiles) {\n        throw new RestrictionError(`${this.i18n('youCanOnlyUploadX', { smart_count: maxNumberOfFiles })}`)\n      }\n    }\n\n    if (allowedFileTypes) {\n      const isCorrectFileType = allowedFileTypes.some((type) => {\n        // check if this is a mime-type\n        if (type.indexOf('/') > -1) {\n          if (!file.type) return false\n          return match(file.type.replace(/;.*?$/, ''), type)\n        }\n\n        // otherwise this is likely an extension\n        if (type[0] === '.' && file.extension) {\n          return file.extension.toLowerCase() === type.substr(1).toLowerCase()\n        }\n        return false\n      })\n\n      if (!isCorrectFileType) {\n        const allowedFileTypesString = allowedFileTypes.join(', ')\n        throw new RestrictionError(this.i18n('youCanOnlyUploadFileTypes', { types: allowedFileTypesString }))\n      }\n    }\n\n    // We can't check maxTotalFileSize if the size is unknown.\n    if (maxTotalFileSize && file.size != null) {\n      let totalFilesSize = 0\n      totalFilesSize += file.size\n      files.forEach((f) => {\n        totalFilesSize += f.size\n      })\n      if (totalFilesSize > maxTotalFileSize) {\n        throw new RestrictionError(this.i18n('exceedsSize', {\n          size: prettierBytes(maxTotalFileSize),\n          file: file.name,\n        }))\n      }\n    }\n\n    // We can't check maxFileSize if the size is unknown.\n    if (maxFileSize && file.size != null) {\n      if (file.size > maxFileSize) {\n        throw new RestrictionError(this.i18n('exceedsSize', {\n          size: prettierBytes(maxFileSize),\n          file: file.name,\n        }))\n      }\n    }\n\n    // We can't check minFileSize if the size is unknown.\n    if (minFileSize && file.size != null) {\n      if (file.size < minFileSize) {\n        throw new RestrictionError(this.i18n('inferiorSize', {\n          size: prettierBytes(minFileSize),\n        }))\n      }\n    }\n  }\n\n  /**\n   * Check if minNumberOfFiles restriction is reached before uploading.\n   *\n   * @private\n   */\n  #checkMinNumberOfFiles (files) {\n    const { minNumberOfFiles } = this.opts.restrictions\n    if (Object.keys(files).length < minNumberOfFiles) {\n      throw new RestrictionError(`${this.i18n('youHaveToAtLeastSelectX', { smart_count: minNumberOfFiles })}`)\n    }\n  }\n\n  /**\n   * Check if requiredMetaField restriction is met before uploading.\n   *\n   */\n  #checkRequiredMetaFields (files) {\n    const { requiredMetaFields } = this.opts.restrictions\n    const { hasOwnProperty } = Object.prototype\n\n    const errors = []\n    for (const fileID of Object.keys(files)) {\n      const file = this.getFile(fileID)\n      for (let i = 0; i < requiredMetaFields.length; i++) {\n        if (!hasOwnProperty.call(file.meta, requiredMetaFields[i]) || file.meta[requiredMetaFields[i]] === '') {\n          const err = new RestrictionError(`${this.i18n('missingRequiredMetaFieldOnFile', { fileName: file.name })}`)\n          errors.push(err)\n          this.#showOrLogErrorAndThrow(err, { file, showInformer: false, throwErr: false })\n        }\n      }\n    }\n\n    if (errors.length) {\n      throw new AggregateRestrictionError(`${this.i18n('missingRequiredMetaField')}`, errors)\n    }\n  }\n\n  /**\n   * Logs an error, sets Informer message, then throws the error.\n   * Emits a 'restriction-failed' event if it\u2019s a restriction error\n   *\n   * @param {object | string} err \u2014 Error object or plain string message\n   * @param {object} [options]\n   * @param {boolean} [options.showInformer=true] \u2014 Sometimes developer might want to show Informer manually\n   * @param {object} [options.file=null] \u2014 File object used to emit the restriction error\n   * @param {boolean} [options.throwErr=true] \u2014 Errors shouldn\u2019t be thrown, for example, in `upload-error` event\n   * @private\n   */\n  #showOrLogErrorAndThrow (err, { showInformer = true, file = null, throwErr = true } = {}) {\n    const message = typeof err === 'object' ? err.message : err\n    const details = (typeof err === 'object' && err.details) ? err.details : ''\n\n    // Restriction errors should be logged, but not as errors,\n    // as they are expected and shown in the UI.\n    let logMessageWithDetails = message\n    if (details) {\n      logMessageWithDetails += ` ${details}`\n    }\n    if (err.isRestriction) {\n      this.log(logMessageWithDetails)\n      this.emit('restriction-failed', file, err)\n    } else {\n      this.log(logMessageWithDetails, 'error')\n    }\n\n    // Sometimes informer has to be shown manually by the developer,\n    // for example, in `onBeforeFileAdded`.\n    if (showInformer) {\n      this.info({ message, details }, 'error', this.opts.infoTimeout)\n    }\n\n    if (throwErr) {\n      throw (typeof err === 'object' ? err : new Error(err))\n    }\n  }\n\n  #assertNewUploadAllowed (file) {\n    const { allowNewUpload } = this.getState()\n\n    if (allowNewUpload === false) {\n      this.#showOrLogErrorAndThrow(new RestrictionError(this.i18n('noMoreFilesAllowed')), { file })\n    }\n  }\n\n  checkIfFileAlreadyExists (fileID) {\n    const { files } = this.getState()\n\n    if (files[fileID] && !files[fileID].isGhost) {\n      return true\n    }\n    return false\n  }\n\n  /**\n   * Create a file state object based on user-provided `addFile()` options.\n   *\n   * Note this is extremely side-effectful and should only be done when a file state object\n   * will be added to state immediately afterward!\n   *\n   * The `files` value is passed in because it may be updated by the caller without updating the store.\n   */\n  #checkAndCreateFileStateObject (files, fileDescriptor) {\n    const fileType = getFileType(fileDescriptor)\n    const fileName = getFileName(fileType, fileDescriptor)\n    const fileExtension = getFileNameAndExtension(fileName).extension\n    const isRemote = Boolean(fileDescriptor.isRemote)\n    const fileID = generateFileID({\n      ...fileDescriptor,\n      type: fileType,\n    })\n\n    if (this.checkIfFileAlreadyExists(fileID)) {\n      const error = new RestrictionError(this.i18n('noDuplicates', { fileName }))\n      this.#showOrLogErrorAndThrow(error, { file: fileDescriptor })\n    }\n\n    const meta = fileDescriptor.meta || {}\n    meta.name = fileName\n    meta.type = fileType\n\n    // `null` means the size is unknown.\n    const size = Number.isFinite(fileDescriptor.data.size) ? fileDescriptor.data.size : null\n\n    let newFile = {\n      source: fileDescriptor.source || '',\n      id: fileID,\n      name: fileName,\n      extension: fileExtension || '',\n      meta: {\n        ...this.getState().meta,\n        ...meta,\n      },\n      type: fileType,\n      data: fileDescriptor.data,\n      progress: {\n        percentage: 0,\n        bytesUploaded: 0,\n        bytesTotal: size,\n        uploadComplete: false,\n        uploadStarted: null,\n      },\n      size,\n      isRemote,\n      remote: fileDescriptor.remote || '',\n      preview: fileDescriptor.preview,\n    }\n\n    const onBeforeFileAddedResult = this.opts.onBeforeFileAdded(newFile, files)\n\n    if (onBeforeFileAddedResult === false) {\n      // Don\u2019t show UI info for this error, as it should be done by the developer\n      this.#showOrLogErrorAndThrow(new RestrictionError('Cannot add the file because onBeforeFileAdded returned false.'), { showInformer: false, fileDescriptor })\n    } else if (typeof onBeforeFileAddedResult === 'object' && onBeforeFileAddedResult !== null) {\n      newFile = onBeforeFileAddedResult\n    }\n\n    try {\n      const filesArray = Object.keys(files).map(i => files[i])\n      this.#checkRestrictions(newFile, filesArray)\n    } catch (err) {\n      this.#showOrLogErrorAndThrow(err, { file: newFile })\n    }\n\n    return newFile\n  }\n\n  // Schedule an upload if `autoProceed` is enabled.\n  #startIfAutoProceed () {\n    if (this.opts.autoProceed && !this.scheduledAutoProceed) {\n      this.scheduledAutoProceed = setTimeout(() => {\n        this.scheduledAutoProceed = null\n        this.upload().catch((err) => {\n          if (!err.isRestriction) {\n            this.log(err.stack || err.message || err)\n          }\n        })\n      }, 4)\n    }\n  }\n\n  /**\n   * Add a new file to `state.files`. This will run `onBeforeFileAdded`,\n   * try to guess file type in a clever way, check file against restrictions,\n   * and start an upload if `autoProceed === true`.\n   *\n   * @param {object} file object to add\n   * @returns {string} id for the added file\n   */\n  addFile (file) {\n    this.#assertNewUploadAllowed(file)\n\n    const { files } = this.getState()\n    let newFile = this.#checkAndCreateFileStateObject(files, file)\n\n    // Users are asked to re-select recovered files without data,\n    // and to keep the progress, meta and everthing else, we only replace said data\n    if (files[newFile.id] && files[newFile.id].isGhost) {\n      newFile = {\n        ...files[newFile.id],\n        data: file.data,\n        isGhost: false,\n      }\n      this.log(`Replaced the blob in the restored ghost file: ${newFile.name}, ${newFile.id}`)\n    }\n\n    this.setState({\n      files: {\n        ...files,\n        [newFile.id]: newFile,\n      },\n    })\n\n    this.emit('file-added', newFile)\n    this.emit('files-added', [newFile])\n    this.log(`Added file: ${newFile.name}, ${newFile.id}, mime type: ${newFile.type}`)\n\n    this.#startIfAutoProceed()\n\n    return newFile.id\n  }\n\n  /**\n   * Add multiple files to `state.files`. See the `addFile()` documentation.\n   *\n   * If an error occurs while adding a file, it is logged and the user is notified.\n   * This is good for UI plugins, but not for programmatic use.\n   * Programmatic users should usually still use `addFile()` on individual files.\n   */\n  addFiles (fileDescriptors) {\n    this.#assertNewUploadAllowed()\n\n    // create a copy of the files object only once\n    const files = { ...this.getState().files }\n    const newFiles = []\n    const errors = []\n    for (let i = 0; i < fileDescriptors.length; i++) {\n      try {\n        let newFile = this.#checkAndCreateFileStateObject(files, fileDescriptors[i])\n        // Users are asked to re-select recovered files without data,\n        // and to keep the progress, meta and everthing else, we only replace said data\n        if (files[newFile.id] && files[newFile.id].isGhost) {\n          newFile = {\n            ...files[newFile.id],\n            data: fileDescriptors[i].data,\n            isGhost: false,\n          }\n          this.log(`Replaced blob in a ghost file: ${newFile.name}, ${newFile.id}`)\n        }\n        files[newFile.id] = newFile\n        newFiles.push(newFile)\n      } catch (err) {\n        if (!err.isRestriction) {\n          errors.push(err)\n        }\n      }\n    }\n\n    this.setState({ files })\n\n    newFiles.forEach((newFile) => {\n      this.emit('file-added', newFile)\n    })\n\n    this.emit('files-added', newFiles)\n\n    if (newFiles.length > 5) {\n      this.log(`Added batch of ${newFiles.length} files`)\n    } else {\n      Object.keys(newFiles).forEach(fileID => {\n        this.log(`Added file: ${newFiles[fileID].name}\\n id: ${newFiles[fileID].id}\\n type: ${newFiles[fileID].type}`)\n      })\n    }\n\n    if (newFiles.length > 0) {\n      this.#startIfAutoProceed()\n    }\n\n    if (errors.length > 0) {\n      let message = 'Multiple errors occurred while adding files:\\n'\n      errors.forEach((subError) => {\n        message += `\\n * ${subError.message}`\n      })\n\n      this.info({\n        message: this.i18n('addBulkFilesFailed', { smart_count: errors.length }),\n        details: message,\n      }, 'error', this.opts.infoTimeout)\n\n      if (typeof AggregateError === 'function') {\n        throw new AggregateError(errors, message)\n      } else {\n        const err = new Error(message)\n        err.errors = errors\n        throw err\n      }\n    }\n  }\n\n  removeFiles (fileIDs, reason) {\n    const { files, currentUploads } = this.getState()\n    const updatedFiles = { ...files }\n    const updatedUploads = { ...currentUploads }\n\n    const removedFiles = Object.create(null)\n    fileIDs.forEach((fileID) => {\n      if (files[fileID]) {\n        removedFiles[fileID] = files[fileID]\n        delete updatedFiles[fileID]\n      }\n    })\n\n    // Remove files from the `fileIDs` list in each upload.\n    function fileIsNotRemoved (uploadFileID) {\n      return removedFiles[uploadFileID] === undefined\n    }\n\n    Object.keys(updatedUploads).forEach((uploadID) => {\n      const newFileIDs = currentUploads[uploadID].fileIDs.filter(fileIsNotRemoved)\n\n      // Remove the upload if no files are associated with it anymore.\n      if (newFileIDs.length === 0) {\n        delete updatedUploads[uploadID]\n        return\n      }\n\n      updatedUploads[uploadID] = {\n        ...currentUploads[uploadID],\n        fileIDs: newFileIDs,\n      }\n    })\n\n    const stateUpdate = {\n      currentUploads: updatedUploads,\n      files: updatedFiles,\n    }\n\n    // If all files were removed - allow new uploads,\n    // and clear recoveredState\n    if (Object.keys(updatedFiles).length === 0) {\n      stateUpdate.allowNewUpload = true\n      stateUpdate.error = null\n      stateUpdate.recoveredState = null\n    }\n\n    this.setState(stateUpdate)\n    this.calculateTotalProgress()\n\n    const removedFileIDs = Object.keys(removedFiles)\n    removedFileIDs.forEach((fileID) => {\n      this.emit('file-removed', removedFiles[fileID], reason)\n    })\n\n    if (removedFileIDs.length > 5) {\n      this.log(`Removed ${removedFileIDs.length} files`)\n    } else {\n      this.log(`Removed files: ${removedFileIDs.join(', ')}`)\n    }\n  }\n\n  removeFile (fileID, reason = null) {\n    this.removeFiles([fileID], reason)\n  }\n\n  pauseResume (fileID) {\n    if (!this.getState().capabilities.resumableUploads\n         || this.getFile(fileID).uploadComplete) {\n      return undefined\n    }\n\n    const wasPaused = this.getFile(fileID).isPaused || false\n    const isPaused = !wasPaused\n\n    this.setFileState(fileID, {\n      isPaused,\n    })\n\n    this.emit('upload-pause', fileID, isPaused)\n\n    return isPaused\n  }\n\n  pauseAll () {\n    const updatedFiles = { ...this.getState().files }\n    const inProgressUpdatedFiles = Object.keys(updatedFiles).filter((file) => {\n      return !updatedFiles[file].progress.uploadComplete\n             && updatedFiles[file].progress.uploadStarted\n    })\n\n    inProgressUpdatedFiles.forEach((file) => {\n      const updatedFile = { ...updatedFiles[file], isPaused: true }\n      updatedFiles[file] = updatedFile\n    })\n\n    this.setState({ files: updatedFiles })\n    this.emit('pause-all')\n  }\n\n  resumeAll () {\n    const updatedFiles = { ...this.getState().files }\n    const inProgressUpdatedFiles = Object.keys(updatedFiles).filter((file) => {\n      return !updatedFiles[file].progress.uploadComplete\n             && updatedFiles[file].progress.uploadStarted\n    })\n\n    inProgressUpdatedFiles.forEach((file) => {\n      const updatedFile = {\n        ...updatedFiles[file],\n        isPaused: false,\n        error: null,\n      }\n      updatedFiles[file] = updatedFile\n    })\n    this.setState({ files: updatedFiles })\n\n    this.emit('resume-all')\n  }\n\n  retryAll () {\n    const updatedFiles = { ...this.getState().files }\n    const filesToRetry = Object.keys(updatedFiles).filter(file => {\n      return updatedFiles[file].error\n    })\n\n    filesToRetry.forEach((file) => {\n      const updatedFile = {\n        ...updatedFiles[file],\n        isPaused: false,\n        error: null,\n      }\n      updatedFiles[file] = updatedFile\n    })\n    this.setState({\n      files: updatedFiles,\n      error: null,\n    })\n\n    this.emit('retry-all', filesToRetry)\n\n    if (filesToRetry.length === 0) {\n      return Promise.resolve({\n        successful: [],\n        failed: [],\n      })\n    }\n\n    const uploadID = this.#createUpload(filesToRetry, {\n      forceAllowNewUpload: true, // create new upload even if allowNewUpload: false\n    })\n    return this.#runUpload(uploadID)\n  }\n\n  cancelAll () {\n    this.emit('cancel-all')\n\n    const { files } = this.getState()\n\n    const fileIDs = Object.keys(files)\n    if (fileIDs.length) {\n      this.removeFiles(fileIDs, 'cancel-all')\n    }\n\n    this.setState({\n      totalProgress: 0,\n      error: null,\n      recoveredState: null,\n    })\n  }\n\n  retryUpload (fileID) {\n    this.setFileState(fileID, {\n      error: null,\n      isPaused: false,\n    })\n\n    this.emit('upload-retry', fileID)\n\n    const uploadID = this.#createUpload([fileID], {\n      forceAllowNewUpload: true, // create new upload even if allowNewUpload: false\n    })\n    return this.#runUpload(uploadID)\n  }\n\n  reset () {\n    this.cancelAll()\n  }\n\n  logout () {\n    this.iteratePlugins(plugin => {\n      if (plugin.provider && plugin.provider.logout) {\n        plugin.provider.logout()\n      }\n    })\n  }\n\n  calculateProgress (file, data) {\n    if (!this.getFile(file.id)) {\n      this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n      return\n    }\n\n    // bytesTotal may be null or zero; in that case we can't divide by it\n    const canHavePercentage = Number.isFinite(data.bytesTotal) && data.bytesTotal > 0\n    this.setFileState(file.id, {\n      progress: {\n        ...this.getFile(file.id).progress,\n        bytesUploaded: data.bytesUploaded,\n        bytesTotal: data.bytesTotal,\n        percentage: canHavePercentage\n          ? Math.round((data.bytesUploaded / data.bytesTotal) * 100)\n          : 0,\n      },\n    })\n\n    this.calculateTotalProgress()\n  }\n\n  calculateTotalProgress () {\n    // calculate total progress, using the number of files currently uploading,\n    // multiplied by 100 and the summ of individual progress of each file\n    const files = this.getFiles()\n\n    const inProgress = files.filter((file) => {\n      return file.progress.uploadStarted\n        || file.progress.preprocess\n        || file.progress.postprocess\n    })\n\n    if (inProgress.length === 0) {\n      this.emit('progress', 0)\n      this.setState({ totalProgress: 0 })\n      return\n    }\n\n    const sizedFiles = inProgress.filter((file) => file.progress.bytesTotal != null)\n    const unsizedFiles = inProgress.filter((file) => file.progress.bytesTotal == null)\n\n    if (sizedFiles.length === 0) {\n      const progressMax = inProgress.length * 100\n      const currentProgress = unsizedFiles.reduce((acc, file) => {\n        return acc + file.progress.percentage\n      }, 0)\n      const totalProgress = Math.round((currentProgress / progressMax) * 100)\n      this.setState({ totalProgress })\n      return\n    }\n\n    let totalSize = sizedFiles.reduce((acc, file) => {\n      return acc + file.progress.bytesTotal\n    }, 0)\n    const averageSize = totalSize / sizedFiles.length\n    totalSize += averageSize * unsizedFiles.length\n\n    let uploadedSize = 0\n    sizedFiles.forEach((file) => {\n      uploadedSize += file.progress.bytesUploaded\n    })\n    unsizedFiles.forEach((file) => {\n      uploadedSize += (averageSize * (file.progress.percentage || 0)) / 100\n    })\n\n    let totalProgress = totalSize === 0\n      ? 0\n      : Math.round((uploadedSize / totalSize) * 100)\n\n    // hot fix, because:\n    // uploadedSize ended up larger than totalSize, resulting in 1325% total\n    if (totalProgress > 100) {\n      totalProgress = 100\n    }\n\n    this.setState({ totalProgress })\n    this.emit('progress', totalProgress)\n  }\n\n  /**\n   * Registers listeners for all global actions, like:\n   * `error`, `file-removed`, `upload-progress`\n   */\n  #addListeners () {\n    /**\n     * @param {Error} error\n     * @param {object} [file]\n     * @param {object} [response]\n     */\n    const errorHandler = (error, file, response) => {\n      let errorMsg = error.message || 'Unknown error'\n      if (error.details) {\n        errorMsg += ` ${error.details}`\n      }\n\n      this.setState({ error: errorMsg })\n\n      if (file != null && file.id in this.getState().files) {\n        this.setFileState(file.id, {\n          error: errorMsg,\n          response,\n        })\n      }\n    }\n\n    this.on('error', errorHandler)\n\n    this.on('upload-error', (file, error, response) => {\n      errorHandler(error, file, response)\n\n      if (typeof error === 'object' && error.message) {\n        const newError = new Error(error.message)\n        newError.details = error.message\n        if (error.details) {\n          newError.details += ` ${error.details}`\n        }\n        newError.message = this.i18n('failedToUpload', { file: file.name })\n        this.#showOrLogErrorAndThrow(newError, {\n          throwErr: false,\n        })\n      } else {\n        this.#showOrLogErrorAndThrow(error, {\n          throwErr: false,\n        })\n      }\n    })\n\n    this.on('upload', () => {\n      this.setState({ error: null })\n    })\n\n    this.on('upload-started', (file) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n      this.setFileState(file.id, {\n        progress: {\n          uploadStarted: Date.now(),\n          uploadComplete: false,\n          percentage: 0,\n          bytesUploaded: 0,\n          bytesTotal: file.size,\n        },\n      })\n    })\n\n    this.on('upload-progress', this.calculateProgress)\n\n    this.on('upload-success', (file, uploadResp) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n\n      const currentProgress = this.getFile(file.id).progress\n      this.setFileState(file.id, {\n        progress: {\n          ...currentProgress,\n          postprocess: this.#postProcessors.size > 0 ? {\n            mode: 'indeterminate',\n          } : null,\n          uploadComplete: true,\n          percentage: 100,\n          bytesUploaded: currentProgress.bytesTotal,\n        },\n        response: uploadResp,\n        uploadURL: uploadResp.uploadURL,\n        isPaused: false,\n      })\n\n      // Remote providers sometimes don't tell us the file size,\n      // but we can know how many bytes we uploaded once the upload is complete.\n      if (file.size == null) {\n        this.setFileState(file.id, {\n          size: uploadResp.bytesUploaded || currentProgress.bytesTotal,\n        })\n      }\n\n      this.calculateTotalProgress()\n    })\n\n    this.on('preprocess-progress', (file, progress) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n      this.setFileState(file.id, {\n        progress: { ...this.getFile(file.id).progress, preprocess: progress },\n      })\n    })\n\n    this.on('preprocess-complete', (file) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n      const files = { ...this.getState().files }\n      files[file.id] = { ...files[file.id], progress: { ...files[file.id].progress } }\n      delete files[file.id].progress.preprocess\n\n      this.setState({ files })\n    })\n\n    this.on('postprocess-progress', (file, progress) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n      this.setFileState(file.id, {\n        progress: { ...this.getState().files[file.id].progress, postprocess: progress },\n      })\n    })\n\n    this.on('postprocess-complete', (file) => {\n      if (!this.getFile(file.id)) {\n        this.log(`Not setting progress for a file that has been removed: ${file.id}`)\n        return\n      }\n      const files = {\n        ...this.getState().files,\n      }\n      files[file.id] = {\n        ...files[file.id],\n        progress: {\n          ...files[file.id].progress,\n        },\n      }\n      delete files[file.id].progress.postprocess\n\n      this.setState({ files })\n    })\n\n    this.on('restored', () => {\n      // Files may have changed--ensure progress is still accurate.\n      this.calculateTotalProgress()\n    })\n\n    // show informer if offline\n    if (typeof window !== 'undefined' && window.addEventListener) {\n      window.addEventListener('online', this.#updateOnlineStatus)\n      window.addEventListener('offline', this.#updateOnlineStatus)\n      setTimeout(this.#updateOnlineStatus, 3000)\n    }\n  }\n\n  updateOnlineStatus () {\n    const online\n      = typeof window.navigator.onLine !== 'undefined'\n        ? window.navigator.onLine\n        : true\n    if (!online) {\n      this.emit('is-offline')\n      this.info(this.i18n('noInternetConnection'), 'error', 0)\n      this.wasOffline = true\n    } else {\n      this.emit('is-online')\n      if (this.wasOffline) {\n        this.emit('back-online')\n        this.info(this.i18n('connectedToInternet'), 'success', 3000)\n        this.wasOffline = false\n      }\n    }\n  }\n\n  #updateOnlineStatus = this.updateOnlineStatus.bind(this)\n\n  getID () {\n    return this.opts.id\n  }\n\n  /**\n   * Registers a plugin with Core.\n   *\n   * @param {object} Plugin object\n   * @param {object} [opts] object with options to be passed to Plugin\n   * @returns {object} self for chaining\n   */\n  // eslint-disable-next-line no-shadow\n  use (Plugin, opts) {\n    if (typeof Plugin !== 'function') {\n      const msg = `Expected a plugin class, but got ${Plugin === null ? 'null' : typeof Plugin}.`\n        + ' Please verify that the plugin was imported and spelled correctly.'\n      throw new TypeError(msg)\n    }\n\n    // Instantiate\n    const plugin = new Plugin(this, opts)\n    const pluginId = plugin.id\n\n    if (!pluginId) {\n      throw new Error('Your plugin must have an id')\n    }\n\n    if (!plugin.type) {\n      throw new Error('Your plugin must have a type')\n    }\n\n    const existsPluginAlready = this.getPlugin(pluginId)\n    if (existsPluginAlready) {\n      const msg = `Already found a plugin named '${existsPluginAlready.id}'. `\n        + `Tried to use: '${pluginId}'.\\n`\n        + 'Uppy plugins must have unique `id` options. See https://uppy.io/docs/plugins/#id.'\n      throw new Error(msg)\n    }\n\n    if (Plugin.VERSION) {\n      this.log(`Using ${pluginId} v${Plugin.VERSION}`)\n    }\n\n    if (plugin.type in this.#plugins) {\n      this.#plugins[plugin.type].push(plugin)\n    } else {\n      this.#plugins[plugin.type] = [plugin]\n    }\n    plugin.install()\n\n    return this\n  }\n\n  /**\n   * Find one Plugin by name.\n   *\n   * @param {string} id plugin id\n   * @returns {BasePlugin|undefined}\n   */\n  getPlugin (id) {\n    for (const plugins of Object.values(this.#plugins)) {\n      const foundPlugin = plugins.find(plugin => plugin.id === id)\n      if (foundPlugin != null) return foundPlugin\n    }\n    return undefined\n  }\n\n  [Symbol.for('uppy test: getPlugins')] (type) {\n    return this.#plugins[type]\n  }\n\n  /**\n   * Iterate through all `use`d plugins.\n   *\n   * @param {Function} method that will be run on each plugin\n   */\n  iteratePlugins (method) {\n    Object.values(this.#plugins).flat(1).forEach(method)\n  }\n\n  /**\n   * Uninstall and remove a plugin.\n   *\n   * @param {object} instance The plugin instance to remove.\n   */\n  removePlugin (instance) {\n    this.log(`Removing plugin ${instance.id}`)\n    this.emit('plugin-remove', instance)\n\n    if (instance.uninstall) {\n      instance.uninstall()\n    }\n\n    const list = this.#plugins[instance.type]\n    // list.indexOf failed here, because Vue3 converted the plugin instance\n    // to a Proxy object, which failed the strict comparison test:\n    // obj !== objProxy\n    const index = list.findIndex(item => item.id === instance.id)\n    if (index !== -1) {\n      list.splice(index, 1)\n    }\n\n    const state = this.getState()\n    const updatedState = {\n      plugins: {\n        ...state.plugins,\n        [instance.id]: undefined,\n      },\n    }\n    this.setState(updatedState)\n  }\n\n  /**\n   * Uninstall all plugins and close down this Uppy instance.\n   */\n  close () {\n    this.log(`Closing Uppy instance ${this.opts.id}: removing all files and uninstalling plugins`)\n\n    this.reset()\n\n    this.#storeUnsubscribe()\n\n    this.iteratePlugins((plugin) => {\n      this.removePlugin(plugin)\n    })\n\n    if (typeof window !== 'undefined' && window.removeEventListener) {\n      window.removeEventListener('online', this.#updateOnlineStatus)\n      window.removeEventListener('offline', this.#updateOnlineStatus)\n    }\n  }\n\n  hideInfo () {\n    const { info } = this.getState()\n\n    this.setState({ info: info.slice(1) })\n\n    this.emit('info-hidden')\n  }\n\n  /**\n   * Set info message in `state.info`, so that UI plugins like `Informer`\n   * can display the message.\n   *\n   * @param {string | object} message Message to be displayed by the informer\n   * @param {string} [type]\n   * @param {number} [duration]\n   */\n  info (message, type = 'info', duration = 3000) {\n    const isComplexMessage = typeof message === 'object'\n\n    this.setState({\n      info: [\n        ...this.getState().info,\n        {\n          type,\n          message: isComplexMessage ? message.message : message,\n          details: isComplexMessage ? message.details : null,\n        },\n      ],\n    })\n\n    setTimeout(() => this.hideInfo(), duration)\n\n    this.emit('info-visible')\n  }\n\n  /**\n   * Passes messages to a function, provided in `opts.logger`.\n   * If `opts.logger: Uppy.debugLogger` or `opts.debug: true`, logs to the browser console.\n   *\n   * @param {string|object} message to log\n   * @param {string} [type] optional `error` or `warning`\n   */\n  log (message, type) {\n    const { logger } = this.opts\n    switch (type) {\n      case 'error': logger.error(message); break\n      case 'warning': logger.warn(message); break\n      default: logger.debug(message); break\n    }\n  }\n\n  /**\n   * Restore an upload by its ID.\n   */\n  restore (uploadID) {\n    this.log(`Core: attempting to restore upload \"${uploadID}\"`)\n\n    if (!this.getState().currentUploads[uploadID]) {\n      this.#removeUpload(uploadID)\n      return Promise.reject(new Error('Nonexistent upload'))\n    }\n\n    return this.#runUpload(uploadID)\n  }\n\n  /**\n   * Create an upload for a bunch of files.\n   *\n   * @param {Array<string>} fileIDs File IDs to include in this upload.\n   * @returns {string} ID of this upload.\n   */\n  #createUpload (fileIDs, opts = {}) {\n    // uppy.retryAll sets this to true \u2014 when retrying we want to ignore `allowNewUpload: false`\n    const { forceAllowNewUpload = false } = opts\n\n    const { allowNewUpload, currentUploads } = this.getState()\n    if (!allowNewUpload && !forceAllowNewUpload) {\n      throw new Error('Cannot create a new upload: already uploading.')\n    }\n\n    const uploadID = nanoid()\n\n    this.emit('upload', {\n      id: uploadID,\n      fileIDs,\n    })\n\n    this.setState({\n      allowNewUpload: this.opts.allowMultipleUploadBatches !== false && this.opts.allowMultipleUploads !== false,\n\n      currentUploads: {\n        ...currentUploads,\n        [uploadID]: {\n          fileIDs,\n          step: 0,\n          result: {},\n        },\n      },\n    })\n\n    return uploadID\n  }\n\n  [Symbol.for('uppy test: createUpload')] (...args) { return this.#createUpload(...args) }\n\n  #getUpload (uploadID) {\n    const { currentUploads } = this.getState()\n\n    return currentUploads[uploadID]\n  }\n\n  /**\n   * Add data to an upload's result object.\n   *\n   * @param {string} uploadID The ID of the upload.\n   * @param {object} data Data properties to add to the result object.\n   */\n  addResultData (uploadID, data) {\n    if (!this.#getUpload(uploadID)) {\n      this.log(`Not setting result for an upload that has been removed: ${uploadID}`)\n      return\n    }\n    const { currentUploads } = this.getState()\n    const currentUpload = { ...currentUploads[uploadID], result: { ...currentUploads[uploadID].result, ...data } }\n    this.setState({\n      currentUploads: { ...currentUploads, [uploadID]: currentUpload },\n    })\n  }\n\n  /**\n   * Remove an upload, eg. if it has been canceled or completed.\n   *\n   * @param {string} uploadID The ID of the upload.\n   */\n  #removeUpload (uploadID) {\n    const currentUploads = { ...this.getState().currentUploads }\n    delete currentUploads[uploadID]\n\n    this.setState({\n      currentUploads,\n    })\n  }\n\n  /**\n   * Run an upload. This picks up where it left off in case the upload is being restored.\n   *\n   * @private\n   */\n  async #runUpload (uploadID) {\n    let { currentUploads } = this.getState()\n    let currentUpload = currentUploads[uploadID]\n    const restoreStep = currentUpload.step || 0\n\n    const steps = [\n      ...this.#preProcessors,\n      ...this.#uploaders,\n      ...this.#postProcessors,\n    ]\n    try {\n      for (let step = restoreStep; step < steps.length; step++) {\n        if (!currentUpload) {\n          break\n        }\n        const fn = steps[step]\n\n        const updatedUpload = {\n          ...currentUpload,\n          step,\n        }\n\n        this.setState({\n          currentUploads: {\n            ...currentUploads,\n            [uploadID]: updatedUpload,\n          },\n        })\n\n        // TODO give this the `updatedUpload` object as its only parameter maybe?\n        // Otherwise when more metadata may be added to the upload this would keep getting more parameters\n        await fn(updatedUpload.fileIDs, uploadID)\n\n        // Update currentUpload value in case it was modified asynchronously.\n        currentUploads = this.getState().currentUploads\n        currentUpload = currentUploads[uploadID]\n      }\n    } catch (err) {\n      this.emit('error', err)\n      this.#removeUpload(uploadID)\n      throw err\n    }\n\n    // Set result data.\n    if (currentUpload) {\n      // Mark postprocessing step as complete if necessary; this addresses a case where we might get\n      // stuck in the postprocessing UI while the upload is fully complete.\n      // If the postprocessing steps do not do any work, they may not emit postprocessing events at\n      // all, and never mark the postprocessing as complete. This is fine on its own but we\n      // introduced code in the @uppy/core upload-success handler to prepare postprocessing progress\n      // state if any postprocessors are registered. That is to avoid a \"flash of completed state\"\n      // before the postprocessing plugins can emit events.\n      //\n      // So, just in case an upload with postprocessing plugins *has* completed *without* emitting\n      // postprocessing completion, we do it instead.\n      currentUpload.fileIDs.forEach((fileID) => {\n        const file = this.getFile(fileID)\n        if (file && file.progress.postprocess) {\n          this.emit('postprocess-complete', file)\n        }\n      })\n\n      const files = currentUpload.fileIDs.map((fileID) => this.getFile(fileID))\n      const successful = files.filter((file) => !file.error)\n      const failed = files.filter((file) => file.error)\n      await this.addResultData(uploadID, { successful, failed, uploadID })\n\n      // Update currentUpload value in case it was modified asynchronously.\n      currentUploads = this.getState().currentUploads\n      currentUpload = currentUploads[uploadID]\n    }\n    // Emit completion events.\n    // This is in a separate function so that the `currentUploads` variable\n    // always refers to the latest state. In the handler right above it refers\n    // to an outdated object without the `.result` property.\n    let result\n    if (currentUpload) {\n      result = currentUpload.result\n      this.emit('complete', result)\n\n      this.#removeUpload(uploadID)\n    }\n    if (result == null) {\n      this.log(`Not setting result for an upload that has been removed: ${uploadID}`)\n    }\n    return result\n  }\n\n  /**\n   * Start an upload for all the files that are not currently being uploaded.\n   *\n   * @returns {Promise}\n   */\n  upload () {\n    if (!this.#plugins.uploader?.length) {\n      this.log('No uploader type plugins are used', 'warning')\n    }\n\n    let { files } = this.getState()\n\n    const onBeforeUploadResult = this.opts.onBeforeUpload(files)\n\n    if (onBeforeUploadResult === false) {\n      return Promise.reject(new Error('Not starting the upload because onBeforeUpload returned false'))\n    }\n\n    if (onBeforeUploadResult && typeof onBeforeUploadResult === 'object') {\n      files = onBeforeUploadResult\n      // Updating files in state, because uploader plugins receive file IDs,\n      // and then fetch the actual file object from state\n      this.setState({\n        files,\n      })\n    }\n\n    return Promise.resolve()\n      .then(() => {\n        this.#checkMinNumberOfFiles(files)\n        this.#checkRequiredMetaFields(files)\n      })\n      .catch((err) => {\n        this.#showOrLogErrorAndThrow(err)\n      })\n      .then(() => {\n        const { currentUploads } = this.getState()\n        // get a list of files that are currently assigned to uploads\n        const currentlyUploadingFiles = Object.values(currentUploads).flatMap(curr => curr.fileIDs)\n\n        const waitingFileIDs = []\n        Object.keys(files).forEach((fileID) => {\n          const file = this.getFile(fileID)\n          // if the file hasn't started uploading and hasn't already been assigned to an upload..\n          if ((!file.progress.uploadStarted) && (currentlyUploadingFiles.indexOf(fileID) === -1)) {\n            waitingFileIDs.push(file.id)\n          }\n        })\n\n        const uploadID = this.#createUpload(waitingFileIDs)\n        return this.#runUpload(uploadID)\n      })\n      .catch((err) => {\n        this.#showOrLogErrorAndThrow(err, {\n          showInformer: false,\n        })\n      })\n  }\n}\n\nmodule.exports = Uppy\n", "idx": 1, "id": 14873, "msg": "Why are we removing this? I think we should keep it.", "proj": "transloadit-uppy", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -214,9 +214,9 @@\n         data: function() {\n             return {\n                 pushNotificationTabs: [\n-                    {title: CV.i18n('push-notification.tab-one-time'), name: countlyPushNotification.service.TypeEnum.ONE_TIME, component: PushNotificationTabView},\n-                    {title: CV.i18n('push-notification.tab-automatic'), name: countlyPushNotification.service.TypeEnum.AUTOMATIC, component: PushNotificationTabView},\n-                    {title: CV.i18n('push-notification.tab-transactional'), name: countlyPushNotification.service.TypeEnum.TRANSACTIONAL, component: PushNotificationTabView}\n+                    {title: CV.i18n('push-notification.one-time'), name: countlyPushNotification.service.TypeEnum.ONE_TIME, component: PushNotificationTabView},\n+                    {title: CV.i18n('push-notification.automatic'), name: countlyPushNotification.service.TypeEnum.AUTOMATIC, component: PushNotificationTabView},\n+                    {title: CV.i18n('push-notification.transactional'), name: countlyPushNotification.service.TypeEnum.TRANSACTIONAL, component: PushNotificationTabView}\n                 ]\n             };\n         },", "y": 0, "oldf": "/* global countlyVue,app,CV,countlyPushNotification,CountlyHelpers,jQuery,countlyManagementView,countlyCommon,$,countlyGlobal,countlyAuth,countlySegmentation,countlyUserdata, components,Backbone*/\n(function() {\n\n    var AUTOMATIC_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS = [\n        {label: CV.i18n(\"push-notification.table-filter-scheduled\"), value: countlyPushNotification.service.StatusEnum.SCHEDULED},\n        {label: CV.i18n(\"push-notification.table-filter-all\"), value: countlyPushNotification.service.StatusEnum.ALL},\n        {label: CV.i18n(\"push-notification.table-filter-sent\"), value: countlyPushNotification.service.StatusEnum.SENT},\n        {label: CV.i18n(\"push-notification.table-filter-sending\"), value: countlyPushNotification.service.StatusEnum.SENDING},\n        {label: CV.i18n(\"push-notification.table-filter-aborted\"), value: countlyPushNotification.service.StatusEnum.ABORTED},\n        {label: CV.i18n(\"push-notification.table-filter-failed\"), value: countlyPushNotification.service.StatusEnum.FAILED},\n        {label: CV.i18n(\"push-notification.table-filter-stopped\"), value: countlyPushNotification.service.StatusEnum.STOPPED}\n    ];\n    var TRANSACTIONAL_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS = AUTOMATIC_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS;\n\n    var ONE_TIME_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS = AUTOMATIC_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS.concat([\n        {label: CV.i18n(\"push-notification.table-filter-draft\"), value: countlyPushNotification.service.StatusEnum.DRAFT},\n        {label: CV.i18n(\"push-notification.table-filter-not-approved\"), value: countlyPushNotification.service.StatusEnum.NOT_APPROVED},\n    ]);\n\n    var statusFilterOptions = {\n        oneTime: ONE_TIME_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS,\n        automatic: AUTOMATIC_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS,\n        transactional: TRANSACTIONAL_PUSH_NOTIFICATION_STATUS_FILTER_OPTIONS\n    };\n\n    var platformFilterOptions = [\n        {label: CV.i18n(\"push-notification.platform-all\"), value: countlyPushNotification.service.PlatformEnum.ALL},\n        {label: CV.i18n(\"push-notification.platform-android\"), value: countlyPushNotification.service.PlatformEnum.ANDROID},\n        {label: CV.i18n(\"push-notification.platform-ios\"), value: countlyPushNotification.service.PlatformEnum.IOS}\n    ];\n\n    var oneTimePeriodFilterOptions = [\n        {label: CV.i18n(\"push-notification.time-chart-period-weekly\"), value: countlyPushNotification.service.PeriodEnum.WEEKLY},\n        {label: CV.i18n(\"push-notification.time-chart-period-monthly\"), value: countlyPushNotification.service.PeriodEnum.MONTHLY},\n    ];\n    var automaticPeriodFilterOptions = [{label: CV.i18n(\"push-notification.time-chart-period-daily\"), value: countlyPushNotification.service.PeriodEnum.DAILY}];\n    var transactionalPeriodFilterOptions = [{label: CV.i18n(\"push-notification.time-chart-period-daily\"), value: countlyPushNotification.service.PeriodEnum.DAILY}];\n\n\n    var PushNotificationTabView = countlyVue.views.BaseView.extend({\n        template: \"#push-notification-tab\",\n        data: function() {\n            return {\n                platformFilters: platformFilterOptions,\n                platformFilterLabels: {\n                    oneTime: CV.i18n('push-notification.platform-filter-label-one-time'),\n                    automatic: CV.i18n('push-notification.platform-filter-label-automatic'),\n                    transactional: CV.i18n('push-notification.platform-filter-label-transactional')\n                },\n                statusFilters: statusFilterOptions,\n                DEFAULT_ALPHA_COLOR_VALUE_HEX: 50,\n                oneTimePeriodFilters: oneTimePeriodFilterOptions,\n                selectedOneTimePeriodFilter: countlyPushNotification.service.PeriodEnum.WEEKLY,\n                automaticPeriodFilters: automaticPeriodFilterOptions,\n                selectedAutomaticPeriodFilter: countlyPushNotification.service.PeriodEnum.DAILY,\n                transactionalPeriodFilters: transactionalPeriodFilterOptions,\n                selectedTransactionalPeriodFilter: countlyPushNotification.service.PeriodEnum.DAILY,\n                TypeEnum: countlyPushNotification.service.TypeEnum,\n                UserRowEventEnum: {\n                    RESEND: 'resend',\n                    DUPLICATE: 'duplicate',\n                    DELETE: 'delete'\n                },\n                optionalTableColumns: [\n                    {\n                        value: \"content\",\n                        label: CV.i18n('push-notification.table-message-content'),\n                        default: false\n                    },\n                    {\n                        value: \"createdBy\",\n                        label: CV.i18n('push-notification.table-created-by'),\n                        default: false\n                    }\n                ]\n            };\n        },\n        computed: {\n            selectedPushNotificationType: function() {\n                return this.$store.state.countlyPushNotification.selectedPushNotificationType;\n            },\n            pushNotifications: function() {\n                return this.$store.state.countlyPushNotification.pushNotifications;\n            },\n            isLoading: function() {\n                return this.$store.state.countlyPushNotification.isLoading;\n            },\n            pushNotificationRows: function() {\n                return this.pushNotifications.rows;\n            },\n            pushNotificationOptions: function() {\n                var series = this.yAxisPushNotificationSeries;\n                return {\n                    xAxis: {\n                        data: this.xAxisPushNotificationPeriods\n                    },\n                    series: series\n                };\n            },\n            totalAppUsers: function() {\n                return this.$store.state.countlyPushNotification.pushNotifications.totalAppUsers;\n            },\n            enabledUsers: function() {\n                return this.$store.state.countlyPushNotification.pushNotifications.enabledUsers;\n            },\n            xAxisPushNotificationPeriods: function() {\n                return this.$store.state.countlyPushNotification.pushNotifications.periods[this.selectedPeriodFilter];\n            },\n            yAxisPushNotificationSeries: function() {\n                return this.pushNotifications.series[this.selectedPeriodFilter].map(function(pushNotificationSerie) {\n                    return {\n                        data: pushNotificationSerie.data,\n                        name: pushNotificationSerie.label\n                    };\n                });\n            },\n            selectedStatusFilter: {\n                get: function() {\n                    return this.$store.state.countlyPushNotification.statusFilter;\n                },\n                set: function(value) {\n                    this.$store.dispatch(\"countlyPushNotification/onSetStatusFilter\", value);\n                //TODO: filter table by status\n                // this.$store.dispatch(\"countlyPushNotification/fetchByType\");\n                }\n            },\n            selectedPlatformFilter: {\n                get: function() {\n                    return this.$store.state.countlyPushNotification.platformFilter;\n                },\n                set: function(value) {\n                    this.$store.dispatch(\"countlyPushNotification/onSetPlatformFilter\", value);\n                    this.$store.dispatch(\"countlyPushNotification/fetchAll\");\n                }\n            },\n            selectedPlatformFilterLabel: function() {\n                return this.platformFilterLabel[this.selectedPushNotificationType];\n            },\n            selectedPeriodFilter: function() {\n                if (this.selectedPushNotificationType === countlyPushNotification.service.TypeEnum.ONE_TIME) {\n                    return this.selectedOneTimePeriodFilter;\n                }\n                else if (this.selectedPushNotificationType === countlyPushNotification.service.TypeEnum.AUTOMATIC) {\n                    return this.selectedAutomaticPeriodFilter;\n                }\n                else {\n                    return this.selectedTransactionalPeriodFilter;\n                }\n            }\n        },\n        methods: {\n            refresh: function() {\n                this.$store.dispatch('countlyPushNotification/fetchAll');\n            },\n            formatPercentage: function(value, decimalPlaces) {\n                return CountlyHelpers.formatPercentage(value, decimalPlaces);\n            },\n            handleUserRowEvents: function(event, pushNotificationId) {\n                if (event === this.UserRowEventEnum.DUPLICATE) {\n                    this.$store.dispatch('countlyPushNotification/onDuplicatePushNotification', pushNotificationId);\n                }\n                else if (event === this.UserRowEventEnum.RESEND) {\n                    this.$store.dispatch('countlyPushNotification/onResendPushNotification', pushNotificationId);\n                }\n                else {\n                    this.$store.dispatch('countlyPushNotification/onDeletePushNotification', pushNotificationId);\n                }\n            },\n            //TODO: use status action specifications when ready\n            // eslint-disable-next-line no-unused-vars\n            shouldShowDuplicateUserRowEvent: function(status) {\n                return true;\n            },\n            //TODO: use status action specifications when ready\n            // eslint-disable-next-line no-unused-vars\n            shouldShowResendUserRowEvent: function(status) {\n                return true;\n            },\n            //TODO: use status action specifications when ready\n            // eslint-disable-next-line no-unused-vars\n            shouldShowDeleteUserRowEvent: function(status) {\n                return true;\n            },\n            getStatusBackgroundColor: function(status) {\n                switch (status) {\n                case countlyPushNotification.service.StatusEnum.SENT: {\n                    return \"#12AF51\";\n                }\n                case countlyPushNotification.service.StatusEnum.ABORTED: {\n                    return \"#D23F00\";\n                }\n                case countlyPushNotification.service.StatusEnum.SCHEDULED: {\n                    return \"#CDAD7A\";\n                }\n                case countlyPushNotification.service.StatusEnum.STOPPED: {\n                    return \"#D23F00\";\n                }\n                default: {\n                    return \"#FFFFFF\";\n                }\n                }\n            },\n            onRowClick: function(row) {\n                window.location.hash = \"#/messaging/details/\" + row._id;\n            }\n        },\n        mounted: function() {\n            this.$store.dispatch('countlyPushNotification/fetchAll');\n        }\n    });\n\n    var PushNotificationView = countlyVue.views.BaseView.extend({\n        template: \"#push-notification\",\n        data: function() {\n            return {\n                pushNotificationTabs: [\n                    {title: CV.i18n('push-notification.tab-one-time'), name: countlyPushNotification.service.TypeEnum.ONE_TIME, component: PushNotificationTabView},\n                    {title: CV.i18n('push-notification.tab-automatic'), name: countlyPushNotification.service.TypeEnum.AUTOMATIC, component: PushNotificationTabView},\n                    {title: CV.i18n('push-notification.tab-transactional'), name: countlyPushNotification.service.TypeEnum.TRANSACTIONAL, component: PushNotificationTabView}\n                ]\n            };\n        },\n        computed: {\n            selectedPushNotificationTab: {\n                get: function() {\n                    return this.$store.state.countlyPushNotification.selectedPushNotificationType;\n                },\n                set: function(value) {\n                    this.$store.dispatch('countlyPushNotification/onSetPushNotificationType', value);\n                    this.$store.dispatch('countlyPushNotification/fetchAll');\n                }\n            }\n        },\n        methods: {\n            onCreatePushNotification: function() {},\n        }\n    });\n\n    var pushNotificationVuex = [{\n        clyModel: countlyPushNotification\n    }];\n\n    var pushNotificationViewWrapper = new countlyVue.views.BackboneWrapper({\n        component: PushNotificationView,\n        vuex: pushNotificationVuex,\n        templates: [\n            \"/push/templates/push-notification.html\",\n            \"/push/templates/push-notification-tab.html\"\n        ]\n    });\n\n    app.route('/messaging', 'messagingDashboardView', function() {\n        this.renderWhenReady(pushNotificationViewWrapper);\n    });\n\n    var PushNotificationDetailsView = countlyVue.views.BaseView.extend({\n        template: \"#push-notification-details\",\n        data: function() {\n            return {};\n        }\n    });\n\n    var pushNotificationDetailsViewWrapper = new countlyVue.views.BackboneWrapper({\n        component: PushNotificationDetailsView,\n        templates: [\n            \"/push/templates/push-notification-details.html\"\n        ]\n    });\n\n    app.route('/messaging/details/*id', \"messagingDetails\", function(id) {\n        pushNotificationDetailsViewWrapper.params = {\n            id: id\n        };\n        this.renderWhenReady(pushNotificationDetailsViewWrapper);\n    });\n\n\n    //countly.views application management settings\n    var featureName = 'push';\n\n    app.addAppManagementView('push', jQuery.i18n.map['push.plugin-title'], countlyManagementView.extend({\n        initialize: function() {\n            this.plugin = 'push';\n            this.templatePath = '/push/templates/push.html';\n            this.resetTemplateData();\n        },\n\n        resetTemplateData: function() {\n            var c = this.config();\n            if (c.i && c.i._id) {\n                this.templateData = {\n                    i: {\n                        _id: c.i._id,\n                        type: c.i.type,\n                        key: c.i.key,\n                        team: c.i.team,\n                        bundle: c.i.bundle,\n                        help: c.i.type === 'apn_universal' && c.i._id ? '<i class=\"fa fa-check-circle\"></i>' + jQuery.i18n.map['mgmt-plugins.push.uploaded.p12'] : c.i.type === 'apn_token' ? '<i class=\"fa fa-check-circle\"></i>' + jQuery.i18n.map['mgmt-plugins.push.uploaded.p8'] : ''\n                    // help: '<a href=\"' + countlyCommon.API_URL + '/i/pushes/download/' + c.i._id + '?api_key=' + countlyGlobal.member.api_key + '\">' + jQuery.i18n.map['mgmt-plugins.push.uploaded'] + '</a>. ' + (c.i.type === 'apn_universal' ? (jQuery.i18n.map['mgmt-plugins.push.uploaded.bundle'] + ' ' + c.i.bundle) : '')\n                    }\n                };\n            }\n            else {\n                this.templateData = {\n                    i: {\n                        type: 'apn_token',\n                        key: '',\n                        team: '',\n                        bundle: '',\n                    }\n                };\n            }\n            var t = c.a && c.a && c.a.key ? jQuery.i18n.map['mgmt-plugins.push.detected'] + ' ' + (c.a.key.length > 50 ? 'FCM' : 'GCM') : '';\n            this.templateData.a = {\n                _id: c.a && c.a._id || '',\n                key: c.a && c.a && c.a.key || '',\n                help: c.a && c.a && c.a.key && c.a.key.length > 50 ? t : '',\n                ehelp: c.a && c.a && c.a.key && c.a.key.length < 50 ? t : ''\n            };\n            this.templateData.h = {\n                _id: c.h && c.h._id || '',\n                key: c.h && c.h && c.h.key || '',\n                secret: c.h && c.h && c.h.secret || ''\n            };\n            this.templateData.rate = {\n                rate: c.rate && c.rate.rate || '',\n                period: c.rate && c.rate.period || ''\n            };\n        },\n\n        onChange: function(name, value) {\n            if (name === 'i.type') {\n                this.resetTemplateData();\n                countlyCommon.dot(this.templateData, name, value);\n                this.render();\n            }\n            else if (name === 'a.key' && value) {\n                this.templateData.a.type = value.length > 100 ? 'fcm' : 'gcm';\n                this.el.find('input[name=\"a.type\"]').val(this.templateData.a.type);\n            }\n            else if (name === 'i.pass' && !value) {\n                delete this.templateData.i.pass;\n            }\n        },\n\n        isSaveAvailable: function() {\n            var td = JSON.parse(JSON.stringify(this.templateData)),\n                std = JSON.parse(this.savedTemplateData);\n\n            if (td.i) {\n                delete td.i.pass;\n            }\n\n            if (std.i) {\n                delete std.i.pass;\n            }\n\n            return JSON.stringify(td) !== JSON.stringify(std);\n        },\n\n        validate: function() {\n            var i = this.config().i || {},\n                //a = this.config().a || {},\n                t = this.templateData;\n\n            if (t.i.type) {\n                if (t.i.file && t.i.file.length) {\n                    if (t.i.type === 'apn_token') {\n                        if (!t.i.key) {\n                            return jQuery.i18n.map['mgmt-plugins.push.error.nokey'];\n                        }\n                        if (!t.i.team) {\n                            return jQuery.i18n.map['mgmt-plugins.push.error.noteam'];\n                        }\n                        if (!t.i.bundle) {\n                            return jQuery.i18n.map['mgmt-plugins.push.error.nobundle'];\n                        }\n                    }\n                }\n                else {\n                    if (t.i.type === 'apn_token') {\n                        if ((t.i.key || '') !== (i.key || '') || (t.i.team || '') !== (i.team || '') || (t.i.bundle || '') !== (i.bundle || '')) {\n                            return jQuery.i18n.map['mgmt-plugins.push.error.nofile'];\n                        }\n                    }\n                }\n            }\n\n            if (!t.h.key && t.h.secret) {\n                return jQuery.i18n.map['mgmt-plugins.push.error.h.key'];\n            }\n            if (t.h.key && !t.h.secret) {\n                return jQuery.i18n.map['mgmt-plugins.push.error.h.secret'];\n            }\n            if (t.h.key && (parseInt(t.h.key) + '') !== t.h.key) {\n                return jQuery.i18n.map['mgmt-plugins.push.error.h.keynum'];\n            }\n        },\n\n        loadFile: function() {\n            var data = JSON.parse(JSON.stringify(this.templateData));\n\n            if (data.i.file) {\n                if (data.i.file.indexOf('.p8') === data.i.file.length - 3) {\n                    data.i.fileType = 'p8';\n                }\n                else if (data.i.file.indexOf('.p12') === data.i.file.length - 4) {\n                    data.i.fileType = 'p12';\n                }\n                else {\n                    return $.Deferred().reject('File type not supported');\n                }\n\n                var d = new $.Deferred(),\n                    reader = new window.FileReader();\n\n                reader.addEventListener('load', function() {\n                    data.i.file = reader.result;\n                    d.resolve({push: data});\n                });\n                reader.addEventListener('error', d.reject.bind(d));\n                reader.readAsDataURL(this.el.find('input[name=\"i.file\"]')[0].files[0]);\n\n                return d.promise();\n            }\n            else {\n                return $.when({push: data});\n            }\n        },\n\n        prepare: function() {\n        // var text = jQuery.i18n.map[\"plugins.confirm\"];\n        // var msg = { title: jQuery.i18n.map[\"plugins.processing\"], message: jQuery.i18n.map[\"plugins.wait\"], info: jQuery.i18n.map[\"plugins.hold-on\"], sticky: true };\n        // CountlyHelpers.confirm(text, \"popStyleGreen popStyleGreenWide\", function (result) {\n        //     if (!result) {\n        //         return true;\n        //     }\n        //     CountlyHelpers.notify(msg);\n        //     app.activeView.togglePlugin(plugins);\n        // },[jQuery.i18n.map[\"common.no-dont-continue\"],jQuery.i18n.map[\"plugins.yes-i-want-to-apply-changes\"]],{title:jQuery.i18n.map[\"plugins-apply-changes-to-plugins\"],image:\"apply-changes-to-plugins\"});\n            return this.loadFile().then(function(data) {\n                delete data.push.i.help;\n                delete data.push.a.help;\n\n                if (!data.push.i.file && !data.push.i._id) {\n                    data.push.i = null;\n                }\n                else if (data.push.i.file) {\n                    delete data.push.i._id;\n                }\n\n                if (!data.push.a.key) {\n                    data.push.a = null;\n                }\n\n                if (!data.push.h.key || !data.push.h.secret) {\n                    data.push.h = null;\n                }\n\n                return data;\n            });\n        }\n    }));\n\n    app.addPageScript('/drill#', function() {\n        if (Array.isArray(countlyGlobal.member.restrict) && countlyGlobal.member.restrict.indexOf('#/messaging') !== -1 || !countlyAuth.validateCreate(featureName)) {\n            return;\n        }\n        if (countlyGlobal.apps[countlyCommon.ACTIVE_APP_ID].type === 'mobile') {\n            if (countlyAuth.validateCreate(featureName)) {\n                var content =\n            '<div class=\"item\" id=\"action-create-message\">' +\n                '<div class=\"item-icon\">' +\n                    '<span class=\"logo ion-chatbox-working\"></span>' +\n                '</div>' +\n                '<div class=\"content\">' +\n                    '<div class=\"title\" data-localize=\"pu.send-message\"></div>' +\n                    '<div class=\"subtitle\" data-localize=\"pu.send-message-desc\"></div>' +\n                '</div>' +\n            '</div>';\n\n                $('#actions-popup').append(content);\n                app.localize();\n                $('#action-create-message').off('click').on('click', function() {\n                    var message = {\n                        apps: [countlyCommon.ACTIVE_APP_ID],\n                        drillConditions: countlySegmentation.getRequestData()\n                    };\n\n                    // for (var k in filterData.dbFilter) {\n                    //     if (k.indexOf('up.') === 0) message.conditions[k.substr(3).replace(\"cmp_\",\"cmp.\")] = filterData.dbFilter[k];\n                    // }\n\n                    components.push.popup.show(message);\n                    app.recordEvent({\n                        \"key\": \"drill-action\",\n                        \"count\": 1,\n                        \"segmentation\": {action: \"push\"}\n                    });\n                });\n                $('#bookmark-view').on('click', '.bookmark-action.send', function() {\n                    var filter = $(this).data('query');\n\n                    var message = {\n                        apps: [countlyCommon.ACTIVE_APP_ID],\n                        drillConditions: filter\n                    };\n\n                    // for (var k in filter) {\n                    //     if (k.indexOf('up.') === 0) message.conditions[k.substr(3).replace(\"cmp_\",\"cmp.\")] = filter[k];\n                    // }\n\n                    components.push.popup.show(message);\n                });\n            }\n            else {\n                $('#drill-actions').remove('.btn-create-message');\n            }\n        }\n    });\n    /**\n* Modify user profile views with push additions\n**/\n    function modifyUserDetailsForPush() {\n        if (Array.isArray(countlyGlobal.member.restrict) && countlyGlobal.member.restrict.indexOf('#/messaging') !== -1 || !countlyAuth.validateCreate(featureName)) {\n            return;\n        }\n        if (Backbone.history.fragment.indexOf('manage/') === -1 && countlyGlobal.apps[countlyCommon.ACTIVE_APP_ID].type === 'mobile') {\n        //check if it is profile view\n            if (app.activeView.updateEngagement) {\n                var userDetails = countlyUserdata.getUserdetails();\n\n                var tokens = [], platforms = [], test = false, prod = false;\n                tokens = Object.keys(userDetails).filter(function(k) {\n                    return k.indexOf('tk') === 0;\n                }).map(function(k) {\n                    return k.substr(2);\n                });\n                if (userDetails.tkid || userDetails.tkia || userDetails.tkip) {\n                    platforms.push('i');\n                }\n                if (userDetails.tkat || userDetails.tkap) {\n                    platforms.push('a');\n                }\n\n                test = !!userDetails.tkid || !!userDetails.tkia || !!userDetails.tkat;\n                prod = !!userDetails.tkip || !!userDetails.tkap;\n\n                if (tokens.length && countlyAuth.validateCreate('push')) {\n                    if (!$('.btn-create-message').length) {\n                        $('#user-profile-detail-buttons .cly-button-menu').append('<div class=\"item btn-create-message\" >' + jQuery.i18n.map['push.create'] + '</div>');\n                        app.activeView.resetExportSubmenu();\n                    }\n                    $('.btn-create-message').show().off('click').on('click', function() {\n                        if (platforms.length) {\n                            components.push.popup.show({\n                                platforms: platforms,\n                                apps: [countlyCommon.ACTIVE_APP_ID],\n                                test: test && !prod,\n                                userConditions: {did: {$in: [app.userdetailsView.user_did]}}\n                            });\n                        }\n                        else {\n                            CountlyHelpers.alert(jQuery.i18n.map['push.no-user-token'], 'red');\n                        }\n                    });\n                    if (!$('#userdata-info > tbody > tr:last-child table .user-property-push').length) {\n                        $('<tr class=\"user-property-push\"><td class=\"text-left\"><span>' + components.t('userdata.push') + '</span></td><td class=\"text-right\"></td></tr>').appendTo($('#userdata-info > tbody > tr:last-child table tbody'));\n                    }\n                    $('#userdata-info > tbody > tr:last-child table .user-property-push td.text-right').html(tokens.map(function(t) {\n                        return components.t('pu.tk.' + t);\n                    }).join('<br />'));\n                }\n                else {\n                    $('#userdata-info > tbody > tr:last-child table .user-property-push').remove();\n                    $('.btn-create-message').remove();\n                    app.activeView.resetExportSubmenu();\n                }\n            }\n            else {\n            //list view\n                if (countlyAuth.validateCreate('push')) {\n                    if (!$('.btn-create-message').length) {\n                        $('.widget-header').append($('<a class=\"icon-button green btn-header right btn-create-message\" data-localize=\"push.create\"></a>').text(jQuery.i18n.map['push.create']));\n\n                    }\n                    $('.btn-create-message').off('click').on('click', function() {\n                        var q = app.userdataView.getExportQuery().query, filterData = {};\n                        if (q) {\n                            try {\n                                filterData = JSON.parse(q);\n                            }\n                            catch (ignored) {\n                            //ignoring error\n                            }\n                        }\n\n                        components.push.popup.show({\n                            apps: [countlyCommon.ACTIVE_APP_ID],\n                            userConditions: filterData\n                        });\n                    });\n                }\n                else {\n                    $('.btn-create-message').remove();\n                }\n            }\n        }\n    }\n\n    app.addRefreshScript('/users#', modifyUserDetailsForPush);\n    app.addPageScript('/users#', modifyUserDetailsForPush);\n\n    //countly.view global management settings\n    $(document).ready(function() {\n        if (countlyAuth.validateRead('push')) {\n            app.addMenuForType(\"mobile\", \"reach\", {code: \"push\", text: \"push.sidebar.section\", icon: '<div class=\"logo ion-chatbox-working\"></div>', priority: 10});\n            app.addSubMenuForType(\"mobile\", \"push\", {code: \"messaging\", url: \"#/messaging\", text: \"push.sidebar.overview\", priority: 10});\n        }\n\n        if (app.configurationsView) {\n            app.configurationsView.registerLabel(\"push\", \"push.plugin-title\");\n            app.configurationsView.registerLabel(\"push.proxyhost\", \"push.proxyhost\");\n            app.configurationsView.registerLabel(\"push.proxyport\", \"push.proxyport\");\n        }\n\n        var notes = countlyGlobal.member.notes;\n        if (notes && notes.push && notes.push.gcm && notes.push.gcm !== true) {\n            CountlyHelpers.notify({\n                type: 'error',\n                title: jQuery.i18n.map['push.note.gcm.t'],\n                message: jQuery.i18n.prop('push.note.gcm.m', notes.push.gcm.apps.map(function(a) {\n                    return a.name;\n                }).join(', ')),\n                sticky: true,\n                onClick: function() {\n                    return $.ajax({\n                        type: \"GET\",\n                        url: countlyCommon.API_URL + \"/i/users/ack\",\n                        data: {\n                            path: 'push.gcm'\n                        },\n                        success: function() {\n                            notes.push.gcm = true;\n                        }\n                    });\n                }\n            });\n        }\n\n        $('body').off('click', '.routename-messagingDashboardView > .tab-buttons > div').on('click', '.routename-messagingDashboardView > .tab-buttons > div', function() {\n            if ($(this).next().length === 1) {\n                $('.widget-content').addClass('hide-zoom');\n            }\n            else {\n                $('.widget-content').removeClass('hide-zoom');\n            }\n        });\n    });\n}());", "idx": 4, "id": 14061, "msg": "", "proj": "Countly-countly-server", "lang": "js", "sampling_weight": 0.12863703974322174}
{"patch": "@@ -312,3 +312,44 @@ class TestGetSigninURL(AWSMockServiceTestCase):\n \n         with self.assertRaises(Exception):\n             self.service_connection.get_signin_url()\n+\n+\n+class TestGenerateCredentialReport(AWSMockServiceTestCase):\n+    connection_class = IAMConnection\n+    \n+    def default_body(self):\n+        return b\"\"\"\n+          <GenerateCredentialReportResponse>\n+            <GenerateCredentialReportResult>\n+              <State>COMPLETE</State>\n+            </GenerateCredentialReportResult>\n+            <ResponseMetadata>\n+              <RequestId>b62e22a3-0da1-11e4-ba55-0990EXAMPLE</RequestId>\n+            </ResponseMetadata>\n+          </GenerateCredentialReportResponse>\n+        \"\"\"\n+\n+    def test_generate_credential_report(self):\n+        self.set_http_response(status_code=200)\n+        self.service_connection.generate_credential_report()\n+\n+\n+class TestGetCredentialReport(AWSMockServiceTestCase):\n+    connection_class = IAMConnection\n+\n+    def default_body(self):\n+        return b\"\"\"\n+          <GetCredentialReportResponse>\n+            <ResponseMetadata>\n+              <RequestId>99e60e9a-0db5-11e4-94d4-b764EXAMPLE</RequestId>\n+            </ResponseMetadata>\n+            <GetCredentialReportResult>\n+              <Content>BASE64BLOB</Content>\n+              <ReportFormat>text/csv</ReportFormat>\n+              <GeneratedTime>2014-07-17T11:09:11Z</GeneratedTime>\n+            </GetCredentialReportResult>\n+          </GetCredentialReportResponse>\n+        \"\"\"\n+    def test_get_credential_report(self):\n+        self.set_http_response(status_code=200)\n+        self.service_connection.get_credential_report()", "y": 1, "oldf": "#!/usr/bin/env python\n# Copyright (c) 2012 Amazon.com, Inc. or its affiliates.  All Rights Reserved\n#\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish, dis-\n# tribute, sublicense, and/or sell copies of the Software, and to permit\n# persons to whom the Software is furnished to do so, subject to the fol-\n# lowing conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-\n# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT\n# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n# IN THE SOFTWARE.\n#\n\nfrom tests.unit import unittest\nfrom boto.compat import json\nfrom boto.iam.connection import IAMConnection\nfrom tests.unit import AWSMockServiceTestCase\n\n\nclass TestCreateSamlProvider(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n            <CreateSAMLProviderResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\n              <CreateSAMLProviderResult>\n                <SAMLProviderArn>arn</SAMLProviderArn>\n              </CreateSAMLProviderResult>\n              <ResponseMetadata>\n                <RequestId>29f47818-99f5-11e1-a4c3-27EXAMPLE804</RequestId>\n              </ResponseMetadata>\n            </CreateSAMLProviderResponse>\n        \"\"\"\n\n    def test_create_saml_provider(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.create_saml_provider('document', 'name')\n\n        self.assert_request_parameters(\n            {'Action': 'CreateSAMLProvider',\n             'SAMLMetadataDocument': 'document',\n             'Name': 'name'},\n            ignore_params_values=['Version'])\n\n        self.assertEqual(response['create_saml_provider_response']\\\n                                 ['create_saml_provider_result']\\\n                                 ['saml_provider_arn'], 'arn')\n\n\nclass TestListSamlProviders(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n            <ListSAMLProvidersResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\n              <ListSAMLProvidersResult>\n                <SAMLProviderList>\n                  <member>\n                    <Arn>arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Database</Arn>\n                    <ValidUntil>2032-05-09T16:27:11Z</ValidUntil>\n                    <CreateDate>2012-05-09T16:27:03Z</CreateDate>\n                  </member>\n                  <member>\n                    <Arn>arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Webserver</Arn>\n                    <ValidUntil>2015-03-11T13:11:02Z</ValidUntil>\n                    <CreateDate>2012-05-09T16:27:11Z</CreateDate>\n                  </member>\n                </SAMLProviderList>\n              </ListSAMLProvidersResult>\n              <ResponseMetadata>\n                <RequestId>fd74fa8d-99f3-11e1-a4c3-27EXAMPLE804</RequestId>\n              </ResponseMetadata>\n            </ListSAMLProvidersResponse>\n        \"\"\"\n\n    def test_list_saml_providers(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.list_saml_providers()\n\n        self.assert_request_parameters(\n            {'Action': 'ListSAMLProviders'},\n            ignore_params_values=['Version'])\n        self.assertEqual(response.saml_provider_list, [\n            {'arn':'arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Database',\n             'valid_until':'2032-05-09T16:27:11Z',\n             'create_date':'2012-05-09T16:27:03Z'},\n            {'arn':'arn:aws:iam::123456789012:instance-profile/application_abc/component_xyz/Webserver',\n             'valid_until':'2015-03-11T13:11:02Z',\n             'create_date':'2012-05-09T16:27:11Z'}])\n\n\nclass TestGetSamlProvider(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n            <GetSAMLProviderResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\n              <GetSAMLProviderResult>\n                <CreateDate>2012-05-09T16:27:11Z</CreateDate>\n                <ValidUntil>2015-12-31T211:59:59Z</ValidUntil>\n                <SAMLMetadataDocument>Pd9fexDssTkRgGNqs...DxptfEs==</SAMLMetadataDocument>\n              </GetSAMLProviderResult>\n              <ResponseMetadata>\n                <RequestId>29f47818-99f5-11e1-a4c3-27EXAMPLE804</RequestId>\n              </ResponseMetadata>\n            </GetSAMLProviderResponse>\n        \"\"\"\n\n    def test_get_saml_provider(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.get_saml_provider('arn')\n\n        self.assert_request_parameters(\n            {\n                'Action': 'GetSAMLProvider',\n                'SAMLProviderArn': 'arn'\n            },\n            ignore_params_values=['Version'])\n\n\nclass TestUpdateSamlProvider(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n            <UpdateSAMLProviderResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\n              <UpdateSAMLProviderResult>\n                <SAMLProviderArn>arn:aws:iam::123456789012:saml-metadata/MyUniversity</SAMLProviderArn>\n              </UpdateSAMLProviderResult>\n              <ResponseMetadata>\n                <RequestId>29f47818-99f5-11e1-a4c3-27EXAMPLE804</RequestId>\n              </ResponseMetadata>\n            </UpdateSAMLProviderResponse>\n        \"\"\"\n\n    def test_update_saml_provider(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.update_saml_provider('arn', 'doc')\n\n        self.assert_request_parameters(\n            {\n                'Action': 'UpdateSAMLProvider',\n                'SAMLMetadataDocument': 'doc',\n                'SAMLProviderArn': 'arn'\n            },\n            ignore_params_values=['Version'])\n\n\nclass TestDeleteSamlProvider(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return \"\"\n\n    def test_delete_saml_provider(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.delete_saml_provider('arn')\n\n        self.assert_request_parameters(\n            {\n                'Action': 'DeleteSAMLProvider',\n                'SAMLProviderArn': 'arn'\n            },\n            ignore_params_values=['Version'])\n\n\nclass TestCreateRole(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n          <CreateRoleResponse xmlns=\"https://iam.amazonaws.com/doc/2010-05-08/\">\n            <CreateRoleResult>\n              <Role>\n                <Path>/application_abc/component_xyz/</Path>\n                <Arn>arn:aws:iam::123456789012:role/application_abc/component_xyz/S3Access</Arn>\n                <RoleName>S3Access</RoleName>\n                <AssumeRolePolicyDocument>{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":[\"ec2.amazonaws.com\"]},\"Action\":[\"sts:AssumeRole\"]}]}</AssumeRolePolicyDocument>\n                <CreateDate>2012-05-08T23:34:01.495Z</CreateDate>\n                <RoleId>AROADBQP57FF2AEXAMPLE</RoleId>\n              </Role>\n            </CreateRoleResult>\n            <ResponseMetadata>\n              <RequestId>4a93ceee-9966-11e1-b624-b1aEXAMPLE7c</RequestId>\n            </ResponseMetadata>\n          </CreateRoleResponse>\n        \"\"\"\n\n    def test_create_role_default(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.create_role('a_name')\n\n        self.assert_request_parameters(\n            {'Action': 'CreateRole',\n             'RoleName': 'a_name'},\n            ignore_params_values=['Version', 'AssumeRolePolicyDocument'])\n        self.assertDictEqual(json.loads(self.actual_request.params[\"AssumeRolePolicyDocument\"]), {\"Statement\": [{\"Action\": [\"sts:AssumeRole\"], \"Effect\": \"Allow\", \"Principal\": {\"Service\": [\"ec2.amazonaws.com\"]}}]})\n\n    def test_create_role_default_cn_north(self):\n        self.set_http_response(status_code=200)\n        self.service_connection.host = 'iam.cn-north-1.amazonaws.com.cn'\n        response = self.service_connection.create_role('a_name')\n\n        self.assert_request_parameters(\n            {'Action': 'CreateRole',\n             'RoleName': 'a_name'},\n            ignore_params_values=['Version', 'AssumeRolePolicyDocument'])\n        self.assertDictEqual(json.loads(self.actual_request.params[\"AssumeRolePolicyDocument\"]), {\"Statement\": [{\"Action\": [\"sts:AssumeRole\"], \"Effect\": \"Allow\", \"Principal\": {\"Service\": [\"ec2.amazonaws.com.cn\"]}}]})\n\n    def test_create_role_string_policy(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.create_role(\n            'a_name',\n            # Historical usage.\n            assume_role_policy_document='{\"hello\": \"policy\"}'\n        )\n\n        self.assert_request_parameters(\n            {'Action': 'CreateRole',\n             'AssumeRolePolicyDocument': '{\"hello\": \"policy\"}',\n             'RoleName': 'a_name'},\n            ignore_params_values=['Version'])\n\n    def test_create_role_data_policy(self):\n        self.set_http_response(status_code=200)\n        response = self.service_connection.create_role(\n            'a_name',\n            # With plain data, we should dump it for them.\n            assume_role_policy_document={\"hello\": \"policy\"}\n        )\n\n        self.assert_request_parameters(\n            {'Action': 'CreateRole',\n             'AssumeRolePolicyDocument': '{\"hello\": \"policy\"}',\n             'RoleName': 'a_name'},\n            ignore_params_values=['Version'])\n\n\nclass TestGetSigninURL(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n          <ListAccountAliasesResponse>\n            <ListAccountAliasesResult>\n              <IsTruncated>false</IsTruncated>\n              <AccountAliases>\n                <member>foocorporation</member>\n                <member>anotherunused</member>\n              </AccountAliases>\n            </ListAccountAliasesResult>\n            <ResponseMetadata>\n              <RequestId>c5a076e9-f1b0-11df-8fbe-45274EXAMPLE</RequestId>\n            </ResponseMetadata>\n          </ListAccountAliasesResponse>\n        \"\"\"\n\n    def test_get_signin_url_default(self):\n        self.set_http_response(status_code=200)\n        url = self.service_connection.get_signin_url()\n        self.assertEqual(\n            url,\n            'https://foocorporation.signin.aws.amazon.com/console/ec2'\n        )\n\n    def test_get_signin_url_s3(self):\n        self.set_http_response(status_code=200)\n        url = self.service_connection.get_signin_url(service='s3')\n        self.assertEqual(\n            url,\n            'https://foocorporation.signin.aws.amazon.com/console/s3'\n        )\n\n    def test_get_signin_url_cn_north(self):\n        self.set_http_response(status_code=200)\n        self.service_connection.host = 'iam.cn-north-1.amazonaws.com.cn'\n        url = self.service_connection.get_signin_url()\n        self.assertEqual(\n            url,\n            'https://foocorporation.signin.aws.amazon.com/console/ec2'\n        )\n\n\nclass TestGetSigninURL(AWSMockServiceTestCase):\n    connection_class = IAMConnection\n\n    def default_body(self):\n        return b\"\"\"\n          <ListAccountAliasesResponse>\n            <ListAccountAliasesResult>\n              <IsTruncated>false</IsTruncated>\n              <AccountAliases></AccountAliases>\n            </ListAccountAliasesResult>\n            <ResponseMetadata>\n              <RequestId>c5a076e9-f1b0-11df-8fbe-45274EXAMPLE</RequestId>\n            </ResponseMetadata>\n          </ListAccountAliasesResponse>\n        \"\"\"\n\n    def test_get_signin_url_no_aliases(self):\n        self.set_http_response(status_code=200)\n\n        with self.assertRaises(Exception):\n            self.service_connection.get_signin_url()\n", "idx": 1, "id": 10315, "msg": "Can you add some assertions on the response for these? How do you expect to access the response?", "proj": "boto-boto", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -139,6 +139,19 @@ def _is_multi_naming_match(match, node_type, confidence):\n             match.lastgroup not in EXEMPT_NAME_CATEGORIES\n             and (node_type != 'method' or confidence != interfaces.INFERENCE_FAILURE))\n \n+def is_typing_type(obj):\n+    # TODO: understanding of subscripted Generics should be part of astroid\n+    if isinstance(obj, astroid.Subscript) and is_typing_type(obj.value):\n+        return True\n+\n+    inferred = utils.safe_infer(obj)\n+    if isinstance(inferred, astroid.FunctionDef):\n+        return inferred.qname() == 'typing.NewType.new_type'\n+    elif isinstance(inferred, (astroid.Instance, astroid.ClassDef)):\n+        return inferred.qname().startswith('typing.')\n+\n+    return False\n+\n \n if sys.version_info < (3, 0):\n     BUILTIN_PROPERTY = '__builtin__.property'", "y": 1, "oldf": "# -*- coding: utf-8 -*-\n# Copyright (c) 2006-2015 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2012-2014 Google, Inc.\n# Copyright (c) 2013-2016 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Brett Cannon <brett@python.org>\n# Copyright (c) 2015 Radu Ciorba <radu@devrandom.ro>\n# Copyright (c) 2015 Michael Kefeder <oss@multiwave.ch>\n# Copyright (c) 2015 Dmitry Pribysh <dmand@yandex.ru>\n# Copyright (c) 2015 Stephane Wirtel <stephane@wirtel.be>\n# Copyright (c) 2015 Nick Bastin <nick.bastin@gmail.com>\n# Copyright (c) 2016 Alex Jurkiewicz <alex@jurkiewi.cz>\n# Copyright (c) 2016 Yannack <yannack@users.noreply.github.com>\n# Copyright (c) 2016 Laura M\u00e9dioni <lmedioni@logilab.fr>\n# Copyright (c) 2016 Ashley Whetter <ashley@awhetter.co.uk>\n\n# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n# For details: https://github.com/PyCQA/pylint/blob/master/COPYING\n\n\"\"\"basic checker for Python code\"\"\"\n\nimport collections\nimport itertools\nimport sys\nimport re\n\nimport six\nfrom six.moves import zip  # pylint: disable=redefined-builtin\n\nimport astroid\nimport astroid.bases\nimport astroid.scoped_nodes\n\nfrom pylint import checkers\nfrom pylint import exceptions\nfrom pylint import interfaces\nfrom pylint.checkers import utils\nfrom pylint import reporters\nfrom pylint.reporters.ureports import nodes as reporter_nodes\n\n\n# regex for class/function/variable/constant name\nCLASS_NAME_RGX = re.compile('[A-Z_][a-zA-Z0-9]+$')\nMOD_NAME_RGX = re.compile('(([a-z_][a-z0-9_]*)|([A-Z][a-zA-Z0-9]+))$')\nCONST_NAME_RGX = re.compile('(([A-Z_][A-Z0-9_]*)|(__.*__))$')\nCOMP_VAR_RGX = re.compile('[A-Za-z_][A-Za-z0-9_]*$')\nDEFAULT_NAME_RGX = re.compile('(([a-z][a-z0-9_]{2,30})|(_[a-z0-9_]*))$')\nCLASS_ATTRIBUTE_RGX = re.compile(r'([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$')\n# do not require a doc string on private/system methods\nNO_REQUIRED_DOC_RGX = re.compile('^_')\nREVERSED_PROTOCOL_METHOD = '__reversed__'\nSEQUENCE_PROTOCOL_METHODS = ('__getitem__', '__len__')\nREVERSED_METHODS = (SEQUENCE_PROTOCOL_METHODS,\n                    (REVERSED_PROTOCOL_METHOD, ))\nTYPECHECK_COMPARISON_OPERATORS = frozenset(('is', 'is not', '==',\n                                            '!=', 'in', 'not in'))\nLITERAL_NODE_TYPES = (astroid.Const, astroid.Dict, astroid.List, astroid.Set)\nUNITTEST_CASE = 'unittest.case'\nBUILTINS = six.moves.builtins.__name__\nTYPE_QNAME = \"%s.type\" % BUILTINS\nPY33 = sys.version_info >= (3, 3)\nPY3K = sys.version_info >= (3, 0)\nPY35 = sys.version_info >= (3, 5)\n\n# Name categories that are always consistent with all naming conventions.\nEXEMPT_NAME_CATEGORIES = set(('exempt', 'ignore'))\n\n# A mapping from builtin-qname -> symbol, to be used when generating messages\n# about dangerous default values as arguments\nDEFAULT_ARGUMENT_SYMBOLS = dict(\n    zip(['.'.join([BUILTINS, x]) for x in ('set', 'dict', 'list')],\n        ['set()', '{}', '[]'])\n)\nREVERSED_COMPS = {'<': '>', '<=': '>=', '>': '<', '>=': '<='}\n\ndel re\n\ndef _redefines_import(node):\n    \"\"\" Detect that the given node (AssName) is inside an\n    exception handler and redefines an import from the tryexcept body.\n    Returns True if the node redefines an import, False otherwise.\n    \"\"\"\n    current = node\n    while current and not isinstance(current.parent, astroid.ExceptHandler):\n        current = current.parent\n    if not current or not utils.error_of_type(current.parent, ImportError):\n        return False\n    try_block = current.parent.parent\n    for import_node in try_block.nodes_of_class((astroid.ImportFrom, astroid.Import)):\n        for name, alias in import_node.names:\n            if alias:\n                if alias == node.name:\n                    return True\n            elif name == node.name:\n                return True\n    return False\n\ndef in_loop(node):\n    \"\"\"return True if the node is inside a kind of for loop\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, (astroid.For, astroid.ListComp, astroid.SetComp,\n                               astroid.DictComp, astroid.GeneratorExp)):\n            return True\n        parent = parent.parent\n    return False\n\ndef in_nested_list(nested_list, obj):\n    \"\"\"return true if the object is an element of <nested_list> or of a nested\n    list\n    \"\"\"\n    for elmt in nested_list:\n        if isinstance(elmt, (list, tuple)):\n            if in_nested_list(elmt, obj):\n                return True\n        elif elmt == obj:\n            return True\n    return False\n\ndef _loop_exits_early(loop):\n    \"\"\"Returns true if a loop has a break statement in its body.\"\"\"\n    loop_nodes = (astroid.For, astroid.While)\n    # Loop over body explicitly to avoid matching break statements\n    # in orelse.\n    for child in loop.body:\n        if isinstance(child, loop_nodes):\n            # break statement may be in orelse of child loop.\n            # pylint: disable=superfluous-parens\n            for orelse in (child.orelse or ()):\n                for _ in orelse.nodes_of_class(astroid.Break, skip_klass=loop_nodes):\n                    return True\n            continue\n        for _ in child.nodes_of_class(astroid.Break, skip_klass=loop_nodes):\n            return True\n    return False\n\ndef _is_multi_naming_match(match, node_type, confidence):\n    return (match is not None and\n            match.lastgroup is not None and\n            match.lastgroup not in EXEMPT_NAME_CATEGORIES\n            and (node_type != 'method' or confidence != interfaces.INFERENCE_FAILURE))\n\n\nif sys.version_info < (3, 0):\n    BUILTIN_PROPERTY = '__builtin__.property'\nelse:\n    BUILTIN_PROPERTY = 'builtins.property'\n\n\ndef _get_properties(config):\n    \"\"\"Returns a tuple of property classes and names.\n\n    Property classes are fully qualified, such as 'abc.abstractproperty' and\n    property names are the actual names, such as 'abstract_property'.\n    \"\"\"\n    property_classes = set((BUILTIN_PROPERTY,))\n    property_names = set()  # Not returning 'property', it has its own check.\n    if config is not None:\n        property_classes.update(config.property_classes)\n        property_names.update((prop.rsplit('.', 1)[-1]\n                               for prop in config.property_classes))\n    return property_classes, property_names\n\n\ndef _determine_function_name_type(node, config=None):\n    \"\"\"Determine the name type whose regex the a function's name should match.\n\n    :param node: A function node.\n    :type node: astroid.node_classes.NodeNG\n    :param config: Configuration from which to pull additional property classes.\n    :type config: :class:`optparse.Values`\n\n    :returns: One of ('function', 'method', 'attr')\n    :rtype: str\n    \"\"\"\n    property_classes, property_names = _get_properties(config)\n    if not node.is_method():\n        return 'function'\n    if node.decorators:\n        decorators = node.decorators.nodes\n    else:\n        decorators = []\n    for decorator in decorators:\n        # If the function is a property (decorated with @property\n        # or @abc.abstractproperty), the name type is 'attr'.\n        if (isinstance(decorator, astroid.Name) or\n                (isinstance(decorator, astroid.Attribute) and\n                 decorator.attrname in property_names)):\n            infered = utils.safe_infer(decorator)\n            if infered and infered.qname() in property_classes:\n                return 'attr'\n        # If the function is decorated using the prop_method.{setter,getter}\n        # form, treat it like an attribute as well.\n        elif (isinstance(decorator, astroid.Attribute) and\n              decorator.attrname in ('setter', 'deleter')):\n            return 'attr'\n    return 'method'\n\n\ndef _has_abstract_methods(node):\n    \"\"\"\n    Determine if the given `node` has abstract methods.\n\n    The methods should be made abstract by decorating them\n    with `abc` decorators.\n    \"\"\"\n    return len(utils.unimplemented_abstract_methods(node)) > 0\n\n\ndef report_by_type_stats(sect, stats, old_stats):\n    \"\"\"make a report of\n\n    * percentage of different types documented\n    * percentage of different types with a bad name\n    \"\"\"\n    # percentage of different types documented and/or with a bad name\n    nice_stats = {}\n    for node_type in ('module', 'class', 'method', 'function'):\n        try:\n            total = stats[node_type]\n        except KeyError:\n            raise exceptions.EmptyReportError()\n        nice_stats[node_type] = {}\n        if total != 0:\n            try:\n                documented = total - stats['undocumented_'+node_type]\n                percent = (documented * 100.) / total\n                nice_stats[node_type]['percent_documented'] = '%.2f' % percent\n            except KeyError:\n                nice_stats[node_type]['percent_documented'] = 'NC'\n            try:\n                percent = (stats['badname_'+node_type] * 100.) / total\n                nice_stats[node_type]['percent_badname'] = '%.2f' % percent\n            except KeyError:\n                nice_stats[node_type]['percent_badname'] = 'NC'\n    lines = ('type', 'number', 'old number', 'difference',\n             '%documented', '%badname')\n    for node_type in ('module', 'class', 'method', 'function'):\n        new = stats[node_type]\n        old = old_stats.get(node_type, None)\n        if old is not None:\n            diff_str = reporters.diff_string(old, new)\n        else:\n            old, diff_str = 'NC', 'NC'\n        lines += (node_type, str(new), str(old), diff_str,\n                  nice_stats[node_type].get('percent_documented', '0'),\n                  nice_stats[node_type].get('percent_badname', '0'))\n    sect.append(reporter_nodes.Table(children=lines, cols=6, rheaders=1))\n\ndef redefined_by_decorator(node):\n    \"\"\"return True if the object is a method redefined via decorator.\n\n    For example:\n        @property\n        def x(self): return self._x\n        @x.setter\n        def x(self, value): self._x = value\n    \"\"\"\n    if node.decorators:\n        for decorator in node.decorators.nodes:\n            if (isinstance(decorator, astroid.Attribute) and\n                    getattr(decorator.expr, 'name', None) == node.name):\n                return True\n    return False\n\n\nclass _BasicChecker(checkers.BaseChecker):\n    __implements__ = interfaces.IAstroidChecker\n    name = 'basic'\n\nclass BasicErrorChecker(_BasicChecker):\n    msgs = {\n        'E0100': ('__init__ method is a generator',\n                  'init-is-generator',\n                  'Used when the special class method __init__ is turned into a '\n                  'generator by a yield in its body.'),\n        'E0101': ('Explicit return in __init__',\n                  'return-in-init',\n                  'Used when the special class method __init__ has an explicit '\n                  'return value.'),\n        'E0102': ('%s already defined line %s',\n                  'function-redefined',\n                  'Used when a function / class / method is redefined.'),\n        'E0103': ('%r not properly in loop',\n                  'not-in-loop',\n                  'Used when break or continue keywords are used outside a loop.'),\n        'E0104': ('Return outside function',\n                  'return-outside-function',\n                  'Used when a \"return\" statement is found outside a function or '\n                  'method.'),\n        'E0105': ('Yield outside function',\n                  'yield-outside-function',\n                  'Used when a \"yield\" statement is found outside a function or '\n                  'method.'),\n        'E0106': ('Return with argument inside generator',\n                  'return-arg-in-generator',\n                  'Used when a \"return\" statement with an argument is found '\n                  'outside in a generator function or method (e.g. with some '\n                  '\"yield\" statements).',\n                  {'maxversion': (3, 3)}),\n        'E0107': (\"Use of the non-existent %s operator\",\n                  'nonexistent-operator',\n                  \"Used when you attempt to use the C-style pre-increment or\"\n                  \"pre-decrement operator -- and ++, which doesn't exist in Python.\"),\n        'E0108': ('Duplicate argument name %s in function definition',\n                  'duplicate-argument-name',\n                  'Duplicate argument names in function definitions are syntax'\n                  ' errors.'),\n        'E0110': ('Abstract class %r with abstract methods instantiated',\n                  'abstract-class-instantiated',\n                  'Used when an abstract class with `abc.ABCMeta` as metaclass '\n                  'has abstract methods and is instantiated.'),\n        'W0120': ('Else clause on loop without a break statement',\n                  'useless-else-on-loop',\n                  'Loops should only have an else clause if they can exit early '\n                  'with a break statement, otherwise the statements under else '\n                  'should be on the same scope as the loop itself.'),\n        'E0112': ('More than one starred expression in assignment',\n                  'too-many-star-expressions',\n                  'Emitted when there are more than one starred '\n                  'expressions (`*x`) in an assignment. This is a SyntaxError.',\n                  {'minversion': (3, 0)}),\n        'E0113': ('Starred assignment target must be in a list or tuple',\n                  'invalid-star-assignment-target',\n                  'Emitted when a star expression is used as a starred '\n                  'assignment target.',\n                  {'minversion': (3, 0)}),\n        'E0114': ('Can use starred expression only in assignment target',\n                  'star-needs-assignment-target',\n                  'Emitted when a star expression is not used in an '\n                  'assignment target.',\n                  {'minversion': (3, 0)}),\n        'E0115': ('Name %r is nonlocal and global',\n                  'nonlocal-and-global',\n                  'Emitted when a name is both nonlocal and global.',\n                  {'minversion': (3, 0)}),\n        'E0116': (\"'continue' not supported inside 'finally' clause\",\n                  'continue-in-finally',\n                  'Emitted when the `continue` keyword is found '\n                  'inside a finally clause, which is a SyntaxError.'),\n        'E0117': (\"nonlocal name %s found without binding\",\n                  'nonlocal-without-binding',\n                  'Emitted when a nonlocal variable does not have an attached '\n                  'name somewhere in the parent scopes',\n                  {'minversion': (3, 0)}),\n        'E0118': (\"Name %r is used prior to global declaration\",\n                  'used-prior-global-declaration',\n                  'Emitted when a name is used prior a global declaration, '\n                  'which results in an error since Python 3.6.',\n                  {'minversion': (3, 6)}),\n        }\n\n    @utils.check_messages('function-redefined')\n    def visit_classdef(self, node):\n        self._check_redefinition('class', node)\n\n    @utils.check_messages('too-many-star-expressions',\n                          'invalid-star-assignment-target')\n    def visit_assign(self, node):\n        starred = list(node.targets[0].nodes_of_class(astroid.Starred))\n        if len(starred) > 1:\n            self.add_message('too-many-star-expressions', node=node)\n\n        # Check *a = b\n        if isinstance(node.targets[0], astroid.Starred):\n            self.add_message('invalid-star-assignment-target', node=node)\n\n    @utils.check_messages('star-needs-assignment-target')\n    def visit_starred(self, node):\n        \"\"\"Check that a Starred expression is used in an assignment target.\"\"\"\n        if isinstance(node.parent, astroid.Call):\n            # f(*args) is converted to Call(args=[Starred]), so ignore\n            # them for this check.\n            return\n        if PY35 and isinstance(node.parent,\n                               (astroid.List, astroid.Tuple,\n                                astroid.Set, astroid.Dict)):\n            # PEP 448 unpacking.\n            return\n\n        stmt = node.statement()\n        if not isinstance(stmt, astroid.Assign):\n            return\n\n        if stmt.value is node or stmt.value.parent_of(node):\n            self.add_message('star-needs-assignment-target', node=node)\n\n    @utils.check_messages('init-is-generator', 'return-in-init',\n                          'function-redefined', 'return-arg-in-generator',\n                          'duplicate-argument-name', 'nonlocal-and-global',\n                          'used-prior-global-declaration')\n    def visit_functiondef(self, node):\n        self._check_nonlocal_and_global(node)\n        self._check_name_used_prior_global(node)\n        if (not redefined_by_decorator(node) and\n                not utils.is_registered_in_singledispatch_function(node)):\n            self._check_redefinition(node.is_method() and 'method' or 'function', node)\n        # checks for max returns, branch, return in __init__\n        returns = node.nodes_of_class(astroid.Return,\n                                      skip_klass=(astroid.FunctionDef,\n                                                  astroid.ClassDef))\n        if node.is_method() and node.name == '__init__':\n            if node.is_generator():\n                self.add_message('init-is-generator', node=node)\n            else:\n                values = [r.value for r in returns]\n                # Are we returning anything but None from constructors\n                if any(v for v in values if not utils.is_none(v)):\n                    self.add_message('return-in-init', node=node)\n        elif node.is_generator():\n            # make sure we don't mix non-None returns and yields\n            if not PY33:\n                for retnode in returns:\n                    if isinstance(retnode.value, astroid.Const) and \\\n                           retnode.value.value is not None:\n                        self.add_message('return-arg-in-generator', node=node,\n                                         line=retnode.fromlineno)\n        # Check for duplicate names\n        args = set()\n        for name in node.argnames():\n            if name in args:\n                self.add_message('duplicate-argument-name', node=node, args=(name,))\n            else:\n                args.add(name)\n\n    visit_asyncfunctiondef = visit_functiondef\n\n    def _check_name_used_prior_global(self, node):\n\n        scope_globals = {\n            name: child\n            for child in node.nodes_of_class(astroid.Global)\n            for name in child.names\n            if child.scope() is node\n        }\n\n        for node_name in node.nodes_of_class(astroid.Name):\n            if node_name.scope() is not node:\n                continue\n\n            name = node_name.name\n            corresponding_global = scope_globals.get(name)\n            if not corresponding_global:\n                continue\n\n            global_lineno = corresponding_global.fromlineno\n            if global_lineno and global_lineno > node_name.fromlineno:\n                self.add_message('used-prior-global-declaration',\n                                 node=node_name, args=(name, ))\n\n    def _check_nonlocal_and_global(self, node):\n        \"\"\"Check that a name is both nonlocal and global.\"\"\"\n        def same_scope(current):\n            return current.scope() is node\n\n        from_iter = itertools.chain.from_iterable\n        nonlocals = set(from_iter(\n            child.names for child in node.nodes_of_class(astroid.Nonlocal)\n            if same_scope(child)))\n        global_vars = set(from_iter(\n            child.names for child in node.nodes_of_class(astroid.Global)\n            if same_scope(child)))\n        for name in nonlocals.intersection(global_vars):\n            self.add_message('nonlocal-and-global',\n                             args=(name, ), node=node)\n\n    @utils.check_messages('return-outside-function')\n    def visit_return(self, node):\n        if not isinstance(node.frame(), astroid.FunctionDef):\n            self.add_message('return-outside-function', node=node)\n\n    @utils.check_messages('yield-outside-function')\n    def visit_yield(self, node):\n        self._check_yield_outside_func(node)\n\n    @utils.check_messages('yield-outside-function')\n    def visit_yieldfrom(self, node):\n        self._check_yield_outside_func(node)\n\n    @utils.check_messages('not-in-loop', 'continue-in-finally')\n    def visit_continue(self, node):\n        self._check_in_loop(node, 'continue')\n\n    @utils.check_messages('not-in-loop')\n    def visit_break(self, node):\n        self._check_in_loop(node, 'break')\n\n    @utils.check_messages('useless-else-on-loop')\n    def visit_for(self, node):\n        self._check_else_on_loop(node)\n\n    @utils.check_messages('useless-else-on-loop')\n    def visit_while(self, node):\n        self._check_else_on_loop(node)\n\n    @utils.check_messages('nonexistent-operator')\n    def visit_unaryop(self, node):\n        \"\"\"check use of the non-existent ++ and -- operator operator\"\"\"\n        if ((node.op in '+-') and\n                isinstance(node.operand, astroid.UnaryOp) and\n                (node.operand.op == node.op)):\n            self.add_message('nonexistent-operator', node=node, args=node.op*2)\n\n    def _check_nonlocal_without_binding(self, node, name):\n        current_scope = node.scope()\n        while True:\n            if current_scope.parent is None:\n                break\n\n            if not isinstance(current_scope, astroid.FunctionDef):\n                self.add_message('nonlocal-without-binding', args=(name, ),\n                                 node=node)\n                return\n            else:\n                if name not in current_scope.locals:\n                    current_scope = current_scope.parent.scope()\n                    continue\n                else:\n                    # Okay, found it.\n                    return\n\n        self.add_message('nonlocal-without-binding', args=(name, ), node=node)\n\n    @utils.check_messages('nonlocal-without-binding')\n    def visit_nonlocal(self, node):\n        for name in node.names:\n            self._check_nonlocal_without_binding(node, name)\n\n    @utils.check_messages('abstract-class-instantiated')\n    def visit_call(self, node):\n        \"\"\" Check instantiating abstract class with\n        abc.ABCMeta as metaclass.\n        \"\"\"\n        try:\n            infered = next(node.func.infer())\n        except astroid.InferenceError:\n            return\n\n        if not isinstance(infered, astroid.ClassDef):\n            return\n\n        klass = utils.node_frame_class(node)\n        if klass is infered:\n            # Don't emit the warning if the class is instantiated\n            # in its own body or if the call is not an instance\n            # creation. If the class is instantiated into its own\n            # body, we're expecting that it knows what it is doing.\n            return\n\n        # __init__ was called\n        metaclass = infered.metaclass()\n        abstract_methods = _has_abstract_methods(infered)\n        if metaclass is None:\n            # Python 3.4 has `abc.ABC`, which won't be detected\n            # by ClassNode.metaclass()\n            for ancestor in infered.ancestors():\n                if ancestor.qname() == 'abc.ABC' and abstract_methods:\n                    self.add_message('abstract-class-instantiated',\n                                     args=(infered.name, ),\n                                     node=node)\n                    break\n            return\n        if metaclass.qname() == 'abc.ABCMeta' and abstract_methods:\n            self.add_message('abstract-class-instantiated',\n                             args=(infered.name, ),\n                             node=node)\n\n    def _check_yield_outside_func(self, node):\n        if not isinstance(node.frame(), (astroid.FunctionDef, astroid.Lambda)):\n            self.add_message('yield-outside-function', node=node)\n\n    def _check_else_on_loop(self, node):\n        \"\"\"Check that any loop with an else clause has a break statement.\"\"\"\n        if node.orelse and not _loop_exits_early(node):\n            self.add_message('useless-else-on-loop', node=node,\n                             # This is not optimal, but the line previous\n                             # to the first statement in the else clause\n                             # will usually be the one that contains the else:.\n                             line=node.orelse[0].lineno - 1)\n\n    def _check_in_loop(self, node, node_name):\n        \"\"\"check that a node is inside a for or while loop\"\"\"\n        _node = node.parent\n        while _node:\n            if isinstance(_node, (astroid.For, astroid.While)):\n                if node not in _node.orelse:\n                    return\n\n            if isinstance(_node, (astroid.ClassDef, astroid.FunctionDef)):\n                break\n            if (isinstance(_node, astroid.TryFinally)\n                    and node in _node.finalbody\n                    and isinstance(node, astroid.Continue)):\n                self.add_message('continue-in-finally', node=node)\n\n            _node = _node.parent\n\n        self.add_message('not-in-loop', node=node, args=node_name)\n\n    def _check_redefinition(self, redeftype, node):\n        \"\"\"check for redefinition of a function / method / class name\"\"\"\n        defined_self = node.parent.frame()[node.name]\n        if defined_self is not node and not astroid.are_exclusive(node, defined_self):\n            self.add_message('function-redefined', node=node,\n                             args=(redeftype, defined_self.fromlineno))\n\n\n\nclass BasicChecker(_BasicChecker):\n    \"\"\"checks for :\n    * doc strings\n    * number of arguments, local variables, branches, returns and statements in\nfunctions, methods\n    * required module attributes\n    * dangerous default values as arguments\n    * redefinition of function / method / class\n    * uses of the global statement\n    \"\"\"\n\n    __implements__ = interfaces.IAstroidChecker\n\n    name = 'basic'\n    msgs = {\n        'W0101': ('Unreachable code',\n                  'unreachable',\n                  'Used when there is some code behind a \"return\" or \"raise\" '\n                  'statement, which will never be accessed.'),\n        'W0102': ('Dangerous default value %s as argument',\n                  'dangerous-default-value',\n                  'Used when a mutable value as list or dictionary is detected in '\n                  'a default value for an argument.'),\n        'W0104': ('Statement seems to have no effect',\n                  'pointless-statement',\n                  'Used when a statement doesn\\'t have (or at least seems to) '\n                  'any effect.'),\n        'W0105': ('String statement has no effect',\n                  'pointless-string-statement',\n                  'Used when a string is used as a statement (which of course '\n                  'has no effect). This is a particular case of W0104 with its '\n                  'own message so you can easily disable it if you\\'re using '\n                  'those strings as documentation, instead of comments.'),\n        'W0106': ('Expression \"%s\" is assigned to nothing',\n                  'expression-not-assigned',\n                  'Used when an expression that is not a function call is assigned '\n                  'to nothing. Probably something else was intended.'),\n        'W0108': ('Lambda may not be necessary',\n                  'unnecessary-lambda',\n                  'Used when the body of a lambda expression is a function call '\n                  'on the same argument list as the lambda itself; such lambda '\n                  'expressions are in all but a few cases replaceable with the '\n                  'function being called in the body of the lambda.'),\n        'W0109': (\"Duplicate key %r in dictionary\",\n                  'duplicate-key',\n                  'Used when a dictionary expression binds the same key multiple '\n                  'times.'),\n        'W0122': ('Use of exec',\n                  'exec-used',\n                  'Used when you use the \"exec\" statement (function for Python '\n                  '3), to discourage its usage. That doesn\\'t '\n                  'mean you cannot use it !'),\n        'W0123': ('Use of eval',\n                  'eval-used',\n                  'Used when you use the \"eval\" function, to discourage its '\n                  'usage. Consider using `ast.literal_eval` for safely evaluating '\n                  'strings containing Python expressions '\n                  'from untrusted sources. '),\n        'W0150': (\"%s statement in finally block may swallow exception\",\n                  'lost-exception',\n                  'Used when a break or a return statement is found inside the '\n                  'finally clause of a try...finally block: the exceptions raised '\n                  'in the try clause will be silently swallowed instead of being '\n                  're-raised.'),\n        'W0199': ('Assert called on a 2-uple. Did you mean \\'assert x,y\\'?',\n                  'assert-on-tuple',\n                  'A call of assert on a tuple will always evaluate to true if '\n                  'the tuple is not empty, and will always evaluate to false if '\n                  'it is.'),\n        'W0124': ('Following \"as\" with another context manager looks like a tuple.',\n                  'confusing-with-statement',\n                  'Emitted when a `with` statement component returns multiple values '\n                  'and uses name binding with `as` only for a part of those values, '\n                  'as in with ctx() as a, b. This can be misleading, since it\\'s not '\n                  'clear if the context manager returns a tuple or if the node without '\n                  'a name binding is another context manager.'),\n        'W0125': ('Using a conditional statement with a constant value',\n                  'using-constant-test',\n                  'Emitted when a conditional statement (If or ternary if) '\n                  'uses a constant value for its test. This might not be what '\n                  'the user intended to do.'),\n        'E0111': ('The first reversed() argument is not a sequence',\n                  'bad-reversed-sequence',\n                  'Used when the first argument to reversed() builtin '\n                  'isn\\'t a sequence (does not implement __reversed__, '\n                  'nor __getitem__ and __len__'),\n\n    }\n\n    reports = (('RP0101', 'Statistics by type', report_by_type_stats),)\n\n    def __init__(self, linter):\n        _BasicChecker.__init__(self, linter)\n        self.stats = None\n        self._tryfinallys = None\n\n    def open(self):\n        \"\"\"initialize visit variables and statistics\n        \"\"\"\n        self._tryfinallys = []\n        self.stats = self.linter.add_stats(module=0, function=0,\n                                           method=0, class_=0)\n\n    @utils.check_messages('using-constant-test')\n    def visit_if(self, node):\n        self._check_using_constant_test(node, node.test)\n\n    @utils.check_messages('using-constant-test')\n    def visit_ifexp(self, node):\n        self._check_using_constant_test(node, node.test)\n\n    @utils.check_messages('using-constant-test')\n    def visit_comprehension(self, node):\n        if node.ifs:\n            for if_test in node.ifs:\n                self._check_using_constant_test(node, if_test)\n\n    def _check_using_constant_test(self, node, test):\n        const_nodes = (\n            astroid.Module,\n            astroid.scoped_nodes.GeneratorExp,\n            astroid.Lambda, astroid.FunctionDef, astroid.ClassDef,\n            astroid.bases.Generator, astroid.UnboundMethod,\n            astroid.BoundMethod, astroid.Module)\n        structs = (astroid.Dict, astroid.Tuple, astroid.Set)\n\n        # These nodes are excepted, since they are not constant\n        # values, requiring a computation to happen. The only type\n        # of node in this list which doesn't have this property is\n        # Getattr, which is excepted because the conditional statement\n        # can be used to verify that the attribute was set inside a class,\n        # which is definitely a valid use case.\n        except_nodes = (astroid.Attribute, astroid.Call,\n                        astroid.BinOp, astroid.BoolOp, astroid.UnaryOp,\n                        astroid.Subscript)\n        inferred = None\n        emit = isinstance(test, (astroid.Const, ) + structs + const_nodes)\n        if not isinstance(test, except_nodes):\n            inferred = utils.safe_infer(test)\n\n        if emit or isinstance(inferred, const_nodes):\n            self.add_message('using-constant-test', node=node)\n\n    def visit_module(self, _):\n        \"\"\"check module name, docstring and required arguments\n        \"\"\"\n        self.stats['module'] += 1\n\n    def visit_classdef(self, node): # pylint: disable=unused-argument\n        \"\"\"check module name, docstring and redefinition\n        increment branch counter\n        \"\"\"\n        self.stats['class'] += 1\n\n    @utils.check_messages('pointless-statement', 'pointless-string-statement',\n                          'expression-not-assigned')\n    def visit_expr(self, node):\n        \"\"\"check for various kind of statements without effect\"\"\"\n        expr = node.value\n        if isinstance(expr, astroid.Const) and isinstance(expr.value,\n                                                          six.string_types):\n            # treat string statement in a separated message\n            # Handle PEP-257 attribute docstrings.\n            # An attribute docstring is defined as being a string right after\n            # an assignment at the module level, class level or __init__ level.\n            scope = expr.scope()\n            if isinstance(scope, (astroid.ClassDef, astroid.Module, astroid.FunctionDef)):\n                if isinstance(scope, astroid.FunctionDef) and scope.name != '__init__':\n                    pass\n                else:\n                    sibling = expr.previous_sibling()\n                    if (sibling is not None and sibling.scope() is scope and\n                            isinstance(sibling, astroid.Assign)):\n                        return\n            self.add_message('pointless-string-statement', node=node)\n            return\n        # ignore if this is :\n        # * a direct function call\n        # * the unique child of a try/except body\n        # * a yield (which are wrapped by a discard node in _ast XXX)\n        # warn W0106 if we have any underlying function call (we can't predict\n        # side effects), else pointless-statement\n        if (isinstance(expr, (astroid.Yield, astroid.Await, astroid.Call)) or\n                (isinstance(node.parent, astroid.TryExcept) and\n                 node.parent.body == [node])):\n            return\n        if any(expr.nodes_of_class(astroid.Call)):\n            self.add_message('expression-not-assigned', node=node,\n                             args=expr.as_string())\n        else:\n            self.add_message('pointless-statement', node=node)\n\n    @staticmethod\n    def _filter_vararg(node, call_args):\n        # Return the arguments for the given call which are\n        # not passed as vararg.\n        for arg in call_args:\n            if isinstance(arg, astroid.Starred):\n                if (isinstance(arg.value, astroid.Name)\n                        and arg.value.name != node.args.vararg):\n                    yield arg\n            else:\n                yield arg\n\n    @staticmethod\n    def _has_variadic_argument(args, variadic_name):\n        if not args:\n            return True\n        for arg in args:\n            if isinstance(arg.value, astroid.Name):\n                if arg.value.name != variadic_name:\n                    return True\n            else:\n                return True\n        return False\n\n    @utils.check_messages('unnecessary-lambda')\n    def visit_lambda(self, node):\n        \"\"\"check whether or not the lambda is suspicious\n        \"\"\"\n        # if the body of the lambda is a call expression with the same\n        # argument list as the lambda itself, then the lambda is\n        # possibly unnecessary and at least suspicious.\n        if node.args.defaults:\n            # If the arguments of the lambda include defaults, then a\n            # judgment cannot be made because there is no way to check\n            # that the defaults defined by the lambda are the same as\n            # the defaults defined by the function called in the body\n            # of the lambda.\n            return\n        call = node.body\n        if not isinstance(call, astroid.Call):\n            # The body of the lambda must be a function call expression\n            # for the lambda to be unnecessary.\n            return\n        if (isinstance(node.body.func, astroid.Attribute) and\n                isinstance(node.body.func.expr, astroid.Call)):\n            # Chained call, the intermediate call might\n            # return something else (but we don't check that, yet).\n            return\n\n        ordinary_args = list(node.args.args)\n        new_call_args = list(self._filter_vararg(node, call.args))\n        if node.args.kwarg:\n            if self._has_variadic_argument(call.kwargs, node.args.kwarg):\n                return\n        elif call.kwargs or call.keywords:\n            return\n\n        if node.args.vararg:\n            if self._has_variadic_argument(call.starargs, node.args.vararg):\n                return\n        elif call.starargs:\n            return\n\n        # The \"ordinary\" arguments must be in a correspondence such that:\n        # ordinary_args[i].name == call.args[i].name.\n        if len(ordinary_args) != len(new_call_args):\n            return\n        for arg, passed_arg in zip(ordinary_args, new_call_args):\n            if not isinstance(passed_arg, astroid.Name):\n                return\n            if arg.name != passed_arg.name:\n                return\n\n        self.add_message('unnecessary-lambda', line=node.fromlineno,\n                         node=node)\n\n    @utils.check_messages('dangerous-default-value')\n    def visit_functiondef(self, node):\n        \"\"\"check function name, docstring, arguments, redefinition,\n        variable names, max locals\n        \"\"\"\n        self.stats[node.is_method() and 'method' or 'function'] += 1\n        self._check_dangerous_default(node)\n\n    visit_asyncfunctiondef = visit_functiondef\n\n    def _check_dangerous_default(self, node):\n        # check for dangerous default values as arguments\n        is_iterable = lambda n: isinstance(n, (astroid.List,\n                                               astroid.Set,\n                                               astroid.Dict))\n        for default in node.args.defaults:\n            try:\n                value = next(default.infer())\n            except astroid.InferenceError:\n                continue\n\n            if (isinstance(value, astroid.Instance) and\n                    value.qname() in DEFAULT_ARGUMENT_SYMBOLS):\n\n                if value is default:\n                    msg = DEFAULT_ARGUMENT_SYMBOLS[value.qname()]\n                elif isinstance(value, astroid.Instance) or is_iterable(value):\n                    # We are here in the following situation(s):\n                    #   * a dict/set/list/tuple call which wasn't inferred\n                    #     to a syntax node ({}, () etc.). This can happen\n                    #     when the arguments are invalid or unknown to\n                    #     the inference.\n                    #   * a variable from somewhere else, which turns out to be a list\n                    #     or a dict.\n                    if is_iterable(default):\n                        msg = value.pytype()\n                    elif isinstance(default, astroid.Call):\n                        msg = '%s() (%s)' % (value.name, value.qname())\n                    else:\n                        msg = '%s (%s)' % (default.as_string(), value.qname())\n                else:\n                    # this argument is a name\n                    msg = '%s (%s)' % (default.as_string(),\n                                       DEFAULT_ARGUMENT_SYMBOLS[value.qname()])\n                self.add_message('dangerous-default-value',\n                                 node=node,\n                                 args=(msg, ))\n\n    @utils.check_messages('unreachable', 'lost-exception')\n    def visit_return(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"\n        self._check_unreachable(node)\n        # Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, 'return', (astroid.FunctionDef,))\n\n    @utils.check_messages('unreachable')\n    def visit_continue(self, node):\n        \"\"\"check is the node has a right sibling (if so, that's some unreachable\n        code)\n        \"\"\"\n        self._check_unreachable(node)\n\n    @utils.check_messages('unreachable', 'lost-exception')\n    def visit_break(self, node):\n        \"\"\"1 - check is the node has a right sibling (if so, that's some\n        unreachable code)\n        2 - check is the node is inside the finally clause of a try...finally\n        block\n        \"\"\"\n        # 1 - Is it right sibling ?\n        self._check_unreachable(node)\n        # 2 - Is it inside final body of a try...finally bloc ?\n        self._check_not_in_finally(node, 'break', (astroid.For, astroid.While,))\n\n    @utils.check_messages('unreachable')\n    def visit_raise(self, node):\n        \"\"\"check if the node has a right sibling (if so, that's some unreachable\n        code)\n        \"\"\"\n        self._check_unreachable(node)\n\n    @utils.check_messages('exec-used')\n    def visit_exec(self, node):\n        \"\"\"just print a warning on exec statements\"\"\"\n        self.add_message('exec-used', node=node)\n\n    @utils.check_messages('eval-used', 'exec-used', 'bad-reversed-sequence')\n    def visit_call(self, node):\n        \"\"\"visit a CallFunc node -> check if this is not a blacklisted builtin\n        call and check for * or ** use\n        \"\"\"\n        if isinstance(node.func, astroid.Name):\n            name = node.func.name\n            # ignore the name if it's not a builtin (i.e. not defined in the\n            # locals nor globals scope)\n            if not (name in node.frame() or\n                    name in node.root()):\n                if name == 'exec':\n                    self.add_message('exec-used', node=node)\n                elif name == 'reversed':\n                    self._check_reversed(node)\n                elif name == 'eval':\n                    self.add_message('eval-used', node=node)\n\n    @utils.check_messages('assert-on-tuple')\n    def visit_assert(self, node):\n        \"\"\"check the use of an assert statement on a tuple.\"\"\"\n        if node.fail is None and isinstance(node.test, astroid.Tuple) and \\\n                len(node.test.elts) == 2:\n            self.add_message('assert-on-tuple', node=node)\n\n    @utils.check_messages('duplicate-key')\n    def visit_dict(self, node):\n        \"\"\"check duplicate key in dictionary\"\"\"\n        keys = set()\n        for k, _ in node.items:\n            if isinstance(k, astroid.Const):\n                key = k.value\n                if key in keys:\n                    self.add_message('duplicate-key', node=node, args=key)\n                keys.add(key)\n\n    def visit_tryfinally(self, node):\n        \"\"\"update try...finally flag\"\"\"\n        self._tryfinallys.append(node)\n\n    def leave_tryfinally(self, node): # pylint: disable=unused-argument\n        \"\"\"update try...finally flag\"\"\"\n        self._tryfinallys.pop()\n\n    def _check_unreachable(self, node):\n        \"\"\"check unreachable code\"\"\"\n        unreach_stmt = node.next_sibling()\n        if unreach_stmt is not None:\n            self.add_message('unreachable', node=unreach_stmt)\n\n    def _check_not_in_finally(self, node, node_name, breaker_classes=()):\n        \"\"\"check that a node is not inside a finally clause of a\n        try...finally statement.\n        If we found before a try...finally bloc a parent which its type is\n        in breaker_classes, we skip the whole check.\"\"\"\n        # if self._tryfinallys is empty, we're not a in try...finally bloc\n        if not self._tryfinallys:\n            return\n        # the node could be a grand-grand...-children of the try...finally\n        _parent = node.parent\n        _node = node\n        while _parent and not isinstance(_parent, breaker_classes):\n            if hasattr(_parent, 'finalbody') and _node in _parent.finalbody:\n                self.add_message('lost-exception', node=node, args=node_name)\n                return\n            _node = _parent\n            _parent = _node.parent\n\n    def _check_reversed(self, node):\n        \"\"\" check that the argument to `reversed` is a sequence \"\"\"\n        try:\n            argument = utils.safe_infer(utils.get_argument_from_call(node, position=0))\n        except utils.NoSuchArgumentError:\n            pass\n        else:\n            if argument is astroid.YES:\n                return\n            if argument is None:\n                # Nothing was infered.\n                # Try to see if we have iter().\n                if isinstance(node.args[0], astroid.Call):\n                    try:\n                        func = next(node.args[0].func.infer())\n                    except astroid.InferenceError:\n                        return\n                    if (getattr(func, 'name', None) == 'iter' and\n                            utils.is_builtin_object(func)):\n                        self.add_message('bad-reversed-sequence', node=node)\n                return\n\n            if isinstance(argument, astroid.Instance):\n                if (argument._proxied.name == 'dict' and\n                        utils.is_builtin_object(argument._proxied)):\n                    self.add_message('bad-reversed-sequence', node=node)\n                    return\n                elif any(ancestor.name == 'dict' and utils.is_builtin_object(ancestor)\n                         for ancestor in argument._proxied.ancestors()):\n                    # Mappings aren't accepted by reversed(), unless\n                    # they provide explicitly a __reversed__ method.\n                    try:\n                        argument.locals[REVERSED_PROTOCOL_METHOD]\n                    except KeyError:\n                        self.add_message('bad-reversed-sequence', node=node)\n                    return\n\n                for methods in REVERSED_METHODS:\n                    for meth in methods:\n                        try:\n                            argument.getattr(meth)\n                        except astroid.NotFoundError:\n                            break\n                    else:\n                        break\n                else:\n                    self.add_message('bad-reversed-sequence', node=node)\n            elif not isinstance(argument, (astroid.List, astroid.Tuple)):\n                # everything else is not a proper sequence for reversed()\n                self.add_message('bad-reversed-sequence', node=node)\n\n    @utils.check_messages('confusing-with-statement')\n    def visit_with(self, node):\n        if not PY3K:\n            # in Python 2 a \"with\" statement with multiple managers coresponds\n            # to multiple nested AST \"With\" nodes\n            pairs = []\n            parent_node = node.parent\n            if isinstance(parent_node, astroid.With):\n                # we only care about the direct parent, since this method\n                # gets called for each with node anyway\n                pairs.extend(parent_node.items)\n            pairs.extend(node.items)\n        else:\n            # in PY3K a \"with\" statement with multiple managers coresponds\n            # to one AST \"With\" node with multiple items\n            pairs = node.items\n        if pairs:\n            for prev_pair, pair in zip(pairs, pairs[1:]):\n                if (isinstance(prev_pair[1], astroid.AssignName) and\n                        (pair[1] is None and not isinstance(pair[0], astroid.Call))):\n                    # don't emit a message if the second is a function call\n                    # there's no way that can be mistaken for a name assignment\n                    if PY3K or node.lineno == node.parent.lineno:\n                        # if the line number doesn't match\n                        # we assume it's a nested \"with\"\n                        self.add_message('confusing-with-statement', node=node)\n\n\n_NAME_TYPES = {\n    'module': (MOD_NAME_RGX, 'module'),\n    'const': (CONST_NAME_RGX, 'constant'),\n    'class': (CLASS_NAME_RGX, 'class'),\n    'function': (DEFAULT_NAME_RGX, 'function'),\n    'method': (DEFAULT_NAME_RGX, 'method'),\n    'attr': (DEFAULT_NAME_RGX, 'attribute'),\n    'argument': (DEFAULT_NAME_RGX, 'argument'),\n    'variable': (DEFAULT_NAME_RGX, 'variable'),\n    'class_attribute': (CLASS_ATTRIBUTE_RGX, 'class attribute'),\n    'inlinevar': (COMP_VAR_RGX, 'inline iteration'),\n}\n\ndef _create_naming_options():\n    name_options = []\n    for name_type, (rgx, human_readable_name) in six.iteritems(_NAME_TYPES):\n        name_type = name_type.replace('_', '-')\n        name_options.append((\n            '%s-rgx' % (name_type,),\n            {'default': rgx, 'type': 'regexp', 'metavar': '<regexp>',\n             'help': 'Regular expression matching correct %s names' % (human_readable_name,)}))\n        name_options.append((\n            '%s-name-hint' % (name_type,),\n            {'default': rgx.pattern, 'type': 'string', 'metavar': '<string>',\n             'help': 'Naming hint for %s names' % (human_readable_name,)}))\n    return tuple(name_options)\n\nclass NameChecker(_BasicChecker):\n    msgs = {\n        'C0102': ('Black listed name \"%s\"',\n                  'blacklisted-name',\n                  'Used when the name is listed in the black list (unauthorized '\n                  'names).'),\n        'C0103': ('Invalid %s name \"%s\"%s',\n                  'invalid-name',\n                  'Used when the name doesn\\'t match the regular expression '\n                  'associated to its type (constant, variable, class...).'),\n    }\n\n    options = (('good-names',\n                {'default' : ('i', 'j', 'k', 'ex', 'Run', '_'),\n                 'type' :'csv', 'metavar' : '<names>',\n                 'help' : 'Good variable names which should always be accepted,'\n                          ' separated by a comma'}\n               ),\n               ('bad-names',\n                {'default' : ('foo', 'bar', 'baz', 'toto', 'tutu', 'tata'),\n                 'type' :'csv', 'metavar' : '<names>',\n                 'help' : 'Bad variable names which should always be refused, '\n                          'separated by a comma'}\n               ),\n               ('name-group',\n                {'default' : (),\n                 'type' :'csv', 'metavar' : '<name1:name2>',\n                 'help' : ('Colon-delimited sets of names that determine each'\n                           ' other\\'s naming style when the name regexes'\n                           ' allow several styles.')}\n               ),\n               ('include-naming-hint',\n                {'default': False, 'type' : 'yn', 'metavar' : '<y_or_n>',\n                 'help': 'Include a hint for the correct naming format with invalid-name'}\n               ),\n               ('property-classes',\n                {'default': ('abc.abstractproperty',),\n                 'type': 'csv',\n                 'metavar': '<decorator names>',\n                 'help': 'List of decorators that produce properties, such as '\n                         'abc.abstractproperty. Add to this list to register '\n                         'other decorators that produce valid properties.'}\n               ),\n              ) + _create_naming_options()\n\n\n    def __init__(self, linter):\n        _BasicChecker.__init__(self, linter)\n        self._name_category = {}\n        self._name_group = {}\n        self._bad_names = {}\n\n    def open(self):\n        self.stats = self.linter.add_stats(badname_module=0,\n                                           badname_class=0, badname_function=0,\n                                           badname_method=0, badname_attr=0,\n                                           badname_const=0,\n                                           badname_variable=0,\n                                           badname_inlinevar=0,\n                                           badname_argument=0,\n                                           badname_class_attribute=0)\n        for group in self.config.name_group:\n            for name_type in group.split(':'):\n                self._name_group[name_type] = 'group_%s' % (group,)\n\n    @utils.check_messages('blacklisted-name', 'invalid-name')\n    def visit_module(self, node):\n        self._check_name('module', node.name.split('.')[-1], node)\n        self._bad_names = {}\n\n    def leave_module(self, node): # pylint: disable=unused-argument\n        for all_groups in six.itervalues(self._bad_names):\n            if len(all_groups) < 2:\n                continue\n            groups = collections.defaultdict(list)\n            min_warnings = sys.maxsize\n            for group in six.itervalues(all_groups):\n                groups[len(group)].append(group)\n                min_warnings = min(len(group), min_warnings)\n            if len(groups[min_warnings]) > 1:\n                by_line = sorted(groups[min_warnings],\n                                 key=lambda group: min(warning[0].lineno for warning in group))\n                warnings = itertools.chain(*by_line[1:])\n            else:\n                warnings = groups[min_warnings][0]\n            for args in warnings:\n                self._raise_name_warning(*args)\n\n    @utils.check_messages('blacklisted-name', 'invalid-name')\n    def visit_classdef(self, node):\n        self._check_name('class', node.name, node)\n        for attr, anodes in six.iteritems(node.instance_attrs):\n            if not any(node.instance_attr_ancestors(attr)):\n                self._check_name('attr', attr, anodes[0])\n\n    @utils.check_messages('blacklisted-name', 'invalid-name')\n    def visit_functiondef(self, node):\n        # Do not emit any warnings if the method is just an implementation\n        # of a base class method.\n        confidence = interfaces.HIGH\n        if node.is_method():\n            if utils.overrides_a_method(node.parent.frame(), node.name):\n                return\n            confidence = (interfaces.INFERENCE if utils.has_known_bases(node.parent.frame())\n                          else interfaces.INFERENCE_FAILURE)\n\n        self._check_name(_determine_function_name_type(node,\n                                                       config=self.config),\n                         node.name, node, confidence)\n        # Check argument names\n        args = node.args.args\n        if args is not None:\n            self._recursive_check_names(args, node)\n\n    visit_asyncfunctiondef = visit_functiondef\n\n    @utils.check_messages('blacklisted-name', 'invalid-name')\n    def visit_global(self, node):\n        for name in node.names:\n            self._check_name('const', name, node)\n\n    @utils.check_messages('blacklisted-name', 'invalid-name')\n    def visit_assignname(self, node):\n        \"\"\"check module level assigned names\"\"\"\n        frame = node.frame()\n        ass_type = node.assign_type()\n        if isinstance(ass_type, astroid.Comprehension):\n            self._check_name('inlinevar', node.name, node)\n        elif isinstance(frame, astroid.Module):\n            if isinstance(ass_type, astroid.Assign) and not in_loop(ass_type):\n                if isinstance(utils.safe_infer(ass_type.value), astroid.ClassDef):\n                    self._check_name('class', node.name, node)\n                else:\n                    if not _redefines_import(node):\n                        # Don't emit if the name redefines an import\n                        # in an ImportError except handler.\n                        self._check_name('const', node.name, node)\n            elif isinstance(ass_type, astroid.ExceptHandler):\n                self._check_name('variable', node.name, node)\n        elif isinstance(frame, astroid.FunctionDef):\n            # global introduced variable aren't in the function locals\n            if node.name in frame and node.name not in frame.argnames():\n                if not _redefines_import(node):\n                    self._check_name('variable', node.name, node)\n        elif isinstance(frame, astroid.ClassDef):\n            if not list(frame.local_attr_ancestors(node.name)):\n                self._check_name('class_attribute', node.name, node)\n\n    def _recursive_check_names(self, args, node):\n        \"\"\"check names in a possibly recursive list <arg>\"\"\"\n        for arg in args:\n            if isinstance(arg, astroid.AssignName):\n                self._check_name('argument', arg.name, node)\n            else:\n                self._recursive_check_names(arg.elts, node)\n\n    def _find_name_group(self, node_type):\n        return self._name_group.get(node_type, node_type)\n\n    def _raise_name_warning(self, node, node_type, name, confidence):\n        type_label = _NAME_TYPES[node_type][1]\n        hint = ''\n        if self.config.include_naming_hint:\n            hint = ' (hint: %s)' % (getattr(self.config, node_type + '_name_hint'))\n        self.add_message('invalid-name', node=node, args=(type_label, name, hint),\n                         confidence=confidence)\n        self.stats['badname_' + node_type] += 1\n\n    def _check_name(self, node_type, name, node, confidence=interfaces.HIGH):\n        \"\"\"check for a name using the type's regexp\"\"\"\n        if utils.is_inside_except(node):\n            clobbering, _ = utils.clobber_in_except(node)\n            if clobbering:\n                return\n        if name in self.config.good_names:\n            return\n        if name in self.config.bad_names:\n            self.stats['badname_' + node_type] += 1\n            self.add_message('blacklisted-name', node=node, args=name)\n            return\n        regexp = getattr(self.config, node_type + '_rgx')\n        match = regexp.match(name)\n\n        if _is_multi_naming_match(match, node_type, confidence):\n            name_group = self._find_name_group(node_type)\n            bad_name_group = self._bad_names.setdefault(name_group, {})\n            warnings = bad_name_group.setdefault(match.lastgroup, [])\n            warnings.append((node, node_type, name, confidence))\n\n        if match is None:\n            self._raise_name_warning(node, node_type, name, confidence)\n\n\nclass DocStringChecker(_BasicChecker):\n    msgs = {\n        'C0111': ('Missing %s docstring', # W0131\n                  'missing-docstring',\n                  'Used when a module, function, class or method has no docstring.'\n                  'Some special methods like __init__ doesn\\'t necessary require a '\n                  'docstring.'),\n        'C0112': ('Empty %s docstring', # W0132\n                  'empty-docstring',\n                  'Used when a module, function, class or method has an empty '\n                  'docstring (it would be too easy ;).'),\n        }\n    options = (('no-docstring-rgx',\n                {'default' : NO_REQUIRED_DOC_RGX,\n                 'type' : 'regexp', 'metavar' : '<regexp>',\n                 'help' : 'Regular expression which should only match '\n                          'function or class names that do not require a '\n                          'docstring.'}\n               ),\n               ('docstring-min-length',\n                {'default' : -1,\n                 'type' : 'int', 'metavar' : '<int>',\n                 'help': ('Minimum line length for functions/classes that'\n                          ' require docstrings, shorter ones are exempt.')}\n               ),\n              )\n\n\n    def open(self):\n        self.stats = self.linter.add_stats(undocumented_module=0,\n                                           undocumented_function=0,\n                                           undocumented_method=0,\n                                           undocumented_class=0)\n    @utils.check_messages('missing-docstring', 'empty-docstring')\n    def visit_module(self, node):\n        self._check_docstring('module', node)\n\n    @utils.check_messages('missing-docstring', 'empty-docstring')\n    def visit_classdef(self, node):\n        if self.config.no_docstring_rgx.match(node.name) is None:\n            self._check_docstring('class', node)\n\n    @staticmethod\n    def _is_setter_or_deleter(node):\n        names = {'setter', 'deleter'}\n        for decorator in node.decorators.nodes:\n            if (isinstance(decorator, astroid.Attribute)\n                    and decorator.attrname in names):\n                return True\n        return False\n\n    @utils.check_messages('missing-docstring', 'empty-docstring')\n    def visit_functiondef(self, node):\n        if self.config.no_docstring_rgx.match(node.name) is None:\n            ftype = 'method' if node.is_method() else 'function'\n            if node.decorators and self._is_setter_or_deleter(node):\n                return\n\n            if isinstance(node.parent.frame(), astroid.ClassDef):\n                overridden = False\n                confidence = (interfaces.INFERENCE if utils.has_known_bases(node.parent.frame())\n                              else interfaces.INFERENCE_FAILURE)\n                # check if node is from a method overridden by its ancestor\n                for ancestor in node.parent.frame().ancestors():\n                    if node.name in ancestor and \\\n                       isinstance(ancestor[node.name], astroid.FunctionDef):\n                        overridden = True\n                        break\n                self._check_docstring(ftype, node,\n                                      report_missing=not overridden,\n                                      confidence=confidence)\n            else:\n                self._check_docstring(ftype, node)\n\n    visit_asyncfunctiondef = visit_functiondef\n\n    def _check_docstring(self, node_type, node, report_missing=True,\n                         confidence=interfaces.HIGH):\n        \"\"\"check the node has a non empty docstring\"\"\"\n        docstring = node.doc\n        if docstring is None:\n            if not report_missing:\n                return\n            if node.body:\n                lines = node.body[-1].lineno - node.body[0].lineno + 1\n            else:\n                lines = 0\n\n            if node_type == 'module' and not lines:\n                # If the module has no body, there's no reason\n                # to require a docstring.\n                return\n            max_lines = self.config.docstring_min_length\n\n            if node_type != 'module' and max_lines > -1 and lines < max_lines:\n                return\n            self.stats['undocumented_'+node_type] += 1\n            if (node.body and isinstance(node.body[0], astroid.Expr) and\n                    isinstance(node.body[0].value, astroid.Call)):\n                # Most likely a string with a format call. Let's see.\n                func = utils.safe_infer(node.body[0].value.func)\n                if (isinstance(func, astroid.BoundMethod)\n                        and isinstance(func.bound, astroid.Instance)):\n                    # Strings in Python 3, others in Python 2.\n                    if PY3K and func.bound.name == 'str':\n                        return\n                    elif func.bound.name in ('str', 'unicode', 'bytes'):\n                        return\n            self.add_message('missing-docstring', node=node, args=(node_type,),\n                             confidence=confidence)\n        elif not docstring.strip():\n            self.stats['undocumented_'+node_type] += 1\n            self.add_message('empty-docstring', node=node, args=(node_type,),\n                             confidence=confidence)\n\n\nclass PassChecker(_BasicChecker):\n    \"\"\"check if the pass statement is really necessary\"\"\"\n    msgs = {'W0107': ('Unnecessary pass statement',\n                      'unnecessary-pass',\n                      'Used when a \"pass\" statement that can be avoided is '\n                      'encountered.'),\n           }\n    @utils.check_messages('unnecessary-pass')\n    def visit_pass(self, node):\n        if len(node.parent.child_sequence(node)) > 1:\n            self.add_message('unnecessary-pass', node=node)\n\n\nclass LambdaForComprehensionChecker(_BasicChecker):\n    \"\"\"check for using a lambda where a comprehension would do.\n\n    See <http://www.artima.com/weblogs/viewpost.jsp?thread=98196>\n    where GvR says comprehensions would be clearer.\n    \"\"\"\n\n    msgs = {'W0110': ('map/filter on lambda could be replaced by comprehension',\n                      'deprecated-lambda',\n                      'Used when a lambda is the first argument to \"map\" or '\n                      '\"filter\". It could be clearer as a list '\n                      'comprehension or generator expression.',\n                      {'maxversion': (3, 0)}),\n           }\n\n    @utils.check_messages('deprecated-lambda')\n    def visit_call(self, node):\n        \"\"\"visit a CallFunc node, check if map or filter are called with a\n        lambda\n        \"\"\"\n        if not node.args:\n            return\n        if not isinstance(node.args[0], astroid.Lambda):\n            return\n        infered = utils.safe_infer(node.func)\n        if (utils.is_builtin_object(infered)\n                and infered.name in ['map', 'filter']):\n            self.add_message('deprecated-lambda', node=node)\n\n\ndef _is_one_arg_pos_call(call):\n    \"\"\"Is this a call with exactly 1 argument,\n    where that argument is positional?\n    \"\"\"\n    return (isinstance(call, astroid.Call)\n            and len(call.args) == 1 and not call.keywords)\n\n\nclass ComparisonChecker(_BasicChecker):\n    \"\"\"Checks for comparisons\n\n    - singleton comparison: 'expr == True', 'expr == False' and 'expr == None'\n    - yoda condition: 'const \"comp\" right' where comp can be '==', '!=', '<',\n      '<=', '>' or '>=', and right can be a variable, an attribute, a method or\n      a function\n    \"\"\"\n    msgs = {'C0121': ('Comparison to %s should be %s',\n                      'singleton-comparison',\n                      'Used when an expression is compared to singleton '\n                      'values like True, False or None.'),\n            'C0122': ('Comparison should be %s',\n                      'misplaced-comparison-constant',\n                      'Used when the constant is placed on the left side '\n                      'of a comparison. It is usually clearer in intent to '\n                      'place it in the right hand side of the comparison.'),\n            'C0123': ('Using type() instead of isinstance() for a typecheck.',\n                      'unidiomatic-typecheck',\n                      'The idiomatic way to perform an explicit typecheck in '\n                      'Python is to use isinstance(x, Y) rather than '\n                      'type(x) == Y, type(x) is Y. Though there are unusual '\n                      'situations where these give different results.',\n                      {'old_names': [('W0154', 'unidiomatic-typecheck')]}),\n            'R0123': ('Comparison to literal',\n                      'literal-comparison',\n                      'Used when comparing an object to a literal, which is usually '\n                      'what you do not want to do, since you can compare to a different '\n                      'literal than what was expected altogether.'),\n           }\n\n    def _check_singleton_comparison(self, singleton, root_node):\n        if singleton.value is True:\n            suggestion = \"just 'expr' or 'expr is True'\"\n            self.add_message('singleton-comparison',\n                             node=root_node,\n                             args=(True, suggestion))\n        elif singleton.value is False:\n            suggestion = \"'not expr' or 'expr is False'\"\n            self.add_message('singleton-comparison',\n                             node=root_node,\n                             args=(False, suggestion))\n        elif singleton.value is None:\n            self.add_message('singleton-comparison',\n                             node=root_node,\n                             args=(None, \"'expr is None'\"))\n\n    def _check_literal_comparison(self, literal, node):\n        \"\"\"Check if we compare to a literal, which is usually what we do not want to do.\"\"\"\n        nodes = (astroid.List,\n                 astroid.Tuple,\n                 astroid.Dict,\n                 astroid.Set)\n        is_other_literal = isinstance(literal, nodes)\n        is_const = False\n        if isinstance(literal, astroid.Const):\n            if literal.value in (True, False, None):\n                # Not interested in this values.\n                return\n            is_const = isinstance(literal.value, (bytes, str, int, float))\n\n        if is_const or is_other_literal:\n            self.add_message('literal-comparison', node=node)\n\n    def _check_misplaced_constant(self, node, left, right, operator):\n        if isinstance(right, astroid.Const):\n            return\n        operator = REVERSED_COMPS.get(operator, operator)\n        suggestion = '%s %s %r' % (right.as_string(), operator, left.value)\n        self.add_message('misplaced-comparison-constant', node=node,\n                         args=(suggestion,))\n\n    @utils.check_messages('singleton-comparison', 'misplaced-comparison-constant',\n                          'unidiomatic-typecheck', 'literal-comparison')\n    def visit_compare(self, node):\n        self._check_unidiomatic_typecheck(node)\n        # NOTE: this checker only works with binary comparisons like 'x == 42'\n        # but not 'x == y == 42'\n        if len(node.ops) != 1:\n            return\n\n        left = node.left\n        operator, right = node.ops[0]\n        if (operator in ('<', '<=', '>', '>=', '!=', '==')\n                and isinstance(left, astroid.Const)):\n            self._check_misplaced_constant(node, left, right, operator)\n\n        if operator == '==':\n            if isinstance(left, astroid.Const):\n                self._check_singleton_comparison(left, node)\n            elif isinstance(right, astroid.Const):\n                self._check_singleton_comparison(right, node)\n        if operator in ('is', 'is not'):\n            self._check_literal_comparison(right, node)\n\n    def _check_unidiomatic_typecheck(self, node):\n        operator, right = node.ops[0]\n        if operator in TYPECHECK_COMPARISON_OPERATORS:\n            left = node.left\n            if _is_one_arg_pos_call(left):\n                self._check_type_x_is_y(node, left, operator, right)\n\n    def _check_type_x_is_y(self, node, left, operator, right):\n        \"\"\"Check for expressions like type(x) == Y.\"\"\"\n        left_func = utils.safe_infer(left.func)\n        if not (isinstance(left_func, astroid.ClassDef)\n                and left_func.qname() == TYPE_QNAME):\n            return\n\n        if operator in ('is', 'is not') and _is_one_arg_pos_call(right):\n            right_func = utils.safe_infer(right.func)\n            if (isinstance(right_func, astroid.ClassDef)\n                    and right_func.qname() == TYPE_QNAME):\n                # type(x) == type(a)\n                right_arg = utils.safe_infer(right.args[0])\n                if not isinstance(right_arg, LITERAL_NODE_TYPES):\n                    # not e.g. type(x) == type([])\n                    return\n        self.add_message('unidiomatic-typecheck', node=node)\n\n\n\ndef register(linter):\n    \"\"\"required method to auto register this checker\"\"\"\n    linter.register_checker(BasicErrorChecker(linter))\n    linter.register_checker(BasicChecker(linter))\n    linter.register_checker(NameChecker(linter))\n    linter.register_checker(DocStringChecker(linter))\n    linter.register_checker(PassChecker(linter))\n    linter.register_checker(LambdaForComprehensionChecker(linter))\n    linter.register_checker(ComparisonChecker(linter))\n", "idx": 1, "id": 9334, "msg": "Maybe this should be a brain plugin rather than a specific inference function here? We just need to infer these as classes.", "proj": "PyCQA-pylint", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -181,7 +181,7 @@ public class TestBKD extends LuceneTestCase {\n           // Random N dims rect query:\n           int[] queryMin = new int[numDims];\n           int[] queryMax = new int[numDims];    \n-          for(int dim=0;dim<numDims;dim++) {\n+          for(int dim=0;dim<numIndexDims;dim++) {\n             queryMin[dim] = random().nextInt();\n             queryMax[dim] = random().nextInt();\n             if (queryMin[dim] > queryMax[dim]) {", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.lucene.util.bkd;\n\n\nimport java.io.IOException;\nimport java.math.BigInteger;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.BitSet;\nimport java.util.List;\n\nimport org.apache.lucene.index.CorruptIndexException;\nimport org.apache.lucene.index.MergeState;\nimport org.apache.lucene.index.PointValues.IntersectVisitor;\nimport org.apache.lucene.index.PointValues.Relation;\nimport org.apache.lucene.index.PointValues;\nimport org.apache.lucene.mockfile.ExtrasFS;\nimport org.apache.lucene.store.CorruptingIndexOutput;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FilterDirectory;\nimport org.apache.lucene.store.IOContext;\nimport org.apache.lucene.store.IndexInput;\nimport org.apache.lucene.store.IndexOutput;\nimport org.apache.lucene.store.MockDirectoryWrapper;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.FutureArrays;\nimport org.apache.lucene.util.IOUtils;\nimport org.apache.lucene.util.LuceneTestCase;\nimport org.apache.lucene.util.NumericUtils;\nimport org.apache.lucene.util.TestUtil;\n\npublic class TestBKD extends LuceneTestCase {\n\n  public void testBasicInts1D() throws Exception {\n    try (Directory dir = getDirectory(100)) {\n      BKDWriter w = new BKDWriter(100, dir, \"tmp\", 1, 4, 2, 1.0f, 100, true);\n      byte[] scratch = new byte[4];\n      for(int docID=0;docID<100;docID++) {\n        NumericUtils.intToSortableBytes(docID, scratch, 0);\n        w.add(scratch, docID);\n      }\n\n      long indexFP;\n      try (IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT)) {\n        indexFP = w.finish(out);\n      }\n\n      try (IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT)) {\n        in.seek(indexFP);\n        BKDReader r = new BKDReader(in);\n\n        // Simple 1D range query:\n        final int queryMin = 42;\n        final int queryMax = 87;\n\n        final BitSet hits = new BitSet();\n        r.intersect(new IntersectVisitor() {\n            @Override\n            public void visit(int docID) {\n              hits.set(docID);\n              if (VERBOSE) {\n                System.out.println(\"visit docID=\" + docID);\n              }\n            }\n\n            @Override\n            public void visit(int docID, byte[] packedValue) {\n              int x = NumericUtils.sortableBytesToInt(packedValue, 0);\n              if (VERBOSE) {\n                System.out.println(\"visit docID=\" + docID + \" x=\" + x);\n              }\n              if (x >= queryMin && x <= queryMax) {\n                hits.set(docID);\n              }\n            }\n\n            @Override\n            public Relation compare(byte[] minPacked, byte[] maxPacked) {\n              int min = NumericUtils.sortableBytesToInt(minPacked, 0);\n              int max = NumericUtils.sortableBytesToInt(maxPacked, 0);\n              assert max >= min;\n              if (VERBOSE) {\n                System.out.println(\"compare: min=\" + min + \" max=\" + max + \" vs queryMin=\" + queryMin + \" queryMax=\" + queryMax);\n              }\n\n              if (max < queryMin || min > queryMax) {\n                return Relation.CELL_OUTSIDE_QUERY;\n              } else if (min >= queryMin && max <= queryMax) {\n                return Relation.CELL_INSIDE_QUERY;\n              } else {\n                return Relation.CELL_CROSSES_QUERY;\n              }\n            }\n          });\n\n        for(int docID=0;docID<100;docID++) {\n          boolean expected = docID >= queryMin && docID <= queryMax;\n          boolean actual = hits.get(docID);\n          assertEquals(\"docID=\" + docID, expected, actual);\n        }\n      }\n    }\n  }\n\n  public void testRandomIntsNDims() throws Exception {\n    int numDocs = atLeast(1000);\n    try (Directory dir = getDirectory(numDocs)) {\n      int numDims = TestUtil.nextInt(random(), 1, 5);\n      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 100);\n      float maxMB = (float) 3.0 + (3*random().nextFloat());\n      BKDWriter w = new BKDWriter(numDocs, dir, \"tmp\", numDims, 4, maxPointsInLeafNode, maxMB, numDocs, true);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDims=\" + numDims + \" numDocs=\" + numDocs);\n      }\n      int[][] docs = new int[numDocs][];\n      byte[] scratch = new byte[4*numDims];\n      int[] minValue = new int[numDims];\n      int[] maxValue = new int[numDims];\n      Arrays.fill(minValue, Integer.MAX_VALUE);\n      Arrays.fill(maxValue, Integer.MIN_VALUE);\n      for(int docID=0;docID<numDocs;docID++) {\n        int[] values = new int[numDims];\n        if (VERBOSE) {\n          System.out.println(\"  docID=\" + docID);\n        }\n        for(int dim=0;dim<numDims;dim++) {\n          values[dim] = random().nextInt();\n          if (values[dim] < minValue[dim]) {\n            minValue[dim] = values[dim];\n          }\n          if (values[dim] > maxValue[dim]) {\n            maxValue[dim] = values[dim];\n          }\n          NumericUtils.intToSortableBytes(values[dim], scratch, dim * Integer.BYTES);\n          if (VERBOSE) {\n            System.out.println(\"    \" + dim + \" -> \" + values[dim]);\n          }\n        }\n        docs[docID] = values;\n        w.add(scratch, docID);\n      }\n\n      long indexFP;\n      try (IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT)) {\n        indexFP = w.finish(out);\n      }\n\n      try (IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT)) {\n        in.seek(indexFP);\n        BKDReader r = new BKDReader(in);\n\n        byte[] minPackedValue = r.getMinPackedValue();\n        byte[] maxPackedValue = r.getMaxPackedValue();\n        for(int dim=0;dim<numDims;dim++) {\n          assertEquals(minValue[dim], NumericUtils.sortableBytesToInt(minPackedValue, dim * Integer.BYTES));\n          assertEquals(maxValue[dim], NumericUtils.sortableBytesToInt(maxPackedValue, dim * Integer.BYTES));\n        }\n\n        int iters = atLeast(100);\n        for(int iter=0;iter<iters;iter++) {\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: iter=\" + iter);\n          }\n\n          // Random N dims rect query:\n          int[] queryMin = new int[numDims];\n          int[] queryMax = new int[numDims];    \n          for(int dim=0;dim<numDims;dim++) {\n            queryMin[dim] = random().nextInt();\n            queryMax[dim] = random().nextInt();\n            if (queryMin[dim] > queryMax[dim]) {\n              int x = queryMin[dim];\n              queryMin[dim] = queryMax[dim];\n              queryMax[dim] = x;\n            }\n          }\n\n          final BitSet hits = new BitSet();\n          r.intersect(new IntersectVisitor() {\n            @Override\n            public void visit(int docID) {\n              hits.set(docID);\n              //System.out.println(\"visit docID=\" + docID);\n            }\n\n            @Override\n            public void visit(int docID, byte[] packedValue) {\n              //System.out.println(\"visit check docID=\" + docID);\n              for(int dim=0;dim<numDims;dim++) {\n                int x = NumericUtils.sortableBytesToInt(packedValue, dim * Integer.BYTES);\n                if (x < queryMin[dim] || x > queryMax[dim]) {\n                  //System.out.println(\"  no\");\n                  return;\n                }\n              }\n\n              //System.out.println(\"  yes\");\n              hits.set(docID);\n            }\n\n            @Override\n            public Relation compare(byte[] minPacked, byte[] maxPacked) {\n              boolean crosses = false;\n              for(int dim=0;dim<numDims;dim++) {\n                int min = NumericUtils.sortableBytesToInt(minPacked, dim * Integer.BYTES);\n                int max = NumericUtils.sortableBytesToInt(maxPacked, dim * Integer.BYTES);\n                assert max >= min;\n\n                if (max < queryMin[dim] || min > queryMax[dim]) {\n                  return Relation.CELL_OUTSIDE_QUERY;\n                } else if (min < queryMin[dim] || max > queryMax[dim]) {\n                  crosses = true;\n                }\n              }\n\n              if (crosses) {\n                return Relation.CELL_CROSSES_QUERY;\n              } else {\n                return Relation.CELL_INSIDE_QUERY;\n              }\n            }\n          });\n\n          for(int docID=0;docID<numDocs;docID++) {\n            int[] docValues = docs[docID];\n            boolean expected = true;\n            for(int dim=0;dim<numDims;dim++) {\n              int x = docValues[dim];\n              if (x < queryMin[dim] || x > queryMax[dim]) {\n                expected = false;\n                break;\n              }\n            }\n            boolean actual = hits.get(docID);\n            assertEquals(\"docID=\" + docID, expected, actual);\n          }\n        }\n      }\n    }\n  }\n\n  // Tests on N-dimensional points where each dimension is a BigInteger\n  public void testBigIntNDims() throws Exception {\n\n    int numDocs = atLeast(1000);\n    try (Directory dir = getDirectory(numDocs)) {\n      int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n      int numDims = TestUtil.nextInt(random(), 1, 5);\n      int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 100);\n      float maxMB = (float) 3.0 + (3*random().nextFloat());\n      BKDWriter w = new BKDWriter(numDocs, dir, \"tmp\", numDims, numBytesPerDim, maxPointsInLeafNode, maxMB, numDocs, true);\n      BigInteger[][] docs = new BigInteger[numDocs][];\n\n      byte[] scratch = new byte[numBytesPerDim*numDims];\n      for(int docID=0;docID<numDocs;docID++) {\n        BigInteger[] values = new BigInteger[numDims];\n        if (VERBOSE) {\n          System.out.println(\"  docID=\" + docID);\n        }\n        for(int dim=0;dim<numDims;dim++) {\n          values[dim] = randomBigInt(numBytesPerDim);\n          NumericUtils.bigIntToSortableBytes(values[dim], numBytesPerDim, scratch, dim * numBytesPerDim);\n          if (VERBOSE) {\n            System.out.println(\"    \" + dim + \" -> \" + values[dim]);\n          }\n        }\n        docs[docID] = values;\n        w.add(scratch, docID);\n      }\n\n      long indexFP;\n      try (IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT)) {\n        indexFP = w.finish(out);\n      }\n\n      try (IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT)) {\n        in.seek(indexFP);\n        BKDReader r = new BKDReader(in);\n\n        int iters = atLeast(100);\n        for(int iter=0;iter<iters;iter++) {\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: iter=\" + iter);\n          }\n\n          // Random N dims rect query:\n          BigInteger[] queryMin = new BigInteger[numDims];\n          BigInteger[] queryMax = new BigInteger[numDims];    \n          for(int dim=0;dim<numDims;dim++) {\n            queryMin[dim] = randomBigInt(numBytesPerDim);\n            queryMax[dim] = randomBigInt(numBytesPerDim);\n            if (queryMin[dim].compareTo(queryMax[dim]) > 0) {\n              BigInteger x = queryMin[dim];\n              queryMin[dim] = queryMax[dim];\n              queryMax[dim] = x;\n            }\n          }\n\n          final BitSet hits = new BitSet();\n          r.intersect(new IntersectVisitor() {\n            @Override\n            public void visit(int docID) {\n              hits.set(docID);\n              //System.out.println(\"visit docID=\" + docID);\n            }\n\n            @Override\n            public void visit(int docID, byte[] packedValue) {\n              //System.out.println(\"visit check docID=\" + docID);\n              for(int dim=0;dim<numDims;dim++) {\n                BigInteger x = NumericUtils.sortableBytesToBigInt(packedValue, dim * numBytesPerDim, numBytesPerDim);\n                if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {\n                  //System.out.println(\"  no\");\n                  return;\n                }\n              }\n\n              //System.out.println(\"  yes\");\n              hits.set(docID);\n            }\n\n            @Override\n            public Relation compare(byte[] minPacked, byte[] maxPacked) {\n              boolean crosses = false;\n              for(int dim=0;dim<numDims;dim++) {\n                BigInteger min = NumericUtils.sortableBytesToBigInt(minPacked, dim * numBytesPerDim, numBytesPerDim);\n                BigInteger max = NumericUtils.sortableBytesToBigInt(maxPacked, dim * numBytesPerDim, numBytesPerDim);\n                assert max.compareTo(min) >= 0;\n\n                if (max.compareTo(queryMin[dim]) < 0 || min.compareTo(queryMax[dim]) > 0) {\n                  return Relation.CELL_OUTSIDE_QUERY;\n                } else if (min.compareTo(queryMin[dim]) < 0 || max.compareTo(queryMax[dim]) > 0) {\n                  crosses = true;\n                }\n              }\n\n              if (crosses) {\n                return Relation.CELL_CROSSES_QUERY;\n              } else {\n                return Relation.CELL_INSIDE_QUERY;\n              }\n            }\n          });\n\n          for(int docID=0;docID<numDocs;docID++) {\n            BigInteger[] docValues = docs[docID];\n            boolean expected = true;\n            for(int dim=0;dim<numDims;dim++) {\n              BigInteger x = docValues[dim];\n              if (x.compareTo(queryMin[dim]) < 0 || x.compareTo(queryMax[dim]) > 0) {\n                expected = false;\n                break;\n              }\n            }\n            boolean actual = hits.get(docID);\n            assertEquals(\"docID=\" + docID, expected, actual);\n          }\n        }\n      }\n    }\n  }\n\n  /** Make sure we close open files, delete temp files, etc., on exception */\n  public void testWithExceptions() throws Exception {\n    int numDocs = atLeast(10000);\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        values[dim] = new byte[numBytesPerDim];\n        random().nextBytes(values[dim]);\n      }\n      docValues[docID] = values;\n    }\n\n    double maxMBHeap = 0.05;\n    // Keep retrying until we 1) we allow a big enough heap, and 2) we hit a random IOExc from MDW:\n    boolean done = false;\n    while (done == false) {\n      MockDirectoryWrapper dir = newMockFSDirectory(createTempDir());\n      try {\n        dir.setRandomIOExceptionRate(0.05);\n        dir.setRandomIOExceptionRateOnOpen(0.05);\n        verify(dir, docValues, null, numDims, numBytesPerDim, 50, maxMBHeap);\n      } catch (IllegalArgumentException iae) {\n        // This just means we got a too-small maxMB for the maxPointsInLeafNode; just retry w/ more heap\n        assertTrue(iae.getMessage().contains(\"either increase maxMBSortInHeap or decrease maxPointsInLeafNode\"));\n        maxMBHeap *= 1.25;\n      } catch (IOException ioe) {\n        if (ioe.getMessage().contains(\"a random IOException\")) {\n          // BKDWriter should fully clean up after itself:\n          done = true;\n        } else {\n          throw ioe;\n        }\n      }\n\n      String[] files = Arrays.stream(dir.listAll())\n          .filter(file -> !ExtrasFS.isExtra(file))\n          .toArray(String[]::new);\n      assertTrue(\"files=\" + Arrays.toString(files), files.length == 0);\n      dir.close();\n    }\n  }\n\n  public void testRandomBinaryTiny() throws Exception {\n    doTestRandomBinary(10);\n  }\n\n  public void testRandomBinaryMedium() throws Exception {\n    doTestRandomBinary(10000);\n  }\n\n  @Nightly\n  public void testRandomBinaryBig() throws Exception {\n    doTestRandomBinary(200000);\n  }\n\n  public void testTooLittleHeap() throws Exception { \n    try (Directory dir = getDirectory(0)) {\n      IllegalArgumentException expected = expectThrows(IllegalArgumentException.class, () -> {\n        new BKDWriter(1, dir, \"bkd\", 1, 16, 1000000, 0.001, 0, true);\n      });\n      assertTrue(expected.getMessage().contains(\"either increase maxMBSortInHeap or decrease maxPointsInLeafNode\"));\n    }\n  }\n\n  private void doTestRandomBinary(int count) throws Exception {\n    int numDocs = TestUtil.nextInt(random(), count, count*2);\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        values[dim] = new byte[numBytesPerDim];\n        random().nextBytes(values[dim]);\n      }\n      docValues[docID] = values;\n    }\n\n    verify(docValues, null, numDims, numBytesPerDim);\n  }\n\n  public void testAllEqual() throws Exception {\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    int numDocs = atLeast(1000);\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      if (docID == 0) {\n        byte[][] values = new byte[numDims][];\n        for(int dim=0;dim<numDims;dim++) {\n          values[dim] = new byte[numBytesPerDim];\n          random().nextBytes(values[dim]);\n        }\n        docValues[docID] = values;\n      } else {\n        docValues[docID] = docValues[0];\n      }\n    }\n\n    verify(docValues, null, numDims, numBytesPerDim);\n  }\n\n  public void testOneDimEqual() throws Exception {\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    int numDocs = atLeast(1000);\n    int theEqualDim = random().nextInt(numDims);\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        values[dim] = new byte[numBytesPerDim];\n        random().nextBytes(values[dim]);\n      }\n      docValues[docID] = values;\n      if (docID > 0) {\n        docValues[docID][theEqualDim] = docValues[0][theEqualDim];\n      }\n    }\n\n    // Use a small number of points in leaf blocks to trigger a lot of splitting\n    verify(docValues, null, numDims, numBytesPerDim, TestUtil.nextInt(random(), 20, 50));\n  }\n\n  // This triggers the logic that makes sure all dimensions get indexed\n  // by looking at how many times each dim has been split\n  public void testOneDimLowCard() throws Exception {\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 2, 5);\n\n    int numDocs = atLeast(10000);\n    int theLowCardDim = random().nextInt(numDims);\n\n    byte[] value1 = new byte[numBytesPerDim];\n    random().nextBytes(value1);\n    byte[] value2 = value1.clone();\n    if (value2[numBytesPerDim-1] == 0 || random().nextBoolean()) {\n      value2[numBytesPerDim-1]++;\n    } else {\n      value2[numBytesPerDim-1]--;\n    }\n\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        if (dim == theLowCardDim) {\n          values[dim] = random().nextBoolean() ? value1 : value2;\n        } else {\n          values[dim] = new byte[numBytesPerDim];\n          random().nextBytes(values[dim]);\n        }\n      }\n      docValues[docID] = values;\n    }\n\n    // Use a small number of points in leaf blocks to trigger a lot of splitting\n    verify(docValues, null, numDims, numBytesPerDim, TestUtil.nextInt(random(), 20, 50));\n  }\n\n  // this should trigger run-length compression with lengths that are greater than 255\n  public void testOneDimTwoValues() throws Exception {\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    int numDocs = atLeast(1000);\n    int theDim = random().nextInt(numDims);\n    byte[] value1 = new byte[numBytesPerDim];\n    random().nextBytes(value1);\n    byte[] value2 = new byte[numBytesPerDim];\n    random().nextBytes(value2);\n    byte[][][] docValues = new byte[numDocs][][];\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        if (dim == theDim) {\n          values[dim] = random().nextBoolean() ? value1 : value2;\n        } else {\n          values[dim] = new byte[numBytesPerDim];\n          random().nextBytes(values[dim]);\n        }\n      }\n      docValues[docID] = values;\n    }\n\n    verify(docValues, null, numDims, numBytesPerDim);\n  }\n\n  public void testMultiValued() throws Exception {\n    int numBytesPerDim = TestUtil.nextInt(random(), 2, 30);\n    int numDims = TestUtil.nextInt(random(), 1, 5);\n\n    int numDocs = atLeast(1000);\n    List<byte[][]> docValues = new ArrayList<>();\n    List<Integer> docIDs = new ArrayList<>();\n\n    for(int docID=0;docID<numDocs;docID++) {\n      int numValuesInDoc = TestUtil.nextInt(random(), 1, 5);\n      for(int ord=0;ord<numValuesInDoc;ord++) {\n        docIDs.add(docID);\n        byte[][] values = new byte[numDims][];\n        for(int dim=0;dim<numDims;dim++) {\n          values[dim] = new byte[numBytesPerDim];\n          random().nextBytes(values[dim]);\n        }\n        docValues.add(values);\n      }\n    }\n\n    byte[][][] docValuesArray = docValues.toArray(new byte[docValues.size()][][]);\n    int[] docIDsArray = new int[docIDs.size()];\n    for(int i=0;i<docIDsArray.length;i++) {\n      docIDsArray[i] = docIDs.get(i);\n    }\n\n    verify(docValuesArray, docIDsArray, numDims, numBytesPerDim);\n  }\n\n  /** docIDs can be null, for the single valued case, else it maps value to docID */\n  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim) throws Exception {\n    verify(docValues, docIDs, numDims, numBytesPerDim, TestUtil.nextInt(random(), 50, 1000));\n  }\n\n  private void verify(byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim,\n      int maxPointsInLeafNode) throws Exception {\n    try (Directory dir = getDirectory(docValues.length)) {\n      double maxMB = (float) 3.0 + (3*random().nextDouble());\n      verify(dir, docValues, docIDs, numDims, numBytesPerDim, maxPointsInLeafNode, maxMB);\n    }\n  }\n\n  private void verify(Directory dir, byte[][][] docValues, int[] docIDs, int numDims, int numBytesPerDim, int maxPointsInLeafNode, double maxMB) throws Exception {\n    int numValues = docValues.length;\n    if (VERBOSE) {\n      System.out.println(\"TEST: numValues=\" + numValues + \" numDims=\" + numDims + \" numBytesPerDim=\" + numBytesPerDim + \" maxPointsInLeafNode=\" + maxPointsInLeafNode + \" maxMB=\" + maxMB);\n    }\n\n    List<Long> toMerge = null;\n    List<MergeState.DocMap> docMaps = null;\n    int seg = 0;\n\n    BKDWriter w = new BKDWriter(numValues, dir, \"_\" + seg, numDims, numBytesPerDim, maxPointsInLeafNode, maxMB, docValues.length, false);\n    IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT);\n    IndexInput in = null;\n\n    boolean success = false;\n\n    try {\n\n      byte[] scratch = new byte[numBytesPerDim*numDims];\n      int lastDocIDBase = 0;\n      boolean useMerge = numDims == 1 && numValues >= 10 && random().nextBoolean();\n      int valuesInThisSeg;\n      if (useMerge) {\n        // Sometimes we will call merge with a single segment:\n        valuesInThisSeg = TestUtil.nextInt(random(), numValues/10, numValues);\n      } else {\n        valuesInThisSeg = 0;\n      }\n\n      int segCount = 0;\n\n      for(int ord=0;ord<numValues;ord++) {\n        int docID;\n        if (docIDs == null) {\n          docID = ord;\n        } else {\n          docID = docIDs[ord];\n        }\n        if (VERBOSE) {\n          System.out.println(\"  ord=\" + ord + \" docID=\" + docID + \" lastDocIDBase=\" + lastDocIDBase);\n        }\n        for(int dim=0;dim<numDims;dim++) {\n          if (VERBOSE) {\n            System.out.println(\"    \" + dim + \" -> \" + new BytesRef(docValues[ord][dim]));\n          }\n          System.arraycopy(docValues[ord][dim], 0, scratch, dim*numBytesPerDim, numBytesPerDim);\n        }\n        w.add(scratch, docID-lastDocIDBase);\n\n        segCount++;\n\n        if (useMerge && segCount == valuesInThisSeg) {\n          if (toMerge == null) {\n            toMerge = new ArrayList<>();\n            docMaps = new ArrayList<>();\n          }\n          final int curDocIDBase = lastDocIDBase;\n          docMaps.add(new MergeState.DocMap() {\n              @Override\n              public int get(int docID) {\n                return curDocIDBase + docID;\n              }\n            });\n          toMerge.add(w.finish(out));\n          valuesInThisSeg = TestUtil.nextInt(random(), numValues/10, numValues/2);\n          segCount = 0;\n\n          seg++;\n          maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 1000);\n          maxMB = (float) 3.0 + (3*random().nextDouble());\n          w = new BKDWriter(numValues, dir, \"_\" + seg, numDims, numBytesPerDim, maxPointsInLeafNode, maxMB, docValues.length, false);\n          lastDocIDBase = docID;\n        }\n      }\n\n      long indexFP;\n\n      if (toMerge != null) {\n        if (segCount > 0) {\n          toMerge.add(w.finish(out));\n          final int curDocIDBase = lastDocIDBase;\n          docMaps.add(new MergeState.DocMap() {\n              @Override\n              public int get(int docID) {\n                return curDocIDBase + docID;\n              }\n            });\n        }\n        out.close();\n        in = dir.openInput(\"bkd\", IOContext.DEFAULT);\n        seg++;\n        w = new BKDWriter(numValues, dir, \"_\" + seg, numDims, numBytesPerDim, maxPointsInLeafNode, maxMB, docValues.length, false);\n        List<BKDReader> readers = new ArrayList<>();\n        for(long fp : toMerge) {\n          in.seek(fp);\n          readers.add(new BKDReader(in));\n        }\n        out = dir.createOutput(\"bkd2\", IOContext.DEFAULT);\n        indexFP = w.merge(out, docMaps, readers);\n        out.close();\n        in.close();\n        in = dir.openInput(\"bkd2\", IOContext.DEFAULT);\n      } else {\n        indexFP = w.finish(out);\n        out.close();\n        in = dir.openInput(\"bkd\", IOContext.DEFAULT);\n      }\n\n      in.seek(indexFP);\n      BKDReader r = new BKDReader(in);\n\n      int iters = atLeast(100);\n      for(int iter=0;iter<iters;iter++) {\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: iter=\" + iter);\n        }\n\n        // Random N dims rect query:\n        byte[][] queryMin = new byte[numDims][];\n        byte[][] queryMax = new byte[numDims][];    \n        for(int dim=0;dim<numDims;dim++) {    \n          queryMin[dim] = new byte[numBytesPerDim];\n          random().nextBytes(queryMin[dim]);\n          queryMax[dim] = new byte[numBytesPerDim];\n          random().nextBytes(queryMax[dim]);\n          if (FutureArrays.compareUnsigned(queryMin[dim], 0, numBytesPerDim, queryMax[dim], 0, numBytesPerDim) > 0) {\n            byte[] x = queryMin[dim];\n            queryMin[dim] = queryMax[dim];\n            queryMax[dim] = x;\n          }\n        }\n\n        final BitSet hits = new BitSet();\n        r.intersect(new IntersectVisitor() {\n            @Override\n            public void visit(int docID) {\n              hits.set(docID);\n              //System.out.println(\"visit docID=\" + docID);\n            }\n\n            @Override\n            public void visit(int docID, byte[] packedValue) {\n              //System.out.println(\"visit check docID=\" + docID);\n              for(int dim=0;dim<numDims;dim++) {\n                if (FutureArrays.compareUnsigned(packedValue, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMin[dim], 0, numBytesPerDim) < 0 ||\n                    FutureArrays.compareUnsigned(packedValue, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMax[dim], 0, numBytesPerDim) > 0) {\n                  //System.out.println(\"  no\");\n                  return;\n                }\n              }\n\n              //System.out.println(\"  yes\");\n              hits.set(docID);\n            }\n\n            @Override\n            public Relation compare(byte[] minPacked, byte[] maxPacked) {\n              boolean crosses = false;\n              for(int dim=0;dim<numDims;dim++) {\n                if (FutureArrays.compareUnsigned(maxPacked, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMin[dim], 0, numBytesPerDim) < 0 ||\n                    FutureArrays.compareUnsigned(minPacked, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMax[dim], 0, numBytesPerDim) > 0) {\n                  return Relation.CELL_OUTSIDE_QUERY;\n                } else if (FutureArrays.compareUnsigned(minPacked, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMin[dim], 0, numBytesPerDim) < 0 ||\n                           FutureArrays.compareUnsigned(maxPacked, dim * numBytesPerDim, dim * numBytesPerDim + numBytesPerDim, queryMax[dim], 0, numBytesPerDim) > 0) {\n                  crosses = true;\n                }\n              }\n\n              if (crosses) {\n                return Relation.CELL_CROSSES_QUERY;\n              } else {\n                return Relation.CELL_INSIDE_QUERY;\n              }\n            }\n          });\n\n        BitSet expected = new BitSet();\n        for(int ord=0;ord<numValues;ord++) {\n          boolean matches = true;\n          for(int dim=0;dim<numDims;dim++) {\n            byte[] x = docValues[ord][dim];\n            if (FutureArrays.compareUnsigned(x, 0, numBytesPerDim, queryMin[dim], 0, numBytesPerDim) < 0 ||\n                FutureArrays.compareUnsigned(x, 0, numBytesPerDim, queryMax[dim], 0, numBytesPerDim) > 0) {\n              matches = false;\n              break;\n            }\n          }\n\n          if (matches) {\n            int docID;\n            if (docIDs == null) {\n              docID = ord;\n            } else {\n              docID = docIDs[ord];\n            }\n            expected.set(docID);\n          }\n        }\n\n        int limit = Math.max(expected.length(), hits.length());\n        for(int docID=0;docID<limit;docID++) {\n          assertEquals(\"docID=\" + docID, expected.get(docID), hits.get(docID));\n        }\n      }\n      in.close();\n      dir.deleteFile(\"bkd\");\n      if (toMerge != null) {\n        dir.deleteFile(\"bkd2\");\n      }\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.closeWhileHandlingException(w, in, out);\n        IOUtils.deleteFilesIgnoringExceptions(dir, \"bkd\", \"bkd2\");\n      }\n    }\n  }\n\n  private BigInteger randomBigInt(int numBytes) {\n    BigInteger x = new BigInteger(numBytes*8-1, random());\n    if (random().nextBoolean()) {\n      x = x.negate();\n    }\n    return x;\n  }\n\n  private Directory getDirectory(int numPoints) {\n    Directory dir;\n    if (numPoints > 100000) {\n      dir = newFSDirectory(createTempDir(\"TestBKDTree\"));\n    } else {\n      dir = newDirectory();\n    }\n    return dir;\n  }\n\n  /** Make sure corruption on an input sort file is caught, even if BKDWriter doesn't get angry */\n  public void testBitFlippedOnPartition1() throws Exception {\n\n    // Generate fixed data set:\n    int numDocs = atLeast(10000);\n    int numBytesPerDim = 4;\n    int numDims = 3;\n\n    byte[][][] docValues = new byte[numDocs][][];\n    byte counter = 0;\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        values[dim] = new byte[numBytesPerDim];\n        for(int i=0;i<values[dim].length;i++) {\n          values[dim][i] = counter;\n          counter++;\n        }\n      }\n      docValues[docID] = values;\n    }\n\n    try (Directory dir0 = newMockDirectory()) {\n\n      Directory dir = new FilterDirectory(dir0) {\n        boolean corrupted;\n        @Override\n        public IndexOutput createTempOutput(String prefix, String suffix, IOContext context) throws IOException {\n          IndexOutput out = in.createTempOutput(prefix, suffix, context);\n          if (corrupted == false && prefix.equals(\"_0_bkd1\") && suffix.equals(\"sort\")) {\n            corrupted = true;\n            return new CorruptingIndexOutput(dir0, 22, out);\n          } else {\n            return out;\n          }\n        }\n      };\n\n      CorruptIndexException e = expectThrows(CorruptIndexException.class, () -> {\n          verify(dir, docValues, null, numDims, numBytesPerDim, 50, 0.1);\n        });\n      assertTrue(e.getMessage().contains(\"checksum failed (hardware problem?)\"));\n    }\n  }\n\n  /** Make sure corruption on a recursed partition is caught, when BKDWriter does get angry */\n  public void testBitFlippedOnPartition2() throws Exception {\n\n    // Generate fixed data set:\n    int numDocs = atLeast(10000);\n    int numBytesPerDim = 4;\n    int numDims = 3;\n\n    byte[][][] docValues = new byte[numDocs][][];\n    byte counter = 0;\n\n    for(int docID=0;docID<numDocs;docID++) {\n      byte[][] values = new byte[numDims][];\n      for(int dim=0;dim<numDims;dim++) {\n        values[dim] = new byte[numBytesPerDim];\n        for(int i=0;i<values[dim].length;i++) {\n          values[dim][i] = counter;\n          counter++;\n        }\n      }\n      docValues[docID] = values;\n    }\n\n    try (Directory dir0 = newMockDirectory()) {\n\n      Directory dir = new FilterDirectory(dir0) {\n        boolean corrupted;\n        @Override\n        public IndexOutput createTempOutput(String prefix, String suffix, IOContext context) throws IOException {\n          IndexOutput out = in.createTempOutput(prefix, suffix, context);\n          //System.out.println(\"prefix=\" + prefix + \" suffix=\" + suffix);\n          if (corrupted == false && suffix.equals(\"bkd_left1\")) {\n            //System.out.println(\"now corrupt byte=\" + x + \" prefix=\" + prefix + \" suffix=\" + suffix);\n            corrupted = true;\n            return new CorruptingIndexOutput(dir0, 22072, out);\n          } else {\n            return out;\n          }\n        }\n      };\n\n      Throwable t = expectThrows(CorruptIndexException.class, () -> {\n          verify(dir, docValues, null, numDims, numBytesPerDim, 50, 0.1);\n        });\n      assertCorruptionDetected(t);\n    }\n  }\n\n  private void assertCorruptionDetected(Throwable t) {\n    if (t instanceof CorruptIndexException) {\n      if (t.getMessage().contains(\"checksum failed (hardware problem?)\")) {\n        return;\n      }\n    }\n\n    for(Throwable suppressed : t.getSuppressed()) {\n      if (suppressed instanceof CorruptIndexException) {\n        if (suppressed.getMessage().contains(\"checksum failed (hardware problem?)\")) {\n          return;\n        }\n      }\n    }\n    fail(\"did not see a suppressed CorruptIndexException\");\n  }\n\n  public void testTieBreakOrder() throws Exception {\n    try (Directory dir = newDirectory()) {\n      int numDocs = 10000;\n      BKDWriter w = new BKDWriter(numDocs+1, dir, \"tmp\", 1, Integer.BYTES, 2, 0.01f, numDocs, true);\n      for(int i=0;i<numDocs;i++) {\n        w.add(new byte[Integer.BYTES], i);\n      }\n\n      IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT);\n      long fp = w.finish(out);\n      out.close();\n\n      IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT);\n      in.seek(fp);\n      BKDReader r = new BKDReader(in);\n      r.intersect(new IntersectVisitor() {\n          int lastDocID = -1;\n\n          @Override\n          public void visit(int docID) {\n            assertTrue(\"lastDocID=\" + lastDocID + \" docID=\" + docID, docID > lastDocID);\n            lastDocID = docID;\n          }\n\n          @Override\n          public void visit(int docID, byte[] packedValue) {\n            visit(docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPacked, byte[] maxPacked) {\n            return Relation.CELL_CROSSES_QUERY;\n          }\n      });\n      in.close();\n    }\n  }\n\n  public void test2DLongOrdsOffline() throws Exception {\n    try (Directory dir = newDirectory()) {\n      int numDocs = 100000;\n      boolean singleValuePerDoc = false;\n      boolean longOrds = true;\n      int offlineSorterMaxTempFiles = TestUtil.nextInt(random(), 2, 20);\n      BKDWriter w = new BKDWriter(numDocs+1, dir, \"tmp\", 2, Integer.BYTES, 2, 0.01f, numDocs,\n                                  singleValuePerDoc, longOrds, 1, offlineSorterMaxTempFiles);\n      byte[] buffer = new byte[2*Integer.BYTES];\n      for(int i=0;i<numDocs;i++) {\n        random().nextBytes(buffer);\n        w.add(buffer, i);\n      }\n\n      IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT);\n      long fp = w.finish(out);\n      out.close();\n\n      IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT);\n      in.seek(fp);\n      BKDReader r = new BKDReader(in);\n      int[] count = new int[1];\n      r.intersect(new IntersectVisitor() {\n\n          @Override\n          public void visit(int docID) {\n            count[0]++;\n          }\n\n          @Override\n          public void visit(int docID, byte[] packedValue) {\n            visit(docID);\n          }\n\n          @Override\n          public Relation compare(byte[] minPacked, byte[] maxPacked) {\n            if (random().nextInt(7) == 1) {\n              return Relation.CELL_CROSSES_QUERY;\n            } else {\n              return Relation.CELL_INSIDE_QUERY;\n            }\n          }\n      });\n      assertEquals(numDocs, count[0]);\n      in.close();\n    }\n  }\n\n  // Claims 16 bytes per dim, but only use the bottom N 1-3 bytes; this would happen e.g. if a user indexes what are actually just short\n  // values as a LongPoint:\n  public void testWastedLeadingBytes() throws Exception {\n    int numDims = TestUtil.nextInt(random(), 1, PointValues.MAX_DIMENSIONS);\n    int bytesPerDim = PointValues.MAX_NUM_BYTES;\n    int bytesUsed = TestUtil.nextInt(random(), 1, 3);\n\n    Directory dir = newFSDirectory(createTempDir());\n    int numDocs = 100000;\n    BKDWriter w = new BKDWriter(numDocs+1, dir, \"tmp\", numDims, bytesPerDim, 32, 1f, numDocs, true);\n    byte[] tmp = new byte[bytesUsed];\n    byte[] buffer = new byte[numDims * bytesPerDim];\n    for(int i=0;i<numDocs;i++) {\n      for(int dim=0;dim<numDims;dim++) {\n        random().nextBytes(tmp);\n        System.arraycopy(tmp, 0, buffer, dim*bytesPerDim+(bytesPerDim-bytesUsed), tmp.length);\n      }\n      w.add(buffer, i);\n    }\n    \n    IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT);\n    long fp = w.finish(out);\n    out.close();\n\n    IndexInput in = dir.openInput(\"bkd\", IOContext.DEFAULT);\n    in.seek(fp);\n    BKDReader r = new BKDReader(in);\n    int[] count = new int[1];\n    r.intersect(new IntersectVisitor() {\n\n        @Override\n        public void visit(int docID) {\n          count[0]++;\n        }\n\n        @Override\n        public void visit(int docID, byte[] packedValue) {\n          visit(docID);\n        }\n\n        @Override\n        public Relation compare(byte[] minPacked, byte[] maxPacked) {\n          if (random().nextInt(7) == 1) {\n            return Relation.CELL_CROSSES_QUERY;\n          } else {\n            return Relation.CELL_INSIDE_QUERY;\n          }\n        }\n      });\n    assertEquals(numDocs, count[0]);\n    in.close();\n    dir.close();\n  }\n\n  public void testEstimatePointCount() throws IOException {\n    Directory dir = newDirectory();\n    final int numValues = atLeast(10000); // make sure to have multiple leaves\n    final int maxPointsInLeafNode = TestUtil.nextInt(random(), 50, 500);\n    final int numBytesPerDim = TestUtil.nextInt(random(), 1, 4);\n    final byte[] pointValue = new byte[numBytesPerDim];\n    final byte[] uniquePointValue = new byte[numBytesPerDim];\n    random().nextBytes(uniquePointValue);\n\n    BKDWriter w = new BKDWriter(numValues, dir, \"_temp\", 1, numBytesPerDim, maxPointsInLeafNode,\n        BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP, numValues, true);\n    for (int i = 0; i < numValues; ++i) {\n      if (i == numValues / 2) {\n        w.add(uniquePointValue, i);\n      } else {\n        do {\n          random().nextBytes(pointValue);\n        } while (Arrays.equals(pointValue, uniquePointValue));\n        w.add(pointValue, i);\n      }\n    }\n    final long indexFP;\n    try (IndexOutput out = dir.createOutput(\"bkd\", IOContext.DEFAULT)) {\n      indexFP = w.finish(out);\n      w.close();\n    }\n    \n    IndexInput pointsIn = dir.openInput(\"bkd\", IOContext.DEFAULT);\n    pointsIn.seek(indexFP);\n    BKDReader points = new BKDReader(pointsIn);\n\n    int actualMaxPointsInLeafNode = numValues;\n    while (actualMaxPointsInLeafNode > maxPointsInLeafNode) {\n      actualMaxPointsInLeafNode = (actualMaxPointsInLeafNode + 1) / 2;\n    }\n\n    // If all points match, then the point count is numLeaves * maxPointsInLeafNode\n    final int numLeaves = Integer.highestOneBit((numValues - 1) / actualMaxPointsInLeafNode) << 1;\n    assertEquals(numLeaves * actualMaxPointsInLeafNode,\n        points.estimatePointCount(new IntersectVisitor() {\n          @Override\n          public void visit(int docID, byte[] packedValue) throws IOException {}\n          \n          @Override\n          public void visit(int docID) throws IOException {}\n          \n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_INSIDE_QUERY;\n          }\n        }));\n\n    // Return 0 if no points match\n    assertEquals(0,\n        points.estimatePointCount(new IntersectVisitor() {\n          @Override\n          public void visit(int docID, byte[] packedValue) throws IOException {}\n          \n          @Override\n          public void visit(int docID) throws IOException {}\n          \n          @Override\n          public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n            return Relation.CELL_OUTSIDE_QUERY;\n          }\n        }));\n\n    // If only one point matches, then the point count is (actualMaxPointsInLeafNode + 1) / 2\n    // in general, or maybe 2x that if the point is a split value\n    final long pointCount = points.estimatePointCount(new IntersectVisitor() {\n      @Override\n      public void visit(int docID, byte[] packedValue) throws IOException {}\n\n      @Override\n      public void visit(int docID) throws IOException {}\n\n      @Override\n      public Relation compare(byte[] minPackedValue, byte[] maxPackedValue) {\n        if (FutureArrays.compareUnsigned(uniquePointValue, 0, numBytesPerDim, maxPackedValue, 0, numBytesPerDim) > 0 ||\n            FutureArrays.compareUnsigned(uniquePointValue, 0, numBytesPerDim, minPackedValue, 0, numBytesPerDim) < 0) {\n          return Relation.CELL_OUTSIDE_QUERY;\n        }\n        return Relation.CELL_CROSSES_QUERY;\n      }\n    });\n    assertTrue(\"\"+pointCount,\n        pointCount == (actualMaxPointsInLeafNode + 1) / 2 || // common case\n        pointCount == 2*((actualMaxPointsInLeafNode + 1) / 2)); // if the point is a split value\n\n    pointsIn.close();\n    dir.close();\n  }\n}\n", "idx": 3, "id": 27665, "msg": "", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -43,7 +43,6 @@ import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.io.Text;\n-import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.mapred.JobClient;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.Master;", "y": 0, "oldf": "/*\n * Copyright 2011 LinkedIn Corp.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n * use this file except in compliance with the License. You may obtain a copy of\n * the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n * License for the specific language governing permissions and limitations under\n * the License.\n */\n\npackage azkaban.security;\n\nimport azkaban.security.commons.HadoopSecurityManager;\nimport azkaban.security.commons.HadoopSecurityManagerException;\nimport azkaban.utils.Props;\nimport azkaban.utils.UndefinedPropertyException;\nimport java.io.DataOutputStream;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\nimport java.net.URL;\nimport java.net.URLClassLoader;\nimport java.security.PrivilegedAction;\nimport java.security.PrivilegedExceptionAction;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ConcurrentMap;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.CommonConfigurationKeys;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.metastore.HiveMetaStoreClient;\nimport org.apache.hadoop.hive.metastore.api.MetaException;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.ipc.RPC;\nimport org.apache.hadoop.mapred.JobClient;\nimport org.apache.hadoop.mapred.JobConf;\nimport org.apache.hadoop.mapred.Master;\nimport org.apache.hadoop.mapreduce.security.TokenCache;\nimport org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier;\nimport org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\nimport org.apache.hadoop.mapreduce.v2.api.HSClientProtocol;\nimport org.apache.hadoop.mapreduce.v2.api.MRClientProtocol;\nimport org.apache.hadoop.mapreduce.v2.api.protocolrecords.CancelDelegationTokenRequest;\nimport org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetDelegationTokenRequest;\nimport org.apache.hadoop.mapreduce.v2.jobhistory.JHAdminConfig;\nimport org.apache.hadoop.net.NetUtils;\nimport org.apache.hadoop.security.Credentials;\nimport org.apache.hadoop.security.SecurityUtil;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.hadoop.security.token.Token;\nimport org.apache.hadoop.security.token.TokenIdentifier;\nimport org.apache.hadoop.yarn.factories.RecordFactory;\nimport org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\nimport org.apache.hadoop.yarn.ipc.YarnRPC;\nimport org.apache.hadoop.yarn.util.ConverterUtils;\nimport org.apache.hadoop.yarn.util.Records;\nimport org.apache.log4j.Logger;\nimport org.apache.thrift.TException;\n\npublic class HadoopSecurityManager_H_2_0 extends HadoopSecurityManager {\n\n  /**\n   * TODO Remove duplicated constants from plugins.\n   *\n   * Azkaban plugins don't depend on a common submodule from which they both can inherit code. Thus,\n   * constants are copied around and any changes to the constant values will break Azkaban. This\n   * needs to be fixed as part of a plugin infrastructure implementation.\n   */\n  public static final String NATIVE_LIB_FOLDER = \"azkaban.native.lib\";\n\n  /**\n   * TODO: This should be exposed as a configurable parameter\n   *\n   * The assumption is that an \"azkaban\" group exists which has access to data created by the\n   * azkaban process. For example, this may include delegation tokens created for other users to run\n   * their jobs.\n   */\n  public static final String GROUP_NAME = \"azkaban\";\n  /**\n   * The Kerberos principal for the job tracker.\n   */\n  public static final String JT_PRINCIPAL = JTConfig.JT_USER_NAME;\n  /**\n   * The Kerberos principal for the resource manager.\n   */\n  public static final String RM_PRINCIPAL = \"yarn.resourcemanager.principal\";\n  // \"mapreduce.jobtracker.kerberos.principal\";\n  public static final String HADOOP_JOB_TRACKER = \"mapred.job.tracker\";\n  public static final String HADOOP_JOB_TRACKER_2 =\n      \"mapreduce.jobtracker.address\";\n  public static final String HADOOP_YARN_RM = \"yarn.resourcemanager.address\";\n  /**\n   * the key that will be used to set proper signature for each of the hcat\n   * token when multiple hcat tokens are required to be fetched.\n   */\n  public static final String HIVE_TOKEN_SIGNATURE_KEY =\n      \"hive.metastore.token.signature\";\n  public static final Text DEFAULT_RENEWER = new Text(\"azkaban mr tokens\");\n  public static final String CHOWN = \"chown\";\n  public static final String CHMOD = \"chmod\";\n  // The file permissions assigned to a Delegation token file on fetch\n  public static final String TOKEN_FILE_PERMISSIONS = \"460\";\n  private static final String FS_HDFS_IMPL_DISABLE_CACHE =\n      \"fs.hdfs.impl.disable.cache\";\n  private static final String OTHER_NAMENODES_TO_GET_TOKEN = \"other_namenodes\";\n  /**\n   * the settings to be defined by user indicating if there are hcat locations\n   * other than the default one the system should pre-fetch hcat token from.\n   * Note: Multiple thrift uris are supported, use comma to separate the values,\n   * values are case insensitive.\n   */\n  private static final String EXTRA_HCAT_LOCATION = \"other_hcat_location\";\n  private static final String AZKABAN_KEYTAB_LOCATION = \"proxy.keytab.location\";\n  private static final String AZKABAN_PRINCIPAL = \"proxy.user\";\n  private static final String OBTAIN_JOBHISTORYSERVER_TOKEN =\n      \"obtain.jobhistoryserver.token\";\n  private final static Logger logger = Logger\n      .getLogger(HadoopSecurityManager_H_2_0.class);\n  private static HadoopSecurityManager hsmInstance = null;\n  private static URLClassLoader ucl;\n  private final RecordFactory recordFactory = RecordFactoryProvider.getRecordFactory(null);\n  private final ExecuteAsUser executeAsUser;\n  private final Configuration conf;\n  private final ConcurrentMap<String, UserGroupInformation> userUgiMap;\n  private UserGroupInformation loginUser = null;\n  private String keytabLocation;\n  private String keytabPrincipal;\n  private boolean shouldProxy = false;\n  private boolean securityEnabled = false;\n\n  private HadoopSecurityManager_H_2_0(final Props props)\n      throws HadoopSecurityManagerException, IOException {\n    this.executeAsUser = new ExecuteAsUser(props.getString(NATIVE_LIB_FOLDER));\n\n    // for now, assume the same/compatible native library, the same/compatible\n    // hadoop-core jar\n    String hadoopHome = props.getString(\"hadoop.home\", null);\n    String hadoopConfDir = props.getString(\"hadoop.conf.dir\", null);\n\n    if (hadoopHome == null) {\n      hadoopHome = System.getenv(\"HADOOP_HOME\");\n    }\n    if (hadoopConfDir == null) {\n      hadoopConfDir = System.getenv(\"HADOOP_CONF_DIR\");\n    }\n\n    final List<URL> resources = new ArrayList<>();\n    URL urlToHadoop = null;\n    if (hadoopConfDir != null) {\n      urlToHadoop = new File(hadoopConfDir).toURI().toURL();\n      logger.info(\"Using hadoop config found in \" + urlToHadoop);\n      resources.add(urlToHadoop);\n    } else if (hadoopHome != null) {\n      urlToHadoop = new File(hadoopHome, \"conf\").toURI().toURL();\n      logger.info(\"Using hadoop config found in \" + urlToHadoop);\n      resources.add(urlToHadoop);\n    } else {\n      logger.info(\"HADOOP_HOME not set, using default hadoop config.\");\n    }\n\n    ucl = new URLClassLoader(resources.toArray(new URL[resources.size()]));\n\n    this.conf = new Configuration();\n    this.conf.setClassLoader(ucl);\n\n    if (props.containsKey(FS_HDFS_IMPL_DISABLE_CACHE)) {\n      logger.info(\"Setting \" + FS_HDFS_IMPL_DISABLE_CACHE + \" to \"\n          + props.get(FS_HDFS_IMPL_DISABLE_CACHE));\n      this.conf.setBoolean(FS_HDFS_IMPL_DISABLE_CACHE,\n          Boolean.valueOf(props.get(FS_HDFS_IMPL_DISABLE_CACHE)));\n    }\n\n    logger.info(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION + \": \"\n        + this.conf.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION));\n    logger.info(CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION + \":  \"\n        + this.conf.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION));\n    logger.info(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY + \": \"\n        + this.conf.get(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY));\n\n    UserGroupInformation.setConfiguration(this.conf);\n\n    this.securityEnabled = UserGroupInformation.isSecurityEnabled();\n    if (this.securityEnabled) {\n      logger.info(\"The Hadoop cluster has enabled security\");\n      this.shouldProxy = true;\n      try {\n\n        this.keytabLocation = props.getString(AZKABAN_KEYTAB_LOCATION);\n        this.keytabPrincipal = props.getString(AZKABAN_PRINCIPAL);\n      } catch (final UndefinedPropertyException e) {\n        throw new HadoopSecurityManagerException(e.getMessage());\n      }\n\n      // try login\n      try {\n        if (this.loginUser == null) {\n          logger.info(\"No login user. Creating login user\");\n          logger.info(\"Using principal from \" + this.keytabPrincipal + \" and \"\n              + this.keytabLocation);\n          UserGroupInformation.loginUserFromKeytab(this.keytabPrincipal,\n              this.keytabLocation);\n          this.loginUser = UserGroupInformation.getLoginUser();\n          logger.info(\"Logged in with user \" + this.loginUser);\n        } else {\n          logger.info(\"loginUser (\" + this.loginUser\n              + \") already created, refreshing tgt.\");\n          this.loginUser.checkTGTAndReloginFromKeytab();\n        }\n      } catch (final IOException e) {\n        throw new HadoopSecurityManagerException(\n            \"Failed to login with kerberos \", e);\n      }\n\n    }\n\n    this.userUgiMap = new ConcurrentHashMap<>();\n\n    logger.info(\"Hadoop Security Manager initialized\");\n  }\n\n  public static HadoopSecurityManager getInstance(final Props props)\n      throws HadoopSecurityManagerException, IOException {\n    if (hsmInstance == null) {\n      synchronized (HadoopSecurityManager_H_2_0.class) {\n        if (hsmInstance == null) {\n          logger.info(\"getting new instance\");\n          hsmInstance = new HadoopSecurityManager_H_2_0(props);\n        }\n      }\n    }\n\n    logger.debug(\"Relogging in from keytab if necessary.\");\n    hsmInstance.reloginFromKeytab();\n\n    return hsmInstance;\n  }\n\n  /**\n   * Create a proxied user based on the explicit user name, taking other\n   * parameters necessary from properties file.\n   */\n  @Override\n  public synchronized UserGroupInformation getProxiedUser(final String userToProxy)\n      throws HadoopSecurityManagerException {\n\n    if (userToProxy == null) {\n      throw new HadoopSecurityManagerException(\"userToProxy can't be null\");\n    }\n\n    UserGroupInformation ugi = this.userUgiMap.get(userToProxy);\n    if (ugi == null) {\n      logger.info(\"proxy user \" + userToProxy\n          + \" not exist. Creating new proxy user\");\n      if (this.shouldProxy) {\n        try {\n          ugi =\n              UserGroupInformation.createProxyUser(userToProxy,\n                  UserGroupInformation.getLoginUser());\n        } catch (final IOException e) {\n          throw new HadoopSecurityManagerException(\n              \"Failed to create proxy user\", e);\n        }\n      } else {\n        ugi = UserGroupInformation.createRemoteUser(userToProxy);\n      }\n      this.userUgiMap.putIfAbsent(userToProxy, ugi);\n    }\n    return ugi;\n  }\n\n  /**\n   * Create a proxied user, taking all parameters, including which user to proxy\n   * from provided Properties.\n   */\n  @Override\n  public UserGroupInformation getProxiedUser(final Props userProp)\n      throws HadoopSecurityManagerException {\n    final String userToProxy = verifySecureProperty(userProp, USER_TO_PROXY);\n    final UserGroupInformation user = getProxiedUser(userToProxy);\n    if (user == null) {\n      throw new HadoopSecurityManagerException(\n          \"Proxy as any user in unsecured grid is not supported!\");\n    }\n    return user;\n  }\n\n  public String verifySecureProperty(final Props props, final String s)\n      throws HadoopSecurityManagerException {\n    final String value = props.getString(s);\n    if (value == null) {\n      throw new HadoopSecurityManagerException(s + \" not set in properties.\");\n    }\n    return value;\n  }\n\n  @Override\n  public FileSystem getFSAsUser(final String user)\n      throws HadoopSecurityManagerException {\n    final FileSystem fs;\n    try {\n      logger.info(\"Getting file system as \" + user);\n      final UserGroupInformation ugi = getProxiedUser(user);\n\n      if (ugi != null) {\n        fs = ugi.doAs(new PrivilegedAction<FileSystem>() {\n\n          @Override\n          public FileSystem run() {\n            try {\n              return FileSystem.get(HadoopSecurityManager_H_2_0.this.conf);\n            } catch (final IOException e) {\n              throw new RuntimeException(e);\n            }\n          }\n        });\n      } else {\n        fs = FileSystem.get(this.conf);\n      }\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to get FileSystem. \", e);\n    }\n    return fs;\n  }\n\n  public boolean shouldProxy() {\n    return this.shouldProxy;\n  }\n\n  @Override\n  public boolean isHadoopSecurityEnabled() {\n    return this.securityEnabled;\n  }\n\n  /*\n   * Gets hadoop tokens for a user to run mapred/pig jobs on a secured cluster\n   */\n  @Override\n  public synchronized void prefetchToken(final File tokenFile,\n      final String userToProxy, final Logger logger)\n      throws HadoopSecurityManagerException {\n    logger.info(\"Getting hadoop tokens for \" + userToProxy);\n\n    final UserGroupInformation proxiedUser = getProxiedUser(userToProxy);\n    try {\n      proxiedUser.doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          getToken(userToProxy);\n          return null;\n        }\n\n        private void getToken(final String userToProxy) throws InterruptedException,\n            IOException, HadoopSecurityManagerException {\n\n          final FileSystem fs = FileSystem.get(HadoopSecurityManager_H_2_0.this.conf);\n          // check if we get the correct FS, and most importantly, the conf\n          logger.info(\"Getting DFS token from \" + fs.getCanonicalServiceName()\n              + fs.getUri());\n          final Token<?> fsToken = fs.getDelegationToken(userToProxy);\n          if (fsToken == null) {\n            logger.error(\"Failed to fetch DFS token for \");\n            throw new HadoopSecurityManagerException(\n                \"Failed to fetch DFS token for \" + userToProxy);\n          }\n          logger.info(\"Created DFS token.\");\n          logger.info(\"Token kind: \" + fsToken.getKind());\n          logger.info(\"Token service: \" + fsToken.getService());\n\n          final JobConf jc = new JobConf(HadoopSecurityManager_H_2_0.this.conf);\n          final JobClient jobClient = new JobClient(jc);\n          logger.info(\"Pre-fetching JT token: Got new JobClient: \" + jc);\n\n          final Token<DelegationTokenIdentifier> mrdt =\n              jobClient.getDelegationToken(new Text(\"mr token\"));\n          if (mrdt == null) {\n            logger.error(\"Failed to fetch JT token for \");\n            throw new HadoopSecurityManagerException(\n                \"Failed to fetch JT token for \" + userToProxy);\n          }\n          logger.info(\"Created JT token.\");\n          logger.info(\"Token kind: \" + mrdt.getKind());\n          logger.info(\"Token service: \" + mrdt.getService());\n\n          jc.getCredentials().addToken(mrdt.getService(), mrdt);\n          jc.getCredentials().addToken(fsToken.getService(), fsToken);\n\n          prepareTokenFile(userToProxy, jc.getCredentials(), tokenFile, logger);\n          // stash them to cancel after use.\n          logger.info(\"Tokens loaded in \" + tokenFile.getAbsolutePath());\n        }\n      });\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\n          \"Failed to get hadoop tokens! \" + e.getMessage() + e.getCause());\n    }\n  }\n\n  private void cancelNameNodeToken(final Token<? extends TokenIdentifier> t,\n      final String userToProxy) throws HadoopSecurityManagerException {\n    try {\n      getProxiedUser(userToProxy).doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          cancelToken(t);\n          return null;\n        }\n\n        private void cancelToken(final Token<?> nt) throws IOException,\n            InterruptedException {\n          nt.cancel(HadoopSecurityManager_H_2_0.this.conf);\n        }\n      });\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to cancel token. \"\n          + e.getMessage() + e.getCause(), e);\n    }\n  }\n\n  private void cancelMRJobTrackerToken(\n      final Token<? extends TokenIdentifier> t, final String userToProxy)\n      throws HadoopSecurityManagerException {\n    try {\n      getProxiedUser(userToProxy).doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          cancelToken((Token<DelegationTokenIdentifier>) t);\n          return null;\n        }\n\n        private void cancelToken(final Token<DelegationTokenIdentifier> jt)\n            throws IOException, InterruptedException {\n          final JobConf jc = new JobConf(HadoopSecurityManager_H_2_0.this.conf);\n          final JobClient jobClient = new JobClient(jc);\n          jobClient.cancelDelegationToken(jt);\n        }\n      });\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to cancel token. \"\n          + e.getMessage() + e.getCause(), e);\n    }\n  }\n\n  private void cancelJhsToken(final Token<? extends TokenIdentifier> t,\n      final String userToProxy) throws HadoopSecurityManagerException {\n    // it appears yarn would clean up this token after app finish, after a long\n    // while though.\n    final org.apache.hadoop.yarn.api.records.Token token =\n        org.apache.hadoop.yarn.api.records.Token.newInstance(t.getIdentifier(),\n            t.getKind().toString(), t.getPassword(), t.getService().toString());\n    final YarnRPC rpc = YarnRPC.create(this.conf);\n    final InetSocketAddress jhsAddress = SecurityUtil.getTokenServiceAddr(t);\n    MRClientProtocol jhsProxy = null;\n    try {\n      jhsProxy =\n          UserGroupInformation.getCurrentUser().doAs(\n              new PrivilegedAction<MRClientProtocol>() {\n                @Override\n                public MRClientProtocol run() {\n                  return (MRClientProtocol) rpc.getProxy(\n                      HSClientProtocol.class, jhsAddress, HadoopSecurityManager_H_2_0.this.conf);\n                }\n              });\n      final CancelDelegationTokenRequest request =\n          Records.newRecord(CancelDelegationTokenRequest.class);\n      request.setDelegationToken(token);\n      jhsProxy.cancelDelegationToken(request);\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to cancel token. \"\n          + e.getMessage() + e.getCause(), e);\n    } finally {\n      RPC.stopProxy(jhsProxy);\n    }\n\n  }\n\n  private void cancelHiveToken(final Token<? extends TokenIdentifier> t,\n      final String userToProxy) throws HadoopSecurityManagerException {\n    try {\n      final HiveConf hiveConf = new HiveConf();\n      final HiveMetaStoreClient hiveClient = new HiveMetaStoreClient(hiveConf);\n      hiveClient.cancelDelegationToken(t.encodeToUrlString());\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to cancel Token. \"\n          + e.getMessage() + e.getCause(), e);\n    }\n  }\n\n  @Override\n  public void cancelTokens(final File tokenFile, final String userToProxy, final Logger logger)\n      throws HadoopSecurityManagerException {\n    // nntoken\n    Credentials cred = null;\n    try {\n      cred =\n          Credentials.readTokenStorageFile(new Path(tokenFile.toURI()),\n              new Configuration());\n      for (final Token<? extends TokenIdentifier> t : cred.getAllTokens()) {\n        logger.info(\"Got token.\");\n        logger.info(\"Token kind: \" + t.getKind());\n        logger.info(\"Token service: \" + t.getService());\n\n        if (t.getKind().equals(new Text(\"HIVE_DELEGATION_TOKEN\"))) {\n          logger.info(\"Cancelling hive token.\");\n          cancelHiveToken(t, userToProxy);\n        } else if (t.getKind().equals(new Text(\"RM_DELEGATION_TOKEN\"))) {\n          logger.info(\"Cancelling mr job tracker token.\");\n          // cancelMRJobTrackerToken(t, userToProxy);\n        } else if (t.getKind().equals(new Text(\"HDFS_DELEGATION_TOKEN\"))) {\n          logger.info(\"Cancelling namenode token.\");\n          // cancelNameNodeToken(t, userToProxy);\n        } else if (t.getKind().equals(new Text(\"MR_DELEGATION_TOKEN\"))) {\n          logger.info(\"Cancelling jobhistoryserver mr token.\");\n          // cancelJhsToken(t, userToProxy);\n        } else {\n          logger.info(\"unknown token type \" + t.getKind());\n        }\n      }\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to cancel tokens \"\n          + e.getMessage() + e.getCause(), e);\n    }\n\n  }\n\n  /**\n   * function to fetch hcat token as per the specified hive configuration and\n   * then store the token in to the credential store specified .\n   *\n   * @param userToProxy String value indicating the name of the user the token will be fetched for.\n   * @param hiveConf the configuration based off which the hive client will be initialized.\n   * @param logger the logger instance which writes the logging content to the job logs.\n   */\n  private Token<DelegationTokenIdentifier> fetchHcatToken(final String userToProxy,\n      final HiveConf hiveConf, final String tokenSignatureOverwrite, final Logger logger)\n      throws IOException, MetaException, TException {\n\n    logger.info(HiveConf.ConfVars.METASTOREURIS.varname + \": \"\n        + hiveConf.get(HiveConf.ConfVars.METASTOREURIS.varname));\n\n    logger.info(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.varname + \": \"\n        + hiveConf.get(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.varname));\n\n    logger.info(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname + \": \"\n        + hiveConf.get(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname));\n\n    final HiveMetaStoreClient hiveClient = new HiveMetaStoreClient(hiveConf);\n    final String hcatTokenStr =\n        hiveClient.getDelegationToken(userToProxy, UserGroupInformation\n            .getLoginUser().getShortUserName());\n    final Token<DelegationTokenIdentifier> hcatToken =\n        new Token<>();\n    hcatToken.decodeFromUrlString(hcatTokenStr);\n\n    // overwrite the value of the service property of the token if the signature\n    // override is specified.\n    if (tokenSignatureOverwrite != null\n        && tokenSignatureOverwrite.trim().length() > 0) {\n      hcatToken.setService(new Text(tokenSignatureOverwrite.trim()\n          .toLowerCase()));\n\n      logger.info(HIVE_TOKEN_SIGNATURE_KEY + \":\"\n          + (tokenSignatureOverwrite == null ? \"\" : tokenSignatureOverwrite));\n    }\n\n    logger.info(\"Created hive metastore token.\");\n    logger.info(\"Token kind: \" + hcatToken.getKind());\n    logger.info(\"Token service: \" + hcatToken.getService());\n    return hcatToken;\n  }\n\n  /*\n   * Gets hadoop tokens for a user to run mapred/hive jobs on a secured cluster\n   */\n  @Override\n  public synchronized void prefetchToken(final File tokenFile,\n      final Props props, final Logger logger)\n      throws HadoopSecurityManagerException {\n\n    final String userToProxy = props.getString(USER_TO_PROXY);\n\n    logger.info(\"Getting hadoop tokens based on props for \" + userToProxy);\n\n    final Credentials cred = new Credentials();\n\n    if (props.getBoolean(OBTAIN_HCAT_TOKEN, false)) {\n      try {\n\n        // first we fetch and save the default hcat token.\n        logger.info(\"Pre-fetching default Hive MetaStore token from hive\");\n\n        HiveConf hiveConf = new HiveConf();\n        Token<DelegationTokenIdentifier> hcatToken =\n            fetchHcatToken(userToProxy, hiveConf, null, logger);\n\n        cred.addToken(hcatToken.getService(), hcatToken);\n\n        // check and see if user specified the extra hcat locations we need to\n        // look at and fetch token.\n        final List<String> extraHcatLocations =\n            props.getStringList(EXTRA_HCAT_LOCATION);\n        if (Collections.EMPTY_LIST != extraHcatLocations) {\n          logger.info(\"Need to pre-fetch extra metaStore tokens from hive.\");\n\n          // start to process the user inputs.\n          for (final String thriftUrl : extraHcatLocations) {\n            logger.info(\"Pre-fetching metaStore token from : \" + thriftUrl);\n\n            hiveConf = new HiveConf();\n            hiveConf.set(HiveConf.ConfVars.METASTOREURIS.varname, thriftUrl);\n            hcatToken =\n                fetchHcatToken(userToProxy, hiveConf, thriftUrl, logger);\n            cred.addToken(hcatToken.getService(), hcatToken);\n          }\n\n        }\n\n      } catch (final Throwable t) {\n        final String message =\n            \"Failed to get hive metastore token.\" + t.getMessage()\n                + t.getCause();\n        logger.error(message, t);\n        throw new HadoopSecurityManagerException(message);\n      }\n    }\n\n    if (props.getBoolean(OBTAIN_JOBHISTORYSERVER_TOKEN, false)) {\n      final YarnRPC rpc = YarnRPC.create(this.conf);\n      final String serviceAddr = this.conf.get(JHAdminConfig.MR_HISTORY_ADDRESS);\n\n      logger.debug(\"Connecting to HistoryServer at: \" + serviceAddr);\n      final HSClientProtocol hsProxy =\n          (HSClientProtocol) rpc.getProxy(HSClientProtocol.class,\n              NetUtils.createSocketAddr(serviceAddr), this.conf);\n      logger.info(\"Pre-fetching JH token from job history server\");\n\n      Token<?> jhsdt = null;\n      try {\n        jhsdt = getDelegationTokenFromHS(hsProxy);\n      } catch (final Exception e) {\n        logger.error(\"Failed to fetch JH token\", e);\n        throw new HadoopSecurityManagerException(\n            \"Failed to fetch JH token for \" + userToProxy);\n      }\n\n      if (jhsdt == null) {\n        logger.error(\"getDelegationTokenFromHS() returned null\");\n        throw new HadoopSecurityManagerException(\n            \"Unable to fetch JH token for \" + userToProxy);\n      }\n\n      logger.info(\"Created JH token.\");\n      logger.info(\"Token kind: \" + jhsdt.getKind());\n      logger.info(\"Token service: \" + jhsdt.getService());\n\n      cred.addToken(jhsdt.getService(), jhsdt);\n    }\n\n    try {\n      getProxiedUser(userToProxy).doAs(new PrivilegedExceptionAction<Void>() {\n        @Override\n        public Void run() throws Exception {\n          getToken(userToProxy);\n          return null;\n        }\n\n        private void getToken(final String userToProxy) throws InterruptedException,\n            IOException, HadoopSecurityManagerException {\n          logger.info(\"Here is the props for \" + OBTAIN_NAMENODE_TOKEN + \": \"\n              + props.getBoolean(OBTAIN_NAMENODE_TOKEN));\n          if (props.getBoolean(OBTAIN_NAMENODE_TOKEN, false)) {\n            final FileSystem fs = FileSystem.get(HadoopSecurityManager_H_2_0.this.conf);\n            // check if we get the correct FS, and most importantly, the\n            // conf\n            logger.info(\"Getting DFS token from \" + fs.getUri());\n            final Token<?> fsToken =\n                fs.getDelegationToken(getMRTokenRenewerInternal(new JobConf())\n                    .toString());\n            if (fsToken == null) {\n              logger.error(\"Failed to fetch DFS token for \");\n              throw new HadoopSecurityManagerException(\n                  \"Failed to fetch DFS token for \" + userToProxy);\n            }\n            logger.info(\"Created DFS token.\");\n            logger.info(\"Token kind: \" + fsToken.getKind());\n            logger.info(\"Token service: \" + fsToken.getService());\n\n            cred.addToken(fsToken.getService(), fsToken);\n\n            // getting additional name nodes tokens\n            final String otherNamenodes = props.get(OTHER_NAMENODES_TO_GET_TOKEN);\n            if ((otherNamenodes != null) && (otherNamenodes.length() > 0)) {\n              logger.info(OTHER_NAMENODES_TO_GET_TOKEN + \": '\" + otherNamenodes\n                  + \"'\");\n              final String[] nameNodeArr = otherNamenodes.split(\",\");\n              final Path[] ps = new Path[nameNodeArr.length];\n              for (int i = 0; i < ps.length; i++) {\n                ps[i] = new Path(nameNodeArr[i].trim());\n              }\n              TokenCache.obtainTokensForNamenodes(cred, ps, HadoopSecurityManager_H_2_0.this.conf);\n              logger.info(\"Successfully fetched tokens for: \" + otherNamenodes);\n            } else {\n              logger.info(OTHER_NAMENODES_TO_GET_TOKEN + \" was not configured\");\n            }\n          }\n\n          if (props.getBoolean(OBTAIN_JOBTRACKER_TOKEN, false)) {\n            final JobConf jobConf = new JobConf();\n            final JobClient jobClient = new JobClient(jobConf);\n            logger.info(\"Pre-fetching JT token from JobTracker\");\n\n            final Token<DelegationTokenIdentifier> mrdt =\n                jobClient\n                    .getDelegationToken(getMRTokenRenewerInternal(jobConf));\n            if (mrdt == null) {\n              logger.error(\"Failed to fetch JT token\");\n              throw new HadoopSecurityManagerException(\n                  \"Failed to fetch JT token for \" + userToProxy);\n            }\n            logger.info(\"Created JT token.\");\n            logger.info(\"Token kind: \" + mrdt.getKind());\n            logger.info(\"Token service: \" + mrdt.getService());\n            cred.addToken(mrdt.getService(), mrdt);\n          }\n\n        }\n      });\n\n      prepareTokenFile(userToProxy, cred, tokenFile, logger);\n      // stash them to cancel after use.\n\n      logger.info(\"Tokens loaded in \" + tokenFile.getAbsolutePath());\n\n    } catch (final Exception e) {\n      throw new HadoopSecurityManagerException(\"Failed to get hadoop tokens! \"\n          + e.getMessage() + e.getCause(), e);\n    } catch (final Throwable t) {\n      throw new HadoopSecurityManagerException(\"Failed to get hadoop tokens! \"\n          + t.getMessage() + t.getCause(), t);\n    }\n\n  }\n\n  /**\n   * Prepare token file.\n   * Writes credentials to a token file and sets appropriate permissions to keep the file secure\n   *\n   * @param user user to be proxied\n   * @param credentials Credentials to be written to file\n   * @param tokenFile file to be written\n   * @param logger logger to use\n   * @throws IOException If there are issues in reading / updating the token file\n   */\n  private void prepareTokenFile(final String user,\n      final Credentials credentials,\n      final File tokenFile,\n      final Logger logger) throws IOException {\n    writeCredentialsToFile(credentials, tokenFile, logger);\n    try {\n      assignPermissions(user, tokenFile, logger);\n    } catch (final IOException e) {\n      // On any error managing token file. delete the file\n      tokenFile.delete();\n      throw e;\n    }\n  }\n\n  private void writeCredentialsToFile(final Credentials credentials, final File tokenFile,\n      final Logger logger)\n      throws IOException {\n    FileOutputStream fos = null;\n    DataOutputStream dos = null;\n    try {\n      fos = new FileOutputStream(tokenFile);\n      dos = new DataOutputStream(fos);\n      credentials.writeTokenStorageToStream(dos);\n    } finally {\n      if (dos != null) {\n        try {\n          dos.close();\n        } catch (final Throwable t) {\n          // best effort\n          logger.error(\"encountered exception while closing DataOutputStream of the tokenFile\", t);\n        }\n      }\n      if (fos != null) {\n        fos.close();\n      }\n    }\n  }\n\n  /**\n   * Uses execute-as-user binary to reassign file permissions to be readable only by that user.\n   *\n   * Step 1. Set file permissions to 460. Readable to self and readable / writable azkaban group\n   * Step 2. Set user as owner of file.\n   *\n   * @param user user to be proxied\n   * @param tokenFile file to be written\n   * @param logger logger to use\n   */\n  private void assignPermissions(final String user, final File tokenFile, final Logger logger)\n      throws IOException {\n    final List<String> changePermissionsCommand = Arrays.asList(\n        CHMOD, TOKEN_FILE_PERMISSIONS, tokenFile.getAbsolutePath()\n    );\n    int result = this.executeAsUser\n        .execute(System.getProperty(\"user.name\"), changePermissionsCommand);\n    if (result != 0) {\n      throw new IOException(\"Unable to modify permissions. User: \" + user);\n    }\n\n    final List<String> changeOwnershipCommand = Arrays.asList(\n        CHOWN, user + \":\" + GROUP_NAME, tokenFile.getAbsolutePath()\n    );\n    result = this.executeAsUser.execute(\"root\", changeOwnershipCommand);\n    if (result != 0) {\n      throw new IOException(\"Unable to set ownership. User: \" + user);\n    }\n  }\n\n  private Text getMRTokenRenewerInternal(final JobConf jobConf) throws IOException {\n    // Taken from Oozie\n    //\n    // Getting renewer correctly for JT principal also though JT in hadoop\n    // 1.x does not have\n    // support for renewing/cancelling tokens\n    final String servicePrincipal =\n        jobConf.get(RM_PRINCIPAL, jobConf.get(JT_PRINCIPAL));\n    final Text renewer;\n    if (servicePrincipal != null) {\n      String target =\n          jobConf.get(HADOOP_YARN_RM, jobConf.get(HADOOP_JOB_TRACKER_2));\n      if (target == null) {\n        target = jobConf.get(HADOOP_JOB_TRACKER);\n      }\n\n      final String addr = NetUtils.createSocketAddr(target).getHostName();\n      renewer =\n          new Text(SecurityUtil.getServerPrincipal(servicePrincipal, addr));\n    } else {\n      // No security\n      renewer = DEFAULT_RENEWER;\n    }\n\n    return renewer;\n  }\n\n  private Token<?> getDelegationTokenFromHS(final HSClientProtocol hsProxy)\n      throws IOException, InterruptedException {\n    final GetDelegationTokenRequest request =\n        this.recordFactory.newRecordInstance(GetDelegationTokenRequest.class);\n    request.setRenewer(Master.getMasterPrincipal(this.conf));\n    final org.apache.hadoop.yarn.api.records.Token mrDelegationToken;\n    mrDelegationToken =\n        hsProxy.getDelegationToken(request).getDelegationToken();\n    return ConverterUtils.convertFromYarn(mrDelegationToken,\n        hsProxy.getConnectAddress());\n  }\n\n  private void cancelDelegationTokenFromHS(\n      final org.apache.hadoop.yarn.api.records.Token t, final HSClientProtocol hsProxy)\n      throws IOException, InterruptedException {\n    final CancelDelegationTokenRequest request =\n        this.recordFactory.newRecordInstance(CancelDelegationTokenRequest.class);\n    request.setDelegationToken(t);\n    hsProxy.cancelDelegationToken(request);\n  }\n\n}\n", "idx": 2, "id": 13752, "msg": "", "proj": "azkaban-azkaban", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -13,21 +13,21 @@\n # limitations under the License.\n \n \"\"\"External project access scanner.\"\"\"\n-# pylint: disable=line-too-long\n import time\n \n from google.auth.exceptions import RefreshError\n from google.cloud.forseti.common.util import logger\n from google.cloud.forseti.common.gcp_api import api_helpers # noqa=E501\n from google.cloud.forseti.common.gcp_type import resource_util # noqa=E501\n-from google.cloud.forseti.common.gcp_api.cloud_resource_manager import CloudResourceManagerClient # noqa=E501\n+from google.cloud.forseti.common.gcp_api.cloud_resource_manager \\\n+    import CloudResourceManagerClient # noqa=E501\n from google.cloud.forseti.services.inventory.storage import DataAccess\n from google.cloud.forseti.services.inventory.storage import Storage\n-from google.cloud.forseti.scanner.audit import external_project_access_rules_engine as epa_rules_engine # noqa=E501\n+from google.cloud.forseti.scanner.audit \\\n+    import external_project_access_rules_engine \\\n+    as epa_rules_engine # noqa=E501\n from google.cloud.forseti.scanner.scanners import base_scanner\n \n-# pylint: enable=line-too-long\n-\n LOGGER = logger.get_logger(__name__)\n \n SCOPES = ['https://www.googleapis.com/auth/cloudplatformprojects.readonly']", "y": 1, "oldf": "# Copyright 2017 The Forseti Security Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"External project access scanner.\"\"\"\n# pylint: disable=line-too-long\nimport time\n\nfrom google.auth.exceptions import RefreshError\nfrom google.cloud.forseti.common.util import logger\nfrom google.cloud.forseti.common.gcp_api import api_helpers # noqa=E501\nfrom google.cloud.forseti.common.gcp_type import resource_util # noqa=E501\nfrom google.cloud.forseti.common.gcp_api.cloud_resource_manager import CloudResourceManagerClient # noqa=E501\nfrom google.cloud.forseti.services.inventory.storage import DataAccess\nfrom google.cloud.forseti.services.inventory.storage import Storage\nfrom google.cloud.forseti.scanner.audit import external_project_access_rules_engine as epa_rules_engine # noqa=E501\nfrom google.cloud.forseti.scanner.scanners import base_scanner\n\n# pylint: enable=line-too-long\n\nLOGGER = logger.get_logger(__name__)\n\nSCOPES = ['https://www.googleapis.com/auth/cloudplatformprojects.readonly']\n\n\ndef _get_inventory_storage(session, inventory_index_id):\n    \"\"\"Creates an open inventory.\n\n    Args:\n        session (object): db session.\n        inventory_index_id (int): The inventory index\n    Returns:\n        Storage: storage object\n    \"\"\"\n    inventory_storage = Storage(session, inventory_index_id, True)\n    inventory_storage.open()\n    return inventory_storage\n\n\ndef get_user_emails(service_config, member_types=None):\n    \"\"\"Retrieves the list of user email addresses from inventory.\n\n    Args:\n        service_config (dict): The service configuration\n        member_types (list): Member types to query in storage. This\n            defaults to 'gsuite_user'.\n\n    Returns:\n        list: List of list of user e-mail addresses.\n    \"\"\"\n    if not member_types:\n        member_types = ['gsuite_user']\n\n    emails = []\n    with service_config.scoped_session() as session:\n        inventory_index_id = (\n            DataAccess.get_latest_inventory_index_id(session))\n        inventory_storage = _get_inventory_storage(session,\n                                                   inventory_index_id)\n        for inventory_row in inventory_storage.iter(type_list=member_types):\n            emails.append(inventory_row.get_resource_data()['primaryEmail'])\n\n    return emails\n\n\ndef extract_project_ids(crm_client):\n    \"\"\"Extract a list of project ID's\n\n    Args:\n        crm_client (CloudResourceManagerClient):\n            An authenticated CRM client\n\n    Returns:\n        list: Project ID's as strings\n    \"\"\"\n\n    project_response = crm_client.get_projects()\n\n    projects = api_helpers.flatten_list_results(project_response, 'projects')\n    return [project['projectId'] for project in projects]\n\n\ndef memoize_ancestry(ancestry_function):\n    # pylint: disable=C0301\n    \"\"\"A decorator function to intelligently retrieve project ancestries, only if necessary.\n\n    Args:\n        ancestry_function (function): The ancestry\n            retrieval function.\n\n    Returns:\n        function: The helper\n    \"\"\"\n    discovered_ancestries = {}\n    # pylint: disable=W9011,W9012,C0111\n\n    def helper(crm_client, project_id):\n        if project_id not in discovered_ancestries:\n            discovered_ancestries[project_id] = (\n                ancestry_function(crm_client, project_id))\n        return discovered_ancestries[project_id]\n    return helper\n\n\n@memoize_ancestry\ndef get_project_ancestry(crm_client, project_id):\n    \"\"\"Get project ancestry as a list of type Resource.\n\n    Args:\n        crm_client (CloudResourceManagerClient):\n            crm client\n        project_id (str): A project ID\n\n    Returns:\n        list: Resource objects defining the ancestry\n            chain from the Project to the Organization\n    \"\"\"\n\n    ancestries = crm_client.get_project_ancestry(project_id)\n    ancestry_resources = (\n        resource_util.cast_to_gcp_resources(ancestries))\n\n    return ancestry_resources\n\n\ndef get_project_ancestries(crm_client, project_id_list):\n    \"\"\"Get the ancestries from a list of project ID's\n\n    Args:\n        crm_client (CloudResourceManagerClient):\n            crm client\n        project_id_list (list): A list of project ID's\n            as strings\n\n    Returns:\n        list: A list of lists ofResource objects\n            defining the ancestrychain from the Project\n            to the Organization\n    \"\"\"\n    ancestry_list = []\n    for project_id in project_id_list:\n        ancestry_list.append(get_project_ancestry(crm_client,\n                                                  project_id))\n    return ancestry_list\n\n\nclass ExternalProjectAccessScanner(base_scanner.BaseScanner):\n    \"\"\"Scanner for external project access.\"\"\"\n\n    def __init__(self, global_configs, scanner_configs, service_config,\n                 model_name, snapshot_timestamp, rules):\n        \"\"\"Initialization.\n\n        Args:\n            global_configs (dict): Global configurations.\n            scanner_configs (dict): Scanner configurations.\n            service_config (ServiceConfig): Forseti 2.0 service configs\n            model_name (str): name of the data model\n            snapshot_timestamp (str): Timestamp, formatted as YYYYMMDDTHHMMSSZ.\n            rules (str): Fully-qualified path and filename of the rules file.\n        \"\"\"\n        super(ExternalProjectAccessScanner, self).__init__(\n            global_configs,\n            scanner_configs,\n            service_config,\n            model_name,\n            snapshot_timestamp,\n            rules)\n\n        self.inventory_configs = self.service_config.get_inventory_config()\n        self.rules_engine = (\n            epa_rules_engine.ExternalProjectAccessRulesEngine(\n                rules_file_path=self.rules,\n                snapshot_timestamp=self.snapshot_timestamp))\n\n        self.rules_engine.build_rule_book(self.inventory_configs)\n\n        self._ancestries = dict()\n\n    def _output_results(self, all_violations):\n        \"\"\"Output results.\n\n        Args:\n            all_violations (list): A list of violations.\n        \"\"\"\n        all_violations = self._flatten_violations(all_violations)\n\n        self._output_results_to_db(all_violations)\n\n    def _find_violations(self, ancestries_by_user):\n        \"\"\"Find violations in the policies.\n\n        Args:\n            ancestries_by_user (dict): The project ancestries collected\n                                               from the scanner\n        Returns:\n            list: A list of ExternalProjectAccess violations\n        \"\"\"\n        all_violations = []\n        LOGGER.info('Finding project access violations...')\n\n        for user_mail, project_ancestries in ancestries_by_user.iteritems():\n            for project_ancestry in project_ancestries:\n                violations = (\n                    self.rules_engine.find_policy_violations(user_mail,\n                                                             project_ancestry))\n                all_violations.extend(violations)\n\n        return all_violations\n\n    @staticmethod\n    def _flatten_violations(violations):\n        \"\"\"Flatten RuleViolations into a dict for each RuleViolation member.\n\n        Args:\n            violations (list): The RuleViolations to flatten.\n\n        Yields:\n            dict: Iterator of RuleViolations as a dict per member.\n        \"\"\"\n        for violation in violations:\n            rule_ancestors_names = []\n\n            for ancestor in violation.rule_ancestors:\n                rule_ancestors_names.append(ancestor.name)\n\n            violation_data = {\n                'full_name': violation.full_name,\n                'member': violation.member,\n                'rule_ancestors': rule_ancestors_names\n            }\n\n            yield {\n                'resource_id': violation.resource_id,\n                'resource_type': violation.resource_type,\n                'full_name': violation.full_name,\n                'rule_index': violation.rule_index,\n                'rule_name': violation.rule_name,\n                'violation_type': violation.violation_type,\n                'violation_data': violation_data,\n                'resource_data': violation.resource_data\n            }\n\n    def _get_crm_client(self, user_email):\n        \"\"\"Get a user scoped CloudResourceManagerClient.\n\n        Args:\n            user_email (str): The e-mail address of the\n                user.\n\n        Returns:\n            CloudResourceManagerClient: crm client\n        \"\"\"\n        user_scoped_credential = (\n            api_helpers.get_delegated_credential(user_email, SCOPES))\n\n        client = CloudResourceManagerClient(\n            global_configs=self.inventory_configs,\n            credentials=user_scoped_credential)\n\n        return client\n\n    def _retrieve(self):\n        # pylint: disable=line-too-long\n        \"\"\"Retrieve the project ancestries for all users.\n\n        Returns:\n            dict: User project relationship.\n            {\"user1@example.com\": [[Project(\"1234\"), Organization(\"1234567\")],\n                                  [Project(\"12345\"), Folder(\"ABCDEFG\"), Organization(\"1234567\")]],\n             \"user2@example.com\": [[Project(\"1234\"), Organization(\"1234567\")],\n                                  [Project(\"12345\"), Folder(\"ABCDEFG\"), Organization(\"1234567\")]]}\n        \"\"\"\n        # pylint: enable=line-too-long\n        # This dictionary is the result of the scan.  The key\n        # is the user ID.  The value is a list of lists of ancestries.\n        user_to_project_ancestries_map = {}\n        user_count = 0\n\n        start_time = time.time()\n\n        user_emails = get_user_emails(self.service_config)\n\n        for user_email in user_emails:\n            user_count += 1\n            try:\n                user_crm_client = self._get_crm_client(user_email)\n\n                project_ids = extract_project_ids(user_crm_client)\n                ancestries = get_project_ancestries(user_crm_client,\n                                                    project_ids)\n\n                user_to_project_ancestries_map[user_email] = ancestries\n            except RefreshError:\n                LOGGER.debug('Unable to access project ancestry %s.',\n                             user_email)\n        # TODO: Remove when instrumentation is implemented.\n        elapsed_time = time.time() - start_time\n\n        LOGGER.debug('It took %f seconds to query projects for %d users',\n                     elapsed_time,\n                     user_count)\n\n        return user_to_project_ancestries_map\n\n    def run(self):\n        \"\"\"Entry point to run the scanner.\"\"\"\n        user_to_project_ancestries_map = self._retrieve()\n        all_violations = self._find_violations(user_to_project_ancestries_map)\n        self._output_results(all_violations)\n", "idx": 1, "id": 32519, "msg": "We never use backslash for line continuation. So, please change this back, including the pylint disable/enable.", "proj": "forseti-security-forseti-security", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -56,10 +56,16 @@ func RunServer(opts *server.Options) *server.Server {\n \tgo s.Start()\n \n \t// Make sure we are running and can bind before returning.\n-\taddr := fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port)\n \tend := time.Now().Add(10 * time.Second)\n \tfor time.Now().Before(end) {\n-\t\tconn, err := net.Dial(\"tcp\", addr)\n+\t\taddr := s.Addr()\n+\t\tif addr == nil {\n+\t\t\ttime.Sleep(10 * time.Millisecond)\n+\t\t\t// Retry. We might take a little while to open a connection.\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tconn, err := net.Dial(\"tcp\", addr.String())\n \t\tif err != nil {\n \t\t\ttime.Sleep(50 * time.Millisecond)\n \t\t\t// Retry", "y": 1, "oldf": "// Copyright 2012-2014 Apcera Inc. All rights reserved.\n\npackage test\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os/exec\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/apcera/gnatsd/server\"\n)\n\nconst natsServerExe = \"../gnatsd\"\n\ntype natsServer struct {\n\targs []string\n\tcmd  *exec.Cmd\n}\n\n// So we can pass tests and benchmarks..\ntype tLogger interface {\n\tFatalf(format string, args ...interface{})\n\tErrorf(format string, args ...interface{})\n}\n\nvar DefaultTestOptions = server.Options{\n\tHost:   \"localhost\",\n\tPort:   4222,\n\tNoLog:  true,\n\tNoSigs: true,\n}\n\nfunc runDefaultServer() *server.Server {\n\treturn RunServer(&DefaultTestOptions)\n}\n\n// New Go Routine based server\nfunc RunServer(opts *server.Options) *server.Server {\n\tif opts == nil {\n\t\topts = &DefaultTestOptions\n\t}\n\ts := server.New(opts)\n\tif s == nil {\n\t\tpanic(\"No NATS Server object returned.\")\n\t}\n\n\t// Run server in Go routine.\n\tgo s.Start()\n\n\t// Make sure we are running and can bind before returning.\n\taddr := fmt.Sprintf(\"%s:%d\", opts.Host, opts.Port)\n\tend := time.Now().Add(10 * time.Second)\n\tfor time.Now().Before(end) {\n\t\tconn, err := net.Dial(\"tcp\", addr)\n\t\tif err != nil {\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\t// Retry\n\t\t\tcontinue\n\t\t}\n\t\tconn.Close()\n\t\treturn s\n\t}\n\tpanic(\"Unable to start NATS Server in Go Routine\")\n}\n\nfunc startServer(t tLogger, port int, other string) *natsServer {\n\tvar s natsServer\n\targs := fmt.Sprintf(\"-p %d %s\", port, other)\n\ts.args = strings.Split(args, \" \")\n\ts.cmd = exec.Command(natsServerExe, s.args...)\n\terr := s.cmd.Start()\n\tif err != nil {\n\t\ts.cmd = nil\n\t\tt.Errorf(\"Could not start <%s> [%s], is NATS installed and in path?\", natsServerExe, err)\n\t\treturn &s\n\t}\n\t// Give it time to start up\n\tstart := time.Now()\n\tfor {\n\t\taddr := fmt.Sprintf(\"localhost:%d\", port)\n\t\tc, err := net.Dial(\"tcp\", addr)\n\t\tif err != nil {\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\tif time.Since(start) > (5 * time.Second) {\n\t\t\t\tt.Fatalf(\"Timed out trying to connect to %s\", natsServerExe)\n\t\t\t\treturn nil\n\t\t\t}\n\t\t} else {\n\t\t\tc.Close()\n\t\t\tbreak\n\t\t}\n\t}\n\treturn &s\n}\n\nfunc (s *natsServer) stopServer() {\n\tif s.cmd != nil && s.cmd.Process != nil {\n\t\ts.cmd.Process.Kill()\n\t\ts.cmd.Process.Wait()\n\t}\n}\n\nfunc stackFatalf(t tLogger, f string, args ...interface{}) {\n\tlines := make([]string, 0, 32)\n\tmsg := fmt.Sprintf(f, args...)\n\tlines = append(lines, msg)\n\n\t// Ignore ourselves\n\t_, testFile, _, _ := runtime.Caller(0)\n\n\t// Generate the Stack of callers:\n\tfor i := 0; true; i++ {\n\t\t_, file, line, ok := runtime.Caller(i)\n\t\tif ok == false {\n\t\t\tbreak\n\t\t}\n\t\tif file == testFile {\n\t\t\tcontinue\n\t\t}\n\t\tmsg := fmt.Sprintf(\"%d - %s:%d\", i, file, line)\n\t\tlines = append(lines, msg)\n\t}\n\n\tt.Fatalf(\"%s\", strings.Join(lines, \"\\n\"))\n}\n\nfunc acceptRouteConn(t tLogger, host string, timeout time.Duration) net.Conn {\n\tl, e := net.Listen(\"tcp\", host)\n\tif e != nil {\n\t\tstackFatalf(t, \"Error listening for route connection on %v: %v\", host, e)\n\t}\n\tdefer l.Close()\n\n\ttl := l.(*net.TCPListener)\n\ttl.SetDeadline(time.Now().Add(timeout))\n\n\tconn, err := l.Accept()\n\tif err != nil {\n\t\tstackFatalf(t, \"Did not receive a route connection request: %v\", err)\n\t}\n\treturn conn\n}\n\nfunc createRouteConn(t tLogger, host string, port int) net.Conn {\n\treturn createClientConn(t, host, port)\n}\n\nfunc createClientConn(t tLogger, host string, port int) net.Conn {\n\taddr := fmt.Sprintf(\"%s:%d\", host, port)\n\tc, err := net.DialTimeout(\"tcp\", addr, 1*time.Second)\n\tif err != nil {\n\t\tstackFatalf(t, \"Could not connect to server: %v\\n\", err)\n\t}\n\treturn c\n}\n\nfunc doConnect(t tLogger, c net.Conn, verbose, pedantic, ssl bool) {\n\tbuf := expectResult(t, c, infoRe)\n\tjs := infoRe.FindAllSubmatch(buf, 1)[0][1]\n\tvar sinfo server.Info\n\terr := json.Unmarshal(js, &sinfo)\n\tif err != nil {\n\t\tstackFatalf(t, \"Could not unmarshal INFO json: %v\\n\", err)\n\t}\n\tcs := fmt.Sprintf(\"CONNECT {\\\"verbose\\\":%v,\\\"pedantic\\\":%v,\\\"ssl_required\\\":%v}\\r\\n\", verbose, pedantic, ssl)\n\tsendProto(t, c, cs)\n}\n\nfunc doDefaultConnect(t tLogger, c net.Conn) {\n\t// Basic Connect\n\tdoConnect(t, c, false, false, false)\n}\n\nfunc checkSocket(t tLogger, addr string, wait time.Duration) {\n\tend := time.Now().Add(wait)\n\tfor time.Now().Before(end) {\n\t\tconn, err := net.Dial(\"tcp\", addr)\n\t\tif err != nil {\n\t\t\ttime.Sleep(50 * time.Millisecond)\n\t\t\t// Retry\n\t\t\tcontinue\n\t\t}\n\t\t// We bound to the addr, so close and return success.\n\t\tconn.Close()\n\t\treturn\n\t}\n\t// We have failed to bind the socket in the time allowed.\n\tt.Fatalf(\"Failed to connect to the socket: %q\", addr)\n}\n\nconst CONNECT_F = \"CONNECT {\\\"verbose\\\":false,\\\"user\\\":\\\"%s\\\",\\\"pass\\\":\\\"%s\\\",\\\"name\\\":\\\"%s\\\"}\\r\\n\"\n\nfunc doRouteAuthConnect(t tLogger, c net.Conn, user, pass, id string) {\n\tcs := fmt.Sprintf(CONNECT_F, user, pass, id)\n\tsendProto(t, c, cs)\n}\n\nfunc setupRouteEx(t tLogger, c net.Conn, opts *server.Options, id string) (sendFun, expectFun) {\n\tuser := opts.ClusterUsername\n\tpass := opts.ClusterPassword\n\tdoRouteAuthConnect(t, c, user, pass, id)\n\tsend := sendCommand(t, c)\n\texpect := expectCommand(t, c)\n\treturn send, expect\n}\n\nfunc setupRoute(t tLogger, c net.Conn, opts *server.Options) (sendFun, expectFun) {\n\tu := make([]byte, 16)\n\tio.ReadFull(rand.Reader, u)\n\tid := fmt.Sprintf(\"ROUTER:%s\", hex.EncodeToString(u))\n\treturn setupRouteEx(t, c, opts, id)\n}\n\nfunc setupConn(t tLogger, c net.Conn) (sendFun, expectFun) {\n\tdoDefaultConnect(t, c)\n\tsend := sendCommand(t, c)\n\texpect := expectCommand(t, c)\n\treturn send, expect\n}\n\ntype sendFun func(string)\ntype expectFun func(*regexp.Regexp) []byte\n\n// Closure version for easier reading\nfunc sendCommand(t tLogger, c net.Conn) sendFun {\n\treturn func(op string) {\n\t\tsendProto(t, c, op)\n\t}\n}\n\n// Closure version for easier reading\nfunc expectCommand(t tLogger, c net.Conn) expectFun {\n\treturn func(re *regexp.Regexp) []byte {\n\t\treturn expectResult(t, c, re)\n\t}\n}\n\n// Send the protocol command to the server.\nfunc sendProto(t tLogger, c net.Conn, op string) {\n\tn, err := c.Write([]byte(op))\n\tif err != nil {\n\t\tstackFatalf(t, \"Error writing command to conn: %v\\n\", err)\n\t}\n\tif n != len(op) {\n\t\tstackFatalf(t, \"Partial write: %d vs %d\\n\", n, len(op))\n\t}\n}\n\nvar (\n\tinfoRe       = regexp.MustCompile(`INFO\\s+([^\\r\\n]+)\\r\\n`)\n\tpingRe       = regexp.MustCompile(`PING\\r\\n`)\n\tpongRe       = regexp.MustCompile(`PONG\\r\\n`)\n\tmsgRe        = regexp.MustCompile(`(?:(?:MSG\\s+([^\\s]+)\\s+([^\\s]+)\\s+(([^\\s]+)[^\\S\\r\\n]+)?(\\d+)\\s*\\r\\n([^\\\\r\\\\n]*?)\\r\\n)+?)`)\n\tokRe         = regexp.MustCompile(`\\A\\+OK\\r\\n`)\n\terrRe        = regexp.MustCompile(`\\A\\-ERR\\s+([^\\r\\n]+)\\r\\n`)\n\tsubRe        = regexp.MustCompile(`SUB\\s+([^\\s]+)((\\s+)([^\\s]+))?\\s+([^\\s]+)\\r\\n`)\n\tunsubRe      = regexp.MustCompile(`UNSUB\\s+([^\\s]+)(\\s+(\\d+))?\\r\\n`)\n\tunsubmaxRe   = regexp.MustCompile(`UNSUB\\s+([^\\s]+)(\\s+(\\d+))\\r\\n`)\n\tunsubnomaxRe = regexp.MustCompile(`UNSUB\\s+([^\\s]+)\\r\\n`)\n\n\tconnectRe = regexp.MustCompile(`CONNECT\\s+([^\\r\\n]+)\\r\\n`)\n)\n\nconst (\n\tSUB_INDEX   = 1\n\tSID_INDEX   = 2\n\tREPLY_INDEX = 4\n\tLEN_INDEX   = 5\n\tMSG_INDEX   = 6\n)\n\n// Reuse expect buffer\n// TODO(dlc) - This may be too simplistic in the long run, may need\n// to consider holding onto data from previous reads matched by conn.\nvar expBuf = make([]byte, 32768)\n\n// Test result from server against regexp\nfunc expectResult(t tLogger, c net.Conn, re *regexp.Regexp) []byte {\n\t// Wait for commands to be processed and results queued for read\n\tc.SetReadDeadline(time.Now().Add(1 * time.Second))\n\tdefer c.SetReadDeadline(time.Time{})\n\n\tn, err := c.Read(expBuf)\n\tif n <= 0 && err != nil {\n\t\tstackFatalf(t, \"Error reading from conn: %v\\n\", err)\n\t}\n\tbuf := expBuf[:n]\n\n\tif !re.Match(buf) {\n\t\tstackFatalf(t, \"Response did not match expected: \\n\\tReceived:'%q'\\n\\tExpected:'%s'\\n\", buf, re)\n\t}\n\treturn buf\n}\n\nfunc expectNothing(t tLogger, c net.Conn) {\n\tc.SetReadDeadline(time.Now().Add(100 * time.Millisecond))\n\tdefer c.SetReadDeadline(time.Time{})\n\n\tn, err := c.Read(expBuf)\n\tif err == nil && n > 0 {\n\t\tstackFatalf(t, \"Expected nothing, received: '%q'\\n\", expBuf[:n])\n\t}\n}\n\n// This will check that we got what we expected.\nfunc checkMsg(t tLogger, m [][]byte, subject, sid, reply, len, msg string) {\n\tif string(m[SUB_INDEX]) != subject {\n\t\tstackFatalf(t, \"Did not get correct subject: expected '%s' got '%s'\\n\", subject, m[SUB_INDEX])\n\t}\n\tif sid != \"\" && string(m[SID_INDEX]) != sid {\n\t\tstackFatalf(t, \"Did not get correct sid: expected '%s' got '%s'\\n\", sid, m[SID_INDEX])\n\t}\n\tif string(m[REPLY_INDEX]) != reply {\n\t\tstackFatalf(t, \"Did not get correct reply: expected '%s' got '%s'\\n\", reply, m[REPLY_INDEX])\n\t}\n\tif string(m[LEN_INDEX]) != len {\n\t\tstackFatalf(t, \"Did not get correct msg length: expected '%s' got '%s'\\n\", len, m[LEN_INDEX])\n\t}\n\tif string(m[MSG_INDEX]) != msg {\n\t\tstackFatalf(t, \"Did not get correct msg: expected '%s' got '%s'\\n\", msg, m[MSG_INDEX])\n\t}\n}\n\n// Closure for expectMsgs\nfunc expectMsgsCommand(t tLogger, ef expectFun) func(int) [][][]byte {\n\treturn func(expected int) [][][]byte {\n\t\tbuf := ef(msgRe)\n\t\tmatches := msgRe.FindAllSubmatch(buf, -1)\n\t\tif len(matches) != expected {\n\t\t\tstackFatalf(t, \"Did not get correct # msgs: %d vs %d\\n\", len(matches), expected)\n\t\t}\n\t\treturn matches\n\t}\n}\n\n// This will check that the matches include at least one of the sids. Useful for checking\n// that we received messages on a certain queue group.\nfunc checkForQueueSid(t tLogger, matches [][][]byte, sids []string) {\n\tseen := make(map[string]int, len(sids))\n\tfor _, sid := range sids {\n\t\tseen[sid] = 0\n\t}\n\tfor _, m := range matches {\n\t\tsid := string(m[SID_INDEX])\n\t\tif _, ok := seen[sid]; ok {\n\t\t\tseen[sid] += 1\n\t\t}\n\t}\n\t// Make sure we only see one and exactly one.\n\ttotal := 0\n\tfor _, n := range seen {\n\t\ttotal += n\n\t}\n\tif total != 1 {\n\t\tstackFatalf(t, \"Did not get a msg for queue sids group: expected 1 got %d\\n\", total)\n\t}\n}\n\n// This will check that the matches include all of the sids. Useful for checking\n// that we received messages on all subscribers.\nfunc checkForPubSids(t tLogger, matches [][][]byte, sids []string) {\n\tseen := make(map[string]int, len(sids))\n\tfor _, sid := range sids {\n\t\tseen[sid] = 0\n\t}\n\tfor _, m := range matches {\n\t\tsid := string(m[SID_INDEX])\n\t\tif _, ok := seen[sid]; ok {\n\t\t\tseen[sid] += 1\n\t\t}\n\t}\n\t// Make sure we only see one and exactly one for each sid.\n\tfor sid, n := range seen {\n\t\tif n != 1 {\n\t\t\tstackFatalf(t, \"Did not get a msg for sid[%s]: expected 1 got %d\\n\", sid, n)\n\n\t\t}\n\t}\n}\n", "idx": 1, "id": 5980, "msg": "Why would it be nil?", "proj": "nats-io-nats-server", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -347,7 +347,7 @@ class SpeechManager(object):\n \t\t\t\tif not command.enter and command.trigger not in enteredTriggers:\n \t\t\t\t\tlog.debugWarning(\"Request to exit trigger which wasn't entered: %r\" % command.trigger.spec)\n \t\t\t\t\tcontinue\n-\t\t\t\tensureEndUtterance(outSeq)\n+\t\t\t\tself._ensureEndUtterance(outSeq, outSeqs, paramsToReplay, paramTracker)\n \t\t\t\toutSeqs.append([command])\n \t\t\t\tif command.enter:\n \t\t\t\t\tenteredTriggers.append(command.trigger)", "y": 0, "oldf": "# -*- coding: UTF-8 -*-\n# A part of NonVisual Desktop Access (NVDA)\n# This file is covered by the GNU General Public License.\n# See the file COPYING for more details.\n# Copyright (C) 2006-2020 NV Access Limited\nimport typing\n\nimport queueHandler\nimport synthDriverHandler\nimport config\nfrom .types import SpeechSequence, _IndexT\nfrom .commands import (\n\t# Commands that are used in this file.\n\tEndUtteranceCommand,\n\tSynthParamCommand,\n\tBaseCallbackCommand,\n\tConfigProfileTriggerCommand,\n\tIndexCommand,\n\t_CancellableSpeechCommand,\n)\n\nfrom .priorities import Spri, SPEECH_PRIORITIES\nfrom logHandler import log\nfrom synthDriverHandler import getSynth\nfrom typing import (\n\tDict,\n\tAny,\n\tList,\n\tTuple,\n\tCallable,\n\tOptional,\n\tcast,\n)\n\n\ndef _shouldCancelExpiredFocusEvents():\n\t# 0: default (yes), 1: yes, 2: no\n\treturn config.conf[\"featureFlag\"][\"cancelExpiredFocusSpeech\"] != 2\n\n\ndef _shouldDoSpeechManagerLogging():\n\treturn config.conf[\"debugLog\"][\"speechManager\"]\n\n\ndef _speechManagerDebug(msg, *args, **kwargs) -> None:\n\t\"\"\"Log 'msg % args' with severity 'DEBUG' if speech manager logging is enabled.\n\t\t'SpeechManager-' is prefixed to all messages to make searching the log easier.\n\t\"\"\"\n\tif not log.isEnabledFor(log.DEBUG) or not _shouldDoSpeechManagerLogging():\n\t\treturn\n\tlog._log(log.DEBUG, f\"SpeechManager- \" + msg, args, **kwargs)\n\n\n#: Turns on unit test logging, logs the key interactions that happen with speech manager. When False,\n# log messages are sent to _speechManagerDebug\nIS_UNIT_TEST_LOG_ENABLED = False\n\n\ndef _speechManagerUnitTest(msg, *args, **kwargs) -> None:\n\t\"\"\"Log 'msg % args' with severity 'DEBUG' if .\n\t\t'SpeechManUnitTest-' is prefixed to all messages to make searching the log easier.\n\t\tWhen\n\t\"\"\"\n\tif not IS_UNIT_TEST_LOG_ENABLED:\n\t\t# Don't reuse _speechManagerDebug, it leads to incorrect function names in the log (all\n\t\t# SpeechManager debug logging appears to come from _speechManagerUnitTest instead of the frame\n\t\t# one stack higher. The codepath argument for _log could also be used to resolve this, but duplication\n\t\t# simpler.\n\t\tif log.isEnabledFor(log.DEBUG) and _shouldDoSpeechManagerLogging():\n\t\t\tlog._log(log.DEBUG, f\"SpeechManager- \" + msg, args, **kwargs)\n\t\treturn\n\tlog._log(log.INFO, f\"SpeechManUnitTest- \" + msg, args, **kwargs)\n\n# Install the custom log handlers.\n#: For extra debug level logging, this is a category that must be enabled in the advanced settings panel.\nlog._speechManagerDebug = _speechManagerDebug\n\n#: Info level logging (only enabled if IS_UNIT_TEST_LOG_ENABLED hardcoded to True). This is a developer\n# tool to ease the creation of unit tests for SpeechManager. It should log all external interactions with\n# SpeechManager so they can be recreated in tests.\nlog._speechManagerUnitTest = _speechManagerUnitTest\n\n\nclass ParamChangeTracker(object):\n\t\"\"\"Keeps track of commands which change parameters from their defaults.\n\tThis is useful when an utterance needs to be split.\n\tAs you are processing a sequence,\n\tyou update the tracker with a parameter change using the L{update} method.\n\tWhen you split the utterance, you use the L{getChanged} method to get\n\tthe parameters which have been changed from their defaults.\n\t\"\"\"\n\n\tdef __init__(self):\n\t\tself._commands = {}\n\n\tdef update(self, command):\n\t\t\"\"\"Update the tracker with a parameter change.\n\t\t@param command: The parameter change command.\n\t\t@type command: L{SynthParamCommand}\n\t\t\"\"\"\n\t\tparamType = type(command)\n\t\tif command.isDefault:\n\t\t\t# This no longer applies.\n\t\t\tself._commands.pop(paramType, None)\n\t\telse:\n\t\t\tself._commands[paramType] = command\n\n\tdef getChanged(self):\n\t\t\"\"\"Get the commands for the parameters which have been changed from their defaults.\n\t\t@return: List of parameter change commands.\n\t\t@type: list of L{SynthParamCommand}\n\t\t\"\"\"\n\t\treturn list(self._commands.values())\n\nclass _ManagerPriorityQueue(object):\n\t\"\"\"A speech queue for a specific priority.\n\tThis is intended for internal use by L{_SpeechManager} only.\n\tEach priority has a separate queue.\n\tIt holds the pending speech sequences to be spoken,\n\tas well as other information necessary to restore state when this queue\n\tis preempted by a higher priority queue.\n\t\"\"\"\n\n\tdef __init__(self, priority: Spri):\n\t\tself.priority = priority\n\t\t#: The pending speech sequences to be spoken.\n\t\t#: These are split at indexes,\n\t\t#: so a single utterance might be split over multiple sequences.\n\t\tself.pendingSequences: List[SpeechSequence] = []\n\t\t#: The configuration profile triggers that have been entered during speech.\n\t\tself.enteredProfileTriggers: List[config.ProfileTrigger] = []\n\t\t#: Keeps track of parameters that have been changed during an utterance.\n\t\tself.paramTracker: ParamChangeTracker = ParamChangeTracker()\n\n\nclass SpeechManager(object):\n\t\"\"\"Manages queuing of speech utterances, calling callbacks at desired points in the speech, profile switching, prioritization, etc.\n\tThis is intended for internal use only.\n\tIt is used by higher level functions such as L{speak}.\n\n\tThe high level flow of control is as follows:\n\t1. A speech sequence is queued with L{speak}, which in turn calls L{_queueSpeechSequence}.\n\t2. L{_processSpeechSequence} is called to normalize, process and split the input sequence.\n\t\tIt converts callbacks to indexes.\n\t\tAll indexing is assigned and managed by this class.\n\t\tIt maps any indexes to their corresponding callbacks.\n\t\tIt splits the sequence at indexes so we easily know what has completed speaking.\n\t\tIf there are end utterance commands, the sequence is split at that point.\n\t\tWe ensure there is an index at the end of all utterances so we know when they've finished speaking.\n\t\tWe ensure any config profile trigger commands are preceded by an utterance end.\n\t\tParameter changes are re-applied after utterance breaks.\n\t\tWe ensure any entered profile triggers are exited at the very end.\n\t3. L{_queueSpeechSequence} places these processed sequences in the queue\n\t\tfor the priority specified by the caller in step 1.\n\t\tThere is a separate queue for each priority.\n\t4. L{_pushNextSpeech} is called to begin pushing speech.\n\t\tIt looks for the highest priority queue with pending speech.\n\t\tBecause there's no other speech queued, that'll be the queue we just touched.\n\t5. If the input begins with a profile switch, it is applied immediately.\n\t6. L{_buildNextUtterance} is called to build a full utterance and it is sent to the synth.\n\t7. For every index reached, L{_handleIndex} is called.\n\t\tThe completed sequence is removed from L{_pendingSequences}.\n\t\tIf there is an associated callback, it is run.\n\t\tIf the index marks the end of an utterance, L{_pushNextSpeech} is called to push more speech.\n\t8. If there is another utterance before a profile switch, it is built and sent as per steps 6 and 7.\n\t9. In L{_pushNextSpeech}, if a profile switch is next, we wait for the synth to finish speaking before pushing more.\n\t\tThis is because we don't want to start speaking too early with a different synth.\n\t\tL{_handleDoneSpeaking} is called when the synth finishes speaking.\n\t\tIt pushes more speech, which includes applying the profile switch.\n\t10. The flow then repeats from step 6 onwards until there are no more pending sequences.\n\t11. If another sequence is queued via L{speak} during speech,\n\t\tit is processed and queued as per steps 2 and 3.\n\t12. If this is the first utterance at priority now, speech is interrupted\n\t\tand L{_pushNextSpeech} is called.\n\t\tOtherwise, L{_pushNextSpeech} is called when the current utterance completes\n\t\tas per step 7.\n\t13. When L{_pushNextSpeech} is next called, it looks for the highest priority queue with pending speech.\n\t\tIf that priority is different to the priority of the utterance just spoken,\n\t\tany relevant profile switches are applied to restore the state for this queue.\n\t14. If a lower priority utterance was interrupted in the middle,\n\t\tL{_buildNextUtterance} applies any parameter changes that applied before the interruption.\n\t15. The flow then repeats from step 6 onwards until there are no more pending sequences.\n\n\tNote:\n\tAll of this activity is (and must be) synchronized and serialized on the main thread.\n\t\"\"\"\n\n\t_cancelCommandsForUtteranceBeingSpokenBySynth: Dict[_CancellableSpeechCommand, _IndexT]\n\t_priQueues: Dict[Any, _ManagerPriorityQueue]\n\t_curPriQueue: Optional[_ManagerPriorityQueue]  # None indicates no more speech.\n\n\tdef __init__(self):\n\t\t#: A counter for indexes sent to the synthesizer for callbacks, etc.\n\t\tself._indexCounter = self._generateIndexes()\n\t\tself._reset()\n\t\tsynthDriverHandler.synthIndexReached.register(self._onSynthIndexReached)\n\t\tsynthDriverHandler.synthDoneSpeaking.register(self._onSynthDoneSpeaking)\n\n\t#: Maximum index number to pass to synthesizers.\n\tMAX_INDEX: _IndexT = 9999\n\n\tdef _generateIndexes(self) -> typing.Generator[_IndexT, None, None]:\n\t\t\"\"\"Generator of index numbers.\n\t\tWe don't want to reuse index numbers too quickly,\n\t\tas there can be race conditions when cancelling speech which might result\n\t\tin an index from a previous utterance being treated as belonging to the current utterance.\n\t\tHowever, we don't want the counter increasing indefinitely,\n\t\tas some synths might not be able to handle huge numbers.\n\t\tTherefore, we use a counter which starts at 1, counts up to L{MAX_INDEX},\n\t\twraps back to 1 and continues cycling thus.\n\t\tThis maximum is arbitrary, but\n\t\tit's small enough that any synth should be able to handle it\n\t\tand large enough that previous indexes won't reasonably get reused\n\t\tin the same or previous utterance.\n\t\t\"\"\"\n\t\twhile True:\n\t\t\tfor index in range(1, self.MAX_INDEX + 1):\n\t\t\t\tyield index\n\n\tdef _reset(self):\n\t\t#: The queues for each priority.\n\t\tself._priQueues = {}\n\t\t#: The priority queue for the utterance currently being spoken.\n\t\tself._curPriQueue = None\n\t\t#: Maps indexes to BaseCallbackCommands.\n\t\tself._indexesToCallbacks = {}\n\t\t#: a list of indexes currently being spoken by the synthesizer\n\t\tself._indexesSpeaking = []\n\t\t#: Whether to push more speech when the synth reports it is done speaking.\n\t\tself._shouldPushWhenDoneSpeaking = False\n\t\tself._cancelCommandsForUtteranceBeingSpokenBySynth = {}\n\t\t#: True if the synth.cancel was called due to cancellableSpeech no longer being valid\n\t\t# and no new speech has been sent.\n\t\tself._cancelledLastSpeechWithSynth = False\n\n\tdef _synthStillSpeaking(self) -> bool:\n\t\treturn 0 < len(self._indexesSpeaking)\n\n\tdef _hasNoMoreSpeech(self):\n\t\treturn self._curPriQueue is None\n\n\tdef speak(self, speechSequence: SpeechSequence, priority: Spri):\n\t\tlog._speechManagerUnitTest(\"speak (priority %r): %r\", priority, speechSequence)\n\t\tinterrupt = self._queueSpeechSequence(speechSequence, priority)\n\t\tself._doRemoveCancelledSpeechCommands()\n\t\t# If speech isn't already in progress, we need to push the first speech.\n\t\tpush = self._hasNoMoreSpeech() or not self._synthStillSpeaking()\n\t\tlog._speechManagerDebug(\n\t\t\tf\"Will interrupt: {interrupt}\"\n\t\t\tf\" Will push: {push}\"\n\t\t\tf\" | _indexesSpeaking: {self._indexesSpeaking!r}\"\n\t\t\tf\" | _curPriQueue valid: {not self._hasNoMoreSpeech()}\"\n\t\t\tf\" | _shouldPushWhenDoneSpeaking: {self._shouldPushWhenDoneSpeaking}\"\n\t\t\tf\" | _cancelledLastSpeechWithSynth {self._cancelledLastSpeechWithSynth}\"\n\t\t)\n\t\tif interrupt:\n\t\t\tlog._speechManagerDebug(\"Interrupting speech\")\n\t\t\tgetSynth().cancel()\n\t\t\tself._indexesSpeaking.clear()\n\t\t\tself._cancelCommandsForUtteranceBeingSpokenBySynth.clear()\n\t\t\tpush = True\n\t\tif push:\n\t\t\tlog._speechManagerDebug(\"Pushing next speech\")\n\t\t\tself._pushNextSpeech(True)\n\t\telse:\n\t\t\tlog._speechManagerDebug(\"Not pushing speech\")\n\n\tdef _queueSpeechSequence(self, inSeq: SpeechSequence, priority: Spri) -> bool:\n\t\t\"\"\"\n\t\t@return: Whether to interrupt speech.\n\t\t\"\"\"\n\t\toutSeq = self._processSpeechSequence(inSeq)\n\t\tlog._speechManagerDebug(\"Out Seq: %r\", outSeq)  # expensive string to build - defer\n\t\tqueue = self._priQueues.get(priority)\n\t\tlog._speechManagerDebug(\n\t\t\tf\"Current priority: {priority},\"\n\t\t\tf\" queLen: {0 if queue is None else len(queue.pendingSequences)}\"\n\t\t)\n\t\tif not queue:\n\t\t\tqueue = self._priQueues[priority] = _ManagerPriorityQueue(priority)\n\t\telse:\n\t\t\tlog._speechManagerDebug(\n\t\t\t\t\"current queue: %r\",  # expensive string to build - defer\n\t\t\t\tqueue.pendingSequences\n\t\t\t)\n\t\tfirst = len(queue.pendingSequences) == 0\n\t\tqueue.pendingSequences.extend(outSeq)\n\t\tif priority is Spri.NOW and first:\n\t\t\t# If this is the first sequence at Spri.NOW, interrupt speech.\n\t\t\treturn True\n\t\treturn False\n\n\tdef _processSpeechSequence(self, inSeq: SpeechSequence):\n\t\tparamTracker = ParamChangeTracker()\n\t\tenteredTriggers = []\n\t\toutSeqs = []\n\t\tparamsToReplay = []\n\n\t\tdef ensureEndUtterance(seq: SpeechSequence):\n\t\t\t# We split at EndUtteranceCommands so the ends of utterances are easily found.\n\t\t\t# This function ensures the given sequence ends with an EndUtterance command,\n\t\t\t# Ensures that the sequence also includes an index command at the end,\n\t\t\t# It places the complete sequence in outSeqs,\n\t\t\t# It clears the given sequence list ready to build a new one,\n\t\t\t# And clears the paramsToReplay list\n\t\t\t# and refills it with any params that need to be repeated if a new sequence is going to be built.\n\t\t\tif seq:\n\t\t\t\t# There have been commands since the last split.\n\t\t\t\tlastOutSeq = paramsToReplay + seq\n\t\t\t\toutSeqs.append(lastOutSeq)\n\t\t\t\tparamsToReplay.clear()\n\t\t\t\tseq.clear()\n\t\t\t\t# Re-apply parameters that have been changed from their defaults.\n\t\t\t\tparamsToReplay.extend(paramTracker.getChanged())\n\t\t\telse:\n\t\t\t\tlastOutSeq = outSeqs[-1] if outSeqs else None\n\t\t\tlastCommand = lastOutSeq[-1] if lastOutSeq else None\n\t\t\tif lastCommand is None or isinstance(lastCommand, (EndUtteranceCommand, ConfigProfileTriggerCommand)):\n\t\t\t\t# It doesn't make sense to start with or repeat EndUtteranceCommands.\n\t\t\t\t# We also don't want an EndUtteranceCommand immediately after a ConfigProfileTriggerCommand.\n\t\t\t\treturn\n\t\t\tif not isinstance(lastCommand, IndexCommand):\n\t\t\t\t# Add an index so we know when we've reached the end of this utterance.\n\t\t\t\treachedIndex = next(self._indexCounter)\n\t\t\t\tlastOutSeq.append(IndexCommand(reachedIndex))\n\t\t\toutSeqs.append([EndUtteranceCommand()])\n\n\t\toutSeq = []\n\t\tfor command in inSeq:\n\t\t\tif isinstance(command, BaseCallbackCommand):\n\t\t\t\t# When the synth reaches this point, we want to call the callback.\n\t\t\t\tspeechIndex = next(self._indexCounter)\n\t\t\t\toutSeq.append(IndexCommand(speechIndex))\n\t\t\t\tself._indexesToCallbacks[speechIndex] = command\n\t\t\t\t# We split at indexes so we easily know what has completed speaking.\n\t\t\t\toutSeqs.append(paramsToReplay + outSeq)\n\t\t\t\tparamsToReplay.clear()\n\t\t\t\toutSeq.clear()\n\t\t\t\tcontinue\n\t\t\tif isinstance(command, ConfigProfileTriggerCommand):\n\t\t\t\tif not command.trigger.hasProfile:\n\t\t\t\t\t# Ignore triggers that have no associated profile.\n\t\t\t\t\tcontinue\n\t\t\t\tif command.enter and command.trigger in enteredTriggers:\n\t\t\t\t\tlog.debugWarning(\"Request to enter trigger which has already been entered: %r\" % command.trigger.spec)\n\t\t\t\t\tcontinue\n\t\t\t\tif not command.enter and command.trigger not in enteredTriggers:\n\t\t\t\t\tlog.debugWarning(\"Request to exit trigger which wasn't entered: %r\" % command.trigger.spec)\n\t\t\t\t\tcontinue\n\t\t\t\tensureEndUtterance(outSeq)\n\t\t\t\toutSeqs.append([command])\n\t\t\t\tif command.enter:\n\t\t\t\t\tenteredTriggers.append(command.trigger)\n\t\t\t\telse:\n\t\t\t\t\tenteredTriggers.remove(command.trigger)\n\t\t\t\tcontinue\n\t\t\tif isinstance(command, EndUtteranceCommand):\n\t\t\t\tensureEndUtterance(outSeq)\n\t\t\t\tcontinue\n\t\t\tif isinstance(command, SynthParamCommand):\n\t\t\t\tparamTracker.update(command)\n\t\t\toutSeq.append(command)\n\t\t# Add the last sequence and make sure the sequence ends the utterance.\n\t\tensureEndUtterance(outSeq)\n\t\t# Exit any profile triggers the caller didn't exit.\n\t\tfor trigger in reversed(enteredTriggers):\n\t\t\tcommand = ConfigProfileTriggerCommand(trigger, False)\n\t\t\toutSeqs.append([command])\n\t\treturn outSeqs\n\n\tdef _pushNextSpeech(self, doneSpeaking: bool):\n\t\tlog._speechManagerDebug(f\"pushNextSpeech - doneSpeaking: {doneSpeaking}\")\n\t\tqueue = self._getNextPriority()\n\t\tif not queue:\n\t\t\t# No more speech.\n\t\t\tlog._speechManagerDebug(\"No more speech\")\n\t\t\tself._curPriQueue = None\n\t\t\treturn\n\t\tif self._hasNoMoreSpeech():\n\t\t\t# First utterance after no speech.\n\t\t\tself._curPriQueue = queue\n\t\telif queue.priority > self._curPriQueue.priority:\n\t\t\t# Preempted by higher priority speech.\n\t\t\tif self._curPriQueue.enteredProfileTriggers:\n\t\t\t\tif not doneSpeaking:\n\t\t\t\t\t# Wait for the synth to finish speaking.\n\t\t\t\t\t# _handleDoneSpeaking will call us again.\n\t\t\t\t\tself._shouldPushWhenDoneSpeaking = True\n\t\t\t\t\treturn\n\t\t\t\tself._exitProfileTriggers(self._curPriQueue.enteredProfileTriggers)\n\t\t\tself._curPriQueue = queue\n\t\telif queue.priority < self._curPriQueue.priority:\n\t\t\t# Resuming a preempted, lower priority queue.\n\t\t\tif queue.enteredProfileTriggers:\n\t\t\t\tif not doneSpeaking:\n\t\t\t\t\t# Wait for the synth to finish speaking.\n\t\t\t\t\t# _handleDoneSpeaking will call us again.\n\t\t\t\t\tself._shouldPushWhenDoneSpeaking = True\n\t\t\t\t\treturn\n\t\t\t\tself._restoreProfileTriggers(queue.enteredProfileTriggers)\n\t\t\tself._curPriQueue = queue\n\t\twhile queue.pendingSequences and isinstance(queue.pendingSequences[0][0], ConfigProfileTriggerCommand):\n\t\t\tif not doneSpeaking:\n\t\t\t\t# Wait for the synth to finish speaking.\n\t\t\t\t# _handleDoneSpeaking will call us again.\n\t\t\t\tself._shouldPushWhenDoneSpeaking = True\n\t\t\t\treturn\n\t\t\tself._switchProfile()\n\t\tif not queue.pendingSequences:\n\t\t\t# The last commands in this queue were profile switches.\n\t\t\t# Call this method again in case other queues are waiting.\n\t\t\treturn self._pushNextSpeech(True)\n\t\tseq = self._buildNextUtterance()\n\t\tif seq:\n\t\t\t# So that we can handle any accidentally skipped indexes.\n\t\t\tfor item in seq:\n\t\t\t\tif isinstance(item, IndexCommand):\n\t\t\t\t\tself._indexesSpeaking.append(item.index)\n\t\t\tself._cancelledLastSpeechWithSynth = False\n\t\t\tlog._speechManagerUnitTest(f\"Synth Gets: {seq}\")\n\t\t\tgetSynth().speak(seq)\n\n\tdef _getNextPriority(self):\n\t\t\"\"\"Get the highest priority queue containing pending speech.\n\t\t\"\"\"\n\t\tfor priority in SPEECH_PRIORITIES:\n\t\t\tqueue = self._priQueues.get(priority)\n\t\t\tif not queue:\n\t\t\t\tcontinue\n\t\t\tif queue.pendingSequences:\n\t\t\t\treturn queue\n\t\treturn None\n\n\tdef _buildNextUtterance(self):\n\t\t\"\"\"Since an utterance might be split over several sequences,\n\t\tbuild a complete utterance to pass to the synth.\n\t\t\"\"\"\n\t\tutterance = []\n\t\t# If this utterance was preempted by higher priority speech,\n\t\t# apply any parameters changed before the preemption.\n\t\tparams = self._curPriQueue.paramTracker.getChanged()\n\t\tutterance.extend(params)\n\t\tlastSequenceIndexAddedToUtterance = None\n\t\tfor seqIndex, seq in enumerate(self._curPriQueue.pendingSequences):\n\t\t\tif isinstance(seq[0], EndUtteranceCommand):\n\t\t\t\t# The utterance ends here.\n\t\t\t\tbreak\n\t\t\tutterance.extend(seq)\n\t\t\tlastSequenceIndexAddedToUtterance = seqIndex\n\t\t# if any items are cancelled, cancel the whole utterance.\n\t\ttry:\n\t\t\tutteranceValid = len(utterance) == 0 or self._checkForCancellations(utterance)\n\t\texcept IndexError:\n\t\t\tlog.error(\n\t\t\t\tf\"Checking for cancellations failed, cancelling sequence: {utterance}\",\n\t\t\t\texc_info=True\n\t\t\t)\n\t\t\t# Avoid infinite recursion by removing the problematic sequences:\n\t\t\tdel self._curPriQueue.pendingSequences[:lastSequenceIndexAddedToUtterance + 1]\n\t\t\tutteranceValid = False\n\n\t\tif utteranceValid:\n\t\t\treturn utterance\n\t\telse:\n\t\t\treturn self._buildNextUtterance()\n\n\tdef _checkForCancellations(self, utterance: SpeechSequence) -> bool:\n\t\t\"\"\"\n\t\tChecks utterance to ensure it is not cancelled (via a _CancellableSpeechCommand).\n\t\tBecause synthesizers do not expect CancellableSpeechCommands, they are removed from the utterance.\n\t\t:arg utterance: The utterance to check for cancellations. Modified in place, CancellableSpeechCommands are\n\t\tremoved.\n\t\t:return True if sequence is still valid, else False\n\t\t\"\"\"\n\t\tif not _shouldCancelExpiredFocusEvents():\n\t\t\treturn True\n\t\tutteranceIndex = self._getUtteranceIndex(utterance)\n\t\tif utteranceIndex is None:\n\t\t\traise IndexError(\n\t\t\t\tf\"no utterance index({utteranceIndex}, cant save cancellable commands\"\n\t\t\t)\n\t\tcancellableItems = list(\n\t\t\titem for item in reversed(utterance) if isinstance(item, _CancellableSpeechCommand)\n\t\t)\n\t\tfor item in cancellableItems:\n\t\t\tutterance.remove(item)  # CancellableSpeechCommands should not be sent to the synthesizer.\n\t\t\tif item.isCancelled:\n\t\t\t\tlog._speechManagerDebug(f\"item already cancelled, canceling up to: {utteranceIndex}\")\n\t\t\t\tself._removeCompletedFromQueue(utteranceIndex)\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\titem._utteranceIndex = utteranceIndex\n\t\t\t\tlog._speechManagerDebug(\n\t\t\t\t\tf\"Speaking utterance with cancellable item, index: {utteranceIndex}\"\n\t\t\t\t)\n\t\t\t\tself._cancelCommandsForUtteranceBeingSpokenBySynth[item] = utteranceIndex\n\t\treturn True\n\n\t_WRAPPED_INDEX_MAGNITUDE = int(MAX_INDEX / 2)\n\n\t@classmethod\n\tdef _isIndexABeforeIndexB(cls, indexA: _IndexT, indexB: _IndexT) -> bool:\n\t\t\"\"\"Was indexB created before indexB\n\t\tBecause indexes wrap after MAX_INDEX, custom logic is needed to compare relative positions.\n\t\tThe boundary for considering a wrapped value as before another value is based on the distance\n\t\tbetween the indexes. If the distance is greater than half the available index space it is no longer\n\t\tbefore.\n\t\t@return True if indexA was created before indexB, else False\n\t\t\"\"\"\n\t\tw = cls._WRAPPED_INDEX_MAGNITUDE\n\t\treturn (\n\t\t\tindexA != indexB and (\n\t\t\t\t(indexA < indexB and w >= indexB - indexA) or (\n\t\t\t\t\t# Test for wrapped values\n\t\t\t\t\tindexB < indexA\n\t\t\t\t\t# Avoid dealing with wrapping logic, check distance in the other direction.\n\t\t\t\t\tand w < indexA - indexB\n\t\t\t\t)\n\t\t\t)\n\t\t)\n\n\t@classmethod\n\tdef _isIndexAAfterIndexB(cls, indexA: _IndexT, indexB: _IndexT) -> bool:\n\t\treturn indexA != indexB and not cls._isIndexABeforeIndexB(indexA, indexB)\n\n\tdef _getMostRecentlyCancelledUtterance(self) -> Optional[_IndexT]:\n\t\t# Index of the most recently cancelled utterance.\n\t\tlatestCancelledUtteranceIndex: Optional[_IndexT] = None\n\t\tlog._speechManagerDebug(\n\t\t\tf\"Length of _cancelCommandsForUtteranceBeingSpokenBySynth: \"\n\t\t\tf\"{len(self._cancelCommandsForUtteranceBeingSpokenBySynth)} \"\n\t\t\tf\"Length of _indexesSpeaking: \"\n\t\t\tf\"{len(self._indexesSpeaking)} \"\n\t\t)\n\t\tcancelledIndexes = (\n\t\t\tindex for command, index\n\t\t\tin self._cancelCommandsForUtteranceBeingSpokenBySynth.items()\n\t\t\tif command.isCancelled\n\t\t)\n\t\tfor index in cancelledIndexes:\n\t\t\tif (\n\t\t\t\tlatestCancelledUtteranceIndex is None\n\t\t\t\tor self._isIndexABeforeIndexB(latestCancelledUtteranceIndex, index)\n\t\t\t):\n\t\t\t\tlatestCancelledUtteranceIndex = index\n\t\treturn latestCancelledUtteranceIndex\n\n\tdef removeCancelledSpeechCommands(self):\n\t\tlog._speechManagerUnitTest(\"removeCancelledSpeechCommands\")\n\t\tself._doRemoveCancelledSpeechCommands()\n\n\tdef _doRemoveCancelledSpeechCommands(self):\n\t\tif not _shouldCancelExpiredFocusEvents():\n\t\t\treturn\n\t\t# Don't delete commands while iterating over _cancelCommandsForUtteranceBeingSpokenBySynth.\n\t\tlatestCancelledUtteranceIndex = self._getMostRecentlyCancelledUtterance()\n\t\tlog._speechManagerDebug(f\"Last index: {latestCancelledUtteranceIndex}\")\n\t\tif latestCancelledUtteranceIndex is not None:\n\t\t\tlog._speechManagerDebug(f\"Cancel and push speech\")\n\t\t\t# Minimise the number of calls to _removeCompletedFromQueue by using the most recently cancelled\n\t\t\t# utterance index. This will remove all older queued speech also.\n\t\t\tself._removeCompletedFromQueue(latestCancelledUtteranceIndex)\n\t\t\tgetSynth().cancel()\n\t\t\tself._cancelledLastSpeechWithSynth = True\n\t\t\tself._cancelCommandsForUtteranceBeingSpokenBySynth.clear()\n\t\t\tself._indexesSpeaking.clear()\n\t\t\tself._pushNextSpeech(True)\n\n\tdef _getUtteranceIndex(self, utterance: SpeechSequence):\n\t\t#  find the index command, should be the last in sequence\n\t\tindexItem: IndexCommand = cast(IndexCommand, utterance[-1])\n\t\tif not isinstance(indexItem, IndexCommand):\n\t\t\tlog.error(\"Expected last item to be an indexCommand.\")\n\t\t\treturn None\n\t\treturn indexItem.index\n\n\tdef _onSynthIndexReached(self, synth=None, index=None):\n\t\tlog._speechManagerUnitTest(f\"synthReachedIndex: {index}, synth: {synth}\")\n\t\tif synth != getSynth():\n\t\t\treturn\n\t\t# This needs to be handled in the main thread.\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue, self._handleIndex, index)\n\n\t# C901 'SpeechManager._removeCompletedFromQueue' is too complex\n\t# SpeechManager needs unit tests and a breakdown of responsibilities.\n\tdef _removeCompletedFromQueue(self, index: int) -> Tuple[bool, bool]:  # noqa: C901\n\t\t\"\"\"Removes completed speech sequences from the queue.\n\t\t@param index: The index just reached indicating a completed sequence.\n\t\t@return: Tuple of (valid, endOfUtterance),\n\t\t\twhere valid indicates whether the index was valid and\n\t\t\tendOfUtterance indicates whether this sequence was the end of the current utterance.\n\t\t@rtype: (bool, bool)\n\t\t\"\"\"\n\t\t# Find the sequence that just completed speaking.\n\t\tif not self._curPriQueue:\n\t\t\t# No speech in progress. Probably from a previous utterance which was cancelled.\n\t\t\treturn False, False\n\t\tfor seqIndex, seq in enumerate(self._curPriQueue.pendingSequences):\n\t\t\tlastCommand = seq[-1] if isinstance(seq, list) else None\n\t\t\tif isinstance(lastCommand, IndexCommand):\n\t\t\t\tif self._isIndexAAfterIndexB(index, lastCommand.index):\n\t\t\t\t\tlog.debugWarning(f\"Reached speech index {index :d}, but index {lastCommand.index :d} never handled\")\n\t\t\t\telif index == lastCommand.index:\n\t\t\t\t\tendOfUtterance = isinstance(self._curPriQueue.pendingSequences[seqIndex + 1][0], EndUtteranceCommand)\n\t\t\t\t\tif endOfUtterance:\n\t\t\t\t\t\t# Remove the EndUtteranceCommand as well.\n\t\t\t\t\t\tseqIndex += 1\n\t\t\t\t\tbreak # Found it!\n\t\telse:\n\t\t\tlog._speechManagerDebug(\n\t\t\t\t\"Unknown index. Probably from a previous utterance which was cancelled.\"\n\t\t\t)\n\t\t\treturn False, False\n\t\tif endOfUtterance:\n\t\t\t# These params may not apply to the next utterance if it was queued separately,\n\t\t\t# so reset the tracker.\n\t\t\t# The next utterance will include the commands again if they do still apply.\n\t\t\tself._curPriQueue.paramTracker = ParamChangeTracker()\n\t\telse:\n\t\t\t# Keep track of parameters changed so far.\n\t\t\t# This is necessary in case this utterance is preempted by higher priority speech.\n\t\t\tfor seqIndex in range(seqIndex + 1):\n\t\t\t\tseq = self._curPriQueue.pendingSequences[seqIndex]\n\t\t\t\tfor command in seq:\n\t\t\t\t\tif isinstance(command, SynthParamCommand):\n\t\t\t\t\t\tself._curPriQueue.paramTracker.update(command)\n\t\t# This sequence is done, so we don't need to track it any more.\n\t\ttoRemove = self._curPriQueue.pendingSequences[:seqIndex + 1]\n\t\tlog._speechManagerDebug(\"Removing: %r\", seq)\n\t\tif _shouldCancelExpiredFocusEvents():\n\t\t\tcancellables = (\n\t\t\t\titem\n\t\t\t\tfor seq in toRemove\n\t\t\t\tfor item in seq\n\t\t\t\tif isinstance(\n\t\t\t\t\titem, _CancellableSpeechCommand\n\t\t\t\t)\n\t\t\t)\n\t\t\tfor item in cancellables:\n\t\t\t\tif log.isEnabledFor(log.DEBUG) and _shouldDoSpeechManagerLogging():\n\t\t\t\t\t# Debug logging for cancelling expired focus events.\n\t\t\t\t\tlog._speechManagerDebug(\n\t\t\t\t\t\tf\"Item is in _cancelCommandsForUtteranceBeingSpokenBySynth: \"\n\t\t\t\t\t\tf\"{item in self._cancelCommandsForUtteranceBeingSpokenBySynth.keys()}\"\n\t\t\t\t\t)\n\t\t\t\tself._cancelCommandsForUtteranceBeingSpokenBySynth.pop(item, None)\n\t\tdel self._curPriQueue.pendingSequences[:seqIndex + 1]\n\n\t\treturn True, endOfUtterance\n\n\tdef _handleIndex(self, index: int):\n\t\tlog._speechManagerDebug(f\"Handle index: {index}\")\n\t\t# A synth (such as OneCore) may skip indexes\n\t\t# If before another index, with no text content in between.\n\t\t# Therefore, detect this and ensure we handle all skipped indexes.\n\t\thandleIndexes = []\n\t\tfor oldIndex in list(self._indexesSpeaking):\n\t\t\tif self._isIndexABeforeIndexB(oldIndex, index):\n\t\t\t\tlog.debugWarning(\"Handling skipped index %s\" % oldIndex)\n\t\t\t\thandleIndexes.append(oldIndex)\n\t\thandleIndexes.append(index)\n\t\tvalid, endOfUtterance = False, False\n\t\tfor i in handleIndexes:\n\t\t\ttry:\n\t\t\t\tself._indexesSpeaking.remove(i)\n\t\t\texcept ValueError:\n\t\t\t\tlog.debug(\"Unknown index %s, speech probably cancelled from main thread.\" % i)\n\t\t\t\tbreak  # try the rest, this is a very unexpected path.\n\t\t\tif i != index:\n\t\t\t\tlog.debugWarning(\"Handling skipped index %s\" % i)\n\t\t\t# we must do the following for each index, any/all of them may be end of utterance, which must\n\t\t\t# trigger _pushNextSpeech\n\t\t\t_valid, _endOfUtterance = self._removeCompletedFromQueue(i)\n\t\t\tvalid = valid or _valid\n\t\t\tendOfUtterance = endOfUtterance or _endOfUtterance\n\t\t\tif _valid:\n\t\t\t\tcallbackCommand = self._indexesToCallbacks.pop(i, None)\n\t\t\t\tif callbackCommand:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tlog._speechManagerUnitTest(f\"CallbackCommand Start: {callbackCommand!r}\")\n\t\t\t\t\t\tcallbackCommand.run()\n\t\t\t\t\t\tlog._speechManagerUnitTest(\"CallbackCommand End\")\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\tlog.exception(\"Error running speech callback\")\n\t\tself._doRemoveCancelledSpeechCommands()\n\t\tshouldPush = (\n\t\t\tendOfUtterance\n\t\t\tand not self._synthStillSpeaking()  # stops double speaking errors\n\t\t)\n\t\tif shouldPush:\n\t\t\tif self._indexesSpeaking:\n\t\t\t\tlog._speechManagerDebug(\n\t\t\t\t\tf\"Indexes speaking: {self._indexesSpeaking!r},\"\n\t\t\t\t\tf\" queue: {self._curPriQueue.pendingSequences}\"\n\t\t\t\t)\n\t\t\t# Even if we have many indexes, we should only push next speech once.\n\t\t\tself._pushNextSpeech(False)\n\n\tdef _onSynthDoneSpeaking(self, synth: Optional[synthDriverHandler.SynthDriver] = None):\n\t\tlog._speechManagerUnitTest(f\"synthDoneSpeaking synth:{synth}\")\n\t\tif synth != getSynth():\n\t\t\treturn\n\t\t# This needs to be handled in the main thread.\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue, self._handleDoneSpeaking)\n\n\tdef _handleDoneSpeaking(self):\n\t\tlog._speechManagerDebug(\n\t\t\tf\"Synth done speaking, should push: {self._shouldPushWhenDoneSpeaking}\"\n\t\t)\n\t\tif self._shouldPushWhenDoneSpeaking:\n\t\t\tself._shouldPushWhenDoneSpeaking = False\n\t\t\tself._pushNextSpeech(True)\n\n\tdef _switchProfile(self):\n\t\tcommand = self._curPriQueue.pendingSequences.pop(0)[0]\n\t\tassert isinstance(command, ConfigProfileTriggerCommand), \"First pending command should be a ConfigProfileTriggerCommand\"\n\t\tif command.enter:\n\t\t\ttry:\n\t\t\t\tcommand.trigger.enter()\n\t\t\texcept:\n\t\t\t\tlog.exception(\"Error entering new trigger %r\" % command.trigger.spec)\n\t\t\tself._curPriQueue.enteredProfileTriggers.append(command.trigger)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tcommand.trigger.exit()\n\t\t\texcept:\n\t\t\t\tlog.exception(\"Error exiting active trigger %r\" % command.trigger.spec)\n\t\t\tself._curPriQueue.enteredProfileTriggers.remove(command.trigger)\n\t\tsynthDriverHandler.handlePostConfigProfileSwitch(resetSpeechIfNeeded=False)\n\n\tdef _exitProfileTriggers(self, triggers):\n\t\tfor trigger in reversed(triggers):\n\t\t\ttry:\n\t\t\t\ttrigger.exit()\n\t\t\texcept:\n\t\t\t\tlog.exception(\"Error exiting profile trigger %r\" % trigger.spec)\n\t\tsynthDriverHandler.handlePostConfigProfileSwitch(resetSpeechIfNeeded=False)\n\n\tdef _restoreProfileTriggers(self, triggers):\n\t\tfor trigger in triggers:\n\t\t\ttry:\n\t\t\t\ttrigger.enter()\n\t\t\texcept:\n\t\t\t\tlog.exception(\"Error entering profile trigger %r\" % trigger.spec)\n\t\tsynthDriverHandler.handlePostConfigProfileSwitch(resetSpeechIfNeeded=False)\n\n\tdef cancel(self):\n\t\tlog._speechManagerUnitTest(\"Cancel\")\n\t\tgetSynth().cancel()\n\t\tif self._curPriQueue and self._curPriQueue.enteredProfileTriggers:\n\t\t\tself._exitProfileTriggers(self._curPriQueue.enteredProfileTriggers)\n\t\tself._reset()\n", "idx": 5, "id": 33391, "msg": "", "proj": "nvaccess-nvda", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -0,0 +1,46 @@\n+/*\n+Copyright 2019 The OpenEBS Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+*/\n+\n+package executor\n+\n+import (\n+\t\"strings\"\n+\n+\terrors \"github.com/pkg/errors\"\n+\n+\t\"github.com/spf13/cobra\"\n+)\n+\n+// MigrateOptions stores information required for migrate\n+type MigrateOptions struct {\n+\topenebsNamespace string\n+\tspcName          string\n+\tpvName           string\n+}\n+\n+var (\n+\toptions = &MigrateOptions{\n+\t\topenebsNamespace: \"openebs\",\n+\t}\n+)\n+\n+// RunPreFlightChecks will ensure the sanity of the common migrate options\n+func (u *MigrateOptions) RunPreFlightChecks(cmd *cobra.Command) error {\n+\tif len(strings.TrimSpace(u.openebsNamespace)) == 0 {\n+\t\treturn errors.Errorf(\"Cannot execute migrate job: namespace is missing\")\n+\t}\n+\treturn nil\n+}", "y": 1, "oldf": "", "idx": 1, "id": 17721, "msg": "`spcName` is unused (from `structcheck`)", "proj": "openebs-maya", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -119,6 +119,9 @@ public class CorePushPull {\n       if (solrCore == null) {\n         throw new Exception(\"Can't find core \" + pushPullData.getCoreName());\n       }\n+      \n+      FileTransferCounter counter = new FileTransferCounter();\n+      boolean isSuccessful = false;\n \n       try {\n         // Creating the new BlobCoreMetadata as a modified clone of the existing one", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.solr.store.blob.metadata;\n\nimport java.io.EOFException;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.lang.invoke.MethodHandles;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.Locale;\nimport java.util.Set;\nimport java.util.concurrent.Future;\nimport java.util.stream.Collectors;\n\nimport com.google.common.annotations.VisibleForTesting;\nimport org.apache.commons.io.IOUtils;\nimport org.apache.lucene.codecs.CodecUtil;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.IOContext;\nimport org.apache.lucene.store.IndexInput;\nimport org.apache.lucene.store.IndexOutput;\nimport org.apache.solr.common.SolrException;\nimport org.apache.solr.core.CoreContainer;\nimport org.apache.solr.core.DirectoryFactory;\nimport org.apache.solr.core.SolrCore;\nimport org.apache.solr.store.blob.client.BlobCoreMetadata;\nimport org.apache.solr.store.blob.client.BlobCoreMetadata.BlobFile;\nimport org.apache.solr.store.blob.client.BlobCoreMetadataBuilder;\nimport org.apache.solr.store.blob.client.CoreStorageClient;\nimport org.apache.solr.store.blob.client.ToFromJson;\nimport org.apache.solr.store.blob.metadata.ServerSideMetadata.CoreFileData;\nimport org.apache.solr.store.blob.metadata.SharedStoreResolutionUtil.SharedMetadataResolutionResult;\nimport org.apache.solr.store.blob.process.BlobDeleteManager;\nimport org.apache.solr.store.blob.util.BlobStoreUtils;\nimport org.apache.solr.store.shared.metadata.SharedShardMetadataController;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Class pushing updates from the local core to the Blob Store and pulling updates from Blob store to local core.\n * This class knows about the Solr core directory structure and can translate it into file system abstractions since that's\n * what needed by Blob store.\n */\npublic class CorePushPull {\n\n    private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n\n    private final CoreStorageClient coreStorageClient;\n    \n    // contains metadata needed to interact with the shared store\n    private final PushPullData pushPullData;\n    // contains the listing of files that need to be pushed or pulled to/from the shared store\n    private final SharedMetadataResolutionResult resolvedMetadataResult;\n    \n    private final ServerSideMetadata solrServerMetadata;\n    private final BlobCoreMetadata blobMetadata;\n    private final CoreContainer container;\n    private final BlobDeleteManager deleteManager;\n\n    /**\n     * Creates an instance allowing pushing and pulling local core content to/from blob store.\n     */\n    public CorePushPull(CoreStorageClient client, BlobDeleteManager deleteManager,\n        PushPullData data, SharedMetadataResolutionResult resolvedMetadataResult, \n        ServerSideMetadata solrServerMetadata, BlobCoreMetadata blobMetadata) {\n      this.coreStorageClient = client;\n      this.deleteManager = deleteManager;\n      this.container = getContainerFromServerMetadata(solrServerMetadata);  \n      this.pushPullData = data;\n      this.resolvedMetadataResult = resolvedMetadataResult;\n      this.solrServerMetadata = solrServerMetadata;\n      this.blobMetadata = blobMetadata;\n    }\n\n    /**\n     * Extracted to be able to pass a null ServerSideCoreMetadata to the constructor in unit tests.\n     */\n    protected CoreContainer getContainerFromServerMetadata(ServerSideMetadata solrServerMetadata) {\n        return solrServerMetadata.getCoreContainer();\n    }\n\n    /**\n     * Writes to the Blob store all the files that should be written to it, then updates and writes the {@link BlobCoreMetadata}.\n     * After that call, the Blob store is fully updated for the core.<p>\n     * In case of exception, it means that the new {@link BlobCoreMetadata} has not been written which means that the old\n     * content of the Blob store remains unchanged (all new files are written with new unique names so do not interfere)\n     * but it also means some garbage collection will eventually be required because new written blobs are not accounted for.<p>\n     *\n     * This method (need to verify this is indeed the case!) can be used either to push small updates from the local core\n     * to Blob or to completely overwrite the Blob content for the core with the local content.\n     * \n     * @param currentMetadataSuffix suffix of the core.metadata file corresponding to {@link CorePushPull#blobMetadata}\n     *                              TODO: there is an existing todo with delete logic where this parameter is consumed  \n     *                                    with that add a metadataSuffix field to {@link BlobCoreMetadata} \n     * @param newMetadataSuffix suffix of the new core.metadata file to be created as part of this push\n     */\n    public BlobCoreMetadata pushToBlobStore(String currentMetadataSuffix, String newMetadataSuffix) throws Exception {\n      long startTimeMs = System.nanoTime();\n      SolrCore solrCore = container.getCore(pushPullData.getCoreName());\n      if (solrCore == null) {\n        throw new Exception(\"Can't find core \" + pushPullData.getCoreName());\n      }\n\n      try {\n        // Creating the new BlobCoreMetadata as a modified clone of the existing one\n        BlobCoreMetadataBuilder bcmBuilder = new BlobCoreMetadataBuilder(blobMetadata, solrServerMetadata.getGeneration());\n\n        /*\n         * Removing from the core metadata the files that are stored on the blob store but no longer needed.\n         *\n         * When this method is executed, the content of the index on Blob is to be replaced with the local content.\n         * The assumption (or normal flow) is for local to refresh from Blob, update locally then push the\n         * changes to Blob.\n         * When merges happen locally the update has to mark old segment files for delete (and for example the\n         * previous \"segments_N\" is to be deleted as a new higher generation segment has been created).\n         */\n        for (BlobCoreMetadata.BlobFile d : resolvedMetadataResult.getFilesToDelete()) {\n            bcmBuilder.removeFile(d);\n            BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(d, System.currentTimeMillis());\n            bcmBuilder.addFileToDelete(bftd);\n        }\n\n        // add the old core.metadata file to delete\n        if (!currentMetadataSuffix.equals(SharedShardMetadataController.METADATA_NODE_DEFAULT_VALUE)) {\n          // TODO This may be inefficient but we'll likely remove this when CorePushPull is refactored to have deletion elsewhere\n          //      could be added to resolvedMetadataResult#getFilesToDelete()\n          ToFromJson<BlobCoreMetadata> converter = new ToFromJson<>();\n          String json = converter.toJson(blobMetadata);\n          int bcmSize = json.getBytes().length;\n\n          String blobCoreMetadataName = BlobStoreUtils.buildBlobStoreMetadataName(currentMetadataSuffix);\n          String coreMetadataPath = blobMetadata.getSharedBlobName() + \"/\" + blobCoreMetadataName;\n          // so far checksum is not used for metadata file\n          BlobCoreMetadata.BlobFileToDelete bftd = new BlobCoreMetadata.BlobFileToDelete(\"\", coreMetadataPath, bcmSize, BlobCoreMetadataBuilder.UNDEFINED_VALUE, System.currentTimeMillis());\n          bcmBuilder.addFileToDelete(bftd);\n        }\n\n        // When we build solrServerMetadata we requested to reserve the commit point for some short duration. Assumption is\n        // it took less than this duration to get here (didn't do anything blocking) and now we actually save the commit\n        // point for the (potentially long) time it takes to push all files to the Blob store.\n        Directory coreIndexDir = solrCore.getDirectoryFactory().get(solrCore.getIndexDir(), DirectoryFactory.DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);\n        solrCore.getDeletionPolicy().saveCommitPoint(solrServerMetadata.getGeneration());\n        try {\n          // Directory's javadoc says: \"Java's i/o APIs not used directly, but rather all i/o is through this API\"\n          // But this is untrue/totally false/misleading. IndexFetcher has File all over.\n          for (CoreFileData cfd : resolvedMetadataResult.getFilesToPush()) {\n            // Sanity check that we're talking about the same file (just sanity, Solr doesn't update files so should never be different)\n            assert cfd.getFileSize() == coreIndexDir.fileLength(cfd.getFileName());\n\n            String blobPath = pushFileToBlobStore(coreStorageClient, coreIndexDir, cfd.getFileName(), cfd.getFileSize());\n            bcmBuilder.addFile(new BlobCoreMetadata.BlobFile(cfd.getFileName(), blobPath, cfd.getFileSize(), cfd.getChecksum()));\n          }\n        } finally {\n          solrCore.getDeletionPolicy().releaseCommitPoint(solrServerMetadata.getGeneration());\n          solrCore.getDirectoryFactory().release(coreIndexDir);\n        }\n\n        // delete what we need\n        enqueueForHardDelete(bcmBuilder);\n\n        BlobCoreMetadata newBcm = bcmBuilder.build();\n\n        String blobCoreMetadataName = BlobStoreUtils.buildBlobStoreMetadataName(newMetadataSuffix);\n        coreStorageClient.pushCoreMetadata(blobMetadata.getSharedBlobName(), blobCoreMetadataName, newBcm);\n        return newBcm;\n      } finally {\n        solrCore.close();\n\n        long filesAffected = resolvedMetadataResult.getFilesToPush().size();\n        long bytesTransferred = resolvedMetadataResult.getFilesToPush().stream().mapToLong(cfd -> cfd.getFileSize()).sum();\n        \n        // todo correctness stuff\n        logBlobAction(\"PUSH\", filesAffected, bytesTransferred, startTimeMs, 0, startTimeMs);\n      }\n    }\n\n    /**\n     * Calls {@link #pullUpdateFromBlob(long, boolean, int)}  with current epoch time and attempt no. 0.\n     * @param waitForSearcher <code>true</code> if this call should wait until the index searcher is created (so that any query\n     *                     after the return from this method sees the new pulled content) or <code>false</code> if we request\n     *                     a new index searcher to be eventually created but do not wait for it to be created (a query\n     *                     following the return from this call might see the old core content).\n     */\n    public void pullUpdateFromBlob(boolean waitForSearcher) throws Exception {\n         pullUpdateFromBlob(System.nanoTime(), waitForSearcher, 0);\n    }\n\n    /**\n     * We're doing here what replication does in {@link org.apache.solr.handler.IndexFetcher#fetchLatestIndex(boolean, boolean)}.<p>\n     *\n     * This method will work in 2 cases:\n     * <ol>\n     * <li>Local core needs to fetch an update from Blob</li>\n     * <li>Local core did not exist (was created empty before calling this method) and is fetched from Blob</li>\n     * </ol>\n     * \n     * @param requestQueuedTimeMs epoch time in milliseconds when the pull request was queued(meaningful in case of async pushing)\n     *                            only used for logging purposes\n     * @param waitForSearcher <code>true</code> if this call should wait until the index searcher is created (so that any query\n     *                     after the return from this method sees the new pulled content) or <code>false</code> if we request\n     *                     a new index searcher to be eventually created but do not wait for it to be created (a query\n     *                     following the return from this call might see the old core content).\n     * @param attempt 0 based attempt number (meaningful in case of pushing with retry mechanism in case of failure)\n     *                only used for logging purposes \n     *\n     * @throws Exception    In the context of the PoC and first runs, we ignore errors such as Blob Server not available\n     *                      or network issues. We therefore consider all exceptions thrown by this method as a sign that\n     *                      it is durably not possible to pull the core from the Blob Store.\n     *                      TODO This has to be revisited before going to real prod, as environemnt issues can cause massive reindexing with this strategy\n     */\n    public void pullUpdateFromBlob(long requestQueuedTimeMs, boolean waitForSearcher, int attempt) throws Exception {\n        long startTimeMs = System.nanoTime();\n        try {\n          SolrCore solrCore = container.getCore(pushPullData.getCoreName());\n          if (solrCore == null) {\n            throw new Exception(\"Can't find core \" + pushPullData.getCoreName());\n          }\n          // if there is a conflict between local and blob contents we will move the core to a new index directory\n          final boolean createNewIndexDir = resolvedMetadataResult.isLocalConflictingWithBlob();\n          boolean coreSwitchedToNewIndexDir = false;\n          try {\n            // Create temp directory (within the core local folder).\n            // If we are moving index to a new directory because of conflict then this will be that new directory.\n            // Even if we are not moving to a newer directory we will first download files from blob store into this temp directory.\n            // Then we will move files from temp directory to index directory. This is to avoid leaving a download half done\n            // in case of failure as well as to limit the time during which we close then reopen the index writer to take\n            // into account the new files. In theory nothing should be changing the local directory as we pull files from\n            // Blob store, but let's be defensive (we're checking further down that local dir hasn't changed in the meantime).\n            String tempIndexDirName = \"index.pull.\" + System.nanoTime();\n            String tempIndexDirPath = solrCore.getDataDir() + tempIndexDirName;\n            Directory tempIndexDir = solrCore.getDirectoryFactory().get(tempIndexDirPath, DirectoryFactory.DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);\n            try {\n              String indexDirPath = solrCore.getIndexDir();\n              Collection<BlobFile> filesToDownload;\n              if (createNewIndexDir) {\n                // This is an optimization to not download everything from blob if possible\n                // This made sense for some rolling start scenario in TLOG replicas and makes here too\n                // https://issues.apache.org/jira/browse/SOLR-11920\n                // https://issues.apache.org/jira/browse/SOLR-11815\n                // TODO: We might want to skip this optimization when healing a locally corrupt core\n                Directory indexDir = solrCore.getDirectoryFactory().get(indexDirPath, DirectoryFactory.DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);\n                try {\n                  filesToDownload = initializeNewIndexDirWithLocallyAvailableFiles(indexDir, tempIndexDir);\n                } finally {\n                  solrCore.getDirectoryFactory().release(indexDir);\n                }\n              } else {\n                filesToDownload = resolvedMetadataResult.getFilesToPull();\n              }\n              downloadFilesFromBlob(tempIndexDir, filesToDownload);\n\n              Directory indexDir = solrCore.getDirectoryFactory().get(indexDirPath, DirectoryFactory.DirContext.DEFAULT, solrCore.getSolrConfig().indexConfig.lockType);\n              try {\n                if (!createNewIndexDir) {\n                  // TODO should we call solrCore.closeSearcher() here? IndexFetcher.fetchLatestIndex() does call it.\n                  // Close the index writer to stop changes to this core\n                  solrCore.getUpdateHandler().getSolrCoreState().closeIndexWriter(solrCore, true);\n                }\n\n                boolean thrownException = false;\n                try {\n                  // Make sure Solr core directory content hasn't changed since we decided what we want to pull from Blob\n                  if (!solrServerMetadata.isSameDirectoryContent(indexDir)) {\n                    // Maybe return something less aggressive than throwing an exception? TBD once we end up calling this method :)\n                    throw new Exception(\"Local Directory content \" + indexDirPath + \" has changed since Blob pull started. Aborting pull.\");\n                  }\n\n                  if (createNewIndexDir) {\n                    // point index to the new directory. Method call below always returns true BTW.\n                    coreSwitchedToNewIndexDir = solrCore.modifyIndexProps(tempIndexDirName);\n                  } else {\n                    moveFilesFromTempToIndexDir(solrCore, tempIndexDir, indexDir);\n                  }\n                } catch (Exception e) {\n                  // Used in the finally below to not mask an exception thrown from the try block above\n                  thrownException = true;\n                  throw e;\n                } finally {\n                  try {\n                    // TODO this has been observed to throw org.apache.lucene.index.CorruptIndexException on certain types of corruptions in Blob Store. We need to handle this correctly (maybe we already do).\n                    if (!createNewIndexDir) {\n                      // The closed index writer must be opened back (in the finally bloc)\n                      solrCore.getUpdateHandler().getSolrCoreState().openIndexWriter(solrCore);\n                    } else if (coreSwitchedToNewIndexDir) {\n                      solrCore.getUpdateHandler().newIndexWriter(true);\n                    }\n                  } catch (IOException ioe) {\n                    // TODO corrupt core handling happened here\n                    // CorruptCoreHandler.notifyBlobPullFailure(container, coreName, blobMetadata);\n                    if (!thrownException) {\n                      // Do not mask a previous exception with a more recent one so only throw the new one if none was thrown previously\n                      throw ioe;\n                    }\n                  }\n                }\n              } finally {\n                try {\n                  if (coreSwitchedToNewIndexDir) {\n                    solrCore.getDirectoryFactory().doneWithDirectory(indexDir);\n                    solrCore.getDirectoryFactory().remove(indexDir);\n                  }\n                } catch (Exception e) {\n                  log.warn(\"Cannot remove previous index directory \" + indexDir, e);\n                } finally {\n                  solrCore.getDirectoryFactory().release(indexDir);\n                }\n              }\n            } finally {\n              try {\n                if (!coreSwitchedToNewIndexDir) {\n                  solrCore.getDirectoryFactory().doneWithDirectory(tempIndexDir);\n                  solrCore.getDirectoryFactory().remove(tempIndexDir);\n                }\n              } catch (Exception e) {\n                log.warn(\"Cannot remove temp directory \" + tempIndexDirPath, e);\n              } finally {\n                solrCore.getDirectoryFactory().release(tempIndexDir);\n              }\n            }\n            \n            try {\n              if (waitForSearcher) {\n                // Open and register a new searcher, we don't need it but we wait for it to be open.\n                Future[] waitSearcher = new Future[1];\n                solrCore.getSearcher(true, false, waitSearcher, true);\n                if (waitSearcher[0] == null) {\n                  throw new Exception(\"Can't wait for index searcher to be created. Future queries might misbehave for core=\" + pushPullData.getCoreName());\n                } else {\n                  waitSearcher[0].get();\n                }\n              } else {\n                // Open and register a new searcher, but don't wait and we don't need it either.\n                solrCore.getSearcher(true, false, null, true);\n              }\n            } catch (SolrException se) {\n              // TODO corrupt core handling happened here\n              // CorruptCoreHandler.notifyBlobPullFailure(container, coreName, blobMetadata);\n              throw se;\n            }\n          } finally {\n            solrCore.close();\n          }\n        } finally {\n          long filesAffected = resolvedMetadataResult.getFilesToPull().size();\n          long bytesTransferred = resolvedMetadataResult.getFilesToPull().stream().mapToLong(bf -> bf.getFileSize()).sum();\n          \n          logBlobAction(\"PULL\", filesAffected, bytesTransferred, requestQueuedTimeMs, attempt, startTimeMs);\n        }\n    }\n\n    private void moveFilesFromTempToIndexDir(SolrCore solrCore, Directory tmpIndexDir, Directory dir) throws IOException {\n      // Copy all files into the Solr directory\n      // Move the segments_N file last once all other are ok.\n      String segmentsN = null;\n      for (BlobFile bf : resolvedMetadataResult.getFilesToPull()) {\n        if (SharedStoreResolutionUtil.isSegmentsNFilename(bf)) {\n          assert segmentsN == null;\n          segmentsN = bf.getSolrFileName();\n        } else {\n          // Copy all non segments_N files\n          moveFileToDirectory(solrCore, tmpIndexDir, bf.getSolrFileName(), dir);\n        }\n      }\n      assert segmentsN != null;\n      // Copy segments_N file. From this point on the local core might be accessed and is up to date with Blob content\n      moveFileToDirectory(solrCore, tmpIndexDir, segmentsN, dir);\n    }\n\n    private Collection<BlobFile> initializeNewIndexDirWithLocallyAvailableFiles(Directory indexDir, Directory newIndexDir) {\n      Collection<BlobFile> filesToDownload = new HashSet<>();\n        for (BlobFile blobFile : resolvedMetadataResult.getFilesToPull()) {\n          try (final IndexInput indexInput = indexDir.openInput(blobFile.getSolrFileName(), IOContext.READONCE)) {\n            long length = indexInput.length();\n            long checksum  = CodecUtil.retrieveChecksum(indexInput);\n            if (length == blobFile.getFileSize() && checksum == blobFile.getChecksum()) {\n              copyFileToDirectory(indexDir, blobFile.getSolrFileName(), newIndexDir);\n            } else {\n              filesToDownload.add(blobFile);\n            }\n          } catch (Exception ex){\n            // Either file does not exist locally or copy not succeeded, we will download from blob store\n            filesToDownload.add(blobFile);\n          }\n        }\n      return filesToDownload;\n    }\n\n    /**\n     * Pushes a local file to blob store and returns a unique path to newly created blob\n     */\n    @VisibleForTesting\n    protected String pushFileToBlobStore(CoreStorageClient blob, Directory dir, String fileName, long fileSize) throws Exception {\n        String blobPath;\n        IndexInput ii = dir.openInput(fileName, IOContext.READONCE);\n        try (InputStream is = new IndexInputStream(ii)) {\n            blobPath = blob.pushStream(blobMetadata.getSharedBlobName(), is, fileSize, fileName);\n        }\n        return blobPath;\n    }\n\n    /**\n     * Logs soblb line for push or pull action \n     * TODO: This is for callers of this method.\n     * fileAffected and bytesTransferred represent correct values only in case of success\n     * In case of failure(partial processing) we are not accurate.\n     * Do we want to change that? If yes, then in case of pull is downloading of files locally to temp folder is considered\n     * transfer or moving from temp dir to final destination. One option could be to just make them -1 in case of failure.\n     */\n    private void logBlobAction(String action, long filesAffected, long bytesTransferred, long requestQueuedTimeMs, int attempt, long startTimeMs) throws Exception {\n      long now = System.nanoTime();\n      long runTime = now - startTimeMs;\n      long startLatency = now - requestQueuedTimeMs;\n\n      String message = String.format(Locale.ROOT,\n            \"PushPullData=[%s] action=%s storageProvider=%s bucketRegion=%s bucketName=%s \"\n              + \"runTime=%s startLatency=%s bytesTransferred=%s attempt=%s filesAffected=%s localGeneration=%s blobGeneration=%s \",\n          pushPullData.toString(), action, coreStorageClient.getStorageProvider().name(), coreStorageClient.getBucketRegion(),\n          coreStorageClient.getBucketName(), runTime, startLatency, bytesTransferred, attempt, filesAffected,\n          solrServerMetadata.getGeneration(), blobMetadata.getGeneration());\n      log.info(message);\n    }\n\n    /**\n     * Downloads files from the Blob store \n     * @param destDir (temporary) directory into which files should be downloaded.\n     * @param filesToDownload blob files to be downloaded\n     */\n    @VisibleForTesting\n    protected void downloadFilesFromBlob(Directory destDir, Collection<? extends BlobFile> filesToDownload) throws Exception {\n      // Synchronously download all Blob blobs (remember we're running on an async thread, so no need to be async twice unless\n      // we eventually want to parallelize downloads of multiple blobs, but for the PoC we don't :)\n      for (BlobFile bf: filesToDownload) {\n        log.info(\"About to create \" + bf.getSolrFileName() + \" for core \" + pushPullData.getCoreName() +\n            \" from index on blob \" + pushPullData.getSharedStoreName());\n        IndexOutput io = destDir.createOutput(bf.getSolrFileName(), DirectoryFactory.IOCONTEXT_NO_CACHE);\n\n        try (OutputStream outStream = new IndexOutputStream(io);\n          InputStream bis = coreStorageClient.pullStream(bf.getBlobName())) {\n          IOUtils.copy(bis, outStream);\n        }\n      }\n    }\n\n    /**\n     * Copies {@code fileName} from {@code fromDir} to {@code toDir}\n     */\n    private void copyFileToDirectory(Directory fromDir, String fileName, Directory toDir) throws IOException {\n      // TODO: Consider optimizing with org.apache.lucene.store.HardlinkCopyDirectoryWrapper\n      toDir.copyFrom(fromDir, fileName, fileName, DirectoryFactory.IOCONTEXT_NO_CACHE);\n    }\n\n    /**\n     * Moves {@code fileName} from {@code fromDir} to {@code toDir}\n     */\n    private void moveFileToDirectory(SolrCore solrCore, Directory fromDir, String fileName, Directory toDir) throws IOException {\n      // We don't need to keep the original files so we move them over.\n      // TODO: Consider optimizing with org.apache.lucene.store.HardlinkCopyDirectoryWrapper\n      solrCore.getDirectoryFactory().move(fromDir, toDir, fileName, DirectoryFactory.IOCONTEXT_NO_CACHE);\n    }\n    \n    @VisibleForTesting\n    void enqueueForHardDelete(BlobCoreMetadataBuilder bcmBuilder) throws Exception {\n      Iterator<BlobCoreMetadata.BlobFileToDelete> it = bcmBuilder.getDeletedFilesIterator();\n      Set<BlobCoreMetadata.BlobFileToDelete> filesToDelete = new HashSet<>();\n      while (it.hasNext()) {\n        BlobCoreMetadata.BlobFileToDelete ftd = it.next();\n        if (okForHardDelete(ftd)) {\n          filesToDelete.add(ftd);\n        }\n      }        \n\n      if (enqueueForDelete(bcmBuilder.getSharedBlobName(), filesToDelete)) {\n        bcmBuilder.removeFilesFromDeleted(filesToDelete);\n      }\n    }\n    \n    /**\n     * Returns true if a deleted blob file (i.e. a file marked for delete but not deleted yet) can be hard deleted now.\n     */\n    @VisibleForTesting\n    protected boolean okForHardDelete(BlobCoreMetadata.BlobFileToDelete file) {\n      // For now we only check how long ago the file was marked for delete.\n      return System.nanoTime() - file.getDeletedAt() >= deleteManager.getDeleteDelayMs();\n    }\n    \n    @VisibleForTesting\n    protected boolean enqueueForDelete(String coreName, Set<BlobCoreMetadata.BlobFileToDelete> blobFiles) {\n      if (blobFiles == null || blobFiles.isEmpty()) {\n        return false;\n      }\n      Set<String> blobNames = blobFiles.stream()\n                                .map(blobFile -> blobFile.getBlobName())\n                                .collect(Collectors.toCollection(HashSet::new));\n      return deleteManager.enqueueForDelete(coreName, blobNames);\n    }\n\n    /**\n     * Wraps an {@link IndexInput} into an {@link InputStream}, while correctly converting the SIGNED bytes returned by\n     * the {@link IndexInput} into the \"unsigned bytes stored in an integer\" expected to be returned by {@link InputStream#read()}.<p>\n     * Class {@link org.apache.solr.util.PropertiesInputStream} does almost the same thing but\n     * {@link org.apache.solr.util.PropertiesInputStream#read()} does not unsign the returned value, breaking the contract\n     * of {@link InputStream#read()} as stated in its Javadoc \"Returns: the next byte of data, or -1 if the end of the stream is reached.\".<p>\n     *\n     * Performance is likely lower using this class than doing direct file manipulations. To keep in mind if we have streaming perf issues.\n     */\n    static class IndexInputStream extends InputStream {\n      private final IndexInput indexInput;\n\n      IndexInputStream(IndexInput indexInput) {\n        this.indexInput = indexInput;\n      }\n\n      @Override\n      public int read() throws IOException {\n        try {\n          return indexInput.readByte() & 0xff;\n        } catch (EOFException e) {\n          return -1;\n        }\n      }\n\n      @Override\n      public void close() throws IOException {\n        super.close();\n        indexInput.close();\n      }\n    }\n\n    /**\n     * This class wraps an {@link IndexOutput} inside an {@link OutputStream}. It happens to be identical to the (at least\n     * current) implementation of {@link org.apache.solr.util.PropertiesOutputStream} but copying it nonetheless for future proofing.<p>\n     * Note then that in theory, {@link org.apache.solr.util.PropertiesOutputStream#write(int)} should have refused negative values as parameters\n     * to retain symmetry with {@link org.apache.solr.util.PropertiesInputStream}... That would have saved me a day of debug.<p>\n     *\n     * Performance is likely lower using this class than doing direct file manipulations. To keep in mind if we have streaming perf issues.\n     */\n    static class IndexOutputStream extends OutputStream {\n      private final IndexOutput indexOutput;\n\n      IndexOutputStream(IndexOutput indexOutput) {\n        this.indexOutput = indexOutput;\n      }\n\n      @Override\n      public void write(int b) throws IOException {\n        indexOutput.writeByte((byte) b);\n      }\n\n      @Override\n      public void close() throws IOException {\n        super.close();\n        indexOutput.close();\n      }\n    }\n}\n", "idx": 4, "id": 32210, "msg": "", "proj": "apache-lucene-solr", "lang": "java", "sampling_weight": 0.1527621930534172}
{"patch": "@@ -23,7 +23,7 @@ from .util import (\n     QuiltException, fix_url, get_from_config, get_install_location, make_s3_url, parse_file_url,\n     parse_s3_url, validate_package_name, quiltignore_filter, validate_key, extract_file_extension, file_is_local\n )\n-from .util import TEMPFILE_DIR_PATH as APP_DIR_TEMPFILE_DIR\n+from .util import CACHE_PATH, TEMPFILE_DIR_PATH as APP_DIR_TEMPFILE_DIR\n \n \n def hash_file(readable_file):", "y": 0, "oldf": "from collections import deque\nimport copy\nimport hashlib\nimport io\nimport json\nimport pathlib\nimport os\nimport time\nfrom multiprocessing import Pool\nfrom urllib.parse import quote, urlparse, unquote\nimport uuid\nimport warnings\n\nimport jsonlines\n\nfrom .data_transfer import (\n    calculate_sha256, copy_file, copy_file_list, get_bytes, get_size_and_version,\n    list_object_versions, put_bytes\n)\nfrom .exceptions import PackageException\nfrom .formats import FormatRegistry\nfrom .util import (\n    QuiltException, fix_url, get_from_config, get_install_location, make_s3_url, parse_file_url,\n    parse_s3_url, validate_package_name, quiltignore_filter, validate_key, extract_file_extension, file_is_local\n)\nfrom .util import TEMPFILE_DIR_PATH as APP_DIR_TEMPFILE_DIR\n\n\ndef hash_file(readable_file):\n    \"\"\" Returns SHA256 hash of readable file-like object \"\"\"\n    buf = readable_file.read(4096)\n    hasher = hashlib.sha256()\n    while buf:\n        hasher.update(buf)\n        buf = readable_file.read(4096)\n\n    return hasher.hexdigest()\n\ndef _to_singleton(physical_keys):\n    \"\"\"\n    Ensure that there is a single physical key, throw otherwise.\n    Temporary utility method to avoid repeated, identical checks.\n\n    Args:\n        pkeys (list): list of physical keys\n    Returns:\n        A physical key\n\n    Throws:\n        NotImplementedError\n\n    TODO:\n        support multiple physical keys\n    \"\"\"\n    if len(physical_keys) > 1:\n        raise NotImplementedError(\"Multiple physical keys not supported\")\n\n    return physical_keys[0]\n\n\ndef _delete_local_physical_key(pk):\n    assert file_is_local(pk), \"This function only works on files that live on a local disk\"\n    pathlib.Path(parse_file_url(urlparse(pk))).unlink()\n\n\n\nclass PackageEntry(object):\n    \"\"\"\n    Represents an entry at a logical key inside a package.\n    \"\"\"\n    __slots__ = ['physical_keys', 'size', 'hash', '_meta']\n    def __init__(self, physical_keys, size, hash_obj, meta):\n        \"\"\"\n        Creates an entry.\n\n        Args:\n            physical_keys: a nonempty list of URIs (either `s3://` or `file://`)\n            size(number): size of object in bytes\n            hash({'type': string, 'value': string}): hash object\n                for example: {'type': 'SHA256', 'value': 'bb08a...'}\n            meta(dict): metadata dictionary\n\n        Returns:\n            a PackageEntry\n        \"\"\"\n        self.physical_keys = [fix_url(x) for x in physical_keys]\n        self.size = size\n        self.hash = hash_obj\n        self._meta = meta or {}\n\n    def __eq__(self, other):\n        return (\n            # Don't check physical keys.\n            self.size == other.size\n            and self.hash == other.hash\n            and self._meta == other._meta\n        )\n\n    def __repr__(self):\n        return f\"PackageEntry('{self.physical_keys[0]}')\"\n\n    def as_dict(self):\n        \"\"\"\n        Returns dict representation of entry.\n        \"\"\"\n        ret = {\n            'physical_keys': self.physical_keys,\n            'size': self.size,\n            'hash': self.hash,\n            'meta': self._meta\n        }\n        return copy.deepcopy(ret)\n\n    def _clone(self):\n        \"\"\"\n        Returns clone of this PackageEntry.\n        \"\"\"\n        return self.__class__(copy.deepcopy(self.physical_keys), self.size, \\\n                              copy.deepcopy(self.hash), copy.deepcopy(self._meta))\n\n    @property\n    def meta(self):\n        return self._meta.get('user_meta', dict())\n\n    def set_meta(self, meta):\n        \"\"\"\n        Sets the user_meta for this PackageEntry.\n        \"\"\"\n        self._meta['user_meta'] = meta\n\n    def _verify_hash(self, read_bytes):\n        \"\"\"\n        Verifies hash of bytes\n        \"\"\"\n        if self.hash is None:\n            raise QuiltException(\"Hash missing - need to build the package\")\n        if self.hash.get('type') != 'SHA256':\n            raise NotImplementedError\n        digest = hashlib.sha256(read_bytes).hexdigest()\n        if digest != self.hash.get('value'):\n            raise QuiltException(\"Hash validation failed\")\n\n    def set(self, path=None, meta=None):\n        \"\"\"\n        Returns self with the physical key set to path.\n\n        Args:\n            logical_key(string): logical key to update\n            path(string): new path to place at logical_key in the package\n                Currently only supports a path on local disk\n            meta(dict): metadata dict to attach to entry. If meta is provided, set just\n                updates the meta attached to logical_key without changing anything\n                else in the entry\n\n        Returns:\n            self\n        \"\"\"\n        if path is not None:\n            self.physical_keys = [fix_url(path)]\n            self.size = None\n            self.hash = None\n        elif meta is not None:\n            self.set_meta(meta)\n        else:\n            raise PackageException('Must specify either path or meta')\n\n    def get(self):\n        \"\"\"\n        Returns the physical key of this PackageEntry.\n        \"\"\"\n        return _to_singleton(self.physical_keys)\n\n    def deserialize(self, func=None, **format_opts):\n        \"\"\"\n        Returns the object this entry corresponds to.\n\n        Args:\n            func: Skip normal deserialization process, and call func(bytes),\n                returning the result directly.\n            **format_opts: Some data formats may take options.  Though\n                normally handled by metadata, these can be overridden here.\n        Returns:\n            The deserialized object from the logical_key\n\n        Raises:\n            physical key failure\n            hash verification fail\n            when deserialization metadata is not present\n        \"\"\"\n        physical_key = _to_singleton(self.physical_keys)\n        data = get_bytes(physical_key)\n\n        if func is not None:\n            return func(data)\n\n        pkey_ext = pathlib.PurePosixPath(unquote(urlparse(physical_key).path)).suffix\n\n        # Verify format can be handled before checking hash.  Raises if none found.\n        formats = FormatRegistry.search(None, self._meta, pkey_ext)\n\n        # Verify hash before deserializing..\n        self._verify_hash(data)\n\n        return formats[0].deserialize(data, self._meta, pkey_ext, **format_opts)\n\n    def fetch(self, dest=None):\n        \"\"\"\n        Gets objects from entry and saves them to dest.\n\n        Args:\n            dest: where to put the files\n                Defaults to the entry name\n\n        Returns:\n            None\n        \"\"\"\n        physical_key = _to_singleton(self.physical_keys)\n\n        if dest is None:\n            name = pathlib.PurePosixPath(unquote(urlparse(physical_key).path)).name\n            dest = (pathlib.Path().resolve() / name).as_uri()\n        else:\n            dest = fix_url(dest)\n\n        copy_file(physical_key, dest)\n\n        # return a package reroot package physical keys after the copy operation succeeds\n        # see GH#388 for context\n        entry = self._clone()\n        entry.physical_keys = [dest]\n        return entry\n\n\n    def __call__(self, func=None, **kwargs):\n        \"\"\"\n        Shorthand for self.deserialize()\n        \"\"\"\n        return self.deserialize(func=func, **kwargs)\n\n\nclass Package(object):\n    \"\"\" In-memory representation of a package \"\"\"\n\n    def __init__(self):\n        self._children = {}\n        self._meta = {'version': 'v0'}\n\n    def __repr__(self, max_lines=20):\n        \"\"\"\n        String representation of the Package.\n        \"\"\"\n        def _create_str(results_dict, level=0, parent=True):\n            \"\"\"\n            Creates a string from the results dict\n            \"\"\"\n            result = ''\n            keys = sorted(results_dict.keys())\n            if not keys:\n                return result\n\n            if parent:\n                has_remote_entries = any(\n                    self.map(\n                        lambda lk, entry: urlparse(\n                            fix_url(_to_singleton(entry.physical_keys))\n                        ).scheme != 'file'\n                    )\n                )\n                pkg_type = 'remote' if has_remote_entries else 'local'\n                result = f'({pkg_type} Package)\\n'\n\n            for key in keys:\n                result += ' ' + ('  ' * level) + '\u2514\u2500' + key + '\\n'\n                result += _create_str(results_dict[key], level + 1, parent=False)\n\n            return result\n\n        if not self.keys():\n            return '(empty Package)'\n\n        # traverse the tree of package directories and entries to get the list of\n        # display objects. candidates is a deque of shape\n        # ((logical_key, Package | PackageEntry), [list of parent key])\n        candidates = deque(([x, []] for x in self._children.items()))\n        results_dict = {}\n        results_total = 0\n        more_objects_than_lines = False\n\n        while candidates:\n            [[logical_key, entry], parent_keys] = candidates.popleft()\n            if isinstance(entry, Package):\n                logical_key = logical_key + '/'\n                new_parent_keys = parent_keys.copy()\n                new_parent_keys.append(logical_key)\n                for child_key in sorted(entry.keys()):\n                    candidates.append([[child_key, entry[child_key]], new_parent_keys])\n\n            current_result_level = results_dict\n            for key in parent_keys:\n                current_result_level = current_result_level[key]\n            current_result_level[logical_key] = {}\n            results_total += 1\n\n            if results_total >= max_lines:\n                more_objects_than_lines = True\n                break\n\n        repr_str = _create_str(results_dict)\n\n        # append '...' if the package is larger than max_size\n        if more_objects_than_lines:\n            repr_str += ' ' + '...\\n'\n\n        return repr_str\n\n    @property\n    def meta(self):\n        return self._meta.get('user_meta', dict())\n\n    @classmethod\n    def install(cls, name, registry=None, top_hash=None, dest=None, dest_registry=None):\n        \"\"\"\n        Installs a named package to the local registry and downloads its files.\n\n        Args:\n            name(str): Name of package to install.\n            registry(str): Registry where package is located. \n                Defaults to the default remote registry.\n            top_hash(str): Hash of package to install. Defaults to latest.\n            dest(str): Local path to download files to.\n            dest_registry(str): Registry to install package to. Defaults to local registry.\n\n        Returns:\n            A new Package that points to files on your local machine.\n        \"\"\"\n        if registry is None:\n            registry = get_from_config('default_remote_registry')\n            if registry is None:\n                raise QuiltException(\n                    \"No registry specified and no default_remote_registry configured. Please \"\n                    \"specify a registry or configure a default remote registry with quilt.config\"\n                )\n\n        if dest_registry is None:\n            dest_registry = get_from_config('default_local_registry')\n        else:\n            dest_registry_parsed = urlparse(fix_url(dest_registry))\n            if dest_registry_parsed.scheme != 'file':\n                raise QuiltException(\n                    f\"Can only 'install' to a local registry, but 'dest_registry' \"\n                    f\"{dest_registry!r} is a remote path. To store a package in a remote \"\n                    f\"registry, use 'push' or 'build' instead.\"\n                )\n\n        if dest is None:\n            dest = get_install_location().rstrip('/') + '/' + quote(name)\n        else:\n            dest_parsed = urlparse(fix_url(dest))\n            if dest_parsed.scheme != 'file':\n                raise QuiltException(\n                    f\"Invalid package destination path {dest!r}. 'dest', if set, must point at \"\n                    f\"the local filesystem. To copy a package to a remote registry use 'push' or \"\n                    f\"'build' instead.\"\n                )\n\n        pkg = cls.browse(name=name, registry=registry, top_hash=top_hash)\n        dest = fix_url(dest)\n        message = pkg._meta.get('message', None)  # propagate the package message\n\n        pkg = pkg._materialize(dest)\n        pkg.build(name, registry=dest_registry, message=message)\n        return pkg\n\n    @classmethod\n    def browse(cls, name=None, registry=None, top_hash=None):\n        \"\"\"\n        Load a package into memory from a registry without making a local copy of\n        the manifest.\n        Args:\n            name(string): name of package to load\n            registry(string): location of registry to load package from\n            top_hash(string): top hash of package version to load\n        \"\"\"\n        if registry is None:\n            registry = get_from_config('default_local_registry')\n\n        registry = registry.rstrip('/')\n        validate_package_name(name)\n        name = quote(name)\n\n        if top_hash is not None:\n            # TODO: verify that name is correct with respect to this top_hash\n            # TODO: allow partial hashes (e.g. first six alphanumeric)\n            pkg_manifest_uri = fix_url(f'{registry}/.quilt/packages/{top_hash}')\n            return cls._from_path(pkg_manifest_uri)\n        else:\n            pkg_timestamp_file = f'{registry}/.quilt/named_packages/{name}/latest'\n            latest_pkg_hash = get_bytes(pkg_timestamp_file).decode('utf-8').strip()\n            pkg_manifest_uri = fix_url(f'{registry}/.quilt/packages/{quote(latest_pkg_hash)}')\n            return cls._from_path(pkg_manifest_uri)\n\n\n    @classmethod\n    def _from_path(cls, uri):\n        \"\"\" Takes a URI and returns a package loaded from that URI \"\"\"\n        src_url = urlparse(uri)\n        if src_url.scheme == 'file':\n            with open(parse_file_url(src_url)) as open_file:\n                pkg = cls.load(open_file)\n        elif src_url.scheme == 's3':\n            body = get_bytes(uri)\n            pkg = cls.load(io.BytesIO(body))\n        else:\n            raise NotImplementedError\n        return pkg\n\n    @classmethod\n    def _split_key(cls, logical_key):\n        \"\"\"\n        Converts a string logical key like 'a/b/c' into a list of ['a', 'b', 'c'].\n        Returns the original key if it's already a list or a tuple.\n        \"\"\"\n        if isinstance(logical_key, str):\n            path = logical_key.split('/')\n        elif isinstance(logical_key, (tuple, list)):\n            path = logical_key\n        else:\n            raise TypeError('Invalid logical_key: %r' % logical_key)\n        return path\n\n    def __contains__(self, logical_key):\n        \"\"\"\n        Checks whether the package contains a specified logical_key.\n\n        Returns:\n            True or False\n        \"\"\"\n        try:\n            self[logical_key]\n            return True\n        except KeyError:\n            return False\n\n    def __getitem__(self, logical_key):\n        \"\"\"\n        Filters the package based on prefix, and returns either a new Package\n            or a PackageEntry.\n\n        Args:\n            prefix(str): prefix to filter on\n\n        Returns:\n            PackageEntry if prefix matches a logical_key exactly\n            otherwise Package\n        \"\"\"\n        pkg = self\n        for key_fragment in self._split_key(logical_key):\n            pkg = pkg._children[key_fragment]\n        return pkg\n\n    def fetch(self, dest='./'):\n        \"\"\"\n        Copy all descendants to `dest`. Descendants are written under their logical\n        names _relative_ to self.\n\n        Args:\n            dest: where to put the files (locally)\n\n        Returns:\n            None\n        \"\"\"\n        nice_dest = fix_url(dest).rstrip('/')\n        file_list = []\n        pkg = Package()\n\n        for logical_key, entry in self.walk():\n            physical_key = _to_singleton(entry.physical_keys)\n            new_physical_key = f'{nice_dest}/{quote(logical_key)}'\n\n            file_list.append((physical_key, new_physical_key, entry.size))\n\n            # return a package reroot package physical keys after the copy operation succeeds\n            # see GH#388 for context\n            new_entry = entry._clone()\n            new_entry.physical_keys = [new_physical_key]\n            pkg.set(logical_key, new_entry)\n\n        copy_file_list(file_list)\n\n        return pkg\n\n    def keys(self):\n        \"\"\"\n        Returns logical keys in the package.\n        \"\"\"\n        return self._children.keys()\n\n    def __iter__(self):\n        return iter(self._children)\n\n    def __len__(self):\n        return len(self._children)\n\n    def walk(self):\n        \"\"\"\n        Generator that traverses all entries in the package tree and returns tuples of (key, entry),\n        with keys in alphabetical order.\n        \"\"\"\n        for name, child in sorted(self._children.items()):\n            if isinstance(child, PackageEntry):\n                yield name, child\n            else:\n                for key, value in child.walk():\n                    yield name + '/' + key, value\n\n    def _walk_dir_meta(self):\n        \"\"\"\n        Generator that traverses all entries in the package tree and returns\n            tuples of (key, meta) for each directory with metadata.\n        Keys will all end in '/' to indicate that they are directories.\n        \"\"\"\n        for key, child in sorted(self._children.items()):\n            if isinstance(child, PackageEntry):\n                continue\n            meta = child.meta\n            if meta:\n                yield key + '/', meta\n            for child_key, child_meta in child._walk_dir_meta():\n                yield key + '/' + child_key, child_meta\n\n    @classmethod\n    def load(cls, readable_file):\n        \"\"\"\n        Loads a package from a readable file-like object.\n\n        Args:\n            readable_file: readable file-like object to deserialize package from\n\n        Returns:\n            A new Package object\n\n        Raises:\n            file not found\n            json decode error\n            invalid package exception\n        \"\"\"\n        reader = jsonlines.Reader(readable_file)\n        meta = reader.read()\n        meta.pop('top_hash', None)  # Obsolete as of PR #130\n        pkg = cls()\n        pkg._meta = meta\n        for obj in reader:\n            path = cls._split_key(obj.pop('logical_key'))\n            subpkg = pkg._ensure_subpackage(path[:-1])\n            key = path[-1]\n            if not obj.get('physical_keys', None):\n                # directory-level metadata\n                subpkg.set_meta(obj['meta'])\n                continue\n            if key in subpkg._children:\n                raise PackageException(\"Duplicate logical key while loading package\")\n            subpkg._children[key] = PackageEntry(\n                obj['physical_keys'],\n                obj['size'],\n                obj['hash'],\n                obj['meta']\n            )\n\n        return pkg\n\n    def set_dir(self, lkey, path=None, meta=None):\n        \"\"\"\n        Adds all files from `path` to the package.\n\n        Recursively enumerates every file in `path`, and adds them to\n            the package according to their relative location to `path`.\n\n        Args:\n            lkey(string): prefix to add to every logical key,\n                use '/' for the root of the package.\n            path(string): path to scan for files to add to package.\n                If None, lkey will be substituted in as the path.\n            meta(dict): user level metadata dict to attach to lkey directory entry.\n\n        Returns:\n            self\n\n        Raises:\n            When `path` doesn't exist\n        \"\"\"\n        lkey = lkey.strip(\"/\")\n\n        if not lkey or lkey == '.' or lkey == './':\n            root = self\n        else:\n            validate_key(lkey)\n            root = self._ensure_subpackage(self._split_key(lkey))\n\n        root.set_meta(meta)\n\n        if not path:\n            current_working_dir = pathlib.Path.cwd()\n            logical_key_abs_path = pathlib.Path(lkey).absolute()\n            path = logical_key_abs_path.relative_to(current_working_dir)\n\n        # TODO: deserialization metadata\n        url = urlparse(fix_url(path).strip('/'))\n        if url.scheme == 'file':\n            src_path = pathlib.Path(parse_file_url(url))\n            if not src_path.is_dir():\n                raise PackageException(\"The specified directory doesn't exist\")\n\n            files = src_path.rglob('*')\n            ignore = src_path / '.quiltignore'\n            if ignore.exists():\n                files = quiltignore_filter(files, ignore, 'file')\n\n            for f in files:\n                if not f.is_file():\n                    continue\n                entry = PackageEntry([f.as_uri()], f.stat().st_size, None, None)\n                logical_key = f.relative_to(src_path).as_posix()\n                root.set(logical_key, entry)\n        elif url.scheme == 's3':\n            src_bucket, src_key, src_version = parse_s3_url(url)\n            if src_version:\n                raise PackageException(\"Directories cannot have versions\")\n            if src_key and not src_key.endswith('/'):\n                src_key += '/'\n            objects, _ = list_object_versions(src_bucket, src_key)\n            for obj in objects:\n                if not obj['IsLatest']:\n                    continue\n                # Skip S3 pseduo directory files and Keys that end in /\n                if obj['Key'].endswith('/'):\n                    if obj['Size'] != 0:\n                        warnings.warn(f'Logical keys cannot end in \"/\", skipping: {obj[\"Key\"]}')\n                    continue\n                obj_url = make_s3_url(src_bucket, obj['Key'], obj.get('VersionId'))\n                entry = PackageEntry([obj_url], obj['Size'], None, None)\n                logical_key = obj['Key'][len(src_key):]\n                root.set(logical_key, entry)\n        else:\n            raise NotImplementedError\n\n        return self\n\n    def get(self, logical_key):\n        \"\"\"\n        Gets object from logical_key and returns its physical path.\n        Equivalent to self[logical_key].get().\n\n        Args:\n            logical_key(string): logical key of the object to get\n\n        Returns:\n            Physical path as a string.\n\n        Raises:\n            KeyError: when logical_key is not present in the package\n            ValueError: if the logical_key points to a Package rather than PackageEntry.\n        \"\"\"\n        obj = self[logical_key]\n        if not isinstance(obj, PackageEntry):\n            raise ValueError(\"Key does not point to a PackageEntry\")\n        return obj.get()\n\n    # def get_as_pathlib(self):\n\n    def set_meta(self, meta):\n        \"\"\"\n        Sets user metadata on this Package.\n        \"\"\"\n        self._meta['user_meta'] = meta\n        return self\n\n    def _fix_sha256(self):\n        entries = [entry for key, entry in self.walk() if entry.hash is None]\n        if not entries:\n            return\n\n        physical_keys = []\n        sizes = []\n        for entry in entries:\n            physical_keys.append(entry.physical_keys[0])\n            sizes.append(entry.size)\n\n        results = calculate_sha256(physical_keys, sizes)\n\n        for entry, obj_hash in zip(entries, results):\n            entry.hash = dict(type='SHA256', value=obj_hash)\n\n    def _set_commit_message(self, msg):\n        \"\"\"\n        Sets a commit message.\n\n        Args:\n            msg: a message string\n\n        Returns:\n            None\n\n        Raises:\n            a ValueError if msg is not a string\n        \"\"\"\n        if msg is not None and not isinstance(msg, str):\n            raise ValueError(\n                f\"The package commit message must be a string, but the message provided is an \"\n                f\"instance of {type(msg)}.\"\n            )\n\n        self._meta.update({'message': msg})\n\n    def build(self, name=None, registry=None, message=None):\n        \"\"\"\n        Serializes this package to a registry.\n\n        Args:\n            name: optional name for package\n            registry: registry to build to\n                    defaults to local registry\n            message: the commit message of the package\n\n        Returns:\n            The top hash as a string.\n        \"\"\"\n        self._set_commit_message(message)\n\n        if registry is None:\n            registry = get_from_config('default_local_registry')\n\n        registry = registry.rstrip('/')\n        validate_package_name(name)\n        name = quote(name)\n\n        self._fix_sha256()\n        manifest = io.BytesIO()\n        self.dump(manifest)\n\n        pkg_manifest_file = f'{registry}/.quilt/packages/{self.top_hash}'\n        put_bytes(\n            manifest.getvalue(),\n            pkg_manifest_file\n        )\n\n        named_path = f'{registry}/.quilt/named_packages/{name}/'\n        # TODO: use a float to string formater instead of double casting\n        hash_bytes = self.top_hash.encode('utf-8')\n        timestamp_path = named_path + str(int(time.time()))\n        latest_path = named_path + \"latest\"\n        put_bytes(hash_bytes, timestamp_path)\n        put_bytes(hash_bytes, latest_path)\n\n        return self\n\n    def dump(self, writable_file):\n        \"\"\"\n        Serializes this package to a writable file-like object.\n\n        Args:\n            writable_file: file-like object to write serialized package.\n\n        Returns:\n            None\n\n        Raises:\n            fail to create file\n            fail to finish write\n        \"\"\"\n        writer = jsonlines.Writer(writable_file)\n        for line in self.manifest:\n            writer.write(line)\n\n    @property\n    def manifest(self):\n        \"\"\"\n        Provides a generator of the dicts that make up the serialized package.\n        \"\"\"\n        yield self._meta\n        for dir_key, meta in self._walk_dir_meta():\n            yield {'logical_key': dir_key, 'meta': meta}\n        for logical_key, entry in self.walk():\n            yield {'logical_key': logical_key, **entry.as_dict()}\n\n    def set(self, logical_key, entry=None, meta=None, serialization_location=None, serialization_format_opts=None):\n        \"\"\"\n        Returns self with the object at logical_key set to entry.\n\n        Args:\n            logical_key(string): logical key to update\n            entry(PackageEntry OR string OR object): new entry to place at logical_key in the package.\n                If entry is a string, it is treated as a URL, and an entry is created based on it.\n                If entry is None, the logical key string will be substituted as the entry value.\n                If entry is an object and quilt knows how to serialize it, it will immediately be serialized and written\n                to disk, either to serialization_location or to a location managed by quilt. List of types that Quilt\n                can serialize is available by calling `quilt3.formats.FormatRegistry.all_supported_formats()`\n            meta(dict): user level metadata dict to attach to entry\n            serialization_format_opts(dict): Optional. If passed in, only used if entry is an object. Options to help\n                Quilt understand how the object should be serialized. Useful for underspecified file formats like csv\n                when content contains confusing characters. Will be passed as kwargs to the FormatHandler.serialize()\n                function. See docstrings for individual FormatHandlers for full list of options -\n                https://github.com/quiltdata/quilt/blob/master/api/python/quilt3/formats.py\n            serialization_location(string): Optional. If passed in, only used if entry is an object. Where the\n                serialized object should be written, e.g. \"./mydataframe.parquet\"\n\n        Returns:\n            self\n        \"\"\"\n        if not logical_key or logical_key.endswith('/'):\n            raise QuiltException(\n                f\"Invalid logical key {logical_key!r}. \"\n                f\"A package entry logical key cannot be a directory.\"\n            )\n\n        validate_key(logical_key)\n\n        if entry is None:\n            current_working_dir = pathlib.Path.cwd()\n            logical_key_abs_path = pathlib.Path(logical_key).absolute()\n            entry = logical_key_abs_path.relative_to(current_working_dir)\n\n        if isinstance(entry, (str, os.PathLike)):\n            url = fix_url(str(entry))\n            size, version = get_size_and_version(url)\n\n            # Determine if a new version needs to be appended.\n            parsed_url = urlparse(url)\n            if parsed_url.scheme == 's3':\n                bucket, key, current_version = parse_s3_url(parsed_url)\n                if not current_version and version:\n                    url = make_s3_url(bucket, key, version)\n            entry = PackageEntry([url], size, None, None)\n        elif isinstance(entry, PackageEntry):\n            entry = entry._clone()\n\n        elif FormatRegistry.object_is_serializable(entry):\n            # Use file extension from serialization_location, fall back to file extension from logical_key\n            # If neither has a file extension, Quilt picks the serialization format.\n            logical_key_ext = extract_file_extension(logical_key)\n\n            serialize_loc_ext = None\n            if serialization_location is not None:\n                serialize_loc_ext = extract_file_extension(serialization_location)\n\n            if logical_key_ext is not None and serialize_loc_ext is not None:\n                assert logical_key_ext == serialize_loc_ext, f\"The logical_key and the serialization_location have \" \\\n                                                             f\"different file extensions: {logical_key_ext} vs \" \\\n                                                             f\"{serialize_loc_ext}. Quilt doesn't know which to use!\"\n\n            if serialize_loc_ext is not None:\n                ext = serialize_loc_ext\n            elif logical_key_ext is not None:\n                ext = logical_key_ext\n            else:\n                ext = None\n\n            format_handlers = FormatRegistry.search(type(entry))\n            if ext:\n                format_handlers = [f for f in format_handlers if ext in f.handled_extensions]\n\n            if len(format_handlers) == 0:\n                error_message = f'Quilt does not know how to serialize a {type(entry)}'\n                if ext is not None:\n                    error_message += f' as a {ext} file.'\n                error_message += f'. If you think this should be supported, please open an issue or PR at ' \\\n                                 f'https://github.com/quiltdata/quilt'\n                raise QuiltException(error_message)\n\n            if serialization_format_opts is None:\n                serialization_format_opts = {}\n            serialized_object_bytes, new_meta = format_handlers[0].serialize(entry, meta=None, ext=ext,\n                                                                             **serialization_format_opts)\n            if serialization_location is None:\n                serialization_path = APP_DIR_TEMPFILE_DIR / str(uuid.uuid4())\n                if ext:\n                    serialization_path = serialization_path.with_suffix(f'.{ext}')\n            else:\n                serialization_path = pathlib.Path(serialization_location).expanduser().resolve()\n\n            serialization_path.parent.mkdir(exist_ok=True, parents=True)\n            serialization_path.write_bytes(serialized_object_bytes)\n\n            size = serialization_path.stat().st_size\n            write_url = serialization_path.as_uri()\n            entry = PackageEntry([write_url], size, hash_obj=None, meta=new_meta)\n\n        else:\n            raise TypeError(f\"Expected a string for entry, but got an instance of {type(entry)}.\")\n\n        if meta is not None:\n            entry.set_meta(meta)\n\n        path = self._split_key(logical_key)\n\n        pkg = self._ensure_subpackage(path[:-1], ensure_no_entry=True)\n        if path[-1] in pkg and isinstance(pkg[path[-1]], Package):\n            raise QuiltException(\"Cannot overwrite directory with PackageEntry\")\n        pkg._children[path[-1]] = entry\n\n        return self\n\n    def _ensure_subpackage(self, path, ensure_no_entry=False):\n        \"\"\"\n        Creates a package and any intermediate packages at the given path.\n\n        Args:\n            path(list): logical key as a list or tuple\n            ensure_no_entry(boolean): if True, throws if this would overwrite\n                a PackageEntry that already exists in the tree.\n\n        Returns:\n            newly created or existing package at that path\n        \"\"\"\n        pkg = self\n        for key_fragment in path:\n            if ensure_no_entry and key_fragment in pkg \\\n                    and isinstance(pkg[key_fragment], PackageEntry):\n                raise QuiltException(\"Already a PackageEntry along the path.\")\n            pkg = pkg._children.setdefault(key_fragment, Package())\n        return pkg\n\n    def delete(self, logical_key):\n        \"\"\"\n        Returns the package with logical_key removed.\n\n        Returns:\n            self\n\n        Raises:\n            KeyError: when logical_key is not present to be deleted\n        \"\"\"\n        path = self._split_key(logical_key)\n        pkg = self[path[:-1]]\n        del pkg._children[path[-1]]\n        return self\n\n    @property\n    def top_hash(self):\n        \"\"\"\n        Returns the top hash of the package.\n\n        Note that physical keys are not hashed because the package has\n            the same semantics regardless of where the bytes come from.\n\n        Returns:\n            A string that represents the top hash of the package\n        \"\"\"\n        top_hash = hashlib.sha256()\n        assert 'top_hash' not in self._meta\n        top_meta = json.dumps(self._meta, sort_keys=True, separators=(',', ':'))\n        top_hash.update(top_meta.encode('utf-8'))\n        for logical_key, entry in self.walk():\n            if entry.hash is None or entry.size is None:\n                raise QuiltException(\n                    \"PackageEntry missing hash and/or size: %s\" % entry.physical_keys[0]\n                )\n            entry_dict = entry.as_dict()\n            entry_dict['logical_key'] = logical_key\n            entry_dict.pop('physical_keys', None)\n            entry_dict_str = json.dumps(entry_dict, sort_keys=True, separators=(',', ':'))\n            top_hash.update(entry_dict_str.encode('utf-8'))\n\n        return top_hash.hexdigest()\n\n    def push(self, name, registry=None, dest=None, message=None):\n        \"\"\"\n        Copies objects to path, then creates a new package that points to those objects.\n        Copies each object in this package to path according to logical key structure,\n        then adds to the registry a serialized version of this package with\n        physical keys that point to the new copies.\n\n        Args:\n            name: name for package in registry\n            dest: where to copy the objects in the package\n            registry: registry where to create the new package\n            message: the commit message for the new package\n\n        Returns:\n            A new package that points to the copied objects.\n        \"\"\"\n        validate_package_name(name)\n\n        if registry is None:\n            registry = get_from_config('default_remote_registry')\n            if not registry:\n                raise QuiltException(\n                    \"No registry specified and no default remote registry configured. Please \"\n                    \"specify a registry or configure a default remote registry with quilt3.config\"\n                )\n            registry_parsed = urlparse(fix_url(registry))\n        else:\n            registry_parsed = urlparse(fix_url(registry))\n            if registry_parsed.scheme == 's3':\n                bucket, path, _ = parse_s3_url(registry_parsed)\n                if path != '':  # parse_s3_url returns path == '' if input is pathless\n                    raise QuiltException(\n                        f\"The 'registry' argument expects an S3 bucket but the S3 object path \"\n                        f\"{registry!r} was provided instead. You probably wanted to set \"\n                        f\"'registry' to {'s3://' + bucket!r} instead. To specify that package \"\n                        f\"data land in a specific directory use 'dest'.\"\n                    )\n                registry = 's3://' + bucket\n            elif registry_parsed.scheme == 'file':\n                raise QuiltException(\n                    f\"Can only 'push' to remote registries in S3, but {registry!r} \"\n                    f\"is a local file. To store a package in the local registry, use \"\n                    f\"'build' instead.\"\n                )\n            else:\n                raise NotImplementedError\n\n        if dest is None:\n            dest = registry.rstrip('/') + '/' + quote(name)\n        else:\n            dest_parsed = urlparse(fix_url(dest))\n            if dest_parsed.scheme != registry_parsed.scheme:\n                raise QuiltException(\n                    f\"Invalid package destination path {dest!r}. 'dest', if set, must be a path \"\n                    f\"in the {registry!r} package registry specified by 'registry'.\"\n                )\n\n            assert dest_parsed.scheme == 's3'\n            registry_bucket, _, _ = parse_s3_url(registry_parsed)\n            dest_bucket, _, _ = parse_s3_url(dest_parsed)\n            if registry_bucket != dest_bucket:\n                raise QuiltException(\n                    f\"Invalid package destination path {dest!r}. 'dest', if set, must be a path \"\n                    f\"in the {registry!r} package registry specified by 'registry'.\"\n                )\n\n        self._fix_sha256()\n        pkg = self._materialize(dest)\n\n        def physical_key_is_temp_file(pk):\n            if not file_is_local(pk):\n                return False\n            return pathlib.Path(parse_file_url(urlparse(pk))).parent == APP_DIR_TEMPFILE_DIR\n\n        temp_file_logical_keys = [lk for lk, entry in self.walk() if physical_key_is_temp_file(entry.physical_keys[0])]\n        temp_file_physical_keys = [self.get(lk) for lk in temp_file_logical_keys]\n\n        # Now that data has been pushed, delete tmp files created by pkg.set('KEY', obj)\n        with Pool(10) as p:\n            p.map(_delete_local_physical_key, temp_file_physical_keys)\n\n        # Update old package to point to the materialized location of the file since the tempfile no longest exists\n        for lk in temp_file_logical_keys:\n            self.set(lk, pkg[lk])\n\n        pkg.build(name, registry=registry, message=message)\n        return pkg\n\n    def _materialize(self, dest_url):\n        \"\"\"\n        Copies all Package entries to the destination, then creates a new package that points to those objects.\n\n        Copies each object in this package to path according to logical key structure,\n        and returns a package with physical_keys that point to the new copies.\n\n        Args:\n            path: where to copy the objects in the package\n\n        Returns:\n            A new package that points to the copied objects\n\n        Raises:\n            fail to get bytes\n            fail to put bytes\n            fail to put package to registry\n        \"\"\"\n        pkg = self.__class__()\n        pkg._meta = self._meta\n        # Since all that is modified is physical keys, pkg will have the same top hash\n        file_list = []\n        entries = []\n        for logical_key, entry in self.walk():\n            # Copy the datafiles in the package.\n            physical_key = _to_singleton(entry.physical_keys)\n            unversioned_physical_key = physical_key.split('?', 1)[0]\n            new_physical_key = dest_url + \"/\" + quote(logical_key)\n            new_entry = entry._clone()\n            if unversioned_physical_key == new_physical_key:\n                # No need to copy - re-use the original physical key.\n                pkg.set(logical_key, new_entry)\n            else:\n                entries.append((logical_key, new_entry))\n                file_list.append((physical_key, new_physical_key, entry.size))\n\n        results = copy_file_list(file_list)\n\n        for (logical_key, new_entry), versioned_key in zip(entries, results):\n            # Create a new package entry pointing to the new remote key.\n            assert versioned_key is not None\n            new_entry.physical_keys = [versioned_key]\n            pkg.set(logical_key, new_entry)\n        return pkg\n\n\n    def diff(self, other_pkg):\n        \"\"\"\n        Returns three lists -- added, modified, deleted.\n\n        Added: present in other_pkg but not in self.\n        Modified: present in both, but different.\n        Deleted: present in self, but not other_pkg.\n\n        Args:\n            other_pkg: Package to diff \n\n        Returns:\n            added, modified, deleted (all lists of logical keys)\n        \"\"\"\n        deleted = []\n        modified = []\n        other_entries = dict(other_pkg.walk())\n        for lk, entry in self.walk():\n            other_entry = other_entries.pop(lk, None)\n            if other_entry is None:\n                deleted.append(lk)\n            elif entry != other_entry:\n                modified.append(lk)\n\n        added = list(sorted(other_entries))\n\n        return added, modified, deleted\n\n    def map(self, f, include_directories=False):\n        \"\"\"\n        Performs a user-specified operation on each entry in the package.\n\n        Args:\n            f(x, y): function\n                The function to be applied to each package entry.\n                It should take two inputs, a logical key and a PackageEntry.\n            include_directories: bool\n                Whether or not to include directory entries in the map.\n\n        Returns: list\n            The list of results generated by the map.\n        \"\"\"\n        if include_directories:\n            for lk, _ in self._walk_dir_meta():\n                yield f(lk, self[lk.rstrip(\"/\")])\n\n        for lk, entity in self.walk():\n            yield f(lk, entity)\n\n    def filter(self, f, include_directories=False):\n        \"\"\"\n        Applies a user-specified operation to each entry in the package,\n        removing results that evaluate to False from the output.\n\n        Args:\n            f(x, y): function\n                The function to be applied to each package entry.\n                It should take two inputs, a logical key and a PackageEntry.\n                This function should return a boolean.\n            include_directories: bool\n                Whether or not to include directory entries in the map.\n\n        Returns:\n            A new package with entries that evaluated to False removed\n        \"\"\"\n        p = Package()\n\n        excluded_dirs = set()\n        if include_directories:\n            for lk, _ in self._walk_dir_meta():\n                if not f(lk, self[lk.rstrip(\"/\")]):\n                    excluded_dirs.add(lk)\n\n        for lk, entity in self.walk():\n            if (not any(p in excluded_dirs\n                        for p in pathlib.PurePosixPath(lk).parents)\n                    and f(lk, entity)):\n                p.set(lk, entity)\n\n        return p\n", "idx": 2, "id": 18050, "msg": "", "proj": "quiltdata-quilt", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -199,7 +199,7 @@ func NewStressTestDeployment(name, namespace string, extraLabels map[string]stri\n \t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t\tLimits: corev1.ResourceList{\n \t\t\t\t\t\t\t\t\tcorev1.ResourceCPU:    resource.MustParse(\"1\"),\n-\t\t\t\t\t\t\t\t\tcorev1.ResourceMemory: resource.MustParse(\"100M\"),\n+\t\t\t\t\t\t\t\t\tcorev1.ResourceMemory: resource.MustParse(\"150M\"),\n \t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t},\n \t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{", "y": 1, "oldf": "// Copyright 2020 Chaos Mesh Authors.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage fixture\n\nimport (\n\t\"sort\"\n\n\t\"k8s.io/apimachinery/pkg/api/resource\"\n\n\tappsv1 \"k8s.io/api/apps/v1\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/util/intstr\"\n\t\"k8s.io/utils/pointer\"\n\n\t\"github.com/chaos-mesh/chaos-mesh/test/e2e/config\"\n)\n\n// NewCommonNginxPod describe that we use common nginx pod to be tested in our chaos-operator test\nfunc NewCommonNginxPod(name, namespace string) *corev1.Pod {\n\treturn &corev1.Pod{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels: map[string]string{\n\t\t\t\t\"app\": \"nginx\",\n\t\t\t},\n\t\t},\n\t\tSpec: corev1.PodSpec{\n\t\t\tContainers: []corev1.Container{\n\t\t\t\t{\n\t\t\t\t\tImage:           \"nginx:latest\",\n\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\tName:            \"nginx\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewCommonNginxDeployment would create a nginx deployment\nfunc NewCommonNginxDeployment(name, namespace string, replicas int32) *appsv1.Deployment {\n\treturn &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels: map[string]string{\n\t\t\t\t\"app\": \"nginx\",\n\t\t\t},\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: &replicas,\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: map[string]string{\n\t\t\t\t\t\"app\": \"nginx\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tLabels: map[string]string{\n\t\t\t\t\t\t\"app\": \"nginx\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tImage:           \"nginx:latest\",\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tName:            \"nginx\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewTimerDeployment creates a timer deployment\nfunc NewTimerDeployment(name, namespace string) *appsv1.Deployment {\n\treturn &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels: map[string]string{\n\t\t\t\t\"app\": name,\n\t\t\t},\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: pointer.Int32Ptr(1),\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: map[string]string{\n\t\t\t\t\t\"app\": name,\n\t\t\t\t},\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tLabels: map[string]string{\n\t\t\t\t\t\t\"app\": name,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tImage:           config.TestConfig.E2EImage,\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tName:            name,\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/test\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewNetworkTestDeployment creates a deployment for e2e test\nfunc NewNetworkTestDeployment(name, namespace string, extraLabels map[string]string) *appsv1.Deployment {\n\tlabels := map[string]string{\n\t\t\"app\": name,\n\t}\n\tfor key, val := range extraLabels {\n\t\tlabels[key] = val\n\t}\n\treturn &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels:    labels,\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: pointer.Int32Ptr(1),\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: labels,\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tLabels: labels,\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tImage:           config.TestConfig.E2EImage,\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tName:            \"network\",\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/test\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewStressTestDeployment creates a deployment for e2e test\nfunc NewStressTestDeployment(name, namespace string, extraLabels map[string]string) *appsv1.Deployment {\n\tlabels := map[string]string{\n\t\t\"app\": name,\n\t}\n\tfor key, val := range extraLabels {\n\t\tlabels[key] = val\n\t}\n\treturn &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels:    labels,\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: pointer.Int32Ptr(1),\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: labels,\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tLabels: labels,\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tImage:           config.TestConfig.E2EImage,\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tName:            \"stress\",\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/test\"},\n\t\t\t\t\t\t\tResources: corev1.ResourceRequirements{\n\t\t\t\t\t\t\t\tRequests: corev1.ResourceList{\n\t\t\t\t\t\t\t\t\tcorev1.ResourceCPU:    resource.MustParse(\"0\"),\n\t\t\t\t\t\t\t\t\tcorev1.ResourceMemory: resource.MustParse(\"0\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\tLimits: corev1.ResourceList{\n\t\t\t\t\t\t\t\t\tcorev1.ResourceCPU:    resource.MustParse(\"1\"),\n\t\t\t\t\t\t\t\t\tcorev1.ResourceMemory: resource.MustParse(\"100M\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"sys\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/sys\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tVolumes: []corev1.Volume{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"sys\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tHostPath: &corev1.HostPathVolumeSource{\n\t\t\t\t\t\t\t\t\tPath: \"/sys\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewIOTestDeployment creates a deployment for e2e test\nfunc NewIOTestDeployment(name, namespace string) *appsv1.Deployment {\n\treturn &appsv1.Deployment{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tName:      name,\n\t\t\tNamespace: namespace,\n\t\t\tLabels: map[string]string{\n\t\t\t\t\"app\": \"io\",\n\t\t\t},\n\t\t},\n\t\tSpec: appsv1.DeploymentSpec{\n\t\t\tReplicas: pointer.Int32Ptr(1),\n\t\t\tSelector: &metav1.LabelSelector{\n\t\t\t\tMatchLabels: map[string]string{\n\t\t\t\t\t\"app\": \"io\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tTemplate: corev1.PodTemplateSpec{\n\t\t\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\t\t\tLabels: map[string]string{\n\t\t\t\t\t\t\"app\": \"io\",\n\t\t\t\t\t},\n\t\t\t\t\tAnnotations: map[string]string{\n\t\t\t\t\t\t\"admission-webhook.chaos-mesh.org/request\": \"chaosfs-io\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tSpec: corev1.PodSpec{\n\t\t\t\t\tContainers: []corev1.Container{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tImage:           config.TestConfig.E2EImage,\n\t\t\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t\t\tName:            \"io\",\n\t\t\t\t\t\t\tCommand:         []string{\"/bin/test\"},\n\t\t\t\t\t\t\tVolumeMounts: []corev1.VolumeMount{\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tName:      \"datadir\",\n\t\t\t\t\t\t\t\t\tMountPath: \"/var/run/data\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tVolumes: []corev1.Volume{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName: \"datadir\",\n\t\t\t\t\t\t\tVolumeSource: corev1.VolumeSource{\n\t\t\t\t\t\t\t\tEmptyDir: &corev1.EmptyDirVolumeSource{},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// NewE2EService creates a service for the E2E helper deployment\nfunc NewE2EService(name, namespace string) *corev1.Service {\n\treturn &corev1.Service{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tNamespace: namespace,\n\t\t\tName:      name,\n\t\t},\n\t\tSpec: corev1.ServiceSpec{\n\t\t\tType: corev1.ServiceTypeNodePort,\n\t\t\tSelector: map[string]string{\n\t\t\t\t\"app\": name,\n\t\t\t},\n\t\t\tPorts: []corev1.ServicePort{\n\t\t\t\t{\n\t\t\t\t\tName:       \"http\",\n\t\t\t\t\tPort:       8080,\n\t\t\t\t\tTargetPort: intstr.IntOrString{IntVal: 8080},\n\t\t\t\t},\n\t\t\t\t// Only used in network chaos\n\t\t\t\t{\n\t\t\t\t\tName:       \"nc-port\",\n\t\t\t\t\tPort:       1070,\n\t\t\t\t\tTargetPort: intstr.IntOrString{IntVal: 8000},\n\t\t\t\t},\n\t\t\t\t// Only used in io chaos\n\t\t\t\t{\n\t\t\t\t\tName:       \"chaosfs\",\n\t\t\t\t\tPort:       65534,\n\t\t\t\t\tTargetPort: intstr.IntOrString{IntVal: 65534},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// HaveSameUIDs returns if pods1 and pods2 are same based on their UIDs\nfunc HaveSameUIDs(pods1 []corev1.Pod, pods2 []corev1.Pod) bool {\n\tcount := len(pods1)\n\tif count != len(pods2) {\n\t\treturn false\n\t}\n\tids1, ids2 := make([]string, count), make([]string, count)\n\tfor i := 0; i < count; i++ {\n\t\tids1[i], ids2[i] = string(pods1[i].UID), string(pods2[i].UID)\n\t}\n\tsort.Strings(ids1)\n\tsort.Strings(ids2)\n\tfor i := 0; i < count; i++ {\n\t\tif ids1[i] != ids2[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n", "idx": 1, "id": 21304, "msg": "Why do we need to increase that limit?", "proj": "chaos-mesh-chaos-mesh", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -2399,12 +2399,15 @@ def ls(obj=None,  # type: Union[str, Packet, Packet_metaclass]\n                   \"layers using a clear GUI\")\n     else:\n         try:\n-            fields = _pkt_ls(obj, verbose=verbose)\n+            fields = _pkt_ls(\n+                obj,  # type: ignore\n+                verbose=verbose\n+            )\n             is_pkt = isinstance(obj, Packet)\n             # Print\n             for fname, cls, clsne, dflt, long_attrs in fields:\n-                cls = cls.__name__ + \" \" + clsne\n-                print(\"%-10s : %-35s =\" % (fname, cls), end=' ')\n+                clsinfo = cls.__name__ + \" \" + clsne\n+                print(\"%-10s : %-35s =\" % (fname, clsinfo), end=' ')\n                 if is_pkt:\n                     print(\"%-15r\" % (getattr(obj, fname),), end=' ')\n                 print(\"(%r)\" % (dflt,))", "y": 0, "oldf": "# This file is part of Scapy\n# See http://www.secdev.org/projects/scapy for more information\n# Copyright (C) Philippe Biondi <phil@secdev.org>\n# This program is published under a GPLv2 license\n\n\"\"\"\nPacket class\n\nProvides:\n - the default Packet classes\n - binding mechanisms\n - fuzz() method\n - exploration methods: explore() / ls()\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom collections import defaultdict\nimport re\nimport time\nimport itertools\nimport copy\nimport types\nimport warnings\n\nfrom scapy.fields import StrField, ConditionalField, Emph, PacketListField, \\\n    BitField, MultiEnumField, EnumField, FlagsField, MultipleTypeField, Field\nfrom scapy.config import conf, _version_checker\nfrom scapy.compat import raw, orb, bytes_encode\nfrom scapy.base_classes import BasePacket, Gen, SetGen, Packet_metaclass, \\\n    _CanvasDumpExtended, Field_metaclass\nfrom scapy.volatile import RandField, VolatileValue\nfrom scapy.utils import import_hexcap, tex_escape, colgen, issubtype, \\\n    pretty_list, EDecimal\nfrom scapy.error import Scapy_Exception, log_runtime, warning\nfrom scapy.extlib import PYX\nimport scapy.modules.six as six\n\nfrom scapy.compat import (\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    NoReturn,\n    Optional,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\ntry:\n    import pyx\nexcept ImportError:\n    pass\n\n\nclass RawVal:\n    def __init__(self, val=\"\"):\n        # type: (str) -> None\n        self.val = val\n\n    def __str__(self):\n        # type: () -> str\n        return str(self.val)\n\n    def __bytes__(self):\n        # type: () -> bytes\n        return bytes_encode(self.val)\n\n    def __repr__(self):\n        # type: () -> str\n        return \"<RawVal [%r]>\" % self.val\n\n\n_T = TypeVar(\"_T\", Dict[str, Any], Optional[Dict[str, Any]])\n\n\n# six.with_metaclass typing is glitchy\nclass Packet(six.with_metaclass(Packet_metaclass,  # type: ignore\n             BasePacket, _CanvasDumpExtended)):\n    __slots__ = [\n        \"time\", \"sent_time\", \"name\",\n        \"default_fields\", \"fields\", \"fieldtype\",\n        \"overload_fields\", \"overloaded_fields\",\n        \"packetfields\",\n        \"original\", \"explicit\", \"raw_packet_cache\",\n        \"raw_packet_cache_fields\", \"_pkt\", \"post_transforms\",\n        # then payload and underlayer\n        \"payload\", \"underlayer\",\n        \"name\",\n        # used for sr()\n        \"_answered\",\n        # used when sniffing\n        \"direction\", \"sniffed_on\",\n        # handle snaplen Vs real length\n        \"wirelen\",\n    ]\n    name = None\n    fields_desc = []  # type: List[Field[Any, Any]]\n    deprecated_fields = {}  # type: Dict[str, Tuple[str, str]]\n    overload_fields = {}  # type: Dict[Packet_metaclass, Dict[str, Any]]\n    payload_guess = []  # type: List[Tuple[Dict[str, Any], Packet_metaclass]]\n    show_indent = 1\n    show_summary = True\n    match_subclass = False\n    class_dont_cache = {}  # type: Dict[Packet_metaclass, bool]\n    class_packetfields = {}  # type: Dict[Packet_metaclass, Any]\n    class_default_fields = {}  # type: Dict[Packet_metaclass, Dict[str, Any]]\n    class_default_fields_ref = {}  # type: Dict[Packet_metaclass, List[str]]\n    class_fieldtype = {}  # type: Dict[Packet_metaclass, Dict[str, Field[Any, Any]]]  # noqa: E501\n\n    @classmethod\n    def from_hexcap(cls):\n        # type: (Packet_metaclass) -> Packet\n        return cls(import_hexcap())  # type: ignore\n\n    @classmethod\n    def upper_bonds(self):\n        # type: () -> None\n        for fval, upper in self.payload_guess:\n            print(\"%-20s  %s\" % (upper.__name__, \", \".join(\"%-12s\" % (\"%s=%r\" % i) for i in six.iteritems(fval))))  # noqa: E501\n\n    @classmethod\n    def lower_bonds(self):\n        # type: () -> None\n        for lower, fval in six.iteritems(self._overload_fields):\n            print(\"%-20s  %s\" % (lower.__name__, \", \".join(\"%-12s\" % (\"%s=%r\" % i) for i in six.iteritems(fval))))  # noqa: E501\n\n    def __init__(self,\n                 _pkt=b\"\",  # type: bytes\n                 post_transform=None,  # type: Any\n                 _internal=0,  # type: int\n                 _underlayer=None,  # type: Optional[Packet]\n                 **fields  # type: Any\n                 ):\n        # type: (...) -> None\n        self.time = time.time()  # type: Union[EDecimal, float]\n        self.sent_time = None  # type: Union[EDecimal, float, None]\n        self.name = (self.__class__.__name__\n                     if self._name is None else\n                     self._name)\n        self.default_fields = {}  # type: Dict[str, Any]\n        self.overload_fields = self._overload_fields\n        self.overloaded_fields = {}  # type: Dict[str, Any]\n        self.fields = {}  # type: Dict[str, Any]\n        self.fieldtype = {}  # type: Dict[str, Field[Any, Any]]\n        self.packetfields = []  # type: List[Field[Any, Any]]\n        self.payload = NoPayload()\n        self.init_fields()\n        self.underlayer = _underlayer\n        self.original = _pkt\n        self.explicit = 0\n        self.raw_packet_cache = None  # type: Optional[bytes]\n        self.raw_packet_cache_fields = None  # type: Optional[Dict[str, Any]]  # noqa: E501\n        self.wirelen = None  # type: Optional[int]\n        self.direction = None  # type: Optional[int]\n        self.sniffed_on = None  # type: Optional[str]\n        if _pkt:\n            self.dissect(_pkt)\n            if not _internal:\n                self.dissection_done(self)\n        # We use this strange initialization so that the fields\n        # are initialized in their declaration order.\n        # It is required to always support MultipleTypeField\n        for field in self.fields_desc:\n            fname = field.name\n            try:\n                value = fields.pop(fname)\n            except KeyError:\n                continue\n            self.fields[fname] = self.get_field(fname).any2i(self, value)\n        # The remaining fields are unknown\n        for fname in fields:\n            if fname in self.deprecated_fields:\n                # Resolve deprecated fields\n                value = fields[fname]\n                fname = self._resolve_alias(fname)\n                self.fields[fname] = self.get_field(fname).any2i(self, value)\n                continue\n            raise AttributeError(fname)\n        if isinstance(post_transform, list):\n            self.post_transforms = post_transform\n        elif post_transform is None:\n            self.post_transforms = []\n        else:\n            self.post_transforms = [post_transform]\n\n    _PickleType = Tuple[\n        bytes,\n        Union[EDecimal, float],\n        Optional[Union[EDecimal, float, None]],\n        Optional[int],\n        Optional[str],\n        Optional[int]\n    ]\n\n    def __reduce__(self):\n        # type: () -> Tuple[Packet_metaclass, Tuple[()], Packet._PickleType]\n        \"\"\"Used by pickling methods\"\"\"\n        return (self.__class__, (), (\n            self.build(),\n            self.time,\n            self.sent_time,\n            self.direction,\n            self.sniffed_on,\n            self.wirelen,\n        ))\n\n    def __getstate__(self):\n        # type: () -> Packet._PickleType\n        \"\"\"Mark object as pickable\"\"\"\n        return self.__reduce__()[2]\n\n    def __setstate__(self, state):\n        # type: (Packet._PickleType) -> Packet\n        \"\"\"Rebuild state using pickable methods\"\"\"\n        self.__init__(state[0])  # type: ignore\n        self.time = state[1]\n        self.sent_time = state[2]\n        self.direction = state[3]\n        self.sniffed_on = state[4]\n        self.wirelen = state[5]\n        return self\n\n    def __deepcopy__(self,\n                     memo,  # type: Any\n                     ):\n        # type: (...) -> Packet\n        \"\"\"Used by copy.deepcopy\"\"\"\n        return self.copy()\n\n    def init_fields(self):\n        # type: () -> None\n        \"\"\"\n        Initialize each fields of the fields_desc dict\n        \"\"\"\n\n        if self.class_dont_cache.get(self.__class__, False):\n            self.do_init_fields(self.fields_desc)\n        else:\n            self.do_init_cached_fields()\n\n    def do_init_fields(self,\n                       flist,  # type: List[Field[Any, Any]]\n                       ):\n        # type: (...) -> None\n        \"\"\"\n        Initialize each fields of the fields_desc dict\n        \"\"\"\n        default_fields = {}\n        for f in flist:\n            default_fields[f.name] = copy.deepcopy(f.default)\n            self.fieldtype[f.name] = f\n            if f.holds_packets:\n                self.packetfields.append(f)\n        # We set default_fields last to avoid race issues\n        self.default_fields = default_fields\n\n    def do_init_cached_fields(self):\n        # type: () -> None\n        \"\"\"\n        Initialize each fields of the fields_desc dict, or use the cached\n        fields information\n        \"\"\"\n\n        cls_name = self.__class__\n\n        # Build the fields information\n        if Packet.class_default_fields.get(cls_name, None) is None:\n            self.prepare_cached_fields(self.fields_desc)\n\n        # Use fields information from cache\n        default_fields = Packet.class_default_fields.get(cls_name, None)\n        if default_fields:\n            self.default_fields = default_fields\n            self.fieldtype = Packet.class_fieldtype[cls_name]\n            self.packetfields = Packet.class_packetfields[cls_name]\n\n            # Deepcopy default references\n            for fname in Packet.class_default_fields_ref[cls_name]:\n                value = self.default_fields[fname]\n                try:\n                    self.fields[fname] = value.copy()\n                except AttributeError:\n                    # Python 2.7 - list only\n                    self.fields[fname] = value[:]\n\n    def prepare_cached_fields(self, flist):\n        # type: (List[Field[Any, Any]]) -> None\n        \"\"\"\n        Prepare the cached fields of the fields_desc dict\n        \"\"\"\n\n        cls_name = self.__class__\n\n        # Fields cache initialization\n        if not flist:\n            return\n\n        class_default_fields = dict()\n        class_default_fields_ref = list()\n        class_fieldtype = dict()\n        class_packetfields = list()\n\n        # Fields initialization\n        for f in flist:\n            if isinstance(f, MultipleTypeField):\n                # Abort\n                self.class_dont_cache[cls_name] = True\n                self.do_init_fields(self.fields_desc)\n                return\n\n            tmp_copy = copy.deepcopy(f.default)\n            class_default_fields[f.name] = tmp_copy\n            class_fieldtype[f.name] = f\n            if f.holds_packets:\n                class_packetfields.append(f)\n\n            # Remember references\n            if isinstance(f.default, (list, dict, set, RandField, Packet)):\n                class_default_fields_ref.append(f.name)\n\n        # Apply\n        Packet.class_default_fields_ref[cls_name] = class_default_fields_ref\n        Packet.class_fieldtype[cls_name] = class_fieldtype\n        Packet.class_packetfields[cls_name] = class_packetfields\n        # Last to avoid racing issues\n        Packet.class_default_fields[cls_name] = class_default_fields\n\n    def dissection_done(self, pkt):\n        # type: (Packet) -> None\n        \"\"\"DEV: will be called after a dissection is completed\"\"\"\n        self.post_dissection(pkt)\n        self.payload.dissection_done(pkt)\n\n    def post_dissection(self, pkt):\n        # type: (Packet) -> None\n        \"\"\"DEV: is called after the dissection of the whole packet\"\"\"\n        pass\n\n    def get_field(self, fld):\n        # type: (str) -> Field[Any, Any]\n        \"\"\"DEV: returns the field instance from the name of the field\"\"\"\n        return self.fieldtype[fld]\n\n    def add_payload(self, payload):\n        # type: (Union[Packet, bytes]) -> None\n        if payload is None:\n            return\n        elif not isinstance(self.payload, NoPayload):\n            self.payload.add_payload(payload)\n        else:\n            if isinstance(payload, Packet):\n                self.payload = payload\n                payload.add_underlayer(self)\n                for t in self.aliastypes:\n                    if t in payload.overload_fields:\n                        self.overloaded_fields = payload.overload_fields[t]\n                        break\n            elif isinstance(payload, bytes):\n                self.payload = conf.raw_layer(load=payload)\n            else:\n                raise TypeError(\"payload must be either 'Packet' or 'bytes', not [%s]\" % repr(payload))  # noqa: E501\n\n    def remove_payload(self):\n        # type: () -> None\n        self.payload.remove_underlayer(self)\n        self.payload = NoPayload()\n        self.overloaded_fields = {}\n\n    def add_underlayer(self, underlayer):\n        # type: (Packet) -> None\n        self.underlayer = underlayer\n\n    def remove_underlayer(self, other):\n        # type: (Packet) -> None\n        self.underlayer = None\n\n    def copy(self):\n        # type: () -> Packet\n        \"\"\"Returns a deep copy of the instance.\"\"\"\n        clone = self.__class__()\n        clone.fields = self.copy_fields_dict(self.fields)\n        clone.default_fields = self.copy_fields_dict(self.default_fields)\n        clone.overloaded_fields = self.overloaded_fields.copy()\n        clone.underlayer = self.underlayer\n        clone.explicit = self.explicit\n        clone.raw_packet_cache = self.raw_packet_cache\n        clone.raw_packet_cache_fields = self.copy_fields_dict(\n            self.raw_packet_cache_fields\n        )\n        clone.wirelen = self.wirelen\n        clone.post_transforms = self.post_transforms[:]\n        clone.payload = self.payload.copy()\n        clone.payload.add_underlayer(clone)\n        clone.time = self.time\n        return clone\n\n    def _resolve_alias(self, attr):\n        # type: (str) -> str\n        new_attr, version = self.deprecated_fields[attr]\n        warnings.warn(\n            \"%s has been deprecated in favor of %s since %s !\" % (\n                attr, new_attr, version\n            ), DeprecationWarning\n        )\n        return new_attr\n\n    def getfieldval(self, attr):\n        # type: (str) -> Any\n        if self.deprecated_fields and attr in self.deprecated_fields:\n            attr = self._resolve_alias(attr)\n        if attr in self.fields:\n            return self.fields[attr]\n        if attr in self.overloaded_fields:\n            return self.overloaded_fields[attr]\n        if attr in self.default_fields:\n            return self.default_fields[attr]\n        return self.payload.getfieldval(attr)\n\n    def getfield_and_val(self, attr):\n        # type: (str) -> Optional[Tuple[Any, Any]]\n        if self.deprecated_fields and attr in self.deprecated_fields:\n            attr = self._resolve_alias(attr)\n        if attr in self.fields:\n            return self.get_field(attr), self.fields[attr]\n        if attr in self.overloaded_fields:\n            return self.get_field(attr), self.overloaded_fields[attr]\n        if attr in self.default_fields:\n            return self.get_field(attr), self.default_fields[attr]\n        return None\n\n    def __getattr__(self, attr):\n        # type: (str) -> Any\n        try:\n            fld, v = self.getfield_and_val(attr)  # type: ignore\n        except TypeError:\n            return self.payload.__getattr__(attr)\n        if fld is not None:\n            return fld.i2h(self, v)\n        return v\n\n    def setfieldval(self, attr, val):\n        # type: (str, Any) -> None\n        if self.deprecated_fields and attr in self.deprecated_fields:\n            attr = self._resolve_alias(attr)\n        if attr in self.default_fields:\n            fld = self.get_field(attr)\n            if fld is None:\n                any2i = lambda x, y: y  # type: Callable[..., Any]\n            else:\n                any2i = fld.any2i\n            self.fields[attr] = any2i(self, val)\n            self.explicit = 0\n            self.raw_packet_cache = None\n            self.raw_packet_cache_fields = None\n            self.wirelen = None\n        elif attr == \"payload\":\n            self.remove_payload()\n            self.add_payload(val)\n        else:\n            self.payload.setfieldval(attr, val)\n\n    def __setattr__(self, attr, val):\n        # type: (str, Any) -> None\n        if attr in self.__all_slots__:\n            if attr == \"sent_time\":\n                self.update_sent_time(val)\n            return object.__setattr__(self, attr, val)\n        try:\n            return self.setfieldval(attr, val)\n        except AttributeError:\n            pass\n        return object.__setattr__(self, attr, val)\n\n    def delfieldval(self, attr):\n        # type: (str) -> None\n        if attr in self.fields:\n            del(self.fields[attr])\n            self.explicit = 0  # in case a default value must be explicit\n            self.raw_packet_cache = None\n            self.raw_packet_cache_fields = None\n            self.wirelen = None\n        elif attr in self.default_fields:\n            pass\n        elif attr == \"payload\":\n            self.remove_payload()\n        else:\n            self.payload.delfieldval(attr)\n\n    def __delattr__(self, attr):\n        # type: (str) -> None\n        if attr == \"payload\":\n            return self.remove_payload()\n        if attr in self.__all_slots__:\n            return object.__delattr__(self, attr)\n        try:\n            return self.delfieldval(attr)\n        except AttributeError:\n            pass\n        return object.__delattr__(self, attr)\n\n    def _superdir(self):\n        # type: () -> Set[str]\n        \"\"\"\n        Return a list of slots and methods, including those from subclasses.\n        \"\"\"\n        attrs = set()\n        cls = self.__class__\n        if hasattr(cls, '__all_slots__'):\n            attrs.update(cls.__all_slots__)\n        for bcls in cls.__mro__:\n            if hasattr(bcls, '__dict__'):\n                attrs.update(bcls.__dict__)\n        return attrs\n\n    def __dir__(self):\n        # type: () -> List[str]\n        \"\"\"\n        Add fields to tab completion list.\n        \"\"\"\n        return sorted(itertools.chain(self._superdir(), self.default_fields))\n\n    def __repr__(self):\n        # type: () -> str\n        s = \"\"\n        ct = conf.color_theme\n        for f in self.fields_desc:\n            if isinstance(f, ConditionalField) and not f._evalcond(self):\n                continue\n            if f.name in self.fields:\n                fval = self.fields[f.name]\n                if isinstance(fval, (list, dict, set)) and len(fval) == 0:\n                    continue\n                val = f.i2repr(self, fval)\n            elif f.name in self.overloaded_fields:\n                fover = self.overloaded_fields[f.name]\n                if isinstance(fover, (list, dict, set)) and len(fover) == 0:\n                    continue\n                val = f.i2repr(self, fover)\n            else:\n                continue\n            if isinstance(f, Emph) or f in conf.emph:\n                ncol = ct.emph_field_name\n                vcol = ct.emph_field_value\n            else:\n                ncol = ct.field_name\n                vcol = ct.field_value\n\n            s += \" %s%s%s\" % (ncol(f.name),\n                              ct.punct(\"=\"),\n                              vcol(val))\n        return \"%s%s %s %s%s%s\" % (ct.punct(\"<\"),\n                                   ct.layer_name(self.__class__.__name__),\n                                   s,\n                                   ct.punct(\"|\"),\n                                   repr(self.payload),\n                                   ct.punct(\">\"))\n\n    if six.PY2:\n        def __str__(self):\n            # type: () -> str\n            return self.build()\n    else:\n        def __str__(self):\n            # type: () -> str\n            warning(\"Calling str(pkt) on Python 3 makes no sense!\")\n            return str(self.build())\n\n    def __bytes__(self):\n        # type: () -> bytes\n        return self.build()\n\n    def __div__(self, other):\n        # type: (Any) -> Packet\n        if isinstance(other, Packet):\n            cloneA = self.copy()\n            cloneB = other.copy()\n            cloneA.add_payload(cloneB)\n            return cloneA\n        elif isinstance(other, (bytes, str)):\n            return self / conf.raw_layer(load=other)  # type: ignore\n        else:\n            return other.__rdiv__(self)  # type: ignore\n    __truediv__ = __div__\n\n    def __rdiv__(self, other):\n        # type: (Any) -> Packet\n        if isinstance(other, (bytes, str)):\n            return conf.raw_layer(load=other) / self  # type: ignore\n        else:\n            raise TypeError\n    __rtruediv__ = __rdiv__\n\n    def __mul__(self, other):\n        # type: (Any) -> List[Packet]\n        if isinstance(other, int):\n            return [self] * other\n        else:\n            raise TypeError\n\n    def __rmul__(self, other):\n        # type: (Any) -> List[Packet]\n        return self.__mul__(other)\n\n    def __nonzero__(self):\n        # type: () -> bool\n        return True\n    __bool__ = __nonzero__\n\n    def __len__(self):\n        # type: () -> int\n        return len(self.__bytes__())\n\n    def copy_field_value(self, fieldname, value):\n        # type: (str, Any) -> Any\n        return self.get_field(fieldname).do_copy(value)\n\n    def copy_fields_dict(self, fields):\n        # type: (_T) -> _T\n        if fields is None:\n            return None\n        return {fname: self.copy_field_value(fname, fval)\n                for fname, fval in six.iteritems(fields)}\n\n    def clear_cache(self):\n        # type: () -> None\n        \"\"\"Clear the raw packet cache for the field and all its subfields\"\"\"\n        self.raw_packet_cache = None\n        for fld, fval in six.iteritems(self.fields):\n            fld = self.get_field(fld)\n            if fld.holds_packets:\n                if isinstance(fval, Packet):\n                    fval.clear_cache()\n                elif isinstance(fval, list):\n                    for fsubval in fval:\n                        fsubval.clear_cache()\n        self.payload.clear_cache()\n\n    def self_build(self, field_pos_list=None):\n        # type: (Optional[Any])-> bytes\n        \"\"\"\n        Create the default layer regarding fields_desc dict\n\n        :param field_pos_list:\n        \"\"\"\n        if self.raw_packet_cache is not None:\n            for fname, fval in six.iteritems(self.raw_packet_cache_fields):\n                if self.getfieldval(fname) != fval:\n                    self.raw_packet_cache = None\n                    self.raw_packet_cache_fields = None\n                    self.wirelen = None\n                    break\n            if self.raw_packet_cache is not None:\n                return self.raw_packet_cache\n        p = b\"\"\n        for f in self.fields_desc:\n            val = self.getfieldval(f.name)\n            if isinstance(val, RawVal):\n                sval = raw(val)\n                p += sval\n                if field_pos_list is not None:\n                    field_pos_list.append((f.name, sval, len(p), len(sval)))\n            else:\n                p = f.addfield(self, p, val)\n        return p\n\n    def do_build_payload(self):\n        # type: () -> bytes\n        \"\"\"\n        Create the default version of the payload layer\n\n        :return: a string of payload layer\n        \"\"\"\n        return self.payload.do_build()\n\n    def do_build(self):\n        # type: () -> bytes\n        \"\"\"\n        Create the default version of the layer\n\n        :return: a string of the packet with the payload\n        \"\"\"\n        if not self.explicit:\n            self = next(iter(self))\n        pkt = self.self_build()\n        for t in self.post_transforms:\n            pkt = t(pkt)\n        pay = self.do_build_payload()\n        if self.raw_packet_cache is None:\n            return self.post_build(pkt, pay)\n        else:\n            return pkt + pay\n\n    def build_padding(self):\n        # type: () -> bytes\n        return self.payload.build_padding()\n\n    def build(self):\n        # type: () -> bytes\n        \"\"\"\n        Create the current layer\n\n        :return: string of the packet with the payload\n        \"\"\"\n        p = self.do_build()\n        p += self.build_padding()\n        p = self.build_done(p)\n        return p\n\n    def post_build(self, pkt, pay):\n        # type: (bytes, bytes) -> bytes\n        \"\"\"\n        DEV: called right after the current layer is build.\n\n        :param str pkt: the current packet (build by self_buil function)\n        :param str pay: the packet payload (build by do_build_payload function)\n        :return: a string of the packet with the payload\n        \"\"\"\n        return pkt + pay\n\n    def build_done(self, p):\n        # type: (bytes) -> bytes\n        return self.payload.build_done(p)\n\n    def do_build_ps(self):\n        # type: () -> Tuple[bytes, List[Tuple[Packet, List[Tuple[Field[Any, Any], str, bytes]]]]]  # noqa: E501\n        p = b\"\"\n        pl = []\n        q = b\"\"\n        for f in self.fields_desc:\n            if isinstance(f, ConditionalField) and not f._evalcond(self):\n                continue\n            p = f.addfield(self, p, self.getfieldval(f.name))\n            if isinstance(p, bytes):\n                r = p[len(q):]\n                q = p\n            else:\n                r = b\"\"\n            pl.append((f, f.i2repr(self, self.getfieldval(f.name)), r))\n\n        pkt, lst = self.payload.build_ps(internal=1)\n        p += pkt\n        lst.append((self, pl))\n\n        return p, lst\n\n    def build_ps(self, internal=0):\n        # type: (int) -> Tuple[bytes, List[Tuple[Packet, List[Tuple[Any, Any, bytes]]]]]  # noqa: E501\n        p, lst = self.do_build_ps()\n#        if not internal:\n#            pkt = self\n#            while pkt.haslayer(conf.padding_layer):\n#                pkt = pkt.getlayer(conf.padding_layer)\n#                lst.append( (pkt, [ (\"loakjkjd\", pkt.load, pkt.load) ] ) )\n#                p += pkt.load\n#                pkt = pkt.payload\n        return p, lst\n\n    def canvas_dump(self, layer_shift=0, rebuild=1):\n        # type: (int, int) -> pyx.canvas.canvas\n        if PYX == 0:\n            raise ImportError(\"PyX and its dependencies must be installed\")\n        canvas = pyx.canvas.canvas()\n        if rebuild:\n            _, t = self.__class__(raw(self)).build_ps()\n        else:\n            _, t = self.build_ps()\n        YTXTI = len(t)\n        for _, l in t:\n            YTXTI += len(l)\n        YTXT = float(YTXTI)\n        YDUMP = YTXT\n\n        XSTART = 1\n        XDSTART = 10\n        y = 0.0\n        yd = 0.0\n        XMUL = 0.55\n        YMUL = 0.4\n\n        backcolor = colgen(0.6, 0.8, 1.0, trans=pyx.color.rgb)\n        forecolor = colgen(0.2, 0.5, 0.8, trans=pyx.color.rgb)\n#        backcolor=makecol(0.376, 0.729, 0.525, 1.0)\n\n        def hexstr(x):\n            # type: (bytes) -> str\n            return \" \".join(\"%02x\" % orb(c) for c in x)\n\n        def make_dump_txt(x, y, txt):\n            # type: (int, float, bytes) -> pyx.text.text\n            return pyx.text.text(\n                XDSTART + x * XMUL,\n                (YDUMP - y) * YMUL,\n                r\"\\tt{%s}\" % hexstr(txt),\n                [pyx.text.size.Large]\n            )\n\n        def make_box(o):\n            # type: (pyx.bbox.bbox) -> pyx.bbox.bbox\n            return pyx.box.rect(\n                o.left(), o.bottom(), o.width(), o.height(),\n                relcenter=(0.5, 0.5)\n            )\n\n        def make_frame(lst):\n            # type: (List[Any]) -> pyx.path.path\n            if len(lst) == 1:\n                b = lst[0].bbox()\n                b.enlarge(pyx.unit.u_pt)\n                return b.path()\n            else:\n                fb = lst[0].bbox()\n                fb.enlarge(pyx.unit.u_pt)\n                lb = lst[-1].bbox()\n                lb.enlarge(pyx.unit.u_pt)\n                if len(lst) == 2 and fb.left() > lb.right():\n                    return pyx.path.path(pyx.path.moveto(fb.right(), fb.top()),\n                                         pyx.path.lineto(fb.left(), fb.top()),\n                                         pyx.path.lineto(fb.left(), fb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(fb.right(), fb.bottom()),  # noqa: E501\n                                         pyx.path.moveto(lb.left(), lb.top()),\n                                         pyx.path.lineto(lb.right(), lb.top()),\n                                         pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(lb.left(), lb.bottom()))  # noqa: E501\n                else:\n                    # XXX\n                    gb = lst[1].bbox()\n                    if gb != lb:\n                        gb.enlarge(pyx.unit.u_pt)\n                    kb = lst[-2].bbox()\n                    if kb != gb and kb != lb:\n                        kb.enlarge(pyx.unit.u_pt)\n                    return pyx.path.path(pyx.path.moveto(fb.left(), fb.top()),\n                                         pyx.path.lineto(fb.right(), fb.top()),\n                                         pyx.path.lineto(fb.right(), kb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(lb.right(), kb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(lb.right(), lb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(lb.left(), lb.bottom()),  # noqa: E501\n                                         pyx.path.lineto(lb.left(), gb.top()),\n                                         pyx.path.lineto(fb.left(), gb.top()),\n                                         pyx.path.closepath(),)\n\n        def make_dump(s,   # type: bytes\n                      shift=0,  # type: int\n                      y=0.,  # type: float\n                      col=None,  # type: pyx.color.color\n                      bkcol=None,  # type: pyx.color.color\n                      large=16  # type: int\n                      ):\n            # type: (...) -> Tuple[pyx.canvas.canvas, pyx.bbox.bbox, int, float]  # noqa: E501\n            c = pyx.canvas.canvas()\n            tlist = []\n            while s:\n                dmp, s = s[:large - shift], s[large - shift:]\n                txt = make_dump_txt(shift, y, dmp)\n                tlist.append(txt)\n                shift += len(dmp)\n                if shift >= 16:\n                    shift = 0\n                    y += 1\n            if col is None:\n                col = pyx.color.rgb.red\n            if bkcol is None:\n                bkcol = pyx.color.rgb.white\n            c.stroke(make_frame(tlist), [col, pyx.deco.filled([bkcol]), pyx.style.linewidth.Thick])  # noqa: E501\n            for txt in tlist:\n                c.insert(txt)\n            return c, tlist[-1].bbox(), shift, y\n\n        last_shift, last_y = 0, 0.0\n        while t:\n            bkcol = next(backcolor)\n            proto, fields = t.pop()\n            y += 0.5\n            pt = pyx.text.text(\n                XSTART,\n                (YTXT - y) * YMUL,\n                r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(\n                    str(proto.name)\n                ),\n                [pyx.text.size.Large]\n            )\n            y += 1\n            ptbb = pt.bbox()\n            ptbb.enlarge(pyx.unit.u_pt * 2)\n            canvas.stroke(ptbb.path(), [pyx.color.rgb.black, pyx.deco.filled([bkcol])])  # noqa: E501\n            canvas.insert(pt)\n            for field, fval, fdump in fields:\n                col = next(forecolor)\n                ft = pyx.text.text(XSTART, (YTXT - y) * YMUL, r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(field.name))  # noqa: E501\n                if isinstance(field, BitField):\n                    fsize = '%sb' % field.size\n                else:\n                    fsize = '%sB' % len(fdump)\n                if (hasattr(field, 'field') and\n                        'LE' in field.field.__class__.__name__[:3] or\n                        'LE' in field.__class__.__name__[:3]):\n                    fsize = r'$\\scriptstyle\\langle$' + fsize\n                st = pyx.text.text(XSTART + 3.4, (YTXT - y) * YMUL, r\"\\font\\cmbxfont=cmssbx10 scaled 600\\cmbxfont{%s}\" % fsize, [pyx.text.halign.boxright])  # noqa: E501\n                if isinstance(fval, str):\n                    if len(fval) > 18:\n                        fval = fval[:18] + \"[...]\"\n                else:\n                    fval = \"\"\n                vt = pyx.text.text(XSTART + 3.5, (YTXT - y) * YMUL, r\"\\font\\cmssfont=cmss10\\cmssfont{%s}\" % tex_escape(fval))  # noqa: E501\n                y += 1.0\n                if fdump:\n                    dt, target, last_shift, last_y = make_dump(fdump, last_shift, last_y, col, bkcol)  # noqa: E501\n\n                    dtb = target\n                    vtb = vt.bbox()\n                    bxvt = make_box(vtb)\n                    bxdt = make_box(dtb)\n                    dtb.enlarge(pyx.unit.u_pt)\n                    try:\n                        if yd < 0:\n                            cnx = pyx.connector.curve(bxvt, bxdt, absangle1=0, absangle2=-90)  # noqa: E501\n                        else:\n                            cnx = pyx.connector.curve(bxvt, bxdt, absangle1=0, absangle2=90)  # noqa: E501\n                    except Exception:\n                        pass\n                    else:\n                        canvas.stroke(cnx, [pyx.style.linewidth.thin, pyx.deco.earrow.small, col])  # noqa: E501\n\n                    canvas.insert(dt)\n\n                canvas.insert(ft)\n                canvas.insert(st)\n                canvas.insert(vt)\n            last_y += layer_shift\n\n        return canvas\n\n    def extract_padding(self, s):\n        # type: (bytes) -> Tuple[bytes, Optional[bytes]]\n        \"\"\"\n        DEV: to be overloaded to extract current layer's padding.\n\n        :param str s: the current layer\n        :return: a couple of strings (actual layer, padding)\n        \"\"\"\n        return s, None\n\n    def post_dissect(self, s):\n        # type: (bytes) -> bytes\n        \"\"\"DEV: is called right after the current layer has been dissected\"\"\"\n        return s\n\n    def pre_dissect(self, s):\n        # type: (bytes) -> bytes\n        \"\"\"DEV: is called right before the current layer is dissected\"\"\"\n        return s\n\n    def do_dissect(self, s):\n        # type: (bytes) -> bytes\n        _raw = s\n        self.raw_packet_cache_fields = {}\n        for f in self.fields_desc:\n            if not s:\n                break\n            s, fval = f.getfield(self, s)\n            # We need to track fields with mutable values to discard\n            # .raw_packet_cache when needed.\n            if f.islist or f.holds_packets or f.ismutable:\n                self.raw_packet_cache_fields[f.name] = f.do_copy(fval)\n            self.fields[f.name] = fval\n        self.raw_packet_cache = _raw[:-len(s)] if s else _raw\n        self.explicit = 1\n        return s\n\n    def do_dissect_payload(self, s):\n        # type: (bytes) -> None\n        \"\"\"\n        Perform the dissection of the layer's payload\n\n        :param str s: the raw layer\n        \"\"\"\n        if s:\n            cls = self.guess_payload_class(s)\n            try:\n                p = cls(s, _internal=1, _underlayer=self)\n            except KeyboardInterrupt:\n                raise\n            except Exception:\n                if conf.debug_dissector:\n                    if issubtype(cls, Packet):\n                        log_runtime.error(\"%s dissector failed\", cls.__name__)\n                    else:\n                        log_runtime.error(\"%s.guess_payload_class() returned \"\n                                          \"[%s]\",\n                                          self.__class__.__name__, repr(cls))\n                    if cls is not None:\n                        raise\n                p = conf.raw_layer(s, _internal=1, _underlayer=self)\n            self.add_payload(p)\n\n    def dissect(self, s):\n        # type: (bytes) -> None\n        s = self.pre_dissect(s)\n\n        s = self.do_dissect(s)\n\n        s = self.post_dissect(s)\n\n        payl, pad = self.extract_padding(s)\n        self.do_dissect_payload(payl)\n        if pad and conf.padding:\n            self.add_payload(conf.padding_layer(pad))\n\n    def guess_payload_class(self, payload):\n        # type: (bytes) -> Packet_metaclass\n        \"\"\"\n        DEV: Guesses the next payload class from layer bonds.\n        Can be overloaded to use a different mechanism.\n\n        :param str payload: the layer's payload\n        :return: the payload class\n        \"\"\"\n        for t in self.aliastypes:\n            for fval, cls in t.payload_guess:\n                try:\n                    if all(v == self.getfieldval(k)\n                           for k, v in six.iteritems(fval)):\n                        return cls\n                except AttributeError:\n                    pass\n        return self.default_payload_class(payload)\n\n    def default_payload_class(self, payload):\n        # type: (bytes) -> Packet_metaclass\n        \"\"\"\n        DEV: Returns the default payload class if nothing has been found by the\n        guess_payload_class() method.\n\n        :param str payload: the layer's payload\n        :return: the default payload class define inside the configuration file\n        \"\"\"\n        return conf.raw_layer\n\n    def hide_defaults(self):\n        # type: () -> None\n        \"\"\"Removes fields' values that are the same as default values.\"\"\"\n        # use list(): self.fields is modified in the loop\n        for k, v in list(six.iteritems(self.fields)):\n            v = self.fields[k]\n            if k in self.default_fields:\n                if self.default_fields[k] == v:\n                    del self.fields[k]\n        self.payload.hide_defaults()\n\n    def update_sent_time(self, time):\n        # type: (Optional[float]) -> None\n        \"\"\"Use by clone_with to share the sent_time value\"\"\"\n        pass\n\n    def clone_with(self, payload=None, share_time=False, **kargs):\n        # type: (Optional[Any], bool, **Any) -> Any\n        pkt = self.__class__()\n        pkt.explicit = 1\n        pkt.fields = kargs\n        pkt.default_fields = self.copy_fields_dict(self.default_fields)\n        pkt.overloaded_fields = self.overloaded_fields.copy()\n        pkt.time = self.time\n        pkt.underlayer = self.underlayer\n        pkt.post_transforms = self.post_transforms\n        pkt.raw_packet_cache = self.raw_packet_cache\n        pkt.raw_packet_cache_fields = self.copy_fields_dict(\n            self.raw_packet_cache_fields\n        )\n        pkt.wirelen = self.wirelen\n        if payload is not None:\n            pkt.add_payload(payload)\n        if share_time:\n            # This binds the subpacket .sent_time to this layer\n            def _up_time(x, parent=self):\n                # type: (float, Packet) -> None\n                parent.sent_time = x\n            pkt.update_sent_time = _up_time  # type: ignore\n        return pkt\n\n    def __iter__(self):\n        # type: () -> Iterator[Packet]\n        \"\"\"Iterates through all sub-packets generated by this Packet.\"\"\"\n        # We use __iterlen__ as low as possible, to lower processing time\n        def loop(todo, done, self=self):\n            # type: (List[str], Dict[str, Any], Any) -> Iterator[Packet]\n            if todo:\n                eltname = todo.pop()\n                elt = self.getfieldval(eltname)\n                if not isinstance(elt, Gen):\n                    if self.get_field(eltname).islist:\n                        elt = SetGen([elt])\n                    else:\n                        elt = SetGen(elt)\n                for e in elt:\n                    done[eltname] = e\n                    for x in loop(todo[:], done):\n                        yield x\n            else:\n                if isinstance(self.payload, NoPayload):\n                    payloads = SetGen([None])\n                else:\n                    payloads = self.payload\n                share_time = False\n                if self.fields == done and payloads.__iterlen__() == 1:\n                    # In this case, the packets are identical. Let's bind\n                    # their sent_time attribute for sending purpose\n                    share_time = True\n                for payl in payloads:\n                    # Let's make sure subpackets are consistent\n                    done2 = done.copy()\n                    for k in done2:\n                        if isinstance(done2[k], VolatileValue):\n                            done2[k] = done2[k]._fix()\n                    pkt = self.clone_with(payload=payl, share_time=share_time,\n                                          **done2)\n                    yield pkt\n\n        if self.explicit or self.raw_packet_cache is not None:\n            todo = []\n            done = self.fields\n        else:\n            todo = [k for (k, v) in itertools.chain(six.iteritems(self.default_fields),  # noqa: E501\n                                                    six.iteritems(self.overloaded_fields))  # noqa: E501\n                    if isinstance(v, VolatileValue)] + list(self.fields)\n            done = {}\n        return loop(todo, done)\n\n    def __iterlen__(self):\n        # type: () -> int\n        \"\"\"Predict the total length of the iterator\"\"\"\n        fields = [key for (key, val) in itertools.chain(six.iteritems(self.default_fields),  # noqa: E501\n                  six.iteritems(self.overloaded_fields))\n                  if isinstance(val, VolatileValue)] + list(self.fields)\n        length = 1\n\n        def is_valid_gen_tuple(x):\n            # type: (Any) -> bool\n            if not isinstance(x, tuple):\n                return False\n            return len(x) == 2 and all(isinstance(z, int) for z in x)\n\n        for field in fields:\n            fld, val = self.getfield_and_val(field)  # type: ignore\n            if hasattr(val, \"__iterlen__\"):\n                length *= val.__iterlen__()\n            elif is_valid_gen_tuple(val):\n                length *= (val[1] - val[0] + 1)\n            elif isinstance(val, list) and not fld.islist:\n                len2 = 0\n                for x in val:\n                    if hasattr(x, \"__iterlen__\"):\n                        len2 += x.__iterlen__()\n                    elif is_valid_gen_tuple(x):\n                        len2 += (x[1] - x[0] + 1)\n                    elif isinstance(x, list):\n                        len2 += len(x)\n                    else:\n                        len2 += 1\n                length *= len2 or 1\n        if not isinstance(self.payload, NoPayload):\n            return length * self.payload.__iterlen__()\n        return length\n\n    def iterpayloads(self):\n        # type: () -> Iterator[Packet]\n        \"\"\"Used to iter through the payloads of a Packet.\n        Useful for DNS or 802.11 for instance.\n        \"\"\"\n        yield self\n        current = self\n        while current.payload:\n            current = current.payload\n            yield current\n\n    def __gt__(self, other):\n        # type: (Packet) -> int\n        \"\"\"True if other is an answer from self (self ==> other).\"\"\"\n        if isinstance(other, Packet):\n            return other < self\n        elif isinstance(other, bytes):\n            return 1\n        else:\n            raise TypeError((self, other))\n\n    def __lt__(self, other):\n        # type: (Packet) -> int\n        \"\"\"True if self is an answer from other (other ==> self).\"\"\"\n        if isinstance(other, Packet):\n            return self.answers(other)\n        elif isinstance(other, bytes):\n            return 1\n        else:\n            raise TypeError((self, other))\n\n    def __eq__(self, other):\n        # type: (Any) -> bool\n        if not isinstance(other, self.__class__):\n            return False\n        for f in self.fields_desc:\n            if f not in other.fields_desc:\n                return False\n            if self.getfieldval(f.name) != other.getfieldval(f.name):\n                return False\n        return self.payload == other.payload\n\n    def __ne__(self, other):\n        # type: (Any) -> bool\n        return not self.__eq__(other)\n\n    # Note: setting __hash__ to None is the standard way\n    # of making an object un-hashable. mypy doesn't know that\n    __hash__ = None  # type: ignore\n\n    def hashret(self):\n        # type: () -> bytes\n        \"\"\"DEV: returns a string that has the same value for a request\n        and its answer.\"\"\"\n        return self.payload.hashret()\n\n    def answers(self, other):\n        # type: (Packet) -> int\n        \"\"\"DEV: true if self is an answer from other\"\"\"\n        if other.__class__ == self.__class__:\n            return self.payload.answers(other.payload)\n        return 0\n\n    def layers(self):\n        # type: () -> List[Packet_metaclass]\n        \"\"\"returns a list of layer classes (including subclasses) in this packet\"\"\"  # noqa: E501\n        layers = []\n        lyr = self  # type: Optional[Packet]\n        while lyr:\n            layers.append(lyr.__class__)\n            lyr = lyr.payload.getlayer(0, _subclass=True)\n        return layers\n\n    def haslayer(self, cls, _subclass=None):\n        # type: (Union[Packet_metaclass, str], Optional[bool]) -> int\n        \"\"\"\n        true if self has a layer that is an instance of cls.\n        Superseded by \"cls in self\" syntax.\n        \"\"\"\n        if _subclass is None:\n            _subclass = self.match_subclass or None\n        if _subclass:\n            match = issubtype\n        else:\n            match = lambda cls1, cls2: bool(cls1 == cls2)\n        if cls is None or match(self.__class__, cls) \\\n           or cls in [self.__class__.__name__, self._name]:\n            return True\n        for f in self.packetfields:\n            fvalue_gen = self.getfieldval(f.name)\n            if fvalue_gen is None:\n                continue\n            if not f.islist:\n                fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)\n            for fvalue in fvalue_gen:\n                if isinstance(fvalue, Packet):\n                    ret = fvalue.haslayer(cls, _subclass=_subclass)\n                    if ret:\n                        return ret\n        return self.payload.haslayer(cls, _subclass=_subclass)\n\n    def getlayer(self,\n                 cls,  # type: Union[int, Packet_metaclass]\n                 nb=1,  # type: int\n                 _track=None,  # type: Optional[List[int]]\n                 _subclass=None,  # type: Optional[bool]\n                 **flt  # type: Any\n                 ):\n        # type: (...) -> Optional[Packet]\n        \"\"\"Return the nb^th layer that is an instance of cls, matching flt\nvalues.\n        \"\"\"\n        if _subclass is None:\n            _subclass = self.match_subclass or None\n        if _subclass:\n            match = issubtype\n        else:\n            match = lambda cls1, cls2: bool(cls1 == cls2)\n        if isinstance(cls, int):\n            nb = cls + 1\n            cls = None\n        ccls = None  # type: Union[None, str]\n        fld = None  # type: Union[None, str]\n        if isinstance(cls, str) and \".\" in cls:\n            ccls, fld = cls.split(\".\", 1)\n        else:\n            ccls, fld = cls, None\n        if cls is None or match(self.__class__, cls) \\\n           or ccls in [self.__class__.__name__, self._name]:\n            if all(self.getfieldval(fldname) == fldvalue\n                   for fldname, fldvalue in six.iteritems(flt)):\n                if nb == 1:\n                    if fld is None:\n                        return self\n                    else:\n                        return self.getfieldval(fld)  # type: ignore\n                else:\n                    nb -= 1\n        for f in self.packetfields:\n            fvalue_gen = self.getfieldval(f.name)\n            if fvalue_gen is None:\n                continue\n            if not f.islist:\n                fvalue_gen = SetGen(fvalue_gen, _iterpacket=0)\n            for fvalue in fvalue_gen:\n                if isinstance(fvalue, Packet):\n                    track = []  # type: List[int]\n                    ret = fvalue.getlayer(cls, nb=nb, _track=track,\n                                          _subclass=_subclass, **flt)\n                    if ret is not None:\n                        return ret\n                    nb = track[0]\n        return self.payload.getlayer(cls, nb=nb, _track=_track,\n                                     _subclass=_subclass, **flt)\n\n    def firstlayer(self):\n        # type: () -> Packet\n        q = self\n        while q.underlayer is not None:\n            q = q.underlayer\n        return q\n\n    def __getitem__(self, cls):\n        # type: (Packet_metaclass) -> Any\n        if isinstance(cls, slice):\n            lname = cls.start\n            if cls.stop:\n                ret = self.getlayer(cls.start, nb=cls.stop, **(cls.step or {}))\n            else:\n                ret = self.getlayer(cls.start, **(cls.step or {}))\n        else:\n            lname = cls\n            ret = self.getlayer(cls)\n        if ret is None:\n            if isinstance(lname, Packet_metaclass):\n                lname = lname.__name__\n            elif not isinstance(lname, bytes):\n                lname = repr(lname)\n            raise IndexError(\"Layer [%s] not found\" % lname)\n        return ret\n\n    def __delitem__(self, cls):\n        # type: (Packet_metaclass) -> None\n        del(self[cls].underlayer.payload)\n\n    def __setitem__(self, cls, val):\n        # type: (Packet_metaclass, Packet) -> None\n        self[cls].underlayer.payload = val\n\n    def __contains__(self, cls):\n        # type: (Packet_metaclass) -> int\n        \"\"\"\n        \"cls in self\" returns true if self has a layer which is an\n        instance of cls.\n        \"\"\"\n        return self.haslayer(cls)\n\n    def route(self):\n        # type: () -> Tuple[Any, Optional[str], Optional[str]]\n        return self.payload.route()\n\n    def fragment(self, *args, **kargs):\n        # type: (*Any, **Any) -> List[Packet]\n        return self.payload.fragment(*args, **kargs)\n\n    def display(self, *args, **kargs):  # Deprecated. Use show()\n        # type: (*Any, **Any) -> None\n        \"\"\"Deprecated. Use show() method.\"\"\"\n        self.show(*args, **kargs)\n\n    def _show_or_dump(self,\n                      dump=False,  # type: bool\n                      indent=3,  # type: int\n                      lvl=\"\",  # type: str\n                      label_lvl=\"\",  # type: str\n                      first_call=True  # type: bool\n                      ):\n        # type: (...) -> Optional[str]\n        \"\"\"\n        Internal method that shows or dumps a hierarchical view of a packet.\n        Called by show.\n\n        :param dump: determine if it prints or returns the string value\n        :param int indent: the size of indentation for each layer\n        :param str lvl: additional information about the layer lvl\n        :param str label_lvl: additional information about the layer fields\n        :param first_call: determine if the current function is the first\n        :return: return a hierarchical view if dump, else print it\n        \"\"\"\n\n        if dump:\n            from scapy.themes import AnsiColorTheme\n            ct = AnsiColorTheme()  # No color for dump output\n        else:\n            ct = conf.color_theme\n        s = \"%s%s %s %s \\n\" % (label_lvl,\n                               ct.punct(\"###[\"),\n                               ct.layer_name(self.name),\n                               ct.punct(\"]###\"))\n        for f in self.fields_desc:\n            if isinstance(f, ConditionalField) and not f._evalcond(self):\n                continue\n            if isinstance(f, Emph) or f in conf.emph:\n                ncol = ct.emph_field_name\n                vcol = ct.emph_field_value\n            else:\n                ncol = ct.field_name\n                vcol = ct.field_value\n            fvalue = self.getfieldval(f.name)\n            if isinstance(fvalue, Packet) or (f.islist and f.holds_packets and isinstance(fvalue, list)):  # noqa: E501\n                s += \"%s  \\\\%-10s\\\\\\n\" % (label_lvl + lvl, ncol(f.name))\n                fvalue_gen = SetGen(fvalue, _iterpacket=0)\n                for fvalue in fvalue_gen:\n                    s += fvalue._show_or_dump(dump=dump, indent=indent, label_lvl=label_lvl + lvl + \"   |\", first_call=False)  # noqa: E501\n            else:\n                begn = \"%s  %-10s%s \" % (label_lvl + lvl,\n                                         ncol(f.name),\n                                         ct.punct(\"=\"),)\n                reprval = f.i2repr(self, fvalue)\n                if isinstance(reprval, str):\n                    reprval = reprval.replace(\"\\n\", \"\\n\" + \" \" * (len(label_lvl) +  # noqa: E501\n                                                                  len(lvl) +\n                                                                  len(f.name) +\n                                                                  4))\n                s += \"%s%s\\n\" % (begn, vcol(reprval))\n        if self.payload:\n            s += self.payload._show_or_dump(  # type: ignore\n                dump=dump,\n                indent=indent,\n                lvl=lvl + (\" \" * indent * self.show_indent),\n                label_lvl=label_lvl,\n                first_call=False\n            )\n\n        if first_call and not dump:\n            print(s)\n            return None\n        else:\n            return s\n\n    def show(self, dump=False, indent=3, lvl=\"\", label_lvl=\"\"):\n        # type: (bool, int, str, str) -> Optional[Any]\n        \"\"\"\n        Prints or returns (when \"dump\" is true) a hierarchical view of the\n        packet.\n\n        :param dump: determine if it prints or returns the string value\n        :param int indent: the size of indentation for each layer\n        :param str lvl: additional information about the layer lvl\n        :param str label_lvl: additional information about the layer fields\n        :return: return a hierarchical view if dump, else print it\n        \"\"\"\n        return self._show_or_dump(dump, indent, lvl, label_lvl)\n\n    def show2(self, dump=False, indent=3, lvl=\"\", label_lvl=\"\"):\n        # type: (bool, int, str, str) -> Optional[Any]\n        \"\"\"\n        Prints or returns (when \"dump\" is true) a hierarchical view of an\n        assembled version of the packet, so that automatic fields are\n        calculated (checksums, etc.)\n\n        :param dump: determine if it prints or returns the string value\n        :param int indent: the size of indentation for each layer\n        :param str lvl: additional information about the layer lvl\n        :param str label_lvl: additional information about the layer fields\n        :return: return a hierarchical view if dump, else print it\n        \"\"\"\n        return self.__class__(raw(self)).show(dump, indent, lvl, label_lvl)\n\n    def sprintf(self, fmt, relax=1):\n        # type: (str, int) -> str\n        \"\"\"\n        sprintf(format, [relax=1]) -> str\n\n        Where format is a string that can include directives. A directive\n        begins and ends by % and has the following format:\n        ``%[fmt[r],][cls[:nb].]field%``\n\n        :param fmt: is a classic printf directive, \"r\" can be appended for raw\n          substitution:\n          (ex: IP.flags=0x18 instead of SA), nb is the number of the layer\n          (ex: for IP/IP packets, IP:2.src is the src of the upper IP layer).\n          Special case : \"%.time%\" is the creation time.\n          Ex::\n\n            p.sprintf(\n              \"%.time% %-15s,IP.src% -> %-15s,IP.dst% %IP.chksum% \"\n              \"%03xr,IP.proto% %r,TCP.flags%\"\n            )\n\n          Moreover, the format string can include conditional statements. A\n          conditional statement looks like : {layer:string} where layer is a\n          layer name, and string is the string to insert in place of the\n          condition if it is true, i.e. if layer is present. If layer is\n          preceded by a \"!\", the result is inverted. Conditions can be\n          imbricated. A valid statement can be::\n\n            p.sprintf(\"This is a{TCP: TCP}{UDP: UDP}{ICMP:n ICMP} packet\")\n            p.sprintf(\"{IP:%IP.dst% {ICMP:%ICMP.type%}{TCP:%TCP.dport%}}\")\n\n          A side effect is that, to obtain \"{\" and \"}\" characters, you must use\n          \"%(\" and \"%)\".\n        \"\"\"\n\n        escape = {\"%\": \"%\",\n                  \"(\": \"{\",\n                  \")\": \"}\"}\n\n        # Evaluate conditions\n        while \"{\" in fmt:\n            i = fmt.rindex(\"{\")\n            j = fmt[i + 1:].index(\"}\")\n            cond = fmt[i + 1:i + j + 1]\n            k = cond.find(\":\")\n            if k < 0:\n                raise Scapy_Exception(\"Bad condition in format string: [%s] (read sprintf doc!)\" % cond)  # noqa: E501\n            cond, format_ = cond[:k], cond[k + 1:]\n            res = False\n            if cond[0] == \"!\":\n                res = True\n                cond = cond[1:]\n            if self.haslayer(cond):\n                res = not res\n            if not res:\n                format_ = \"\"\n            fmt = fmt[:i] + format_ + fmt[i + j + 2:]\n\n        # Evaluate directives\n        s = \"\"\n        while \"%\" in fmt:\n            i = fmt.index(\"%\")\n            s += fmt[:i]\n            fmt = fmt[i + 1:]\n            if fmt and fmt[0] in escape:\n                s += escape[fmt[0]]\n                fmt = fmt[1:]\n                continue\n            try:\n                i = fmt.index(\"%\")\n                sfclsfld = fmt[:i]\n                fclsfld = sfclsfld.split(\",\")\n                if len(fclsfld) == 1:\n                    f = \"s\"\n                    clsfld = fclsfld[0]\n                elif len(fclsfld) == 2:\n                    f, clsfld = fclsfld\n                else:\n                    raise Scapy_Exception\n                if \".\" in clsfld:\n                    cls, fld = clsfld.split(\".\")\n                else:\n                    cls = self.__class__.__name__\n                    fld = clsfld\n                num = 1\n                if \":\" in cls:\n                    cls, snum = cls.split(\":\")\n                    num = int(snum)\n                fmt = fmt[i + 1:]\n            except Exception:\n                raise Scapy_Exception(\"Bad format string [%%%s%s]\" % (fmt[:25], fmt[25:] and \"...\"))  # noqa: E501\n            else:\n                if fld == \"time\":\n                    val = time.strftime(\n                        \"%H:%M:%S.%%06i\",\n                        time.localtime(float(self.time))\n                    ) % int((self.time - int(self.time)) * 1000000)\n                elif cls == self.__class__.__name__ and hasattr(self, fld):\n                    if num > 1:\n                        val = self.payload.sprintf(\"%%%s,%s:%s.%s%%\" % (f, cls, num - 1, fld), relax)  # noqa: E501\n                        f = \"s\"\n                    elif f[-1] == \"r\":  # Raw field value\n                        val = getattr(self, fld)\n                        f = f[:-1]\n                        if not f:\n                            f = \"s\"\n                    else:\n                        val = getattr(self, fld)\n                        if fld in self.fieldtype:\n                            val = self.fieldtype[fld].i2repr(self, val)\n                else:\n                    val = self.payload.sprintf(\"%%%s%%\" % sfclsfld, relax)\n                    f = \"s\"\n                s += (\"%\" + f) % val\n\n        s += fmt\n        return s\n\n    def mysummary(self):\n        # type: () -> str\n        \"\"\"DEV: can be overloaded to return a string that summarizes the layer.\n           Only one mysummary() is used in a whole packet summary: the one of the upper layer,  # noqa: E501\n           except if a mysummary() also returns (as a couple) a list of layers whose  # noqa: E501\n           mysummary() must be called if they are present.\"\"\"\n        return \"\"\n\n    def _do_summary(self):\n        # type: () -> Tuple[int, str, List[Any]]\n        found, s, needed = self.payload._do_summary()\n        ret = \"\"\n        if not found or self.__class__ in needed:\n            ret = self.mysummary()\n            if isinstance(ret, tuple):\n                ret, n = ret\n                needed += n\n        if ret or needed:\n            found = 1\n        if not ret:\n            ret = self.__class__.__name__ if self.show_summary else \"\"\n        if self.__class__ in conf.emph:\n            impf = []\n            for f in self.fields_desc:\n                if f in conf.emph:\n                    impf.append(\"%s=%s\" % (f.name, f.i2repr(self, self.getfieldval(f.name))))  # noqa: E501\n            ret = \"%s [%s]\" % (ret, \" \".join(impf))\n        if ret and s:\n            ret = \"%s / %s\" % (ret, s)\n        else:\n            ret = \"%s%s\" % (ret, s)\n        return found, ret, needed\n\n    def summary(self, intern=0):\n        # type: (int) -> str\n        \"\"\"Prints a one line summary of a packet.\"\"\"\n        return self._do_summary()[1]\n\n    def lastlayer(self, layer=None):\n        # type: (Optional[Packet]) -> Packet\n        \"\"\"Returns the uppest layer of the packet\"\"\"\n        return self.payload.lastlayer(self)\n\n    def decode_payload_as(self, cls):\n        # type: (Packet_metaclass) -> None\n        \"\"\"Reassembles the payload and decode it using another packet class\"\"\"\n        s = raw(self.payload)\n        self.payload = cls(s, _internal=1, _underlayer=self)\n        pp = self\n        while pp.underlayer is not None:\n            pp = pp.underlayer\n        self.payload.dissection_done(pp)\n\n    def command(self):\n        # type: () -> str\n        \"\"\"\n        Returns a string representing the command you have to type to\n        obtain the same packet\n        \"\"\"\n        f = []\n        for fn, fv in six.iteritems(self.fields):\n            fld = self.get_field(fn)\n            if isinstance(fv, (list, dict, set)) and len(fv) == 0:\n                continue\n            if isinstance(fv, Packet):\n                fv = fv.command()\n            elif fld.islist and fld.holds_packets and isinstance(fv, list):\n                fv = \"[%s]\" % \",\".join(map(Packet.command, fv))\n            elif isinstance(fld, FlagsField):\n                fv = int(fv)\n            elif callable(getattr(fv, 'command', None)):\n                fv = fv.command()\n            else:\n                fv = repr(fv)\n            f.append(\"%s=%s\" % (fn, fv))\n        c = \"%s(%s)\" % (self.__class__.__name__, \", \".join(f))\n        pc = self.payload.command()\n        if pc:\n            c += \"/\" + pc\n        return c\n\n    def convert_to(self, other_cls, **kwargs):\n        # type: (Packet_metaclass, **Any) -> Packet\n        \"\"\"Converts this Packet to another type.\n\n        This is not guaranteed to be a lossless process.\n\n        By default, this only implements conversion to ``Raw``.\n\n        :param other_cls: Reference to a Packet class to convert to.\n        :type other_cls: Type[scapy.packet.Packet]\n        :return: Converted form of the packet.\n        :rtype: other_cls\n        :raises TypeError: When conversion is not possible\n        \"\"\"\n        if not issubtype(other_cls, Packet):\n            raise TypeError(\"{} must implement Packet\".format(other_cls))\n\n        if other_cls is Raw:\n            return Raw(raw(self))\n\n        if \"_internal\" not in kwargs:\n            return other_cls.convert_packet(self, _internal=True, **kwargs)  # type: ignore  # noqa: E501\n\n        raise TypeError(\"Cannot convert {} to {}\".format(\n            type(self).__name__, other_cls.__name__))\n\n    @classmethod\n    def convert_packet(cls, pkt, **kwargs):\n        # type: (Packet, **Any) -> Packet\n        \"\"\"Converts another packet to be this type.\n\n        This is not guaranteed to be a lossless process.\n\n        :param pkt: The packet to convert.\n        :type pkt: scapy.packet.Packet\n        :return: Converted form of the packet.\n        :rtype: cls\n        :raises TypeError: When conversion is not possible\n        \"\"\"\n        if not isinstance(pkt, Packet):\n            raise TypeError(\"Can only convert Packets\")\n\n        if \"_internal\" not in kwargs:\n            return pkt.convert_to(cls, _internal=True, **kwargs)\n\n        raise TypeError(\"Cannot convert {} to {}\".format(\n            type(pkt).__name__, cls.__name__))\n\n    @classmethod\n    def convert_packets(cls,\n                        pkts,  # type: List[Packet]\n                        **kwargs  # type: Any\n                        ):\n        # type: (...) -> Iterator[Iterator[Packet]]\n        \"\"\"Converts many packets to this type.\n\n        This is implemented as a generator.\n\n        See ``Packet.convert_packet``.\n        \"\"\"\n        for pkt in pkts:\n            yield cls.convert_packet(pkt, **kwargs)\n\n\nclass NoPayload(Packet):\n    def __new__(cls, *args, **kargs):\n        # type: (Packet_metaclass, *Any, **Any) -> Packet\n        singl = cls.__dict__.get(\"__singl__\")\n        if singl is None:\n            cls.__singl__ = singl = Packet.__new__(cls)\n            Packet.__init__(singl)\n        return singl  # type: ignore\n\n    def __init__(self, *args, **kargs):\n        # type: (*Any, **Any) -> None\n        pass\n\n    def dissection_done(self, pkt):\n        # type: (Packet) -> None\n        pass\n\n    def add_payload(self, payload):\n        # type: (Union[Packet, bytes]) -> NoReturn\n        raise Scapy_Exception(\"Can't add payload to NoPayload instance\")\n\n    def remove_payload(self):\n        # type: () -> None\n        pass\n\n    def add_underlayer(self, underlayer):\n        # type: (Any) -> None\n        pass\n\n    def remove_underlayer(self, other):\n        # type: (Packet) -> None\n        pass\n\n    def copy(self):\n        # type: () -> NoPayload\n        return self\n\n    def clear_cache(self):\n        # type: () -> None\n        pass\n\n    def __repr__(self):\n        # type: () -> str\n        return \"\"\n\n    def __str__(self):\n        # type: () -> str\n        return \"\"\n\n    def __bytes__(self):\n        # type: () -> bytes\n        return b\"\"\n\n    def __nonzero__(self):\n        # type: () -> bool\n        return False\n    __bool__ = __nonzero__\n\n    def do_build(self):\n        # type: () -> bytes\n        return b\"\"\n\n    def build(self):\n        # type: () -> bytes\n        return b\"\"\n\n    def build_padding(self):\n        # type: () -> bytes\n        return b\"\"\n\n    def build_done(self, p):\n        # type: (bytes) -> bytes\n        return p\n\n    def build_ps(self, internal=0):\n        # type: (int) -> Tuple[bytes, List[Any]]\n        return b\"\", []\n\n    def getfieldval(self, attr):\n        # type: (str) -> NoReturn\n        raise AttributeError(attr)\n\n    def getfield_and_val(self, attr):\n        # type: (str) -> NoReturn\n        raise AttributeError(attr)\n\n    def setfieldval(self, attr, val):\n        # type: (str, Any) -> NoReturn\n        raise AttributeError(attr)\n\n    def delfieldval(self, attr):\n        # type: (str) -> NoReturn\n        raise AttributeError(attr)\n\n    def hide_defaults(self):\n        # type: () -> None\n        pass\n\n    def __iter__(self):\n        # type: () -> Iterator[Packet]\n        return iter([])\n\n    def __eq__(self, other):\n        # type: (Any) -> bool\n        if isinstance(other, NoPayload):\n            return True\n        return False\n\n    def hashret(self):\n        # type: () -> bytes\n        return b\"\"\n\n    def answers(self, other):\n        # type: (NoPayload) -> bool\n        return isinstance(other, NoPayload) or isinstance(other, conf.padding_layer)  # noqa: E501\n\n    def haslayer(self, cls, _subclass=None):\n        # type: (Union[Packet_metaclass, str], Optional[bool]) -> int\n        return 0\n\n    def getlayer(self,\n                 cls,  # type: Union[int, Packet_metaclass]\n                 nb=1,  # type: int\n                 _track=None,  # type: Optional[List[int]]\n                 _subclass=None,  # type: Optional[bool]\n                 **flt  # type: Any\n                 ):\n        # type: (...) -> Optional[Packet]\n        if _track is not None:\n            _track.append(nb)\n        return None\n\n    def fragment(self, *args, **kargs):\n        # type: (*Any, **Any) -> List[Packet]\n        raise Scapy_Exception(\"cannot fragment this packet\")\n\n    def show(self, dump=False, indent=3, lvl=\"\", label_lvl=\"\"):\n        # type: (bool, int, str, str) -> None\n        pass\n\n    def sprintf(self, fmt, relax=1):\n        # type: (str, int) -> str\n        if relax:\n            return \"??\"\n        else:\n            raise Scapy_Exception(\"Format not found [%s]\" % fmt)\n\n    def _do_summary(self):\n        # type: () -> Tuple[int, str, List[Any]]\n        return 0, \"\", []\n\n    def layers(self):\n        # type: () -> List[Packet_metaclass]\n        return []\n\n    def lastlayer(self, layer=None):\n        # type: (Optional[Packet]) -> Packet\n        return layer or self\n\n    def command(self):\n        # type: () -> str\n        return \"\"\n\n    def route(self):\n        # type: () -> Tuple[None, None, None]\n        return (None, None, None)\n\n\n####################\n#  packet classes  #\n####################\n\n\nclass Raw(Packet):\n    name = \"Raw\"\n    fields_desc = [StrField(\"load\", b\"\")]\n\n    def __init__(self, _pkt=b\"\", *args, **kwargs):\n        # type: (bytes, *Any, **Any) -> None\n        if _pkt and not isinstance(_pkt, bytes):\n            _pkt = bytes_encode(_pkt)\n        super(Raw, self).__init__(_pkt, *args, **kwargs)\n\n    def answers(self, other):\n        # type: (Packet) -> int\n        return 1\n\n    def mysummary(self):\n        # type: () -> str\n        cs = conf.raw_summary\n        if cs:\n            if callable(cs):\n                return \"Raw %s\" % cs(self.load)\n            else:\n                return \"Raw %r\" % self.load\n        return Packet.mysummary(self)\n\n    @classmethod\n    def convert_packet(cls, pkt, **kwargs):\n        # type: (Packet, **Any) -> Raw\n        return Raw(raw(pkt))\n\n\nclass Padding(Raw):\n    name = \"Padding\"\n\n    def self_build(self, field_pos_list=None):\n        # type: (Optional[Any]) -> bytes\n        return b\"\"\n\n    def build_padding(self):\n        # type: () -> bytes\n        return (\n            bytes_encode(self.load) if self.raw_packet_cache is None\n            else self.raw_packet_cache\n        ) + self.payload.build_padding()\n\n\nconf.raw_layer = Raw\nconf.padding_layer = Padding\nif conf.default_l2 is None:\n    conf.default_l2 = Raw\n\n#################\n#  Bind layers  #\n#################\n\n\ndef bind_bottom_up(lower,  # type: Packet_metaclass\n                   upper,  # type: Packet_metaclass\n                   __fval=None,  # type: Optional[Any]\n                   **fval  # type: Any\n                   ):\n    # type: (...) -> None\n    r\"\"\"Bind 2 layers for dissection.\n    The upper layer will be chosen for dissection on top of the lower layer, if\n    ALL the passed arguments are validated. If multiple calls are made with\n    the same layers, the last one will be used as default.\n\n    ex:\n        >>> bind_bottom_up(Ether, SNAP, type=0x1234)\n        >>> Ether(b'\\xff\\xff\\xff\\xff\\xff\\xff\\xd0P\\x99V\\xdd\\xf9\\x124\\x00\\x00\\x00\\x00\\x00')  # noqa: E501\n        <Ether  dst=ff:ff:ff:ff:ff:ff src=d0:50:99:56:dd:f9 type=0x1234 |<SNAP  OUI=0x0 code=0x0 |>>  # noqa: E501\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    lower.payload_guess = lower.payload_guess[:]\n    lower.payload_guess.append((fval, upper))\n\n\ndef bind_top_down(lower,  # type: Packet_metaclass\n                  upper,  # type: Packet_metaclass\n                  __fval=None,  # type: Optional[Any]\n                  **fval  # type: Any\n                  ):\n    # type: (...) -> None\n    \"\"\"Bind 2 layers for building.\n    When the upper layer is added as a payload of the lower layer, all the\n    arguments will be applied to them.\n\n    ex:\n        >>> bind_top_down(Ether, SNAP, type=0x1234)\n        >>> Ether()/SNAP()\n        <Ether  type=0x1234 |<SNAP  |>>\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    upper._overload_fields = upper._overload_fields.copy()\n    upper._overload_fields[lower] = fval\n\n\n@conf.commands.register\ndef bind_layers(lower,  # type: Packet_metaclass\n                upper,  # type: Packet_metaclass\n                __fval=None,  # type: Optional[Dict[str, int]]\n                **fval  # type: Any\n                ):\n    # type: (...) -> None\n    \"\"\"Bind 2 layers on some specific fields' values.\n\n    It makes the packet being built and dissected when the arguments\n    are present.\n\n    This function calls both bind_bottom_up and bind_top_down, with\n    all passed arguments.\n\n    Please have a look at their docs:\n     - help(bind_bottom_up)\n     - help(bind_top_down)\n     \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    bind_top_down(lower, upper, **fval)\n    bind_bottom_up(lower, upper, **fval)\n\n\ndef split_bottom_up(lower,  # type: Packet_metaclass\n                    upper,  # type: Packet_metaclass\n                    __fval=None,  # type: Optional[Any]\n                    **fval  # type: Any\n                    ):\n    # type: (...) -> None\n    \"\"\"This call un-links an association that was made using bind_bottom_up.\n    Have a look at help(bind_bottom_up)\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n\n    def do_filter(params, cls):\n        # type: (Dict[str, int], Packet_metaclass) -> bool\n        params_is_invalid = any(\n            k not in params or params[k] != v for k, v in six.iteritems(fval)\n        )\n        return cls != upper or params_is_invalid\n    lower.payload_guess = [x for x in lower.payload_guess if do_filter(*x)]\n\n\ndef split_top_down(lower,  # type: Packet_metaclass\n                   upper,  # type: Packet_metaclass\n                   __fval=None,  # type: Optional[Any]\n                   **fval  # type: Any\n                   ):\n    # type: (...) -> None\n    \"\"\"This call un-links an association that was made using bind_top_down.\n    Have a look at help(bind_top_down)\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    if lower in upper._overload_fields:\n        ofval = upper._overload_fields[lower]\n        if any(k not in ofval or ofval[k] != v for k, v in six.iteritems(fval)):  # noqa: E501\n            return\n        upper._overload_fields = upper._overload_fields.copy()\n        del(upper._overload_fields[lower])\n\n\n@conf.commands.register\ndef split_layers(lower,  # type: Packet_metaclass\n                 upper,  # type: Packet_metaclass\n                 __fval=None,  # type: Optional[Any]\n                 **fval  # type: Any\n                 ):\n    # type: (...) -> None\n    \"\"\"Split 2 layers previously bound.\n    This call un-links calls bind_top_down and bind_bottom_up. It is the opposite of  # noqa: E501\n    bind_layers.\n\n    Please have a look at their docs:\n     - help(split_bottom_up)\n     - help(split_top_down)\n    \"\"\"\n    if __fval is not None:\n        fval.update(__fval)\n    split_bottom_up(lower, upper, **fval)\n    split_top_down(lower, upper, **fval)\n\n\n@conf.commands.register\ndef explore(layer=None):\n    # type: (Optional[str]) -> None\n    \"\"\"Function used to discover the Scapy layers and protocols.\n    It helps to see which packets exists in contrib or layer files.\n\n    params:\n     - layer: If specified, the function will explore the layer. If not,\n              the GUI mode will be activated, to browse the available layers\n\n    examples:\n      >>> explore()  # Launches the GUI\n      >>> explore(\"dns\")  # Explore scapy.layers.dns\n      >>> explore(\"http2\")  # Explore scapy.contrib.http2\n      >>> explore(scapy.layers.bluetooth4LE)\n\n    Note: to search a packet by name, use ls(\"name\") rather than explore.\n    \"\"\"\n    if layer is None:  # GUI MODE\n        if not conf.interactive:\n            raise Scapy_Exception(\"explore() GUI-mode cannot be run in \"\n                                  \"interactive mode. Please provide a \"\n                                  \"'layer' parameter !\")\n        # 0 - Imports\n        try:\n            import prompt_toolkit\n        except ImportError:\n            raise ImportError(\"prompt_toolkit is not installed ! \"\n                              \"You may install IPython, which contains it, via\"\n                              \" `pip install ipython`\")\n        if not _version_checker(prompt_toolkit, (2, 0)):\n            raise ImportError(\"prompt_toolkit >= 2.0.0 is required !\")\n        # Only available with prompt_toolkit > 2.0, not released on PyPi yet\n        from prompt_toolkit.shortcuts.dialogs import radiolist_dialog, \\\n            button_dialog\n        from prompt_toolkit.formatted_text import HTML\n        # Check for prompt_toolkit >= 3.0.0\n        call_ptk = lambda x: cast(str, x)  # type: Callable[[Any], str]\n        if _version_checker(prompt_toolkit, (3, 0)):\n            call_ptk = lambda x: x.run()  # type: ignore\n        # 1 - Ask for layer or contrib\n        btn_diag = button_dialog(\n            title=six.text_type(\"Scapy v%s\" % conf.version),\n            text=HTML(\n                six.text_type(\n                    '<style bg=\"white\" fg=\"red\">Chose the type of packets'\n                    ' you want to explore:</style>'\n                )\n            ),\n            buttons=[\n                (six.text_type(\"Layers\"), \"layers\"),\n                (six.text_type(\"Contribs\"), \"contribs\"),\n                (six.text_type(\"Cancel\"), \"cancel\")\n            ])\n        action = call_ptk(btn_diag)\n        # 2 - Retrieve list of Packets\n        if action == \"layers\":\n            # Get all loaded layers\n            lvalues = conf.layers.layers()\n            # Restrict to layers-only (not contribs) + packet.py and asn1*.py\n            values = [x for x in lvalues if (\"layers\" in x[0] or\n                                             \"packet\" in x[0] or\n                                             \"asn1\" in x[0])]\n        elif action == \"contribs\":\n            # Get all existing contribs\n            from scapy.main import list_contrib\n            cvalues = cast(List[Dict[str, str]], list_contrib(ret=True))\n            values = [(x['name'], x['description'])\n                      for x in cvalues]\n            # Remove very specific modules\n            values = [x for x in values if \"can\" not in x[0]]\n        else:\n            # Escape/Cancel was pressed\n            return\n        # Python 2 compat\n        if six.PY2:\n            values = [(six.text_type(x), six.text_type(y))\n                      for x, y in values]\n        # Build tree\n        if action == \"contribs\":\n            # A tree is a dictionary. Each layer contains a keyword\n            # _l which contains the files in the layer, and a _name\n            # argument which is its name. The other keys are the subfolders,\n            # which are similar dictionaries\n            tree = defaultdict(list)  # type: Dict[str, Union[List[Any], Dict[str, Any]]]  # noqa: E501\n            for name, desc in values:\n                if \".\" in name:  # Folder detected\n                    parts = name.split(\".\")\n                    subtree = tree\n                    for pa in parts[:-1]:\n                        if pa not in subtree:\n                            subtree[pa] = {}\n                        # one layer deeper\n                        subtree = subtree[pa]  # type: ignore\n                        subtree[\"_name\"] = pa  # type: ignore\n                    if \"_l\" not in subtree:\n                        subtree[\"_l\"] = []\n                    subtree[\"_l\"].append((parts[-1], desc))  # type: ignore\n                else:\n                    tree[\"_l\"].append((name, desc))  # type: ignore\n        elif action == \"layers\":\n            tree = {\"_l\": values}\n        # 3 - Ask for the layer/contrib module to explore\n        current = tree  # type: Any\n        previous = []  # type: List[Dict[str, Union[List[Any], Dict[str, Any]]]]  # noqa: E501\n        while True:\n            # Generate tests & form\n            folders = list(current.keys())\n            _radio_values = [\n                (\"$\" + name, six.text_type('[+] ' + name.capitalize()))\n                for name in folders if not name.startswith(\"_\")\n            ] + current.get(\"_l\", [])  # type: List[str]\n            cur_path = \"\"\n            if previous:\n                cur_path = \".\".join(\n                    itertools.chain(\n                        (x[\"_name\"] for x in previous[1:]),  # type: ignore\n                        (current[\"_name\"],)\n                    )\n                )\n            extra_text = (\n                '\\n<style bg=\"white\" fg=\"green\">> scapy.%s</style>'\n            ) % (action + (\".\" + cur_path if cur_path else \"\"))\n            # Show popup\n            rd_diag = radiolist_dialog(\n                values=_radio_values,\n                title=six.text_type(\n                    \"Scapy v%s\" % conf.version\n                ),\n                text=HTML(\n                    six.text_type((\n                        '<style bg=\"white\" fg=\"red\">Please select a file'\n                        'among the following, to see all layers contained in'\n                        ' it:</style>'\n                    ) + extra_text)\n                ),\n                cancel_text=\"Back\" if previous else \"Cancel\"\n            )\n            result = call_ptk(rd_diag)\n            if result is None:\n                # User pressed \"Cancel/Back\"\n                if previous:  # Back\n                    current = previous.pop()\n                    continue\n                else:  # Cancel\n                    return\n            if result.startswith(\"$\"):\n                previous.append(current)\n                current = current[result[1:]]\n            else:\n                # Enter on layer\n                if previous:  # In subfolder\n                    result = cur_path + \".\" + result\n                break\n        # 4 - (Contrib only): load contrib\n        if action == \"contribs\":\n            from scapy.main import load_contrib\n            load_contrib(result)\n            result = \"scapy.contrib.\" + result\n    else:  # NON-GUI MODE\n        # We handle layer as a short layer name, full layer name\n        # or the module itself\n        if isinstance(layer, types.ModuleType):\n            layer = layer.__name__\n        if isinstance(layer, str):\n            if layer.startswith(\"scapy.layers.\"):\n                result = layer\n            else:\n                if layer.startswith(\"scapy.contrib.\"):\n                    layer = layer.replace(\"scapy.contrib.\", \"\")\n                from scapy.main import load_contrib\n                load_contrib(layer)\n                result_layer, result_contrib = ((\"scapy.layers.%s\" % layer),\n                                                (\"scapy.contrib.%s\" % layer))\n                if result_layer in conf.layers.ldict:\n                    result = result_layer\n                elif result_contrib in conf.layers.ldict:\n                    result = result_contrib\n                else:\n                    raise Scapy_Exception(\"Unknown scapy module '%s'\" % layer)\n        else:\n            warning(\"Wrong usage ! Check out help(explore)\")\n            return\n\n    # COMMON PART\n    # Get the list of all Packets contained in that module\n    try:\n        all_layers = conf.layers.ldict[result]\n    except KeyError:\n        raise Scapy_Exception(\"Unknown scapy module '%s'\" % layer)\n    # Print\n    print(conf.color_theme.layer_name(\"Packets contained in %s:\" % result))\n    rtlst = []  # type: List[Tuple[Union[str, List[str]], ...]]\n    rtlst = [(lay.__name__ or \"\", lay._name or \"\") for lay in all_layers]\n    print(pretty_list(rtlst, [(\"Class\", \"Name\")], borders=True))\n\n\ndef _pkt_ls(obj,  # type: Union[Packet, Packet_metaclass]\n            verbose=False,  # type: bool\n            ):\n    # type: (...) -> List[Tuple[str, Field_metaclass, str, str, List[str]]]\n    \"\"\"Internal function used to resolve `fields_desc` to display it.\n\n    :param obj: a packet object or class\n    :returns: a list containing tuples [(name, clsname, clsname_extras,\n        default, long_attrs)]\n    \"\"\"\n    is_pkt = isinstance(obj, Packet)\n    if not issubtype(obj, Packet) and not is_pkt:\n        raise ValueError\n    fields = []\n    for f in obj.fields_desc:\n        cur_fld = f\n        attrs = []  # type: List[str]\n        long_attrs = []  # type: List[str]\n        while isinstance(cur_fld, (Emph, ConditionalField)):\n            if isinstance(cur_fld, ConditionalField):\n                attrs.append(cur_fld.__class__.__name__[:4])\n            cur_fld = cur_fld.fld\n        if verbose and isinstance(cur_fld, EnumField) \\\n           and hasattr(cur_fld, \"i2s\"):\n            if len(cur_fld.i2s or []) < 50:\n                long_attrs.extend(\n                    \"%s: %d\" % (strval, numval)\n                    for numval, strval in\n                    sorted(six.iteritems(cur_fld.i2s))\n                )\n        elif isinstance(cur_fld, MultiEnumField):\n            fld_depend = cur_fld.depends_on(obj.__class__\n                                            if is_pkt else obj)\n            attrs.append(\"Depends on %s\" % fld_depend)\n            if verbose:\n                cur_i2s = cur_fld.i2s_multi.get(\n                    cur_fld.depends_on(obj if is_pkt else obj()), {}\n                )\n                if len(cur_i2s) < 50:\n                    long_attrs.extend(\n                        \"%s: %d\" % (strval, numval)\n                        for numval, strval in\n                        sorted(six.iteritems(cur_i2s))\n                    )\n        elif verbose and isinstance(cur_fld, FlagsField):\n            names = cur_fld.names\n            long_attrs.append(\", \".join(names))\n        cls = cur_fld.__class__\n        class_name_extras = \"(%s)\" % (\n            \", \".join(attrs)\n        ) if attrs else \"\"\n        if isinstance(cur_fld, BitField):\n            class_name_extras += \" (%d bit%s)\" % (\n                cur_fld.size,\n                \"s\" if cur_fld.size > 1 else \"\"\n            )\n        fields.append(\n            (f.name,\n             cls,\n             class_name_extras,\n             repr(f.default),\n             long_attrs)\n        )\n    return fields\n\n\n@conf.commands.register\ndef ls(obj=None,  # type: Union[str, Packet, Packet_metaclass]\n       case_sensitive=False,  # type: bool\n       verbose=False  # type: bool\n       ):\n    # type: (...) -> None\n    \"\"\"List  available layers, or infos on a given layer class or name.\n\n    :param obj: Packet / packet name to use\n    :param case_sensitive: if obj is a string, is it case sensitive?\n    :param verbose:\n    \"\"\"\n    is_string = isinstance(obj, str)\n\n    if obj is None or is_string:\n        tip = False\n        if obj is None:\n            tip = True\n            all_layers = sorted(conf.layers, key=lambda x: x.__name__)\n        else:\n            pattern = re.compile(obj, 0 if case_sensitive else re.I)\n            # We first order by accuracy, then length\n            if case_sensitive:\n                sorter = lambda x: (x.__name__.index(obj), len(x.__name__))\n            else:\n                obj = obj.lower()\n                sorter = lambda x: (x.__name__.lower().index(obj),\n                                    len(x.__name__))\n            all_layers = sorted((layer for layer in conf.layers\n                                 if (isinstance(layer.__name__, str) and\n                                     pattern.search(layer.__name__)) or\n                                 (isinstance(layer.name, str) and\n                                     pattern.search(layer.name))),\n                                key=sorter)\n        for layer in all_layers:\n            print(\"%-10s : %s\" % (layer.__name__, layer._name))\n        if tip and conf.interactive:\n            print(\"\\nTIP: You may use explore() to navigate through all \"\n                  \"layers using a clear GUI\")\n    else:\n        try:\n            fields = _pkt_ls(obj, verbose=verbose)\n            is_pkt = isinstance(obj, Packet)\n            # Print\n            for fname, cls, clsne, dflt, long_attrs in fields:\n                cls = cls.__name__ + \" \" + clsne\n                print(\"%-10s : %-35s =\" % (fname, cls), end=' ')\n                if is_pkt:\n                    print(\"%-15r\" % (getattr(obj, fname),), end=' ')\n                print(\"(%r)\" % (dflt,))\n                for attr in long_attrs:\n                    print(\"%-15s%s\" % (\"\", attr))\n            # Restart for payload if any\n            if is_pkt:\n                obj = cast(Packet, obj)\n                if isinstance(obj.payload, NoPayload):\n                    return\n                print(\"--\")\n                ls(obj.payload)\n        except ValueError:\n            print(\"Not a packet class or name. Type 'ls()' to list packet classes.\")  # noqa: E501\n\n\n@conf.commands.register\ndef rfc(cls, ret=False, legend=True):\n    # type: (Packet_metaclass, bool, bool) -> Optional[str]\n    \"\"\"\n    Generate an RFC-like representation of a packet def.\n\n    :param cls: the Packet class\n    :param ret: return the result instead of printing (def. False)\n    :param legend: show text under the diagram (default True)\n\n    Ex::\n\n        >>> rfc(Ether)\n\n    \"\"\"\n    if not issubclass(cls, Packet):\n        raise TypeError(\"Packet class expected\")\n    cur_len = 0\n    cur_line = []\n    lines = []\n    # Get the size (width) that a field will take\n    # when formatted, from its length in bits\n    clsize = lambda x: 2 * x - 1  # type: Callable[[int], int]\n    ident = 0  # Fields UUID\n    # Generate packet groups\n    for f in cls.fields_desc:\n        flen = int(f.sz * 8)\n        cur_len += flen\n        ident += 1\n        # Fancy field name\n        fname = f.name.upper().replace(\"_\", \" \")\n        # The field might exceed the current line or\n        # take more than one line. Copy it as required\n        while True:\n            over = max(0, cur_len - 32)  # Exceed\n            len1 = clsize(flen - over)  # What fits\n            cur_line.append((fname[:len1], len1, ident))\n            if cur_len >= 32:\n                # Current line is full. start a new line\n                lines.append(cur_line)\n                cur_len = flen = over\n                fname = \"\"  # do not repeat the field\n                cur_line = []\n                if not over:\n                    # there is no data left\n                    break\n            else:\n                # End of the field\n                break\n    # Add the last line if un-finished\n    if cur_line:\n        lines.append(cur_line)\n    # Calculate separations between lines\n    seps = []\n    seps.append(\"+-\" * 32 + \"+\\n\")\n    for i in range(len(lines) - 1):\n        # Start with a full line\n        sep = \"+-\" * 32 + \"+\\n\"\n        # Get the line above and below the current\n        # separation\n        above, below = lines[i], lines[i + 1]\n        # The last field of above is shared with below\n        if above[-1][2] == below[0][2]:\n            # where the field in \"above\" starts\n            pos_above = sum(x[1] for x in above[:-1])\n            # where the field in \"below\" ends\n            pos_below = below[0][1]\n            if pos_above < pos_below:\n                # they are overlapping.\n                # Now crop the space between those pos\n                # and fill it with \" \"\n                pos_above = pos_above + pos_above % 2\n                sep = (\n                    sep[:1 + pos_above] +\n                    \" \" * (pos_below - pos_above) +\n                    sep[1 + pos_below:]\n                )\n        # line is complete\n        seps.append(sep)\n    # Graph\n    result = \"\"\n    # Bytes markers\n    result += \" \" + (\" \" * 19).join(\n        str(x) for x in range(4)\n    ) + \"\\n\"\n    # Bits markers\n    result += \" \" + \" \".join(\n        str(x % 10) for x in range(32)\n    ) + \"\\n\"\n    # Add fields and their separations\n    for line, sep in zip(lines, seps):\n        result += sep\n        for elt, flen, _ in line:\n            result += \"|\" + elt.center(flen, \" \")\n        result += \"|\\n\"\n    result += \"+-\" * (cur_len or 32) + \"+\\n\"\n    # Annotate with the figure name\n    if legend:\n        result += \"\\n\" + (\"Fig. \" + cls.__name__).center(66, \" \")\n    # return if asked for, else print\n    if ret:\n        return result\n    print(result)\n    return None\n\n\n#############\n#  Fuzzing  #\n#############\n\n@conf.commands.register\ndef fuzz(p,  # type: Packet\n         _inplace=0,  # type: int\n         ):\n    # type: (...) -> Packet\n    \"\"\"\n    Transform a layer into a fuzzy layer by replacing some default values\n    by random objects.\n\n    :param p: the Packet instance to fuzz\n    :return: the fuzzed packet.\n    \"\"\"\n    if not _inplace:\n        p = p.copy()\n    q = p\n    while not isinstance(q, NoPayload):\n        new_default_fields = {}\n        multiple_type_fields = []  # type: List[str]\n        for f in q.fields_desc:\n            if isinstance(f, PacketListField):\n                for r in getattr(q, f.name):\n                    fuzz(r, _inplace=1)\n            elif isinstance(f, MultipleTypeField):\n                # the type of the field will depend on others\n                multiple_type_fields.append(f.name)\n            elif f.default is not None:\n                if not isinstance(f, ConditionalField) or f._evalcond(q):\n                    rnd = f.randval()\n                    if rnd is not None:\n                        new_default_fields[f.name] = rnd\n        # Process packets with MultipleTypeFields\n        if multiple_type_fields:\n            # freeze the other random values\n            new_default_fields = {\n                key: (val._fix() if isinstance(val, VolatileValue) else val)\n                for key, val in six.iteritems(new_default_fields)\n            }\n            q.default_fields.update(new_default_fields)\n            # add the random values of the MultipleTypeFields\n            for name in multiple_type_fields:\n                fld = cast(MultipleTypeField, q.get_field(name))\n                rnd = fld._find_fld_pkt(q).randval()\n                if rnd is not None:\n                    new_default_fields[name] = rnd\n        q.default_fields.update(new_default_fields)\n        q = q.payload\n    return p\n", "idx": 40, "id": 18547, "msg": "", "proj": "secdev-scapy", "lang": "py", "sampling_weight": 0.22083191983507738}
{"patch": "@@ -290,3 +290,29 @@ type NetworkPolicyNodeStatus struct {\n \t// The generation realized by the Node.\n \tGeneration int64\n }\n+\n+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n+// Group is the message format of antrea/pkg/controller/types.Group in an API response.\n+type Group struct {\n+\tmetav1.TypeMeta\n+\tmetav1.ObjectMeta\n+\t// GroupMembers is a list of resources selected by this group.\n+\tGroupMembers []GroupMember\n+}\n+\n+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n+// GroupPatch describes the incremental update of an Group.\n+type GroupPatch struct {\n+\tmetav1.TypeMeta\n+\tmetav1.ObjectMeta\n+\tAddedGroupMembers   []GroupMember\n+\tRemovedGroupMembers []GroupMember\n+}\n+\n+// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n+// GroupList is a list of Group objects.\n+type GroupList struct {\n+\tmetav1.TypeMeta\n+\tmetav1.ListMeta\n+\tItems []Group\n+}", "y": 1, "oldf": "// Copyright 2019 Antrea Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage controlplane\n\nimport (\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/types\"\n\t\"k8s.io/apimachinery/pkg/util/intstr\"\n\n\tsecv1alpha1 \"github.com/vmware-tanzu/antrea/pkg/apis/security/v1alpha1\"\n\tstatsv1alpha1 \"github.com/vmware-tanzu/antrea/pkg/apis/stats/v1alpha1\"\n)\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AppliedToGroup is the message format of antrea/pkg/controller/types.AppliedToGroup in an API response.\ntype AppliedToGroup struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\t// GroupMembers is a list of resources selected by this group.\n\tGroupMembers []GroupMember\n}\n\n// PodReference represents a Pod Reference.\ntype PodReference struct {\n\t// The name of this pod.\n\tName string\n\t// The namespace of this pod.\n\tNamespace string\n}\n\n// NamedPort represents a Port with a name on Pod.\ntype NamedPort struct {\n\t// Port represents the Port number.\n\tPort int32\n\t// Name represents the associated name with this Port number.\n\tName string\n\t// Protocol for port. Must be UDP, TCP, or SCTP.\n\tProtocol Protocol\n}\n\n// ExternalEntityReference represents a ExternalEntity Reference.\ntype ExternalEntityReference struct {\n\t// The name of this ExternalEntity.\n\tName string\n\t// The namespace of this ExternalEntity.\n\tNamespace string\n}\n\n// GroupMember represents an resource member to be populated in Groups.\ntype GroupMember struct {\n\t// Pod maintains the reference to the Pod.\n\tPod *PodReference\n\t// ExternalEntity maintains the reference to the ExternalEntity.\n\tExternalEntity *ExternalEntityReference\n\t// IP is the IP address of the Endpoints associated with the GroupMember.\n\tIPs []IPAddress\n\t// Ports is the list NamedPort of the GroupMember.\n\tPorts []NamedPort\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AppliedToGroupPatch describes the incremental update of an AppliedToGroup.\ntype AppliedToGroupPatch struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\tAddedGroupMembers   []GroupMember\n\tRemovedGroupMembers []GroupMember\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AppliedToGroupList is a list of AppliedToGroup objects.\ntype AppliedToGroupList struct {\n\tmetav1.TypeMeta\n\tmetav1.ListMeta\n\tItems []AppliedToGroup\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AddressGroup is the message format of antrea/pkg/controller/types.AddressGroup in an API response.\ntype AddressGroup struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\t// GroupMembers is a list of GroupMember selected by this group.\n\tGroupMembers []GroupMember\n}\n\n// IPAddress describes a single IP address. Either an IPv4 or IPv6 address must be set.\ntype IPAddress []byte\n\n// IPNet describes an IP network.\ntype IPNet struct {\n\tIP           IPAddress\n\tPrefixLength int32\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AddressGroupPatch describes the incremental update of an AddressGroup.\ntype AddressGroupPatch struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\tAddedGroupMembers   []GroupMember\n\tRemovedGroupMembers []GroupMember\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// AddressGroupList is a list of AddressGroup objects.\ntype AddressGroupList struct {\n\tmetav1.TypeMeta\n\tmetav1.ListMeta\n\tItems []AddressGroup\n}\n\ntype NetworkPolicyType string\n\nconst (\n\tK8sNetworkPolicy           NetworkPolicyType = \"K8sNetworkPolicy\"\n\tAntreaClusterNetworkPolicy NetworkPolicyType = \"AntreaClusterNetworkPolicy\"\n\tAntreaNetworkPolicy        NetworkPolicyType = \"AntreaNetworkPolicy\"\n)\n\ntype NetworkPolicyReference struct {\n\t// Type of the NetworkPolicy.\n\tType NetworkPolicyType\n\t// Namespace of the NetworkPolicy. It's empty for Antrea ClusterNetworkPolicy.\n\tNamespace string\n\t// Name of the NetworkPolicy.\n\tName string\n\t// UID of the NetworkPolicy.\n\tUID types.UID\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// NetworkPolicy is the message format of antrea/pkg/controller/types.NetworkPolicy in an API response.\ntype NetworkPolicy struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\t// Rules is a list of rules to be applied to the selected GroupMembers.\n\tRules []NetworkPolicyRule\n\t// AppliedToGroups is a list of names of AppliedToGroups to which this policy applies.\n\t// Cannot be set in conjunction with any NetworkPolicyRule.AppliedToGroups in Rules.\n\tAppliedToGroups []string\n\t// Priority represents the relative priority of this NetworkPolicy as compared to\n\t// other NetworkPolicies. Priority will be unset (nil) for K8s NetworkPolicy.\n\tPriority *float64\n\t// TierPriority represents the priority of the Tier associated with this NetworkPolicy.\n\t// The TierPriority will remain nil for K8s NetworkPolicy.\n\tTierPriority *int32\n\t// Reference to the original NetworkPolicy that the internal NetworkPolicy is created for.\n\tSourceRef *NetworkPolicyReference\n}\n\n// Direction defines traffic direction of NetworkPolicyRule.\ntype Direction string\n\nconst (\n\tDirectionIn  Direction = \"In\"\n\tDirectionOut Direction = \"Out\"\n)\n\n// NetworkPolicyRule describes a particular set of traffic that is allowed.\ntype NetworkPolicyRule struct {\n\t// The direction of this rule.\n\t// If it's set to In, From must be set and To must not be set.\n\t// If it's set to Out, To must be set and From must not be set.\n\tDirection Direction\n\t// From represents sources which should be able to access the GroupMembers selected by the policy.\n\tFrom NetworkPolicyPeer\n\t// To represents destinations which should be able to be accessed by the GroupMembers selected by the policy.\n\tTo NetworkPolicyPeer\n\t// Services is a list of services which should be matched.\n\tServices []Service\n\t// Priority defines the priority of the Rule as compared to other rules in the\n\t// NetworkPolicy.\n\tPriority int32\n\t// Action specifies the action to be applied on the rule. i.e. Allow/Drop. An empty\n\t// action \u201cnil\u201d defaults to Allow action, which would be the case for rules created for\n\t// K8s NetworkPolicy.\n\tAction *secv1alpha1.RuleAction\n\t// EnableLogging is used to indicate if agent should generate logs\n\t// when rules are matched. Should be default to false.\n\tEnableLogging bool\n\t// AppliedToGroups is a list of names of AppliedToGroups to which this rule applies.\n\t// Cannot be set in conjunction with NetworkPolicy.AppliedToGroups of the NetworkPolicy\n\t// that this Rule is referred to.\n\tAppliedToGroups []string\n}\n\n// Protocol defines network protocols supported for things like container ports.\ntype Protocol string\n\nconst (\n\t// ProtocolTCP is the TCP protocol.\n\tProtocolTCP Protocol = \"TCP\"\n\t// ProtocolUDP is the UDP protocol.\n\tProtocolUDP Protocol = \"UDP\"\n\t// ProtocolSCTP is the SCTP protocol.\n\tProtocolSCTP Protocol = \"SCTP\"\n)\n\n// Service describes a port to allow traffic on.\ntype Service struct {\n\t// The protocol (TCP, UDP, or SCTP) which traffic must match. If not specified, this\n\t// field defaults to TCP.\n\t// +optional\n\tProtocol *Protocol\n\t// The port name or number on the given protocol. If not specified, this matches all port numbers.\n\t// +optional\n\tPort *intstr.IntOrString\n\t// EndPort defines the end of the port range, being the end included within the range.\n\t// It can only be specified when a numerical `port` is specified.\n\t// +optional\n\tEndPort *int32\n}\n\n// NetworkPolicyPeer describes a peer of NetworkPolicyRules.\n// It could be a list of names of AddressGroups and/or a list of IPBlock.\ntype NetworkPolicyPeer struct {\n\t// A list of names of AddressGroups.\n\tAddressGroups []string\n\t// A list of IPBlock.\n\tIPBlocks []IPBlock\n}\n\n// IPBlock describes a particular CIDR (Ex. \"192.168.1.1/24\"). The except entry describes CIDRs that should\n// not be included within this rule.\ntype IPBlock struct {\n\t// CIDR is an IPNet represents the IP Block.\n\tCIDR IPNet\n\t// Except is a slice of IPNets that should not be included within an IP Block.\n\t// Except values will be rejected if they are outside the CIDR range.\n\t// +optional\n\tExcept []IPNet\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// NetworkPolicyList is a list of NetworkPolicy objects.\ntype NetworkPolicyList struct {\n\tmetav1.TypeMeta\n\tmetav1.ListMeta\n\tItems []NetworkPolicy\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// NodeStatsSummary contains stats produced on a Node. It's used by the antrea-agents to report stats to the antrea-controller.\ntype NodeStatsSummary struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\n\t// The TrafficStats of K8s NetworkPolicies collected from the Node.\n\tNetworkPolicies []NetworkPolicyStats\n\t// The TrafficStats of Antrea ClusterNetworkPolicies collected from the Node.\n\tAntreaClusterNetworkPolicies []NetworkPolicyStats\n\t// The TrafficStats of Antrea NetworkPolicies collected from the Node.\n\tAntreaNetworkPolicies []NetworkPolicyStats\n}\n\n// NetworkPolicyStats contains the information and traffic stats of a NetworkPolicy.\ntype NetworkPolicyStats struct {\n\t// The reference of the NetworkPolicy.\n\tNetworkPolicy NetworkPolicyReference\n\t// The stats of the NetworkPolicy.\n\tTrafficStats statsv1alpha1.TrafficStats\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// NetworkPolicyStatus is the status of a NetworkPolicy.\ntype NetworkPolicyStatus struct {\n\tmetav1.TypeMeta\n\tmetav1.ObjectMeta\n\t// Nodes contains statuses produced on a list of Nodes.\n\tNodes []NetworkPolicyNodeStatus\n}\n\n// NetworkPolicyNodeStatus is the status of a NetworkPolicy on a Node.\ntype NetworkPolicyNodeStatus struct {\n\t// The name of the Node that produces the status.\n\tNodeName string\n\t// The generation realized by the Node.\n\tGeneration int64\n}\n", "idx": 1, "id": 29838, "msg": "s/an Group/a Group", "proj": "antrea-io-antrea", "lang": "go", "sampling_weight": 0.13389525325539522}
{"patch": "@@ -243,7 +243,9 @@ func TestBatchContainerHappyPath(t *testing.T) {\n \t\t\tcredentialsManager.EXPECT().RemoveCredentials(credentialsID)\n \n \t\t\tsleepTask := testdata.LoadTask(\"sleep5\")\n-\t\t\tsleepTask.ExecCommandAgentEnabledUnsafe = tc.execCommandAgentEnabled\n+\t\t\tif tc.execCommandAgentEnabled && len(sleepTask.Containers) > 0 {\n+\t\t\t\tenableExecCommandAgentForContainer(sleepTask.Containers[0], apicontainer.ManagedAgentState{})\n+\t\t\t}\n \t\t\tsleepTask.SetCredentialsID(credentialsID)\n \t\t\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n \t\t\t// containerEventsWG is used to force the test to wait until the container created and started", "y": 1, "oldf": "// +build unit\n\n// Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\"). You may\n// not use this file except in compliance with the License. A copy of the\n// License is located at\n//\n//\thttp://aws.amazon.com/apache2.0/\n//\n// or in the \"license\" file accompanying this file. This file is distributed\n// on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n// express or implied. See the License for the specific language governing\n// permissions and limitations under the License.\n\npackage engine\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/aws/amazon-ecs-agent/agent/api\"\n\t\"github.com/aws/amazon-ecs-agent/agent/api/appmesh\"\n\tapicontainer \"github.com/aws/amazon-ecs-agent/agent/api/container\"\n\tapicontainerstatus \"github.com/aws/amazon-ecs-agent/agent/api/container/status\"\n\tapieni \"github.com/aws/amazon-ecs-agent/agent/api/eni\"\n\tapierrors \"github.com/aws/amazon-ecs-agent/agent/api/errors\"\n\tapitask \"github.com/aws/amazon-ecs-agent/agent/api/task\"\n\tapitaskstatus \"github.com/aws/amazon-ecs-agent/agent/api/task/status\"\n\t\"github.com/aws/amazon-ecs-agent/agent/asm\"\n\tmock_asm_factory \"github.com/aws/amazon-ecs-agent/agent/asm/factory/mocks\"\n\tmock_secretsmanageriface \"github.com/aws/amazon-ecs-agent/agent/asm/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/config\"\n\tmock_containermetadata \"github.com/aws/amazon-ecs-agent/agent/containermetadata/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/credentials\"\n\tmock_credentials \"github.com/aws/amazon-ecs-agent/agent/credentials/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/dockerclient\"\n\t\"github.com/aws/amazon-ecs-agent/agent/dockerclient/dockerapi\"\n\tmock_dockerapi \"github.com/aws/amazon-ecs-agent/agent/dockerclient/dockerapi/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/ecscni\"\n\tmock_ecscni \"github.com/aws/amazon-ecs-agent/agent/ecscni/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/dockerstate\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/execcmd\"\n\tmock_execcmdagent \"github.com/aws/amazon-ecs-agent/agent/engine/execcmd/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/image\"\n\tmock_engine \"github.com/aws/amazon-ecs-agent/agent/engine/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/engine/testdata\"\n\t\"github.com/aws/amazon-ecs-agent/agent/eventstream\"\n\tmock_ssm_factory \"github.com/aws/amazon-ecs-agent/agent/ssm/factory/mocks\"\n\tmock_ssmiface \"github.com/aws/amazon-ecs-agent/agent/ssm/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/taskresource\"\n\t\"github.com/aws/amazon-ecs-agent/agent/taskresource/asmauth\"\n\t\"github.com/aws/amazon-ecs-agent/agent/taskresource/asmsecret\"\n\tmock_taskresource \"github.com/aws/amazon-ecs-agent/agent/taskresource/mocks\"\n\t\"github.com/aws/amazon-ecs-agent/agent/taskresource/ssmsecret\"\n\ttaskresourcevolume \"github.com/aws/amazon-ecs-agent/agent/taskresource/volume\"\n\tmock_ttime \"github.com/aws/amazon-ecs-agent/agent/utils/ttime/mocks\"\n\t\"github.com/aws/aws-sdk-go/aws\"\n\t\"github.com/aws/aws-sdk-go/service/secretsmanager\"\n\t\"github.com/aws/aws-sdk-go/service/ssm\"\n\t\"github.com/containernetworking/cni/pkg/types/current\"\n\t\"github.com/docker/docker/api/types\"\n\tdockercontainer \"github.com/docker/docker/api/types/container\"\n\t\"github.com/docker/docker/api/types/network\"\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/pborman/uuid\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nconst (\n\tcredentialsID               = \"credsid\"\n\tipv4                        = \"10.0.0.1\"\n\tgatewayIPv4                 = \"10.0.0.2/20\"\n\tmac                         = \"1.2.3.4\"\n\tipv6                        = \"f0:234:23\"\n\tdockerContainerName         = \"docker-container-name\"\n\tcontainerPid                = 123\n\ttaskIP                      = \"169.254.170.3\"\n\texitCode                    = 1\n\tlabelsTaskARN               = \"arn:aws:ecs:us-east-1:012345678910:task/c09f0188-7f87-4b0f-bfc3-16296622b6fe\"\n\ttaskSteadyStatePollInterval = time.Millisecond\n\tsecretID                    = \"meaning-of-life\"\n\tregion                      = \"us-west-2\"\n\tusername                    = \"irene\"\n\tpassword                    = \"sher\"\n\tignoredUID                  = \"1337\"\n\tproxyIngressPort            = \"15000\"\n\tproxyEgressPort             = \"15001\"\n\tappPort                     = \"9000\"\n\tegressIgnoredIP             = \"169.254.169.254\"\n\texpectedDelaySeconds        = 10\n\texpectedDelay               = expectedDelaySeconds * time.Second\n\tnetworkBridgeIP             = \"bridgeIP\"\n\tnetworkModeBridge           = \"bridge\"\n\tnetworkModeAWSVPC           = \"awsvpc\"\n\ttestTaskARN                 = \"arn:aws:ecs:region:account-id:task/task-id\"\n)\n\nvar (\n\tdefaultConfig config.Config\n\tnsResult      = mockSetupNSResult()\n\n\tmockENI = &apieni.ENI{\n\t\tID: \"eni-id\",\n\t\tIPV4Addresses: []*apieni.ENIIPV4Address{\n\t\t\t{\n\t\t\t\tPrimary: true,\n\t\t\t\tAddress: ipv4,\n\t\t\t},\n\t\t},\n\t\tMacAddress: mac,\n\t\tIPV6Addresses: []*apieni.ENIIPV6Address{\n\t\t\t{\n\t\t\t\tAddress: ipv6,\n\t\t\t},\n\t\t},\n\t\tSubnetGatewayIPV4Address: gatewayIPv4,\n\t}\n\n\t// createdContainerName is used to save the name of the created\n\t// container from the validateContainerRunWorkflow method. This\n\t// variable should never be accessed directly.\n\t// The `getCreatedContainerName` and `setCreatedContainerName`\n\t// methods should be used instead.\n\tcreatedContainerName string\n\t// createdContainerNameLock guards access to the createdContainerName\n\t// var.\n\tcreatedContainerNameLock sync.Mutex\n)\n\nfunc init() {\n\tdefaultConfig = config.DefaultConfig()\n\tdefaultConfig.TaskCPUMemLimit.Value = config.ExplicitlyDisabled\n}\n\nfunc getCreatedContainerName() string {\n\tcreatedContainerNameLock.Lock()\n\tdefer createdContainerNameLock.Unlock()\n\n\treturn createdContainerName\n}\n\nfunc setCreatedContainerName(name string) {\n\tcreatedContainerNameLock.Lock()\n\tdefer createdContainerNameLock.Unlock()\n\n\tcreatedContainerName = name\n}\n\nfunc mocks(t *testing.T, ctx context.Context, cfg *config.Config) (*gomock.Controller,\n\t*mock_dockerapi.MockDockerClient, *mock_ttime.MockTime, TaskEngine,\n\t*mock_credentials.MockManager, *mock_engine.MockImageManager, *mock_containermetadata.MockManager) {\n\tctrl := gomock.NewController(t)\n\tclient := mock_dockerapi.NewMockDockerClient(ctrl)\n\tmockTime := mock_ttime.NewMockTime(ctrl)\n\tcredentialsManager := mock_credentials.NewMockManager(ctrl)\n\n\tcontainerChangeEventStream := eventstream.NewEventStream(\"TESTTASKENGINE\", ctx)\n\tcontainerChangeEventStream.StartListening()\n\timageManager := mock_engine.NewMockImageManager(ctrl)\n\tmetadataManager := mock_containermetadata.NewMockManager(ctrl)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\n\ttaskEngine := NewTaskEngine(cfg, client, credentialsManager, containerChangeEventStream,\n\t\timageManager, dockerstate.NewTaskEngineState(), metadataManager, nil, execCmdMgr)\n\ttaskEngine.(*DockerTaskEngine)._time = mockTime\n\ttaskEngine.(*DockerTaskEngine).ctx = ctx\n\n\treturn ctrl, client, mockTime, taskEngine, credentialsManager, imageManager, metadataManager\n}\n\nfunc mockSetupNSResult() *current.Result {\n\t_, ip, _ := net.ParseCIDR(taskIP + \"/32\")\n\treturn &current.Result{\n\t\tIPs: []*current.IPConfig{\n\t\t\t{\n\t\t\t\tAddress: *ip,\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc TestBatchContainerHappyPath(t *testing.T) {\n\ttestcases := []struct {\n\t\tname                    string\n\t\tmetadataCreateError     error\n\t\tmetadataUpdateError     error\n\t\tmetadataCleanError      error\n\t\ttaskCPULimit            config.Conditional\n\t\texecCommandAgentEnabled bool\n\t}{\n\t\t{\n\t\t\tname:                \"Metadata Manager Succeeds\",\n\t\t\tmetadataCreateError: nil,\n\t\t\tmetadataUpdateError: nil,\n\t\t\tmetadataCleanError:  nil,\n\t\t\ttaskCPULimit:        config.ExplicitlyDisabled,\n\t\t},\n\t\t{\n\t\t\tname:                    \"ExecCommandAgent is started\",\n\t\t\tmetadataCreateError:     nil,\n\t\t\tmetadataUpdateError:     nil,\n\t\t\tmetadataCleanError:      nil,\n\t\t\ttaskCPULimit:            config.ExplicitlyDisabled,\n\t\t\texecCommandAgentEnabled: true,\n\t\t},\n\t\t{\n\t\t\tname:                \"Metadata Manager Fails to Create, Update and Cleanup\",\n\t\t\tmetadataCreateError: errors.New(\"create metadata error\"),\n\t\t\tmetadataUpdateError: errors.New(\"update metadata error\"),\n\t\t\tmetadataCleanError:  errors.New(\"clean metadata error\"),\n\t\t\ttaskCPULimit:        config.ExplicitlyDisabled,\n\t\t},\n\t}\n\n\tfor _, tc := range testcases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tmetadataConfig := defaultConfig\n\t\t\tmetadataConfig.TaskCPUMemLimit.Value = tc.taskCPULimit\n\t\t\tmetadataConfig.ContainerMetadataEnabled = config.BooleanDefaultFalse{Value: config.ExplicitlyEnabled}\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, mockTime, taskEngine, credentialsManager, imageManager, metadataManager := mocks(\n\t\t\t\tt, ctx, &metadataConfig)\n\t\t\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\t\t\ttaskEngine.(*DockerTaskEngine).execCmdMgr = execCmdMgr\n\t\t\tdefer ctrl.Finish()\n\n\t\t\troleCredentials := credentials.TaskIAMRoleCredentials{\n\t\t\t\tIAMRoleCredentials: credentials.IAMRoleCredentials{CredentialsID: \"credsid\"},\n\t\t\t}\n\t\t\tcredentialsManager.EXPECT().GetTaskCredentials(credentialsID).Return(roleCredentials, true).AnyTimes()\n\t\t\tcredentialsManager.EXPECT().RemoveCredentials(credentialsID)\n\n\t\t\tsleepTask := testdata.LoadTask(\"sleep5\")\n\t\t\tsleepTask.ExecCommandAgentEnabledUnsafe = tc.execCommandAgentEnabled\n\t\t\tsleepTask.SetCredentialsID(credentialsID)\n\t\t\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\t\t\t// containerEventsWG is used to force the test to wait until the container created and started\n\t\t\t// events are processed\n\t\t\tcontainerEventsWG := sync.WaitGroup{}\n\n\t\t\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\t\t\tcontainerName := make(chan string)\n\t\t\tgo func() {\n\t\t\t\tname := <-containerName\n\t\t\t\tsetCreatedContainerName(name)\n\t\t\t}()\n\n\t\t\tfor _, container := range sleepTask.Containers {\n\t\t\t\tvalidateContainerRunWorkflow(t, container, sleepTask, imageManager,\n\t\t\t\t\tclient, &roleCredentials, &containerEventsWG,\n\t\t\t\t\teventStream, containerName, func() {\n\t\t\t\t\t\tmetadataManager.EXPECT().Create(gomock.Any(), gomock.Any(),\n\t\t\t\t\t\t\tgomock.Any(), gomock.Any(), gomock.Any()).Return(tc.metadataCreateError)\n\t\t\t\t\t\tmetadataManager.EXPECT().Update(gomock.Any(), gomock.Any(), gomock.Any(),\n\t\t\t\t\t\t\tgomock.Any()).Return(tc.metadataUpdateError)\n\n\t\t\t\t\t\tif tc.execCommandAgentEnabled {\n\t\t\t\t\t\t\texecCmdMgr.EXPECT().InitializeTask(sleepTask).Times(1)\n\t\t\t\t\t\t\t// TODO: [ecs-exec] validate call control plane to report ExecCommandAgent SUCCESS/FAIL here\n\t\t\t\t\t\t\texecCmdMgr.EXPECT().StartAgent(gomock.Any(), client, sleepTask, sleepTask.Containers[0], containerID)\n\t\t\t\t\t\t\texecCmdMgr.EXPECT().AddAgentConfigMount(gomock.Any(), gomock.Any()).Return(nil).Times(1)\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t}\n\n\t\t\tclient.EXPECT().Info(gomock.Any(), gomock.Any()).Return(\n\t\t\t\ttypes.Info{}, nil)\n\t\t\taddTaskToEngine(t, ctx, taskEngine, sleepTask, mockTime, &containerEventsWG)\n\t\t\tcleanup := make(chan time.Time, 1)\n\t\t\tdefer close(cleanup)\n\t\t\tmockTime.EXPECT().After(gomock.Any()).Return(cleanup).MinTimes(1)\n\t\t\tclient.EXPECT().DescribeContainer(gomock.Any(), gomock.Any()).AnyTimes()\n\t\t\t// Simulate a container stop event from docker\n\t\t\teventStream <- dockerapi.DockerContainerChangeEvent{\n\t\t\t\tStatus: apicontainerstatus.ContainerStopped,\n\t\t\t\tDockerContainerMetadata: dockerapi.DockerContainerMetadata{\n\t\t\t\t\tDockerID: containerID,\n\t\t\t\t\tExitCode: aws.Int(exitCode),\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// StopContainer might be invoked if the test execution is slow, during\n\t\t\t// the cleanup phase. Account for that.\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\t\t\tdockerapi.DockerContainerMetadata{DockerID: containerID}).AnyTimes()\n\t\t\twaitForStopEvents(t, taskEngine.StateChangeEvents(), true)\n\t\t\t// This ensures that managedTask.waitForStopReported makes progress\n\t\t\tsleepTask.SetSentStatus(apitaskstatus.TaskStopped)\n\t\t\t// Extra events should not block forever; duplicate acs and docker events are possible\n\t\t\tgo func() { eventStream <- createDockerEvent(apicontainerstatus.ContainerStopped) }()\n\t\t\tgo func() { eventStream <- createDockerEvent(apicontainerstatus.ContainerStopped) }()\n\n\t\t\tsleepTaskStop := testdata.LoadTask(\"sleep5\")\n\n\t\t\tsleepTaskStop.SetCredentialsID(credentialsID)\n\t\t\tsleepTaskStop.SetDesiredStatus(apitaskstatus.TaskStopped)\n\t\t\ttaskEngine.AddTask(sleepTaskStop)\n\t\t\t// As above, duplicate events should not be a problem\n\t\t\ttaskEngine.AddTask(sleepTaskStop)\n\t\t\ttaskEngine.AddTask(sleepTaskStop)\n\t\t\t// Expect a bunch of steady state 'poll' describes when we trigger cleanup\n\t\t\tclient.EXPECT().RemoveContainer(gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\t\tfunc(ctx interface{}, removedContainerName string, timeout time.Duration) {\n\t\t\t\t\tassert.Equal(t, containerID, removedContainerName,\n\t\t\t\t\t\t\"Container name mismatch\")\n\t\t\t\t}).Return(nil)\n\n\t\t\timageManager.EXPECT().RemoveContainerReferenceFromImageState(gomock.Any())\n\t\t\tmetadataManager.EXPECT().Clean(gomock.Any()).Return(tc.metadataCleanError)\n\t\t\t// trigger cleanup\n\t\t\tcleanup <- time.Now()\n\t\t\tgo func() { eventStream <- createDockerEvent(apicontainerstatus.ContainerStopped) }()\n\t\t\t// Wait for the task to actually be dead; if we just fallthrough immediately,\n\t\t\t// the remove might not have happened (expectation failure)\n\t\t\tfor {\n\t\t\t\ttasks, _ := taskEngine.(*DockerTaskEngine).ListTasks()\n\t\t\t\tif len(tasks) == 0 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\ttime.Sleep(5 * time.Millisecond)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestTaskWithSteadyStateResourcesProvisioned tests container and task transitions\n// when the steady state for the pause container is set to RESOURCES_PROVISIONED and\n// the steady state for the normal container is set to RUNNING\nfunc TestTaskWithSteadyStateResourcesProvisioned(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockCNIClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = mockCNIClient\n\t// sleep5 contains a single 'sleep' container, with DesiredStatus == RUNNING\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepContainer := sleepTask.Containers[0]\n\tsleepContainer.TransitionDependenciesMap = make(map[apicontainerstatus.ContainerStatus]apicontainer.TransitionDependencySet)\n\tsleepContainer.BuildContainerDependency(\"pause\", apicontainerstatus.ContainerResourcesProvisioned, apicontainerstatus.ContainerPulled)\n\t// Add a second container with DesiredStatus == RESOURCES_PROVISIONED and\n\t// steadyState == RESOURCES_PROVISIONED\n\tpauseContainer := apicontainer.NewContainerWithSteadyState(apicontainerstatus.ContainerResourcesProvisioned)\n\tpauseContainer.Name = \"pause\"\n\tpauseContainer.Image = \"pause\"\n\tpauseContainer.CPU = 10\n\tpauseContainer.Memory = 10\n\tpauseContainer.Essential = true\n\tpauseContainer.Type = apicontainer.ContainerCNIPause\n\tsleepTask.Containers = append(sleepTask.Containers, pauseContainer)\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\t// containerEventsWG is used to force the test to wait until the container created and started\n\t// events are processed\n\tcontainerEventsWG := sync.WaitGroup{}\n\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\t// We cannot rely on the order of pulls between images as they can still be downloaded in\n\t// parallel. The dependency graph enforcement comes into effect for CREATED transitions.\n\t// Hence, do not enforce the order of invocation of these calls\n\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\tclient.EXPECT().PullImage(gomock.Any(), sleepContainer.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{})\n\timageManager.EXPECT().RecordContainerReference(sleepContainer).Return(nil)\n\timageManager.EXPECT().GetImageStateFromImageName(sleepContainer.Image).Return(nil, false)\n\n\tgomock.InOrder(\n\t\t// Ensure that the pause container is created first\n\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\tfunc(ctx interface{}, config *dockercontainer.Config, hostConfig *dockercontainer.HostConfig, containerName string, z time.Duration) {\n\t\t\t\tsleepTask.AddTaskENI(mockENI)\n\t\t\t\tsleepTask.SetAppMesh(&appmesh.AppMesh{\n\t\t\t\t\tIgnoredUID:       ignoredUID,\n\t\t\t\t\tProxyIngressPort: proxyIngressPort,\n\t\t\t\t\tProxyEgressPort:  proxyEgressPort,\n\t\t\t\t\tAppPorts: []string{\n\t\t\t\t\t\tappPort,\n\t\t\t\t\t},\n\t\t\t\t\tEgressIgnoredIPs: []string{\n\t\t\t\t\t\tegressIgnoredIP,\n\t\t\t\t\t},\n\t\t\t\t})\n\t\t\t\tassert.Equal(t, \"none\", string(hostConfig.NetworkMode))\n\t\t\t\tassert.True(t, strings.Contains(containerName, pauseContainer.Name))\n\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerCreated)\n\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t}()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID + \":\" + pauseContainer.Name}),\n\t\t// Ensure that the pause container is started after it's created\n\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID+\":\"+pauseContainer.Name, defaultConfig.ContainerStartTimeout).Do(\n\t\t\tfunc(ctx interface{}, id string, timeout time.Duration) {\n\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t}()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID + \":\" + pauseContainer.Name}),\n\t\tclient.EXPECT().InspectContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(&types.ContainerJSON{\n\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\tID:    containerID,\n\t\t\t\tState: &types.ContainerState{Pid: 23},\n\t\t\t},\n\t\t}, nil),\n\t\t// Then setting up the pause container network namespace\n\t\tmockCNIClient.EXPECT().SetupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nsResult, nil),\n\n\t\t// Once the pause container is started, sleep container will be created\n\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\tfunc(ctx interface{}, config *dockercontainer.Config, hostConfig *dockercontainer.HostConfig, containerName string, z time.Duration) {\n\t\t\t\tassert.True(t, strings.Contains(containerName, sleepContainer.Name))\n\t\t\t\tassert.Equal(t, \"container:\"+containerID+\":\"+pauseContainer.Name, string(hostConfig.NetworkMode))\n\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerCreated)\n\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t}()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID + \":\" + sleepContainer.Name}),\n\t\t// Next, the sleep container is started\n\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID+\":\"+sleepContainer.Name, defaultConfig.ContainerStartTimeout).Do(\n\t\t\tfunc(ctx interface{}, id string, timeout time.Duration) {\n\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t}()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID + \":\" + sleepContainer.Name}),\n\t)\n\n\taddTaskToEngine(t, ctx, taskEngine, sleepTask, mockTime, &containerEventsWG)\n\ttaskARNByIP, ok := taskEngine.(*DockerTaskEngine).state.GetTaskByIPAddress(taskIP)\n\tassert.True(t, ok)\n\tassert.Equal(t, sleepTask.Arn, taskARNByIP)\n\tcleanup := make(chan time.Time, 1)\n\tmockTime.EXPECT().After(gomock.Any()).Return(cleanup).AnyTimes()\n\tclient.EXPECT().InspectContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\t&types.ContainerJSON{\n\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\tID:    containerID,\n\t\t\t\tState: &types.ContainerState{Pid: 23},\n\t\t\t},\n\t\t}, nil)\n\tmockCNIClient.EXPECT().CleanupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)\n\tclient.EXPECT().StopContainer(gomock.Any(), containerID+\":\"+pauseContainer.Name, gomock.Any()).MinTimes(1)\n\tmockCNIClient.EXPECT().ReleaseIPResource(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil).MaxTimes(1)\n\n\t// Simulate a container stop event from docker\n\teventStream <- dockerapi.DockerContainerChangeEvent{\n\t\tStatus: apicontainerstatus.ContainerStopped,\n\t\tDockerContainerMetadata: dockerapi.DockerContainerMetadata{\n\t\t\tDockerID: containerID + \":\" + sleepContainer.Name,\n\t\t\tExitCode: aws.Int(exitCode),\n\t\t},\n\t}\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), true)\n}\n\n// TestRemoveEvents tests if the task engine can handle task events while the task is being\n// cleaned up. This test ensures that there's no regression in the task engine and ensures\n// there's no deadlock as seen in #313\nfunc TestRemoveEvents(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\t// containerEventsWG is used to force the test to wait until the container created and started\n\t// events are processed\n\tcontainerEventsWG := sync.WaitGroup{}\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tclient.EXPECT().StopContainer(gomock.Any(), gomock.Any(), gomock.Any()).AnyTimes()\n\tcontainerName := make(chan string)\n\tgo func() {\n\t\tname := <-containerName\n\t\tsetCreatedContainerName(name)\n\t}()\n\n\tfor _, container := range sleepTask.Containers {\n\t\tvalidateContainerRunWorkflow(t, container, sleepTask, imageManager,\n\t\t\tclient, nil, &containerEventsWG,\n\t\t\teventStream, containerName, func() {\n\t\t\t})\n\t}\n\n\taddTaskToEngine(t, ctx, taskEngine, sleepTask, mockTime, &containerEventsWG)\n\tcleanup := make(chan time.Time, 1)\n\tdefer close(cleanup)\n\tmockTime.EXPECT().After(gomock.Any()).Return(cleanup).MinTimes(1)\n\tclient.EXPECT().DescribeContainer(gomock.Any(), gomock.Any()).AnyTimes()\n\n\t// Simulate a container stop event from docker\n\teventStream <- dockerapi.DockerContainerChangeEvent{\n\t\tStatus: apicontainerstatus.ContainerStopped,\n\t\tDockerContainerMetadata: dockerapi.DockerContainerMetadata{\n\t\t\tDockerID: containerID,\n\t\t\tExitCode: aws.Int(exitCode),\n\t\t},\n\t}\n\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), true)\n\tsleepTaskStop := testdata.LoadTask(\"sleep5\")\n\tsleepTaskStop.SetDesiredStatus(apitaskstatus.TaskStopped)\n\ttaskEngine.AddTask(sleepTaskStop)\n\n\tclient.EXPECT().RemoveContainer(gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, removedContainerName string, timeout time.Duration) {\n\t\t\tassert.Equal(t, containerID, removedContainerName,\n\t\t\t\t\"Container name mismatch\")\n\n\t\t\t// Emit a couple of events for the task before cleanup finishes. This forces\n\t\t\t// discardEventsUntil to be invoked and should test the code path that\n\t\t\t// caused the deadlock, which was fixed with #320\n\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerStopped)\n\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerStopped)\n\t\t}).Return(nil)\n\n\timageManager.EXPECT().RemoveContainerReferenceFromImageState(gomock.Any())\n\n\t// This ensures that managedTask.waitForStopReported makes progress\n\tsleepTask.SetSentStatus(apitaskstatus.TaskStopped)\n\t// trigger cleanup\n\tcleanup <- time.Now()\n\t// Wait for the task to actually be dead; if we just fallthrough immediately,\n\t// the remove might not have happened (expectation failure)\n\tfor {\n\t\ttasks, _ := taskEngine.(*DockerTaskEngine).ListTasks()\n\t\tif len(tasks) == 0 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(5 * time.Millisecond)\n\t}\n}\n\nfunc TestStartTimeoutThenStart(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, testTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\ttestTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\ttestTime.EXPECT().After(gomock.Any())\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil)\n\tfor _, container := range sleepTask.Containers {\n\t\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{})\n\n\t\timageManager.EXPECT().RecordContainerReference(container)\n\t\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false)\n\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\tfunc(ctx interface{}, x, y, z, timeout interface{}) {\n\t\t\t\tgo func() { eventStream <- createDockerEvent(apicontainerstatus.ContainerCreated) }()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID})\n\n\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID, defaultConfig.ContainerStartTimeout).Return(dockerapi.DockerContainerMetadata{\n\t\t\tError: &dockerapi.DockerTimeoutError{},\n\t\t})\n\t}\n\n\t// Start timeout triggers a container stop as we force stop containers\n\t// when startcontainer times out. See #1043 for details\n\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Return(dockerapi.DockerContainerMetadata{\n\t\tError: dockerapi.CannotStartContainerError{fmt.Errorf(\"cannot start container\")},\n\t}).AnyTimes()\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\ttaskEngine.AddTask(sleepTask)\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), false)\n\n\t// Now surprise surprise, it actually did start!\n\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\t// However, if it starts again, we should not see it be killed; no additional expect\n\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\n\tselect {\n\tcase <-stateChangeEvents:\n\t\tt.Fatal(\"Should be out of events\")\n\tdefault:\n\t}\n}\n\nfunc TestSteadyStatePoll(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, testTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttaskEngine.(*DockerTaskEngine).taskSteadyStatePollInterval = taskSteadyStatePollInterval\n\tcontainerEventsWG := sync.WaitGroup{}\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepTask.Arn = uuid.New()\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tcontainerName := make(chan string)\n\tgo func() {\n\t\t<-containerName\n\t}()\n\n\t// set up expectations for each container in the task calling create + start\n\tfor _, container := range sleepTask.Containers {\n\t\tvalidateContainerRunWorkflow(t, container, sleepTask, imageManager,\n\t\t\tclient, nil, &containerEventsWG,\n\t\t\teventStream, containerName, func() {\n\t\t\t})\n\t}\n\ttestTime.EXPECT().Now().Return(time.Now()).MinTimes(1)\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\tclient.EXPECT().DescribeContainer(gomock.Any(), containerID).Return(\n\t\tapicontainerstatus.ContainerStopped,\n\t\tdockerapi.DockerContainerMetadata{\n\t\t\tDockerID: containerID,\n\t\t}).Do(func(ctx interface{}, x interface{}) {\n\t\twg.Done()\n\t})\n\tclient.EXPECT().DescribeContainer(gomock.Any(), containerID).Return(\n\t\tapicontainerstatus.ContainerStopped,\n\t\tdockerapi.DockerContainerMetadata{\n\t\t\tDockerID: containerID,\n\t\t}).AnyTimes()\n\tclient.EXPECT().StopContainer(gomock.Any(), containerID, dockerclient.StopContainerTimeout).AnyTimes()\n\n\terr := taskEngine.Init(ctx) // start the task engine\n\tassert.NoError(t, err)\n\ttaskEngine.AddTask(sleepTask) // actually add the task we created\n\twaitForRunningEvents(t, taskEngine.StateChangeEvents())\n\tcontainerMap, ok := taskEngine.(*DockerTaskEngine).State().ContainerMapByArn(sleepTask.Arn)\n\tassert.True(t, ok)\n\tdockerContainer, ok := containerMap[sleepTask.Containers[0].Name]\n\tassert.True(t, ok)\n\n\t// Wait for container create and start events to be processed\n\tcontainerEventsWG.Wait()\n\twg.Wait()\n\n\tcleanup := make(chan time.Time)\n\tdefer close(cleanup)\n\ttestTime.EXPECT().After(gomock.Any()).Return(cleanup).MinTimes(1)\n\tclient.EXPECT().RemoveContainer(gomock.Any(), dockerContainer.DockerID, dockerclient.RemoveContainerTimeout).Return(nil)\n\timageManager.EXPECT().RemoveContainerReferenceFromImageState(gomock.Any()).Return(nil)\n\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), false)\n\t// trigger cleanup, this ensures all the goroutines were finished\n\tsleepTask.SetSentStatus(apitaskstatus.TaskStopped)\n\tcleanup <- time.Now()\n\tfor {\n\t\ttasks, _ := taskEngine.(*DockerTaskEngine).ListTasks()\n\t\tif len(tasks) == 0 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(5 * time.Millisecond)\n\t}\n}\n\nfunc TestStopWithPendingStops(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, testTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\ttestTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\ttestTime.EXPECT().After(gomock.Any()).AnyTimes()\n\n\tsleepTask1 := testdata.LoadTask(\"sleep5\")\n\tsleepTask1.StartSequenceNumber = 5\n\tsleepTask2 := testdata.LoadTask(\"sleep5\")\n\tsleepTask2.Arn = \"arn2\"\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\n\tdefer discardEvents(stateChangeEvents)()\n\n\tpullDone := make(chan bool)\n\tpullInvoked := make(chan bool)\n\tclient.EXPECT().PullImage(gomock.Any(), gomock.Any(), nil, gomock.Any()).Do(func(w, x, y, z interface{}) {\n\t\tpullInvoked <- true\n\t\t<-pullDone\n\t}).MaxTimes(2)\n\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).AnyTimes()\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).AnyTimes()\n\n\ttaskEngine.AddTask(sleepTask2)\n\t<-pullInvoked\n\tstopSleep2 := testdata.LoadTask(\"sleep5\")\n\tstopSleep2.Arn = \"arn2\"\n\tstopSleep2.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tstopSleep2.StopSequenceNumber = 4\n\ttaskEngine.AddTask(stopSleep2)\n\n\ttaskEngine.AddTask(sleepTask1)\n\tstopSleep1 := testdata.LoadTask(\"sleep5\")\n\tstopSleep1.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tstopSleep1.StopSequenceNumber = 5\n\ttaskEngine.AddTask(stopSleep1)\n\tpullDone <- true\n\t// this means the PullImage is only called once due to the task is stopped before it\n\t// gets the pull image lock\n}\n\nfunc TestCreateContainerSaveDockerIDAndName(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\tdataClient, cleanup := newTestDataClient(t)\n\tdefer cleanup()\n\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine.SetDataClient(dataClient)\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepTask.Arn = testTaskARN\n\tsleepContainer, _ := sleepTask.ContainerByName(\"sleep5\")\n\tsleepContainer.TaskARNUnsafe = testTaskARN\n\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Return(dockerapi.DockerContainerMetadata{\n\t\tDockerID: testDockerID,\n\t})\n\tmetadata := taskEngine.createContainer(sleepTask, sleepContainer)\n\trequire.NoError(t, metadata.Error)\n\n\tcontainers, err := dataClient.GetContainers()\n\trequire.NoError(t, err)\n\trequire.Len(t, containers, 1)\n\tassert.Equal(t, testDockerID, containers[0].DockerID)\n\tassert.Contains(t, containers[0].DockerName, sleepContainer.Name)\n}\n\nfunc TestCreateContainerMetadata(t *testing.T) {\n\ttestcases := []struct {\n\t\tname  string\n\t\tinfo  types.Info\n\t\terror error\n\t}{\n\t\t{\n\t\t\tname:  \"Selinux Security Option\",\n\t\t\tinfo:  types.Info{SecurityOptions: []string{\"selinux\"}},\n\t\t\terror: nil,\n\t\t},\n\t\t{\n\t\t\tname:  \"Docker Info Error\",\n\t\t\tinfo:  types.Info{},\n\t\t\terror: errors.New(\"Error getting docker info\"),\n\t\t},\n\t}\n\n\tfor _, tc := range testcases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, _, privateTaskEngine, _, _, metadataManager := mocks(t, ctx, &config.Config{})\n\t\t\tdefer ctrl.Finish()\n\n\t\t\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\t\t\ttaskEngine.cfg.ContainerMetadataEnabled = config.BooleanDefaultFalse{Value: config.ExplicitlyEnabled}\n\n\t\t\tsleepTask := testdata.LoadTask(\"sleep5\")\n\t\t\tsleepContainer, _ := sleepTask.ContainerByName(\"sleep5\")\n\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil)\n\t\t\tclient.EXPECT().Info(ctx, dockerclient.InfoTimeout).Return(tc.info, tc.error)\n\t\t\tmetadataManager.EXPECT().Create(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), tc.info.SecurityOptions)\n\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any())\n\n\t\t\tmetadata := taskEngine.createContainer(sleepTask, sleepContainer)\n\t\t\tassert.NoError(t, metadata.Error)\n\t\t})\n\t}\n}\n\nfunc TestCreateContainerMergesLabels(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttestTask := &apitask.Task{\n\t\tArn:     labelsTaskARN,\n\t\tFamily:  \"myFamily\",\n\t\tVersion: \"1\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName: \"c1\",\n\t\t\t\tDockerConfig: apicontainer.DockerConfig{\n\t\t\t\t\tConfig: aws.String(`{\"Labels\":{\"key\":\"value\"}}`),\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\texpectedConfig, err := testTask.DockerConfig(testTask.Containers[0], defaultDockerClientAPIVersion)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\texpectedConfig.Labels = map[string]string{\n\t\t\"com.amazonaws.ecs.task-arn\":                labelsTaskARN,\n\t\t\"com.amazonaws.ecs.container-name\":          \"c1\",\n\t\t\"com.amazonaws.ecs.task-definition-family\":  \"myFamily\",\n\t\t\"com.amazonaws.ecs.task-definition-version\": \"1\",\n\t\t\"com.amazonaws.ecs.cluster\":                 \"\",\n\t\t\"key\":                                       \"value\",\n\t}\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\tclient.EXPECT().CreateContainer(gomock.Any(), expectedConfig, gomock.Any(), gomock.Any(), gomock.Any())\n\ttaskEngine.(*DockerTaskEngine).createContainer(testTask, testTask.Containers[0])\n}\n\n// TestCreateContainerAddV3EndpointIDToState tests that in createContainer, when the\n// container's v3 endpoint id is set, we will add mappings to engine state\nfunc TestCreateContainerAddV3EndpointIDToState(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\n\ttestContainer := &apicontainer.Container{\n\t\tName:         \"c1\",\n\t\tV3EndpointID: \"v3EndpointID\",\n\t}\n\n\ttestTask := &apitask.Task{\n\t\tArn:     \"myTaskArn\",\n\t\tFamily:  \"myFamily\",\n\t\tVersion: \"1\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\ttestContainer,\n\t\t},\n\t}\n\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\t// V3EndpointID mappings are only added to state when dockerID is available. So return one here.\n\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Return(dockerapi.DockerContainerMetadata{\n\t\tDockerID: \"dockerID\",\n\t})\n\ttaskEngine.createContainer(testTask, testContainer)\n\n\t// check that we have added v3 endpoint mappings to state\n\tstate := taskEngine.state\n\n\taddedTaskARN, ok := state.TaskARNByV3EndpointID(\"v3EndpointID\")\n\tassert.True(t, ok)\n\tassert.Equal(t, testTask.Arn, addedTaskARN)\n\n\taddedDockerID, ok := state.DockerIDByV3EndpointID(\"v3EndpointID\")\n\tassert.True(t, ok)\n\tassert.Equal(t, \"dockerID\", addedDockerID)\n}\n\n// TestTaskTransitionWhenStopContainerTimesout tests that task transitions to stopped\n// only when terminal events are received from docker event stream when\n// StopContainer times out\nfunc TestTaskTransitionWhenStopContainerTimesout(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tmockTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\tmockTime.EXPECT().After(gomock.Any()).AnyTimes()\n\tcontainerStopTimeoutError := dockerapi.DockerContainerMetadata{\n\t\tError: &dockerapi.DockerTimeoutError{\n\t\t\tTransition: \"stop\",\n\t\t\tDuration:   dockerclient.StopContainerTimeout,\n\t\t},\n\t}\n\tdockerEventSent := make(chan int)\n\tfor _, container := range sleepTask.Containers {\n\t\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{})\n\t\timageManager.EXPECT().RecordContainerReference(container)\n\t\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false)\n\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil)\n\n\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\tfunc(ctx interface{}, x, y, z, timeout interface{}) {\n\t\t\t\tgo func() { eventStream <- createDockerEvent(apicontainerstatus.ContainerCreated) }()\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID})\n\n\t\tgomock.InOrder(\n\t\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID, defaultConfig.ContainerStartTimeout).Do(\n\t\t\t\tfunc(ctx interface{}, id string, timeout time.Duration) {\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\t\t}()\n\t\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID}),\n\n\t\t\t// StopContainer times out\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Return(containerStopTimeoutError),\n\t\t\t// Since task is not in steady state, progressContainers causes\n\t\t\t// another invocation of StopContainer. Return a timeout error\n\t\t\t// for that as well.\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Do(\n\t\t\t\tfunc(ctx interface{}, id string, timeout time.Duration) {\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\tdockerEventSent <- 1\n\t\t\t\t\t\t// Emit 'ContainerStopped' event to the container event stream\n\t\t\t\t\t\t// This should cause the container and the task to transition\n\t\t\t\t\t\t// to 'STOPPED'\n\t\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerStopped)\n\t\t\t\t\t}()\n\t\t\t\t}).Return(containerStopTimeoutError).MinTimes(1),\n\t\t)\n\t}\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\n\tgo taskEngine.AddTask(sleepTask)\n\t// wait for task running\n\twaitForRunningEvents(t, taskEngine.StateChangeEvents())\n\t// Set the task desired status to be stopped and StopContainer will be called\n\tupdateSleepTask := testdata.LoadTask(\"sleep5\")\n\tupdateSleepTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tgo taskEngine.AddTask(updateSleepTask)\n\n\t// StopContainer timeout error shouldn't cause cantainer/task status change\n\t// until receive stop event from docker event stream\n\tselect {\n\tcase <-stateChangeEvents:\n\t\tt.Error(\"Should not get task events\")\n\tcase <-dockerEventSent:\n\t\tt.Logf(\"Send docker stop event\")\n\t\tgo func() {\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase <-dockerEventSent:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\n\t// StopContainer was called again and received stop event from docker event stream\n\t// Expect it to go to stopped\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), false)\n}\n\n// TestTaskTransitionWhenStopContainerReturnsUnretriableError tests if the task transitions\n// to stopped without retrying stopping the container in the task when the initial\n// stop container call returns an unretriable error from docker, specifically the\n// NoSuchContainer error\nfunc TestTaskTransitionWhenStopContainerReturnsUnretriableError(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tmockTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\tmockTime.EXPECT().After(gomock.Any()).AnyTimes()\n\tcontainerEventsWG := sync.WaitGroup{}\n\tfor _, container := range sleepTask.Containers {\n\t\tgomock.InOrder(\n\t\t\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes(),\n\t\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{}),\n\t\t\timageManager.EXPECT().RecordContainerReference(container),\n\t\t\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false),\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\t\t// Simulate successful create container\n\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\t\tfunc(ctx interface{}, x, y, z, timeout interface{}) {\n\t\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerCreated)\n\t\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t\t}()\n\t\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID}),\n\t\t\t// Simulate successful start container\n\t\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID, defaultConfig.ContainerStartTimeout).Do(\n\t\t\t\tfunc(ctx interface{}, id string, timeout time.Duration) {\n\t\t\t\t\tcontainerEventsWG.Add(1)\n\t\t\t\t\tgo func() {\n\t\t\t\t\t\teventStream <- createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\t\t\tcontainerEventsWG.Done()\n\t\t\t\t\t}()\n\t\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: containerID}),\n\t\t\t// StopContainer errors out. However, since this is a known unretriable error,\n\t\t\t// the task engine should not retry stopping the container and move on.\n\t\t\t// If there's a delay in task engine's processing of the ContainerRunning\n\t\t\t// event, StopContainer will be invoked again as the engine considers it\n\t\t\t// as a stopped container coming back. MinTimes() should guarantee that\n\t\t\t// StopContainer is invoked at least once and in protecting agasint a test\n\t\t\t// failure when there's a delay in task engine processing the ContainerRunning\n\t\t\t// event.\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Return(dockerapi.DockerContainerMetadata{\n\t\t\t\tError: dockerapi.CannotStopContainerError{dockerapi.NoSuchContainerError{}},\n\t\t\t}).MinTimes(1),\n\t\t)\n\t}\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tgo taskEngine.AddTask(sleepTask)\n\t// wait for task running\n\twaitForRunningEvents(t, taskEngine.StateChangeEvents())\n\tcontainerEventsWG.Wait()\n\t// Set the task desired status to be stopped and StopContainer will be called\n\tupdateSleepTask := testdata.LoadTask(\"sleep5\")\n\tupdateSleepTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tgo taskEngine.AddTask(updateSleepTask)\n\t// StopContainer was called again and received stop event from docker event stream\n\t// Expect it to go to stopped\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), false)\n}\n\n// TestTaskTransitionWhenStopContainerReturnsTransientErrorBeforeSucceeding tests if the task\n// transitions to stopped only after receiving the container stopped event from docker when\n// the initial stop container call fails with an unknown error.\nfunc TestTaskTransitionWhenStopContainerReturnsTransientErrorBeforeSucceeding(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tmockTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\tmockTime.EXPECT().After(gomock.Any()).AnyTimes()\n\tcontainerStoppingError := dockerapi.DockerContainerMetadata{\n\t\tError: dockerapi.CannotStopContainerError{errors.New(\"Error stopping container\")},\n\t}\n\tfor _, container := range sleepTask.Containers {\n\t\tgomock.InOrder(\n\t\t\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes(),\n\t\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{}),\n\t\t\timageManager.EXPECT().RecordContainerReference(container),\n\t\t\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false),\n\t\t\t// Simulate successful create container\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\t\t\tdockerapi.DockerContainerMetadata{DockerID: containerID}),\n\t\t\t// Simulate successful start container\n\t\t\tclient.EXPECT().StartContainer(gomock.Any(), containerID, defaultConfig.ContainerStartTimeout).Return(\n\t\t\t\tdockerapi.DockerContainerMetadata{DockerID: containerID}),\n\t\t\t// StopContainer errors out a couple of times\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Return(containerStoppingError).Times(2),\n\t\t\t// Since task is not in steady state, progressContainers causes\n\t\t\t// another invocation of StopContainer. Return the 'succeed' response,\n\t\t\t// which should cause the task engine to stop invoking this again and\n\t\t\t// transition the task to stopped.\n\t\t\tclient.EXPECT().StopContainer(gomock.Any(), containerID, gomock.Any()).Return(dockerapi.DockerContainerMetadata{}),\n\t\t)\n\t}\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tgo taskEngine.AddTask(sleepTask)\n\t// wait for task running\n\twaitForRunningEvents(t, taskEngine.StateChangeEvents())\n\t// Set the task desired status to be stopped and StopContainer will be called\n\tupdateSleepTask := testdata.LoadTask(\"sleep5\")\n\tupdateSleepTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tgo taskEngine.AddTask(updateSleepTask)\n\t// StopContainer invocation should have caused it to stop eventually.\n\twaitForStopEvents(t, taskEngine.StateChangeEvents(), false)\n}\n\nfunc TestGetTaskByArn(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\t// Need a mock client as AddTask not only adds a task to the engine, but\n\t// also causes the engine to progress the task.\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).AnyTimes()\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).AnyTimes()\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\tdefer taskEngine.Disable()\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tsleepTaskArn := sleepTask.Arn\n\tsleepTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\ttaskEngine.AddTask(sleepTask)\n\n\t_, found := taskEngine.GetTaskByArn(sleepTaskArn)\n\tassert.True(t, found, \"Task %s not found\", sleepTaskArn)\n\n\t_, found = taskEngine.GetTaskByArn(sleepTaskArn + \"arn\")\n\tassert.False(t, found, \"Task with invalid arn found in the task engine\")\n}\n\nfunc TestPauseContainerHappyPath(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, dockerClient, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tcniClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = cniClient\n\ttaskEngine.(*DockerTaskEngine).taskSteadyStatePollInterval = taskSteadyStatePollInterval\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepContainer := sleepTask.Containers[0]\n\tsleepContainer.TransitionDependenciesMap = make(map[apicontainerstatus.ContainerStatus]apicontainer.TransitionDependencySet)\n\n\t// Add eni information to the task so the task can add dependency of pause container\n\tsleepTask.AddTaskENI(mockENI)\n\n\tsleepTask.SetAppMesh(&appmesh.AppMesh{\n\t\tIgnoredUID:       ignoredUID,\n\t\tProxyIngressPort: proxyIngressPort,\n\t\tProxyEgressPort:  proxyEgressPort,\n\t\tAppPorts: []string{\n\t\t\tappPort,\n\t\t},\n\t\tEgressIgnoredIPs: []string{\n\t\t\tegressIgnoredIP,\n\t\t},\n\t})\n\n\tdockerClient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\n\tpauseContainerID := \"pauseContainerID\"\n\t// Pause container will be launched first\n\tgomock.InOrder(\n\t\tdockerClient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\tdockerClient.EXPECT().CreateContainer(\n\t\t\tgomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\tfunc(ctx interface{}, config *dockercontainer.Config, x, y, z interface{}) {\n\t\t\t\tname, ok := config.Labels[labelPrefix+\"container-name\"]\n\t\t\t\tassert.True(t, ok)\n\t\t\t\tassert.Equal(t, apitask.NetworkPauseContainerName, name)\n\t\t\t}).Return(dockerapi.DockerContainerMetadata{DockerID: \"pauseContainerID\"}),\n\t\tdockerClient.EXPECT().StartContainer(gomock.Any(), pauseContainerID, defaultConfig.ContainerStartTimeout).Return(\n\t\t\tdockerapi.DockerContainerMetadata{DockerID: \"pauseContainerID\"}),\n\t\tdockerClient.EXPECT().InspectContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\t\t&types.ContainerJSON{\n\t\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\t\tID:    pauseContainerID,\n\t\t\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t\t\t},\n\t\t\t}, nil),\n\t\tcniClient.EXPECT().SetupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nsResult, nil),\n\t)\n\n\t// For the other container\n\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\tdockerClient.EXPECT().PullImage(gomock.Any(), gomock.Any(), nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{})\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).Return(nil)\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false)\n\tdockerClient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil)\n\tdockerClient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(),\n\t\tgomock.Any(), gomock.Any()).Return(dockerapi.DockerContainerMetadata{DockerID: containerID})\n\tdockerClient.EXPECT().StartContainer(gomock.Any(), containerID, defaultConfig.ContainerStartTimeout).Return(\n\t\tdockerapi.DockerContainerMetadata{DockerID: containerID})\n\n\tcleanup := make(chan time.Time)\n\tdefer close(cleanup)\n\tmockTime.EXPECT().Now().Return(time.Now()).MinTimes(1)\n\tdockerClient.EXPECT().DescribeContainer(gomock.Any(), containerID).AnyTimes()\n\tdockerClient.EXPECT().DescribeContainer(gomock.Any(), pauseContainerID).AnyTimes()\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\ttaskEngine.AddTask(sleepTask)\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\tverifyTaskIsRunning(stateChangeEvents, sleepTask)\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tmockTime.EXPECT().After(gomock.Any()).Return(cleanup).MinTimes(1)\n\tdockerClient.EXPECT().InspectContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(&types.ContainerJSON{\n\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\tID:    pauseContainerID,\n\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t},\n\t}, nil)\n\tcniClient.EXPECT().CleanupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)\n\tdockerClient.EXPECT().StopContainer(gomock.Any(), pauseContainerID, gomock.Any()).Return(\n\t\tdockerapi.DockerContainerMetadata{DockerID: pauseContainerID})\n\tcniClient.EXPECT().ReleaseIPResource(gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx context.Context, cfg *ecscni.Config, timeout time.Duration) {\n\t\t\twg.Done()\n\t\t}).Return(nil)\n\tdockerClient.EXPECT().RemoveContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil).Times(2)\n\timageManager.EXPECT().RemoveContainerReferenceFromImageState(gomock.Any()).Return(nil)\n\n\t// Simulate a container stop event from docker\n\teventStream <- dockerapi.DockerContainerChangeEvent{\n\t\tStatus: apicontainerstatus.ContainerStopped,\n\t\tDockerContainerMetadata: dockerapi.DockerContainerMetadata{\n\t\t\tDockerID: containerID,\n\t\t\tExitCode: aws.Int(exitCode),\n\t\t},\n\t}\n\n\tverifyTaskIsStopped(stateChangeEvents, sleepTask)\n\tsleepTask.SetSentStatus(apitaskstatus.TaskStopped)\n\tcleanup <- time.Now()\n\tfor {\n\t\ttasks, _ := taskEngine.(*DockerTaskEngine).ListTasks()\n\t\tif len(tasks) == 0 {\n\t\t\tbreak\n\t\t}\n\t\tt.Logf(\"Found %d tasks in the engine; first task arn: %s\", len(tasks), tasks[0].Arn)\n\t\tfmt.Printf(\"Found %d tasks in the engine; first task arn: %s\\n\", len(tasks), tasks[0].Arn)\n\t\ttime.Sleep(5 * time.Millisecond)\n\t}\n\twg.Wait()\n}\n\nfunc TestBuildCNIConfigFromTaskContainer(t *testing.T) {\n\tconfig := defaultConfig\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &config)\n\tdefer ctrl.Finish()\n\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\ttestTask.AddTaskENI(mockENI)\n\ttestTask.SetAppMesh(&appmesh.AppMesh{\n\t\tIgnoredUID:       ignoredUID,\n\t\tProxyIngressPort: proxyIngressPort,\n\t\tProxyEgressPort:  proxyEgressPort,\n\t\tAppPorts: []string{\n\t\t\tappPort,\n\t\t},\n\t\tEgressIgnoredIPs: []string{\n\t\t\tegressIgnoredIP,\n\t\t},\n\t})\n\tcontainerInspectOutput := &types.ContainerJSON{\n\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\tID:    containerID,\n\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t},\n\t}\n\n\tcniConfig, err := taskEngine.(*DockerTaskEngine).buildCNIConfigFromTaskContainer(testTask, containerInspectOutput, true)\n\tassert.NoError(t, err)\n\tassert.Equal(t, containerID, cniConfig.ContainerID)\n\tassert.Equal(t, strconv.Itoa(containerPid), cniConfig.ContainerPID)\n\tassert.Equal(t, mac, cniConfig.ID, \"ID should be set to the mac of eni\")\n\t// We expect 3 NetworkConfig objects in the cni Config wrapper object:\n\t// ENI, Bridge and Appmesh\n\trequire.Len(t, cniConfig.NetworkConfigs, 3)\n}\n\nfunc TestProvisionContainerResources(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, dockerClient, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tdataClient, cleanup := newTestDataClient(t)\n\tdefer cleanup()\n\ttaskEngine.SetDataClient(dataClient)\n\n\tmockCNIClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = mockCNIClient\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\tpauseContainer := &apicontainer.Container{\n\t\tName: \"pausecontainer\",\n\t\tType: apicontainer.ContainerCNIPause,\n\t}\n\ttestTask.Containers = append(testTask.Containers, pauseContainer)\n\ttestTask.AddTaskENI(mockENI)\n\tvolRes := &taskresourcevolume.VolumeResource{}\n\ttestTask.ResourcesMapUnsafe = map[string][]taskresource.TaskResource{\n\t\t\"dockerVolume\": {volRes},\n\t}\n\ttaskEngine.(*DockerTaskEngine).State().AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).State().AddContainer(&apicontainer.DockerContainer{\n\t\tDockerID:   containerID,\n\t\tDockerName: dockerContainerName,\n\t\tContainer:  pauseContainer,\n\t}, testTask)\n\n\tgomock.InOrder(\n\t\tdockerClient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).Return(&types.ContainerJSON{\n\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\tID:    containerID,\n\t\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t\t},\n\t\t}, nil),\n\t\tmockCNIClient.EXPECT().SetupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nsResult, nil),\n\t)\n\n\trequire.Nil(t, taskEngine.(*DockerTaskEngine).provisionContainerResources(testTask, pauseContainer).Error)\n\tassert.Equal(t, strconv.Itoa(containerPid), volRes.GetPauseContainerPID())\n\tassert.Equal(t, taskIP, testTask.GetLocalIPAddress())\n\tsavedTasks, err := dataClient.GetTasks()\n\trequire.NoError(t, err)\n\tassert.Len(t, savedTasks, 1)\n}\n\nfunc TestProvisionContainerResourcesInspectError(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, dockerClient, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockCNIClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = mockCNIClient\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\tpauseContainer := &apicontainer.Container{\n\t\tName: \"pausecontainer\",\n\t\tType: apicontainer.ContainerCNIPause,\n\t}\n\ttestTask.Containers = append(testTask.Containers, pauseContainer)\n\ttestTask.AddTaskENI(mockENI)\n\ttaskEngine.(*DockerTaskEngine).State().AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).State().AddContainer(&apicontainer.DockerContainer{\n\t\tDockerID:   containerID,\n\t\tDockerName: dockerContainerName,\n\t\tContainer:  pauseContainer,\n\t}, testTask)\n\n\tdockerClient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).Return(nil, errors.New(\"test error\"))\n\n\tassert.NotNil(t, taskEngine.(*DockerTaskEngine).provisionContainerResources(testTask, pauseContainer).Error)\n}\n\n// TestStopPauseContainerCleanupCalled tests when stopping the pause container\n// its network namespace should be cleaned up first\nfunc TestStopPauseContainerCleanupCalled(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, dockerClient, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockCNIClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = mockCNIClient\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\tpauseContainer := &apicontainer.Container{\n\t\tName: \"pausecontainer\",\n\t\tType: apicontainer.ContainerCNIPause,\n\t}\n\ttestTask.Containers = append(testTask.Containers, pauseContainer)\n\ttestTask.AddTaskENI(mockENI)\n\ttestTask.SetAppMesh(&appmesh.AppMesh{\n\t\tIgnoredUID:       ignoredUID,\n\t\tProxyIngressPort: proxyIngressPort,\n\t\tProxyEgressPort:  proxyEgressPort,\n\t\tAppPorts: []string{\n\t\t\tappPort,\n\t\t},\n\t\tEgressIgnoredIPs: []string{\n\t\t\tegressIgnoredIP,\n\t\t},\n\t})\n\ttaskEngine.(*DockerTaskEngine).State().AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).State().AddContainer(&apicontainer.DockerContainer{\n\t\tDockerID:   containerID,\n\t\tDockerName: dockerContainerName,\n\t\tContainer:  pauseContainer,\n\t}, testTask)\n\n\tgomock.InOrder(\n\t\tdockerClient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).Return(&types.ContainerJSON{\n\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\tID:    containerID,\n\t\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t\t},\n\t\t}, nil),\n\t\tmockCNIClient.EXPECT().CleanupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil),\n\t\tdockerClient.EXPECT().StopContainer(gomock.Any(),\n\t\t\tcontainerID,\n\t\t\tdefaultConfig.DockerStopTimeout,\n\t\t).Return(dockerapi.DockerContainerMetadata{}),\n\t)\n\n\ttaskEngine.(*DockerTaskEngine).stopContainer(testTask, pauseContainer)\n}\n\n// TestStopPauseContainerCleanupCalled tests when stopping the pause container\n// its network namespace should be cleaned up first\nfunc TestStopPauseContainerCleanupDelay(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\n\tcfg := config.DefaultConfig()\n\tcfg.TaskCPUMemLimit.Value = config.ExplicitlyDisabled\n\tcfg.ENIPauseContainerCleanupDelaySeconds = expectedDelaySeconds\n\n\tdelayedChan := make(chan time.Duration, 1)\n\tctrl, dockerClient, _, taskEngine, _, _, _ := mocks(t, ctx, &cfg)\n\ttaskEngine.(*DockerTaskEngine).handleDelay = func(d time.Duration) {\n\t\tdelayedChan <- d\n\t}\n\n\tmockCNIClient := mock_ecscni.NewMockCNIClient(ctrl)\n\ttaskEngine.(*DockerTaskEngine).cniClient = mockCNIClient\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\tpauseContainer := &apicontainer.Container{\n\t\tName: \"pausecontainer\",\n\t\tType: apicontainer.ContainerCNIPause,\n\t}\n\ttestTask.Containers = append(testTask.Containers, pauseContainer)\n\ttestTask.AddTaskENI(mockENI)\n\ttaskEngine.(*DockerTaskEngine).State().AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).State().AddContainer(&apicontainer.DockerContainer{\n\t\tDockerID:   containerID,\n\t\tDockerName: dockerContainerName,\n\t\tContainer:  pauseContainer,\n\t}, testTask)\n\n\tgomock.InOrder(\n\t\tdockerClient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).Return(&types.ContainerJSON{\n\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\tID:    containerID,\n\t\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t\t},\n\t\t}, nil),\n\t\tmockCNIClient.EXPECT().CleanupNS(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil),\n\t\tdockerClient.EXPECT().StopContainer(gomock.Any(),\n\t\t\tcontainerID,\n\t\t\tdefaultConfig.DockerStopTimeout,\n\t\t).Return(dockerapi.DockerContainerMetadata{}),\n\t)\n\n\ttaskEngine.(*DockerTaskEngine).stopContainer(testTask, pauseContainer)\n\n\tselect {\n\tcase actualDelay := <-delayedChan:\n\t\tassert.Equal(t, expectedDelay, actualDelay)\n\tdefault:\n\t\tassert.Fail(t, \"engine.handleDelay wasn't called\")\n\t}\n}\n\n// TestTaskWithCircularDependency tests the task with containers of which the\n// dependencies can't be resolved\nfunc TestTaskWithCircularDependency(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tclient.EXPECT().ContainerEvents(gomock.Any())\n\n\ttask := testdata.LoadTask(\"circular_dependency\")\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tevents := taskEngine.StateChangeEvents()\n\tgo taskEngine.AddTask(task)\n\tevent := <-events\n\tassert.Equal(t, event.(api.TaskStateChange).Status, apitaskstatus.TaskStopped, \"Expected task to move to stopped directly\")\n\t_, ok := taskEngine.(*DockerTaskEngine).state.TaskByArn(task.Arn)\n\tassert.True(t, ok, \"Task state should be added to the agent state\")\n\n\t_, ok = taskEngine.(*DockerTaskEngine).managedTasks[task.Arn]\n\tassert.False(t, ok, \"Task should not be added to task manager for processing\")\n}\n\n// TestCreateContainerOnAgentRestart tests when agent restarts it should use the\n// docker container name restored from agent state file to create the container\nfunc TestCreateContainerOnAgentRestart(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, _, _ := mocks(t, ctx, &config.Config{})\n\tdefer ctrl.Finish()\n\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\tstate := taskEngine.State()\n\tsleepTask := testdata.LoadTask(\"sleep5\")\n\tsleepContainer, _ := sleepTask.ContainerByName(\"sleep5\")\n\t// Store the generated container name to state\n\tstate.AddContainer(&apicontainer.DockerContainer{DockerID: \"dockerID\", DockerName: \"docker_container_name\", Container: sleepContainer}, sleepTask)\n\n\tgomock.InOrder(\n\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil),\n\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), \"docker_container_name\", gomock.Any()),\n\t)\n\n\tmetadata := taskEngine.createContainer(sleepTask, sleepContainer)\n\tif metadata.Error != nil {\n\t\tt.Error(\"Unexpected error\", metadata.Error)\n\t}\n}\n\nfunc TestPullCNIImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, privateTaskEngine, _, _, _ := mocks(t, ctx, &config.Config{})\n\tdefer ctrl.Finish()\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\n\tcontainer := &apicontainer.Container{\n\t\tType: apicontainer.ContainerCNIPause,\n\t}\n\ttask := &apitask.Task{\n\t\tContainers: []*apicontainer.Container{container},\n\t}\n\tmetadata := taskEngine.pullContainer(task, container)\n\tassert.Equal(t, dockerapi.DockerContainerMetadata{}, metadata, \"expected empty metadata\")\n}\n\nfunc TestPullNormalImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, &config.Config{})\n\tdefer ctrl.Finish()\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine._time = nil\n\timageName := \"image\"\n\tcontainer := &apicontainer.Container{\n\t\tType:  apicontainer.ContainerNormal,\n\t\tImage: imageName,\n\t}\n\ttask := &apitask.Task{\n\t\tContainers: []*apicontainer.Container{container},\n\t}\n\timageState := &image.ImageState{\n\t\tImage: &image.Image{ImageID: \"id\"},\n\t}\n\n\tclient.EXPECT().PullImage(gomock.Any(), imageName, nil, gomock.Any())\n\timageManager.EXPECT().RecordContainerReference(container)\n\timageManager.EXPECT().GetImageStateFromImageName(imageName).Return(imageState, true)\n\tmetadata := taskEngine.pullContainer(task, container)\n\tassert.Equal(t, dockerapi.DockerContainerMetadata{}, metadata, \"expected empty metadata\")\n}\n\nfunc TestPullImageWithImagePullOnceBehavior(t *testing.T) {\n\ttestcases := []struct {\n\t\tname          string\n\t\tpullSucceeded bool\n\t}{\n\t\t{\n\t\t\tname:          \"PullSucceeded is true\",\n\t\t\tpullSucceeded: true,\n\t\t},\n\t\t{\n\t\t\tname:          \"PullSucceeded is false\",\n\t\t\tpullSucceeded: false,\n\t\t},\n\t}\n\n\tfor _, tc := range testcases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, &config.Config{ImagePullBehavior: config.ImagePullOnceBehavior})\n\t\t\tdefer ctrl.Finish()\n\t\t\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\t\t\ttaskEngine._time = nil\n\t\t\timageName := \"image\"\n\t\t\tcontainer := &apicontainer.Container{\n\t\t\t\tType:  apicontainer.ContainerNormal,\n\t\t\t\tImage: imageName,\n\t\t\t}\n\t\t\ttask := &apitask.Task{\n\t\t\t\tContainers: []*apicontainer.Container{container},\n\t\t\t}\n\t\t\timageState := &image.ImageState{\n\t\t\t\tImage:         &image.Image{ImageID: \"id\"},\n\t\t\t\tPullSucceeded: tc.pullSucceeded,\n\t\t\t}\n\t\t\tif !tc.pullSucceeded {\n\t\t\t\tclient.EXPECT().PullImage(gomock.Any(), imageName, nil, gomock.Any())\n\t\t\t}\n\t\t\timageManager.EXPECT().RecordContainerReference(container)\n\t\t\timageManager.EXPECT().GetImageStateFromImageName(imageName).Return(imageState, true).Times(2)\n\t\t\tmetadata := taskEngine.pullContainer(task, container)\n\t\t\tassert.Equal(t, dockerapi.DockerContainerMetadata{}, metadata, \"expected empty metadata\")\n\t\t})\n\t}\n}\n\nfunc TestPullImageWithImagePullPreferCachedBehaviorWithCachedImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, &config.Config{ImagePullBehavior: config.ImagePullPreferCachedBehavior})\n\tdefer ctrl.Finish()\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine._time = nil\n\timageName := \"image\"\n\tcontainer := &apicontainer.Container{\n\t\tType:  apicontainer.ContainerNormal,\n\t\tImage: imageName,\n\t}\n\ttask := &apitask.Task{\n\t\tContainers: []*apicontainer.Container{container},\n\t}\n\timageState := &image.ImageState{\n\t\tImage: &image.Image{ImageID: \"id\"},\n\t}\n\tclient.EXPECT().InspectImage(imageName).Return(nil, nil)\n\timageManager.EXPECT().RecordContainerReference(container)\n\timageManager.EXPECT().GetImageStateFromImageName(imageName).Return(imageState, true)\n\tmetadata := taskEngine.pullContainer(task, container)\n\tassert.Equal(t, dockerapi.DockerContainerMetadata{}, metadata, \"expected empty metadata\")\n}\n\nfunc TestPullImageWithImagePullPreferCachedBehaviorWithoutCachedImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, &config.Config{ImagePullBehavior: config.ImagePullPreferCachedBehavior})\n\tdefer ctrl.Finish()\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine._time = nil\n\timageName := \"image\"\n\tcontainer := &apicontainer.Container{\n\t\tType:  apicontainer.ContainerNormal,\n\t\tImage: imageName,\n\t}\n\ttask := &apitask.Task{\n\t\tContainers: []*apicontainer.Container{container},\n\t}\n\timageState := &image.ImageState{\n\t\tImage: &image.Image{ImageID: \"id\"},\n\t}\n\tclient.EXPECT().InspectImage(imageName).Return(nil, errors.New(\"error\"))\n\tclient.EXPECT().PullImage(gomock.Any(), imageName, nil, gomock.Any())\n\timageManager.EXPECT().RecordContainerReference(container)\n\timageManager.EXPECT().GetImageStateFromImageName(imageName).Return(imageState, true)\n\tmetadata := taskEngine.pullContainer(task, container)\n\tassert.Equal(t, dockerapi.DockerContainerMetadata{}, metadata, \"expected empty metadata\")\n}\n\nfunc TestUpdateContainerReference(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, &config.Config{})\n\tdefer ctrl.Finish()\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine._time = nil\n\timageName := \"image\"\n\tcontainer := &apicontainer.Container{\n\t\tType:  apicontainer.ContainerNormal,\n\t\tImage: imageName,\n\t}\n\ttask := &apitask.Task{\n\t\tContainers: []*apicontainer.Container{container},\n\t}\n\timageState := &image.ImageState{\n\t\tImage: &image.Image{ImageID: \"id\"},\n\t}\n\n\timageManager.EXPECT().RecordContainerReference(container)\n\timageManager.EXPECT().GetImageStateFromImageName(imageName).Return(imageState, true)\n\ttaskEngine.updateContainerReference(true, container, task.Arn)\n\tassert.True(t, imageState.PullSucceeded, \"PullSucceeded set to false\")\n}\n\n// TestMetadataFileUpdatedAgentRestart checks whether metadataManager.Update(...) is\n// invoked in the path DockerTaskEngine.Init() -> .synchronizeState() -> .updateMetadataFile(...)\n// for the following case:\n// agent starts, container created, metadata file created, agent restarted, container recovered\n// during task engine init, metadata file updated\nfunc TestMetadataFileUpdatedAgentRestart(t *testing.T) {\n\tconf := &defaultConfig\n\tconf.ContainerMetadataEnabled = config.BooleanDefaultFalse{Value: config.ExplicitlyEnabled}\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, imageManager, metadataManager := mocks(t, ctx, conf)\n\tdefer ctrl.Finish()\n\n\tvar metadataUpdateWG sync.WaitGroup\n\tmetadataUpdateWG.Add(1)\n\ttaskEngine, _ := privateTaskEngine.(*DockerTaskEngine)\n\tassert.True(t, taskEngine.cfg.ContainerMetadataEnabled.Enabled(), \"ContainerMetadataEnabled set to false.\")\n\n\ttaskEngine._time = nil\n\tstate := taskEngine.State()\n\ttask := testdata.LoadTask(\"sleep5\")\n\tcontainer, _ := task.ContainerByName(\"sleep5\")\n\tassert.False(t, container.MetadataFileUpdated)\n\tcontainer.SetKnownStatus(apicontainerstatus.ContainerRunning)\n\tdockerContainer := &apicontainer.DockerContainer{DockerID: containerID, Container: container}\n\texpectedTaskARN := task.Arn\n\texpectedDockerID := dockerContainer.DockerID\n\texpectedContainerName := container.Name\n\n\tstate.AddTask(task)\n\tstate.AddContainer(dockerContainer, task)\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tclient.EXPECT().DescribeContainer(gomock.Any(), gomock.Any())\n\timageManager.EXPECT().RecordContainerReference(gomock.Any())\n\n\tmetadataManager.EXPECT().Update(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, dockerID string, task *apitask.Task, containerName string) {\n\t\t\tassert.Equal(t, expectedTaskARN, task.Arn)\n\t\t\tassert.Equal(t, expectedContainerName, containerName)\n\t\t\tassert.Equal(t, expectedDockerID, dockerID)\n\t\t\tmetadataUpdateWG.Done()\n\t\t})\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\tdefer taskEngine.Disable()\n\tmetadataUpdateWG.Wait()\n}\n\n// TestTaskUseExecutionRolePullECRImage tests the agent will use the execution role\n// credentials to pull from an ECR repository\nfunc TestTaskUseExecutionRolePullECRImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, credentialsManager, imageManager, _ := mocks(\n\t\tt, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tcredentialsID := \"execution role\"\n\taccessKeyID := \"akid\"\n\tsecretAccessKey := \"sakid\"\n\tsessionToken := \"token\"\n\texecutionRoleCredentials := credentials.IAMRoleCredentials{\n\t\tCredentialsID:   credentialsID,\n\t\tAccessKeyID:     accessKeyID,\n\t\tSecretAccessKey: secretAccessKey,\n\t\tSessionToken:    sessionToken,\n\t}\n\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\t// Configure the task and container to use execution role\n\ttestTask.SetExecutionRoleCredentialsID(credentialsID)\n\ttestTask.Containers[0].RegistryAuthentication = &apicontainer.RegistryAuthenticationData{\n\t\tType: \"ecr\",\n\t\tECRAuthData: &apicontainer.ECRAuthData{\n\t\t\tUseExecutionRole: true,\n\t\t},\n\t}\n\tcontainer := testTask.Containers[0]\n\n\tmockTime.EXPECT().Now().AnyTimes()\n\tcredentialsManager.EXPECT().GetTaskCredentials(credentialsID).Return(credentials.TaskIAMRoleCredentials{\n\t\tARN:                \"\",\n\t\tIAMRoleCredentials: executionRoleCredentials,\n\t}, true)\n\tclient.EXPECT().PullImage(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, image string, auth *apicontainer.RegistryAuthenticationData, timeout interface{}) {\n\t\t\tassert.Equal(t, container.Image, image)\n\t\t\tassert.Equal(t, auth.ECRAuthData.GetPullCredentials(), executionRoleCredentials)\n\t\t}).Return(dockerapi.DockerContainerMetadata{})\n\timageManager.EXPECT().RecordContainerReference(container).Return(nil)\n\timageManager.EXPECT().GetImageStateFromImageName(container.Image)\n\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n}\n\n// TestTaskUseExecutionRolePullPrivateRegistryImage tests the agent will use the\n// execution role credentials to pull from a private repository\nfunc TestTaskUseExecutionRolePullPrivateRegistryImage(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, credentialsManager, imageManager, _ := mocks(\n\t\tt, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tcredentialsID := \"execution role\"\n\taccessKeyID := \"akid\"\n\tsecretAccessKey := \"sakid\"\n\tsessionToken := \"token\"\n\texecutionRoleCredentials := credentials.IAMRoleCredentials{\n\t\tCredentialsID:   credentialsID,\n\t\tAccessKeyID:     accessKeyID,\n\t\tSecretAccessKey: secretAccessKey,\n\t\tSessionToken:    sessionToken,\n\t}\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\t// Configure the task and container to use execution role\n\ttestTask.SetExecutionRoleCredentialsID(credentialsID)\n\tasmAuthData := &apicontainer.ASMAuthData{\n\t\tCredentialsParameter: secretID,\n\t\tRegion:               region,\n\t}\n\ttestTask.Containers[0].RegistryAuthentication = &apicontainer.RegistryAuthenticationData{\n\t\tType:        \"asm\",\n\t\tASMAuthData: asmAuthData,\n\t}\n\trequiredASMResources := []*apicontainer.ASMAuthData{asmAuthData}\n\tasmClientCreator := mock_asm_factory.NewMockClientCreator(ctrl)\n\tasmAuthRes := asmauth.NewASMAuthResource(testTask.Arn, requiredASMResources,\n\t\tcredentialsID, credentialsManager, asmClientCreator)\n\ttestTask.ResourcesMapUnsafe = map[string][]taskresource.TaskResource{\n\t\tasmauth.ResourceName: {asmAuthRes},\n\t}\n\tmockASMClient := mock_secretsmanageriface.NewMockSecretsManagerAPI(ctrl)\n\tasmAuthDataBytes, _ := json.Marshal(&asm.AuthDataValue{\n\t\tUsername: aws.String(username),\n\t\tPassword: aws.String(password),\n\t})\n\tasmAuthDataVal := string(asmAuthDataBytes)\n\tasmSecretValue := &secretsmanager.GetSecretValueOutput{\n\t\tSecretString: aws.String(asmAuthDataVal),\n\t}\n\n\tgomock.InOrder(\n\t\tcredentialsManager.EXPECT().GetTaskCredentials(credentialsID).Return(\n\t\t\tcredentials.TaskIAMRoleCredentials{\n\t\t\t\tARN:                \"\",\n\t\t\t\tIAMRoleCredentials: executionRoleCredentials,\n\t\t\t}, true),\n\t\tasmClientCreator.EXPECT().NewASMClient(region, executionRoleCredentials).Return(mockASMClient),\n\t\tmockASMClient.EXPECT().GetSecretValue(gomock.Any()).Return(asmSecretValue, nil),\n\t)\n\trequire.NoError(t, asmAuthRes.Create())\n\tcontainer := testTask.Containers[0]\n\n\tmockTime.EXPECT().Now().AnyTimes()\n\tclient.EXPECT().PullImage(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, image string, auth *apicontainer.RegistryAuthenticationData, timeout interface{}) {\n\t\t\tassert.Equal(t, container.Image, image)\n\t\t\tdac := auth.ASMAuthData.GetDockerAuthConfig()\n\t\t\tassert.Equal(t, username, dac.Username)\n\t\t\tassert.Equal(t, password, dac.Password)\n\t\t}).Return(dockerapi.DockerContainerMetadata{})\n\timageManager.EXPECT().RecordContainerReference(container).Return(nil)\n\timageManager.EXPECT().GetImageStateFromImageName(container.Image)\n\n\tret := taskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\tassert.Nil(t, ret.Error)\n}\n\n// TestTaskUseExecutionRolePullPrivateRegistryImageNoASMResource tests the\n// docker task engine code path for returning error for missing ASM resource\nfunc TestTaskUseExecutionRolePullPrivateRegistryImageNoASMResource(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, mockTime, taskEngine, _, _, _ := mocks(\n\t\tt, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\t// Configure the task and container to use execution role\n\ttestTask.SetExecutionRoleCredentialsID(credentialsID)\n\tasmAuthData := &apicontainer.ASMAuthData{\n\t\tCredentialsParameter: secretID,\n\t\tRegion:               region,\n\t}\n\ttestTask.Containers[0].RegistryAuthentication = &apicontainer.RegistryAuthenticationData{\n\t\tType:        \"asm\",\n\t\tASMAuthData: asmAuthData,\n\t}\n\n\t// no asm auth resource in task\n\ttestTask.ResourcesMapUnsafe = map[string][]taskresource.TaskResource{}\n\n\tcontainer := testTask.Containers[0]\n\tmockTime.EXPECT().Now().AnyTimes()\n\n\t// ensure pullContainer returns error\n\tret := taskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\tassert.NotNil(t, ret.Error)\n}\n\n// TestNewTaskTransitionOnRestart tests the agent will process the task recorded in\n// the state file on restart\nfunc TestNewTaskTransitionOnRestart(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockTime.EXPECT().Now().AnyTimes()\n\tclient.EXPECT().Version(gomock.Any(), gomock.Any()).MaxTimes(1)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).MaxTimes(1)\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\tstate := dockerTaskEngine.State()\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\t// add the task to the state to simulate the agent restored the state on restart\n\tstate.AddTask(testTask)\n\t// Set the task to be stopped so that the process can done quickly\n\ttestTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tdockerTaskEngine.synchronizeState()\n\t_, ok := dockerTaskEngine.managedTasks[testTask.Arn]\n\tassert.True(t, ok, \"task wasnot started\")\n}\n\n// TestTaskWaitForHostResourceOnRestart tests task stopped by acs but hasn't\n// reached stopped should block the later task to start\nfunc TestTaskWaitForHostResourceOnRestart(t *testing.T) {\n\t// Task 1 stopped by backend\n\ttaskStoppedByACS := testdata.LoadTask(\"sleep5\")\n\ttaskStoppedByACS.SetDesiredStatus(apitaskstatus.TaskStopped)\n\ttaskStoppedByACS.SetStopSequenceNumber(1)\n\ttaskStoppedByACS.SetKnownStatus(apitaskstatus.TaskRunning)\n\t// Task 2 has essential container stopped\n\ttaskEssentialContainerStopped := testdata.LoadTask(\"sleep5\")\n\ttaskEssentialContainerStopped.Arn = \"task_Essential_Container_Stopped\"\n\ttaskEssentialContainerStopped.SetDesiredStatus(apitaskstatus.TaskStopped)\n\ttaskEssentialContainerStopped.SetKnownStatus(apitaskstatus.TaskRunning)\n\t// Normal task 3 needs to be started\n\ttaskNotStarted := testdata.LoadTask(\"sleep5\")\n\ttaskNotStarted.Arn = \"task_Not_started\"\n\n\tconf := &defaultConfig\n\tconf.ContainerMetadataEnabled = config.BooleanDefaultFalse{Value: config.ExplicitlyDisabled}\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, privateTaskEngine, _, imageManager, _ := mocks(t, ctx, conf)\n\tdefer ctrl.Finish()\n\n\tclient.EXPECT().Version(gomock.Any(), gomock.Any()).MaxTimes(1)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).MaxTimes(1)\n\n\terr := privateTaskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\ttaskEngine := privateTaskEngine.(*DockerTaskEngine)\n\ttaskEngine.State().AddTask(taskStoppedByACS)\n\ttaskEngine.State().AddTask(taskNotStarted)\n\ttaskEngine.State().AddTask(taskEssentialContainerStopped)\n\n\ttaskEngine.State().AddContainer(&apicontainer.DockerContainer{\n\t\tContainer:  taskStoppedByACS.Containers[0],\n\t\tDockerID:   containerID + \"1\",\n\t\tDockerName: dockerContainerName + \"1\",\n\t}, taskStoppedByACS)\n\ttaskEngine.State().AddContainer(&apicontainer.DockerContainer{\n\t\tContainer:  taskNotStarted.Containers[0],\n\t\tDockerID:   containerID + \"2\",\n\t\tDockerName: dockerContainerName + \"2\",\n\t}, taskNotStarted)\n\ttaskEngine.State().AddContainer(&apicontainer.DockerContainer{\n\t\tContainer:  taskEssentialContainerStopped.Containers[0],\n\t\tDockerID:   containerID + \"3\",\n\t\tDockerName: dockerContainerName + \"3\",\n\t}, taskEssentialContainerStopped)\n\n\t// these are performed in synchronizeState on restart\n\tclient.EXPECT().DescribeContainer(gomock.Any(), gomock.Any()).Return(apicontainerstatus.ContainerRunning, dockerapi.DockerContainerMetadata{\n\t\tDockerID: containerID,\n\t}).Times(3)\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).Times(3)\n\n\t// start the two tasks\n\ttaskEngine.synchronizeState()\n\n\tvar waitStopWG sync.WaitGroup\n\twaitStopWG.Add(1)\n\tgo func() {\n\t\t// This is to confirm the other task is waiting\n\t\ttime.Sleep(1 * time.Second)\n\t\t// Remove the task sequence number 1 from waitgroup\n\t\ttaskEngine.taskStopGroup.Done(1)\n\t\twaitStopWG.Done()\n\t}()\n\n\t// task with sequence number 2 should wait until 1 is removed from the waitgroup\n\ttaskEngine.taskStopGroup.Wait(2)\n\twaitStopWG.Wait()\n}\n\n// TestPullStartedStoppedAtWasSetCorrectly tests the PullStartedAt and PullStoppedAt\n// was set correctly\nfunc TestPullStartedStoppedAtWasSetCorrectly(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttestTask := &apitask.Task{\n\t\tArn: \"taskArn\",\n\t}\n\tcontainer := &apicontainer.Container{\n\t\tImage: \"image1\",\n\t}\n\tstartTime1 := time.Now()\n\tstartTime2 := startTime1.Add(time.Second)\n\tstartTime3 := startTime2.Add(time.Second)\n\tstopTime1 := startTime3.Add(time.Second)\n\tstopTime2 := stopTime1.Add(time.Second)\n\tstopTime3 := stopTime2.Add(time.Second)\n\n\tclient.EXPECT().PullImage(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(3)\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).Times(3)\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false).Times(3)\n\n\tgomock.InOrder(\n\t\t// three container pull start timestamp\n\t\tmockTime.EXPECT().Now().Return(startTime1),\n\t\tmockTime.EXPECT().Now().Return(startTime2),\n\t\tmockTime.EXPECT().Now().Return(startTime3),\n\n\t\t// threre container pull stop timestamp\n\t\tmockTime.EXPECT().Now().Return(stopTime1),\n\t\tmockTime.EXPECT().Now().Return(stopTime2),\n\t\tmockTime.EXPECT().Now().Return(stopTime3),\n\t)\n\n\t// Pull three images, the PullStartedAt should be the pull of the first container\n\t// and PullStoppedAt should be the pull completion of the last container\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\n\tassert.Equal(t, testTask.PullStartedAtUnsafe, startTime1)\n\tassert.Equal(t, testTask.PullStoppedAtUnsafe, stopTime3)\n}\n\n// TestPullStoppedAtWasSetCorrectlyWhenPullFail tests the PullStoppedAt was set\n// correctly when the pull failed\nfunc TestPullStoppedAtWasSetCorrectlyWhenPullFail(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\ttestTask := &apitask.Task{\n\t\tArn: \"taskArn\",\n\t}\n\tcontainer := &apicontainer.Container{\n\t\tImage: \"image1\",\n\t}\n\n\tstartTime1 := time.Now()\n\tstartTime2 := startTime1.Add(time.Second)\n\tstartTime3 := startTime2.Add(time.Second)\n\tstopTime1 := startTime3.Add(time.Second)\n\tstopTime2 := stopTime1.Add(time.Second)\n\tstopTime3 := stopTime2.Add(time.Second)\n\n\tgomock.InOrder(\n\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{}),\n\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(dockerapi.DockerContainerMetadata{}),\n\t\tclient.EXPECT().PullImage(gomock.Any(), container.Image, nil, gomock.Any()).Return(\n\t\t\tdockerapi.DockerContainerMetadata{Error: dockerapi.CannotPullContainerError{fmt.Errorf(\"error\")}}),\n\t)\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).Times(3)\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false).Times(3)\n\tgomock.InOrder(\n\t\t// three container pull start timestamp\n\t\tmockTime.EXPECT().Now().Return(startTime1),\n\t\tmockTime.EXPECT().Now().Return(startTime2),\n\t\tmockTime.EXPECT().Now().Return(startTime3),\n\n\t\t// threre container pull stop timestamp\n\t\tmockTime.EXPECT().Now().Return(stopTime1),\n\t\tmockTime.EXPECT().Now().Return(stopTime2),\n\t\tmockTime.EXPECT().Now().Return(stopTime3),\n\t)\n\n\t// Pull three images, the PullStartedAt should be the pull of the first container\n\t// and PullStoppedAt should be the pull completion of the last container\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\ttaskEngine.(*DockerTaskEngine).pullContainer(testTask, container)\n\n\tassert.Equal(t, testTask.PullStartedAtUnsafe, startTime1)\n\tassert.Equal(t, testTask.PullStoppedAtUnsafe, stopTime3)\n}\n\nfunc TestSynchronizeContainerStatus(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tdockerID := \"1234\"\n\tdockerContainer := &apicontainer.DockerContainer{\n\t\tDockerID:   dockerID,\n\t\tDockerName: \"c1\",\n\t\tContainer:  &apicontainer.Container{},\n\t}\n\tlabels := map[string]string{\n\t\t\"name\": \"metadata\",\n\t}\n\tvolumes := []types.MountPoint{\n\t\t{\n\t\t\tName:        \"volume\",\n\t\t\tSource:      \"/src/vol\",\n\t\t\tDestination: \"/vol\",\n\t\t},\n\t}\n\tcreated := time.Now()\n\tgomock.InOrder(\n\t\tclient.EXPECT().DescribeContainer(gomock.Any(), dockerID).Return(apicontainerstatus.ContainerRunning,\n\t\t\tdockerapi.DockerContainerMetadata{\n\t\t\t\tLabels:    labels,\n\t\t\t\tDockerID:  dockerID,\n\t\t\t\tCreatedAt: created,\n\t\t\t\tVolumes:   volumes,\n\t\t\t}),\n\t\timageManager.EXPECT().RecordContainerReference(dockerContainer.Container),\n\t)\n\ttaskEngine.(*DockerTaskEngine).synchronizeContainerStatus(dockerContainer, nil)\n\tassert.Equal(t, created, dockerContainer.Container.GetCreatedAt())\n\tassert.Equal(t, labels, dockerContainer.Container.GetLabels())\n\tassert.Equal(t, volumes, dockerContainer.Container.GetVolumes())\n}\n\n// TestHandleDockerHealthEvent tests the docker health event will only cause the\n// container health status change\nfunc TestHandleDockerHealthEvent(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tstate := taskEngine.(*DockerTaskEngine).State()\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\ttestContainer := testTask.Containers[0]\n\ttestContainer.HealthCheckType = \"docker\"\n\n\tstate.AddTask(testTask)\n\tstate.AddContainer(&apicontainer.DockerContainer{DockerID: \"id\",\n\t\tDockerName: \"container_name\",\n\t\tContainer:  testContainer,\n\t}, testTask)\n\n\ttaskEngine.(*DockerTaskEngine).handleDockerEvent(dockerapi.DockerContainerChangeEvent{\n\t\tStatus: apicontainerstatus.ContainerRunning,\n\t\tType:   apicontainer.ContainerHealthEvent,\n\t\tDockerContainerMetadata: dockerapi.DockerContainerMetadata{\n\t\t\tDockerID: \"id\",\n\t\t\tHealth: apicontainer.HealthStatus{\n\t\t\t\tStatus: apicontainerstatus.ContainerHealthy,\n\t\t\t},\n\t\t},\n\t})\n\tassert.Equal(t, testContainer.Health.Status, apicontainerstatus.ContainerHealthy)\n}\n\nfunc TestContainerMetadataUpdatedOnRestart(t *testing.T) {\n\tdockerID := \"dockerID_created\"\n\tlabels := map[string]string{\n\t\t\"name\": \"metadata\",\n\t}\n\ttestCases := []struct {\n\t\tstage        string\n\t\tstatus       apicontainerstatus.ContainerStatus\n\t\tcreated      time.Time\n\t\tstarted      time.Time\n\t\tfinished     time.Time\n\t\tportBindings []apicontainer.PortBinding\n\t\texitCode     *int\n\t\terr          dockerapi.DockerStateError\n\t}{\n\t\t{\n\t\t\tstage:   \"created\",\n\t\t\tstatus:  apicontainerstatus.ContainerCreated,\n\t\t\tcreated: time.Now(),\n\t\t},\n\t\t{\n\t\t\tstage:   \"started\",\n\t\t\tstatus:  apicontainerstatus.ContainerRunning,\n\t\t\tstarted: time.Now(),\n\t\t\tportBindings: []apicontainer.PortBinding{\n\t\t\t\t{\n\t\t\t\t\tContainerPort: 80,\n\t\t\t\t\tHostPort:      80,\n\t\t\t\t\tBindIP:        \"0.0.0.0/0\",\n\t\t\t\t\tProtocol:      apicontainer.TransportProtocolTCP,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tstage:    \"stopped\",\n\t\t\tfinished: time.Now(),\n\t\t\texitCode: aws.Int(1),\n\t\t},\n\t\t{\n\t\t\tstage:    \"failed\",\n\t\t\tstatus:   apicontainerstatus.ContainerStopped,\n\t\t\terr:      dockerapi.NewDockerStateError(\"error\"),\n\t\t\texitCode: aws.Int(1),\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(fmt.Sprintf(\"Agent restarted during container: %s\", tc.stage), func(t *testing.T) {\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\n\t\t\tctrl, client, _, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\t\t\tdefer ctrl.Finish()\n\t\t\tdockerContainer := &apicontainer.DockerContainer{\n\t\t\t\tDockerID:   dockerID,\n\t\t\t\tDockerName: fmt.Sprintf(\"docker%s\", tc.stage),\n\t\t\t\tContainer:  &apicontainer.Container{},\n\t\t\t}\n\t\t\ttask := &apitask.Task{}\n\n\t\t\tif tc.stage == \"created\" {\n\t\t\t\tdockerContainer.DockerID = \"\"\n\t\t\t\ttask.Volumes = []apitask.TaskVolume{\n\t\t\t\t\t{\n\t\t\t\t\t\tName:   \"empty\",\n\t\t\t\t\t\tVolume: &taskresourcevolume.LocalDockerVolume{},\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tclient.EXPECT().InspectContainer(gomock.Any(), dockerContainer.DockerName, gomock.Any()).Return(&types.ContainerJSON{\n\t\t\t\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\t\t\t\tID:      dockerID,\n\t\t\t\t\t\tCreated: (tc.created).Format(time.RFC3339),\n\t\t\t\t\t\tState: &types.ContainerState{\n\t\t\t\t\t\t\tHealth: &types.Health{},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tConfig: &dockercontainer.Config{\n\t\t\t\t\t\tLabels: labels,\n\t\t\t\t\t},\n\t\t\t\t}, nil)\n\t\t\t\timageManager.EXPECT().RecordContainerReference(dockerContainer.Container).AnyTimes()\n\t\t\t} else {\n\t\t\t\tclient.EXPECT().DescribeContainer(gomock.Any(), dockerID).Return(tc.status, dockerapi.DockerContainerMetadata{\n\t\t\t\t\tLabels:       labels,\n\t\t\t\t\tDockerID:     dockerID,\n\t\t\t\t\tCreatedAt:    tc.created,\n\t\t\t\t\tStartedAt:    tc.started,\n\t\t\t\t\tFinishedAt:   tc.finished,\n\t\t\t\t\tPortBindings: tc.portBindings,\n\t\t\t\t\tExitCode:     tc.exitCode,\n\t\t\t\t\tError:        tc.err,\n\t\t\t\t})\n\t\t\t\timageManager.EXPECT().RecordContainerReference(dockerContainer.Container).AnyTimes()\n\t\t\t}\n\n\t\t\ttaskEngine.(*DockerTaskEngine).synchronizeContainerStatus(dockerContainer, task)\n\t\t\tassert.Equal(t, labels, dockerContainer.Container.GetLabels())\n\t\t\tassert.Equal(t, (tc.created).Format(time.RFC3339), (dockerContainer.Container.GetCreatedAt()).Format(time.RFC3339))\n\t\t\tassert.Equal(t, (tc.started).Format(time.RFC3339), (dockerContainer.Container.GetStartedAt()).Format(time.RFC3339))\n\t\t\tassert.Equal(t, (tc.finished).Format(time.RFC3339), (dockerContainer.Container.GetFinishedAt()).Format(time.RFC3339))\n\t\t\tif tc.stage == \"started\" {\n\t\t\t\tassert.Equal(t, uint16(80), dockerContainer.Container.KnownPortBindingsUnsafe[0].ContainerPort)\n\t\t\t}\n\t\t\tif tc.stage == \"finished\" {\n\t\t\t\tassert.False(t, task.GetExecutionStoppedAt().IsZero())\n\t\t\t\tassert.Equal(t, tc.exitCode, dockerContainer.Container.GetKnownExitCode())\n\t\t\t}\n\t\t\tif tc.stage == \"failed\" {\n\t\t\t\tassert.Equal(t, tc.exitCode, dockerContainer.Container.GetKnownExitCode())\n\t\t\t\tassert.NotNil(t, dockerContainer.Container.ApplyingError)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestContainerProgressParallize tests the container can be processed parallelly\nfunc TestContainerProgressParallize(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, testTime, taskEngine, _, imageManager, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\teventStream := make(chan dockerapi.DockerContainerChangeEvent)\n\tstate := taskEngine.(*DockerTaskEngine).State()\n\n\tfastPullImage := \"fast-pull-image\"\n\tslowPullImage := \"slow-pull-image\"\n\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\n\tcontainerTwo := &apicontainer.Container{\n\t\tName:  fastPullImage,\n\t\tImage: fastPullImage,\n\t}\n\n\ttestTask.Containers = append(testTask.Containers, containerTwo)\n\ttestTask.Containers[0].Image = slowPullImage\n\ttestTask.Containers[0].Name = slowPullImage\n\n\tvar fastContainerDockerName string\n\tvar slowContainerDockerName string\n\tfastContainerDockerID := \"fast-pull-container-id\"\n\tslowContainerDockerID := \"slow-pull-container-id\"\n\n\tvar waitForFastPullContainer sync.WaitGroup\n\twaitForFastPullContainer.Add(1)\n\n\tclient.EXPECT().Version(gomock.Any(), gomock.Any()).Return(\"17.12.0\", nil).AnyTimes()\n\ttestTime.EXPECT().Now().Return(time.Now()).AnyTimes()\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\timageManager.EXPECT().AddAllImageStates(gomock.Any()).AnyTimes()\n\timageManager.EXPECT().RecordContainerReference(gomock.Any()).Return(nil).AnyTimes()\n\timageManager.EXPECT().GetImageStateFromImageName(gomock.Any()).Return(nil, false).AnyTimes()\n\tclient.EXPECT().ContainerEvents(gomock.Any()).Return(eventStream, nil)\n\tclient.EXPECT().PullImage(gomock.Any(), fastPullImage, gomock.Any(), gomock.Any())\n\tclient.EXPECT().PullImage(gomock.Any(), slowPullImage, gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, image interface{}, auth interface{}, timeout interface{}) {\n\t\t\twaitForFastPullContainer.Wait()\n\t\t})\n\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx interface{}, cfg interface{}, hostconfig interface{}, name string, duration interface{}) {\n\t\t\tif strings.Contains(name, slowPullImage) {\n\t\t\t\tslowContainerDockerName = name\n\t\t\t\tstate.AddContainer(&apicontainer.DockerContainer{\n\t\t\t\t\tDockerID:   slowContainerDockerID,\n\t\t\t\t\tDockerName: slowContainerDockerName,\n\t\t\t\t\tContainer:  testTask.Containers[0],\n\t\t\t\t}, testTask)\n\t\t\t\tgo func() {\n\t\t\t\t\tevent := createDockerEvent(apicontainerstatus.ContainerCreated)\n\t\t\t\t\tevent.DockerID = slowContainerDockerID\n\t\t\t\t\teventStream <- event\n\t\t\t\t}()\n\t\t\t} else if strings.Contains(name, fastPullImage) {\n\t\t\t\tfastContainerDockerName = name\n\t\t\t\tstate.AddTask(testTask)\n\t\t\t\tstate.AddContainer(&apicontainer.DockerContainer{\n\t\t\t\t\tDockerID:   fastContainerDockerID,\n\t\t\t\t\tDockerName: fastContainerDockerName,\n\t\t\t\t\tContainer:  testTask.Containers[1],\n\t\t\t\t}, testTask)\n\t\t\t\tgo func() {\n\t\t\t\t\tevent := createDockerEvent(apicontainerstatus.ContainerCreated)\n\t\t\t\t\tevent.DockerID = fastContainerDockerID\n\t\t\t\t\teventStream <- event\n\t\t\t\t}()\n\t\t\t} else {\n\t\t\t\tt.Fatalf(\"Got unexpected name for creating container: %s\", name)\n\t\t\t}\n\t\t}).Times(2)\n\tclient.EXPECT().StartContainer(gomock.Any(), fastContainerDockerID, gomock.Any()).Do(\n\t\tfunc(ctx interface{}, id string, duration interface{}) {\n\t\t\tgo func() {\n\t\t\t\tevent := createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\tevent.DockerID = fastContainerDockerID\n\t\t\t\teventStream <- event\n\t\t\t}()\n\t\t})\n\tclient.EXPECT().StartContainer(gomock.Any(), slowContainerDockerID, gomock.Any()).Do(\n\t\tfunc(ctx interface{}, id string, duration interface{}) {\n\t\t\tgo func() {\n\t\t\t\tevent := createDockerEvent(apicontainerstatus.ContainerRunning)\n\t\t\t\tevent.DockerID = slowContainerDockerID\n\t\t\t\teventStream <- event\n\t\t\t}()\n\t\t})\n\n\ttaskEngine.Init(ctx)\n\ttaskEngine.AddTask(testTask)\n\n\t// Expect the fast pulled container to be running firs\n\tfastPullContainerRunning := false\n\tfor event := range stateChangeEvents {\n\t\tcontainerEvent, ok := event.(api.ContainerStateChange)\n\t\tif ok && containerEvent.Status == apicontainerstatus.ContainerRunning {\n\t\t\tif containerEvent.ContainerName == fastPullImage {\n\t\t\t\tfastPullContainerRunning = true\n\t\t\t\t// The second container should start processing now\n\t\t\t\twaitForFastPullContainer.Done()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tassert.True(t, fastPullContainerRunning, \"got the slower pulled container running events first\")\n\t\t\tcontinue\n\t\t}\n\n\t\ttaskEvent, ok := event.(api.TaskStateChange)\n\t\tif ok && taskEvent.Status == apitaskstatus.TaskRunning {\n\t\t\tbreak\n\t\t}\n\t\tt.Errorf(\"Got unexpected task event: %v\", taskEvent.String())\n\t}\n\tdefer discardEvents(stateChangeEvents)()\n\t// stop and clean up the task\n\tcleanup := make(chan time.Time)\n\tclient.EXPECT().StopContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\tdockerapi.DockerContainerMetadata{DockerID: fastContainerDockerID}).AnyTimes()\n\tclient.EXPECT().StopContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\tdockerapi.DockerContainerMetadata{DockerID: slowContainerDockerID}).AnyTimes()\n\ttestTime.EXPECT().After(gomock.Any()).Return(cleanup).MinTimes(1)\n\tclient.EXPECT().RemoveContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil).Times(2)\n\timageManager.EXPECT().RemoveContainerReferenceFromImageState(gomock.Any()).Return(nil).Times(2)\n\n\tcontainerStoppedEvent := createDockerEvent(apicontainerstatus.ContainerStopped)\n\tcontainerStoppedEvent.DockerID = slowContainerDockerID\n\teventStream <- containerStoppedEvent\n\n\ttestTask.SetSentStatus(apitaskstatus.TaskStopped)\n\tcleanup <- time.Now()\n\tfor {\n\t\ttasks, _ := taskEngine.(*DockerTaskEngine).ListTasks()\n\t\tif len(tasks) == 0 {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(5 * time.Millisecond)\n\t}\n}\n\nfunc TestSynchronizeResource(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockTime.EXPECT().Now().AnyTimes()\n\tclient.EXPECT().Version(gomock.Any(), gomock.Any()).MaxTimes(1)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).MaxTimes(1)\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\tstate := dockerTaskEngine.State()\n\tcgroupResource := mock_taskresource.NewMockTaskResource(ctrl)\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\ttestTask.ResourcesMapUnsafe = map[string][]taskresource.TaskResource{\n\t\t\"cgroup\": {\n\t\t\tcgroupResource,\n\t\t},\n\t}\n\t// add the task to the state to simulate the agent restored the state on restart\n\tstate.AddTask(testTask)\n\tcgroupResource.EXPECT().Initialize(gomock.Any(), gomock.Any(), gomock.Any())\n\tcgroupResource.EXPECT().SetDesiredStatus(gomock.Any()).MaxTimes(1)\n\tcgroupResource.EXPECT().GetDesiredStatus().MaxTimes(2)\n\tcgroupResource.EXPECT().TerminalStatus().MaxTimes(1)\n\tcgroupResource.EXPECT().SteadyState().MaxTimes(1)\n\tcgroupResource.EXPECT().GetKnownStatus().MaxTimes(1)\n\tcgroupResource.EXPECT().GetName().AnyTimes().Return(\"cgroup\")\n\tcgroupResource.EXPECT().StatusString(gomock.Any()).AnyTimes()\n\n\t// Set the task to be stopped so that the process can done quickly\n\ttestTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tdockerTaskEngine.synchronizeState()\n}\n\nfunc TestSynchronizeENIAttachment(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, mockTime, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tmockTime.EXPECT().Now().AnyTimes()\n\tclient.EXPECT().Version(gomock.Any(), gomock.Any()).MaxTimes(1)\n\tclient.EXPECT().ContainerEvents(gomock.Any()).MaxTimes(1)\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\tstate := dockerTaskEngine.State()\n\ttestTask := testdata.LoadTask(\"sleep5\")\n\texpiresAt := time.Now().Unix() + 1\n\tattachment := &apieni.ENIAttachment{\n\t\tTaskARN:       \"TaskARN\",\n\t\tAttachmentARN: \"AttachmentARN\",\n\t\tMACAddress:    \"MACAddress\",\n\t\tStatus:        apieni.ENIAttachmentNone,\n\t\tExpiresAt:     time.Unix(expiresAt, 0),\n\t}\n\tstate.AddENIAttachment(attachment)\n\n\tstate.AddTask(testTask)\n\ttestTask.SetDesiredStatus(apitaskstatus.TaskStopped)\n\tdockerTaskEngine.synchronizeState()\n\n\t// If the below call doesn't panic on NPE, it means the ENI attachment has been properly initialized in synchronizeState.\n\tattachment.StopAckTimer()\n}\n\nfunc TestSynchronizeENIAttachmentRemoveData(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tdataClient, cleanup := newTestDataClient(t)\n\tdefer cleanup()\n\n\tclient.EXPECT().ContainerEvents(gomock.Any()).MaxTimes(1)\n\n\terr := taskEngine.Init(ctx)\n\tassert.NoError(t, err)\n\n\ttaskEngine.(*DockerTaskEngine).dataClient = dataClient\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\n\tattachment := &apieni.ENIAttachment{\n\t\tTaskARN:          \"TaskARN\",\n\t\tAttachmentARN:    testAttachmentArn,\n\t\tMACAddress:       \"MACAddress\",\n\t\tStatus:           apieni.ENIAttachmentNone,\n\t\tAttachStatusSent: false,\n\t}\n\n\t// eni attachment data is removed if AttachStatusSent is unset\n\tdockerTaskEngine.state.AddENIAttachment(attachment)\n\tassert.NoError(t, dockerTaskEngine.dataClient.SaveENIAttachment(attachment))\n\n\tdockerTaskEngine.synchronizeState()\n\tattachments, err := dockerTaskEngine.dataClient.GetENIAttachments()\n\tassert.NoError(t, err)\n\tassert.Len(t, attachments, 0)\n}\n\nfunc TestTaskSecretsEnvironmentVariables(t *testing.T) {\n\t// metadata required for createContainer workflow validation\n\ttaskARN := \"secretsTask\"\n\ttaskFamily := \"secretsTaskFamily\"\n\ttaskVersion := \"1\"\n\ttaskContainerName := \"secretsContainer\"\n\n\t// metadata required for ssm secret resource validation\n\tssmSecretName := \"mySSMSecret\"\n\tssmSecretValueFrom := \"ssm/mySecret\"\n\tssmSecretRetrievedValue := \"mySSMSecretValue\"\n\tssmSecretRegion := \"us-west-2\"\n\n\t// metadata required for asm secret resource validation\n\tasmSecretName := \"myASMSecret\"\n\tasmSecretValueFrom := \"arn:aws:secretsmanager:region:account-id:secret:\" + asmSecretName\n\tasmSecretRetrievedValue := \"myASMSecretValue\"\n\tasmSecretRegion := \"us-west-2\"\n\tasmSecretKey := asmSecretValueFrom + \"_\" + asmSecretRegion\n\n\tssmExpectedEnvVar := ssmSecretName + \"=\" + ssmSecretRetrievedValue\n\tasmExpectedEnvVar := asmSecretName + \"=\" + asmSecretRetrievedValue\n\n\ttestCases := []struct {\n\t\tname        string\n\t\tsecrets     []apicontainer.Secret\n\t\tssmSecret   apicontainer.Secret\n\t\tasmSecret   apicontainer.Secret\n\t\texpectedEnv []string\n\t}{\n\t\t{\n\t\t\tname: \"ASMSecretAsEnv\",\n\t\t\tsecrets: []apicontainer.Secret{\n\t\t\t\t{\n\t\t\t\t\tName:      ssmSecretName,\n\t\t\t\t\tValueFrom: ssmSecretValueFrom,\n\t\t\t\t\tRegion:    ssmSecretRegion,\n\t\t\t\t\tTarget:    \"LOG_DRIVER\",\n\t\t\t\t\tProvider:  \"ssm\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName:      asmSecretName,\n\t\t\t\t\tValueFrom: asmSecretValueFrom,\n\t\t\t\t\tRegion:    asmSecretRegion,\n\t\t\t\t\tType:      \"ENVIRONMENT_VARIABLE\",\n\t\t\t\t\tProvider:  \"asm\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tssmSecret: apicontainer.Secret{\n\t\t\t\tName:      ssmSecretName,\n\t\t\t\tValueFrom: ssmSecretValueFrom,\n\t\t\t\tRegion:    ssmSecretRegion,\n\t\t\t\tTarget:    \"LOG_DRIVER\",\n\t\t\t\tProvider:  \"ssm\",\n\t\t\t},\n\t\t\tasmSecret: apicontainer.Secret{\n\t\t\t\tName:      asmSecretName,\n\t\t\t\tValueFrom: asmSecretValueFrom,\n\t\t\t\tRegion:    asmSecretRegion,\n\t\t\t\tType:      \"ENVIRONMENT_VARIABLE\",\n\t\t\t\tProvider:  \"asm\",\n\t\t\t},\n\t\t\texpectedEnv: []string{asmExpectedEnvVar},\n\t\t},\n\t\t{\n\t\t\tname: \"SSMSecretAsEnv\",\n\t\t\tsecrets: []apicontainer.Secret{\n\t\t\t\t{\n\t\t\t\t\tName:      ssmSecretName,\n\t\t\t\t\tValueFrom: ssmSecretValueFrom,\n\t\t\t\t\tRegion:    ssmSecretRegion,\n\t\t\t\t\tType:      \"ENVIRONMENT_VARIABLE\",\n\t\t\t\t\tProvider:  \"ssm\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName:      asmSecretName,\n\t\t\t\t\tValueFrom: asmSecretValueFrom,\n\t\t\t\t\tRegion:    asmSecretRegion,\n\t\t\t\t\tTarget:    \"LOG_DRIVER\",\n\t\t\t\t\tProvider:  \"asm\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tssmSecret: apicontainer.Secret{\n\t\t\t\tName:      ssmSecretName,\n\t\t\t\tValueFrom: ssmSecretValueFrom,\n\t\t\t\tRegion:    ssmSecretRegion,\n\t\t\t\tType:      \"ENVIRONMENT_VARIABLE\",\n\t\t\t\tProvider:  \"ssm\",\n\t\t\t},\n\t\t\tasmSecret: apicontainer.Secret{\n\t\t\t\tName:      asmSecretName,\n\t\t\t\tValueFrom: asmSecretValueFrom,\n\t\t\t\tRegion:    asmSecretRegion,\n\t\t\t\tTarget:    \"LOG_DRIVER\",\n\t\t\t\tProvider:  \"asm\",\n\t\t\t},\n\t\t\texpectedEnv: []string{ssmExpectedEnvVar},\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, mockTime, taskEngine, credentialsManager, _, _ := mocks(t, ctx, &defaultConfig)\n\t\t\tdefer ctrl.Finish()\n\n\t\t\t// sample test\n\t\t\ttestTask := &apitask.Task{\n\t\t\t\tArn:     taskARN,\n\t\t\t\tFamily:  taskFamily,\n\t\t\t\tVersion: taskVersion,\n\t\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t\t{\n\t\t\t\t\t\tName:    taskContainerName,\n\t\t\t\t\t\tSecrets: tc.secrets,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\t// metadata required for execution role authentication workflow\n\t\t\tcredentialsID := \"execution role\"\n\t\t\texecutionRoleCredentials := credentials.IAMRoleCredentials{\n\t\t\t\tCredentialsID: credentialsID,\n\t\t\t}\n\t\t\ttaskIAMcreds := credentials.TaskIAMRoleCredentials{\n\t\t\t\tIAMRoleCredentials: executionRoleCredentials,\n\t\t\t}\n\n\t\t\t// configure the task and container to use execution role\n\t\t\ttestTask.SetExecutionRoleCredentialsID(credentialsID)\n\n\t\t\t// validate base config\n\t\t\texpectedConfig, err := testTask.DockerConfig(testTask.Containers[0], defaultDockerClientAPIVersion)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\texpectedConfig.Labels = map[string]string{\n\t\t\t\t\"com.amazonaws.ecs.task-arn\":                taskARN,\n\t\t\t\t\"com.amazonaws.ecs.container-name\":          taskContainerName,\n\t\t\t\t\"com.amazonaws.ecs.task-definition-family\":  taskFamily,\n\t\t\t\t\"com.amazonaws.ecs.task-definition-version\": taskVersion,\n\t\t\t\t\"com.amazonaws.ecs.cluster\":                 \"\",\n\t\t\t}\n\n\t\t\t// required to validate container config includes secrets as environment variables\n\t\t\texpectedConfig.Env = tc.expectedEnv\n\n\t\t\t// required for validating ssm workflows\n\t\t\tssmClientCreator := mock_ssm_factory.NewMockSSMClientCreator(ctrl)\n\t\t\tmockSSMClient := mock_ssmiface.NewMockSSMClient(ctrl)\n\n\t\t\tssmRequirements := map[string][]apicontainer.Secret{\n\t\t\t\tssmSecretRegion: []apicontainer.Secret{\n\t\t\t\t\ttc.ssmSecret,\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tssmSecretRes := ssmsecret.NewSSMSecretResource(\n\t\t\t\ttestTask.Arn,\n\t\t\t\tssmRequirements,\n\t\t\t\tcredentialsID,\n\t\t\t\tcredentialsManager,\n\t\t\t\tssmClientCreator)\n\n\t\t\t// required for validating asm workflows\n\t\t\tasmClientCreator := mock_asm_factory.NewMockClientCreator(ctrl)\n\t\t\tmockASMClient := mock_secretsmanageriface.NewMockSecretsManagerAPI(ctrl)\n\n\t\t\tasmRequirements := map[string]apicontainer.Secret{\n\t\t\t\tasmSecretKey: tc.asmSecret,\n\t\t\t}\n\n\t\t\tasmSecretRes := asmsecret.NewASMSecretResource(\n\t\t\t\ttestTask.Arn,\n\t\t\t\tasmRequirements,\n\t\t\t\tcredentialsID,\n\t\t\t\tcredentialsManager,\n\t\t\t\tasmClientCreator)\n\n\t\t\ttestTask.ResourcesMapUnsafe = map[string][]taskresource.TaskResource{\n\t\t\t\tssmsecret.ResourceName: {ssmSecretRes},\n\t\t\t\tasmsecret.ResourceName: {asmSecretRes},\n\t\t\t}\n\n\t\t\tssmClientOutput := &ssm.GetParametersOutput{\n\t\t\t\tInvalidParameters: []*string{},\n\t\t\t\tParameters: []*ssm.Parameter{\n\t\t\t\t\t&ssm.Parameter{\n\t\t\t\t\t\tName:  aws.String(ssmSecretValueFrom),\n\t\t\t\t\t\tValue: aws.String(ssmSecretRetrievedValue),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tasmClientOutput := &secretsmanager.GetSecretValueOutput{\n\t\t\t\tSecretString: aws.String(asmSecretRetrievedValue),\n\t\t\t}\n\n\t\t\treqSecretNames := []*string{aws.String(ssmSecretValueFrom)}\n\n\t\t\tcredentialsManager.EXPECT().GetTaskCredentials(credentialsID).Return(taskIAMcreds, true).Times(2)\n\t\t\tssmClientCreator.EXPECT().NewSSMClient(region, executionRoleCredentials).Return(mockSSMClient)\n\t\t\tasmClientCreator.EXPECT().NewASMClient(region, executionRoleCredentials).Return(mockASMClient)\n\n\t\t\tmockSSMClient.EXPECT().GetParameters(gomock.Any()).Do(func(in *ssm.GetParametersInput) {\n\t\t\t\tassert.Equal(t, in.Names, reqSecretNames)\n\t\t\t}).Return(ssmClientOutput, nil).Times(1)\n\n\t\t\tmockASMClient.EXPECT().GetSecretValue(gomock.Any()).Do(func(in *secretsmanager.GetSecretValueInput) {\n\t\t\t\tassert.Equal(t, asmSecretValueFrom, aws.StringValue(in.SecretId))\n\t\t\t}).Return(asmClientOutput, nil).Times(1)\n\n\t\t\trequire.NoError(t, ssmSecretRes.Create())\n\t\t\trequire.NoError(t, asmSecretRes.Create())\n\n\t\t\tmockTime.EXPECT().Now().AnyTimes()\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\n\t\t\t// test validates that the expectedConfig includes secrets are appended as\n\t\t\t// environment varibles\n\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), expectedConfig, gomock.Any(), gomock.Any(), gomock.Any())\n\t\t\tret := taskEngine.(*DockerTaskEngine).createContainer(testTask, testTask.Containers[0])\n\t\t\tassert.Nil(t, ret.Error)\n\n\t\t})\n\t}\n}\n\n// TestCreateContainerAddFirelensLogDriverConfig tests that in createContainer, when the\n// container is using firelens log driver, its logConfig is properly set.\nfunc TestCreateContainerAddFirelensLogDriverConfig(t *testing.T) {\n\ttaskName := \"logSenderTask\"\n\ttaskARN := \"arn:aws:ecs:region:account-id:task/task-id\"\n\ttaskID := \"task-id\"\n\ttaskFamily := \"logSenderTaskFamily\"\n\ttaskVersion := \"1\"\n\tlogDriverTypeFirelens := \"awsfirelens\"\n\tdataLogDriverPath := \"/data/firelens/\"\n\tdataLogDriverSocketPath := \"/socket/fluent.sock\"\n\tsocketPathPrefix := \"unix://\"\n\tnetworkModeBridge := \"bridge\"\n\tnetworkModeAWSVPC := \"awsvpc\"\n\tbridgeIPAddr := \"bridgeIP\"\n\tenvVarBridgeMode := \"FLUENT_HOST=bridgeIP\"\n\tenvVarPort := \"FLUENT_PORT=24224\"\n\tenvVarAWSVPCMode := \"FLUENT_HOST=127.0.0.1\"\n\teniIPv4Address := \"10.0.0.2\"\n\tgetTask := func(logDriverType string, networkMode string) *apitask.Task {\n\t\trawHostConfigInput := dockercontainer.HostConfig{\n\t\t\tLogConfig: dockercontainer.LogConfig{\n\t\t\t\tType: logDriverType,\n\t\t\t\tConfig: map[string]string{\n\t\t\t\t\t\"key1\": \"value1\",\n\t\t\t\t\t\"key2\": \"value2\",\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\trawHostConfig, err := json.Marshal(&rawHostConfigInput)\n\t\trequire.NoError(t, err)\n\t\treturn &apitask.Task{\n\t\t\tArn:     taskARN,\n\t\t\tVersion: taskVersion,\n\t\t\tFamily:  taskFamily,\n\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t{\n\t\t\t\t\tName: taskName,\n\t\t\t\t\tDockerConfig: apicontainer.DockerConfig{\n\t\t\t\t\t\tHostConfig: func() *string {\n\t\t\t\t\t\t\ts := string(rawHostConfig)\n\t\t\t\t\t\t\treturn &s\n\t\t\t\t\t\t}(),\n\t\t\t\t\t},\n\t\t\t\t\tNetworkModeUnsafe: networkMode,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"test-container\",\n\t\t\t\t\tFirelensConfig: &apicontainer.FirelensConfig{\n\t\t\t\t\t\tType: \"fluentd\",\n\t\t\t\t\t},\n\t\t\t\t\tNetworkModeUnsafe: networkMode,\n\t\t\t\t\tNetworkSettingsUnsafe: &types.NetworkSettings{\n\t\t\t\t\t\tDefaultNetworkSettings: types.DefaultNetworkSettings{\n\t\t\t\t\t\t\tIPAddress: bridgeIPAddr,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\tgetTaskWithENI := func(logDriverType string, networkMode string) *apitask.Task {\n\t\trawHostConfigInput := dockercontainer.HostConfig{\n\t\t\tLogConfig: dockercontainer.LogConfig{\n\t\t\t\tType: logDriverType,\n\t\t\t\tConfig: map[string]string{\n\t\t\t\t\t\"key1\": \"value1\",\n\t\t\t\t\t\"key2\": \"value2\",\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\trawHostConfig, err := json.Marshal(&rawHostConfigInput)\n\t\trequire.NoError(t, err)\n\t\treturn &apitask.Task{\n\t\t\tArn:     taskARN,\n\t\t\tVersion: taskVersion,\n\t\t\tFamily:  taskFamily,\n\t\t\tENIs: []*apieni.ENI{\n\t\t\t\t{\n\t\t\t\t\tIPV4Addresses: []*apieni.ENIIPV4Address{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tAddress: eniIPv4Address,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t{\n\t\t\t\t\tName: taskName,\n\t\t\t\t\tDockerConfig: apicontainer.DockerConfig{\n\t\t\t\t\t\tHostConfig: func() *string {\n\t\t\t\t\t\t\ts := string(rawHostConfig)\n\t\t\t\t\t\t\treturn &s\n\t\t\t\t\t\t}(),\n\t\t\t\t\t},\n\t\t\t\t\tNetworkModeUnsafe: networkMode,\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tName: \"test-container\",\n\t\t\t\t\tFirelensConfig: &apicontainer.FirelensConfig{\n\t\t\t\t\t\tType: \"fluentd\",\n\t\t\t\t\t},\n\t\t\t\t\tNetworkModeUnsafe: networkMode,\n\t\t\t\t\tNetworkSettingsUnsafe: &types.NetworkSettings{\n\t\t\t\t\t\tDefaultNetworkSettings: types.DefaultNetworkSettings{\n\t\t\t\t\t\t\tIPAddress: bridgeIPAddr,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\ttestCases := []struct {\n\t\tname                           string\n\t\ttask                           *apitask.Task\n\t\texpectedLogConfigType          string\n\t\texpectedLogConfigTag           string\n\t\texpectedLogConfigFluentAddress string\n\t\texpectedFluentdAsyncConnect    string\n\t\texpectedSubSecondPrecision     string\n\t\texpectedIPAddress              string\n\t\texpectedPort                   string\n\t}{\n\t\t{\n\t\t\tname:                           \"test container that uses firelens log driver with default mode\",\n\t\t\ttask:                           getTask(logDriverTypeFirelens, \"\"),\n\t\t\texpectedLogConfigType:          logDriverTypeFluentd,\n\t\t\texpectedLogConfigTag:           taskName + \"-firelens-\" + taskID,\n\t\t\texpectedFluentdAsyncConnect:    strconv.FormatBool(true),\n\t\t\texpectedSubSecondPrecision:     strconv.FormatBool(true),\n\t\t\texpectedLogConfigFluentAddress: socketPathPrefix + filepath.Join(defaultConfig.DataDirOnHost, dataLogDriverPath, taskID, dataLogDriverSocketPath),\n\t\t\texpectedIPAddress:              envVarBridgeMode,\n\t\t\texpectedPort:                   envVarPort,\n\t\t},\n\t\t{\n\t\t\tname:                           \"test container that uses firelens log driver with bridge mode\",\n\t\t\ttask:                           getTask(logDriverTypeFirelens, networkModeBridge),\n\t\t\texpectedLogConfigType:          logDriverTypeFluentd,\n\t\t\texpectedLogConfigTag:           taskName + \"-firelens-\" + taskID,\n\t\t\texpectedFluentdAsyncConnect:    strconv.FormatBool(true),\n\t\t\texpectedSubSecondPrecision:     strconv.FormatBool(true),\n\t\t\texpectedLogConfigFluentAddress: socketPathPrefix + filepath.Join(defaultConfig.DataDirOnHost, dataLogDriverPath, taskID, dataLogDriverSocketPath),\n\t\t\texpectedIPAddress:              envVarBridgeMode,\n\t\t\texpectedPort:                   envVarPort,\n\t\t},\n\t\t{\n\t\t\tname:                           \"test container that uses firelens log driver with awsvpc mode\",\n\t\t\ttask:                           getTaskWithENI(logDriverTypeFirelens, networkModeAWSVPC),\n\t\t\texpectedLogConfigType:          logDriverTypeFluentd,\n\t\t\texpectedLogConfigTag:           taskName + \"-firelens-\" + taskID,\n\t\t\texpectedFluentdAsyncConnect:    strconv.FormatBool(true),\n\t\t\texpectedSubSecondPrecision:     strconv.FormatBool(true),\n\t\t\texpectedLogConfigFluentAddress: socketPathPrefix + filepath.Join(defaultConfig.DataDirOnHost, dataLogDriverPath, taskID, dataLogDriverSocketPath),\n\t\t\texpectedIPAddress:              envVarAWSVPCMode,\n\t\t\texpectedPort:                   envVarPort,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\t\t\tdefer ctrl.Finish()\n\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\t\t\tfunc(ctx context.Context,\n\t\t\t\t\tconfig *dockercontainer.Config,\n\t\t\t\t\thostConfig *dockercontainer.HostConfig,\n\t\t\t\t\tname string,\n\t\t\t\t\ttimeout time.Duration) {\n\t\t\t\t\tassert.Equal(t, tc.expectedLogConfigType, hostConfig.LogConfig.Type)\n\t\t\t\t\tassert.Equal(t, tc.expectedLogConfigTag, hostConfig.LogConfig.Config[\"tag\"])\n\t\t\t\t\tassert.Equal(t, tc.expectedLogConfigFluentAddress, hostConfig.LogConfig.Config[\"fluentd-address\"])\n\t\t\t\t\tassert.Equal(t, tc.expectedFluentdAsyncConnect, hostConfig.LogConfig.Config[\"fluentd-async-connect\"])\n\t\t\t\t\tassert.Equal(t, tc.expectedSubSecondPrecision, hostConfig.LogConfig.Config[\"fluentd-sub-second-precision\"])\n\t\t\t\t\tassert.Contains(t, config.Env, tc.expectedIPAddress)\n\t\t\t\t\tassert.Contains(t, config.Env, tc.expectedPort)\n\t\t\t\t})\n\t\t\tret := taskEngine.(*DockerTaskEngine).createContainer(tc.task, tc.task.Containers[0])\n\t\t\tassert.NoError(t, ret.Error)\n\t\t})\n\n\t}\n}\n\nfunc TestCreateFirelensContainerSetFluentdUID(t *testing.T) {\n\ttestTask := &apitask.Task{\n\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName: \"test-container\",\n\t\t\t\tFirelensConfig: &apicontainer.FirelensConfig{\n\t\t\t\t\tType: \"fluentd\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Do(\n\t\tfunc(ctx context.Context,\n\t\t\tconfig *dockercontainer.Config,\n\t\t\thostConfig *dockercontainer.HostConfig,\n\t\t\tname string,\n\t\t\ttimeout time.Duration) {\n\t\t\tassert.Contains(t, config.Env, \"FLUENT_UID=0\")\n\t\t})\n\tret := taskEngine.(*DockerTaskEngine).createContainer(testTask, testTask.Containers[0])\n\tassert.NoError(t, ret.Error)\n}\n\nfunc TestGetBridgeIP(t *testing.T) {\n\tnetworkDefaultIP := \"defaultIP\"\n\tgetNetwork := func(defaultIP string, bridgeIP string, networkMode string) *types.NetworkSettings {\n\t\tendPoint := network.EndpointSettings{\n\t\t\tIPAddress: bridgeIP,\n\t\t}\n\t\treturn &types.NetworkSettings{\n\t\t\tDefaultNetworkSettings: types.DefaultNetworkSettings{\n\t\t\t\tIPAddress: defaultIP,\n\t\t\t},\n\t\t\tNetworks: map[string]*network.EndpointSettings{\n\t\t\t\tnetworkMode: &endPoint,\n\t\t\t},\n\t\t}\n\t}\n\ttestCases := []struct {\n\t\tdefaultIP         string\n\t\tbridgeIP          string\n\t\tnetworkMode       string\n\t\texpectedOk        bool\n\t\texpectedIPAddress string\n\t}{\n\t\t{\n\t\t\tdefaultIP:         networkDefaultIP,\n\t\t\tbridgeIP:          networkBridgeIP,\n\t\t\tnetworkMode:       networkModeBridge,\n\t\t\texpectedOk:        true,\n\t\t\texpectedIPAddress: networkDefaultIP,\n\t\t},\n\t\t{\n\t\t\tdefaultIP:         \"\",\n\t\t\tbridgeIP:          networkBridgeIP,\n\t\t\tnetworkMode:       networkModeBridge,\n\t\t\texpectedOk:        true,\n\t\t\texpectedIPAddress: networkBridgeIP,\n\t\t},\n\t\t{\n\t\t\tdefaultIP:         \"\",\n\t\t\tbridgeIP:          networkBridgeIP,\n\t\t\tnetworkMode:       networkModeAWSVPC,\n\t\t\texpectedOk:        false,\n\t\t\texpectedIPAddress: \"\",\n\t\t},\n\t\t{\n\t\t\tdefaultIP:         \"\",\n\t\t\tbridgeIP:          \"\",\n\t\t\tnetworkMode:       networkModeBridge,\n\t\t\texpectedOk:        false,\n\t\t\texpectedIPAddress: \"\",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tIPAddress, ok := getContainerHostIP(getNetwork(tc.defaultIP, tc.bridgeIP, tc.networkMode))\n\t\tassert.Equal(t, tc.expectedOk, ok)\n\t\tassert.Equal(t, tc.expectedIPAddress, IPAddress)\n\t}\n}\n\nfunc TestStartFirelensContainerRetryForContainerIP(t *testing.T) {\n\tdockerMetaDataWithoutNetworkSettings := dockerapi.DockerContainerMetadata{\n\t\tDockerID: containerID,\n\t\tVolumes: []types.MountPoint{\n\t\t\t{\n\t\t\t\tName:        \"volume\",\n\t\t\t\tSource:      \"/src/vol\",\n\t\t\t\tDestination: \"/vol\",\n\t\t\t},\n\t\t},\n\t}\n\trawHostConfigInput := dockercontainer.HostConfig{\n\t\tLogConfig: dockercontainer.LogConfig{\n\t\t\tType: \"fluentd\",\n\t\t\tConfig: map[string]string{\n\t\t\t\t\"key1\": \"value1\",\n\t\t\t\t\"key2\": \"value2\",\n\t\t\t},\n\t\t},\n\t}\n\tjsonBaseWithoutNetwork := &types.ContainerJSON{\n\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\tID:    containerID,\n\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t},\n\t}\n\n\tjsonBaseWithNetwork := &types.ContainerJSON{\n\t\tContainerJSONBase: &types.ContainerJSONBase{\n\t\t\tID:    containerID,\n\t\t\tState: &types.ContainerState{Pid: containerPid},\n\t\t},\n\t\tNetworkSettings: &types.NetworkSettings{\n\t\t\tDefaultNetworkSettings: types.DefaultNetworkSettings{\n\t\t\t\tIPAddress: networkBridgeIP,\n\t\t\t},\n\t\t\tNetworks: map[string]*network.EndpointSettings{\n\t\t\t\tapitask.BridgeNetworkMode: &network.EndpointSettings{\n\t\t\t\t\tIPAddress: networkBridgeIP,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\trawHostConfig, err := json.Marshal(&rawHostConfigInput)\n\trequire.NoError(t, err)\n\ttestTask := &apitask.Task{\n\t\tArn:     \"arn:aws:ecs:region:account-id:task/task-id\",\n\t\tVersion: \"1\",\n\t\tFamily:  \"logSenderTaskFamily\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName: \"logSenderTask\",\n\t\t\t\tDockerConfig: apicontainer.DockerConfig{\n\t\t\t\t\tHostConfig: func() *string {\n\t\t\t\t\t\ts := string(rawHostConfig)\n\t\t\t\t\t\treturn &s\n\t\t\t\t\t}(),\n\t\t\t\t},\n\t\t\t\tNetworkModeUnsafe: apitask.BridgeNetworkMode,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName: \"test-container\",\n\t\t\t\tFirelensConfig: &apicontainer.FirelensConfig{\n\t\t\t\t\tType: \"fluentd\",\n\t\t\t\t},\n\t\t\t\tNetworkModeUnsafe: apitask.BridgeNetworkMode,\n\t\t\t},\n\t\t},\n\t}\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\ttaskEngine.(*DockerTaskEngine).state.AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).state.AddContainer(&apicontainer.DockerContainer{\n\t\tContainer:  testTask.Containers[1],\n\t\tDockerName: dockerContainerName,\n\t\tDockerID:   containerID,\n\t}, testTask)\n\n\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil).AnyTimes()\n\tclient.EXPECT().StartContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(dockerMetaDataWithoutNetworkSettings).AnyTimes()\n\tgomock.InOrder(\n\t\tclient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).\n\t\t\tReturn(jsonBaseWithoutNetwork, nil),\n\t\tclient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).\n\t\t\tReturn(jsonBaseWithoutNetwork, nil),\n\t\tclient.EXPECT().InspectContainer(gomock.Any(), containerID, gomock.Any()).\n\t\t\tReturn(jsonBaseWithNetwork, nil),\n\t)\n\tret := taskEngine.(*DockerTaskEngine).startContainer(testTask, testTask.Containers[1])\n\tassert.NoError(t, ret.Error)\n\tassert.Equal(t, jsonBaseWithNetwork.NetworkSettings, ret.NetworkSettings)\n}\n\nfunc TestStartExecAgent(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tnowTime := time.Now()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\tdockerTaskEngine.execCmdMgr = execCmdMgr\n\tdefer ctrl.Finish()\n\tconst (\n\t\ttestContainerId = \"123\"\n\t)\n\ttestCases := []struct {\n\t\texecCommandAgentEnabled bool\n\t\texpectContainerEvent    bool\n\t\texecAgentStatus         apicontainerstatus.ManagedAgentStatus\n\t}{\n\t\t{\n\t\t\texecCommandAgentEnabled: false,\n\t\t\texpectContainerEvent:    false,\n\t\t\texecAgentStatus:         apicontainerstatus.ManagedAgentStopped,\n\t\t},\n\t\t{\n\t\t\texecCommandAgentEnabled: true,\n\t\t\texpectContainerEvent:    true,\n\t\t\texecAgentStatus:         apicontainerstatus.ManagedAgentRunning,\n\t\t},\n\t}\n\tfor _, tc := range testCases {\n\t\tstateChangeEvents := taskEngine.StateChangeEvents()\n\t\ttestTask := &apitask.Task{\n\t\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t{\n\t\t\t\t\tName:              \"test-container\",\n\t\t\t\t\tRuntimeID:         testContainerId,\n\t\t\t\t\tKnownStatusUnsafe: apicontainerstatus.ContainerStopped,\n\t\t\t\t\tExecCommandAgentMetadata: apicontainer.ExecCommandAgentMetadata{\n\t\t\t\t\t\tStartedAt: nowTime,\n\t\t\t\t\t\tStatus:    tc.execAgentStatus,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tExecCommandAgentEnabledUnsafe: tc.execCommandAgentEnabled,\n\t\t}\n\n\t\tmTestTask := &managedTask{\n\t\t\tTask:              testTask,\n\t\t\tengine:            dockerTaskEngine,\n\t\t\tctx:               ctx,\n\t\t\tstateChangeEvents: stateChangeEvents,\n\t\t}\n\n\t\tdockerTaskEngine.state.AddTask(testTask)\n\t\tdockerTaskEngine.managedTasks[testTask.Arn] = mTestTask\n\n\t\t// check for expected containerEvent in stateChangeEvents\n\t\twaitDone := make(chan struct{})\n\t\texpectedManagedAgent := apicontainer.ManagedAgent{\n\t\t\tStatus: apicontainerstatus.ManagedAgentRunning,\n\t\t}\n\t\tgo checkManagedAgentEvents(t, tc.expectContainerEvent, stateChangeEvents, expectedManagedAgent, waitDone)\n\n\t\tclient.EXPECT().StartContainer(gomock.Any(), gomock.Any(), gomock.Any()).Return(\n\t\t\tdockerapi.DockerContainerMetadata{DockerID: containerID}).AnyTimes()\n\t\tif tc.execCommandAgentEnabled {\n\t\t\texecCmdMgr.EXPECT().InitializeTask(testTask).AnyTimes()\n\t\t\texecCmdMgr.EXPECT().StartAgent(gomock.Any(), client, testTask, testTask.Containers[0], testContainerId).AnyTimes()\n\t\t}\n\t\tret := taskEngine.(*DockerTaskEngine).startContainer(testTask, testTask.Containers[0])\n\t\tassert.NoError(t, ret.Error)\n\n\t\ttimeout := false\n\t\tselect {\n\t\tcase <-waitDone:\n\t\tcase <-time.After(time.Second):\n\t\t\ttimeout = true\n\t\t}\n\t\tassert.False(t, timeout)\n\t}\n}\n\nfunc TestMonitorExecAgentRunning(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\tdockerTaskEngine.execCmdMgr = execCmdMgr\n\tdockerTaskEngine.monitorExecAgentsInterval = 2 * time.Millisecond\n\tdefer ctrl.Finish()\n\tconst (\n\t\ttestContainerId = \"123\"\n\t)\n\ttestCases := []struct {\n\t\tcontainerStatus                apicontainerstatus.ContainerStatus\n\t\texecCommandAgentMetadata       apicontainer.ExecCommandAgentMetadata\n\t\texecAgentStatus                apicontainerstatus.ManagedAgentStatus\n\t\tsimulateBadContainerId         bool\n\t\texpectedRestartInUnhealthyCall bool\n\t\texpectContainerEvent           bool\n\t}{\n\t\t{\n\t\t\tcontainerStatus:      apicontainerstatus.ContainerStopped,\n\t\t\texpectContainerEvent: false,\n\t\t\texecAgentStatus:      apicontainerstatus.ManagedAgentStopped,\n\t\t},\n\t\t{\n\t\t\tcontainerStatus:        apicontainerstatus.ContainerRunning,\n\t\t\tsimulateBadContainerId: true,\n\t\t\texpectContainerEvent:   false,\n\t\t\texecAgentStatus:        apicontainerstatus.ManagedAgentStopped,\n\t\t},\n\t\t{\n\t\t\tcontainerStatus:      apicontainerstatus.ContainerRunning,\n\t\t\texecAgentStatus:      apicontainerstatus.ManagedAgentRunning,\n\t\t\texpectContainerEvent: true,\n\t\t},\n\t}\n\tfor _, tc := range testCases {\n\t\tnowTime := time.Now()\n\t\tstateChangeEvents := taskEngine.StateChangeEvents()\n\t\ttestTask := &apitask.Task{\n\t\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t{\n\t\t\t\t\tName:      \"test-container\",\n\t\t\t\t\tRuntimeID: testContainerId,\n\t\t\t\t\tExecCommandAgentMetadata: apicontainer.ExecCommandAgentMetadata{\n\t\t\t\t\t\tStartedAt: nowTime,\n\t\t\t\t\t\tStatus:    tc.execAgentStatus,\n\t\t\t\t\t},\n\n\t\t\t\t\tKnownStatusUnsafe: tc.containerStatus,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\n\t\tmTestTask := &managedTask{\n\t\t\tTask:              testTask,\n\t\t\tengine:            dockerTaskEngine,\n\t\t\tctx:               ctx,\n\t\t\tstateChangeEvents: stateChangeEvents,\n\t\t}\n\n\t\tdockerTaskEngine.state.AddTask(testTask)\n\n\t\tif tc.simulateBadContainerId {\n\t\t\ttestTask.Containers[0].RuntimeID = \"\"\n\t\t}\n\t\tif tc.containerStatus == apicontainerstatus.ContainerRunning && !tc.simulateBadContainerId {\n\t\t\texecCmdMgr.EXPECT().RestartAgentIfStopped(dockerTaskEngine.ctx, dockerTaskEngine.client, testTask, testTask.Containers[0], testContainerId).\n\t\t\t\tReturn(execcmd.NotRestarted, nil).\n\t\t\t\tTimes(1)\n\t\t}\n\t\t// check for expected containerEvent in stateChangeEvents\n\t\twaitDone := make(chan struct{})\n\t\texpectedManagedAgent := apicontainer.ManagedAgent{\n\t\t\tStatus: apicontainerstatus.ManagedAgentRunning,\n\t\t}\n\t\tgo checkManagedAgentEvents(t, tc.expectContainerEvent, stateChangeEvents, expectedManagedAgent, waitDone)\n\n\t\ttaskEngine.(*DockerTaskEngine).monitorExecAgentRunning(ctx, mTestTask, testTask.Containers[0])\n\n\t\ttimeout := false\n\t\tselect {\n\t\tcase <-waitDone:\n\t\tcase <-time.After(time.Second):\n\t\t\ttimeout = true\n\t\t}\n\t\tassert.False(t, timeout)\n\n\t}\n}\n\nfunc TestMonitorExecAgentProcesses(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tnowTime := time.Now()\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\tdockerTaskEngine.execCmdMgr = execCmdMgr\n\tdockerTaskEngine.monitorExecAgentsInterval = 2 * time.Millisecond\n\tdefer ctrl.Finish()\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\ttestTask := &apitask.Task{\n\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName:      \"test-container\",\n\t\t\t\tRuntimeID: \"runtime-ID\",\n\t\t\t\tExecCommandAgentMetadata: apicontainer.ExecCommandAgentMetadata{\n\t\t\t\t\tStartedAt: nowTime,\n\t\t\t\t\tStatus:    apicontainerstatus.ManagedAgentRunning,\n\t\t\t\t},\n\t\t\t\tKnownStatusUnsafe: apicontainerstatus.ContainerRunning,\n\t\t\t},\n\t\t},\n\t\tKnownStatusUnsafe:             apitaskstatus.TaskRunning,\n\t\tExecCommandAgentEnabledUnsafe: true,\n\t}\n\tmTestTask := &managedTask{\n\t\tTask:              testTask,\n\t\tengine:            dockerTaskEngine,\n\t\tctx:               ctx,\n\t\tstateChangeEvents: stateChangeEvents,\n\t}\n\tdockerTaskEngine.state.AddTask(testTask)\n\tdockerTaskEngine.managedTasks[testTask.Arn] = mTestTask\n\trestartCtx, restartCancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer restartCancel()\n\texecCmdMgr.EXPECT().RestartAgentIfStopped(dockerTaskEngine.ctx, dockerTaskEngine.client, testTask, testTask.Containers[0], testTask.Containers[0].RuntimeID).\n\t\tDoAndReturn(\n\t\t\tfunc(ctx context.Context, client dockerapi.DockerClient, task *apitask.Task, container *apicontainer.Container, containerId string) (execcmd.RestartStatus, error) {\n\t\t\t\tdefer restartCancel()\n\t\t\t\treturn execcmd.NotRestarted, nil\n\t\t\t}).\n\t\tTimes(1)\n\n\texpectContainerEvent := true\n\twaitDone := make(chan struct{})\n\texpectedManagedAgent := apicontainer.ManagedAgent{\n\t\tStatus:        apicontainerstatus.ManagedAgentRunning,\n\t\tName:          \"EXECUTE_COMMAND_AGENT\",\n\t\tLastStartedAt: nowTime,\n\t}\n\n\tgo checkManagedAgentEvents(t, expectContainerEvent, stateChangeEvents, expectedManagedAgent, waitDone)\n\n\tdockerTaskEngine.monitorExecAgentProcesses(dockerTaskEngine.ctx)\n\t<-restartCtx.Done()\n\ttime.Sleep(5 * time.Millisecond)\n\n\ttimeout := false\n\tselect {\n\tcase <-waitDone:\n\tcase <-time.After(time.Second):\n\t\ttimeout = true\n\t}\n\n\tassert.False(t, timeout)\n}\n\nfunc TestMonitorExecAgentProcessExecDisabled(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\tdockerTaskEngine.execCmdMgr = execCmdMgr\n\tdefer ctrl.Finish()\n\ttt := []struct {\n\t\texecCommandAgentEnabled bool\n\t\ttaskStatus              apitaskstatus.TaskStatus\n\t}{\n\t\t{\n\t\t\texecCommandAgentEnabled: false,\n\t\t\ttaskStatus:              apitaskstatus.TaskRunning,\n\t\t},\n\t\t{\n\t\t\texecCommandAgentEnabled: true,\n\t\t\ttaskStatus:              apitaskstatus.TaskStopped,\n\t\t},\n\t}\n\tfor _, test := range tt {\n\t\ttestTask := &apitask.Task{\n\t\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\t\tContainers: []*apicontainer.Container{\n\t\t\t\t{\n\t\t\t\t\tName:              \"test-container\",\n\t\t\t\t\tRuntimeID:         \"runtime-ID\",\n\t\t\t\t\tKnownStatusUnsafe: apicontainerstatus.ContainerRunning,\n\t\t\t\t},\n\t\t\t},\n\t\t\tExecCommandAgentEnabledUnsafe: test.execCommandAgentEnabled,\n\t\t\tKnownStatusUnsafe:             test.taskStatus,\n\t\t}\n\t\tdockerTaskEngine.state.AddTask(testTask)\n\t\tdockerTaskEngine.managedTasks[testTask.Arn] = &managedTask{Task: testTask}\n\t\tdockerTaskEngine.monitorExecAgentProcesses(ctx)\n\t\t// absence of top container expect call indicates it shouldn't have been called\n\t\ttime.Sleep(10 * time.Millisecond)\n\t}\n}\nfunc TestMonitorExecAgentsMultipleContainers(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, _, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdockerTaskEngine := taskEngine.(*DockerTaskEngine)\n\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\tdockerTaskEngine.execCmdMgr = execCmdMgr\n\tdockerTaskEngine.monitorExecAgentsInterval = 2 * time.Millisecond\n\tdefer ctrl.Finish()\n\tstateChangeEvents := taskEngine.StateChangeEvents()\n\n\ttestTask := &apitask.Task{\n\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName:              \"test-container1\",\n\t\t\t\tRuntimeID:         \"runtime-ID1\",\n\t\t\t\tKnownStatusUnsafe: apicontainerstatus.ContainerRunning,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:              \"test-container2\",\n\t\t\t\tRuntimeID:         \"runtime-ID2\",\n\t\t\t\tKnownStatusUnsafe: apicontainerstatus.ContainerRunning,\n\t\t\t},\n\t\t},\n\t\tExecCommandAgentEnabledUnsafe: true,\n\t\tKnownStatusUnsafe:             apitaskstatus.TaskRunning,\n\t}\n\n\tmTestTask := &managedTask{\n\t\tTask:              testTask,\n\t\tengine:            dockerTaskEngine,\n\t\tctx:               ctx,\n\t\tstateChangeEvents: stateChangeEvents,\n\t}\n\n\tdockerTaskEngine.state.AddTask(testTask)\n\tdockerTaskEngine.managedTasks[testTask.Arn] = mTestTask\n\twg := &sync.WaitGroup{}\n\tnumContainers := len(testTask.Containers)\n\twg.Add(numContainers)\n\n\tfor i := 0; i < numContainers; i++ {\n\t\texecCmdMgr.EXPECT().RestartAgentIfStopped(dockerTaskEngine.ctx, dockerTaskEngine.client, testTask, testTask.Containers[i], testTask.Containers[i].RuntimeID).\n\t\t\tDoAndReturn(\n\t\t\t\tfunc(ctx context.Context, client dockerapi.DockerClient, task *apitask.Task, container *apicontainer.Container, containerId string) (execcmd.RestartStatus, error) {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\tdefer discardEvents(stateChangeEvents)()\n\t\t\t\t\treturn execcmd.NotRestarted, nil\n\t\t\t\t}).\n\t\t\tTimes(1)\n\n\t}\n\ttaskEngine.(*DockerTaskEngine).monitorExecAgentProcesses(dockerTaskEngine.ctx)\n\n\twaitDone := make(chan struct{})\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(waitDone)\n\t}()\n\n\ttimeout := false\n\tselect {\n\tcase <-waitDone:\n\tcase <-time.After(time.Second):\n\t\ttimeout = true\n\t}\n\tassert.False(t, timeout)\n\n}\n\nfunc TestPeriodicExecAgentsMonitoring(t *testing.T) {\n\tctx, cancel := context.WithCancel(context.TODO())\n\tdefer cancel()\n\tctrl, client, _, taskEngine, _, _, _ := mocks(t, ctx, &defaultConfig)\n\tdefer ctrl.Finish()\n\texecAgentPID := \"1234\"\n\tresp := &dockercontainer.ContainerTopOKBody{\n\t\tProcesses: [][]string{{\"root\", execAgentPID}},\n\t}\n\ttestTask := &apitask.Task{\n\t\tArn: \"arn:aws:ecs:region:account-id:task/test-task-arn\",\n\t\tContainers: []*apicontainer.Container{\n\t\t\t{\n\t\t\t\tName:      \"test-container\",\n\t\t\t\tRuntimeID: \"runtime-ID\",\n\t\t\t\tExecCommandAgentMetadata: apicontainer.ExecCommandAgentMetadata{\n\t\t\t\t\tPID: execAgentPID,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\tExecCommandAgentEnabledUnsafe: true,\n\t}\n\ttaskEngine.(*DockerTaskEngine).monitorExecAgentsInterval = 2 * time.Millisecond\n\ttaskEngine.(*DockerTaskEngine).state.AddTask(testTask)\n\ttaskEngine.(*DockerTaskEngine).managedTasks[testTask.Arn] = &managedTask{Task: testTask}\n\ttopCtx, topCancel := context.WithTimeout(context.Background(), time.Second)\n\tdefer topCancel()\n\tclient.EXPECT().TopContainer(gomock.Any(), testTask.Containers[0].RuntimeID, 30*time.Second, execAgentPID).DoAndReturn(\n\t\tfunc(ctx context.Context, containerID string, timeout time.Duration, psArgs ...string) (*dockercontainer.ContainerTopOKBody, error) {\n\t\t\tdefer topCancel()\n\t\t\treturn resp, nil\n\t\t}).AnyTimes()\n\tgo taskEngine.(*DockerTaskEngine).startPeriodicExecAgentsMonitoring(ctx)\n\t<-topCtx.Done()\n\ttime.Sleep(5 * time.Millisecond)\n\tassert.Equal(t, execAgentPID, testTask.Containers[0].GetExecCommandAgentMetadata().PID)\n}\n\nfunc TestCreateContainerWithExecAgent(t *testing.T) {\n\ttestcases := []struct {\n\t\tname  string\n\t\terror error\n\t}{\n\t\t{\n\t\t\tname:  \"ExecAgent config mount success\",\n\t\t\terror: nil,\n\t\t},\n\t\t{\n\t\t\tname:  \"ExecAgent config mount Error\",\n\t\t\terror: errors.New(\"mount error\"),\n\t\t},\n\t}\n\n\tfor _, tc := range testcases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tctx, cancel := context.WithCancel(context.TODO())\n\t\t\tdefer cancel()\n\t\t\tctrl, client, _, engine, _, _, _ := mocks(t, ctx, &config.Config{})\n\t\t\tdefer ctrl.Finish()\n\t\t\ttaskEngine, _ := engine.(*DockerTaskEngine)\n\t\t\texecCmdMgr := mock_execcmdagent.NewMockManager(ctrl)\n\t\t\ttaskEngine.execCmdMgr = execCmdMgr\n\t\t\tsleepTask := testdata.LoadTask(\"sleep5\")\n\t\t\tsleepTask.ExecCommandAgentEnabledUnsafe = true\n\t\t\tsleepContainer, _ := sleepTask.ContainerByName(\"sleep5\")\n\t\t\texecCmdMgr.EXPECT().AddAgentConfigMount(gomock.Any(), gomock.Any()).Return(tc.error)\n\t\t\tclient.EXPECT().APIVersion().Return(defaultDockerClientAPIVersion, nil)\n\t\t\tif tc.error == nil {\n\t\t\t\tclient.EXPECT().CreateContainer(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any())\n\t\t\t}\n\t\t\tmetadata := taskEngine.createContainer(sleepTask, sleepContainer)\n\t\t\tif tc.error != nil {\n\t\t\t\tassert.Equal(t, &apierrors.HostConfigError{Msg: tc.error.Error()}, metadata.Error)\n\t\t\t} else {\n\t\t\t\tassert.NoError(t, metadata.Error)\n\t\t\t}\n\t\t})\n\t}\n}\n", "idx": 1, "id": 25483, "msg": "after `sleepTask := testdata.LoadTask(\"sleep5\")` when can we see a sleepTask with 0 containers? (Is this check just here for thoroughness?)", "proj": "aws-amazon-ecs-agent", "lang": "go", "sampling_weight": 0.13389525325539522}
